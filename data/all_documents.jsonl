{"post_id": 40128, "title": "Mastering Elasticsearch: From Setup to Advanced Index Management", "url": "https://www.luminis.eu/blog-en/mastering-elasticsearch-from-setup-to-advanced-index-management/", "updated_at": "2024-04-30T14:17:07", "body": "Welcome back to our journey through the Elastic Stack, where we transform the complexity of data into clarity and insight. Having laid the groundwork with the setup of Elasticsearch and Kibana using Docker, we embarked on a path from basic indexing to initial data ingestion. Now, it\u2019s time to dive deeper into the heart of data management, and start mastering Elasticsearch.\nAfter the first and second installments, we now navigate the intricacies of Index Lifecycle Management (ILM), and unveil advanced indexing strategies for optimal performance. Join us as we transition from foundational concepts to mastering techniques that enhance application observability, ensuring your data enlightens your operational intelligence. Stay tuned as we unlock the next level of data mastery.\nUnderstanding and Managing Index Lifecycle\nIn Elasticsearch, data isn\u2019t just stored; it embarks on a life cycle journey. Index Lifecycle Management (ILM) emerges as a pivotal feature, orchestrating this journey with finesse. ILM enables the automation of index administration tasks based on predefined policies, optimizing performance and cost. This automation is particularly crucial as data volumes grow and access patterns shift. The life cycle of an index is divided into four main phases: Hot, Warm, Cold, and Delete, each tailored to different stages of data utility and storage requirements.\nSetting Up ILM Policies\nImplementing ILM policies begins with understanding your data\u2019s life cycle. For instance, logs from our e-commerce and inventory management applications might be frequently accessed in the first few days but less so over time. Here\u2019s how to create an ILM policy reflective of this pattern:\n\nHot Phase: Data is actively written and frequently accessed. Indices in this phase are optimized for high performance.\nCold Phase: Data is rarely accessed and can be stored on the cheapest media. It remains searchable but with minimal resource allocation.\nDelete Phase: Data is no longer needed and can be safely deleted to free up storage space.\n\nThrough Elasticsearch\u2019s Kibana interface, we can define these policies with granularity, specifying actions like rollover thresholds, shard allocation, and data deletion criteria.\nHowever, as developers, let\u2019s dive into this with some coding magic!\nFollow these steps to set up Index Lifecycle Management (ILM) policies for your two indices through Elasticsearch\u2019s Kibana Dev Tools. This example includes creating a basic ILM policy that defines actions for the hot, warm, cold, and delete phases tailored to the lifecycle needs of your application logs. Note that these configurations are examples and might need adjustments based on your requirements. We\u2019ve applied a single policy to both indices for simplicity in this example. However, in practice, you might find it beneficial to make individual policies for each index to better align with their specific data management needs.\nFirst, we\u2019ll define a policy named\u00a0log_policy\u00a0for both our indices:\nPUT _ilm/policy/log_policy\r\n{\r\n  \"policy\": {\r\n    \"phases\": {\r\n      \"hot\": {\r\n        \"min_age\": \"0ms\",\r\n        \"actions\": {\r\n          \"rollover\": {\r\n            \"max_age\": \"30d\",\r\n            \"max_size\": \"50GB\"\r\n          }\r\n        }\r\n      },\r\n      \"warm\": {\r\n        \"min_age\": \"30d\",\r\n        \"actions\": {\r\n          \"set_priority\": {\r\n            \"priority\": 50\r\n          },\r\n          \"shrink\": {\r\n            \"number_of_shards\": 1\r\n          }\r\n        }\r\n      },\r\n      \"cold\": {\r\n        \"min_age\": \"90d\",\r\n        \"actions\": {\r\n          \"set_priority\": {\r\n            \"priority\": 20\r\n          },\r\n          \"freeze\": {}\r\n        }\r\n      },\r\n      \"delete\": {\r\n        \"min_age\": \"120d\",\r\n        \"actions\": {\r\n          \"delete\": {}\r\n        }\r\n      }\r\n    }\r\n  }\r\n}\nFour phases\nIt outlines four phases \u2014 hot, warm, cold, and delete \u2014 each with specific actions to optimize storage and access based on the age of the data:\n\nHot Phase: Starts immediately upon index creation (min_age: 0ms). Indices in this phase are actively written to and frequently accessed. The policy triggers a rollover action when an index reaches 30 days of age (max_age: 30d) or grows to 50GB in size (max_size: 50GB), whichever comes first. Rollover prepares the index for the warm phase and creates a new index for incoming data.\nWarm Phase: Begins 30 days after the index\u2019s creation (min_age: 30d). In this phase, the index is less frequently accessed. The policy reduces the index\u2019s priority to 50 (set_priority: 50) to deprioritize it in favor of newer, hot indices. It also shrinks the index to a single shard (number_of_shards: 1), optimizing it for less frequent access and saving resources. More about shards later.\nCold Phase: Activates 90 days after creation (min_age: 90d). Here, the index is rarely accessed and is considered archival. The priority is further reduced to 20 (set_priority: 20), and the index is frozen (freeze: {}), making it read-only and reducing its resource footprint on the cluster.\nDelete Phase: Occurs 120 days after the index\u2019s creation (min_age: 120d). This final action deletes the index, freeing up storage space used by data no longer needed.\n\nOverall, this\u00a0log_policy\u00a0aims to automate data transition through its lifecycle, from active use to deletion, optimizing for performance, accessibility, and storage efficiency at each stage.\nAfter creating this policy, you must apply them to the respective indices. This can be achieved by updating the index settings to use the newly created ILM policy. Here\u2019s how to update the settings for both indices:\nPUT /ecommerce_app_logs/_settings\r\n{\r\n  \"lifecycle.name\": \"log_policy\"\r\n}\r\n\r\nPUT /inventory_management_logs/_settings\r\n{\r\n  \"lifecycle.name\": \"log_policy\"\r\n}\nThese commands will associate each index with the corresponding ILM policy, automating the lifecycle management based on the phases and actions defined. Ensure to adjust the\u00a0max_age,\u00a0max_size, and\u00a0min_age\u00a0settings according to your specific data retention and size requirements.\nNow, you\u2019ve set up ILM policies for your indices directly through Kibana\u2019s Dev Tools, helping manage the data lifecycle efficiently.\nAutomating Rollover with ILM\nOne of ILM\u2019s strengths is its ability to automate the rollover process, creating new indices when the current ones meet certain conditions, such as size, age, or document count. This ensures that indices remain at an optimal size for performance and manageability. Automating rollovers not only streamlines operations but also aids in maintaining consistent performance across your Elasticsearch cluster.\nBy integrating ILM into our Elasticsearch strategy, we can ensure that our data is managed and optimized for every stage of its lifecycle. This results in a more efficient, cost-effective, and performance-oriented data ecosystem.\nAdvanced Indexing Strategies\nElevating our Elasticsearch setup involves refining our indexing strategies to enhance performance, manageability, and scalability. Adopting advanced indexing techniques becomes crucial as our data grows and our needs evolve. Here, we explore optimizing index performance, employing index templates for dynamic creation, and strategies for managing large datasets effectively.\nOptimizing Index Performance\nPerformance optimization of Elasticsearch indices is pivotal for ensuring quick response times and efficient resource utilization. Key strategies include:\n\nSharding: Distribute data across multiple shards to parallelize operations and increase throughput. Consider the number of shards during index creation based on the expected data volume and query load.\n\nPUT /ecommerce_app_logs\r\n{\r\n  \"settings\": {\r\n    \"index\": {\r\n      \"number_of_shards\": 3,\r\n      \"number_of_replicas\": 1\r\n    }\r\n  }\r\n}\n\nReplicas: Increase the number of replicas to improve search performance and ensure high availability. Adjust replica settings based on your cluster\u2019s capacity and search throughput requirements.\n\nPUT /ecommerce_app_logs/_settings\r\n{\r\n  \"index\": {\r\n    \"number_of_replicas\": 2\r\n  }\r\n}\r\n\r\n\nDynamic Index Creation with Templates\nIndex templates automate applying predefined settings, mappings, and aliases to new indices. In dynamic environments where new indices are regularly created, index templates ensure consistency and simplifies management across multiple indice.\n\nCreating an Index Template:\n\nPUT _index_template/ecommerce_template\r\n{\r\n\"template\": {\r\n\"settings\": {\r\n\"number_of_shards\": 3,\r\n\"number_of_replicas\": 1\r\n},\r\n\"mappings\": {\r\n\"properties\": {\r\n\"application-name\": { \"type\": \"text\" },\r\n\"timestamp\": { \"type\": \"date\" },\r\n\"log_level\": { \"type\": \"keyword\" },\r\n\"message\": { \"type\": \"text\" }\r\n}\r\n}\r\n},\r\n\"index_patterns\": [\"ecommerce_app_logs*\"],\r\n\"priority\": 1\r\n}\r\n\nThis template applies to any new index that matches the pattern:\u00a0ecommerce_app_logs*, automating the setup process and ensuring that each index adheres to the defined structure and settings.\nManaging Large Datasets\nHandling large datasets and high ingestion rates requires careful planning:\n\nRollover Indices: Use the rollover API to manage large data volumes by automatically creating new indices based on size, age, or document count. This keeps indices at an optimal size for both write and search operations.\n\nPUT /ecommerce_app_logs\r\n{\r\n\"settings\": {\r\n\"index\": {\r\n\"number_of_shards\": 3,\r\n\"number_of_replicas\": 1\r\n}\r\n}\r\n}\r\n\n\nReplicas: Increase the number of replicas to improve search performance and ensure high availability. Adjust replica settings based on your cluster\u2019s capacity and search throughput requirements.\n\nPUT /ecommerce_app_logs/_settings\r\n{\r\n\"index\": {\r\n\"number_of_replicas\": 2\r\n}\r\n}\nUnlocking the Next Chapter: Visualization Mastery with Kibana\nAs we wrap up our deep dive into Elasticsearch\u2019s powerful capabilities, from setting up our environment with Docker to mastering advanced index management techniques, we\u2019ve laid a robust foundation for enhanced application observability. By understanding and implementing Index Lifecycle Management (ILM) and exploring advanced indexing strategies, we\u2019re not just managing data but optimizing our data\u2019s journey for performance, scalability, and efficiency.\nBut our exploration doesn\u2019t end here. The true potential of the data we\u2019ve managed and optimized is unlocked through visualization and analysis. In our next installment, we\u2019ll venture into the realm of Kibana, where data comes to life. We\u2019ll explore how to transform our indexed data into actionable insights through Kibana\u2019s powerful visualization tools, diving into advanced dashboard creation, and uncovering the stories hidden within our data.\nStay tuned for our next post, where we\u2019ll unlock the visual power of Kibana and elevate our data\u2019s narrative. Together, let\u2019s turn data into decisions and insights into action.\n", "tags": [], "categories": ["Blog", "Search"]}
{"post_id": 40070, "title": "Ditch Postman and save time with IntelliJ\u2019s HTTP client", "url": "https://www.luminis.eu/blog-en/ditch-postman-and-save-time-with-intellijs-http-client/", "updated_at": "2024-04-18T18:19:21", "body": "In this blog post, I explore a vital guide to IntelliJ\u2019s HTTP client.\nWhile setting up an application with REST endpoints, it is often the case that you want to call an endpoint and examine the received response. In more complex situations, you may want to use different methods, send headers, or include cookies. For this purpose, there are, of course, various applications you can use, such as Postman or Insomnia. But why switch between your IDE and another app when you can simply use these functionalities within IntelliJ itself?\nIntelliJ Ultimate users have access to the HTTP client. This client has received a good update, as previously you had to configure the REST calls via a UI \u2013 which resulted in a lot of clicking \u2013 but now it\u2019s possible to configure the REST calls via a text file. This is ideal, as these files are easy to exchange and can even be checked into version control.\nWhy IntelliJ\u2019s HTTP client?\nIntelliJ\u2019s HTTP client seamlessly integrates into your development workflow, eliminating the need to switch between different applications. No longer do I find myself constantly switching between various applications. Here are some reasons why IntelliJ\u2019s HTTP client might just become your preferred choice as well:\n\nNative Integration: When you already use IntelliJ as your primary IDE, using its HTTP client means you don\u2019t have to install or switch to additional software.\nCode Autocomplete: You can leverage IntelliJ\u2019s powerful autocomplete feature to quickly fill in endpoint URLs and headers to improve your productivity.\nCode Navigation: IntelliJ allows you to navigate through your HTTP client files just like any other code file, making it easy to find and organize your API calls within your project.\n\nI\u2019ve found that using IntelliJ for HTTP requests is a solid choice, and its user-friendly design makes it a great tool. Let\u2019s explore some fundamental abilities that I like to use.\nA simple GET call\nSo, how do you use it? First of all, you create a .http file, then you can use plain text to configure calls. We will dive into all the syntax later on. In this example, I\u2019ll first demonstrate how easy it is to make a simple GET call:\nGET https://api.example.com/cards\nThat\u2019s it! And the beauty of this is that IntelliJ knows about the endpoints you are exposing in your project, so it also provides autocomplete for your URLs.\n\nOnce the call is executed, you\u2019ll receive a response code and body directly within your IDE. One way to run this call is by clicking the green play button in the gutter, but we can also search for comments we have provided to run the call. All we have to do is add a comment above the call in the http file by using three hashes and then search for the comment in the Run Anything window (press control twice for Mac users).\n### get all cards\nGET https://api.example.com/cards\n\nThis is very helpful when you want to run a quick call without navigating to the .http file.\nCrafting a call with headers\nThe HTTP client can also handle headers, request bodies, and variables. Let\u2019s use a POST call to explore these features.\n\nIn this example, we\u2019ve included headers specifying the content type as JSON and provided a JSON request body. IntelliJ provides warnings if the syntax of the body doesn\u2019t match the specified content type, ensuring you catch errors early in the development process.\n\nAdditionally, IntelliJ supports variables, both predefined and custom, allowing for dynamic and reusable requests. There are a bunch of predefined variables to help you in generating test values like random.uuid or random.email.\n\nOf course it is also possible to set your own variables. This is useful when you want to prepare data before a call or process it after. But you will also need it if you want to store environment related variables like host, key or password variables. For this purpose we create a separate .env.json file.\n\nIn this file you can set multiple environments, for now we only configure a dev environment. Then we can use this variable in our .http file by using the double curly brackets and selecting the environment in the \u2018Run with\u2019 drop down menu (in this case dev).\n\nHarnessing the power of JavaScript\nIntelliJ\u2019s HTTP client goes beyond simple request crafting; it allows you to leverage JavaScript to run tests and manipulate data before making API calls. Here\u2019s a glimpse of its capabilities:\n\nIn this example, we\u2019re using JavaScript to validate the response status. IntelliJ provides default objects and methods like client.assert() and client.test() for writing tests, as well as access to response data such as headers and body. You can even set your own global variables using client.global.set() or request-specific variables using request.variables.set().\n\nThe above examples are designed to give you an idea of what is possible. But for inspiration to see what more is possible you can always browse the examples button in the right top corner when you have your http file open.\nConclusion\nIn conclusion, IntelliJ\u2019s HTTP client offers a powerful and intuitive solution for API development and testing, seamlessly integrating into your development environment. With features like autocomplete, variable support, and JavaScript capabilities, it provides a comprehensive toolkit for crafting and testing API calls right from your IDE. Whether you\u2019re a seasoned developer or just getting started, IntelliJ\u2019s HTTP client is a valuable addition to your toolbox.\n", "tags": [], "categories": ["Blog", "Development"]}
{"post_id": 39663, "title": "Serverless Websocket with AWS: Chat Application", "url": "https://www.luminis.eu/blog-en/serverless-websocket-with-aws-chat-application/", "updated_at": "2024-04-05T16:14:53", "body": "Serverless and Websockets seemed an unnatural combination to me so when I found out it was possible, I had to try it. Serverless is event-driven in it\u2019s foundation so instead of unnatural it appears to be a match made in heaven. In this blog, I show how to set up a serverless websocket with AWS without much code. Easy as a breeze!\nThis may appear like a lot at first but I promise, we only need 120 (easy to read) lines of code to get the basics running: a chat application. Add 75 extra lines and three commands in the terminal and you have yourself automated deployment via CDK too.\n\nThe basics\nWe need to do a few things, so let\u2019s spell them out:\n\nAn index.html file for the UI\nAn API Gateway with Websocket configuration\nThree lambda\u2019s to connect, send messages and disconnect\nA DynamoDB table to store connection ID\u2019s\n\nThe code\nWe only need an HTML form and Lambda\u2019s to connect, disconnect and send messages. The API Gateway and DynamoDB table are configuration only.\nHTML\nLets write the first 26 lines of code, a simple form. Note that the Stop button isn\u2019t needed, if you close the browser window it will terminate automatically and remove the database ID.\n\n<div id=\"chat-box\"></div>\r\n<input type=\"text\" id=\"message\" placeholder=\"Enter message\"/>\r\n<button onclick=\"connect()\">start</button>\r\n<button onclick=\"sendMessage()\">Send</button>\r\n<button onclick=\"webSocket.close();\">Stop</button>\r\n\r\n<script>\r\nlet webSocket;\r\nconst chatBox = document.getElementById('chat-box');\r\nconst messageInput = document.getElementById('message');\r\n\r\nfunction connect() {\r\n    const webSocketURL = 'WEBSOCKET API GATEWAY URL HERE';\r\n    webSocket = new WebSocket(webSocketURL);\r\n\r\n    webSocket.onmessage = message => \r\n      chatBox.innerHTML += `<div> ${message.data} </div>`;\r\n    webSocket.onopen = () => \r\n      chatBox.innerHTML += '<div>Connected!</div>';\r\n    webSocket.onclose = () => \r\n      chatBox.innerHTML += '<div>Disconnected!</div>';\r\n}\r\n\r\nfunction sendMessage() {\r\n    const message = messageInput.value;\r\n    webSocket.send(JSON.stringify({message}));\r\n    messageInput.value = '';\r\n}\r\n</script>\n\n\u00a0Lambda\u2019s\nWe need 3 lambda\u2019s: one to connect, one to send messages to other connections and one to disconnect. I wrote them in Typescript. You need to remove the types to deploy them to a Lambda as javascript, or follow the guide using CDK (recommended!).\nIn this blog, I won\u2019t show the imports, but you can always find them on GitHub (link at the bottom of this post).\nConnecting\n\nexport const handler = async (event) => {\r\n    const addConnectionParameters = {\r\n        TableName: process.env.TABLE_NAME!,\r\n        Item: {\r\n            connectionId: event.requestContext.connectionId,\r\n            timestamp: new Date().toISOString(),\r\n        }\r\n    };\r\n\r\n    try {\r\n        await dynamo.put(addConnectionParameters);\r\n        return { statusCode: 200, body: 'Connected.' };\r\n    } catch (err) {\r\n        console.error('Error during onConnect:', err);\r\n        return { statusCode: 500, body: 'Failed.' };\r\n    }\r\n};\n\nDisconnecting\n\nexport const handler = async (event) => {\r\n    const deleteConnectionsParameters = {\r\n        TableName: process.env.TABLE_NAME!,\r\n        Key: {\r\n            connectionId: event.requestContext.connectionId\r\n        }\r\n    };\r\n\r\n    try {\r\n        await dynamo.delete(deleteConnectionsParameters);\r\n        return { statusCode: 200, body: 'Disconnected.' };\r\n    } catch (err) {\r\n        console.error('Error during onDisconnect:', err);\r\n        return { statusCode: 500, body: 'Failed.' };\r\n    }\r\n};\n\nSending messages\nWe read the connection ID\u2019s from the database and tell the API Gateway to send them to all active connections that appeared in the database. You could add metadata to group connections.\n\nexport const handler = async ({ body, requestContext }) => {\r\n    const messageData = JSON.parse(body!);\r\n\r\n    const sendMessage = (connectionId: string) => {\r\n        const apiGatewayClient = new ApiGatewayManagementApiClient({\r\n            endpoint: `https://${requestContext.domainName}/${requestContext.stage}`\r\n        });\r\n        const postCommand = new PostToConnectionCommand({\r\n            ConnectionId: connectionId,\r\n            Data: Buffer.from(JSON.stringify(messageData))\r\n        });\r\n        return apiGatewayClient.send(postCommand);\r\n    };\r\n\r\n    const connectionTableName = {\r\n        TableName: process.env.TABLE_NAME!,\r\n    };\r\n    const connections = await dynamoClient.scan(connectionTableName);\r\n\r\n    const sendMessagesToAllConnections = connections.Items?.map((item) =>\r\n        sendMessage(item.connectionId)\r\n    );\r\n\r\n    try {\r\n        await Promise.all(sendMessagesToAllConnections!);\r\n        return { statusCode: 200, body: 'Message sent.' };\r\n    } catch (err) {\r\n        console.error('Error during sendMessage:', err);\r\n        return { statusCode: 500, body: 'Failed.' };\r\n    }\r\n};\n\nCDK\nCDK is not a requirement. This is for automated deployment and makes my life a lot easier, yours too probably! If you don\u2019t want to use it you can read the configuration I used and apply that via the user interface AWS provides.\nTo begin, install NPM and the AWS CLI and log in\n$ aws sso login\nNext, install the CDK CLI using the command\n$ npm i -g aws-cdk\nFinally, create an empty directory for your project and run this command:\n$ cdk init app --language=typescript\nWe need a database, 3 lambda\u2019s with the code shown above and an API Gateaway (plus some settings to make them communicate).\nFirst, let\u2019s build a table using CDK:\n\nconst table = new dynamodb.Table(this, 'MessagesTable', {\r\n  tableName: \"ChatConnections\",\r\n  partitionKey: {\r\n    name: 'connectionId',\r\n    type: dynamodb.AttributeType.STRING\r\n  },\r\n  billingMode: dynamodb.BillingMode.PAY_PER_REQUEST\r\n});\n\nThat table is going to be used to store the connection ID. In the data we will also store a timestamp to see when users connected, but that\u2019s fully optional.\nNext are the Lambda\u2019s. They will be invoked by the API Gateway and store, read and delete the connection ID in the database. I stored the code for these lambda\u2019s in a folder called \u2018lambda\u2019 in the root directory of the project.\n\nconst onConnectLambda = new lambda.Function(this, 'OnConnectLambda', {\r\n  functionName: 'ConnectLambda',\r\n  runtime: lambda.Runtime.NODEJS_20_X,\r\n  handler: 'connect.handler',\r\n  code: lambda.Code.fromAsset(path.join(__dirname, '../lambda')),\r\n  environment: {\r\n    TABLE_NAME: table.tableName\r\n  }\r\n});\r\n\r\nconst onDisconnectLambda = new lambda.Function(this, 'OnDisconnectLambda', {\r\n  functionName: 'DisconnectLambda',\r\n  runtime: lambda.Runtime.NODEJS_20_X,\r\n  handler: 'disconnect.handler',\r\n  code: lambda.Code.fromAsset(path.join(__dirname, '../lambda')),\r\n  environment: {\r\n    TABLE_NAME: table.tableName\r\n  }\r\n});\r\n\r\nconst sendMessageLambda = new lambda.Function(this, 'SendMessageLambda', {\r\n  functionName: 'MessageLambda',\r\n  runtime: lambda.Runtime.NODEJS_20_X,\r\n  handler: 'message.handler',\r\n  code: lambda.Code.fromAsset(path.join(__dirname, '../lambda')),\r\n  environment: {\r\n    TABLE_NAME: table.tableName\r\n  }\r\n});\n\nNow, we need an API Gateway with configuration to support websockets.\n\nconst webSocketApi = new apigateway.WebSocketApi(this, 'WebSocketApi', {\r\n  connectRouteOptions: { integration: new integrations.WebSocketLambdaIntegration( 'connect', onConnectLambda ) },\r\n  disconnectRouteOptions: { integration: new integrations.WebSocketLambdaIntegration('disconnect', onDisconnectLambda) },\r\n  defaultRouteOptions: { integration: new integrations.WebSocketLambdaIntegration('message', sendMessageLambda ) }\r\n});\r\n\r\nconst deploymentStage = new apigateway.WebSocketStage(this, 'DevelopmentStage', {\r\n  webSocketApi,\r\n  stageName: 'dev',\r\n  autoDeploy: true\r\n});\r\n\r\n// Output the WebSocket URL\r\nnew CfnOutput(this, 'WebSocketUrl', { value: deploymentStage.url });\n\nMake sure to copy the lambda code of the previous chapter into three files:\nlambda/connect.ts\nlambda/disconnect.ts\nlambda/message.ts\nPut the lambda folder in the root directory (not the /lib directory).\n\nThat\u2019s it! We now have the code required to create everything. Lets do this thing. Only a few more commands:\n$ npm run build\n$ cdk deploy\nSee the results\nThe console should output the URL of the websocket connection. Make sure to replace the right string in the index.html file.\nAfter that, you should be able to run the index.html file in any browser (no need for a webserver). If everything was alright, it should work!\nIf you see any errors and think you might have skipped something, clone my repository and run it from there to make sure there are no typo\u2019s.\nConclusion\nThis websocket setup is easy! We hardly wrote any code and got this cool serverless setup that is super cheap and reliable. It feels awesome and makes reactive applications truly reactive. Note that there is a maximum message size of 128kb so for media or large queries you may want to refer to another type of API call and only send notifications that data has been updated through websockets. If you are building an application with websockets I would recommend reading up on event-driven architectures as that is a great fit. Check out the repository in the link below for the full code and CDK configurations.\n\nhttps://github.com/vroegop/websockets-serverless-aws-blog\n", "tags": [], "categories": ["Blog", "Cloud"]}
{"post_id": 40000, "title": "AWS Cloud Foundation Series, part 2: Governance", "url": "https://www.luminis.eu/blog-en/aws-cloud-foundation-series-governance/", "updated_at": "2024-03-27T11:25:59", "body": "Taking control of your cloud foundation\nIn the previous blog post of this series on the AWS Cloud Foundation, we looked at the capability-based approach defined by AWS to set up an AWS Cloud Foundation. The first capability, based on that approach, is Governance.\nIn general, \u201cGovernance\u201d refers to structures, policies, and practices that determine how decisions are made, how power and authority are distributed, and how accountability is maintained. This definition also holds in the context of setting up a Cloud Foundation for your business.\nIn this blog post, we look at the decisions that need to be made at the start of your cloud journey to provide the Governance capability for your organization. The benefits of having this capability in your organization include:\n\nOwnership of cloud foundation: It becomes clear who is responsible and thus accountable for which part of the cloud foundation of your organization. This increases the chances that the other capabilities are developed and maintained properly, allowing you to run your AWS workloads in an efficient, secure, reliable, and cost-effective way.\nAvoid unnecessary mistakes: By spending some time upfront on decisions that guide the rest of your cloud journey, you can avoid unnecessary mistakes and the cost that comes with remediating those mistakes later on.\n\nGovernance is a capability that guides all other capabilities that we discuss in this series. We define a structure of ownership for these capabilities and define policies that need to be adhered to while building them.\nThe Structures\nThe first thing we want to define is the governance structure. AWS has presented a set of seven functional areas, each having one owner who is responsible for that functional area:\n\nPlease read the first blog of this series on the AWS Cloud Foundation for more info on this if you have not yet done so. In the illustration above we can see the seven functional areas.\nThe first order of business when starting with the Governance capability is to designate someone to be the owner of the Governance functional area. That person is responsible for everything else that is mentioned in the rest of this blog.\nOnce you have done so, this owner should designate the other members of your organization to be owners of the other functional areas. However, remember that for each of these functional areas, the owner can create a team for support. Once this is done, your organization has a governance structure to achieve a solid Cloud Foundation.\nBased on your organization and the workloads you are going to build for your business, you are free to create any kind of structure on top of or next to this Cloud Foundation structure. For example, a common approach is to assign an owner per workload.\nOne common anti-pattern is to not assign functional area owners, thus leading to the workload owners becoming \u201cshared owners\u201d of the foundation capabilities, which effectively means that no one is responsible for them. This leads to a foundation that is created based on just-in-time needs of the functionality to be built instead of a well thought out foundation based on best practices.\nThe following is an example overview of owners in a Cloud organization with 3 workloads on AWS:\n\nEach of the functional area owners only work on developing foundational capabilities for the organization and not with any functionality that needs to be built for the workloads.\nHowever, the workloads are upon the foundation and use of the foundational capabilities.\nFor example, in the next blog in this series, we cover the capability of \u201cIdentity Management and Access Control\u201d. The owner of the Security Functional Area is responsible for developing this capability. Once that capability is available for the organization, all workloads can make use of it.\nThis increases the speed of development and introduces consistency since the workload owners do not have to think about this (too much) and make use of the same Identity Management and Access Control capabilities as the other workloads in the organization. However, the functional area owners can always take input from the workload owners when specific demands arise and expand the capability in this case.\nAs said in the first blog, depending on the size of your organization certain people might need to take on multiple owner roles. You should assess your organization and decide how to assign these roles in the best way.\nIt is up to you to decide how to map these owners to your organization. For example, the owner of the Finance functional area could be the CFO or someone who reports directly to the CFO. The same with Security and the CSO and so on.\nWith this structure in place, it is now clear who is responsible for what part of the cloud journey, who has authority, and who is accountable.\nAWS Cloud Foundation: the Policies\nWith the governing structures in place, the next step is to decide on the governing policies. These policies guide decision-making for your Cloud Foundation and everything built on top of it.\nWe start by defining some starting policies that have to do with the entire organization and that apply to all foundational capabilities, however, throughout this blog series, we introduce more policies as we go through the capabilities. As your organization grows, so will the Governance capability of your organization.\nPolicy: Security and Compliance\nDepending on the type of business, geographical location, and other factors, you need to adhere to certain regulations when it comes to security and compliance. This is always the case for any business and also holds for when you run your business on AWS.\nSince AWS is now responsible for part of the stack of your workloads, it now falls on AWS to provide proof of compliance to different types of regulation. However, AWS is not completely responsible for your entire workload. This is what AWS refers to as the \u201cshared responsibility model\u201d:\n\n\u00a0 \u00a0 \u00a0 AWS shared responsibility model.\nAs we can see in the image above, you as a customer are still responsible for certain parts of your workload. However, this image is not completely accurate for all scenarios. You can shift even more responsibility to AWS depending on which services you use:\n\nResponsibility based on level of service usage.\nFor example, there is a difference in responsibility if you choose to use AWS EC2 (IaaS) for your computing needs instead of AWS Lambda (PaaS), since with Lambda you are no longer responsible for the OS and patching security vulnerabilities. However, if you can find a SaaS for the functionality you need, you will shift even more to the right and have fewer responsibilities.\nHow does this information help guide you in building your workloads? First of all, understanding the shared responsibility model is important so that you are aware that just because your workloads run in AWS does not mean that they are automatically secure. This means that all security regulations that would hold for your organization when running your workload outside of the Cloud still holds now.\nFor example, when looking at the General Data Protection Regulation (GDPR), AWS provides many tools to help you manage certain aspects. However, it is still your responsibility to use these tools accordingly. If we take the \u201cright to be forgotten\u201d principle in GDRP, AWS provides all the mechanisms to be able to delete a user\u2019s data, however, you are responsible for making use of this in the correct way.\nKnowing this upfront can help you design your workloads in a way that adheres to the regulations that are relevant to your organization while making use of AWS to lessen the burden.\nProof of compliance\nWhen you need to provide proof of compliance to external auditors, you must combine the proof of compliance of your organization and that of the AWS services you use in your workload.\nFor the AWS proof of compliance, AWS has created a service called AWS Artifact. It can be accessed through the AWS console and contains reports and agreements.\nReports are a set of documents that prove that AWS adheres to certain regulatory programs. These are performed by external auditors. You can search through all of the reports and download the ones that you might need to give to external auditors.\n\nTo download a report, you first must download a pdf that includes the terms & conditions as well as instructions on how to actually download the report itself.\nAgreements are legal agreements that you can make with AWS when this is necessary. One situation is when your business deals with personal health information (HIPAA). In this case, you can also download the agreement and follow the instructions to accept this agreement. You can always see in this overview which agreements are active:\n\nThere are only 5 agreements, if these do not apply to your organization then you can skip this.\nPolicy: Regions\nAWS runs all over the world at this point. They operate in different \u201cRegions\u201d across the world. When setting up your Cloud Foundation on AWS, it is important to decide which Regions your organization will operate in. This policy creates the boundaries that other foundational capabilities need and ensures that no resources are being used in unwanted Regions.\n\nThese Regions are geographical locations around the world where AWS has multiple physical data centers. Within each Region there are multiple Availability Zones. An Availability Zone is a data center or a cluster of data centers within a Region. Availability Zones are isolated from each other, meaning that they do not impact or depend on each other when it comes to power, networking and connectivity. AWS provides an overview of all Regions and Availability Zones.\nHow do you decide as an organization which AWS Regions to deploy your workloads in? Here are the most important (general) factors:\nCompliance and Legal Requirements\nIf you don\u2019t adhere to the Compliance and Legal Requirements, then none of the other points in this list matter. Laws in certain countries or regions may dictate its mandatory to store and process data in a specific geographic location. This is particularly relevant in industries like finance, healthcare, and public sector. Specific compliance requirements (e.g., GDPR in Europe) might also influence the choice of Region to ensure adherence to legal and regulatory standards.\nLatency\nWhere will the end-users be geographically when using your workload? Deploying applications closer to the end-users can significantly reduce latency, improving user experience. If your users are in multiple regions, then it is possible to deploy to multiple Regions. More on that later.\nCost\nThe cost of AWS services can vary between Regions. Depending on the service mix, choosing a Region with lower costs can lead to significant savings. Also consider the cost of data transfer both into and out of AWS Regions, especially if your architecture involves significant data movement. Keeping data transfer inside one Region is more cost-effective than moving data between Regions.\nMost AWS services have a page related to the pricing of that service and they allow you to select different Regions to be able to see the difference in pricing.\nService Availability\nNot all AWS services are available in every Region. If your application depends on certain AWS services, you\u2019ll need to choose a Region where these services are available. Also, AWS often launches new services and features in certain Regions before rolling them out globally. If access to the latest AWS innovations is critical, this may influence your Region choice.\nFor the United States, the US-East(N. Virginia) region tends to get the newest functionality first. In Europe, this has been the EU-West (Ireland) region.\nDisaster Recovery and High Availability\nTo design a robust disaster recovery plan, it\u2019s essential to deploy applications across multiple Regions or Availability Zones to ensure that an outage in one Region doesn\u2019t impact your entire operation. The number of Availability Zones in a Region can affect your ability to design highly available architectures.\nThe newer regions tend to have less availability zones, so if your High Availability plan requires more than 3 Availability Zones in a Region, you might be better off not using the newest regions.\nMultiple Regions\nIf based on the information above it turns out that you need to use multiple regions, then it is still recommended to choose one AWS Region as your main region. Typically this would be the AWS Region closest to your organization\u2019s headquarters. Later in this blog series we cover the capability \u201cWorkload Isolation\u201d, where we look at how to structure your AWS account structure. The decisions made here in the Governance capability are there to determine the best structure.\nPolicy: Procurement Agreements\nDepending on the size of the workload you want to run on AWS, a procurement agreement with AWS might be beneficial for your organization. Such an agreement outlines the terms and conditions under which you purchase AWS resources.\n\nThe agreement can contain a description of services to be used, potential pricing and payment terms that can be beneficial to you based on the volume of usage, SLA\u2019s that AWS must adhere to, the duration of the agreement, details about the support from AWS and many legal terms and conditions.\nWant to know if a procurement agreement is a good fit for your organization?\u00a0 Contact AWS Sales or a Partner in the AWS Partner Network (like Luminis).\nBefore you do this, however, it is useful to first identify your needs. If you are migrating from on-premise to the Cloud for example, then it is useful to know what the current production load is and what kind of functionality you need. If it is a new project/product, then it can be useful to at least have some estimates for the future production load and needs. In your discussion with the AWS Sales representative or partner this information is necessary.\nThe process\nOnce you have made contact with them, they guide you in the process. The procurement agreement looks different for each company. The best thing to do is indentifying your needs and make contact.\nOnce such a procurement agreement is made (or not), this information guides you to develop your foundational capabilities or workloads.\nSuppose your procurement agreement provides significant discounts on Amazon EC2 and Amazon RDS when you commit to a certain level of usage over a three-year period. As a result, when developing a new web application, you decide to architect it using EC2 instances for the web servers and RDS for the relational database backend to take advantage of these discounts. Furthermore, knowing that you\u2019re committed to these services helps justify investing in automation and optimization specific to EC2 and RDS, such as automated scaling and backup solutions, to maximize their value over the agreement period.\nThe AWS Cloud Foundation Culture\nWith the structure and initial policies in place, you can start to develop the other Cloud Foundation capabilities. You need to hold regular meetings with the functional area owners to follow the progress, learn from each other and make sure that all areas are respecting the Governance policies that are defined for the organization.\nAt some point, you need to start working on the actual functionality that needs to be built. It becomes a balancing act to decide what work to prioritize, be it to continue working on your Cloud foundation or start to work on functionality.\nIn an ideal world, you would have all the time and budget to develop all of the foundational capabilities. However, in the real world this is not possible. My advice would be to at least develop the \u201cGovernance\u201d and \u201cIdentity Management and Access Controls\u201d before starting with workload functionality. After that, hold regular meetings and decide how to move forward, developing foundational capabilities while building workload functionality.\nAnother suggested action is to motivate your colleagues to get certification and training in AWS. There are many ways to learn more about AWS. I used CloudAcademy and Udemy in the past to learn about AWS, however, there is also a lot of content to be found on Youtube.\n\nAs for certifications, it would be beneficial if all colleagues could get the AWS Cloud Practitioner, which covers foundational level knowledge of AWS. From a governance point of view, making budget available, learning about AWS and getting certifications is strongly encouraged.\nThe benefits of having knowledgeable people are not only beneficial for your workloads, but also for your Cloud Foundation, as many of the topics discussed in this blog series are also present in these certifications.\nUp next in AWS Cloud Foundation Series: The Identity Management & Access Control\nThis blog covered the Governance capability for an AWS Cloud foundation. We covered the structures that need to be in place to move forward and which policies you need to define at the start of your cloud journey. This helps create ownership for the cloud foundation and prevents us from making unnecessary mistakes later on.\nThese policies are only some initial policies for your organization. This blog series also covers items such as \u201cWorkload Isolation\u201d and \u201cCloud Financial Management.\u201d However, this framework sees these capabilities as separate from Governance.\nThe next blog post in this series covers the Identity Management & Access Control capability.\n", "tags": [], "categories": ["Blog", "Cloud"]}
{"post_id": 39999, "title": "AWS Cloud Foundation Series, part 1: Introduction", "url": "https://www.luminis.eu/blog-en/aws-cloud-foundation-series-introduction/", "updated_at": "2024-03-27T11:22:13", "body": "How to set up a solid AWS Cloud foundation for your organization\nScenario: All stakeholders for your project are convinced you need to use AWS as a cloud provider for your projects, and it\u2019s finally time to start. Jumping into the world of AWS can be daunting. Where do you start?\nLike many problems in IT, the answer might be: \u201cIt depends\u201d. It depends on what your business is and what functionality of the AWS Cloud can help achieve your business goals in the most efficient, cost-effective, and secure way.\nIn this blog series, we look at a part of AWS Cloud that does not \u201cdepend\u201d on your specific organization (at least not as much). Regardless of what kind of organization you will be running on the AWS Cloud, there are certain capabilities that you should strive to have in place to create a solid cloud foundation on AWS.\nEstablishing these capabilities will allow you to run your AWS workloads in an efficient, secure, reliable, and cost-optimized way from the start. Many organizations just start in the cloud without these considerations and end up having to invest a lot to migrate their existing workloads to a certain state to achieve a better cloud posture. This can be avoided by setting up a good cloud foundation from the start, and these capabilities can give you a guided way to achieve that.\nAWS itself has created a capability-based approach to create a cloud foundation, however it is not easy to find and contains many pages.\nThere is also the Cloud Adoption Framework which overlaps with this approach and will be leveraged from time to time during this series, however, it also contains even more information that might be overwhelming.\nWho is this series for?\n\nStartup or bigger: This series is for you if your organization is at the startup level or bigger. If you want to get your feet wet and try out some AWS functionality, then you might be here too early.\nAlready decided to use AWS: This blog series assumes that you have already decided to use AWS as a cloud provider. The blog series will not attempt to convince you of the benefits of Cloud computing or AWS as a cloud provider.\nMinimal AWS Cloud presence: If you have yet to start your AWS journey or are not that far yet, then you are just in time to follow the steps of this series. If you have already been working in AWS for a while and are already operational, you can still follow this series to gain insights on the several capabilities, however, I also suggest conducting a Well-Architected Review of your workloads and use the results there to improve your AWS cloud posture.\n\nA capability-based approach\nAWS has defined a set of capabilities and an order in which to develop these capabilities for your organization to create a solid cloud foundation on AWS.\nA capability has a definition and contains scenarios, guidance, and supporting automation to achieve the capability defined. These capabilities can help you plan, implement, and operate your workloads on AWS. These capabilities not only contain technology considerations but also people and processes. Examples of capabilities are \u201cIdentity Management & Access Control\u201d and \u201cObservability\u201d.\nThe capabilities that AWS has defined for a solid cloud foundation are split up into six categories and can be found in the following diagram:\n\nThese are capabilities that will apply to any organization running in AWS regardless of the functionality that is being built. This blog series will go through how to best set up these capabilities.\nWorking with capabilities\nTo work with these capabilities, we must also first define capability owners and stakeholders. To make this process easier, AWS has defined six functional areas, each having one owner. The owner of the primary functional area will be responsible for realizing the capability.\nAWS defines the following functional areas:\n\nOne owner should be assigned to each of these functional areas. If your company is not that big yet, the same person may take on the role of owner for multiple functional areas. More on this is covered in the second part of the blog series about \u201cGovernance\u201c.\nA guided path to a solid Cloud Foundation\nNow that we know what capabilities and functional areas are and we have assigned owners to each functional area, how do we start? To start, AWS has created a dependency graph indicating which capabilities are dependent on each other:\n\nFor example, if we want to begin with the \u201cEncryption & Key Management\u201d capability, we first need to start with the \u201cIdentity Management & Access Control\u201d capability.\nHowever, looking at this dependency graph there are still many ways and order to traverse it. AWS has also provided a timeline which you can find here. However, it is not that easy to read. The first 5 capabilities that should be worked on, based on this timeline, are:\n\nGovernance\nIdentity Management & Access Control\nLog Storage\nTagging\nNetwork Connectivity\n\nUp next: The Governance Capability\nThis blog re-introduced a capability-based approach to creating a solid AWS Cloud Foundation. In the next blogs of this series, we look at the very first capability to start your journey with: Governance. We look at the AWS definition while tapping into the experiences at Luminis to give the best advice on this capability.\nThe blog series continues by following the timeline presented by AWS. At the end of this series, you should understand what is necessary to create a solid AWS Cloud Foundation. This allows you to run your AWS workloads in an efficient, secure, reliable, and cost-optimized way.\n", "tags": [], "categories": ["Blog", "Cloud"]}
{"post_id": 39945, "title": "Luminis\u2019 Jurgen Allewijn accepted as Microsoft MVP in Cloud Native", "url": "https://www.luminis.eu/blog-en/luminis-jurgen-allewijn-accepted-as-microsoft-mvp-in-cloud-native/", "updated_at": "2024-03-13T15:57:21", "body": "Luminis colleague Jurgen Allewijn can call himself Microsoft Most Valuable Professional (MVP) in the field of Cloud Native. This acknowledgment is a fantastic recognition of Jurgen\u2019s knowledge and skills. Luminis is extremely proud of Jurgen\u2019s efforts within our organization, for our customers and, in relation to Microsoft.\n\nThe Microsoft MVP program is a Microsoft initiative to recognize individuals who excel in their expertise in Microsoft technology areas and actively contribute to the Microsoft community. These experts, called MVPs, are selected based on their technical capability, demonstrated contributions to the community and willingness to share knowledge.\nThe MVPs represent a diverse group of professionals, including developers, IT professionals, architects, educators, and community leaders, each with in-depth knowledge of specific Microsoft products, platforms or technologies. As a Microsoft MVP in Cloud Native, Jurgen has demonstrated extraordinary knowledge, dedication and passion for advancing Cloud-native technologies and supporting others within the industry.\nJurgen, part of Luminis as an Azure Cloud architect and consultant since August 2021, is committed to Microsoft on a daily basis. Both as an expert to help customers get the best possible solutions, but also as a thought leader and community leader for our colleagues and others. As such, Jurgen can be found at conferences regularly to demonstrate and share his knowledge.\nIn reaction to the news that Jurgen can call himself Microsoft MVP, he said:\n\u201cI am extremely excited to be part of the MVP community. I am looking forward to the conversations I will have with the product groups and the access to Microsoft resources that I can call upon. With the result of being able to serve our customers and the community even better.\u201d\nLuminis is incredibly proud of Jurgen for achieving this impressive milestone. This recognition not only highlights Jurgen\u2019s expertise and dedication, but also reflects the caliber of talent we have in house. We are very happy to have Jurgen as part of Luminis, and we look forward to continuing to support him in his ongoing efforts as an expert and thought leader in the Microsoft community.\nRonald Voets, Managing Director of Luminis, says the following about the attribution of MVP to Jurgen:\n\u201cThat Jurgen has been accepted as a Microsoft Most Valuable Professional in the area of \u2018Cloud Native\u2019 is fantastic news! Not only because it recognizes Jurgen\u2019s individual expertise and contributions, but it also provides a platform for further growth, collaboration and impact within the Microsoft community and beyond. We look forward to the opportunities this will bring Jurgen.\u201d\nAs Microsoft MVP, Jurgen is granted access to exclusive resources, collaboration opportunities and direct communication with Microsoft product groups. Jurgen will continue to represent Luminis within the Microsoft technology community, sharing his expertise, insights and best practices to drive innovation and development.\nDo you want to congratulate Jurgen personally or learn more or what we can do for you in the Microsoft technology field? Contact jurgen.allewijn@luminis.eu or send Jurgen a message on LinkedIn.\n", "tags": [], "categories": ["Blog", "News"]}
{"post_id": 39906, "title": "Automate customer interaction using OpenAI Assistants", "url": "https://www.luminis.eu/blog-en/automate-customer-interaction-using-openai-assistants/", "updated_at": "2024-03-20T09:55:26", "body": "Almost everybody knows what ChatGPT is. At workshops I give, about 90% of the people have used ChatGPT. Most of them know about the company, but only some know about Assistants. That is a pity; assistants can give you or your users a better experience. \u00a0After reading this blog, you understand what OpenAI Assistants are, how they work and what they can do for you and your users.\n\nDALL E generated coffee bar inspired by the famous Starbucks\nThe use case we use for the demo is a coffee-ordering application. Using the chat application, you talk to the barista, ask for suggestions, and order a nice cup of coffee or something else if you do not like coffee. The demo shows how to work with the different aspects of OpenAI assistants. It shows how to use functions and retrievers. It also shows how to combine it with the hybrid search of Weaviate to find recommended products and verify if the product you want is available in the shop.\nUnderstanding of OpenAI Assistants\nAn assistant is there to help your users interact with a set of tools using natural language. An assistant is configured with instructions and can access an LLM and a set of tools. The provider, OpenAI, provides some of these tools. Other tools are functions that you provide yourself. This might sound abstract. Let\u2019s have a look at an example. One of the provided tools is a code interpreter. The assistant uses this tool to execute generated Python code. Using this tool overcomes one of the well-known problems with doing calculations.\n\nInstructions: You are a personal math tutor. Write and run code to answer math questions.\ntools: code_interpreter\nmodel: gpt-4-turbo-preview\n\nThat is enough to initialise an assistant. You provide access to the assistant using a Thread. Think of a Thread as the chat window. You and the assistant both add messages to the Thread. After adding a message to the Thread, you push the run button to start the interaction with the assistant.\nThe following section introduces the assistant we are creating during this blog post.\nThe coffee-ordering assistant\nI like, or better need, a nice cup of coffee every day, multiple times. I am a black coffee fan. But these hip coffee bars have so many choices. For some people, it is hard to choose the proper coffee. Therefore, we create a coffee assistant that can help you make a choice and assist you during the ordering process.\n\nHave yourself a nice cup of coffee.\nFirst, we give the assistant our instructions.\nYou are a barista in a coffee shop. You help users choose the products the shop has to offer. You have tools available to help you with this task. There are tools to find available products, add products, give suggestions based on ingredients, and finalise the order. You are also allowed to do small talk with the visitors.\nWe provide the assistant with the following tools:\n\nfind_available_products\u200a\u2014\u200aFinds available products based on the given input. The result is an array with valid product names or an empty array if no products are found.\nstart_order\u200a\u2014\u200aStart an order, and the result is ERROR or OK. You can use this to notify the user.\nadd_product_to_order\u200a\u2014\u200aAdd a product to the order. The result is ERROR or OK. You can use this to inform the user\nremove_product_from_order\u200a\u2014\u200aRemove a product from the order. The result is ERROR or OK. You can use this to notify the user\ncheckout_order\u200a\u2014\u200acheck out the order. The result is ERROR or OK. You can use this to notify the user\nsuggest_product\u200a\u2014\u200aSuggests a product based on the input. The result is the name of the product that best matches the input.\n\nThe description of the tool or function is essential. The assistant uses the description to determine what tool to use and when.\nThe video below gives you an impression of what we will build.\n\nThe code\nThe first component for an OpenAI assistant is the Assistant class. I am not rewriting the complete OpenAI documentation here. I do point out the essential parts. The assistant is the component that interacts with the LLM, and it knows the available tools.\nThe assistant can be loaded from OpenAI. No get or load function accepts a name. Therefore, we have a method that loops over the available assistants until it finds the one with the provided name. When creating or loading an assistant, you have to provide the tools_module_name. This is used to locate the tools that the assistant can use. It is essential to keep the tools definitions at the exact location so we can automatically call them. More on this feature when talking about runs.\nWe create the coffee assistant using the code below:\n\r\ndef create_assistant():\r\n  name = \"Coffee Assistant\"\r\n  instructions = (\"You are a barista in a coffee shop. You\"\r\n                  \"help users choose the products the shop\"\r\n                  \"has to offer. You have tools available\"\r\n                  \"to help you with this task. You can\"\r\n                  \"answer questions of visitors, you should\"\r\n                  \"answer with short answers. You can ask\"\r\n                  \"questions to the visitor if you need more\"\r\n                  \"information. more ...\")\r\n\r\n  return Assistant.create_assistant(\r\n      client=client,\r\n      name=name,\r\n      instructions=instructions,\r\n      tools_module_name=\"openai_assistant.coffee.tools\")\r\n\nNotice that we created our own Assistant class, not to confuse it with the OpenAI Assistant class. It is a wrapper for the interactions with the OpenAI assistant class. Below is the method to store function tools in the assistant.\n\r\ndef add_tools_to_assistant(assistant: Assistant):\r\n    assistant.register_functions(\r\n        [\r\n            def_find_available_products, \r\n            def_start_order, \r\n            def_add_product_to_order, \r\n            def_checkout_order,\r\n            def_remove_product_from_order, \r\n            def_suggest_coffee_based_on_description\r\n        ])\r\n\nWe have to create the assistant only once, the next time we can load the assistant to use it for interactions. The next code block shows how to load the assistant.\n\r\ntry:\r\n    assistant = Assistant.load_assistant_by_name(\r\n        client=client, \r\n        name=\"Coffee Assistant\",\r\n        tools_module_name=\"openai_assistant.coffee.tools\")\r\n    logging.info(f\"Tools: {assistant.available_tools}\")\r\nexcept AssistantNotFoundError as exp:\r\n    logging.error(f\"Assistant not found: {exp}\")\r\n    raise exp\r\n\nLook at the complete code for the Assistant class at this location.\nThreads\nThe thread is an interaction between a user and the assistant. Therefore, a Thread object is unique per user. In the application, we use the streamlid session to store the thread_id. Therefore, each new session means a new Thread. The thread is responsible for accepting messages and sending them to the assistant. After a message is sent, a response message is awaited. Each interaction with an assistant is done using a run. The image below presents the flow of the application using these different components.\nOverview of the Assistant flow: First, the user creates a Thread. Next, the user sends a message to the Thread and runs the Thread against the Assistant. The Assistant knows all the available tools and asks the LLM what to do. If a tool needs to be called, the Assistant outputs that request. Our Assistant knows how to call the Tools, but this is the client application. The tool\u2019s output is returned to the LLM, and an answer is generated.\nIt is essential to understand that our Assistant wraps the OpenAI Assistant. Calling the tools is done using our Assistant. Detecting the difference between an output with an answer and an output with the request to call a tool is done using the status of the run. If the status is requires_action, our Assistant finds the tool_calls and calls the tools. This is what happens in the following code block taken from the thread.py.\n\r\ndef __handle_run(self, run: Run) -> Run:\r\n    run = self.__verify_run(run_id=run.id)\r\n\r\n    while run.status == \"requires_action\":\r\n        logger_thread.debug(f\"Run {run.id} requires action\")\r\n        tools_calls = run.required_action.submit_tool_outputs.tool_calls\r\n\r\n        tool_outputs = []\r\n        for tool_call in tools_calls:\r\n            result = self.assistant.call_tool(\r\n                tool_call.function.name, \r\n                json.loads(tool_call.function.arguments))\r\n            logger_thread.debug(f\"Result of call: {result}\")\r\n            tool_outputs.append({\r\n                \"tool_call_id\": tool_call.id,\r\n                \"output\": result\r\n            })\r\n        run = self.client.beta.threads.runs.submit_tool_outputs(\r\n            run_id=run.id,\r\n            thread_id=self.thread_id,\r\n            tool_outputs=tool_outputs\r\n        )\r\n        run = self.__verify_run(run_id=run.id)\r\n\r\n    logger_thread.info(f\"Handle run {run.id} completed.\")\r\n    return run\r\n\r\ndef __verify_run(self, run_id: str):\r\n    \"\"\"\r\n    Verify the status of the run, if it is still in \r\n    progress, wait for a second and try again.\r\n    :param run_id: identifier of the run\r\n    :return: the run\r\n    \"\"\"\r\n    run = self.client.beta.threads.runs.retrieve(\r\n        run_id=run_id, thread_id=self.thread_id)\r\n    logger_thread.debug(f\"Run: {run.id}, status: {run.status}\")\r\n    if run.status not in [\"in_progress\", \"queued\"]:\r\n        return run\r\n    time.sleep(1)\r\n    return self.__verify_run(run_id=run.id)\nNotice how we use the __verify_run function to check the status of the run. If the run is queued or in_progress, we wait for it to finish.\nThe source code for the thread can be found at this location.\nTools\nWe already mentioned the tools that the assistant can use. We have to provide the description of the tool to the Assistant. The following code block shows the specification for one function.\n\r\ndef_suggest_coffee_based_on_description = {\r\n    \"name\": \"suggest_coffee_based_on_description\",\r\n    \"description\": (\"Suggests a product based on the given \"\r\n                    \"ingredients. The result is a valid product \"\r\n                    \"name or an empty string if no products \r\n                    \"are found.\"),\r\n    \"parameters\": {\r\n        \"type\": \"object\",\r\n        \"properties\": {\r\n            \"input\": {\r\n                \"type\": \"string\",\r\n                \"description\": \"The coffee to suggest a coffee for\"\r\n            }\r\n        },\r\n        \"required\": [\"input\"]\r\n    }\r\n}\r\n\nIn the code block you see the name, this is important to us. We use the name to call the function. Therefore the name of the function needs to be the same as specified here. The description is really important to the LLM to understand what the tools brings. The parameters are the values provided by the LLM to call the tool with. Again, the description is really important for the LLM to understand what values to provide.\nIn this example we use Weaviate to recommend a drink using the provided text. The next code block shows the implementation.\n\r\ndef suggest_coffee_based_on_description(input: str):\r\n    weaviate = AccessWeaviate(\r\n        url=os.getenv(\"WEAVIATE_URL\"),\r\n        access_key=os.getenv(\"WEAVIATE_ACCESS_KEY\"),\r\n        openai_api_key=os.getenv(\"OPENAI_API_KEY\"))\r\n\r\n    result = weaviate.query_collection(\r\n        question=input, collection_name=\"coffee\")\r\n\r\n    weaviate.close()\r\n\r\n    if len(result.objects) == 0:\r\n        logger_coffee.warning(\"No products found\")\r\n        return \"\"\r\n\r\n    return result.objects[0].properties[\"name\"]\r\n\nConcluding\nThis blog post intends to give you an idea of what it means to work with Assistants. Check the code repository if you want to try it out yourself. The readme file contains the order in which you have to run the different scripts. One is to create the Assistant, one is to load the data into Weaviate, and one is to run the sample application.\nHope you like the post, feel free to comment or get in touch if you have questions.\nReferences\n\nhttps://platform.openai.com/docs/assistants/overview\nhttps://github.com/jettro/ai-assistant\n\n", "tags": ["Assistant", "Generative AI", "OpenAI"], "categories": ["Blog", "Machine learning &amp; AI"]}
{"post_id": 39930, "title": "Nieuwe lichting IT-toptalenten van Thales, de Belastingdienst en Luminis rondt succesvol Accelerate traject af", "url": "https://www.luminis.eu/blog-en/nieuwe-lichting-it-toptalenten-van-thales-de-belastingdienst-en-luminis-rondt-succesvol-accelerate-traject-af/", "updated_at": "2024-03-05T14:59:30", "body": "Traditiegetrouw wordt het Accelerate traject afgerond in de hangar op vliegveld Teuge. Tijdens het feestelijke graduation event krijgt een nieuwe lichting deelnemers hun welverdiende Accelerate wings opgespeld.\n\nIn dit nieuwe, versnelde Accelerate traject van tien maanden hebben de Belastingdienst, Thales en Luminis voor de derde keer de handen ineengeslagen. Accelerate is een op maat gemaakt opleidingstraject voor software toptalenten van de drie organisaties. In deze nieuwste versie, Accelerate Craftsmanship, krijgen minder ervaren talenten de kans om een sprong in hun ontwikkeling te maken. Talenten die nog niet direct toe zijn aan een leiderschapspositie maar wel grote stappen in die richting kunnen zetten.\nDe twintig deelnemers en elf coaches ronden dit traject met succes af, en vieren dit op het feestelijke graduation event te midden van collega\u2019s, vrienden en familie.\nTijdens het graduation event krijgen de deelnemers de kans om een afsluitende speech te geven over hun ervaringen tijdens het opleidingstraject. Zo vertelt deelnemer Lennaerd Bakker van de Belastingdienst:\n\u201cIk heb geleerd op zoek te gaan naar datgene wat mij motiveert en waar ik enthousiast van word. Daardoor heb ik een nieuwe passie gevonden, doelen gesteld, doelen behaald, en ben ik positiever. Ik wil iedereen meegeven: Ga op zoek naar hetgeen waar jij blij van wordt, houd dat vast, en laat het niet meer los.\u201d\nNaast soms ontroerende speeches van de deelnemers, krijgt ook Bert Ertman, VP Technology bij Luminis en initiatiefnemer van Accelerate, de kans om wat afsluitende woorden te delen. Hij roept de deelnemers op om buiten hun comfortzone te blijven treden: \u201cLeer groter denken, verbind er een doel aan vanuit je persoonlijke waarden, sorteer je acties op \u2018lef\u2019 en \u2018just do it!\u2019\u201d.\nAccelerate Craftsmanship is het derde traject in een succesvolle Accelerate-reeks. Vanuit een initiatief van Thales en Luminis startte in 2020 het eerste traject om tech-toptalent uit de eigen organisatie klaar te stomen voor een toekomst als softwareleiders. Het centrale thema in dit derde traject is Software Craftsmanship, belicht vanuit zowel technisch inhoudelijk vlak maar vooral in combinatie met persoonlijke ontwikkeling.\nHet persoonlijke ontwikkeltraject wordt in samenwerking met How Company gerealiseerd. How Company is sinds het eerste traject partner van Accerelate, en ziet door middel van hun communicatie- en persoonlijk leiderschapstrainingen hoe de deelnemers veranderen. Jeroen Ogier, trainer bij How Company en nauw betrokken bij Accelerate vertelt:\n\u201cDoordat de deelnemers persoonlijke doelen stellen en daar leren actief mee aan de gang te gaan, worden gedachten en idee\u00ebn echt omgezet in concrete resultaten. Dit leidt tot bijzondere ervaringen, groei en ontwikkeling. Het is in \u00e9\u00e9n woord fantastisch om hierin namens How Company een partner te zijn!\u201d\nDat Accelerate het beoogde doel, IT-toptalent klaarstomen voor de top van het tech-landschap, waarmaakt, beaamt Robert van den Breemen, Teamleider Concern IT-Architecten bij de Belastingdienst:\n\u201cHet succes van het Accelerate programma onderstreept het belang van vakmanschap in de ontwikkeling van talentvolle collega\u2019s. De Belastingdienst kiest ervoor om te investeren in het talent en dit te faciliteren met dit waardevolle ontwikkeltraject. Accelerate gaat niet alleen in op de technische vaardigheden maar ook op de persoonlijke ontwikkeling en het bevorderen van een cultuur van continue persoonlijke en professionele groei. Door het aanbieden van dit traject krijgen de talenten de mogelijkheid om te excelleren en te groeien.\u201d\n\nDe voorgaande trajecten, waarin ook werd samengewerkt met de Belastingdienst en Thales, vormden de basis van deze verkorte Accelerate versie (10 maanden in plaats van 18 maanden red.) waarin de meest relevante onderwerpen en sessies uit eerdere trajecten aan bod kwamen. Of het succes van de eerste twee traject ge\u00ebvenaard kon worden in dit verkorte programma was in het begin wel even spannend, vertelt Henk van Steeg, Head Software Engineering bij Thales:\n\u201cDe vraag of de succesformule van Accelerate ook in tien maanden werkt kan ik ondertussen volmondig met \u2018ja\u2019 beantwoorden. In deze tien maanden hebben we veel collega\u2019s een enorme groei zien doormaken waarbij ze nu meer impact maken dan ze zelf voor mogelijk hielden. Als groeiend bedrijf helpen dergelijke trajecten ons enorm om samen met deze collega\u2019s de uitdagingen aan te kunnen.\u201d\nOok Luminis heeft het eerste Accelerate Craftsmanship als succesvol ervaren. Met elk afgerond programma wordt de succesformule van Accelerate alleen maar beter, en het Craftsmanship-programma is een welkome aanvulling op het Accelerate Leadership-traject, zegt ook Jeroen Bouvrie, Director of Operations bij Luminis en stuurgroeplid van Accelerate:\n\u201cAccelerate Craftsmanship heeft laten zien dat de basis van het Accelerate programma zeer geschikt is om de ontwikkeling van deelnemers die nog minder ver in hun carri\u00e8re zijn te versnellen. Door de inmiddels beproefde mix van human skills en technische skills zie je dat de deelnemers echt gegroeid zijn als mens. Ook wanneer je nog niet toe bent aan een meer leadership-achtige rol, laat deze variant van Accelerate zien dat je grote stappen kan maken in je ontwikkeling.\u201d\nNa tien maanden kijken we terug op een bijzonder waardevol ontwikkeltraject. De afgelopen maanden hebben de deelnemers hard gewerkt aan hun skillset, overwonnen ze samen talloze uitdagingen en behaalden ze persoonlijke doelstellingen. We kijken uit naar de impact die deze groep Accelerate-deelnemers gaat hebben op onze organisaties, nu in en de toekomst.\nMeer weten over het Accelerate-programma of IT-trainingen? Bekijk de website van de Luminis Academy of neem contact op met Louis Pouwels, contactpersoon van de Luminis Academy (academy@luminis.eu).\n", "tags": [], "categories": ["Blog", "News"]}
{"post_id": 39876, "title": "Tech Talks and Tastings: A Recap of Jfokus 2024", "url": "https://www.luminis.eu/blog-en/tech-talks-and-tastings-a-recap-of-jfokus-2024/", "updated_at": "2024-03-01T10:27:59", "body": "In this blog post, I tell you all about my experience at the Jfokus Conference 2024 in Stockholm, where my colleague Jettro and I gave a workshop about question-answering systems with Retrieval Augmented Generation.\nBack in October 2023, I wrote my first conference experience blogpost: A Devoxx of Firsts. This felt like a good way for me to look back at the conference and what it brought me. Usually, my gained knowledge and experiences from conferences gradually fade away. Writing it down keeps the memories alive and I can relive them when rereading my own story. So I thought I\u2019d do the same for my visit to Jfokus by writing a recap.\nDay 0: Arrival, Vasa and Burgers\nThe conference started on Monday. But since the workshop we had to give was already at 09:00, Jettro and I decided to travel to Stockholm on Sunday. It was an early wake. We wanted to see the city of Stockholm so we took an early flight (08:30) and landed around 10:30. We couldn\u2019t get into our apartment until 16:00, so I decided to leave my small suitcase in a locker at the central train station. Without having to hassle with my luggage we went on our adventure. We had a nice long walk through the beautiful city center. Eventually, we ended up at the Vasa Museum.\n\nFor our workshops, we always pick a relevant data source to work with. Because we were in Stockholm we wanted something relevant for Stockholm. Jettro visited the city in the summer before. At that time he visited the Vasa Museum where the Vasa Warship is displayed. A ship that sank short after it sailed out around 400 years ago. It was preserved very well, so they were able to get it out of the sea about 350 years later and made it a museum piece. Jettro came up with using the history page of the Vasa Warship for our dataset. I agreed because it was a good fit for a question-answering system. However, I did not see the ship with my own eyes like Jettro did. Luckily the people of Jfokus organised a tour at the Vasa Museum for speakers of the conference on our day of arrival, so we joined them there. This made it possible for me to see the ship from a close distance!\n\u00a0\nAfter the Vasa Museum, it was time to head to our apartment and drop off our bags. We took the advice of a friend and former colleague to go eat at Franky\u2019s Burgers. It turned out to be pretty close to our apartment, so we didn\u2019t have to walk that far. Once there, Jettro and I both ordered the Texas BBQ burger (also on advice). While I was there anyway I made it a double. We definitely made a good choice going there. It tasted so good, I can honestly say that it\u2019s one of the best burgers I ever ate at a restaurant.\nAfter we finished our burger we went back to our apartment and did a last revision of our workshop material to make sure everything was complete and working as it should.\n\u00a0\nDay 1: Workshop, Old Town and Dinner\nMonday was the first conference day. A day full of different workshops. This was also the day we had to give our own workshop: Creating a Semantic Search-Based Question-Answering System with LLMs. The start time was 09:00, so we made sure we arrived at the conference center in time (08:15) and had enough time to register, explore the environment, and set up our equipment. We grabbed some sandwiches at the 7/11 for breakfast and went on to the conference. Turned out they had breakfast there as well. Good thing to know for the next few days. The registering and setting up our equipment all went very smoothly. We even got ourselves a nice Jfokus hoodie. Gotta love merchandise.\n\u00a0\nThe Workshop\n\nPeople started dripping in and it became quite packed. Around 33 people joined the workshop, which I think is a large enough group for two trainers. The theoretical parts of the workshop went well. Jettro and I have developed a good synergy presenting together. Unfortunately, we still had some people struggling with setting up the environment for the practical part. We tried to make it as easy as possible to start, but we forgot to mention in the talk description and prerequisites that we used Java 21. This caused some problems with a few.\n\nEventually, we got all but one (due to laptop restrictions) running so they could start on the exercises. Overall we were satisfied with our performance and the course of the workshop. Judging from the response of the crowd they also thought it was good.\nWe got many positive reactions and also some solid feedback for next time. And most importantly: we all had fun!\n\u00a0\nOld Town\nThe workshops that came after ours were not really of our interest, so we decided to do some more exploring of the city. This time we went to Old Town, Stockholm\u2019s oldest district. I really liked the bright colours of the buildings and the architecture. It\u2019s a beautiful city altogether. When having seen the Old Town walking in cold icy wind, we decided it was time to celebrate the success of our workshop. We still had some time to kill before the speaker dinner started, so we found a nice pub. They had some IPAs on draft with good-sounding names and we couldn\u2019t help to taste a few of them. In our awesome Luminis outfit obviously! After tasting a few beers and bites we went to attend the speaker dinner.\n\n\nThe Speaker Dinner\nThe speaker dinner was nice. It started with a short announcement/speech by someone from Oracle, the sponsor of the dinner. I guess he knew everybody was hungry, so he went over everything very quickly. One thing that stuck with me was that Oracle DB now also supports vector search. Then everybody sat down at the dinner tables and the feast began. One of the attendees from our workshop joined at our table. He was a real fan. Even called it his best workshop ever. Nice compliment! The dinner itself was pretty fancy. The main course was fish. Even though I don\u2019t really like fish, I still tried and with success. It tasted pretty good. The rest of the evening I don\u2019t remember very well because of all the champagne and wine we got. Jettro was smart enough to not consume any alcohol after our pub beers.\nDay 2: Tiring Talks\nThe title of this section might imply the talks were boring, but this was definitely not the case. I consumed one too many alcoholic beverage at the speaker dinner, which made this a tiring day for me. Luckily some interesting talks kept me going throughout the day. This time we ate breakfast at the conference, which was really nice with healthy sandwiches. After that, we went into the main room for the keynotes. Jfokus started as a rave party with flashing lights and music created by Sam Aaron with Sonic Pi, which is a live coding environment to synthesise music. After that, the keynotes started.\n\u00a0\nKeynote 1-2\nThe first keynote was an introduction to the conference by the organisers. The second one was Java in 2024: Constant Change, Delivered by Georges Saab. Basically a background story about the Java release cycle and how they changed it from big releases spanning over years to smaller releases per 6 months. Nothing new if you\u2019re a little up to date with Java.\n\u00a0\nKeynote 3\nAfter that was\u00a0The New Super Power in the Developer\u2019s Toolbox keynote by Lize Raes. If you\u2019ve read my Devoxx blog post, you might recognise her name. There she gave, at least for me, the best talk of the conference. Which was about LangChain4j. She\u2019s a good speaker and also this time she put up a good performance. It was all about AI and how it will help us be more productive in our work and daily lives. We don\u2019t have to fear that AI will take over our jobs, we just have to fear that people using AI will replace us. So now is the time to start learning and using it before it\u2019s too late.\nWhen the keynotes ended, it was time to start the rest of the conference. Below is a brief overview of all the presentations I attended on day 2. The titles are links to the YouTube videos.\n\u00a0\nEnhancing LLM Reasoning and Precision Using RAG and ReAct on AWS by Begum Firdousi Abbas\nMy main topic of interest for the last couple of months has been Retrieval Augmented Generation (RAG). Our own workshop was also about RAG. Luminis is very AWS-oriented, so this talk piqued my interest. Most of the talk felt like an AWS services advertisement. But the ReAct prompting part was new to me. An interesting approach to capturing the reasoning of LLMs, why and how they come to a certain answer. Good food for thought.\n\u00a0\nFrom Serverful to Serverless Java by Dennis Kieselhorst & Maximilian Schellhorn\nIn the same room as the previous AWS talk, so we stayed in our seats for this one. Other presentations during this time slot didn\u2019t appeal much to us. There was the LangChain4j talk by Lize Raes (best-rated talk of Jfokus), but we already attended it at Devoxx. So we decided to stick with this one. It was a decent talk, but I personally didn\u2019t learn much. We have some experienced colleagues who blog and share knowledge about this topic. I might have also missed some things due to my tiredness. Luckily there was a well-needed coffee break after this.\n\u00a0\nAI-powered software development from the trenches by Henrik Kniberg\nI\u2019m not really fond of the phrase \u201cfrom the trenches\u201d, because\u00a0 software development work doesn\u2019t feel like being in the trenches for me. Even though I have already seen a few presentations about AI code assistants on Devoxx, I wanted to see a fresh view on this subject. Henrik did a live demo coding with Cursor, a VS Code fork with a code assistant built in. I sometimes use ChatGPT to help me with my coding problems, but it was nice to see again how a code assistant built in your IDE can really boost productivity. Now onto convincing my employer and clients that I should be able to use this.\n\u00a0\nTechnical Neglect by Kevlin Henney\nAn interesting talk from Kevlin Henney about his view on technical debt. It isn\u2019t necessarily a bad thing if you manage it well. But that last part is where it mostly goes wrong. Watch this if you want to know why \u2018neglect\u2019 is sometimes a better term to use than \u2018debt\u2019.\n\u00a0\nAfter the last talk, we had some food and I decided to go to sleep very early, between 20:00 and 21:00. It was a tiresome day for me, so I needed the rest.\n\u00a0\nDay 3: Refreshing Talks and Departure\nUnlike the day before, I started this day fresh and fit. We had to leave a bit early to catch our flight back, so we only had the chance to attend three presentations. But they were definitely not the least three!\n\u00a0\nFive things every developer should know about software architecture by Simon Brown\nThe week before Jfokus I helped my colleague and friend Robbert-Jan facilitate two Accelerate Craftmanship training sessions about architecture. We did an architectural kata and used Simon\u2019s famous C4 model. Very useful and interesting topic to me, so it was an easy choice for me to attend this talk. I was a bit afraid that this was going to be a C4 model commercial, but it turned out to be a solid talk about architecture in general with only a brief mention of C4. Very good start to the day. I would recommend to watch this.\n\u00a0\nHow hacking works by Espen Sande-Larsen\nFor me, this was the surprise of Jfokus. It\u2019s not the kind of talk I usually go to, but it was in the same room (the main room) as the presentation from Simon. We also had some pretty good spots there, so we decided to stay. Luckily we did. It was a fun and interesting talk with a small introduction to hacking and live examples of hacking an application. Espen showed how easy it sometimes is to hack an application, so it\u2019s important to be aware of vulnerabilities in your code. He suggested regularly doing a Capture the Flag (CTF) exercise with your team (sort of a gamified way to pentest) to see if your code is still secure. Espen is a good presenter and has a solid backstory to support his presentation, which made this the most fun and interesting talk of the conference for me. Recommended to watch!\n\u00a0\nBreaking AI: Live coding and hacking applications with Generative AI by Simon Maple\nAlso in the same room. We were in the hacking mood from the previous talk, so we stayed in our seats again. The title might suggest they hacked applications with Generative AI, but it was actually about exploiting vulnerabilities in the code produced by AI code assistants. It became quite clear that the presenters were from Snyk, but I don\u2019t think they overdid the advertising. It was a nice continuation of the previous presentation where they showed that code produced by code assistants isn\u2019t always the most secure code. So please be aware when using any code assistant that you don\u2019t introduce any vulnerabilities in your code. Tools (like Snyk) can help you to analyse your code to catch vulnerabilities early on.\n\u00a0\nAfter these presentations, it was time to head out to the airport for our flight back. The conference center was next to the central station, where I picked up 800 gr of Swedish candy in a Pressbyr\u00e5n to taste at home.\n\u00a0\nOverall Conference Experience\nOverall a very nice conference, with lots of perks for speakers. The organisers are a group of nice and helpful people, always there to assist you or just have a nice conversation. The location was perfect, in the city center next to the central station of Stockholm. All the rooms were good and well equipped, sound was good everywhere too. The food served looked luxurious and was really good. Water was available throughout the conference center, but I couldn\u2019t find a place to get soda drinks (at Devoxx they had fridges everywhere). Although it\u2019s a bit Sweden focussed, I would recommend going to this conference if you have the chance, especially as a speaker.\nRecommended talks to watch back, in order of my most favourite first:\n\nHow hacking works by Espen Sande-Larsen\nThe New Super Power in the Developer\u2019s Toolbox by Lize Raes\nFive things every developer should know about software architecture by Simon Brown\n\n\u00a0\nKey Takeaways\nWe need to embrace AI. It will not replace us, but the people not using it will be replaced by people who do. It can greatly improve productivity. Just be aware it\u2019s not perfect (yet?), so always verify what it produces does not introduce nasty bugs or security risks. I will start using it wherever possible if allowed by company policies.\nNext to that, I came to the conclusion that I should be more aware of security when writing my code. Seeing someone live coding and easily exploiting the vulnerabilities of that code made me really think. I might start introducing CTFs in teams when possible, which could be a fun way to make everyone aware of security (risks) in their codebase.\nAll Jfokus 2024 presentations can be watched on YouTube in this playlist. The workshops, however, are not recorded.\n", "tags": ["ai", "conference", "java", "jfokus"], "categories": ["Blog", "Working at Luminis"]}
{"post_id": 39795, "title": "Analyze and debug Quarkus based AWS Lambda functions with X-Ray", "url": "https://www.luminis.eu/blog-en/analyze-and-debug-quarkus-based-aws-lambda-functions-with-x-ray/", "updated_at": "2024-02-09T14:09:45", "body": "Serverless architectures, like AWS Lambda, have emerged as a paradigm-shifting approach to building, fast, scalable and cost efficient applications. While Serverless architectures provide unparalleled flexibility, they also introduce new challenges in terms of monitoring and troubleshooting.\nIn this blog post, we explore how Quarkus integrates with AWS X-Ray and how using a Jakarta CDI Interceptor can keep your code clean while adding custom instrumentation.\nQuarkus and AWS Lambda\nQuarkus is a Java based framework tailored for GraalVM and HotSpot, which results in an amazingly fast boot time while having an incredibly low memory footprint. It offers near instant scale up and high density memory utilization, which can be very useful for container orchestration platforms like Kubernetes or Serverless runtimes like AWS Lambda.\nBuilding AWS Lambda Functions can be as easy as starting a Quarkus project, adding the quarkus-amazon-lambda dependency, and defining your AWS Lambda Handler function.\n<dependency>\r\n    <groupId>io.quarkus</groupId>\r\n    <artifactId>quarkus-amazon-lambda</artifactId>\r\n</dependency>\nAn extensive guide on how to develop AWS Lambda Functions with Quarkus can be found in the official Quarkus AWS Lambda Guide.\nEnabling X-Ray for your AWS Lambda functions\nQuarkus provides out of the box support for X-Ray, but you will need to add a dependency to your project and configure some setting to make it work with GraalVM / native compiled Quarkus applications. Let\u2019s first start with adding the quarkus-amazon-lambda-xray dependency.\n\n<dependency>\r\n    <groupId>io.quarkus</groupId>\r\n    <artifactId>quarkus-amazon-lambda-xray</artifactId>\r\n</dependency>\nDon\u2019t forget to enable tracing for your Lambda function otherwise it won\u2019t work. An example of doing that is by setting the tracing argument to active within your AWS CDK code.\nfunction = Function.Builder.create(this, \"feed-parsing-function\")\r\n      ...\r\n      .memorySize(512)\r\n      .tracing(Tracing.ACTIVE)\r\n      .runtime(Runtime.PROVIDED_AL2023)\r\n      .logRetention(RetentionDays.ONE_WEEK)\r\n      .build();\r\n\nAfter the deployment of your function and a function invocation you should be able to see the X-Ray traces from within the Cloudwatch interface. By default it will show you some basic timing information for your function like the initialisation and the invocation duration.\n\nAdding more instrumentation\nNow that the dependencies are in place and tracing is enabled for our function we can enrich the traces in X-Ray by leveraging the X-Ray SDKs TracingIntercepter . For instance for the SQS and DynamoDB client you can explicitly set the intercepter inside the application.properties file.\nquarkus.dynamodb.async-client.type=aws-crt\r\nquarkus.dynamodb.interceptors=com.amazonaws.xray.interceptors.TracingInterceptor\r\nquarkus.sqs.async-client.type=aws-crt\r\nquarkus.sqs.interceptors=com.amazonaws.xray.interceptors.TracingInterceptor\r\n\nAfter putting these properties in place, redeploying and executing the function, the TracingIntercepter will wrap around each API call to SQS and DynamoDB and store the actual trace information along side the trace.\n\nThis is very useful for debugging purposes as it will allow you to validate your code and check for any mistakes. Requests to AWS Services are part of the pricing model, so if you make a mistake in your code and you make too many calls it can become quite costly.\nCustom subsegments\nWith the AWS SDK TracingInterceptor configured we get information about the calls to the AWS APIs. But what if we want to see information about our own code or remote calls to services outside of AWS?\nThe Java SDK for X-Ray supports the concept of adding custom subsegments to your traces. You can add subsegments to a trace by adding a few lines of code to your own business logic. You can see in the following code snippet.\n  public void someMethod(String argument)  {\r\n    // wrap in subsegment\r\n    Subsegment subsegment = AWSXRay.beginSubsegment(\"someMethod\");\r\n    try {\r\n      // Your business logic\r\n    } catch (Exception e) {\r\n      subsegment.addException(e);\r\n      throw e;\r\n    } finally {\r\n      AWSXRay.endSubsegment();\r\n    }\r\n  }\r\n\nAlthough this is trivial to do, it will become quit messy if you have a lot of methods you want to apply tracing to. This isn\u2019t ideal and it would be better of we don\u2019t have to mix our own code with the X-Ray instrumentation.\nQuarkus and Jakarta CDI Interceptors\nThe Quarkus programming model is based on the Lite version of the Jakarta Contexts and Dependency Injection 4.0 specification. Besides dependency injection the specification also describes other features like:\n\nLifecycle Callbacks \u2013 A bean class may declare lifecycle @PostConstruct and @PreDestroy callbacks.\nInterceptors \u2013 used to separate cross-cutting concerns from business logic.\nDecorators \u2013 similar to interceptors, but because they implement interfaces with business semantics, they are able to implement business logic.\nEvents and Observers \u2013 Beans may also produce and consume events to interact in a completely decoupled fashion.\n\nAs mentioned, CDI Interceptors are used to separate cross-cutting concerns from business logic. As tracing is a cross-cutting concern this sounds like a great fit. Let\u2019s take a look at how we can create an interceptor for our AWS X-Ray instrumentation.\nWe start with defining our interceptor binding which we will call XRayTracing. Interceptor bindings are intermediate annotations that may be used to associate interceptors with target beans.\npackage com.jeroenreijn.aws.quarkus.xray;\r\n\r\nimport jakarta.annotation.Priority;\r\nimport jakarta.interceptor.InterceptorBinding;\r\n\r\nimport java.lang.annotation.Retention;\r\n\r\nimport static java.lang.annotation.RetentionPolicy.RUNTIME;\r\n\r\n@InterceptorBinding\r\n@Retention(RUNTIME)\r\n@Priority(0)\r\npublic @interface XRayTracing {\r\n}\r\n\nThe next step is to define the actual Interceptor logic, the code that will add the additional X-Ray instructions for creating the subsegment and wrapping it around our business logic.\npackage com.jeroenreijn.aws.quarkus.xray;\r\n\r\nimport com.amazonaws.xray.AWSXRay;\r\nimport jakarta.interceptor.AroundInvoke;\r\nimport jakarta.interceptor.Interceptor;\r\nimport jakarta.interceptor.InvocationContext;\r\n\r\n@Interceptor\r\n@XRayTracing\r\npublic class XRayTracingInterceptor {\r\n\r\n    @AroundInvoke\r\n    public Object tracingMethod(InvocationContext ctx) throws Exception {\r\n        AWSXRay.beginSubsegment(\"## \" + ctx.getMethod().getName());\r\n        try {\r\n            return ctx.proceed();\r\n        } catch (Exception e) {\r\n            AWSXRay.getCurrentSubsegment().addException(e);\r\n            throw e;\r\n        } finally {\r\n            AWSXRay.endSubsegment();\r\n        }\r\n    }\r\n}\r\n\nAn important part of the interceptor is the @AroundInvoke annotation, which means that this interceptor code will be wrapped around the invocation of our own business logic.\nNow that we\u2019ve defined both our interceptor binding and our interceptor it\u2019s time to start using it. Every method that we want to create a subsegment for, can now be annotated with the @XRayTracing annotation.\n@XRayTracing\r\npublic SyndFeed getLatestFeed() {\r\n    InputStream feedContent = getFeedContent();\r\n    return getSyndFeed(feedContent);\r\n}\r\n\r\n@XRayTracing\r\npublic SyndFeed getSyndFeed(InputStream feedContent) {\r\n    try {\r\n        SyndFeedInput feedInput = new SyndFeedInput();\r\n        return feedInput.build(new XmlReader(feedContent));\r\n    } catch (FeedException | IOException e) {\r\n        throw new RuntimeException(e);\r\n    }\r\n}\r\n\nThat\u2019s looks much better. Pretty clean if I say so myself.\nBased on the hierarchy of subsegments for a trace, X-Ray is able to show a nested tree structure with the timing information.\n\nClosing thoughts\nThe integration between Quarkus and X-Ray is quite simple to enable. The developer experience is really good out of the box with defining the interceptors on a per client basis. With the help of CDI interceptors you can keep your code clean without worrying too much about X-Ray specific code inside your business logic.\nAn alternative to building your own Interceptor might be to start using AWS PowerTools for Lambda (Java). Powertools for Java is a great way to boost your developer productivity. However, it can be used for more than X-Ray, so I\u2019ll save it for another post.\n", "tags": ["aws", "java", "observability", "serverless"], "categories": ["Blog", "Cloud", "Development"]}
{"post_id": 39738, "title": "Deploying SageMaker Pipelines Using AWS CDK", "url": "https://www.luminis.eu/blog-en/deploying-sagemaker-pipelines-using-aws-cdk/", "updated_at": "2024-02-09T14:00:34", "body": "Introduction\nSageMaker is a loved and feared AWS service. You can do anything with it, from building data pipelines, to training machine learning models, to serving said models to your customers. Because of this, there is a range of approaches for any of these problems, which can often be a source of confusion on how to proceed.\nIn this blog, I clear up one such confusion about the deployment of SageMaker pipelines. I show you how to write your own pipeline definitions and how to deploy them using AWS CDK into your SageMaker domain.\nIf you are not yet working with AWS SageMaker I highly encourage you to try it out before proceeding with this walkthrough, specifically because we will be addressing some quite advanced concepts.\nWhat is SageMaker?\nBefore we delve into the how to of deploying SageMaker Pipelines using AWS CDK, it\u2019s essential to understand what SageMaker is and what it brings to the table.\nAmazon SageMaker is a fully managed machine learning service provided by AWS. It\u2019s a comprehensive service that covers a wide range of machine learning tasks. It assists with data preparation, provides a notebook development environment, handles endpoint deployment, provides tools for model evaluation and much more. In essence, it\u2019s a one-stop-shop for machine learning operations, designed to simplify the process of building, training, and deploying machine learning models.\nHowever, these components, while individually powerful, need a maestro to orchestrate them into a cohesive workflow. That\u2019s where SageMaker Pipelines come in. They bridge the gap between these elements, ensuring they work together seamlessly. This orchestration acts as the connecting piece in your MLOps workflow, reducing the complexity and enhancing the manageability of your projects.\nWhat is SageMaker Pipelines?\nSageMaker Pipelines are a versatile service to orchestrate various tasks within an ML model lifecycle. Each pipeline consists of interconnected steps, each of which can run a configured docker container within SageMaker runtime or call one of the services within SageMaker. A few notable features include, but are not limited to:\n\nAllows using custom docker images from AWS ECR.\nCan seamlessly pass large files or metrics between various steps.\nSupport has a Local Mode for testing the pipelines and containerized steps locally.\nIntegrates with services such as AWS Athena and SageMaker Feature Store gathering the necessary (training) data.\nExecutable from services such as AWS StepFunctions and AWS Lambda using AWS SDK.\n\n\nDeploying SageMaker Pipelines using AWS CDK: a high level overview\nBefore we delve into the specifics, it is beneficial to understand the overall structure of our deployment. The following diagram illustrates the components involved in this blog:\n\nOne important aspect to note is that the SageMaker Pipeline does not directly depend on the SageMaker domain. This is correct, the pipeline is a standalone resource, and can be launched programmatically using the AWS SDK or step functions, which is useful in minimal setups.\nHowever, for manual launches, a SageMaker workspace is required. This is where the SageMaker domain becomes necessary.\nTherefore, to ensure a comprehensive understanding of the process, we will also cover the creation of a SageMaker domain in this blog. This will provide a complete overview of the deployment process, equipping you with the knowledge to effectively manage your machine learning projects.\nSetting Up Your Infrastructure\nIn this section, we will focus on the initial steps required to set up the necessary infrastructure for our project. The first task involves creating a CloudFormation project which will deploy our AWS resources including: SageMaker domain, users, data buckets and optionally the VPC.\nFor those interested in the complete code, it is available on\u00a0Github.\nCreate a VPC (optional)\nIf you\u2019ve already got a VPC up and running, you\u2019re one step ahead. Just update the vpc_name in the cdk.json file and feel free to skip this section. However, if you\u2019re looking around and realizing you\u2019re VPC-less, don\u2019t fret. We\u2019ve got you covered.\nCreating a SageMaker domain requires a VPC. By adding the following snippet to your infrastructure CDK stack, will create one for you.\nNote that this particular VPC comes with a public IP. Be aware that this could incur some running costs.\n\nvpc = ec2.Vpc(  \r\n    self,  \r\n    id=\"VpcConstruct\",  \r\n    ip_addresses=ec2.IpAddresses.cidr(\"10.0.0.0/16\"),  \r\n    vpc_name=f\"{self.prefix}-vpc\",  \r\n    max_azs=3,  \r\n    nat_gateways=1,  \r\n    subnet_configuration=[  \r\n        ec2.SubnetConfiguration(  \r\n            cidr_mask=24,  \r\n            name=\"Public\",  \r\n            subnet_type=ec2.SubnetType.PUBLIC,  \r\n        ),  \r\n        ec2.SubnetConfiguration(  \r\n            cidr_mask=23,  \r\n            name=\"Private\",  \r\n            subnet_type=ec2.SubnetType.PRIVATE_WITH_EGRESS,  \r\n        ),  \r\n        ec2.SubnetConfiguration(  \r\n            cidr_mask=24,  \r\n            name=\"Isolated\",  \r\n            subnet_type=ec2.SubnetType.PRIVATE_ISOLATED,  \r\n        ),  \r\n    ],  \r\n)\n\nDeploying SageMaker Domain\nFirst things first, before we get into the details of creating a SageMaker domain, we need to establish a default role that all users will assume. This can be fine-tuned or overridden later, depending on your specific use case. Here\u2019s how you can create an execution role:\nvpc = ec2.Vpc(  \r\n    self,  \r\n    id=\"VpcConstruct\",  \r\n    ip_addresses=ec2.IpAddresses.cidr(\"10.0.0.0/16\"),  \r\n    vpc_name=f\"{self.prefix}-vpc\",  \r\n    max_azs=3,  \r\n    nat_gateways=1,  \r\n    subnet_configuration=[  \r\n        ec2.SubnetConfiguration(  \r\n            cidr_mask=24,  \r\n            name=\"Public\",  \r\n            subnet_type=ec2.SubnetType.PUBLIC,  \r\n        ),  \r\n        ec2.SubnetConfiguration(  \r\n            cidr_mask=23,  \r\n            name=\"Private\",  \r\n            subnet_type=ec2.SubnetType.PRIVATE_WITH_EGRESS,  \r\n        ),  \r\n        ec2.SubnetConfiguration(  \r\n            cidr_mask=24,  \r\n            name=\"Isolated\",  \r\n            subnet_type=ec2.SubnetType.PRIVATE_ISOLATED,  \r\n        ),  \r\n    ],  \r\n)\r\nCopy\nNow, let\u2019s talk about storage. In SageMaker, scripts, notebooks, and similar resources are all stored in an S3 bucket. By default, SageMaker creates one centralized storage bucket for code and data when you create it using AWS console.\nWe on the other hand will create a separate source and data buckets with the following settings. Both buckets are configured to be inaccessible to the public for obvious reasons.\nself.sm_sources_bucket = s3.Bucket(  \r\n\tself,  \r\n\tid=\"SourcesBucket\",  \r\n\tbucket_name=f\"{self.prefix}-sm-sources\",  \r\n\tlifecycle_rules=[],  \r\n\tversioned=False,  \r\n\tremoval_policy=cdk.RemovalPolicy.DESTROY,  \r\n\tauto_delete_objects=True,  \r\n\t# Access  \r\n\taccess_control=s3.BucketAccessControl.PRIVATE,  \r\n\tblock_public_access=s3.BlockPublicAccess.BLOCK_ALL,  \r\n\tpublic_read_access=False,  \r\n\tobject_ownership=s3.ObjectOwnership.OBJECT_WRITER,  \r\n\tenforce_ssl=True,  \r\n\t# Encryption  \r\n\tencryption=s3.BucketEncryption.S3_MANAGED,  \r\n)\r\nCopy\nThe pipeline, by default, will assume the user\u2019s role unless specified otherwise. For our purposes, the user, or the pipeline, should have enough permissions to read the code for pipeline execution and write the results to the data bucket. It\u2019s a good practice to keep the code read-only when running the pipeline, both for security reasons and to avoid any issues during runtime.\n# Grant read access to SageMaker execution role  \r\nself.sm_sources_bucket.grant_read(self.sm_execution_role)\r\n# Grant read/write access to SageMaker execution role  \r\nself.sm_data_bucket.grant_read_write(self.sm_execution_role)\r\nCopy\nCreating a SageMaker domain itself is a very straightforward process. You just need to give it a name, attach it to the domain VPC you have from the previous steps, and attach the execution role to the default user config. If you want to specify additional security settings such as \u201cVPC Only\u201d mode, you can do it here as well. Similarly, we set tags so all the resources that start under the specific domain or user will inherit cost allocation tags accordingly.\n# Fetch VPC information  \r\nvpc_name = self.node.try_get_context(\"vpc_name\")  \r\nself.vpc = ec2.Vpc.from_lookup(  \r\n    self, id=\"ImportedVpc\",  \r\n    vpc_name=vpc_name if vpc_name else f\"{self.prefix}-vpc\"  \r\n)  \r\npublic_subnet_ids = [public_subnet.subnet_id for public_subnet in self.vpc.public_subnets]  \r\n  \r\n# Create SageMaker Studio domain  \r\nself.domain = sm.CfnDomain(  \r\n    self, \"SagemakerDomain\",  \r\n    auth_mode='IAM',  \r\n    domain_name=f'{self.prefix}-SG-Project',  \r\n    default_user_settings=sm.CfnDomain.UserSettingsProperty(  \r\n        execution_role=self.sm_execution_role.role_arn  \r\n    ),  \r\n    app_network_access_type='PublicInternetOnly',  \r\n    vpc_id=self.vpc.vpc_id,  \r\n    subnet_ids=public_subnet_ids,  \r\n    tags=[cdk.CfnTag(  \r\n\t    key=\"project\",  \r\n\t    value=\"example-pipelines\"  \r\n\t)],\r\n)\r\nCopy\nFinally, we create a user that will be used for invoking the pipeline when run manually.\n# Create SageMaker Studio default user profile  \r\nself.user = sm.CfnUserProfile(  \r\n    self, 'SageMakerStudioUserProfile',  \r\n    domain_id=self.domain.attr_domain_id,  \r\n    user_profile_name='default-user',  \r\n    user_settings=sm.CfnUserProfile.UserSettingsProperty()  \r\n)\r\nCopy\nRun the deploy command using CDK and there you have it! You\u2019ve successfully deployed a SageMaker domain. You can always tweak and customize your setup to better suit your project\u2019s needs, such as configuring roles, attaching ECR images and git repos for notebooks. In the next section, we\u2019ll dive into deploying a simple pipeline.\ncd ./infrastructure_project\r\n\r\ncdk deploy\r\nCopy\nDeploying a Simple Pipeline\nThe deployment of a SageMaker pipeline is a complicated process that involves two key tasks. First, we need to generate a pipeline definition using the SageMaker SDK. Then, we deploy this definition using CloudFormation. Let\u2019s delve into the details of each task.\n\nThe Pipeline Definition\nThe pipeline definition is a structured JSON document that instructs AWS on the sequence of steps to execute, the location for execution, the code to be run, the resources required, and the interdependencies of these steps. Essentially, it is a detailed execution plan for your machine learning pipeline.\nCreating this JSON document manually can be cumbersome and prone to errors. To mitigate this, the SageMaker SDK provides an abstraction layer that enables the use of Python code constructs to build the pipeline definition. You can start using it by adding it as a python dependency with pip install sagemaker.\nTo streamline the process of pipeline creation, we establish a base class. This class serves as an interface, which will be particularly useful when we integrate our pipeline with the rest of our CDK code. Here, we utilize Pydantic BaseModel class to enable type checking on configuration parameters you might want to pass to the pipeline.\nclass SagemakerPipelineFactory(BaseModel):  \r\n    \"\"\"Base class for all pipeline factories.\"\"\"  \r\n    @abstractmethod  \r\n    def create(  \r\n        self,  \r\n        role: str,  \r\n        pipeline_name: str,  \r\n        sm_session: sagemaker.Session,  \r\n    ) -> Pipeline:  \r\n        raise NotImplementedError\r\nCopy\nWe can now proceed to write the actual pipeline declaration using the SageMaker SDK, and one such configuration parameter (pipeline_config_parameter).\nclass ExamplePipeline(SagemakerPipelineFactory):  \r\n    pipeline_config_parameter: str  \r\n  \r\n    def create(  \r\n        self,  \r\n        role: str,  \r\n        pipeline_name: str,  \r\n        sm_session: sagemaker.Session,  \r\n    ) -> Pipeline:\r\n\t    ...\r\nCopy\nWe proceed by declaring a runtime configurable parameter for the instance type. Then we add ScriptProcessor which defines the environment our script will be running in; including the machine instance count, the IAM execution role and the base image.\n...\r\n# Use the SKLearn image provided by AWS SageMaker  \r\nimage_uri = sagemaker.image_uris.retrieve(  \r\n\tframework=\"sklearn\",  \r\n\tregion=sm_session.boto_region_name,  \r\n\tversion=\"0.23-1\",  \r\n)  \r\n\r\n# Create a ScriptProcessor and add code / run parameters  \r\nprocessor = ScriptProcessor(  \r\n\timage_uri=image_uri,  \r\n\tcommand=[\"python3\"],  \r\n\tinstance_type=instance_type_var,  \r\n\tinstance_count=1,  \r\n\trole=role,  \r\n\tsagemaker_session=sm_session,  \r\n)\r\nCopy\nNext we define our first processing step that will use the defined processor (environment definition) to run our script with given job arguments, as well as, input and output definitions.\nprocessing_step = ProcessingStep(  \r\n\tname=\"processing-example\",  \r\n\tstep_args=processor.run(  \r\n\t\tcode=\"pipelines/sources/example_pipeline/evaluate.py\",  \r\n\r\n\t),  \r\n\tjob_arguments=[  \r\n\t\t\"--config_parameter\", self.pipeline_config_parameter  \r\n\t],\r\n\tinputs=[],\r\n\toutputs=[]\r\n)\r\n\r\n\r\nCopy\nOne single step is already enough to define a pipeline. While defining the pipeline, make sure to list it\u2019s runtime parameters.\nreturn Pipeline(  \r\n\tname=pipeline_name,  \r\n\tsteps=[processing_step],  \r\n\tsagemaker_session=sm_session,  \r\n\tparameters=[instance_type_var],  \r\n)\r\nCopy\nHere is the simple script that our job will be runing. It essentially prints the input job argument.\nimport argparse\r\n\r\nparser = argparse.ArgumentParser()\r\nparser.add_argument(\"--config_parameter\", type=str)\r\nargs = parser.parse_args()\r\n\r\nprint(f\"Hello {args.config_parameter}!\")\r\nCopy\nAbove, we have demonstrated a minimal example for building a machine learning pipeline. If you are interested in a deeper dive of the possibilities, check out the examples in The Official Documentation.\nDeploying the Pipeline Definition\nNow that we have our pipeline definition, the next step is deploying it to your AWS Account. This is where CloudFormation comes into play, as it supports the AWS::SageMaker::Pipeline Resource. Looking at the arguments, we see that the pipeline definition should be embedded as a JSON document within the CloudFormation template. This JSON document, in our case, is emitted by SageMaker SDK, which we call during the synthesis phase of the CloudFormation stack creation.\n# Define the pipeline (this step uploads required code and packages by the pipeline to S3)  \r\npipeline = pipeline_factory.create(  \r\n\tpipeline_name=pipeline_name,  \r\n\trole=sm_execution_role_arn,  \r\n\tsm_session=sagemaker_session,  \r\n)  \r\n\r\npipeline_def_json = json.dumps(json.loads(pipeline.definition()), indent=2, sort_keys=True)\r\nCopy\nNote that a new version of the code is deployed into the source bucket by SageMaker SDK before the CloudFormation stack is applied. This might raise a few eyebrows, but it will not cause issues with existing processes, as it is stored in a folder based on a derived version identifier. This does mean that you may need additional cleanup scripts later down the line.\nOnce we have a pipeline definition JSON, we can declare the CfnPipeline construct.\ndef create_pipeline_resource(  \r\n    self,  \r\n    pipeline_name: str,  \r\n    pipeline_factory: SagemakerPipelineFactory,  \r\n    sources_bucket_name: str,  \r\n    sm_execution_role_arn: str,  \r\n) -> Tuple[sm.CfnPipeline, str]:\r\n\t...\r\n\r\n\t# Define the pipeline (this step uploads required code and packages by the pipeline to S3)  \r\n\t...\r\n\t\r\n\t# Define CloudFormation resource for the pipeline, so it can be deployed to your account  \r\n\tpipeline_cfn = sm.CfnPipeline(  \r\n\t\tself,  \r\n\t\tid=f\"SagemakerPipeline-{pipeline_name}\",  \r\n\t\tpipeline_name=pipeline_name,  \r\n\t\tpipeline_definition={\"PipelineDefinitionBody\": pipeline_def_json},  \r\n\t\trole_arn=sm_execution_role_arn,  \r\n\t)  \r\n\tarn = self.format_arn(  \r\n\t\tservice='sagemaker',  \r\n\t\tresource='pipeline',  \r\n\t\tresource_name=pipeline_cfn.pipeline_name,  \r\n\t)\r\n\treturn pipeline_cfn, arn\r\nCopy\nFinally, we combine all everything together by passing our pipeline factory to pipeline resource creation function along with our source and data buckets.\n# Load infrastructure stack outputs as value parameters (resolved at cdk deploy time)  \r\nsources_bucket_name = ssm.StringParameter.value_from_lookup(  \r\n    self, f\"/{self.prefix}/SourcesBucketName\")  \r\nsm_execution_role_arn = ssm.StringParameter.value_from_lookup(  \r\n    self, f\"/{self.prefix}/SagemakerExecutionRoleArn\")  \r\n  \r\n# Create a configured pipeline  \r\nself.example_pipeline, self.example_pipeline_arn = self.create_pipeline_resource(  \r\n    pipeline_name='example-pipeline',  \r\n    pipeline_factory=ExamplePipeline(  \r\n        pipeline_config_parameter=\"Hello world!\"  \r\n    ),  \r\n    sources_bucket_name=sources_bucket_name,  \r\n    sm_execution_role_arn=sm_execution_role_arn,  \r\n)\r\nCopy\nNow the code is complete, deploy the pipeline using the CDK commands.\ncd ./data_project\r\n\r\ncdk deploy\r\nCopy\nTesting the Result: deploying SageMaker Pipelines using AWS CDK\nAfter deploying both of the stacks, we can view and run our pipeline in SageMaker Studio.\nNavigate to the SageMaker service in the AWS Management Console and click on \u201cDomains.\u201d Ensure that your SageMaker domain, created as part of the infrastructure stack, is visible.\n\nInside the SageMaker domain, click on \u201cLaunch\u201d near your created user and launch the SageMaker Studio.\n\nIn the navigation select \u201cPipelines\u201d to see a list of deployed pipelines. Confirm that your example pipeline is listed.\n\nClick on the specific pipeline (e.g., \u201cexample-pipeline\u201d) to view its details and start an exectution to start and monitor your pipeline.\n\nConclusion on deploying SageMaker Pipelines using AWS CDK\nIn this blog, we have learned how to write a simple SageMaker Pipeline in Python and deploy it in AWS CDK. While doing so, we also have deployed a SageMaker Domain, discussed how the pipeline code is stored in AWS, and shared a few best practices for configuration.\nWe have only scratched the surface of what is possible with SageMaker, there are various topics that are equally important within MLOps projects such as testing your code and pipelines, local development, and automated quality monitoring.\nStay tuned for more, or contact me if you have any questions.\n", "tags": [], "categories": ["Blog", "Cloud"]}
{"post_id": 39685, "title": "AWS Multi-Account GitOps Deployment 3: AWS GitHub Deployment", "url": "https://www.luminis.eu/blog-en/cloud-en/aws-multi-account-gitops-deployment-3-aws-github-deployment/", "updated_at": "2024-02-08T12:31:30", "body": "Welcome to the final installment of our series on multi-account GitOps deployment on AWS.\u00a0After setting up a multi-account AWS organizational structure and integrating AWS accounts with GitHub Actions with short-lived credentials,\u00a0we\u2019re now ready to dive into the GitHub Actions deployment process.\nThe Importance of CICD in Modern Software Development\nContinuous Integration and Continuous Delivery\u00a0(CICD)\u00a0is one of the most important tools in today\u2019s software development endeavours.\u00a0It allows the developers to build,\u00a0test and see the effect of their addition to the codebase in an automated fashion.\u00a0Within the vast amount of providers available for creating pipelines such as GitHub Actions,\u00a0GitLab CICD,\u00a0AWS CodePipeline etc.,\u00a0we will be opting to use GitHub Actions for this series.\u00a0The reason,\u00a0we are using GitHub Actions is that it is a free tool that integrates perfectly with one of the best code repository GitHub that has a huge number of quality custom steps created and maintained by the community.\nSetting Up the GitHub Repository for Deployment\nIn this tutorial, we will create a simple serverless api that has a single Lambda function behind an API Gateway that responds differently with respect to the environment that the stack is deployed in.\n\nInitialize the Repository: Begin by creating a new directory for your project and initialize it as a git repository.\nmkdir simple-api && cd simple-api\r\ngit init\r\nnpx projen new awscdk-app-ts\nSetting Up AWS CDK and Projen: Update the\u00a0.projenrc.ts\u00a0configuration file similar to following:\nimport { awscdk } from 'projen';\r\nimport { ApprovalLevel } from 'projen/lib/awscdk';\r\n\r\nconst project = new awscdk.AwsCdkTypeScriptApp({\r\nauthorEmail: 'utku.demir@luminis.eu',\r\nauthorName: 'Utku Demir',\r\ncdkVersion: '2.96.2',\r\ndefaultReleaseBranch: 'main',\r\nname: 'simple-api',\r\ndescription: 'A CDK project for Simple Api GitOps Deployments',\r\ngithub: false,\r\nprojenrcTs: true,\r\nkeywords: [\r\n'AWS CDK',\r\n'projen',\r\n'Typescript',\r\n'Deployment',\r\n],\r\nrequireApproval: ApprovalLevel.NEVER,\r\ngitignore: ['.idea'],\r\nlicense: 'MIT',\r\nlicensed: true,\r\n});\r\nproject.synth();\r\n After updating,\u00a0generate the project:\nyarn projen\nImplementing the Necessary CDK Stack: Create a new file named\u00a0simple_api_stack.ts\u00a0under\u00a0src\u00a0to include the necessary configuration for our stack:\nimport { Stack, StackProps } from 'aws-cdk-lib';\r\nimport * as apigateway from 'aws-cdk-lib/aws-apigateway';\r\nimport { EndpointType } from 'aws-cdk-lib/aws-apigateway';\r\nimport * as lambda from 'aws-cdk-lib/aws-lambda';\r\nimport { Construct } from 'constructs';\r\n\r\nexport interface SimpleApiStackProps extends StackProps {\r\n  environment: string;\r\n}\r\n\r\nexport class SimpleApiStack extends Stack {\r\n  constructor(scope: Construct, id: string, props: SimpleApiStackProps) {\r\n    super(scope, id, props);\r\n\r\n    // Define the Lambda function\r\n    const helloLambda = new lambda.Function(this, 'HelloLambda', {\r\n      runtime: lambda.Runtime.NODEJS_18_X,\r\n      handler: 'index.handler',\r\n      code: lambda.Code.fromInline(`\r\n                exports.handler = async function(event, context) {\r\n                    return {\r\n                        statusCode: 200,\r\n                        body: JSON.stringify({ message: \"Hello, World from ${props.environment} environment!\" })\r\n                    };\r\n                };\r\n            `),\r\n    });\r\n\r\n    // Define the API Gateway\r\n    new apigateway.LambdaRestApi(this, 'Endpoint', {\r\n      handler: helloLambda,\r\n      proxy: true,\r\n      deploy: true,\r\n      cloudWatchRole: true,\r\n      endpointTypes: [EndpointType.EDGE],\r\n    });\r\n  }\r\n}\nHere,\u00a0we create an edge optimized API Gateway Rest Api with a single Lambda handler that will return the message formatted with the correct environment given a get request to the base url.\nLastly,\u00a0edit the\u00a0main.ts\u00a0under\u00a0src\u00a0to include this stack as:\nimport { App } from 'aws-cdk-lib';\r\nimport { SimpleApiStack } from './simple_api_stack';\r\n\r\n\r\n// for development, use account/region from cdk cli\r\nconst devEnv = {\r\n   account: process.env.AWS_ACCOUNT,\r\n   region: process.env.AWS_REGION,\r\n};\r\n\r\nconst app = new App();\r\n\r\nconst environment = process.env.ENVIRONMENT || 'dev';\r\n\r\nnew SimpleApiStack(app, `${environment}-simple-api`, {\r\n   env: devEnv,\r\n   environment: environment,\r\n});\r\n\r\napp.synth();\nOne important thing to note here is that we have an environment variable called\u00a0ENVIRONMENT\u00a0that will be injected through the GitHub Actions.\nIntegrating AWS with GitHub Actions\nRepository Environments\u00a0Under settings of the repository, you will find the Environments tab. This will allow you to manage your environments for the application. Create all the environments, namely, dev, test and prod as follows:\n\nEnvironment Secrets and Variables: Set up necessary environment secrets and variables on the environment. These variables will store sensitive information like AWS credentials, ensuring they are securely managed and not hard-coded in your scripts is critical.\n\nHere,\u00a0AWS_ACCOUNT\u00a0is the 12-digit account id corresponding to the environment that the stack will be deployed in.\nEnvironment Protection: Add protection to the environments for test and prod environments. This step is crucial to ensure that changes can only be made to these environments under certain conditions, such as after code reviews or passing automated tests. This works like a manual approval and keep in mind that for private repositories, this is only possible if your organization\u2019s plan is GitHub Enterprise. If this is not the case for your organization, I would suggest taking a look at the amazing custom manual approval step made by trstringer linked in the references.\n\nRepository Secrets: Add the\u00a0DEPLOY_ROLE\u00a0as a repository secret in GitHub. This role is necessary for GitHub Actions to interact with your AWS environment securely, and it will be assumed from the deployment account, hence, it will be same for all the environments.\n\nReplace the censored part with your deployment AWS account id.\nCreating the Deployment Workflow\nHere,\u00a0are the necessary steps to create a GitHub Actions Workflow that will deploy our CDK stack to AWS Cloud.\n\nWorkflow File: Write a GitHub Actions workflow file. This file will define the steps and conditions under which your code is deployed to AWS.\nTriggering Deployments: Set up triggers for deployment. For example, you might trigger deployments on pushing to specific branches or tagging a release.\nDeployment Steps: Detail the steps for deploying your application. This may include building your application, running tests, and using AWS CDK commands to deploy to AWS.\nPost-Deployment Verification: Implement steps to verify the successful deployment of your application. This might include health checks, smoke tests, or rollback procedures in case of failure.\n\nFollowing is the deploy workflow file for our simple api for the dev environment:\n# .github/workflows/deploy.yml\r\nname: deploy\r\n\r\non:\r\n  push:\r\n    branches:\r\n      - main\r\n\r\njobs:\r\n  build:\r\n    runs-on: ubuntu-latest\r\n    defaults:\r\n      run:\r\n        working-directory: ./simple-api\r\n    steps:\r\n      - name: Pull repository\r\n        uses: actions/checkout@v3\r\n\r\n      - name: Setup Nodejs and npm\r\n        uses: actions/setup-node@v3\r\n        with:\r\n          node-version: \"18\"\r\n\r\n      - name: Setup yarn\r\n        run: npm install -g yarn\r\n\r\n      - name: Setup Nodejs with yarn caching\r\n        uses: actions/setup-node@v3\r\n        with:\r\n          node-version: \"18\"\r\n          cache: yarn\r\n          cache-dependency-path: './simple-api/yarn.lock'\r\n\r\n      - name: Install dependencies\r\n        run: yarn install --frozen-lockfile\r\n\r\n      - name: Build and test the project\r\n        run: npx projen build\r\n\r\n  deploy-dev-simple-api:\r\n    runs-on: ubuntu-latest\r\n    defaults:\r\n      run:\r\n        working-directory: ./simple-api\r\n    needs: build\r\n    environment: dev\r\n    env:\r\n      AWS_ACCOUNT: ${{ secrets.AWS_ACCOUNT }}\r\n      AWS_REGION: ${{ vars.AWS_REGION }}\r\n      ENVIRONMENT: ${{ vars.ENVIRONMENT }}\r\n    permissions:\r\n      id-token: write\r\n      contents: read\r\n    steps:\r\n      - name: Pull repository\r\n        uses: actions/checkout@v3\r\n\r\n      - name: Setup Nodejs and npm\r\n        uses: actions/setup-node@v3\r\n        with:\r\n          node-version: \"18\"\r\n\r\n      - name: Setup yarn\r\n        run: npm install -g yarn\r\n\r\n      - name: Setup Nodejs with yarn caching\r\n        uses: actions/setup-node@v3\r\n        with:\r\n          node-version: \"18\"\r\n          cache: yarn\r\n          cache-dependency-path: './simple-api/yarn.lock'\r\n\r\n      - name: Install dependencies\r\n        run: yarn install --frozen-lockfile\r\n\r\n      - name: Assume deploy role\r\n        uses: aws-actions/configure-aws-credentials@v2\r\n        with:\r\n          role-to-assume: ${{ secrets.DEPLOY_ROLE }}\r\n          aws-region: ${{ vars.AWS_REGION }}\r\n\r\n      - name: Deploy the application\r\n        run: npx projen deploy --all\nThis workflow file triggers when a push is made to our main branch. It first builds and tests the stack and then runs the deploy step for the dev\u00a0environment.\u00a0To implement the\u00a0test\u00a0and\u00a0prod\u00a0environment deploy steps,\u00a0just copy the\u00a0dev\u00a0job and replace the\u00a0dev\u00a0with\u00a0test\u00a0and follow a similar a process for the\u00a0prod\u00a0environment.\u00a0It is also crucial to make these steps run sequentially so that the deployment to environments don\u2019t happen all at once.\u00a0As such,\u00a0we would need to make sure\u00a0test\u00a0deployment depends on\u00a0dev\u00a0and the\u00a0prod\u00a0deployment depends on\u00a0test.\u00a0For that,\u00a0change the needs part to the name of the\u00a0dev\u00a0or\u00a0test\u00a0job\u2019s name,\u00a0respectively.\nFor this tutorial,\u00a0I haven\u2019t included the steps necessary for the 4th step for post deployment health checks but in a production environment,\u00a0it is crucial to make sure everything is working as expected.\nThe Deployment of the AWS CDK Stack\nFinally,\u00a0when we push our application to our remote\u00a0main\u00a0branch,\u00a0we should expect to see our workflow start running.\u00a0Make sure the working directories are correctly set within the workflow file to achieve successful execution!\n\nWhen the deployment finishes,\u00a0you should be able to use the api endpoint presented in the outputs of the deployment within the workflow logs.\n\nHere is how the manual approval step of the GitHub Actions is seen in the workflow page:\n\nConclusion on AWS GitHub Deployment\nAs we conclude our series on AWS Multi-Account GitOps Deployment on AWS using GitHub Actions, let\u2019s reflect on the journey we\u2019ve embarked on and the key takeaways from this process.\nEmbracing Automation and Efficiency\nThrough this series,\u00a0we\u2019ve explored how to effectively leverage automation to manage and deploy resources across multiple AWS accounts.\u00a0By integrating AWS with GitHub Actions,\u00a0we\u2019ve demonstrated a powerful combination of cloud infrastructure management and modern CI/CD practices.\nAdvantages of Multi-Account Strategy\nThe multi-account strategy on AWS,\u00a0when combined with GitOps practices,\u00a0offers enhanced security,\u00a0better resource segregation,\u00a0and more granular control over access and billing.\u00a0This approach is particularly beneficial for larger teams and organizations aiming to scale their cloud infrastructure efficiently.\nThe Power of AWS Multi-Account GitHub\nGitHub Actions stands out as a versatile and user-friendly tool for CI/CD,\u00a0providing a seamless integration with GitHub repositories.\u00a0Its ability to handle complex workflows,\u00a0along with the vast community-driven actions available,\u00a0makes it an excellent choice for teams looking to implement GitOps.\nKey Learning Points\n\nSetting Up the Foundation: The importance of establishing a strong organizational structure within AWS to support multi-account deployment.\nIntegrating Tools: How to effectively integrate AWS with GitHub Actions, taking advantage of short-lived credentials for enhanced security.\nDeployment Workflows: Crafting GitHub Actions workflows to automate the deployment process, ensuring consistency and reliability.\nSecurity and Best Practices: Emphasizing the importance of security best practices, particularly in managing secrets and permissions.\n\nEncouraging Continuous Learning\nThe landscape of cloud computing and DevOps is ever-evolving.\u00a0This series,\u00a0while comprehensive,\u00a0is just the beginning.\u00a0I encourage you to keep experimenting,\u00a0learning,\u00a0and adapting to new tools and methodologies.\nYour Next Steps\nWith the foundation set and the knowledge gained, you\u2019re now equipped to extend and customize your GitOps workflows. Experiment with different types of deployments, explore new GitHub Actions, and continue to refine your AWS multi-account management strategy. As you make progress,\u00a0share your learnings and collaborate with the community.\u00a0The collective knowledge and experiences help everyone grow and innovate.\nFinal Thoughts\nThank you for joining me in this series.\u00a0I hope it has been enlightening and empowering.\u00a0As you embark on your journey of cloud engineering,\u00a0remember that the path to mastery is through continuous practice and adaptation.\nReferences\nFor further reading and advanced topics,\u00a0here are some resources:\n\nManual Workflow Approval on GitHub Actions\nAWS Documentation on Multi-Account Strategies\nGitHub Actions Documentation\nProduction-Ready CDK\n\n\nHappy Cloud Engineering and until next time!\n", "tags": ["aws", "aws cdk", "cdk", "CI/CD", "cloud", "cloud security", "devops", "github", "infrastructure as code"], "categories": ["Blog", "Cloud"]}
{"post_id": 39643, "title": "Simplifying UI with Web Components", "url": "https://www.luminis.eu/blog-en/simplifying-ui-with-webcomponents/", "updated_at": "2024-02-07T15:05:52", "body": "In the ever-evolving landscape of web development, the quest for more efficient, scalable, and maintainable front-end solutions is perpetual. In this blog post, we dive into simplifying UI with Web Components.\nUnlike native HTML elements, Web Components are not provided by the browser by default. Instead, they are crafted by developers, empowering them to extend HTML itself, defining new elements that behave as if they were native to the web platform.\nThis approach not only leads to more robust and less error-prone applications but also significantly streamlines the development process. The best thing of all: you can use them in all frameworks without a big learning curve, or without frameworks at all!\nSection 1: the basics of using\u00a0Web Components\nComponents in frameworks\nIf you have used a framework you may have noticed that your components usually come with a lot of imports or framework-specific file extensions. For example, two major frameworks:\n// Angular\r\nimport { Component } from '@angular/core';\r\n@Component({\r\n  selector: 'app-component-overview',\r\n  templateUrl: './component-overview.component.html',\r\n})\r\n\r\n// React\r\nclass Welcome extends React.Component {\r\n  render() {\r\n    return <h1>Hello, {this.props.name}</h1>;\r\n  }\r\n}\r\n\nThese imports or file-extensions mean a lot of (potentially big) dependencies and framework specific knowledge. Web Components are native to the browser and as a result they come without complex patterns. This brings upsides and downsides. For sharing components, it brings mostly downsides to use big frameworks if the other project don\u2019t use the same framework. That\u2019s where Web Components jump in.\nUpsides of native Web Components:\n\nEvery framework supports them without any complexity because they are native\nThey are private using Shadow Dom, they don\u2019t mess up the rest of your page\nYou don\u2019t need a framework to create them, although many support the creation\nThey easily integrate into Storybook\n\nDownsides of native Web Components:\n\nNo out-of-the-box solutions for routing\nNo data management solutions built in\nIDE\u2019s don\u2019t support parameter or css styles in autocomplete features\n\nWeb Components\nWeb Components are a browser-native way to write components that can be used in any framework but even without any framework at all. Let me show you an example of a Web Component that uses no frameworks at all:\nclass HelloWorld extends HTMLElement {\r\n  constructor() {\r\n    super();\r\n    this.attachShadow({ mode: 'open' });\r\n    this.shadowRoot.innerHTML = `\r\n      <span class=\"hello\">Hello World!</span>\r\n    `;\r\n  }\r\n}\r\ncustomElements.define('hello-world', HelloWorld);\r\n\nYou can write this in a JavaScript file and simply write your HTML like this:\n<body>\r\n    <hello-world></hello-world>\r\n    <script src=\u201dhello-world.js\u201d></script>\r\n</body>\r\n\nWithout any frameworks or magic, you now have a native look-a-like HTML tag!\n\nShadow DOM\nShadow DOM allows developers to encapsulate a piece of the webpage, so its styles and scripts do not interfere with the rest of the page, and vice versa. Think of it like a small, self-contained bubble within your webpage. Elements inside this bubble can have their own HTML, CSS, and JavaScript, which are separate from the rest of the page.\nIn the following example, let\u2019s assume I used the HelloWorld example written in the previous section. I imported the scripts the right way. The HTML looks like this:\n<hello-world></hello-world>\r\n<span class=\"hi\">Hello World!</span>\r\n\nBoth elements render the exact same If I query a native element, or any Web Component without a shadow DOM, you get to see the structure like this:\nconsole.log(document.querySelector('.hi'));\r\n// <span class=\"hi\">Hello World!</span>\r\n\nWe now query the Web Component with an open Shadow DOM attached to it, and the result might be different from what you expect to see:\nconsole.log(document.querySelector('hello-world'));\r\n// You might expect:\r\n// <hello-world><span class=\"hello\">\r\n//   Hello World!\r\n// </span></hello-world>\r\n// The actual result:\r\n// <hello-world></hello-world>\r\n\nShadow DOM shields the internal structure from the script, which means the component is private. As a result, the browser allows you to interact with the element internals through the Shadow DOM. By default your component is left alone and you don\u2019t have to be afraid of leaking style issues!\n\nSection 2: useful concepts for web components\nRendering HTML and CSS\nHTML is the foundation of everything on the web. If you want to show text on the web you use a <p> tag. If you want to draw, you use <canvas>. If you want your awesome-stuff, you use <awesome-stuff>.\nI think you can guess what the following element does.\nclass AwesomeStuff extends HTMLElement {\r\n    constructor() {\r\n        super();\r\n        this.attachShadow({ mode: 'open' });\r\n        this.render();\r\n    }\r\n\r\n    css() {\r\n        return `\r\n            img {\r\n                width: 150px;\r\n            }\r\n        `;\r\n    }\r\n\r\n    render() {\r\n        this.shadowRoot.innerHTML = `\r\n            <img src=\"https://cataas.com/cat\" />\r\n\r\n            <style>${this.css()}</style>\r\n        `;\r\n    }\r\n}\r\ncustomElements.define('awesome-stuff', AwesomeStuff);\r\n\n\nUsing Attributes to insert information\nWeb Components are usually stand-alone components that allow or desire no direct mutations and interactions with the outside world. You can, however, define your own so-to-say \u2018api\u2019 by reading Attributes and emitting Events.\nAttributes can be used to put information into a Web Component:\n<awesome-stuff data-name=\"Randy\"></awesome-stuff>\nThe \u2018data-name\u2019 attribute holds the value \u2018Randy\u2019 and can be read from the custom-element like this:\nclass AwesomeStuff extends HTMLElement {\r\n    static observedAttributes = ['name'];\r\n\r\n    constructor() {\r\n        super();\r\n        this.attachShadow({ mode: 'open' });\r\n        this.name = 'World'; // Default value for the name\r\n        this.render();\r\n    }\r\n\r\n    // This is a native method that gets called by the browser\r\n    attributeChangedCallback(attrName, oldValue, newValue) {\r\n        if (attrName === 'name') {\r\n            this.name = newValue; // Update the name property\r\n            this.render();\r\n        }\r\n    }\r\n\r\n    render() {\r\n        this.shadowRoot.innerHTML = `<h1>Hello ${this.name}!</h1>`;\r\n    }\r\n}\r\ncustomElements.define('awesome-stuff', AwesomeStuff);\r\n\nYou can see I used \u2018data-name\u2019 and not simply \u2018name\u2019. They both work, but some frameworks use strict HTML parsing and \u2018data-name\u2019 is a supported property while oficially \u2018name\u2019 is not.\nhttps://developer.mozilla.org/en-US/docs/Learn/HTML/Howto/Use_data_attributes\n\nUsing events to receive information\nWeb Components can share information by emitting events. The browser can be told to observe these events and trigger functions. The elements inside of the Web Component will also trigger events, but these won\u2019t be accessible outside of the Shadow DOM.\nIn the next example you see a Web Component which holds a button, triggering logic and emitting custom events.\nclass AwesomeStuff extends HTMLElement {\r\n  constructor() {\r\n    super();\r\n    this.attachShadow({ mode: \"open\" });\r\n    this.render();\r\n  }\r\n\r\n  connectedCallback() {\r\n    const helloButton = this.shadowRoot.querySelector(\"#hello-button\");\r\n\r\n    helloButton.addEventListener(\"click\", () =>\r\n      this.dispatchEvent(new CustomEvent(\"custom-event-hello\", { detail: \"Data from hello button\" }))\r\n    );\r\n  }\r\n\r\n  render() {\r\n    this.shadowRoot.innerHTML = `\r\n      <button id=\"hello-button\">Hello!</button>\r\n    `;\r\n  }\r\n}\r\ncustomElements.define('awesome-stuff', AwesomeStuff);\r\n\nListening to these events requires the use of a little JavaScript:\nfunction setupEventListeners() {\r\n  const awesomeStuffElement = document.querySelector('awesome-stuff');\r\n\r\n  awesomeStuffElement.addEventListener('custom-event-hello', (e) => {\r\n    alert('Custom event triggered:', e.detail);\r\n  });\r\n}\r\n\n\nSlots\nWeb Components are shielded from the outside, and the outside is shielded from the Web Components. But that does not mean they cannot co\u00f6perate! We\u2019ve seen Attributes and Events, which allow data flowing in and out of the components. Next to those, we can use Slots. It does not share data or expose anything but it allows the outside to rent some space within the Web Component. Let\u2019s go back to the basics:\nclass AwesomeStuff extends HTMLElement {\r\n  constructor() {\r\n    super();\r\n    this.attachShadow({ mode: 'open' });\r\n    this.shadowRoot.innerHTML = `<span class=\"hello\">Hello World!</span>`;\r\n  }\r\n}\r\ncustomElements.define('awesome-stuff', AwesomeStuff);\r\n\r\n<awesome-stuff>I am not yet rendered</awesome-stuff>\r\n\nThis still renders \u2018Hello World!\u2019. We need to add a <slot></slot> to render the text inside of the <awesome-stuff> tags like this:\nclass AwesomeStuff extends HTMLElement {\r\n  constructor() {\r\n    super();\r\n    this.attachShadow({ mode: \"open\" });\r\n    this.shadowRoot.innerHTML = `<slot><p>Default content.</p></slot>`;\r\n  }\r\n}\r\ncustomElements.define('awesome-stuff', AwesomeStuff);\r\n\r\n<awesome-stuff>I am now inside of awesome-stuff</awesome-stuff>\r\n\nThe text \u2018Default content.\u2019 is only rendered if nothing, not even whitespace, is between the <awesome-stuff> tags.\n\nSection 3: Creating a complex web component together\nLet\u2019s create some 3D dice. These are so complex, you might want to re-use it instead of rewriting it later in other places or other projects. Good thing we use Web Components to create a Custom Element.\nThe HTML is simple:\n<div id=\"die\">\r\n    <div class=\"face face-1\">1</div>\r\n    <div class=\"face face-2\">2</div>\r\n    <div class=\"face face-3\">3</div>\r\n    <div class=\"face face-4\">4</div>\r\n    <div class=\"face face-5\">5</div>\r\n    <div class=\"face face-6\">6</div>\r\n</div>\r\n\nThe CSS will quickly become complex, so lets do it in steps. First, let the die rotate in an animation so we can see the 3D changes. Then we make sure the fonts are set up and all faces are the same size. We also give the faces an orange background.\n#die {\r\n    /* note these CSS variables and calculations are native CSS! */\r\n    --die-size: 50px;\r\n    --face-offset: calc(var(--die-size) / 2);\r\n    position: relative;\r\n    width: var(--die-size);\r\n    aspect-ratio: 1 / 1;\r\n    font-size: var(--face-offset);\r\n    transform-style: preserve-3d;\r\n    animation: rotate 5s infinite linear;\r\n}\r\n\r\n.face {\r\n    position: absolute;\r\n    width: var(--die-size);\r\n    aspect-ratio: 1 / 1;\r\n    background: orange;\r\n    text-align: center;\r\n    font-size: var(--die-size);\r\n    line-height: var(--die-size);\r\n}\r\n\r\n@keyframes rotate {\r\n    0% { transform: rotateX(0deg) rotateY(0deg); }\r\n    100% { transform: rotateX(360deg) rotateY(360deg); }\r\n}\r\n\nWe now have 6 orange squares in the same absolute position (so you only see one). Now let\u2019s add some magic that makes them a 3D cube!\n.face-1 {\r\n    transform: rotateY(0deg) translateZ(var(--face-offset));\r\n}\r\n.face-2 {\r\n    transform: rotateX(270deg) translateZ(var(--face-offset));\r\n}\r\n.face-3 {\r\n    transform: rotateY(90deg) translateZ(var(--face-offset));\r\n}\r\n.face-4 {\r\n    transform: rotateY(270deg) translateZ(var(--face-offset));\r\n}\r\n.face-5 {\r\n    transform: rotateX(90deg) translateZ(var(--face-offset));\r\n}\r\n.face-6 {\r\n    transform: rotateY(180deg) translateZ(var(--face-offset));\r\n}\r\n\nNow let\u2019s add some logic to let the user decide which side the die should roll to.\n  static get observedAttributes() {\r\n    return [\"value\"];\r\n  }\r\n\r\n  attributeChangedCallback(name, oldValue, newValue) {\r\n    if (name === \"value\") {\r\n      this.updateFace(newValue);\r\n    }\r\n  }\r\n\r\n  updateFace(value) {\r\n    const die = this.shadowRoot.querySelector(\"#die\");\r\n    if (value) {\r\n      die.style.animation = \"none\";\r\n      switch (value) {\r\n        case \"1\":\r\n          return die.style.transform = \"rotateX(0deg) rotateY(0deg)\";\r\n        case \"2\":\r\n          return die.style.transform = \"rotateX(90deg) rotateY(0deg)\";\r\n        case \"3\":\r\n          return die.style.transform = \"rotateX(0deg) rotateY(-90deg)\";\r\n        case \"4\":\r\n          return die.style.transform = \"rotateX(0deg) rotateY(90deg)\";\r\n        case \"5\":\r\n          return die.style.transform = \"rotateX(-90deg) rotateY(0deg)\";\r\n        case \"6\":\r\n          return die.style.transform = \"rotateX(0deg) rotateY(-180deg)\";\r\n      }\r\n    } else {\r\n      die.style.animation = \"rotate 5s infinite linear\";\r\n    }\r\n  }\r\n\nThe resulting Web Component\nclass AwesomeStuff extends HTMLElement {\r\n  constructor() {\r\n    super();\r\n    this.attachShadow({ mode: \"open\" });\r\n    this.render();\r\n  }\r\n\r\n  static get observedAttributes() {\r\n    return [\"value\"];\r\n  }\r\n\r\n  attributeChangedCallback(name, oldValue, newValue) {\r\n    if (name === \"value\") {\r\n      this.updateFace(newValue);\r\n    }\r\n  }\r\n\r\n  updateFace(value) {\r\n    const die = this.shadowRoot.querySelector(\"#die\");\r\n    if (value) {\r\n      switch (value) {\r\n        case \"1\":\r\n          return die.style.transform = \"rotateX(0deg) rotateY(0deg)\";\r\n        case \"2\":\r\n          return die.style.transform = \"rotateX(90deg) rotateY(0deg)\";\r\n        case \"3\":\r\n          return die.style.transform = \"rotateX(0deg) rotateY(-90deg)\";\r\n        case \"4\":\r\n          return die.style.transform = \"rotateX(0deg) rotateY(90deg)\";\r\n        case \"5\":\r\n          return die.style.transform = \"rotateX(-90deg) rotateY(0deg)\";\r\n        case \"6\":\r\n          return die.style.transform = \"rotateX(0deg) rotateY(-180deg)\";\r\n      }\r\n    }\r\n  }\r\n\r\n  render() {\r\n    this.shadowRoot.innerHTML = `\r\n    <div id=\"die\">\r\n      <div class=\"face face-1\">1</div>\r\n      <div class=\"face face-2\">2</div>\r\n      <div class=\"face face-3\">3</div>\r\n      <div class=\"face face-4\">4</div>\r\n      <div class=\"face face-5\">5</div>\r\n      <div class=\"face face-6\">6</div>\r\n    </div>\r\n    <style>${this.css()}</style>\r\n  `;\r\n  }\r\n\r\n  css() {\r\n    return `\r\n      #die {\r\n          --die-size: 50px;\r\n          --face-offset: calc(var(--die-size) / 2);\r\n          position: relative;\r\n          width: var(--die-size);\r\n          aspect-ratio: 1 / 1;\r\n          font-size: var(--face-offset);\r\n          transform-style: preserve-3d;\r\n          transition: transform 1s ease-out;\r\n      }\r\n\r\n      .face {\r\n          position: absolute;\r\n          width: var(--die-size);\r\n          aspect-ratio: 1 / 1;\r\n          background: orange;\r\n          text-align: center;\r\n          font-size: var(--die-size);\r\n          line-height: var(--die-size);\r\n      }\r\n\r\n      .face-1 {\r\n          transform: rotateY(0deg) translateZ(var(--face-offset));\r\n      }\r\n      .face-2 {\r\n          transform: rotateX(270deg) translateZ(var(--face-offset));\r\n      }\r\n      .face-3 {\r\n          transform: rotateY(90deg) translateZ(var(--face-offset));\r\n      }\r\n      .face-4 {\r\n          transform: rotateY(270deg) translateZ(var(--face-offset));\r\n      }\r\n      .face-5 {\r\n          transform: rotateX(90deg) translateZ(var(--face-offset));\r\n      }\r\n      .face-6 {\r\n          transform: rotateY(180deg) translateZ(var(--face-offset));\r\n      }\r\n    `;\r\n  }\r\n}\r\ncustomElements.define(\"awesome-stuff\", AwesomeStuff);\r\n\nThis button generates a random value between 1 and 6, and applies it to the component:\n<awesome-stuff id=\"awesome\"></awesome-stuff>\r\n\r\n<button onclick=\"document.getElementById('awesome').setAttribute('value', Math.ceil(Math.random() * 6))\">roll</button>\r\n\n\nSo why are Web Components not mainstream yet?\nI know some big companies use them. A big bank in The Netherlands uses almost nothing else for their entire front-end, including the mobile app. They are production ready, awesome and well structured. The problem I see is that the big frameworks don\u2019t have an easy way to build them and don\u2019t easily steer you towards Web Components (probably because they want you to use their framework \ud83d\ude09). Like I mentioned, they are not a replacement of any framework because of missing features, so instead of using them natively a lot of projects fall back to frameworks.\nConclusion on Web Components\nWeb Components are awesome! You can further optimize this dice component and re-use it everywhere, or build something completely different and still use it everywhere! Web Components are great for your low-level components like custom buttons, form elements and other elements that have specific company-wide layouts. They are also great for isolated components such as wizards that guide users through something, complete forms that appear many times in your web application or even your entire website with the right tooling!\nHowever,\u00a0 I do recommend using a framework if you are going to create an entire web application with Web Components, such as Lit, to make sure you have an easier way to do lifecycle hooks, data management and testability. Lit is super small and very close to the native specification so everything I just explained, will work (better) using Lit and the components are equally well distributable if you do have Lit as a dependency. Check out this next codepen for a nice example of two Web Components, the dice with optional 4, 6, 8, 10, 12 and 20 sides and a dicetray, which are a little more complex.\n\nFurther recommondations:\nI built (better) dice and a dicetray, find it here. Looking for the NPM release of the dice component? Find that here. Want to share your thoughts? Get in touch with me via LinkedIn.\n", "tags": ["component", "front end", "frontend", "native", "ui", "ux", "web", "webcomponent", "webdevelopment"], "categories": ["Blog", "Concepting &amp; UX", "Development"]}
{"post_id": 39710, "title": "AI Ethics: Navigating the Landscape", "url": "https://www.luminis.eu/blog-en/ai-ethics/", "updated_at": "2024-01-29T08:58:17", "body": "Last year, some of the most influential tech gurus signed an open letter to pause AI development to develop AI ethics. The letter was signed by 20,000 people, including Elon Musk, Andrew Yang, and the philosopher Yuval Noah Harari.\nThe goal was to work together to develop and implement shared safety protocols for advanced AI design and development. These protocols would be rigorously audited and overseen by independent outside experts to ensure the safety of AI technology. What has been done since 12th April 2023? Were nine months enough to achieve that enormous task? Seriously, what is lying underneath all of this?\n\nIn this blog post, we dive into the origins of AI ethics, discuss whether AI can have ethics, and look into what the ethical concerns regarding AI are today.\nThe origins of AI ethics\nThe letter asks for AI systems to be \u201cmore accurate, safer, interpretable, transparent, robust, aligned, trustworthy, and loyal\u201d. Also, it asks for more governmental regulations, independent audits before training AI systems. It also points to the need for more \u201ctracking highly capable AI systems and large pools of computational capability\u201d. And finally more \u201crobust public funding for technical AI safety research\u201d.\nAll this belongs to the ethics domain, a philosophical discipline developed over the centuries, mainly by philosophers. After WWII, during the doctor\u2019s trial in Nuremberg, doctors were judged for using war prisoners for experiments without their consent. All of them were sentenced as guilty. Situations like this founded \u201capplied ethics\u201d which are codes or guidelines that help professionals behave according to what is expected.\nIf we want to specifically trace the origins of ethics in AI, we must talk about Isaac Asimov\u2019s famous Three Laws of Robotics (Runaround, 1942):\n\nA robot may not injure a human being or, through inaction, allow a human being to come to harm.\nA robot must obey the orders given by human beings except where such orders would conflict with the First Law.\nA robot must protect its existence as long as such protection does not conflict with the First or Second Law.\n\nIf you read Asimov, you know how his stories revolve around how these simple laws can be conflictive. A lot has changed since he wrote this, but the need to regulate machine behavior remains. So, what are the main concerns today? How can companies benefit from applying ethics in AI systems?\nDoes AI have ethics?\nNo matter whether you are trying to provide a better recommendation system or creating a chatbot for hiring new employees. Your objective should be to provide the best service to your customers. But what does better mean? Should it just be faster no matter what? Should it run autonomously and decide what is better for its users?\nOf course, the need for a special ethic in each AI domain is crucial. It is not going to be the same principles while designing autonomous vehicles as when we are dealing with cybersecurity. Each sector will focus on different aspects, but we can also make sure that some of them will be common.\nIf we look at the history of ethics, there is one principle that has been followed since the very beginning, the no harm principle. The Hippocratic oath has been followed by doctors for over two hundred years. And even with updates, not harming directly or indirectly remains a top priority.\nThis aspect is key. We should be careful while we try to do things better, otherwise we could end up harming others and ourselves. Imagine you have a chatbot spreading misinformation. It won\u2019t only hurt people who receive that information but also the company\u2019s reputation.\nAs has been said hundreds of times before, there is no moral in technology itself. We must adapt discoveries to our social values.\nWhat are the main ethical concerns today?\nThere are four major concerns when applying AI technology for use in business.\nBias in AI:\nBiased data has a huge impact on AI training. We know that the system is going to be as good as the data used in its training. Stereotypic data can target certain groups as less trustworthy making it more complicated to access personal credits, etc.\nAs an example, we have face recognition systems that perform better if the user is male and white. This is a well-known problem and has been widely covered. In future posts, we will cover some practices to help you avoid these situations.\nTransparency of AI Systems:\nMany AI systems rely on Machine Learning algorithms that extract patterns from a given dataset during training. On top of that, these patterns can change with every update the system gets, so even if we find an explanation for a pattern, it can change or get outdated when new data arrives. This means that patterns are often hidden from the programmers or, in other words, not transparent. As I mentioned, the biased data on the system can remain hidden from the users and enforce it.\nPrivacy and surveillance:\nThis problem has been well-known for the last decade. Since the term Big Data was everywhere, people started paying attention to the data they were given free on the internet. I\u2019ve seen this on close friends that went from \u201cI have nothing to hide\u201d to \u201cI only turn on my location when I\u2019m using my GPS\u201d. Technology has changed significantly in the last decade, but the regulation responded slowly. The same is going to happen to AI.\n\u201cFree\u201d services like social networks are well known for exploiting the weaknesses of their users (procrastination, addiction, manipulation, etc.) for the extraction of personal data and maintaining their attention. We all know that big companies exploit this situation, sometimes hiding this fact. The occupation of \u201cinfluencer\u201d does not get the name for anything else than that.\nAI increases both the possibilities of intelligent data collection and the possibilities for data analysis. Among other problems, personal privacy and surveillance can be at risk with technologies that include device fingerprinting, face recognition, etc., and can lead to identification and search for individuals way more easily. These systems will often reveal facts about us that we wish to suppress or are not aware of like where have we been by making use of the GPS without notice.\nManipulation of behavior:\nAs I was explaining, AI problems in surveillance go further in the accumulation of data. It is quite common that the collected data is used to retain users\u2019 attention, which not only generates an addiction to their service but also manipulates the decisions that the users make online and offline.\nMisinformation is one of the main concerns that online platform users have. It is well known the case of Cambridge Analytica, which used social media as a propaganda platform to alter the vote of thousands of Americans in 2016. Of course, a lot has changed since then, but so has the sophistication of these systems. Today we deal with deep fake photos, videos, and phone calls so real that we have to include fact-checking as part of our daily routine.\nWhat are governments doing?\nThere are different ways to legislate a technology that is supposed to change everything. On one side we have the US vision, which doesn\u2019t put almost any obstacle in the development. On the other hand, there are countries like France or Germany that share the Europe vision to legislate AI more restrictively. Even the Vatican has its manual.You can check the AI Act, created by the EU, which defines some act guides depending on the risk the system supposes. \u201cUnacceptable risks\u201d like social scoring or cognitive behavioral manipulation of people will be banned, for \u201climited risk\u201d the AI system should comply with minimal transparency, allow the user to make informed decisions, etc. You can read the whole document here.\nIn October 2023, during the G7, Japan presented a different approach named the Hiroshima AI Process. This approach is a point in between the other two, but it\u2019s enough specific to focus on problems like copyright or the revelation of personal data. Some of the defined rules include:\n\nBefore and during deployment: identify, evaluate, and migrate risks.\nMitigate vulnerabilities and patterns of misuse.\nTransparency about limitations and/or inappropriate use.\nShare information responsibly with other organizations.\nImplement measures to protect personal data and intellectual property.\n\nConclusions\nAlthough AI is a relatively new tool, ethics for AI proposes an extension of data management in terms of ethics and legislation. If we do not consider AI systems as moral agents, and we still do not, the treatment of data that we should adhere to for our applications extends to the usage and regulations currently in place for GDPR data management.\nQuestions like who is collecting the data, why it is being collected, how long it will be stored, or with whom it will be shared, must be answered before using it. Let\u2019s say that you are using a chatbot that talks with your customers. If you want to store the conversations your users have with the chatbot, you should know how to handle that data, and maybe treat it as if it\u2019s sensitive content, inform the users, and even rethink why your company needs that data in the first place.\nTrying to set up some general rules or guidance is a complicated task, and as we saw with earlier technologies, general law restrictions come later than expected and sometimes even get outdated when published.\nIn the same way that we do not rely on government legislation to make our workspace safer or better for the environment, we must start doing this also to create better AI systems. Assume that you will be responsible for your AI system, so it will be your responsibility to double-check if your data is biased, collect just the relevant information, and most importantly, not harm its users.\nIf we as humans do not implement ethics within our AI designs, they will certainly not do it on their own, AI doesn\u2019t know how to, if we don\u2019t teach them.\n", "tags": [], "categories": ["Blog", "Machine learning &amp; AI"]}
{"post_id": 39703, "title": "Easy on The Eyes: UX Design in Extreme Environments", "url": "https://www.luminis.eu/blog-en/easy-on-the-eyes-ux-design-in-extreme-environments/", "updated_at": "2024-02-13T20:58:46", "body": "If the design of a user interface is executed poorly, it will obviously lead to frustrated users. In some situations, consequences can be more dire, like the false missile warning on Hawaii in 2018. In this blog post, you\u2019ll find out why UX design for extreme environments is important and learn some tips and tricks for your next project.\nA while ago, I got involved as a designer at a company that puts oil pipes on the bottom of the ocean. The weak point of these oil pipes is where two pipes are welded together. These seams are welded several times aboard a ship, and are then scanned with ultrasonic sound. These scans are rendered in an user interface. An expert is checking these raw readouts, looking for air bubbles and other faults. Doing so in long shifts, on a ship at sea.\nObviously, I wanted to be sure that the interface was a fit for this high demanding environment. Besides making sure the experts make the right call each time, other challenges like eye strain and fatigue also need to be overcome. So, how do you limit eye strain and fatigue in task-focussed, high-stakes GUIs?\n\nPreventing eye strain\nActivities that require you to look at various details for a longer time will tire your eyes and its muscles. The result is eye strain: you have trouble to focus your vision, you feel pain in your eyes or get a headache. Unpleasant for sure, but if those details in the interface are critical and reading them correctly matters a lot, like UX design in extreme environments, it becomes problematic.\nTo relieve eye strain, you can do several things:\n\n\nUse legible fonts in a larger size\nLarger font sizes can be read better, even from a distance. It then also takes less effort for your eyes to follow and read lines in a paragraph. Choose a typeface that has a good legibility, like the Atkinson Hyperlegible font, which has a good readability by starkly differentiating similar letterforms. Larger font sizes could mean less content fits on a screen, especially on mobile devices, but that\u2019s not necessarily a problem:\n\nPrevent reading the whole screen\nPrimary details and components that help the person perform a task and make decisions should not be spread throughout the whole screen. Otherwise, this would mean eyes have to jump around the screen and refocus on each element to \u2018read\u2019 the screen. Decide what is important in each moment of the flow of your application, and put those items together.\n\nEnough contrast\nMake sure there is enough contrast between text and icons, and their background (but do not overdo it). If contrast is bad (grey text on lighter grey background for example), it\u2019s hard to read and it makes the user squint and refocus, leading to eye strain. Putting the contrast on par with W3C\u2019s AA-level standards even helps users with colorblindness, since good contrast is colorblind itself. So, take it up as a challenge: create more contrast while your inner graphic designer still loves the end result. There are several tools available for this, like LearnUI and Get Stark.\n\nAway from the screen\nEye strain can also be prevented with how the application is used. Perhaps you do have some control over this. In that case, make sure there are frequent breaks from looking at any screen. Other options are preventing screen glare by reducing overhead lighting, and making sure there\u2019s enough distance between the user and the screen for the correct viewing distance.\n\nDark mode:\nTurning on dark mode could reduce the brightness of a screen while contrast can still be on W3C\u2019s AA-level. So when using a screen for long hours it can definitely be more comfortable to switch to a darker interface, and it will reduce eye strain. Readability or reading errors are pretty similar for both light and dark mode as well. (It\u2019s good to know that dark mode is not more beneficial than light mode during the night, when considering sleep health.)\n\n\u00a0\nReducing cognitive load\nEverything you see, hear, feel and think about while using a product will use some brain power to make sense of it, and to decide what to do next to finish the task. This is called the cognitive load. The higher the load, the more tiresome the task.\nSo, limiting this cognitive load can be beneficial for the application\u2019s usability: a person can work longer with it, stay focused for longer, and make less mistakes.\nBe aware that context has a big impact as well. Consider an application that will be used on a factory floor. The noise and distractions of the environment are adding to the cognitive load, before we even consider the use of the application. You have to design with that context in mind and do user tests in those conditions to verify of your solution is usable on that factory floor.\nThere are several ways to reduce cognitive load:\n\n\nFamiliar and intuitive interactions\nCreate a solution with predictable interactions and layouts. Many typical UI challenges have evolved into design patterns throughout the years. These familiar design patterns make an application obvious and intuitive in use, so less brain power wasted. Be aware that the domain you\u2019re working in can have it own unique design patterns.\n\nReduce visual noise\nConsider each screen a person will see and use to perform a task. Some items are crucial for completing that task while others are supportive or only for edge cases.\nCreate a hierarchy to show that difference (by using visual design principles for example). Take a step further by removing everything that is not important, or by moving it to another screen or slide-in.\n\nSimplify and remove ambiguity\nLike reducing visual noise, you can reduce interactive noise as well. Show less information, reduce interactions and limit navigation steps. Hick\u2019s Law states that the time it takes for a person to make a decision will increase with more (equally probable) options. So, limit those options.\nAnd make sure the navigation is easy and requires little contemplation. Navigation is not where you want to do new and unexpected things in critical environments.\nSimplifying does not have a minimal design. It is about taking away ambiguity. Use shorter and more to the point texts, and use everyday words that fit the domain you\u2019re working in. Sometimes, an illustration or additional information is very helpful.\nIcons can be hard to understand, so add labels. And make them more readable by adding meaning and contrast, so each is recognizable in a glance. A good example provided by MegDraws:\n\n\nOffloading tasks:\nOffloading tasks is taking it one step further. Let the application understand or remember details, or make a decision. Not asking for an input will limit the cognitive load of the person.\n\n\u00a0\nDesign against fatigue\nEye strain and limited cognitive load are specific symptoms of fatigue. A person is tired, cannot concentrate, and starts making mistakes. The right UX design in extreme environments can make huge differences. The suggestions mentioned above will help you to reduce symptoms like fatigue and eye strain. As a result, the user is less likely to make mistakes that are related to the user-experience of applications.\nIf fatigue really is an obstacle, consider solutions that go beyond what\u2019s on the screen. Create more variation in tasks or work, as monotonous work will lead to less focus. Let the person have short breaks so they can go on a walk. Even drinking water and having bright lights helps.\nWhen you work on a project that is more task-focused, in a high-stakes context, consider the suggestions above. Go a bit further, add wiggle room and don\u2019t go for \u201cjust enough\u201d. And while designing and developing this user interface, test it, and test it again, as realistically as possible. Make sure your end-result won\u2019t lead to the project\u2019s equivalent of a false inbound missile alarm.\n", "tags": [], "categories": ["Blog", "Concepting &amp; UX"]}
{"post_id": 39675, "title": "GenAI, It\u2019s Happening Now", "url": "https://www.luminis.eu/blog-en/genai-its-happening-now/", "updated_at": "2024-01-17T12:25:00", "body": "AI is all around us, and the rise of Generative AI, also known as GenAI, is increasingly being discussed. Numerous articles have been written about it, but one pressing question remains: Is GenAI already usable within my business, or is it just a fun gadget?\nIn this blog post, we explore the potential of GenAI. Together, we investigate the current possibilities, how we already use GenAI for our clients and the associated risks. The goal is to inspire you to think about the impact of GenAI on your business and your daily activities and to provide you with practical tools. Together, we can explore how you can benefit from GenAI.\n\nDALL E created an advertisement poster that convinces business people to start using Generative AI.\nGenAI, a short introduction\nGenAI is the abbreviation for Generative Artificial Intelligence. AI is a broad field where computers perform tasks we do not want or cannot do due to size, speed, or accuracy. Much of AI is used to find data patterns to make predictions and group or compare data. AI has been applied for decades. Consumers came into contact with it through chess computers, voice assistants, self-driving cars, and much more.\nGenerative Artificial Intelligence\nGenerative AI is a particular area of AI where a system converts questions or commands into new data or content. The general public came into contact with GenAI when the company OpenAI launched the product ChatGPT at the end of 2022. ChatGPT allowed us to generate larger pieces of text and create summaries. It became even more interesting when people started using it to go from one language to another. You could even go from an ordinary language to a programming language. This made it possible to write an SQL query based on natural language.\nReview all invoices from the past month, group them by product categories, and calculate the total revenue and the average revenue per category.\nBesides text and, thereby, programming languages, the next step was the generation of imagery. By utilising the technology of Stable Diffusion, it became possible to create visual material. Tools such as DALL-E and Midjourney can create an image based on your question or command. The image above this article was also generated using a prompt.\nCreate an advertisement poster that convinces business people to start using Generative AI, do NOT use text characters in the image.\nEen deepfake foto van Jettro als een Cowboy\nIn addition to static images, there are possibilities to generate moving images. This generation of imagery is now also known as Deepfakes. I have also experimented with it myself; suddenly, I became a rugged cowboy from the movie The Good, the Bad, and the Ugly.\nAll this technology is also finding its way into the business world. In the next part of this blog, we look at applications and the possible consequences of these applications for your work.\nApplications of GenAI in a Business Context\nWithin the business world, GenAI is received differently. Many companies need a strategy for using tools like ChatGPT. This leads to uncontrolled growth within a company, making it unclear who uses it. Let alone that everyone understands the potential risks, for example, which data is or is not used to train the model better. This poses the risk that private data may become available through the model. Proper use of GenAI tools does provide a performance boost to employees. If we, for now, forget about the risks for a moment and focus primarily on the possibilities, what are the typical applications we encounter?\nCustomer contact with Large Language Models\nConsider the initial contact with customers through a chatbot and answering the phone using voice technology. Those recordings with \u2018press option 1, press option 3, press the hash key\u2019 can be replaced by an AI Bot that listens to the question and converses with a person on the phone. If the bot can\u2019t handle it, or if the person doesn\u2019t want to talk to a bot, the person only needs to indicate this, and they will be transferred to a natural person instead of the bot. For the company, this means flexibility and not spending time developing such an option menu for the phone. This is possible with tools like OpenAI\u2019s Whisper and Elevenlabs.\nBy leveraging the capacity of Large Language Models, you can engage in different forms of interaction with your users. Imagine you are a company offering people an unforgettable vacation experience. Then, you can let your users search and filter by choosing a country, type of vacation, number of people, and many other filter options. Alternatively, you can start a conversation with them through chat. Let them describe the kind of vacation they want. You can create an intermediary step by automatically selecting filters based on the text so the user can adjust.\nI want to vacation with two adults for three weeks sometime in August-September. I want to go to the mountains. I do not need warm weather. Besides the beautiful views, I also want to relax occasionally. I prefer apartments, but sometimes a hotel is also acceptable. I want a maximum of 6 locations to propose my vacation.\nGenAI for developers\nAs a developer, you have to maintain an old application. You program everything in Rust or Python these days, and now you have to work with Java. It\u2019s been a while, and you need help understanding everything. You start your development environment, select a piece of code, and ask what that code does. It\u2019s been explained to you correctly. You need to migrate the code to Java 21, request a suggestion, paste the suggestion into your code, and run all the unit tests again. If you get an error, you ask what this error means and how you can fix it. A developer can do this with a tool like GitHub Copilot and JetBrains AI Assistant.\nWe could go on like this for a while. You will also start to see more and more tools adding suggestions that use AI. Microsoft is also working hard on an assistant that is permanently present in all your Office Tools. Then, you can ask Excel to create a formula in a cell that calculates the average and standard deviation of all columns D to K. Or ask PowerPoint to create a presentation about last year\u2019s figures with the Excel file as input.\nIs it all fantastic, and should everyone dive in headfirst? Are there no risks? We will discuss this in the next part of this blog.\nThe risks of GenAI\n\nUsed from euro parliament document.\nThe use of Generative AI has risks. The EU is busy developing legislation around the use of AI. But what are these risks? The risks exist on several levels. First, I will examine the legislation the EU is working on. Here, it mainly concerns the use, transparency, fairness, and privacy.\nIn the EU AI Act, careful consideration has been given to the risk of AI to society. Four levels of risks have been identified. In the highest layer, the risk is unacceptable. Therefore, it is prohibited by law. An example is the use of real-time facial recognition in public places.\nAnother example is the exploitation of people from vulnerable groups through AI. The next layer is High Risk. In this case, mandatory registration of the system is necessary, along with certification. You must prove that the system meets the established requirements regarding risk management, testing robustness, transparency, and safety. Examples of applications are toys, aircraft, vehicles, and medical devices. The other two layers have fewer requirements.\nThe discussion about the legal definition of AI is interesting. Some find it too broad, thereby hindering innovation, while others believe the opposite.\nRisks of ChatGPT and Co-pilot\nAnother risk I want to discuss is using services like ChatGPT from OpenAI and Co-pilot from Github. These tools work with a Large Language Model (LLM) and a prompt. The prompt contains the question and the context. This context can include data you do not want publicly available. An LLM is a model trained on a lot of data. Especially in the early days of the popularity of these tools, it was unclear whether your data was being reused to improve the model or to train it.\nI looked into the policies of both tools regarding the use of the data sent with them. Your data is either used by default or not, depending on your subscription. But in both cases, you can turn off the use of your data for training their model. The business subscriptions do not use your data by default, and the individual subscriptions allow you to turn it off. Free subscriptions do not give you that option. Therefore, it is essential that you thoroughly investigate each tool to understand how you can use it without sharing your data and without using it to improve the model.\nNow that you have an idea of the possibilities of GenAI for your business and insight into the risks, how can you get started? That is the topic of the next part of this blog.\nStarting with GenAI\nYou can, of course, wait to implement AI, and particularly GenAI, within your company. Wait until the legislation is finalised. Wait until everyone is doing it. By then, you\u2019ll probably be too late. It\u2019s much better to start now. Begin small, and start playing with tools that are already available. Let yourself be inspired by what already exists. Do not dive in headfirst without thinking. Start with a strategy and policy for using GenAI within your company. Help the people within your company to use it safely.\nTips and tricks\nWhen you\u2019re comfortable taking the next step, moving beyond just playing around, investigate where you could make the most impact. Assess the risk of AI for that specific situation. Use the layered model according to European legislation. Implementing GenAI or AI within your organisation requires knowledge. Knowledge about processing and storing data. For AI, too, the principle \u2018Garbage in, Garbage out\u2019 applies.\nFor GenAI, the rule \u2018Garbage in is garbage out\u2019 also applies. Invest in your data platform.\nWhile you are working on establishing your internal rules and policies around GenAI, you can already start with content creation. You can test your steps in setting up your policy right away. You will notice that by using the existing tools, on the one hand, you will use them more and more, and on the other hand, more and more questions about policy will arise.\nGenAI can also be used effectively to inspire internal workshops. For instance, part of a planning meeting could involve discussing ideas generated by GenAI. You can see it as an additional team member who can think creatively, not as a replacement.\nThen, proceed to look for repetitive steps in business processes. Due to the repetition, this is a good entry point where GenAI can take over. We have already been able to implement and set up GenAI successfully.\n", "tags": ["Generative AI"], "categories": ["Blog", "Machine learning &amp; AI"]}
{"post_id": 39574, "title": "Convert a REST API to AWS Lambda with minimal effort using Quarkus", "url": "https://www.luminis.eu/blog-en/convert-a-rest-api-to-aws-lambda-with-minimal-effort-using-quarkus/", "updated_at": "2024-01-17T09:31:24", "body": "In this blog post, you learn how to convert a REST API to an aws Lambda without any refactoring. The application is already using the Quarkus framework, but with an API based on RestEASY Reactive. Keep reading to find out why you might need the conversion , how I utilized AWS Lambda with AWS\u2019s http gateway and learn about the benefits of saving costs and development time.\nPlaying darts at the office\nWe have a nice side-project at the office, a simple app that we use when playing dart games. Recently, I took upon myself to add a leaderboard to the app. In short, the back end application would be used to save game data and serve high score lists. Because I already loved the Quarkus framework and wanted to show its power to my colleagues, I decided to use Quarkus for this task.\n\nThe idea was simple: Accept different types of game data (for different game modes that the web app provides) and store it in a DynamoDB table. Have some endpoints to query data for different types of games or for specific players, and there you have it.\nI already wrote a functioning application when a colleague asked: \u201cWhy not make it in the form of Lambdas?\u201d Since the application would only be used sporadically, it would be a waste of money to have an EC2 instance running continuously. Response time is not of much importance in this case, and cold start response times could be mitigated by compiling a native executable using GraalVM native image, so a serverless solution seemed ideal.\nWhich Quarkus extension to use?\nThere are multiple Quarkus extensions available for AWS Lambda development, each with their own advantages and drawbacks:\n\nquarkus-amazon-lambda: the bare-bones Lambda development kit;\nquarkus-funqy-amazon-lambda: Lambda binding for the cloud agnostic funqy library;\nquarkus-amazon-lambda-http: develop Lambdas to deploy behind an aws http gateway;\nquarkus-amazon-lambda-rest: develop Lambdas to deploy behind an aws API gateway.\n\nSince I had already written the application, extensions number 1 and 2 would require lots of refactoring because each function needs their own Quarkus project. The extensions use a shared code imported as a dependency (unless you have no problem with hacking the framework to change this behavior).\nExtensions 3 and 4 allowed me to keep my application code as it was. My resource (or controller) classes looked like any other resource class.\u00a0 However, it would, with a simple command, be deployed as a single lambda, along with a gateway that handles incoming requests. This doesn\u2019t only work for RestEASY Reactive, but for any http framework offered by Quarkus such as Undertow,\u00a0Reactive Routes,\u00a0Funqy-HTTP\u00a0or\u00a0Spring Web API. \u00a0For this case, I chose the quarkus-amazon-lambda-http extension because it didn\u2019t need the extra functionality provided by the API gateway. Additionally, the HTTP gateway comes with lower latency at a lower price point.\nHow to convert a REST API to AWS Lambda?\nWhen converting a REST API to AWS Lambda, I recommend to consider the following methods. This is an example of an API built on RestEASY, which is Quarkus\u2019 default extension for http servers. There are small differences between the classic and the reactive versions of RestEASY. However, the code below is valid for both. If you know Jakarta, this code might also look familiar, because the annotations (as well as the MediaType\u00a0and\u00a0Response\u00a0classes) all come from the\u00a0jakarta.ws.rs\u00a0package.\n// 1\r\n    @GET\r\n    @Path(\"hello\")\r\n    public String hello() {\r\n        return \"hello, world\";\r\n    }\r\n// 2\r\n    @GET\r\n    @Path(\"hello/{input}\")\r\n    public String hello(String input) {\r\n        return \"hello, \" + input;\r\n    }\r\n// 3\r\n    @POST\r\n    @Path(\"hello\")\r\n    public String hello(MyClass myObject) {\r\n        return \"hello, \" + myObject.getValue();\r\n    }\nWhen you include the\u00a0quarkus-amazon-lambda-http\u00a0extension in your POM file and deploy your application as a Lambda, you can just send http requests to your\u00a0gateway url. Then you can use paths and pass json objects like you normally would for any REST API. As a result, all of the methods above are still accessible. For instance, method number 2 can be invoked with a GET request to https://{restapi_id}.execute-api.{region}.amazonaws.com/{stage_name}/hello/Luminis\u00a0and should return \u201chello, Luminis\u201d.\nIf you want to test your Lambda directly via the aws Lambda test console, you should wrap your http request in a Lambda request like so:\n{\r\n  \"body\": \"{ \\\"myValue\\\" : \\\"lambda\\\"}\",\r\n  \"resource\": \"/{proxy+}\",\r\n  \"path\": \"/hello\",\r\n  \"httpMethod\": \"POST\",\r\n  \"isBase64Encoded\": false\r\n}\r\n// Output: \"hello, lambda\"\nThis is how the gateway passes requests to your Lambda. Notice that the value of the body parameter has quotes around it and that the inner quotes escaped. This is because, even though we are including a json object, the body must be provided as a string.\nWait, that\u2019s it? All we did was add a dependency and it just works?\nShort answer: Yes!\nLonger answer: Yes, but you need to have your\u00a0AWS credentials configured correctly\u00a0and you need the\u00a0SAM cli\u00a0to test and deploy your lambda.\nDeploying and testing\nTo build our application, we can use the maven or gradle wrapper that comes with your Quarkus project. However, we\u2019re going to use the Quarkus cli. Run the following command in the root directory of your project:\nquarkus build --native --no-tests -Dquarkus.native.container-build=true\nthe\u00a0--native\u00a0flag tells the cli to build a native image using GraalVM. When you use the\u00a0-Dquarkus.native.container-build=true flag, you don\u2019t need to have GraalVM installed. However, you need to have Docker\u00a0or\u00a0Podman running. Your machine automatically pulls a Docker image and spins up a container, which then builds your Lambda! This is great because GraalVM native image normally only allows you to build binaries for your OS. Instead, the Docker image is always Linux, which is what we need. If you\u2019re on Linux and have GraalVM configured correctly, you can just run quarkus build --native. If you\u2019re wondering why we\u2019ve added an --no-tests\u00a0argument, it\u2019s because the test code is not included in the build. You can have the tests run separately in your pipeline, but that\u2019s a story for another time.\nNext, we can use the SAM cli to deploy on a local environment on docker for testing:\nsam local start-api --template target/sam.native.yaml\nOr we can deploy on AWS Lambda:\nsam deploy -t target/sam.native.yaml -g\nConsiderations\nWhile it\u2019s convient to convert a REST API to AWS Lambda, it\u2019s possibly considered a better practice to split your application into multiple Lambdas for the sake of atomicity and performance. However, for simple applications, the pros might outweigh the cons. Especially because deploying a native image reduces startup time drastically.\nOne more thing to consider is that, apart from getting billed for the lambda\u2019s invocations and computing time, you\u2019ll also get billed for the gateway. Luckily, the pricing for the http gateway is only $1 per million requests for the first 300 million requests. It get even cheaper if you go over that threshold.\nConclusion on how to convert a REST API to AWS Lambda\nAny Quarkus-based http-server can be converted to a single AWS Lambda by simply adding the quarkus-amazon-lambda-rest\u00a0or\u00a0quarkus-amazon-lambda-http\u00a0extension to the POM file. Just make sure you have the required aws tools installed and configured on your machine and build a native image using GraalVM if you don\u2019t want unacceptable startup times. It can save you, your company, or your customer serious money if it means getting rid of a mostly idle EC2 instance and requires no refactoring on your server code at all.\n", "tags": [], "categories": ["Blog", "Cloud"]}
{"post_id": 39528, "title": "AWS Multi-Account GitOps Deployment 2: GitHub Actions Setup", "url": "https://www.luminis.eu/blog-en/aws-multi-account-gitops-deployment-2-github-actions-setup/", "updated_at": "2024-02-08T12:31:56", "body": "Welcome back to the second installment of our series on multi-account GitOps deployment on AWS. In the first part, we navigated through setting up a multi-account AWS organizational structure. Now, we will focus on integrating GitHub for our deployment processes and ensuring all the accounts are ready for deployments.\nIntegrating AWS Accounts with GitHub\n1. Access Management Setup:\n\nLog in to the Management Account: Navigate to AWS and sign in to your management account.\nGo to IAM Identity Center: Once you are logged in, access the IAM Identity Center.\nAccess AWS Accounts: Under IAM Identity Center, find and click on AWS Accounts.\n\n\n\nAssign Users or Groups:\u00a0Select all the newly created accounts (deployment, dev, test, prod) and click on \u201cAssign Users or Groups\u201d.\nSelect the Main User:\u00a0Choose the main user and click next.\nSelect the Permission Set:\u00a0Now, select the Administrator Access permission set (or any other relevant permission set you have created) and click next.\nSubmission:\u00a0Review your configurations and click submit\n\n\n\nVerify Account Access:\u00a0You should now be able to see all the accounts listed in your AWS Access portal at\u00a0https://lutku.awsapps.com/start#/\n\n\n2. AWS CLI and SSO Configuration:\nFor all the newly created accounts,\u00a0add them to the same session name using AWS CLI as we have done in the first instalment of the series:\naws configure sso\r\n\nFollow the prompts and perform this action for all the accounts\u00a0(deployment,\u00a0dev,\u00a0test,\u00a0prod).\u00a0Example session names could be:\n\nlutku-deployment\nlutku-dev\nlutku-test\nlutku-prod\n\n3. GitHub Actions Setup:\nNow,\u00a0let\u2019s set up our repository for AWS CDK and GitHub Actions:\nmkdir actions-setup && cd actions-setup\r\ngit init\r\nnpx projen new awscdk-app-ts\nUpdate the\u00a0.projenrc.ts\u00a0configuration file similar to following:\n\nimport { awscdk } from 'projen';\r\n\r\nconst project = new awscdk.AwsCdkTypeScriptApp({\r\n   authorEmail: 'utku.demir@luminis.eu',\r\n   authorName: 'Utku Demir',\r\n   cdkVersion: '2.96.2',\r\n   defaultReleaseBranch: 'main',\r\n   name: 'actions-setup',\r\n   description: 'A CDK project for GitOps Deployments',\r\n   github: false,\r\n   projenrcTs: true,\r\n   keywords: [\r\n      'AWS CDK',\r\n      'projen',\r\n      'Typescript',\r\n      'Deployment',\r\n   ],\r\n   gitignore: ['.idea'],\r\n   license: 'MIT',\r\n   licensed: true,\r\n});\r\nproject.synth();\r\n\nAfter updating,\u00a0generate the project:\nyarn projen\nCreate a new file named\u00a0actions_setup_stack.ts\u00a0under\u00a0src\u00a0to include the necessary configuration for our stack:\n\nimport { Stack, StackProps } from 'aws-cdk-lib';\r\nimport * as iam from 'aws-cdk-lib/aws-iam';\r\nimport { Construct } from 'constructs';\r\n\r\nexport interface ActionsSetupStackProps extends StackProps {\r\n  repositoryOwner: string;\r\n  gitDeployableAccounts: string[];\r\n}\r\n\r\nexport class ActionsSetupStack extends Stack {\r\n  constructor(scope: Construct, id: string, props: ActionsSetupStackProps) {\r\n    super(scope, id, props);\r\n\r\n    const githubOidcProvider = new iam.OpenIdConnectProvider(this, 'github-oidc-provider', {\r\n      url: 'https://token.actions.githubusercontent.com',\r\n      clientIds: ['sts.amazonaws.com'],\r\n      thumbprints: ['6938fd4d98bab03faadb97b34396831e3780aea1'],\r\n    });\r\n\r\n    const webIdentityPrincipal = new iam.WebIdentityPrincipal(githubOidcProvider.openIdConnectProviderArn, {\r\n      StringEquals: {\r\n        'token.actions.githubusercontent.com:aud': 'sts.amazonaws.com',\r\n      },\r\n      StringLike: {\r\n        'token.actions.githubusercontent.com:sub': `repo:${props.repositoryOwner}/*`,\r\n      },\r\n    });\r\n\r\n    new iam.Role(this, 'github-actions-deploy-role', {\r\n      roleName: 'github-actions-deploy-role',\r\n      assumedBy: webIdentityPrincipal,\r\n      inlinePolicies: {\r\n        AllowCrossAccount: new iam.PolicyDocument({\r\n          statements: [\r\n            new iam.PolicyStatement({\r\n              effect: iam.Effect.ALLOW,\r\n              actions: ['sts:AssumeRole'],\r\n              resources: [\r\n                `arn:aws:iam::${this.account}:role/cdk-hnb659fds-*-role-${this.account}-*`,\r\n                ...props.gitDeployableAccounts.map(appAccount => `arn:aws:iam::${appAccount}:role/cdk-hnb659fds-*-role-${appAccount}-*`),\r\n              ],\r\n            }),\r\n          ],\r\n        }),\r\n      },\r\n    },\r\n    );\r\n  }\r\n}\r\n\nThe above stack,\u00a0based on the work of Wojciech Matuszewski on Deploying AWS CDK apps using short-lived credentials and Github Actions,\u00a0creates a web identity principal for the GitHub OIDC provider and a role for this web identity principal to assume to be able to conduct deployments.\nDifferent from the stack that deployed the organization structure,\u00a0I recommend creating a file named\u00a0cdk.context.json\u00a0in the root of the project to store the configuration necessary for this stack.\n\n{\r\n  \"repositoryOwner\": \"<owner_of_the_repository>\",\r\n  \"gitDeployableAccounts\": [\r\n    \"<dev_account_id>\",\r\n    \"<test_account_id>\",\r\n    \"<prod_account_id>\"\r\n  ]\r\n}\r\n\nLastly,\u00a0edit the main.ts under src to include this stack as:\n\nimport { App } from 'aws-cdk-lib';\r\nimport { ActionsSetupStack } from './actions_setup_stack';\r\n\r\nconst devEnv = {\r\n  account: process.env.CDK_DEFAULT_ACCOUNT,\r\n  region: process.env.CDK_DEFAULT_REGION,\r\n};\r\nconst app = new App();\r\n\r\nconst repositoryOwner = app.node.tryGetContext('repositoryOwner');\r\nconst gitDeployableAccounts = app.node.tryGetContext('gitDeployableAccounts');\r\n\r\nnew ActionsSetupStack(app, 'actions-setup-stack', {\r\n  env: devEnv,\r\n  repositoryOwner: repositoryOwner,\r\n  gitDeployableAccounts: gitDeployableAccounts,\r\n});\r\n\r\napp.synth();\r\n\n4. Bootstrapping and Deployment:\nNow you\u2019ll need to bootstrap your AWS accounts to prepare them for CDK deployments.\nStart with the deployment account:\n\ncdk bootstrap aws://<deployment_account_id>/eu-west-1 --profile lutku-deployment\r\nyarn deploy --all --profile lutku-deployment\r\n\nNavigate to the AWS Management Console,\u00a0go to the IAM Roles in the deployment account,\u00a0and find the\u00a0github-actions-deploy-role.\u00a0Copy the role\u2019s ARN for the next steps.\n\nFor each environment\u00a0(dev,\u00a0test,\u00a0prod),\u00a0run the following commands:\n\ncdk bootstrap aws://<env_account_id>/eu-west-1 --profile lutku-<env> --trust <deployment_role_arn> --cloudformation-execution-policies 'arn:aws:iam::aws:policy/AdministratorAccess'\r\n\nReplace\u00a0<env_account_id>\u00a0with the respective account ID,\u00a0<env>\u00a0with the environment\u00a0(dev,\u00a0test,\u00a0prod),\u00a0and\u00a0<deployment_role_arn>\u00a0with the role ARN copied earlier.\n5. Verification:\nAfter running the above commands,\u00a0navigate to the CloudFormation console in each environment account\u00a0(dev,\u00a0test,\u00a0prod).\u00a0You should see a new stack created by the CDK bootstrap command.\u00a0This confirms that the accounts are now ready for deployments using GitHub Actions.\n\nConclusion\nAnd that\u2019s it!\u00a0You have successfully set up and integrated your AWS accounts with GitHub for GitOps deployments.\u00a0By following these steps,\u00a0you have created a seamless and efficient workflow for deploying your applications across multiple AWS accounts.\nRemember,\u00a0while this setup provides a robust starting point,\u00a0always continue to explore and adapt to the ever-evolving cloud landscape.\u00a0Stay tuned for the final installment of this series,\u00a0where we will delve deeper into advanced GitOps strategies and best practices.\nReferences\n\nAWS Multi-Account GitOps Deployment: Organizational Setup\nDeploying AWS CDK Apps Using Short-lived Credentials and GitHub Actions\n\nHappy Cloud Engineering and until next time!\n", "tags": ["aws", "aws cdk", "CI/CD", "cloud", "github", "GitHub Actions", "iam", "infrastructure as code", "OIDC"], "categories": ["Blog", "Cloud"]}
{"post_id": 39480, "title": "LLM Series, part 1: A Comprehensive Introduction to Large Language Models", "url": "https://www.luminis.eu/blog-en/llm-series-part-1-a-comprehensive-introduction-to-large-language-models/", "updated_at": "2023-12-22T09:47:14", "body": "Large language models (LLMs) are all the buzz these days. From big corporations like Microsoft enhancing their office products to Snapchat having an assistant for entertainment, to high schoolers trying to cheat their assignments. Everyone is trying to incorporate LLMs into their products, services, and workflow.\nWith the surge in popularity, there\u2019s a flurry of discussions, blogs, and news articles about fine-tuning these models and their myriad applications. Even if you\u2019ve dipped your toes into the LLM pool, you might find yourself stumbling upon unfamiliar terms and concepts.\nIn this three-part blog series, we\u2019ll map out all the key concepts related to LLMs, so you can finally understand what your Machine Learning (ML) enthusiast colleague is talking about, but also potentially incorporate these powerful models into your projects. So, buckle up, and let\u2019s dive into the fascinating world of Large Language Models!\nPrior Knowledge\nBefore we dive deeper into the key LLM concepts, it\u2019s helpful to cover some foundational background knowledge. This will ensure we\u2019re all on the same page as we explore the intricacies of LLMs.\nTokens\nLet\u2019s start with a simple question: how would you split a sentence into words? Seems straightforward, right? But what if the sentence uses contractions like \u201ccan\u2019t\u201d or \u201cI\u2019m\u201d? And what if we switch to a different language, say, Swedish or Polish? Would you split it the same way?\nThis is where the concept of \u201ctokenization\u201d comes into play. It\u2019s all about splitting text into smaller, discrete units (or \u201ctokens\u201c), preferably, in a reversible way. This provides a neat, organized way for our models to process the text.\nOne of the key properties of tokens is that they belong to a fixed-size set, aptly named the \u201cvocabulary\u201c. This makes them much easier to work with mathematically. Each token can be represented by its unique ID in the set or as a one-hot vector.\n\nThe document is tokenized and one-hot encoded producing a fixed-size matrix of vectors. These vectors are fed through a function that transforms them into embeddings, effectively reducing the dimensionality.\nIn the olden days, tokenizers were quite lossy. It was common to work on stemmed words and only consider a set of the most common words. Modern tokenizers, instead, have evolved to focus on efficiency and losslessness. Instead of encoding whole words, algorithms such as Byte Pair Encoding (BPE) take a compression-like approach by breaking words apart.\nVocabulary construction is done in a purely data-driven manner, resulting in token splits that make sense semantically, such as the common verb ending \u201c-ing\u201d. Words like \u201cworking\u201d, \u201ceating\u201d, and \u201clearning\u201d all share this ending, thus an efficient encoding is to give \u201c-ing\u201d its own token. Some splits don\u2019t make sense, producing semantically dissimilar tokens such as \u201clab-elling\u201d which requires the model to do more work to infer its true meaning.\n\nOpenAI\u2019s `cl100k_base` tokenizer encoding a sentence. In this case, 100k refers to its vocabulary size.\nBut what if a word doesn\u2019t exist in the vocabulary? Like \u201cw0rk1ng\u201d? In this case, the tokenizer breaks it down into smaller chunks, sometimes even character by character.\nNow, it\u2019s tempting to assume that a token corresponds to a word. And while that\u2019s often the case, it\u2019s not a hard and fast rule. For simplicity\u2019s sake, we\u2019ll often use the terms \u201cword\u201d and \u201ctoken\u201d interchangeably in this series. But remember, in the wild world of tokenization, a token could be anything from a whole word to a single character or a common word part.\nFor ballpark estimates of token count, you can multiply your word count by 1.25 (assuming English text). This gives a reasonable approximation of the number of tokens in a given piece of text. However, if you\u2019re looking for a more precise estimate, you can use the OpenAI tokenizer web tool.\nToken Embeddings\nNow that we\u2019ve got our tokens, we need a way to represent them that captures more than just their identity. We\u2019ve seen that tokens can be represented as one-hot vectors, which are great for basic math but not so helpful when it comes to comparing different words together. They don\u2019t capture the nuances of language, like how \u201ccat\u201d and \u201cdog\u201d are more similar to each other than \u201ccat\u201d and \u201ccar\u201d.\nEnter the concept of \u201ctoken embeddings\u201c, also known as \u201cword vectors\u201d. The seminal paper Word2Vec was instrumental in bringing this concept to the mainstream. The authors built on two key assumptions from prior work:\n\nSimilar words occur in the same context (a concept known as Distributional Semantics).\nSimilar words have similar meanings.\n\nIt\u2019s important to note that these two assumptions are distinct. The first is about the context in which words are used, while the second is about the meanings of the words themselves.\n\nDemonstration of Linear Relationships Between Words Visualized in Two Dimensional Space. Image from Google Blog\nAt first glance, \u201csimilarity\u201d might seem like a subjective concept. But what if we think of words as high-dimensional vectors? Suddenly, similarity becomes a very concrete concept. It could be the L2 distance between vectors, the cosine similarity, the dot product similarity, and so on.\nIn the Word2Vec paper, the authors used gradient descent techniques to find embeddings for each word. The goal was to ensure that the above assumptions hold true when comparing these embeddings. In other words, they wanted to find a way to represent words as vectors in a high-dimensional space such that similar words (in terms of context and meaning) are close together in that space.\nThis was a game-changer in the field of natural language processing. Suddenly, we had a way to capture the richness and complexity of language in a mathematical form that machines could understand and work with.\nBut the real beauty of these embeddings is that they can capture relationships between words. For example, the vector difference between \u201cking\u201d and \u201cqueen\u201d is similar to the difference between \u201cman\u201d and \u201cwoman\u201d. This suggests that the embeddings have learned something about the concept of gender.\nHowever, it\u2019s important to remember that these embeddings are not perfect. They are learned from data, and as such, they can reflect and perpetuate the biases present in that data. This is an important consideration when using these embeddings in real-world applications.\nEncoders and Decoders\nWhile methods like Word2Vec are great for creating simple word embeddings, they produce what we call \u201cshallow embeddings\u201d. In this context, shallow means that a matrix of weights is trained directly, and thus can be used like a dictionary. As such, the number of possible embeddings is equal to the number of tokens in your vocabulary. This works fine when you\u2019re dealing with individual words, but it starts to break down when you\u2019re working with sentences or whole documents.\nWhy? Well, if you encode all the tokens in a sentence into embeddings, you lose all the order of the words. And as anyone who\u2019s ever played a game of \u201cMad Libs\u201d knows, word order is crucial when it comes to making sense of a sentence. By losing the order, you also lose the context, which can drastically change the meaning of a word.\n\nThe encoder produces a document embedding by combining the individual word embeddings.\nTo overcome this limitation, we need an additional model, the \u201cEncoder\u201d, which does some neural net math magic to create an embedding that takes both context and order into account.\nOn the other side of the equation, we have the \u201cDecoder\u201d. Its job is to produce a token from the input, which is typically a latent vector.\n\nVisualization of a encoding a sequence of tokens embeddings into a single latent representation, and decoding it into a sequence of token probabilities.\nThe architecture of how Encoders and Decoders work can vary greatly. It can be based on Transformers, LSTMs, or a combination of both. We\u2019ll dive deeper into these architectures in a later blog.\nOne interesting thing to keep in mind is that, since encoders and decoders operate in latent space, their input is not limited to text. They can also take embeddings produced from images, audio, and other modalities. This is thanks to innovations like CLIP, which are trained on multimodal tasks by introducing an encoder for each data type.\n\nNExT-GPT with text, image, audio, and video modalities.\nModeling Methods\nThe architecture of large language models isn\u2019t the only factor that gives them an edge. How they model natural language processing tasks also contributes greatly to their performance. Rather than taking a one-size-fits-all approach, large language models specialize in different modeling methods optimized for certain tasks.\nCausal Language Models (CLM)\nCausal Language Models (CLMs) are trained with an autoregressive objective, which is a fancy way of saying they\u2019re trained to predict the next token in a sequence based solely on the previous tokens.\nCLMs typically work in an unidirectional manner, meaning the next token depends only on the previous tokens. It\u2019s a bit like reading a book \u2013 you don\u2019t know what\u2019s coming next until you\u2019ve read what\u2019s come before. Their architecture reflects this, as CLMs are typically decoder-only.\nBecause of their autoregressive nature, CLMs are great for tasks like text (and code) completion, chat, and story writing. Examples of CLMs include Generalized Pretrained Transformer (GPT), and its derivatives, such as Meta\u2019s Llama.\nMasked Language Models (MLM)\nOn the other hand, Masked Language Models (MLMs) are trained to predict masked tokens in a given input by randomly masking certain tokens during training. Its objective task is to \u201cfill in the blanks\u201d given a sentence, but complementary tasks are also used, like predicting which token has been replaced.\nUnlike CLMs, MLMs typically use a bidirectional architecture, meaning they use the context on both sides of a word. This gives them a broader perspective and generally leads to a better understanding of the relationships between words.\n\nDifferences between attention direction. BERT uses a bi-directional Transformer. OpenAI GPT uses a unidirectional left-to-right Transformer.\nMLMs are particularly suitable for tasks like text classification, sentiment analysis, and text tagging. Semantic search is driven by LLMs, where the mathematical distance between document embeddings us used as distance. However, they don\u2019t add much value for incremental token prediction tasks because of their bidirectional nature. Nor can they fill in an arbitrary amount of words.\n\nComparison between architectures of influential models from different modelling methods. (from left to right) BERT is an MLM, Original Transformer is a Seq2Seq model, and LLaMA is a CLM.\nBERT (Bidirectional Encoder Representations from Transformers) model is highly effective in document embedding. Both BERT and ELMo (Embeddings from Language Models) have been instrumental in advancing the field of natural language processing and continue to be widely used in a variety of applications.\nSequence-to-Sequence Models (Seq2Seq)\nThe Sequence-to-Sequence models (Seq2Seq) aim to transform an input sequence (source) into a new one (target), and both sequences can be of arbitrary lengths. Intuitively, it works like translating a sentence from one language to another \u2013 the input and output sentences don\u2019t have to be the same length, but they do relate to one another.\nSeq2Seq models are typically composed of an encoder-decoder architecture, which can be based on Transformers or Recursive Neural Networks (RNNs). The encoder processes the input sequence and compresses it into a latent representation, and the decoder then generates the output sequence from this representation. It is a common sentiment that RNN-based models, while being more expensive (and poorly parallelizable)32, are better than transformer-only models. Thus, various works such as RWKV try to combine the best of both worlds to create hybrid models.\nThese models can generally generate coherent, much larger output based on input, making them suitable for tasks like summarization, translation, and question answering.\n\nVisualization of encoding and decoding flow of an Seq2Seq model.\nA popular example of a Seq2Seq model is T5 (Text-to-Text Transfer Transformer) which during training frames all NLP tasks (such as translation, classification, summarization, and more) into text-to-text problems. Doing so, allows it to learn patterns useful for a variety of tasks. Another popular example is BART (Bidirectional and Auto-Regressive Transformers) which is pre-trained by corrupting text and forcing it to reconstruct the original, which improves its text comprehension. These models have shown impressive results on a wide range of tasks, with only a fraction of parameters they can outperform CLMs on various tasks.\n\nTransformations for corrupting (noising) the text during pre-training of BART (Lewis et al., 2019).\nThe Current State-of-the-Art\nIn the current landscape of large language models (LLM), transformer-based architectures largely steal the limelight. If you are already wondering what\u2019s working under the hood, we\u2019re planning on taking a deeper dive into their components in the next blog.\nThe ever-growing families of models and variants for architectures like GPT or BERT would cause anyone a headache to keep up. Besides the model architecture and modeling methods we have the label of foundational models to help us further organize our taxonomy. The foundational models are the Swiss army knives of AI models. Unlike conventional AI systems, they are trained broadly to be adapted to a variety of tasks with minimal labeled data. The core idea is that if you need more performance at a specialized task, you can start fine-tuning from a solid basis and not from scratch.\nThere are now many commercial and open-source options available for Causal Language Models (CLMs). Notable commercial CLMs include OpenAI\u2019s GPT, Google\u2019s PaLM and Anthropic\u2019s Claude. GPT-4 is a particularly impressive model, with an ensemble of 8 models, each with 220 billion weights. It amounts to an effective size of 1.7 trillion parameters while providing reasonable latency.\nOn the open-source side, Meta\u2019s Meta\u2019s LLaMA and Mistral have gained significant popularity. LLaMA models are available in a range of sizes, from 7 to 70 billion weights. This gives companies the flexibility to choose the model that best fits their needs or to fine-tune it themselves. The community has also developed many tools and optimizations to facilitate running LLaMA.\nWhen picking your model, one should always consider the use case and the amount of effort you are willing to spend on it. There are various benchmarks such as Huggingface\u2019s Open LLM Leaderboard and Massive Text Embedding Benchmark (MTEB) Leaderboard that evaluate both open source and commercial models performance an various tasks.\nFor open source models, however, it\u2019s worth noting that both LLaMA and Mistral models are trained on English text corpus, potentially impacting their performance on tasks in languages other than English.\nAddressing Large Language Models Limitations\nAs exciting as Large Language Models (LLMs) may be, they\u2019re not a one-size-fits-all solution. Just like you wouldn\u2019t use a hammer to drive a screw, there are many tasks that LLMs are well-suited for, and equally as many that they aren\u2019t. Let\u2019s take a look at some limitations posed with LLMs and what techniques exist to work around.\nRetrieval Augmented Generation\nA common challenge with LLMs is their ability \u2013 or rather, inability \u2013 to accurately recall things from memory. Despite their impressive capacity, these models don\u2019t actually \u201cknow\u201d anything. They generate text based on patterns they\u2019ve learned during training, which can sometimes lead to them making stuff up (a phenomenon referred to as \u201challucination\u201d).\nTo counteract this, we can use techniques like Retrieval Augmented Generation (RAG). This approach involves retrieving documents related to a given prompt and feeding them into the LLM to provide the correct context for answering the question.\nThis retrieval process can be done through semantic or vector searches, and the exciting part is, that it can be applied to your custom data as well as external systems like Google Search, essentially giving the LLM a searchable \u201cknowledge base\u201d to draw from.\n\nRetrieval Augmented Generation workflow. Image from AWS Sagemaker Docs.\nExternal Systems and Multimodality\nLLMs, despite their sophistication, still fall short when it comes to tasks like performing math calculations or executing code. A model won\u2019t be able to solve complex mathematical equations or compile and run a piece of Python code without some external help.\nThis is where \u201cTools\u201d come to the rescue, it is another key concept related to LLMs. This involves connecting the LLM to external programs by exposing their API interface within the input context. LLM can call these tools to perform the specialized tags by writing API calls which are executed as part of the generation process.\nA prime example of this concept in action is ChatGPT plugins, which enhances the capabilities of ChatGPT by allowing it to reach out to a suite of community-made plugins. Similarly, Langchain is a more developer-focused platform, that creates API abstractions and pre-built blocks to incorporate this functionality into your application.\nExtending the reach of LLMs even further is the integration of multiple modalities, such as vision and audio. These components convert inputs like images or sound into latent representations, a universal language that our LLM understands.\nCLIP, a breakthrough technology from OpenAI, revolutionized the way we bridge the gap between text and images. Similarly, GPT-4V(ision) and Large Language and Vision Assistant (LLaVA) expand the capabilities of LLMs to comprehend and reason over images.\nChat and Agents\nWe have all become familiar with LLMs through user-friendly interfaces like ChatGPT. Traditionally, LLMs provide a single answer as a completion to the input provided. However, various variants are fine-tuned or use prompt engineering to respond in a chat format, allowing for these interactive conversations.\nTo address the limitation of LLMs being limited to single-turn conversations, AI agents are designed as systems consisting of multiple LLM agents, each instructed with their specific task. These agents communicate with each other over a chat interface, moderated by AI. By working together, these LLM agents form a collaborative machine that can work towards completing a certain task.\n\nAn example of a conversation flow between a python code execution agent, progamming agent and the user (From AutoGen).\nThis concept is explored in-depth in the article Introduction to Autonomous Agents in AI. This collaborative approach has been implemented in projects like ChatDev, which in true spirit of Conway\u2019s law models agents as a company designed to tackle specific tasks, and Autogen by Microsoft, which provides developer tools to create your agent-based applications.\nYour Own Tasks\nThere may be instances where you find that LLMs are not producing satisfactory results for your specific task. However, there are several strategies you can employ to address this.\nOne simple trick can be to rephrase your task. The choice of phrasing has a significant influence on how the model responds. Similarly, applying prompt engineering techniques, like few-shot prompting by providing some examples, can prove useful, giving the model hints about what kind of output you\u2019re hoping for.\nYou can also experiment with different completion regimens such as introducing human-in-the-loop agents. This approach mixes AI-generated completions with human guidance to ensure the outputs align with your expectations.\nIn Conclusion\nWe\u2019ve covered a lot of ground in this blog series on large language models. By now, you should have a solid grasp of the key concepts underlying LLMs \u2013 their inputs, how to apply them for different tasks, and their capabilities.\nI hope you\u2019ve found this exploration illuminating. If you\u2019re eager to go deeper into any of the concepts we\u2019ve discussed, I\u2019ve included some additional resources below. Feel free to check those out while I work on the next installments.\nIf you have any other questions as you continue your LLM journey, don\u2019t hesitate to reach out. I\u2019m always happy to help explain concepts or provide guidance on applying LLMs in business contexts. Whether you need help building an LLM pipeline from scratch, measuring impact and ROI, scaling them up for production, or determining the best use cases for your needs, I\u2019m here. LLMs are powerful tools, but it takes thoughtful implementation to unlock their full potential.\nResources\n\nStart building RAG systems on AWS:\n\nQuestion Answering with your own data, LLMs and Java: Meet Langchain4j \u2013 Luminis\nImprove LLM responses in RAG use cases by interacting with the user | AWS Machine Learning Blog\n\n\nStart building multimodal applications:\n\nSearching through images using the CLIP model \u2013 Luminis\n\n\nAwesome LLM Tools:\n\nLangChain\nGitHub \u2013 microsoft/semantic-kernel: Integrate cutting-edge LLM technology quickly and easily into your apps\n\n\nStart Building Autonomous LLM Applications:\n\nGitHub \u2013 OpenBMB/ChatDev: Create Customized Software using Natural Language Idea (through LLM-powered Multi-Agent Collaboration)\nGitHub \u2013 microsoft/autogen: Building LLM Agent Applications\n\n\nWork on your prompts: Prompt Engineering Guide\n\n", "tags": [], "categories": ["Blog", "Machine learning &amp; AI"]}
{"post_id": 39509, "title": "The Power of a Strategy Map: Aligning Business and Development", "url": "https://www.luminis.eu/blog-en/the-power-of-a-strategy-map-aligning-business-and-development/", "updated_at": "2024-01-22T13:11:21", "body": "In today\u2019s fast-paced business environment, it is essential for organizations to ensure that a product team\u2019s initiatives align with overall business goals. But connecting product strategy to organization strategy is not as straightforward as it sounds. More than often, there is a gap between \u201cthe business\u201d and \u201cdevelopment\u201d. Symptoms include endless prioritization meetings, a focus on features instead of value, and half-baked projects that end up gathering dust.\nIn this blog, I show how the Strategy Map and the Product Roadmap bridge gaps and achieve alignment. I set out the basics and explain what four benefits these powerful tools provide.\n\nThe Strategy Map\nA Strategy Map is a structured representation of an organization\u2019s strategy. It outlines the key objectives that are critical to achieve success and the metrics to monitor it. A Strategy Map helps organizations orchestrate and communicate the strategy to all stakeholders, aligning all parties to work towards the same goals. It includes the key drivers of performance, how they are interrelated and contribute to the goals. This literally is \u201cthe big picture\u201d that makes it easier to identify potential blind spots, trade-offs, and conflicts between different parts of the strategy.\n\nA fictitious Strategy Map with linked goals in 4 perspectives to support overarching mission, vision and value proposition.\nBalanced Scorecard framework\nYou might already know the Balanced Scorecard framework (BSC), as it has been one of the most influential approaches in analyzing and reporting business performance of the past two decades (Kaplan and Norton, 1992, 1993). BSC identifies four generic perspectives: growth and development (later: learning and growth); internal business processes; the customer; and financial. The Strategy Map (Kaplan & Norton, 2001; 2004) is a concept extending BSC with cause-effect relations between these perspectives. As learning and growth is developed within a company, upward links are made to the internal (business process) perspective. Business processes are in turn linked to customers who, ultimately, influence the financial perspective of the company (Kaplan & Norton, 1996: 31). The relations between the different perspectives add logic and coherence to the strategic goals. This makes the Strategy Map helpful in prioritizing and planning product initiatives on the Product Roadmap.\n\n\u00a0\nThe example above shows a single value creation chain taken from a fictitious Strategy Map. The goal in the Financial Perspective is to \u201cincrease value\u201d. This is achieved by \u201cexcellent customer experience\u201d, which is a listed goal in the Customer Perspective. One way to reach that goal is described in the Internal Perspective, by \u201cContinuous delivery\u201d, as it enables continuous improvement of the customer experience. And lastly, achieving that goal requires a \u201cMove to the Cloud\u201d in the Learning and Growth Perspective. In this example, it\u2019s wise to hold any large product initiatives aimed at improving customer experience, and instead aim resources on moving to the Cloud and implementing CI/CD first.\n\nMoney, money, money\nThe \u201cformal\u201d definition of The Strategy Map framework supports the notion that success is ultimately expressed in financial results, like shareholder value. And, to achieve this in a sustainable way, it needs to be achieved through customer loyalty. While this may be true for most businesses, money is not always the (only) motivation behind organizations. For schools, public services, NGOs and other non-profit organizations, Strategy Maps can be used with other perspectives to model the value creation chain.\n\nThe Product Roadmap\nThe Product Roadmap is a plan that outlines how the development of a product contributes to the organization\u2019s strategy over time. It details the specific initiatives, projects and activities needed to achieve the desired outcome: moving the metrics (I\u2019ll get to those in a minute) towards achieving the strategic goals. When Roadmaps stem from the organization\u2019s strategy, they help ensure that everyone in the organization is on the same page and working towards the same goals. This becomes increasingly important for larger businesses, where aligning multiple teams, products, and departments becomes a challenge.\n\nRoadmap or Backlog?\nA Product Roadmap is different from the product backlog, but they are related. The product backlog is a detailed, ever-evolving list of tasks and features that guides the day-to-day development work. It supports the execution of the Product Roadmap\u2019s strategic vision.\n\nFocus on the outcome\nVolatile environments require plans to be updated frequently. However, a Product Roadmap that\u2019s constantly changing does not instill confidence and can break a product team\u2019s morale. Therefore, it\u2019s better to make the Product Roadmap about value (outcomes) instead of features (output). The desired outcomes are directly tied to the organization\u2019s strategy and goals, and more consistent than a list of most wanted features at any given time. So, instead of planning \u201cadd welcome message for new customers\u201d, better plan the initiative \u201cimprove customer onboarding experience\u201d and measure if the team\u2019s efforts contribute to the desired outcome.\nChoosing metrics\nMetrics are used to clarify what goals mean, to set the level of ambition, and to measure progress. Initiatives on a Product Roadmap are aimed at moving these metrics to contribute to the strategic goals. A goal like \u201cExcellent experience\u201d could mean many things, so it is important to decide what it means to your product, customer, and organization, and express that in metrics. For example, you can use the Net Promotor Score to measure customer experience. You can also choose to look at engagement and conversion rates instead. Choosing what metrics to use is important, because these are the metrics that initiatives on the Product Roadmap are aimed at to get moving. There is a lot more to say about metrics, so I will spend another blog on that topic later. For now, the most important thing to know is that metrics are the connection between the initiatives on the Product Roadmap and the goals on the Strategy Map.\n\nMetrics specify the exact meaning and ambition of a goal. To contribute to the strategy, product initiatives should be aimed at moving these same metrics.\nBenefits of Strategy Maps and Product Roadmaps\nStrategy Maps and Product Roadmaps help ensure that product initiatives are aligned with both product and business goals. But like any tool, they only help if understood and used correctly. And even then, they won\u2019t make decisions by themselves. They do help product teams (or any other team) to make choices aligned with organization strategy. Following are the four main benefits of working with Strategy Maps and Product Roadmaps.\nBenefit #1: Catalyst for making clear choices\nCreating and maintaining a Strategy Map and one or multiple Product Roadmaps forces the organization to make fundamental choices. If there is no clarity and agreement on the overall business goals yet, drawing a Strategy Map makes issues tangible and negotiable. Issues need to be resolved before communicating the strategy, to prevent initiatives that head in a wrong direction. The Strategy Map can (and will) evolve over time. However, too many changes too often break people\u2019s trust and willingness to align their efforts with the strategy. Additionally, a Product Roadmap that is just a to-do list with no clear connection to the strategy doesn\u2019t help to achieve business goals. With a confident strategy and a clear map, product teams (or any other team) understand what is needed and are able to make choices aimed at moving the right metrics.\nBenefit #2: A common understanding\nBy using a Strategy Map and Product Roadmaps, organizations provide greater clarity and improve communication about their product initiatives. It encourages everyone in the organization to understand the goals they are working towards. It also helps achieve a common understanding of how initiatives relate to the organization\u2019s overall business goals. This clarity and communication help get buy-in. More importantly, it is essential to ensure that everyone stays aligned and keeps working towards the same objectives.\nBenefit #3: Track progress and measure success\nA goal represents an ambition: something that the organization wants to achieve. To know when you achieved a goal, each goal should have at least one metric to express the ambition. Sometimes defining metrics for goals is not straightforward. For example, consider the business goal \u201cimprove onboarding\u201d. How do you know when you improved this? And how much has it improved? It is not always hard science. It might take asking the user how they experienced the onboarding prior to and after making changes. Or a combination of indicative metrics, like activation and engagement rates. All initiatives on the Product Roadmap need to move one or more of these metrics for strategic justification. This ensures that all product initiatives are aligned with the organization\u2019s business goals, making it possible to track progress and measure success.\nBenefit #4: Prioritization of initiatives\nA Strategy Map helps prioritize product initiatives by identifying the most critical objectives, as well as necessary initiatives to reach the organization\u2019s overall goals. By focusing on the most critical initiatives first, the organization ensures that it is making the most effective use of its resources. A Product Roadmap helps scheduling those initiatives over time and in more detail. This helps ensure that the most important product initiatives have the highest priority, reducing the risk of wasting resources on less critical initiatives.\nStart mapping today\nSo, if you are fed up with endless prioritization meetings about features instead of value, get your team together and start drawing your Strategy Map. As soon as you have your goals and metrics in place, focus the initiatives on your Product Roadmap on these metrics, starting with the highest priority goals. You will see that Strategy Maps and Product Roadmaps are powerful tools to help you align your product initiatives with your overall business goals, prioritize your product initiatives correctly, and allocate your resources effectively. Would you like expert help? Luminis gets you started and can facilitate the strategic alignment process for your organization. Just let me know if you\u2019re interested!\n", "tags": [], "categories": ["Blog", "Strategy &amp; Innovation"]}
{"post_id": 39220, "title": "An Epic Software Estimation: giving continuous insight in large projects", "url": "https://www.luminis.eu/blog-en/an-epic-software-estimation-how-to-give-continuous-insight-in-large-projects/", "updated_at": "2023-11-15T10:21:22", "body": "In this blog post, we explore the invaluable practice of continuous insight into delivery capacity, and how it can be a game-changer when it comes to keeping your stakeholders happy. We delve into the why, the how, and offer practical tips to help you and your team master the art of software or project estimation. So, let\u2019s embark on a journey to unlock the secrets of effective project estimation and the impact it can have on your organization\u2019s success.\nIntroduction\nA few months ago, a product owner in our team knocked on my door. He was anxious about an upcoming meeting where he, other product owners, and some C-level stakeholders discuss our roadmap\u2019s current status. In these meetings, having project durations is crucial, and he needed to provide a rough estimate for our project\u2019s duration.\nRecalling that a colleague and dear friend had developed a template for tracking epic durations based on estimations, I reassured our product owner: \u201cLet me assist you with that. I\u2019ll call for a team meeting next week, and by the end of it, I can give you an estimation how long this project will take.\u201d PO: \u201cThat would be awesome.\u201d\nWell\u2026 time to figure out how to actually do this.\nWhy software estimations?\nWhat is an epic? An epic is essentially a feature that is too large to be contained in a single user story. Therefore an epic is cut-up into multiple, more manageable, user stories. The term epic is often also used for grouping many user stories. This is because tools like JIRA refer to an epic as a bunch of tasks, user stories, etc. Just know, in this blog post, I use epics to describe a project that is so big, it takes multiple sprints to complete.\nEnough about definitions. What brings an estimation to us? In the words of Steve McConnell: \u201cAn estimation gives us the ability to commit to a delivery and control the project to meet its targets\u201d. This quote comes from \u201cSoftware Estimation: Demystifying the Black Art\u201d, a book that has greatly impacted this blog and my view on software estimation. In this quote, \u201ccontrol\u201d can be read as the decisions we make based on events that have an impact on the project\u2019s duration and quality. In the context of this blog, estimation revolves around how long it will take to deliver a certain part of functionality. The ability to address the question \u201cWhat do we need to finish this project in the time we would like it to take?\u201d gives a great perspective on the decisions you make.\nWhile the \u201ctarget\u201d of a project is important for the business, that doesn\u2019t mean it is achievable. The target is a description of a desirable business objective. The \u201ccommitment\u201d is a promise by the team to deliver a set of functionality by a certain date. The commitment can be more aggressive or less aggressive than the estimate. An estimate merely helps you get control over a project. It can help you make decisions about the path of the project, and help you get a clear view of how certain events impact your project.\nDon\u2019t be afraid to estimate epics\nMore often than not, we shy away from estimating projects with a large scope. The uncertainty that comes with a large project makes it hard to estimate precisely. This is a wrong way of viewing estimations. As mentioned before, we don\u2019t want a precise calculation of how much a project costs. We want insights so that we can exert control over a project. An estimation is something you work on during the entire duration of the project. A feedback loop in which you keep track of your estimation gives great insight into your project. It gives control over the project because you can see the impact of decisions you make.\nHow to make a software estimation?\nWhen working on a project with a defined set of requirements, I think the best way to start estimating epics is by using relative estimations. Quite possibly, it is not the first epic your team works on (and hopefully not the last!!). That\u2019s great! That gives us a reference point to what the new epic can relate to. So say, for example, you want to estimate epic C after already completing epic A & B.\n\nA common way to start estimating is by relating to the size of the epic to a T-shirt size. This would look something like this:\nSimple for A & B, because they are already done. Epic C can be estimated relative to A and B. This process is pretty straightforward. Knowing that C is larger than B and somewhat smaller than A, we can classify C as a size L.\n\nThe estimation gets even more useful when we look back on how much story points it cost us to complete A and B. Say it was 150 points for A and 50 for B. That gives us a range to roughly calculate the size of epic C. For some projects, roughly knowing the size of your epic and the time you need to spend on it is enough. In that case, you can stop the project estimation at this point.\nAs mentioned before, we are not measuring the exact size of the epic. We are merely estimating how long it takes to complete. When you know the velocity of the team and the rough boundaries of the estimation, you have enough information to make a software estimation. We can communicate the size of the epic with stakeholders and see the impact on the duration of the project based on the decisions we make.\nFrom a rough to a specific software estimation\nHowever, I don\u2019t think you should stop there. Because after meeting with the project team once more, you can assign specific story points to epic C, based on previous experiences of epics A and B, and possible complexity.\n\nThis is the best scenario! Because now, it\u2019s even possible to track the progress of the epic, based on the initial estimation. After completing or assigning story points to a user story, you can subtract those points from the 110 we estimated it would cost us to complete epic C. This is helpful for a number of reasons:\nFirst off, it helps the team focus. If your main goal is to complete epic C, we should see a steady decrease in the amount of points left in the epic. If this is not the case, it is a great moment to ask yourself, the team, and perhaps the product owner: \u201cWhat\u2019s going on? What is preventing us from completing this epic?\u201d So, it gives us a great tool to control the project.\nEstimating an epic is also super helpful for timely communication to a stakeholder. Say, after a few months\u2019 work, epic C is left with 40 points. But the team knows there is still much more work to be done. This information is fed through the feedback loop of adjusting your estimation based on new knowledge, which means you can communicate this new estimation to the stakeholders, who can choose to assert control or not. People tend to show greater understanding when you inform them in advance that a feature will take longer, instead of waiting until the expected delivery date.\nSo, you get a lot by taking a bit more time to estimate your epic and continuously communicating about it.\nTips\nOne of the tools I like is the use of milestones. Sometimes it is hard for your team to estimate a whole epic. Even if you have relative epics to relate to. I like to cut the epic into milestones and estimate those individually. In our project, we work towards a minimal viable product (mvp) which is milestone 1. After completing that, we will integrate it; milestone 2, and so on. It gives the team a better overview of the work they need to do.\nAnother tip is to try not to get too fixed on the estimation or story points you initially gave your project. Projects change, whether it\u2019s features that come up halfway through the project or some complexity you didn\u2019t foresee. Just make sure you review the estimation from time to time. When you feel like the work it takes to complete the project doesn\u2019t align with the plan, estimate again and let stakeholders know.\nOur project\nBack to the beginning of this blog post: the promise I made to my product owner. We had a team meeting, and within an hour or so we had a decent idea of what our estimation for this project was. The milestones gave the product owner a tool to communicate the work that needed to be done and what our estimation was based on, giving them more control over the project. So far, we are still on course for our project, but last sprint we saw we needed to include some more stories relating to the epic to keep on track. It has truly helped tracking our progress and adjust the planning when needed.\nSo, give it a try. Call for a team meeting and estimate your epic! If you have any questions, please feel free to reach out to me.\n", "tags": [], "categories": ["Blog", "Strategy &amp; Innovation"]}
{"post_id": 39103, "title": "AWS Multi-Account GitOps Deployment 1: Organizational Setup", "url": "https://www.luminis.eu/blog-en/aws-multi-account-gitops-deployment-organizational-setup/", "updated_at": "2024-02-08T12:32:20", "body": "In the world of AWS, scaling your infrastructure often means the need for multiple AWS accounts to segregate resources, manage access and centralize billing. But how do you organize these accounts efficiently? Let\u2019s set up an AWS multi-account organizational structure, and leverage the GitOps methodology for deployments within this blog series.\nSetting the Stage\nIn the first installment of this three-part series, we walk through the foundational setup necessary to deploy a multi-account GitOps strategy on the AWS Cloud. Leveraging the AWS Cloud Development Kit (CDK), we create an organizational setup to harness GitOps deployment to the fullest across different AWS accounts.\nPrerequisites:\n\nAWS Account with appropriate permissions, preferably a fresh one.\nAWS CLI v2, AWS CDK, projen and TypeScript.\n\nStep 1: Account Preparation\nTo roll out our CDK stack for organizational structuring, we start with enabling the IAM Identity Center (an evolution from AWS Single Sign-On). This simplifies access, whether it\u2019s through the command line or console. Given the prevailing trend of centralized user, group, and permission management, IAM Identity Center offers AWS customers a cohesive solution. While it has proprietary user and permission management tools, it also smoothly integrates with popular external solutions, like Microsoft Active Directory. For our tutorial, we\u2019re sticking to the straightforward Identity Center Directory by AWS.\n\nAfter logging into your AWS account as the root user (always make sure to enable mfa in the IAM console for the root user), navigate to the IAM Identity Center page for your preferred region. Throughout our series, we\u2019re going with Ireland (eu-west-1).\nOn this page, select Enable and Create AWS organization. This not only grants access via the AWS access portal, but also establishes an AWS Organization.\nFollowing, I suggest personalizing your AWS Access portal URL and assigning a subdomain within awsapps. For instance, I\u2019ve set mine to https://lutku.awsapps.com/start.\nNow that we have set up the Identity Center Directory and the AWS Organizations, we will create our first user to access the console and the command line. Go to Users within the IAM Identity Center and click Add user.\nOnce you\u2019ve entered the essential user details and chosen your login credentials, navigate to the Permission sets page and Create permission set. Here, we\u2019ll create Administrator Access permissions to assign to our user for the management account. Don\u2019t forget to save the credentials necessary to login to the AWS access portal!\nSelect the AdministratorAccess policy under the predefined permission set and hit next. Create a name for it and preferably extend the session duration to 12 hours, though this should be met with the policies of your organization, and click next. Review the details and create the permission set.\nAfter creating the user and the permission set, we will assign the permission set for the management account, so that we can access the management account using the AWS access portal. Go to AWS Accounts and hit Assign users or groups after selecting the management account:\nGo to the Users tab and select the user created as in and hit next:\nSelect the AdministratorAccess permission set created, and click next. After reviewing, submit the request and assign the permission set to the user.\nFollowing these configurations, you are able to use the AWS Access portal to access your management account. Use your personalized URL, and you should be greeted with a login prompt.\nLogin using the credentials and you see the following page:\n\nStep 2: Infrastructure as Code Preparation\nFor scalability and ease of maintenance of your AWS multi-Account organizational structure, we\u2019ll employ AWS CDK, a favorite among AWS users. It aids in deploying AWS resources using your preferred programming language. To generate our CDK application, we\u2019ll use projen, which helps manage our CDK configuration. My colleague has a comprehensive series that delves deeper into its benefits.\n\nInitialize a new repository:\n\nmkdir organisational-setup && cd organisational-setup\r\ngit init\r\nnpx projen new awscdk-app-ts\n\nUpdate the .projenrc.ts configuration file:\n\nimport { awscdk } from 'projen';\r\n\r\nconst project = new awscdk.AwsCdkTypeScriptApp({\r\n  authorEmail: 'utku.demir@luminis.eu',\r\n  authorName: 'Utku Demir',\r\n  cdkVersion: '2.96.2',\r\n  defaultReleaseBranch: 'main',\r\n  name: 'organisational-setup',\r\n  github: false,\r\n  projenrcTs: true,\r\n  keywords: [\r\n    'AWS CDK',\r\n    'projen',\r\n    'Typescript',\r\n    'Deployment',\r\n  ],\r\n  gitignore: ['.idea'],\r\n  license: 'MIT',\r\n  licensed: true,\r\n\r\n  deps: ['@pepperize/cdk-organizations'],\r\n});\r\nproject.synth();\r\n\nIt is important to add @pepperize/cdk-organizations to the dependencies as it is the community construct library we will use to generate the AWS Organizations resources.\nAfter updating, generate the project:\nyarn projen\n\nCreate a new file named organization_setup_stack.ts under src to include the necessary configuration for our stack:\n\nimport * as orgs from '@pepperize/cdk-organizations';\r\nimport { Stack, StackProps } from 'aws-cdk-lib';\r\nimport { Construct } from 'constructs';\r\n\r\nexport interface OrganizationSetupStackProps extends StackProps {\r\n  environments: string[];\r\n}\r\n\r\nexport class OrganizationSetupStack extends Stack {\r\n  constructor(scope: Construct, id: string, props: OrganizationSetupStackProps) {\r\n    super(scope, id, props);\r\n\r\n    const organization = new orgs.Organization(this, 'organization', {\r\n      featureSet: orgs.FeatureSet.ALL,\r\n    });\r\n\r\n    const deployment = new orgs.OrganizationalUnit(this, 'deployment', {\r\n      organizationalUnitName: 'Deployment',\r\n      parent: organization.root,\r\n    });\r\n\r\n    new orgs.Account(this, 'deployment-account', {\r\n      accountName: 'DeploymentAccount',\r\n      email: 'utku.demir+deployment@luminis.eu',\r\n      roleName: 'OrganizationAccountAccessRole',\r\n      iamUserAccessToBilling: orgs.IamUserAccessToBilling.ALLOW,\r\n      parent: deployment,\r\n    });\r\n\r\n    props.environments.forEach(appEnvironment => {\r\n      const environmentOrganizationalUnit = new orgs.OrganizationalUnit(this, appEnvironment, {\r\n        organizationalUnitName: appEnvironment,\r\n        parent: organization.root,\r\n      });\r\n\r\n      new orgs.Account(this, `${appEnvironment}-account`, {\r\n        accountName: `${appEnvironment}-account`,\r\n        email: `utku.demir+${appEnvironment}env@luminis.eu`,\r\n        roleName: 'OrganizationAccountAccessRole',\r\n        iamUserAccessToBilling: orgs.IamUserAccessToBilling.DENY,\r\n        parent: environmentOrganizationalUnit,\r\n      });\r\n    });\r\n    organization.enablePolicyType(orgs.PolicyType.SERVICE_CONTROL_POLICY);\r\n  }\r\n}\r\n\nThe above stack, based on the work of Matt Lewis on Setting up a AWS Multi-Account environment, creates organizational units and accounts for application deployment and application hosting environments.\n\nEdit the main.ts under src to include this stack as:\n\nimport { App } from 'aws-cdk-lib';\r\nimport { OrganizationSetupStack } from './organization_setup_stack';\r\n\r\nconst devEnv = {\r\n  account: process.env.CDK_DEFAULT_ACCOUNT,\r\n  region: process.env.CDK_DEFAULT_REGION,\r\n};\r\nconst environments = ['dev', 'test', 'prod'];\r\n\r\nconst app = new App();\r\n\r\nnew OrganizationSetupStack(app, 'organisational-setup-stack', {\r\n  env: devEnv,\r\n  environments: environments,\r\n});\r\n\r\napp.synth();\r\n\nHere we create three accounts for the three environments for our application: dev, test and prod.\nStep 3: Account Bootstrapping and CDK Stack Deployment\nAs the last step, we will configure our AWS CLI, bootstrap the management account and deploy the stacks.\n\nConfigure AWS CLI for SSO:\n\naws configure sso\n\nFollow the on-screen prompts to associate your SSO with your company:\n\n\n\nBootstrap your AWS account to prepare for CDK deployments (don\u2019t forget to replace 123456789012 with your own account id):\n\ncdk bootstrap aws://123456789012/eu-west-1 --profile lutku-management\n\nDeploy the stack:\n\nyarn deploy --all --profile lutku-management\nCongratulations! Now, you should be able to see the basic organizational structure in place in the AWS Accounts page on IAM Identity Center like:\n\nConclusion\nIn this first installment of our three-part series on multi-account GitOps deployment on AWS, we\u2019ve dived deep into setting up an efficient organizational structure. Using AWS\u2019s tools and services, like the AWS Cloud Development Kit (CDK) and IAM Identity Center, we\u2019ve demonstrated how easy it is to set up multiple AWS accounts and create an organizational structure. By the end of this guide, you should have a basic organizational structure in place that paves the way for more advanced GitOps strategies in the subsequent posts. Whether you\u2019re scaling your infrastructure or optimizing access management, the blend of AWS tools and GitOps methodology offers a robust solution. Stay tuned for the next parts, where we\u2019ll delve further into the intricacies of GitOps and multi-account management on AWS.\nRemember, as you embark on this journey, resources and references provided here are just a starting point. The cloud landscape is vast and ever-evolving, so always be open to exploration and learning.\nHappy Cloud Engineering and until next time!\nReferences:\n\u2013 Production Ready CDK Project Structure\n\u2013 GitHub: CDK Organizations\n\u2013 Setting Up a Multi-Account AWS Environment\n", "tags": ["aws cdk", "AWS re:Invent 2021", "CI/CD", "cloud", "infrastructure as code"], "categories": ["Blog", "Cloud"]}
{"post_id": 39058, "title": "A Devoxx of Firsts", "url": "https://www.luminis.eu/blog-en/a-devoxx-of-firsts/", "updated_at": "2023-10-13T11:19:56", "body": "In this blog post, I tell you all about my experience of Devoxx Belgium 2023. A Devoxx of Firsts, where I did a lot of things for the first time. Keep reading to hear all about the workshopI gave, the awesome keynote on imposter syndrome, and the workshops and talks of my colleagues\nFor the last 2 years, I focused a lot on self-improvement. I was chosen to participate in Luminis\u2019 leadership program called Accelerate. Want to know more about Accelerate? We have a great video explaining it here (starring me!).\nDuring Accelerate, I faced my fears of presenting. As a result of facing my fears, I presented at a conference last year, which was J-Fall. Although only a byte (or lightning) talk of 15 minutes, it was exciting and fun. I did more presentations after that, but I didn\u2019t go to any conferences. Until my colleague Jettro\u2019s and my workshop proposal for Devoxx got accepted. And so the Devoxx of Firsts started: my first conference for the year, my first workshop at a conference, my first conference in a foreign country, and my very first Devoxx. For my colleague Niels, Devoxx even was the first ever conference he attended. Hence the title of this blog post, a Devoxx of Firsts. Also, my first time blogging about a conference experience!\nDay 1 (Monday): LangChain 4 Workshop\n\nMy colleague Nico was so kind to drive me and Jettro to Antwerp, in his awesome Subaru. The first talk I attended was a deep dive talk about LangChain, a tool we also use in our workshop. It proved to be a really good introduction talk to our workshop. During the break, I approached the speaker and asked if he could mention our workshop, so the attendees of this deep dive could put their new knowledge into practice right away. Funnily I felt a bit anxious to ask, as this was the first time I approached a speaker at a conference and talked to them. But I did ask, and he was kind to do so.\nRight after that talk was our workshop. Somehow, the mention by the previous speaker gave me a boost. For the first time, I was not anxious about doing something like this. I felt excited instead\u2014the power of overcoming your fears. The workshop itself didn\u2019t go as well as we hoped, at least in our experience. A few people struggled to get their environments up and running. We learned something there.\nAt some point during a presentation part, the power went down and the room went dark (see picture below). It made presenting a bit harder, but we didn\u2019t let that bother us, and we just continued until the lights went back on. We had some good discussions at the end of the session with some exciting people, which made me leave with a positive vibe. We ended up with a 3.6/5 rating, which I think is not bad for my first workshop. After this, we had a nice dinner in the center of Antwerp with our Luminis group, and went to bed afterwards.\n\nDay 2 (Tuesday): Workshopping\nOn the second day of the conference, I started with a workshop given by my colleagues Nico, Niels, and Jettro (see photo). Their workshop was about how to translate your Event Storming outcome into code. I had done some (practice) Event Storming sessions before, but never made it to the next step. We had to work in pairs, ping-pong style. It was a good session where I saw the power of TDD and pair programming combined. I\u2019m going to do this more often.\n\nFollowing their workshop, we all attended a workshop about Roblox & Quarkus. We created a back-end service in Java with Quarkus, deployed it in AWS, and called that back-end from a Roblox game. The service was used to provide questions and to verify the answers. With the right answer, a gate opened within the game. It was fun to do. Even got my first experience with Amazon CodeWhisperer. Not fully convinced yet, but I saw the potential of code assistants. If it improves a bit more, I\u2019ll definitely use it. I filled the rest of the day with visiting booths to gather goodies and a talk about Machine Learning in Java.\nWhen the conference ended, I attended my first speaker dinner. One of the perks you get when presenting at a conference. We sat down at a table in a corner, so we weren\u2019t really able to mingle with other speakers. That was a bit unfortunate, but I\u2019ll definitely try to do that next time!\nA lot of talks on Devoxx were about ML, LLMs, and AI. Even though Devoxx is mainly a Java conference, I already saw a lot of Python code, because a lot of ML/LLM/AI work is done with Python. During one of the talks, my colleague Jettro made a reference to Monty Python. I then concluded that I\u2019d never seen any of the movies. So back at the apartment after the speaker dinner, my colleagues showed me Monty Python and the Holy Grail for the first time. I totally laughed my cheeks off.\nDay 3 (Wednesday): A Day of Keynotes and AI\nWednesday started with three keynotes. The first one was about the history of Devoxx, as this was the 20th edition. After that, there was one about Java 21. This was refreshing because, for the last couple of years, I focused a lot on self-improvement and search technologies, and not so much on new Java features. The last keynote was about embracing imposter syndrome by Dom Hodgson. We all start off as an imposter when we do something, and that\u2019s not a bad thing. It was a lot of fun, and, for me, the best talk of the conference. You can watch it here if you are interested.\nThe rest of my day was full of interesting data and AI talks:\n\nAI Unbounded: Multiplying the Collective Intelligence of Humanity: an interesting talk about the potential of AI as a catalyst for exponential human progress and how it can help us reach our goals.\nUnderstanding Probabilistic Data Structures with 112,092 UFO Sightings: A fun take on UFO sightings, while also explaining probabilistic data structures. They use hashes to give you faster and smaller data structures in exchange for precision. If you\u2019ve got a mountain of data to process, this could be useful.\nHow to Build a GPT4All: Introduction to GPT4All, an open-source software ecosystem that allows anyone to train and deploy\u00a0powerful\u00a0and\u00a0customized\u00a0large language models on\u00a0everyday hardware.\nGenerative AI in practice: Concrete LLM use cases in Java, with the PaLM API: Working with LLMs in Java with the use of the PaLM API, provided by Google Cloud\u2019s Vertex AI services.\n\nTo close off the day, we watched Monty Python\u2019s Life of Brian back at the apartment. Or so I tried. I fell asleep halfway because of exciting but long days we\u2019ve had, so I decided to go to bed.\nDay 4 (Thursday): More AI, and some Java\nWhile I attended a lot of talks on AI already the previous days, I still had some coming. I heard the term MLOps a few times before, but didn\u2019t really know what it meant. So I chose to attend an introduction to MLOps as my first talk of the day. Now I know what it takes to deploy Machine Learning models to production. Talking about ML, my second talk of the day was about lessons learned about ML in Java with a home project. I always like this kind of talks, because it\u2019s all about experimenting and making mistakes.\n\nAt the same time as the ML in Java session, my colleague Peter also presented (photo on the left). He talked about the QUIC protocol. Although I didn\u2019t attend it, I watched it later online and want to give him a shout-out on how well he did. You can watch his talk here.\nAfter that, finally, the talk I had been waiting for. A talk about LangChain4j. A Java version of LangChain. In preparation for our workshop, I spent a lot of time experimenting with this framework because we didn\u2019t want to limit our attendees to Python. I also wrote a blog post about it. So I was curious about what this talk would bring me. It was\nan informative and fun presentation/demo by one of the founders. I vote this as the second-best talk of the conference for me.\nAt the end of this talk, I approached the speaker and for the first time, I complemented a speaker in person on their performance. We had a short conversation with the dev-team and they made me keen on contributing to the project. If I will be able to help them, that would be my very first contribution to an open-source project. So let\u2019s see what I can do!\n\nThe day continued with more talks:\n\nSemantic Kernal: A Microsoft competitor of LangChain. Interesting content, but a bit of a boring talk. Probably a combination of AI tiredness and a less enthusiastic speaker.\nBattle AI coding assistants: AI could be a great help in writing better, cleaner, and more secure code. So I wanted to see this talk in which one is best. This battle was between Tabnine, GitHub Co-Pilot, and ChatGPT. Too bad it didn\u2019t include Amazon CodeWhisperer and JetBrains AI.\nJava patterns and practices for serverless applications: AI tiredness kicked in, so I chose to go to a talk about using Java in the cloud. And who said Germans have no sense of humor? Here\u2019s one for you.\n\nAnd not specifically on this day, but in the context of A Devoxx of Firsts: Luminis recently acquired a company and throughout Devoxx I met with a few of my new colleagues for the first time. Good to see we have new enthusiastic colleagues.\nThe End: A Devoxx of Firsts\nOne benefit of organizing a conference in a movie theater is that you can also watch a movie. Every year on the Thursday evening, all Devoxx attendees are invited to watch a new movie together. So my conference experience ended by watching the movie \u201cThe Creator\u201d. It had a very fitting topic: AI. And since this was a new movie, I saw it for the first time ;).\nAll Devoxx presentations can be viewed on the Devoxx YouTube channel. The workshops, however, are not recorded.\n", "tags": [], "categories": ["Blog", "Working at Luminis"]}
{"post_id": 39029, "title": "Omscholen tot Software Developer: Rick\u2019s verhaal", "url": "https://www.luminis.eu/blog-en/omscholen-tot-software-developer-ricks-verhaal/", "updated_at": "2024-01-23T16:04:55", "body": "Jezelf omscholen naar een nieuw vakgebied is een reis die niet altijd even makkelijk is, maar nu ik er op terugkijk is het de beste keuze geweest in mijn carrie\u0300re. In dit blog wil ik graag mijn persoonlijke reis met jullie delen, de uitdagingen die ik ben tegen gekomen en de waardevolle lessen die ik onderweg heb geleerd tijdens mijn omscholing tot Software Developer.\nDe vonk\nZo\u2019n reis begin je natuurlijk niet zomaar. In mijn geval was het de groeiende fascinatie voor technologie. Ik was werkzaam als Marketeer, maar luister in mijn vrije tijd voornamelijk podcasts over de laatste technologische ontwikkelingen. Ik was in mijn dagelijkse werk veel bezig met branding en online marketing campagnes, maar ik houd me liever bezig met het manipuleren van data. Zelfs Excel bestanden automatiseren met VBA geeft mij meer voldoening dan het verzinnen van marketingcampagnes. Wat ging hier mis? Waarom kost het doen van mijn werk zoveel energie? Dat hoort er een beetje bij toch? De leuke dingen vinden plaats in het weekend\u2026 Of niet?\nDe verkenning\nEn toen begon het te knagen. Is dit wel de baan die het beste bij mij past? Is dit wat ik de komende jaren nog wil gaan doen als werk? En het belangrijkste, word ik hier blij van? Eigenlijk kon ik al die vragen redelijk snel met een overtuigende nee beantwoorden. Maar ja, waar begin je dan? Mijn eerste stap was met mensen praten die werkzaam zijn in de IT branche. Ik vertelde hen dat ik dacht aan een overstap, maar dat ik mij afvroeg of dit wel realistisch is op mijn leeftijd, toen 33 lentes jong. Romantiseer ik het misschien teveel? En waar in hemelsnaam begin je zo\u2019n omscholing?\n\nDe sprong\nNa een aantal gesprekken met Software Developers uit mijn kenniskring werd snel duidelijk dat omscholing grofweg in twee opties opgedeeld kan worden. Een eerste optie is om bij een bedrijf dat omscholingen verzorgt aan te kloppen en je aan te melden voor een omscholing, soms ook wel bootcamps genoemd. Er zijn hier namelijk genoeg van. Echter voelde het voor mij niet als het juiste pad. Na zo een training/bootcamp moet je jezelf namelijk terug gaan verdienen en op zich is dat niet erg, en zelfs logisch, het vervelende is dat je vaak ook minder keuze krijgt over waar je uiteindelijk geplaatst wordt/komt te werken. Dus besloot ik te gaan voor optie 2: omscholen via zelfstudie. Wellicht een lastigere route, maar wel eentje waar ik zelf in controle blijf. Een van de kennissen die ik had gesproken tijdens mijn verkenning wees mij op een gratis online cursus \u2018Harvard University: CS50\u2019s Introduction to Computer Science\u2019. Gelijk een goede peiler om te checken of Software Development wel een goede match is. Let\u2019s go!\nDe learning curve\nEn een match was het zeker. De cursus was een perfecte voorbode dat ik de eerste stap had gezet in de goede richting. Basisprincipes van computer science worden behandeld en je krijgt te maken met programmeertalen als C, Python en Java. Gelijk een goed moment om af te tasten welke taal mij het meeste aanspreekt. Na de cursus met veel plezier afgerond te hebben, had ik besloten mij te focussen op Java. Nu was het zaak vlieguren te gaan maken. In een jaar tijd ben ik mijn Github gaan vullen met hobbyprojecten, heb ik mijn Oracle Certified Associate en Professional gehaald en ben ik een priv\u00e9project gestart met twee vrienden (een backend en frontend developer) om zo vertrouwd te raken met het scrum framework.\nDe uitdagingen\nDeze route klinkt nu alsof ik geen obstakels ben tegen gekomen, maar die waren er natuurlijk wel. Ook al besteed ik met veel plezier mijn uren aan IT/Software Development, die uren moeten wel ergens vandaan komen. In eerste instantie probeerde ik zoveel mogelijk in de avonduren te studeren. Echter loop je hier tegen de uitdaging aan dat je na een werkdag soms best moe thuis komt en dan vraagt het veel energie om nieuwe dingen te leren, zeker in een nieuw vakgebied. Daarom had ik besloten een dag minder te gaan werken om zo naast de avonduren ook een volle werkdag erbij te kunnen pakken. Vaak sneuvelde er in het weekend ook wel een dag aan het studeren, maar het weekend probeerde ik toch zoveel mogelijk te ontzien om tijd met mijn vrouw en kind te kunnen besteden. Deed ik dit niet, dan is mijn zoon (7 jaar) niet de beroerdste om mij erop te wijzen dat het weekend is. Mijn vrouw trouwens ook niet. Uiteindelijk resulteerde dit in grofweg 10 tot 15 uur per week die ik spendeerde aan mijn omscholing.\nDe gesprekken\nToen ik mij zeker genoeg voelde begon ik met solliciteren en mijn profiel bij recruiters neer te leggen. Eerlijk gezegd was ik nog niet zeker genoeg of ik de baan volledig in zou kunnen vullen, maar ik was zo gemotiveerd dat het voor mij de vraag was \u2018wanneer\u2019 ik Software Developer zou worden en niet \u2018of\u2019. En die vastberadenheid sloeg over toen ik in gesprek kwam met Luminis. De toenmalige chef zag het wel zitten, hij voelde namelijk wat ik zelf ook lang voelde: passie voor het vak. En natuurlijk had hij door dat ik wat betreft Software Development nog nat achter de oren was. Maar met een gezond stel hersenen, levens- en werkervaring uit mijn voorgaande banen en een goede dosis enthousiasme durfde hij het wel aan en hebben we elkaar de hand geschud. Sindsdien ben ik met veel plezier werkzaam bij Luminis. De match had in mijn ogen niet beter kunnen zijn. Ik ben gedreven mij te blijven ontwikkelen en Luminis faciliteert dit op verschillende vlakken. Zo is iedereen binnen het bedrijf enorm toegankelijk om vragen aan te stellen, worden er met grote regelmaat sessie gegeven waar collega\u2019s hun kennis delen of een nieuwe techniek willen proberen en zijn er verschillende interne trainingen om jezelf te ontwikkelen.\nEn nu?\nMijn reis van Marketing naar Software Developer heeft mij veranderd op verschillende vlakken. Het heeft mij geleerd dat met toewijding, discipline en de bereidheid te veranderen het mogelijk is om een succesvolle carri\u00e8reswitch te maken. De weg kan uitdagend en intimiderend zijn, de beloningen aan het einde zijn immens.\nZit jij op je plek? Geeft jouw werk je nog energie? Twijfel je ook een carri\u00e8reswitch te maken naar Software Developer? Mijn advies zou zijn, ga op onderzoek uit. Wacht niet langer en ontdek of het een match is. Want als dat zo is, dan staat er een prachtige reis op je te wachten.\nVoel je vooral vrij om contact te zoeken als je vragen hebt of als je gedachten wilt uitwisselen, via LinkedIn of rick.deruiter@luminis.eu.\n", "tags": [], "categories": ["Blog", "Working at Luminis"]}
{"post_id": 39003, "title": "Eleven Software Development and Opserve become part of Luminis", "url": "https://www.luminis.eu/blog-en/eleven-and-opserve-become-part-of-luminis/", "updated_at": "2023-09-26T11:16:23", "body": "With due pride, Luminis announces today that Eleven Software Development and Opserve from Rijswijk become part of IT consulting company Luminis. Joining Luminis offers Eleven and Opserve the opportunity to further shape their growth ambitions.\nApeldoorn, September 26th, 2023 \u2013 During the first meeting between Luminis and the Rijswijk companies, it was immediately apparent that all parties have a number of important things in common. Values such as craftsmanship and collaboration with clients are important parts of their DNA. Within the organizations Eleven and Opserve, and Luminis, there is lots of room for knowledge sharing and learning from each other, and the offices form a nice home base for all colleagues.\nEleven has been realizing high-quality software for its customers since 2009, developing long-term relationships with them. Opserve provides management and monitoring of Linux servers and arranges migration of applications to the Cloud. Luminis was founded in 2002 and helps organizations innovate with smart solutions in the field of Cloud and data.\nDon Olsthoorn, Commercial Director Eleven and Opserve:\n\u201cWith this collaboration, we gain access to more knowledge and expertise and the synergy between the companies allows us to increase our commercial strength. We\u2019ll be able to respond even better to challenges in our client projects and our broadened perspective enables us to accelerate innovation for our customers.\u201d\nEleven customers are supported by fixed teams that proactively provide value through professionalism and customer knowledge. Not only does Eleven develop software for its customers, it also takes care of server management and migrations of applications to the Cloud through sister organization Opserve. Luminis offers customers a very similar service through Luminis Cloud Services (LCS) and will now be able to strengthen these services thanks to the capabilities of Opserve.\nRonald Voets, Managing Director Luminis:\n\u201cEleven and Opserve are great companies that, like Luminis, strive to add value for its customers as quickly as possible. The management team of Eleven and Opserve was looking to strengthen their organization to enable further growth. There is plenty of room for this within Luminis and together we will realize those growth ambitions.\u201d\nEleven and Opserve employees will continue to work for their customers from their current office in Rijswijk. This collaboration provides an opportunity to analyze the customer base of both Luminis and Eleven and Opserve in order to share the expertise and knowledge of the organizations with current and new customers.\nLuminis is part of Yuma, a new player in the field of digital transformation. Yuma combines a people-centric approach with a hands-on mentality and best-in-class expertise. Since Eleven and Opserve are now part of Luminis, they\u2019re also part of the bigger Yuma group.\nPascal Laffineur, CEO Yuma:\n\u201cEleven and Opserve, through Luminis, will strengthen Yuma\u2019s ambition and strategy to provide best-in-class expertise in digital transformations. By establishing long-term relationships with their clients in fixed teams, they are a great addition to the companies within the Yuma group.\u201d\nEleven and Opserve Board members decided to join Luminis\nAbout Eleven and Opserve\nEleven develops high-quality software and builds long-term relationships with their clients. Since 2009, Eleven realized 500+ projects with 200+ customers, such as DELTA Fiber Nederland, Koninklijke Metaalunie, Royal FloraHolland, D\u00fcmmen Orange, Stigas, Noviflora and Syngenta, and they have grown to a team of 23 professionals. Opserve is growing strong in serving Eleven customers and other companies in Linux server management, and is active as an AWS Consulting Partner to help developers migrate applications to AWS Cloud. www.eleven.nl, www.opserve.nl\nAbout Luminis\nLuminis helps organizations innovate successfully. The world of technology is constantly changing, and the complexity and speed of this change continues to increase. The organization has 150 employees, has offices in Amsterdam, Rotterdam, Arnhem and Apeldoorn, and provides its services to, for example, Alliander, Huuskes, BDR Thermea, bol.com and The Learning Network.\nAbout Yuma\nYuma is a new group in BeNeLux that puts people first in digital transformations. This best-in-class expertise is provided by the combination of companies that make up the group: XPLUS, Total Design, Luminis and BPSOLUTIONS. More companies are expected to join the group. Yuma expects a turnover of about 100 million euros this year and is working with 400 employees spread across the BeNeLux. www.weareyuma.com\n", "tags": [], "categories": ["Blog", "News"]}
{"post_id": 38757, "title": "Question Answering with your own data, LLMs and Java: meet Langchain4j", "url": "https://www.luminis.eu/blog-en/search-en/question-answering-with-your-own-data-llms-and-java-meet-langchain4j/", "updated_at": "2023-09-12T16:33:58", "body": "\nPython has become the de facto programming language when working on data related tasks. I\u2019ve recently started exploring the world of Machine Learning (ML), Large Language Models (LLMs) and vector databases. See my previous blog post about using LLMs and Generative AI to improve search.\nThis is also when I started ramping up my Python programming skills, because most companies and organizations release very neat Python client libraries for their products and services. But sometimes you want or need to use Java. In this blog post, I will explain how you can easily create a question answering system with Java and Langchain4j.\nAt the time of writing this blog post, I am doing research and preparations for conference talks and workshops with my colleague Jettro. These talks and workshops are all about creating question answering systems with the combination of LLMs and semantic (vector) search. We use a powerful tool there called\u00a0LangChain. This tool makes it very easy to connect all the pieces together. Jettro has written a blog about using that tool with their official client, which is a Python library. While the main programming language in our talks and workshops is also Python, we would also like to give the attendees that are not comfortable with Python the opportunity to work with Java. Meet Langchain4j, a Java port of LangChain. Although not yet as feature rich as the original, it already provides sufficient features to be used. I will show how easy it is to work with.\nEmbedding Model\nFirst, you want to start by defining your embedding model. This model is used to covert your text into embeddings. Embeddings are mathematical representations of your text, to be able to do calculations on them. Like calculating the similarity of pieces of text. If you want to know more about this, see this blogpost of Jettro, or watch out for the upcoming part 2 in my \u201cUsing Machine Learning to Improve Search\u201d blog series.\nLangchain4j supports multiple clients for embedding models, like OpenAI, HuggingFace or even local in process models. These are all really simple to initiate. For instance, the OpenAI version looks like this:\n...\r\n    @Qualifier(\"openaiEmbeddingModel\")\r\n    @Bean\r\n    public EmbeddingModel openaiEmbeddingModel() {\r\n        return OpenAiEmbeddingModel.builder()\r\n                .apiKey(\"your-key\")\r\n                .modelName(TEXT_EMBEDDING_ADA_002)\r\n                .build();\r\n    }\r\n...\nAnd the local in process one looks like this:\n...\r\n    @Qualifier(\"inMemoryModel\")\r\n    @Bean\r\n    public EmbeddingModel inMemoryEmbeddingModel() {\r\n        return new InProcessEmbeddingModel(ALL_MINILM_L6_V2);\r\n    }\r\n...\nThat EmbeddingModel interface holds a couple of easy to use methods you can use to convert text to embeddings. You\u2019ll see that later when we\u2019re going to create an embedding for our question.\nEmbedding Store\nAfter creating the embeddings, they need to be stored in an embedding store. This could be an in memory embeddings store, but Langchain4j also supports a few vector databases, like Weaviate and PineCone. Just like the embedding models, the setup for the stores is also very easy. This is what it looks like for an in memory store:\n...\r\n    @Qualifier(\"inMemoryEmbeddingStore\")\r\n    @Bean\r\n    public EmbeddingStore inMemoryEmbeddingStore() {\r\n        return new InMemoryEmbeddingStore<>();\r\n    }\r\n...\nBut if you want to go with Weaviate, for example, it\u2019s not complex either:\n...\r\n    @Qualifier(\"weaviateEmbeddingStore\")\r\n    @Bean\r\n    public EmbeddingStore weaviateEmbeddingStore() {\r\n        return WeaviateEmbeddingStore.builder()\r\n                .apiKey(\"your-key\")\r\n                .scheme(\"https\")\r\n                .host(\"your.weaviate.host\")\r\n                .build();\r\n    }\r\n...\nThe WeaviateEmbeddingStore builder has a few more methods which you can use, you can explore those in the example section.\nData Ingestion\nWhen you have your embedding model and store ready, you want to ingest your data in the embedding store. This can be done with an EmbeddingStoreIngestor:\n...\r\n    Document document = Document.from(\"text\");\r\n    DocumentSplitter documentSplitter = DocumentSplitters.recursive(300);\r\n    EmbeddingStoreIngestor ingestor = EmbeddingStoreIngestor.builder()\r\n            .documentSplitter(documentSplitter)\r\n            .embeddingModel(embeddingModel)\r\n            .embeddingStore(embeddingStore)\r\n            .build();\r\n    ingestor.ingest(document);\r\n...\nIn this example, I created a Document object from the string \u201ctext\u201d, but in reality you would probably have some larger text there. Langchain4j includes some parsers for PDF or DocX (MS Word) and some other types of files. These parsers also output a Document object which can be used to ingest into the store.\nA DocumentSplitter is needed to cut your long text into smaller chunks (300 characters in this case). The Python LangChain library also supports overlap in chunks, but Langchain4j doesn\u2019t support that (yet). Cutting your text into chunks is an important step in vector search, because embeddings of larger chunks tend to be less accurate. Also, sending large texts results in higher token counts,\u00a0which will increase the cost of your language model if you use OpenAI for example. You should experiment with this thoroughly until you find the best chunk size for your data and use case.\nChat Language Model\nNow we have an embedding model and a store with data in it. Next thing is to set up a chat language model to convert our search results into an actual answer to the question asked. For this part, we need an LLM. Langchain4j currently supports clients for OpenAI and HuggingFace. Here is an example for OpenAI:\n...\r\n    @Qualifier(\"openaiChatLanguageModel\")\r\n    @Bean\r\n    public ChatLanguageModel openaiChatLanguageModel() {\r\n        return OpenAiChatModel.builder()\r\n                .apiKey(\"your-key\")\r\n                .modelName(GPT_3_5_TURBO)\r\n                .temperature(0.8)\r\n                .timeout(ofSeconds(15))\r\n                .maxRetries(3)\r\n                .logResponses(true)\r\n                .logRequests(true)\r\n                .build();\r\n    }\r\n...\nHere you can set timeout, retries, choose what model you want to use and set the sampling temperature. The sampling temperature is a parameter for language models that governs the randomness/creativity of the responses of the model. This should be a value between 0 and 1. Higher values like 0.8 will make the output more random. Lower values like 0.2 will make it more focused and deterministic, meaning you almost always get the same response to a given prompt.\nGetting Answers\nWhen all of the above is done, it\u2019s time to put all the pieces together to query for relevant texts, send those together with the question to the ChatLanguageModel and let it write an answer to your question from the provided information.\n...\r\n    public String askQuestion(String question) {\r\n        Embedding queryEmbedding = embeddingModel.embed(question);\r\n        List<EmbeddingMatch> relevant = embeddingStore.findRelevant(queryEmbedding, 4, 0.8);\r\n\r\n        Map<String, Object> variables = new HashMap<>();\r\n        variables.put(\"question\", question);\r\n        variables.put(\"information\", relevant.stream().map(match -> match.embedded().text()).collect(Collectors.joining(\"\\n\\n\")));\r\n\r\n        PrompTemplate promptTemplate = PromptTemplate.from(\r\n                \"Answer the following question to the best of your abilities: \\\"{{question}}\\\"\\n\\n\" +\r\n                        \"Base your answer on the following information:\\n{{information}}\");\r\n        Prompt prompt = promptTemplate.apply(variables);\r\n\r\n        LOGGER.info(\"Sending following prompt to LLM:\\n{}\", prompt.text());\r\n\r\n        AiMessage aiMessage = chatLanguageModel.sendUserMessage(prompt.toUserMessage());\r\n        return aiMessage.text();\r\n    }\r\n...\nFirst, we use the EmbeddingModel to create an embedding for the question string, so it can be used to find semantically relevant pieces of text from the EmbeddingStore. The two other parameters in the findRelevant method are respectively the max amount of results we want to get back (4) and the minimal score the results should have (0.8). For these values you should experiment and find the sweet spot that works best for your use case and data.\nThe next step is to create a Map of variables which holds the question and the relevant texts (information). This map is then applied to the PromptTemplate where you specify the text (or prompt) that goes to the ChatLanguageModel. Prompt engineering, the art of writing instructions for your language model, is a very important step to get the answers you want in the format you want. I would really advise to spend time on writing the best prompt for your use case and data. Deeplearning.ai has a great free course on this.\nLast but not least we send the prompt to the ChatLanguageModel and get back an answer to your question from the information you provide yourself. This could also mean your data does not contain anything related to your question and the answer is something like \u201cI could not answer this question\u201d, depending on your settings and prompt.\nFinal Words\nAs you could see it doesn\u2019t really take much coding to create an awesome question answering system in Java with the use of Langchain4j. In my GitHub repository you can find all the above code with a couple REST endpoints. One to ingest a PDF created from the Devoxx Belgium conference FAQ page. Another one to ask questions which will use the information of that FAQ page as source. A question like \u201cWhat is the address of the venue?\u201d will result in something like \u201cThe address of the venue is Groenendaallaan 394, 2030 Antwerp, Belgium.\u201d, which is very cool.\nKeep in mind that for a production ready system you need to do a lot of investigation and experimentation. You need to choose or train the right embedding model, pick the ideal chunk size for your data, find the most fitting settings, like temperature for ChatLanguageModel and minimum score for matches. To see if your system is performing well you need to have a set of questions available that could be asked by users and verify if the generated answers are similar to what you expect them to be.\nThat being said, with the information in this post you can already start experimenting!\n", "tags": ["LangChain", "Large Language Models", "search", "weaviate"], "categories": ["Blog", "Search"]}
{"post_id": 38642, "title": "Vector Databases Unveiled: The Heart of Modern Data Processing", "url": "https://www.luminis.eu/blog-en/vector-databases/", "updated_at": "2023-09-18T13:38:19", "body": "In this blog post, we dive deeper into vector databases. and why they are a hot topic. Data can be represented in many ways. Take an analog calendar as an example, you can see all the days stored on lines that represent the weeks, and those weeks are packed on a month. Group 12 of these packs and you have a full calendar. Add some cat pics, update the data, and you can sell it every January. This is a silly example, but I hope you get the idea behind it.\nWe have stored data since the writing was invented. First writings were not epic poems, but just a list of objects in a warehouse or shiploads. Since the 60s, we have used databases to store and manage data. It is more efficient, more consistent, more secure, more scalable, etc. That said, the way the data is stored on these databases will directly affect the benefits offered. Continuing with the first example, calendars are efficient because they are faster and consistent. It is pragmatic to daily life to know that today is the 7th of September, and not the 250th of 2023.\nAs time passed, the needs and the benefits you can get from databases changed and expanded. Let\u2019s take relational databases as an example. If you store \u201cMath 101\u201d on the table subjects and create a relation with the table subjects, you are able to get all the students enlisted in the course. This \u201crelation\u201d creates a feeling of belonging, something that is not there, is not real, but helps us create meaning of the data, and organize it with a purpose. It helps us recreate the model we have in our minds.\nIntroducing Vector Databases\nThis year, vector databases have been a hot topic. We discuss later why and which specific problems they solve, but at this moment, the important thing is to understand what they store, the vectors.\nVectors are mathematical objects that represent direction and magnitude in space. Us humans have problems if we try to imagine more than 3 dimensions, but computers don\u2019t have that problem, in fact, vectors are highly versatile and almost everything can be represented as a vector.\nLet\u2019s say that you have an image and you want to store it. One way to do this is to split the image into pixels and assign a number based on a scale. By doing this, we will have another way of representing the data: instead of an image, we have a high-dimensional vector.\nImage via pinecone.io\nSo, as you probably already imagined, vector databases are databases made for storing vectors. But weren\u2019t vectors just numbers? Why can we not just store it on an SQL database?\nWell, the short answer is that you can, and (even better news) there are databases you already know that support vectors, like OpenSearch (via \u00a0k-NN), PostgreSQL (via PGVector) or ElasticSearch (via Knn search). That said, you will probably lose all the benefits of using dedicated vector databases.\nVector databases unveiled: the benefits\nImagine you have a basket with fruits. You pick an orange, and you need to choose another fruit similar to the orange. Will you pick an apple? Probably not. Maybe a lemon? Maybe, both are acid, and the skin is similar. Vector databases can help you with this task.\nSemantic search allows you to get values that are more related to others depending on the context. If relational databases establish a relation between the different tables, which creates a sense of belonging, the vector database does the same for similarity. The examples in this picture can help you understand this concept.\nImage via developers.google.com\nSimilarity search is one of the strongest points vector databases have. It\u2019s one of the reasons why databases are a hot topic right now: it allows you to store text (as a vector) and based on this use LLMs like ChatGPT or LLaMA to ask questions that can be answered based on a simulated context or just retrieve similar documents. Some databases that get especially good results are Weaviate, Pinecone and DeepLake. On the other hand, if we want a fast implementation with Python, you could use Chroma.\nThe same logic that applies to similarity can be used to get recommendation engines. These engines provide custom suggestions or recommendations to users based on their preferences, behavior, or characteristics, which in this case will be stored as vectors. Weaviate (they explain how on this post) is a database especially good on this, but other mentioned databases can also fit on this.\nAnother useful case for vector databases is real-time geospatial search and analytics. This involves the instantaneous querying, retrieval, and analysis of location-based data. This is particularly useful in applications where time-sensitive information related to geographic locations needs to be processed and visualized quickly. Optimize routes, tracking vehicles, emergency responses, etc. are some real problems that can benefit from vector databases, and the best option for this scenario is Qdrant.\nConclusion\nThis blog pretends to be an introduction to vector databases. Understanding their fundamentals and some use cases is more important than just using them, because everybody does. That said, we strongly encourage you to use them when working with LLMs, as it is the standard right now, but make sure you pick the right option before you start developing.\nIf you check out the top tending repos on github, you will see that most of them are trying to create an AGI (Artificial General Intelligence), like Auto-GPT, babyagi or jarvis (Microsoft). These tools make use of LLMs and vector databases because they allow them to create long-term memory, storing the prompts as vectors and using these prompts themselves to generate even more content with their own context.\n", "tags": [], "categories": ["Blog", "Machine learning &amp; AI"]}
{"post_id": 38716, "title": "Decoding Similarity Search with FAISS: A Practical Approach", "url": "https://www.luminis.eu/blog-en/search-en/decoding-similarity-search-with-faiss-a-practical-approach/", "updated_at": "2023-08-29T13:30:09", "body": "I am preparing for a series of conference talks and workshops on Retrieval Augmented Generation. With my strong information retrieval (search) background, I have been working on vector-based technologies. I found some excellent resources on using FAISS from James Briggs.\nIn this blog post, you read about FAISS and your options for implementing similarity search using FAISS. I decided to use the Python Notebook approach to enable you to tag along.\nSemantic Search\nBefore we dive into FAISS, let us spend some time on Semantic Search. What is Semantic Search? I assume you are used to search engines on websites and most likely Google. Most of the site search engines use search engines like Elasticsearch, OpenSearch, and Solr. Most websites may not even be. A lot of sites still use SQL queries to perform searches. Most of these engines use some form of Lexical search or keyword matching. As a user, you must type precisely the terms known in the data. More advanced engines use mechanisms to overcome typos and synonyms. These engines need help understanding what you are talking about. They need to get the semantic meaning of the words you use in the query.\nWe have all been trained by using these websites only to enter a few words in a search bar. If you, as a company, are lucky, your users will try other words when you do not present them with the right results. Suppose you better understand the semantic meaning of the user and match concepts rather than keywords. In that case, you can present much better results, positively impacting your user experience and business outcome.\nWe need a way to match concepts, but how do we determine this meaning or similarity between concepts in pieces of text? One answer is transforming text into mathematical representations, typically vectors. Once our data is represented as vectors in a multi-dimensional space, finding similar items becomes a matter of calculating distances between these vectors. Here\u2019s where algorithms like k-nearest neighbors (kNN) come into play.\nSimilarity search using\u00a0kNN\nCalculation is more straightforward with numbers than with text. The moment we started talking about calculating distances, we stepped over many concepts. First, we need to transform text into a vector of numbers. Second, we need to calculate distances between these vectors of numbers. We discuss converting text into numbers in the section about embeddings.\nFor now, imagine we have the vectors. In the image below, we simplified them into a two-dimensional space. However, in reality, this is a lot higher. The vectors we work with have 1536 dimensions. More about that later in the embeddings section.\nkNN diagram for BrickHeadz titles\nThere are different formulas to calculate the similarity between vectors. Examples are l2, cosine similarity, and inner product.\nTo get an intuition on the distance metrics, below you get an idea for calculating the similarity between the vector for voetbal and Frodo & Gollem.\nCalculate L2 distance for two vectors, the query voetbal and frodo\nNow, we can compare two vectors and calculate how similar they are. We can use brute force and exact calculations to find the most similar vectors. This is okay if you have less than 100k documents. Approximate Nearest Neighbours is available for speed. It will be less accurate. You trade in accuracy for speed. For most models, you can tune this balance. We discuss ANN, HNSW, IVF in another section.\nThe next sections give more background information on the topics discussed here. Together with the Notebook, these sections give you a fair understanding of working with vector similarity.\nEmbeddings\nDid you look at the Notebook? In one block, we create a method called create_embedding. The method uses the OpenAI API. Through this API, we use the model text-embedding-ada-002. This model accepts text and outputs a vector of dimension 1536.\nTraining such a model is easier than training Large Language Models. However, taking an existing embedding model is usually easier with a larger corpus of texts. Training your custom model can improve results if you have a corpus with many non-familiar words. Techniques like word2vec that train on guessing missing words or next words are well documented. I like this tutorial from Tensorflow.\nDistance Metrics\nThe similarity between two vectors is calculated using a distance metric. You can choose different metrics. L2 is a good first choice. Similarity is used a lot as well. The inner product is another good choice. I am not going into all the details. Choosing the best model is a matter of testing them.\nL2 or Euclidean distance is also called the straight line distance between two vectors. Values must be normalized. The metric performs poorly if the dimensions increase to tens of thousands.\nCosine similarity is a good second choice that only measures the angle between vectors. It does not consider the lengths of vectors.\nThe inner product, or dot product, is a specialization of the cosine similarity. Faiss uses this next to L2 as a standard metric. IP performs better on higher-dimension vectors.\nThe Weaviate documentation has a nice overview of distance metrics.\nFAISS Indexes\nThe beginning of this blog post shows how to work with the Flat index of FAISS. This index is called exhaustive, and the results are exact. They are exact, as we determine the distance for all available vectors to the query\u2019s vector. You can imagine that the performance for such a query becomes a problem with too many vectors. If speed is important, we need another index. Meet the approximate indexes. These indexes are not exact anymore, but they are faster. They have a way of limiting the scope of the search to perform. Now, the trick is to find the best balance between performance and quality of results.\nHNSW\u200a-\u200aHierarchical Navigable Small\u00a0World\nAn approach that uses a graph to find the vector that best matches the query vector. The graph vertices have several friends. The graph traverses the friend closest to our query if it is closer to our query than the current vector. Combining multiple graphs that become more accurate per layer creates a hierarchy of graphs. The highest layer is the entry point. When we reach the closest distance, we go to the connected lower layer connected to the vertex in the top layer.\nWe can configure the amount of friends each vertex has. More friends mean higher accuracy, but lower performance. Imagine you are searching for a kitchen. You ask five companies for an offer and choose the lowest. If you ask 100 companies for their price, you can get a better price. It will take a lot of effort, though.\nYou can read more about HNSW and Faiss in this post from Pinecone.\nIVF\u200a-\u200aInverted File System or\u00a0Index\nIVF is an interesting clustering technique to select a subset of the vectors to calculate the best matches from. It uses a Voronoi diagram to select cells with vectors to consider. The idea is to have several centroids in cells with vectors closer to that centroid than any other centroid. Next, we find the most similar centroid to the query and select that cell to find the most similar vectors.\nSometimes, we miss a similar vector right at a cell\u2019s border. This hurts the accuracy of the index. Therefore, we can select multiple cells to limit the chance of missing a good vector. That does increase the latency of the index. So again, we have to find the right balance.\nFinal words\nWorking with vector stores like Weaviate, Amazon OpenSearch Service, and Pinecone gives you much power. Without the knowledge of the technology supporting these data stores, you cannot create the best-performing solution. You must understand the trade-off between performance and accuracy\u200a-\u200a the difference between exhaustive similarity calculation and exact results with approximate similarity.\nTherefore, when working with one of those vector stores, read the documentation for the right parameters to find the optimal balance of accuracy and performance for your specific application.\nReferences\nPinecone has a nice series of posts bundled in Faiss: The Missing Manual.\nJames Briggs has created some incredible YouTube videos. His series about similarity search accompanies the posts from the link above.\nNice overview of vector search with Weaviate that explains most concepts in the Weaviate context.\nExtensive documentation for the kNN plugin in OpenSearch.\n", "tags": ["faiss", "knn", "relevance", "search", "similarity"], "categories": ["Blog", "Search"]}
{"post_id": 38593, "title": "Hands-On with Observability: Installing Elasticsearch and Kibana", "url": "https://www.luminis.eu/blog-en/hands-on-with-observability-installing-elasticsearch-and-kibana/", "updated_at": "2024-04-30T14:03:20", "body": "Following our previous discussion on the fundamentals of Elastic Stack and observability while using Elastic Stack, we\u2019re set to take our exploration to the next level. In this chapter, we shift our focus from theory to practice, diving deep into the setup and utilization of Elasticsearch and Kibana via Docker.\nBefore we begin, ensure you have Docker installed and running on your machine. If not, you can download it here.\nDiving into Observability: The Docker-Facilitated Elasticsearch and Kibana Setup\nAs we embark on this journey to enhance application observability with Elasticsearch, our first task involves creating an Elasticsearch cluster tailored to store and process our specific data. This is generally achieved through a series of steps, including the installation of Elasticsearch, the configuration of indices and mappings, as well as the establishment of data ingestion pipelines to collect and manipulate data from a variety of sources. Today, however, our focus is centered on the initial setup \u2014 the installation of Elasticsearch and Kibana facilitated by Docker.\nDeploying Elasticsearch Using Docker CLI\nYour journey into application observability with Elasticsearch starts with setting up an Elasticsearch cluster. To do this, pull the Docker image with the following command:\ndocker pull docker.elastic.co/elasticsearch/elasticsearch:8.7.0\nNow that you have the image, you can run the following commands to start a single-node Elasticsearch cluster for development:\n# create a new docket network for Elasticsearch and Kibana\r\ndocker network create elastic\r\n\r\n# start Elasticsearch in Docker. Generates credentials --> Save it somewhere!\r\ndocker run --name es01 --net elastic -p 9200:9200 -it docker.elastic.co/elasticsearch/elasticsearch:8.7.0\r\n\r\n# copy the security certificate from Docker to local\r\ndocker cp es01:/usr/share/elasticsearch/config/certs/http_ca.crt .\r\n\r\n# open a new terminal and verify that you can connect to your cluster\r\ncurl --cacert http_ca.crt -u elastic https://localhost:9200\nMake sure you copy the generated password and enrollment token and save them in a secure location. These values are shown only when you start Elasticsearch for the first time. You\u2019ll use these to enroll Kibana with your Elasticsearch cluster and log in.\nOnce the Elasticsearch cluster is set up and configured, developers can use tools like Kibana to create dashboards and charts to visualize the data and identify trends and issues. Kibana is a source-available data visualization dashboard software for Elasticsearch.\nDeploying Kibana Using Docker CLI\nWith the Elasticsearch cluster now set up, we\u2019ll turn our attention to Kibana, a powerful tool for visualizing Elasticsearch data:\n# start Kibana in Docker and connect it to existing Elasticsearch cluster\r\ndocker run --name kib-01 --net elastic -p 5601:5601 docker.elastic.co/kibana/kibana:8.7.0\nThis will start Kibana in docker and will output the next to the terminal when done:\n\nKibana has not been configured.\nGo to\u00a0http://0.0.0.0:5601/?code=579012\u00a0to get started.\n\nWhen you click on the link you will see this screen:\n\nEnter your enrollment token and generated password from before and you will get access to Kibana.\n\nRolling Up Our Sleeves: Diving Into Practical Application\nAs we pivot from theory to practice, our narrative unfolds around two main applications: an E-commerce Website (Application A) and an Inventory Management Service (Application B).\n\nE-commerce Website (Application A): it could include features like user registration, product browsing, adding to cart, and purchase transactions.\nInventory Management Service (Application B): This could be a back-end microservice that manages the inventory for the e-commerce website. It could have features like adding new stock, updating existing stock, marking stock as expired, etc.\n\n\n\u00a0\nIntroducing Dev Tools: Our Gateway to Observability in Elasticsearch\nEager to start communicating directly with Elasticsearch? You\u2019re in luck! By navigating to the Management \u2192 Dev Tools in Kibana, you\u2019ll gain direct access. This powerful tool enables us to execute HTTP requests against the Elasticsearch REST API, facilitating operations like creating indices, adding documents, and running queries. We\u2019ll now create two indices to accommodate our different logs: \u2018ecommerce_app_logs\u2019 for the E-commerce application and \u2018inventory_management_logs\u2019 for the Inventory Management service.\nDeploying Indices: Laying the Groundwork\nInitiate your journey with index creation using the following commands in Dev Tools:\nPUT /ecommerce_app_logs\r\n{\r\n  \"mappings\": {\r\n    \"properties\": {\r\n      \"application-name\": {\r\n        \"type\": \"text\"\r\n      },\r\n      \"timestamp\": {\r\n        \"type\": \"date\"\r\n      },\r\n      \"log_level\": {\r\n        \"type\": \"keyword\"\r\n      },\r\n      \"message\": {\r\n        \"type\": \"text\"\r\n      }\r\n    }\r\n  }\r\n}\r\n\r\nPUT /inventory_management_logs\r\n{\r\n  \"mappings\": {\r\n    \"properties\": {\r\n      \"application-name\": {\r\n        \"type\": \"text\"\r\n      },\r\n      \"timestamp\": {\r\n        \"type\": \"date\"\r\n      },\r\n      \"log_level\": {\r\n        \"type\": \"keyword\"\r\n      },\r\n      \"message\": {\r\n        \"type\": \"text\"\r\n      }\r\n    }\r\n  }\r\n}\nThese commands generate the necessary environment for data ingestion, defining the properties of each document that will be inserted into these indices.\nHowever, in production environments, it\u2019s often more scalable and maintainable to utilize index templates. Index templates provide a way to automatically set up mappings, settings, and aliases as new indices are created. By adopting index templates, you can ensure that every new index conforms to a pre-defined structure, making your data ingestion process more streamlined and consistent. This not only reduces the risk of manual configuration errors but also simplifies operations when dealing with a multitude of similar indices. If you\u2019re eager to learn about index templates, you can explore this subject further in this blog from Luminis colleague Jettro Coenradie. Don\u2019t worry if you prefer to stay tuned here, as index templates will also be covered later in this series.\nFeeding Data: Populating Our Indices\nNow that we made these indices we can start adding documents/logs to them.\nPOST /ecommerce_app_logs/_doc\r\n{\r\n  \"application-name\": \"ecommerce_app\",\r\n  \"timestamp\": \"2023-07-24T14:00:23\",\r\n  \"log_level\": \"INFO\",\r\n  \"message\": \"User 'john_doe' successfully logged in\"\r\n}\r\n\r\nPOST /inventory_management_logs/_doc\r\n{\r\n  \"application-name\": \"inventory_management\",\r\n  \"timestamp\": \"2023-07-24T10:15:00\",\r\n  \"log_level\": \"INFO\",\r\n  \"message\": \"New shipment of 'Samsung Galaxy S20' arrived. Quantity: 100\"\r\n}\nAs we progress, we\u2019ll explore automated ways of populating these indices, particularly through tools such as Filebeat and Logstash.\nPeeking at Our Data: Exploring the Discover Tab\nCongratulations on taking your first steps toward indexing and data ingestion. To view your logs, head over to the Analytics \u2192 Discover tab in Kibana. All your logs appear here, but should you need to focus on a specific application, simply add a filter. This not only organizes your data but also sets the stage for more advanced data manipulation techniques we\u2019ll delve into in the future.\n\n\nAssessing Our Initial Journey Into Observability\nHow did your journey into observability start? Was your path clear and straightforward, or did you grapple with unexpected obstacles? Either way, remember that every hurdle overcome is a stepping stone toward mastery. We\u2019d love to hear your stories, your triumphs, and even your challenges. So, feel free to share your experiences in the comments below!\nAnd don\u2019t forget: our exploration into the world of Elasticsearch-driven observability is just beginning. In our next post, we\u2019ll expand our knowledge, shifting from the ingestion of data to its management. We\u2019ll delve deeper into managing the index life-cycle and ensuring our data remains organized and accessible, even as it grows. So, stay tuned for more hands-on guidance in our observability series. Together, let\u2019s unlock the power of data and transform how we see our applications.\n", "tags": [], "categories": ["Blog", "Search"]}
{"post_id": 38528, "title": "Data Quality Series, part 3: Overview of Data Lineage", "url": "https://www.luminis.eu/blog-en/data-quality-series-part-3-overview-of-data-lineage/", "updated_at": "2023-08-13T12:24:28", "body": "In this article, we delve into the often overlooked, but crucial aspect of data quality \u2013 data lineage. Data lineage records the flow of data and all the transformations throughout its life-cycle, from source to destination. Understanding this is vital for maintaining data integrity and transparency in data processes, making it an essential component of the data quality workflow.\nWe previously explored the significance of data quality in the blog post, \u201cIntroduction to Data Quality\u201d, which emphasizes the importance of clean and standardized data for accurate analysis and decision-making. If you are interested in getting more hands-on experience with data quality testing, read the second blog post in this series: \u201cData Quality Testing with Deequ in Spark\u201d.\nNow, we take a closer look at data lineage, its benefits, and how it contributes to maintaining data reliability. As a whole, we aim to compile a comprehensive overview of important concepts to guide a user who is considering implementing data lineage within their organization.\nThe rest of the blog is structured as follows:\n\nWhat is Data Lineage?\nRequirements for Effective Data Lineage\nBenefits of Data Lineage\nTools for Data Lineage\n\nWhat is it?\nTraditionally, the data resided in a data warehouse with only a few connections to external systems. Today, as the demand has grown, the data flows between a multitude of systems, teams, and (external) organizations. Consequently, it is easy to overlook the impact of a single change somewhere in the life-cycle of the data.\nData lineage refers to the steps a dataset took to reach its current state. It encompasses the entire life-cycle of data, from its creation or ingestion to its consumption and usage in various processes and applications. By understanding data lineage, organizations gain visibility into how data is transformed and manipulated as it moves through different systems, processes, and transformations. It is an important tool for data engineers to debug potential issues in the data flow processes.\nThere are two primary types of data lineage: table-level lineage and field-level lineage. Table-level lineage provides an overview of the tables or datasets involved in the data flow, whereas field-level lineage goes deeper, tracking the lineage of individual fields or columns within those tables.\nRequirements for Data Lineage\nData lineage works like documentation: when done right, it should not put an additional burden on your development workflow, it should only enhance o. To harness the full potential, there are some general guidelines that should be satisfied, as described in the Data Quality Fundamentals book by Moses, et al.:\n\nFast Time to Value: Abstracting the relationships between data objects down to the field level is crucial for quick remediation. Simply tracking at the table level may be too broad and insufficient for understanding the impact of changes and identifying specific issues. (split point)\nSecure by Design: Data lineage shouldn\u2019t directly access the data. Instead, it should rely on metadata, logs, and queries to gather information about the data flow. This simplifies the design, as well as ensures that no potentially private business data leaks into your documentation.\nAutomation: Manual maintenance becomes increasingly challenging and error-prone as data pipelines become more complex. Investing in an automated data lineage generation approach saves time and reduces the risk of human error.\nIntegration with Popular Data Tools: A data project typically orchestrates data flow between multiple tools. The lineage tracking should seamlessly integrate with these technologies to create a unified view of your business, rather than dictating your workflow.\n\nThe benefits\nImplementing robust data lineage practices offers several benefits to organizations:\n\nCommunication and Transparency: It acts as a communication channel between data producers and data consumers, helping to bridge the gap between different teams by providing a clear understanding of the impact of broken or changed data on downstream consumers.\nImproved Data Quality and Trust: Data lineage allows organizations to build trust in their data assets. By providing visibility into the data\u2019s journey and transformation, it enhances data quality, reliability, and accuracy. This, in turn, promotes better decision-making based on trustworthy information.\nCompliance and Auditability: Data lineage supports compliance efforts by enabling organizations to demonstrate adherence to regulations, such as the General Data Protection Regulation (GDPR). It provides an audit trail of data usage and ensures transparency in data management practices.\n\nSome practical applications of data lineage in use include:\n\nDebugging: When issues arise in data analysis or reporting, data lineage can be invaluable for root cause analysis. By tracing the lineage of problematic data, analysts can identify where the issue originated and take corrective action more efficiently.\nReducing Technical Debt: Data lineage helps identify columns or fields that are no longer in use or have been deprecated. By marking and propagating these changes downstream, organizations can reduce technical debt and streamline their data pipelines.\nGovernance: With privacy regulations and data governance becoming increasingly important, data lineage provides a way to track how personally identifiable information (PII) is used within an organization. It enables organizations to understand who has access to sensitive data, how it is utilized, and ensures compliance with data protection regulations.\n\nTools\nNow, let\u2019s explore some powerful tools that can help you establish and maintain a seamless data lineage process.\nOpenLineage\nOpenLineage is an emerging industry standard for data lineage tracking that is gaining traction. It is supported by the Linux Foundation, Atronomer, Collibra. It aims to establish a unified framework for capturing, managing, and sharing metadata across various tools and platforms. OpenLineage provides a consistent way to represent data lineage, making it easier to integrate with different systems and tools. You can easily incorporate it with any tool by submitting events to its API endpoint.\nOne exciting integration with OpenLineage is the combination with Marquez, a metadata service that tracks data workflows and lineage, open-sourced by WeWork. Together, they offer a simple, yet powerful solution to maintain a comprehensive and standardized view of data lineage. With this integration, you can easily trace data transformations, dependencies, and the origin of data through various data pipelines.\nMicrosoft Purview\nMicrosoft Purview is a comprehensive data governance and data cataloging solution that also offers data lineage capabilities. Purview is part of the Microsoft Azure ecosystem and integrates well with other Azure services. It allows organizations to discover, classify, and understand their data assets, making it easier to implement robust data lineage practices.\nOne notable feature of Purview is its integration with Azure Data Factory (ADF). While ADF provides some level of data lineage tracking through job dependencies, Purview enhances this functionality by offering a more unified and visual representation across the data ecosystem.\n\u00a0\nData Lineage in Microsoft Purview\nDatahub\nDatahub is a versatile data platform that provides robust data lineage capabilities, among other features. It offers extensive integration support, making it suitable for various data environments. While it is open source, the installation is heavy and requires both Kafka and Elasticsearch to operate, making it a tough choice for small projects.\nDatahub can handle large-scale data lineage requirements. Data engineers and data analysts can rely on Datahub to trace data paths, identify data inconsistencies, and ensure data quality across their pipelines, making it a one-stop shop data quality tool.\nDataset Lineage overview in DataHub\nSpline\nIf your organization mainly uses Apache Spark for data processing, Spline is an excellent tool to consider for data lineage tracking. Spline offers the ability to join lineage across multiple datasets, providing a comprehensive view of how data transformations take place.\nOne notable advantage of Spline is its compatibility with OpenLineage (currently as POC). This allows you to leverage OpenLineage\u2019s ecosystem to combine lineage across environments for visualization.\nDataset High Level Data Lineage overview in Spline UI\nDBT (Data Build Tool) and Dagster\nDBT and Dagster are two powerful data tools that emphasize data-first practices and can significantly contribute to your data lineage efforts.\nThe first one mentioned, DBT, is a popular data transformation tool that enables data engineers and analysts to model, transform, and organize data in a structured manner. By leveraging DBT\u2019s features, you can ensure that your data lineage accurately reflects data transformations and helps maintain data integrity.\nOn the other hand, Dagster is a data orchestration tool designed to facilitate the development and management of data workflows. With Dagster, you can build robust data pipelines that capture data lineage effectively, making it easier to identify and resolve issues in your data processes.\nData Graph in Dagster Combining FiveTran, DBT and Tensorflow Assets\nApache Airflow\nApache Airflow is a workflow management platform that, while not a strict data lineage tool, supports data lineage indirectly through its connectors and integrations. By utilizing these connectors, you can associate data pipelines with metadata about the data sources, dependencies, and transformations.\nWhile Airflow\u2019s data lineage capabilities might not be as sophisticated as some dedicated data lineage tools, it can still play a significant role in providing visibility into your data workflows and their impact on downstream processes.\nConclusion\nIn conclusion, data lineage is a vital aspect of data quality, providing transparency in processes and transformations. Building your lineage with best practices in mind, such as automation and the correct level of abstraction, brings a multitude of benefits like improved communication, enhanced data quality, and compliance support.\nPowerful tools are available for establishing and maintaining data lineage, offering unified frameworks for metadata management and comprehensive tracking across workflows.\nEmbracing data lineage and leveraging these tools empowers everyone within the organization to make better decisions, ensure data reliability, and build trust in their data.\n", "tags": ["Data Engineering", "Data Lineage", "Data Quality", "DataOps"], "categories": ["Blog", "Data"]}
{"post_id": 38492, "title": "How to improve Observability using the Elastic Stack", "url": "https://www.luminis.eu/blog-en/how-to-improve-observability-using-the-elastic-stack/", "updated_at": "2024-04-30T14:02:51", "body": "In the fast-paced world of modern software applications, ensuring a smooth and reliable user experience is paramount. Currently, I am working on improving the observability of the applications of a customer, using the Elastic stack. In this blog, I take you along with me on that journey.\nBy improving observability, the customer hopes to identify and resolve application issues more quickly, ensuring a smooth and reliable user experience for their customers. In this blog, you read about what observability is and how we can achieve it by implementing the Elastic Stack. This blog post is part of a series in which we explore a range of use cases, each complemented by an appropriate solution.\nNext in the series on Observability using Elastic Stack:\nThroughout this series, I spotlight various use cases and challenges to demonstrate how you can effectively leverage Elasticsearch within your business for better observability. Key topics that I dive into include:\n\nGetting started with the ELK Stack and Docker\nManaging the index life cycle\nCreating Kibana dashboards\nImplementing alerting systems\nEmploying monitoring techniques with Application Performance Monitoring (APM)\nSecuring Your Elastic Stack: Tips for Optimal Security\nLeveraging Machine Learning Features in the Elastic Stack\n\nObservability: What Does It Mean?\nThe ability to monitor and understand the behavior of a system through the collection and analysis of metrics, logs, and tracing data. That is a general definition of observability. Usually, observability is divided into three pillars. These pillars of observability refer to the three critical aspects of monitoring and understanding the behavior of a system: metrics, logging, and tracing.\n\n\nMetrics are numerical values that describe the performance and behavior of a system. They can be used to monitor the health and capacity of a system, as well as identify trends and patterns over time.\nLogging is the process of recording events and messages generated by a system. Logs can track a system\u2019s behavior, identify issues, and provide context for debugging and troubleshooting.\nTracing is following a request or operation flow through a distributed system. Tracing can help identify bottlenecks and performance issues, as well as provide a complete view of how a system is functioning.\n\nTogether, these three pillars of observability provide a comprehensive understanding of the behavior and performance of a system. They are essential for ensuring the reliability and stability of modern software applications. The Elastic Stack is a powerful tool that can help with application observability by providing a central location for storing, analyzing, and visualizing various data types. Now, let\u2019s dive deeper into understanding this powerful resource.\nThe Elastic Stack: A Closer Look\nThe Elastic Stack, formerly known as the ELK Stack, is a robust suite of open-source software tools designed to take data from any source, in any format. It enables users to search, analyze, and visualize that data in real time. The Elastic Stack is composed of four main components: Elasticsearch, Logstash, Kibana, and Beats.\n\n\nElasticsearch is the heart of the Elastic Stack. It is a distributed, RESTful search and analytics engine capable of handling a wide variety of data types, including textual, numerical, geospatial, structured, and unstructured. You can use Elasticsearch for log and event data storage, but also for full-text search, distributed search, and analytics.\nLogstash is a server-side data processing pipeline that accepts data from multiple sources simultaneously, transforms it, and then sends it to a \u201cstash\u201d like Elasticsearch. It\u2019s extremely useful in gathering logging data and other event data from different sources and provides filters to transform the data.\nKibana is the visualization layer of the Elastic Stack. It allows you to explore your Elasticsearch log data through a web interface, and build dashboards that highlight the relationships between your data over time. Kibana also allows for the management of Elasticsearch indices and the manipulation of the data contained within.\nBeats is a platform for single-purpose data shippers. They install as lightweight agents and send data from hundreds or thousands of machines to Logstash or Elasticsearch. Each Beat is designed for a specific data type, such as system metrics, network metrics, or log files.\n\nTogether, these components provide a flexible, scalable, and effective way to collect, process, store, search, analyze, and visualize data in real time. Therefore, they are invaluable in improving observability within systems.\nNext Up: Hands-On with Elastic\nThe Elastic Stack offers a powerful suite of tools that provide valuable insights into the behavior and performance of your systems, enhancing observability and consequently, improving operational efficiency. But the true power of these tools lies not only in understanding them conceptually, but also in employing them hands-on. So, what\u2019s the next step in our journey? It\u2019s time to roll up our sleeves and get practical. We explore how to implement each component and leverage their combined capabilities for effective system observability. Get ready for an insightful and dynamic journey \u2014 you don\u2019t want to miss it!\nIf you\u2019re an experienced reader not planning to follow this series to the end, we\u2019d love to hear from you as well! How have you harnessed the power of the Elastic Stack to enhance observability in your projects? Share your insights and stories in the comments below. Your experience could be the inspiration or solution someone else is seeking!\n", "tags": [], "categories": ["Blog", "Search"]}
{"post_id": 38432, "title": "Accelerate Graduation: deelnemers vieren afsluiting met een vlucht en feestelijke middag", "url": "https://www.luminis.eu/blog-en/accelerate-graduation-deelnemers-vieren-afsluiting-met-een-vlucht-en-feestelijke-middag/", "updated_at": "2023-07-26T09:12:08", "body": "Op vliegveld Teuge werd vrijdag 7 juli het tweede Accelerate-traject afgesloten met een feestelijk graduation event. Dit opleidingstraject, waarvoor Thales, de Belastingdienst, Bosch en Luminis de handen ineenslaan, is een op maat gesneden traject voor toptalenten van de vier organisaties. De 23 deelnemers rondden het traject van achttien maanden succesvol af en kregen hun Accelerate wings opgespeld.\u00a0\nHet afronden van het programma werd op een bijzondere manier gevierd, de deelnemers stapten namelijk in een Cessna voor een korte vlucht. Vervolgens deelden ze persoonlijke speeches op het podium en namen ze complimenten en hun felbegeerde wings in ontvangst van de stuurgroep en coaches.\n\nBert Ertman, VP Technology bij Luminis en initiatiefnemer van het Accelerate concept, riep de geslaagden op uit hun comfortzone te blijven stappen: \u201cDe sky was de afgelopen 18 maanden de limit, die hebben jullie nu bereikt maar kijk eens hoe veel verder je nog kunt gaan. Blijf in beweging en blijf anderen inspireren.\u201d\nHet meest indrukwekkend zijn de verhalen van deelnemers over de impact van Accelerate op zowel hun carri\u00e8re als priv\u00e9leven. Met name de persoonlijk leiderschap-trainingen van How Company hebben veel teweeggebracht. Meerdere deelnemers spraken over een onomkeerbare groei in hun zelfvertrouwen en eigenwaarde, wat ze niet alleen een betere collega maakt, maar ook een betere ouder, partner of vriend. Of zoals een deelnemer zelf zei: \u201cDankzij het Accelerate-netwerk leer je obstakels overwinnen. De kracht van gemeenschap en de kracht van kwetsbaarheid daarin was duidelijk. Je leert niet alleen van elkaar, maar juist ook met elkaar.\u201d\nTijdens het traject werden de deelnemers begeleid door coaches die de deelnemers waar nodig een duwtje in de rug gaven. Robert en Erik, coaches van Accelerate, lieten zien dat het coach-zijn hen ook veel heeft gebracht: \u201cCoaching is als een spiegel voor je zelfbewustzijn. Wat je uitdraagt, moet je ook zelf toepassen. Dit heeft ervoor gezorgd dat wij nu zelf ook beter weten waar onze krachten liggen en wat onze uitdagingen zijn.\u201d\nAl met al blikken we terug op een succesvolle afsluiting van een bijzonder traject. Achttien maanden lang werkten deelnemers aan hun skill set, overwonnen ze samen talloze uitdagingen en behaalden ze persoonlijke doelstellingen. We kijken uit naar de impact die deze groep Accelerate-deelnemers gaan hebben op onze organisaties, nu in en de toekomst.\n", "tags": [], "categories": ["Blog", "News"]}
{"post_id": 38415, "title": "Searching through images using the CLIP model", "url": "https://www.luminis.eu/blog-en/search-en/searching-through-images-using-the-clip-model/", "updated_at": "2023-07-12T08:53:32", "body": "Screenshot Streamlit app for image search\nA picture says more than a thousand words. But can you use words to find images? I do not mean going through the meta-data of images but searching for the actual image. That is what we are going to see in this blog post. An app that is searching through images using the CLIP model\nI took a few small images from different websites, stored them in a vector store, and sent textual queries to find matches. The screenshot shows the Streamlit app that interacts with the vector store. This blog post shows how many lines of code you need to create your image search.\nHow it works \u2013 the CLIP model\nCLIP \u2013 Contrastive Language\u2013Image Pre-training is a model that originates from OpenAI. The main idea is to add Natural Language Supervision to a model learning from images. The idea is to create one vector space containing text and images. The idea of a vector space is that similar concepts are close to each other in that vector space. The smaller the distance between two concepts in a vector space, the more likely they talk about the same concept. If we have a combined vector store for text and images, we can compare the concept from text with the concept from an image. Creating such a model is also called a multi-modal model. A good starting point to learn more about CLIP is the webpage from OpenAI.\nCLIP is the base for a lot of other models. Huggingface has some excellent models, and a lot of them are free to use. In the next section, you can see how easy it is to use these models with Weaviate, the vector store we use that comes with many extras. The model that I have chosen is sentence-transformers/clip-ViT-B-32-multilingual-v1. I choose this model for its multilingual capabilities. To be honest, the support for the Dutch language is not great. I was surprised by the quality of English requests. The video at the end of this blog gives an idea of the quality.\nRunning the sample\nYou can find the code for this sample in my GitHub repository. You want to look at the file run_weaviate_clip.py. You can check the readme.md for instructions on how to run the sample. You can also configure PyCharm to run Streamlit. The following image shows you how to do that.\npycharm run configuration for a streamlt app\nSetting up Weaviate\nYou can use docker-compose to set up a local running Weaviate instance. The project contains a docker-compose file named docker-compose-weaviate-clip.yml. The file is in the infra folder. You can start docker using the following command. Beware, it does download the complete CLIP model of around 5 Gb. So you need space, internet, and patience. Notice the two services that we deploy, weaviate and the multi2vec-clip module with the specified model.\n---\r\nversion: '3.4'\r\nservices:\r\n  weaviate:\r\n  command:\r\n    - --host\r\n    - 0.0.0.0\r\n    - --port\r\n    - '8080'\r\n    - --scheme\r\n    - HTTP\r\n    image: semitechnologies/weaviate:1.19.11\r\n    ports:\r\n    - 8080:8080\r\n    restart: on-failure:0\r\n    environment:\r\n      CLIP_INFERENCE_API: 'http://multi2vec-clip:8080'\r\n      QUERY_DEFAULTS_LIMIT: 25\r\n      AUTHENTICATION_ANONYMOUS_ACCESS_ENABLED: 'true'\r\n      PERSISTENCE_DATA_PATH: '/var/lib/weaviate'\r\n      DEFAULT_VECTORIZER_MODULE: 'multi2vec-clip'\r\n      ENABLE_MODULES: 'multi2vec-clip'\r\n      CLUSTER_HOSTNAME: 'node1'\r\n    volumes:\r\n    - ./data_clip:/var/lib/weaviate\r\n  multi2vec-clip:\r\n    image: semitechnologies/multi2vec-clip:sentence-transformers-clip-ViT-B-32-multilingual-v1\r\n    environment:\r\n      ENABLE_CUDA: '0'\r\n...\nConfiguring the Schema\nWeaviate can work without a schema, but providing the schema makes it explicit what you want. Within the schema, we also configure the multi2vec-clip module. Below you can find the schema and the code to load the schema.\n{\r\n  \"class\": \"Toys\",\r\n  \"description\": \"A sample to search for toys\",\r\n  \"moduleConfig\": {\r\n    \"multi2vec-clip\": {\r\n      \"imageFields\": [\r\n        \"image\"\r\n      ]\r\n    }\r\n  },\r\n  \"vectorIndexType\": \"hnsw\",\r\n  \"vectorizer\": \"multi2vec-clip\",\r\n  \"properties\": [\r\n    {\r\n      \"dataType\": [\r\n        \"string\"\r\n      ],\r\n      \"name\": \"filename\"\r\n    },\r\n    {\r\n      \"dataType\": [\r\n        \"blob\"\r\n      ],\r\n      \"name\": \"image\"\r\n    }\r\n  ]\r\n}\r\n\ndef load_weaviate_schema(client: WeaviateClient, schema_path: str) -> None:\r\n    if client.does_class_exist(WEAVIATE_CLASS):\r\n        client.delete_class(WEAVIATE_CLASS)\r\n        run_logging.info(\"Removed the existing Weaviate schema.\")\r\n\r\n    client.create_classes(path_to_schema=schema_path)\r\n    run_logging.info(\"New schema loaded for class '%s'.\", WEAVIATE_CLASS)\r\n\nNotice that we throw away the schema if it already exists\nStoring the images\nNext, we store the images that are provided in the data_sources folder. First, we encode the image as a base64 byte array. Next, we decode the bite array into a utf-8 string. The string is added to weaviate in the field image of type datablob.\ndef store_images(client: WeaviateClient) -> None:\r\n    with client.client.batch(batch_size=5) as batch:\r\n        for file_name in os.listdir(IMG_PATH):\r\n            if file_name.endswith(\".jpg\"):\r\n                with open(IMG_PATH + file_name, \"rb\") as img_file:\r\n                    b64_string = base64.b64encode(img_file.read())\r\n\r\n                data_obj = {\"filename\": file_name, \"image\": b64_string.decode('utf-8')}\r\n                batch.add_data_object(data_obj, WEAVIATE_CLASS)\r\n                run_logging.info(\"Stored file: %s\", file_name)\r\n\nQuerying the images\nNow we can import all the images. The Streamlit app has a button to do exactly that. With all the images in the store, we can finally execute queries. Weaviate makes it easy to execute a near_text query. The next code block executes the query and extracts the required data from the response.\ndef query(client: WeaviateClient, query_text: str, the_limit: int = 3):\r\n    run_logging.info(\"Executing the query '%s'\", query_text)\r\n    near_text = {\"concepts\": [query_text]}\r\n\r\n    response = (client.client.query\r\n                   .get(WEAVIATE_CLASS, [\"filename\"])\r\n                   .with_near_text(near_text)\r\n                   .with_limit(the_limit)\r\n                   .with_additional(properties=[\"certainty\", \"distance\"])\r\n                   .do())\r\n    if response[\"data\"][\"Get\"][\"Toys\"]:\r\n        found_picts = response[\"data\"][\"Get\"][\"Toys\"]\r\n        return [{\"filename\": pict[\"filename\"], \"certainty\": pict[\"_additional\"][\"certainty\"]} for pict in found_picts]\r\n\r\n    return []\r\n\r\n\nDemo time\nYou can look at the video below for the demo of searching through images using the CLIP model. Pay attention to the search terms and the results. In the beginning, I highlight the value of certainty. A higher value means the model is more certain it found a good match. I like that you can search for wood and colors. Even those terms return reasonable results.\n\nhttps://www.luminis.eu/wp-content/uploads/2023/07/streamlit-run_weaviate_clip-2023-07-11-12-07-54.webm\nResults that do not look like a match\nYou might have thought there were many bad results if you watched the demo. One thing that is different in a vector search from the well-known index-based search. With index-based search, whether there is a match depends on the terms you are searching for and the terms in the index. With vector search, we look at the closest match. Therefore, you get the six closest results when asking for six results. That does not mean they are always a good match, but they are the closest match we could find.\nYou can use certainty to limit results to a specific score, but it is hard to predict which score works for you. That depends on the model, your data, and your query.\nConcluding\nI hope you like my blog post and you learned something. I also hope you are now convinced that image search has changed dramatically. If you need help, please get in touch with me. I am sure we can figure something out.\n", "tags": [], "categories": ["Blog", "Search"]}
{"post_id": 38404, "title": "Tips on assuring your happiness job wise", "url": "https://www.luminis.eu/blog-en/tips-on-assuring-your-happiness-job-wise/", "updated_at": "2023-07-12T15:57:07", "body": "Have you ever wondered, at any point in your career, if you have made the right decision job wise? It could be that you are not satisfied anymore with your current employer, the customer you are working for, or that you want to trade your job as a lecturer to become a software developer.\nThe most important thing to learn from turning your career upside down is the feeling you will wake up with every day that you have made the right decision. It is your happiness that counts above anything else!\nInspiration\nThe reason I decided to write this blog post is that there has not been much written about this topic. This year, I read a book called \u2018Met je voeten in het stopcontact,\u2019 which translates to \u2018Putting your feet in a power outlet,\u2019 by Gwen van Poorten.\nIn this book, Gwen talks about how she became her own best friend, and created a life she feels excited about. She shows a vulnerable side of herself, talking about having a burnout aged 22 and how she managed to turn her life around and become the best version of herself. Also, the current media occasionally drops a post about this topic, but it is still something that does not get covered enough. Even though I never let it come that far, this book and online posts relate to my blog post. I hope the tips I describe in my post will help you reflect and ensure happiness in your career.\nI have a BSc in Computer Science, but I do not see myself working as a back-end developer, what should I do?\nIt is important to create a work environment that you are pleased with. For example, you get excited from working at a marketing firm as a social media director. In this case, you have created an environment that you want to be in. I did not have that energized feeling even before I started my first job. But what did I want to do after I got my bachelor\u2019s degree in computer science? I had no idea what the possibilities were, only that I had to work as a back-end developer.\nThat first job excitement\nLuckily, a miracle happened. A former colleague asked me to stay at the university I studied at. That removed my first barrier of not having to work as a back-end developer. I did not have to work behind a desk all day and write back-end code. I could become a teacher!\n\nWhen I worked as a lecturer, I really liked the fact that there was no certain schedule or task you had to do for eight hours straight. It was a truly diverse environment where you had to prepare classes, help in labs, or give a lecture to students on a study related topic. I thought it was only possible to have this diversity in tasks for jobs in education or in a similar sector. But it was not until I changed jobs that I found out you can also have that type of diversity in a job as front-end developer.\nHow can I afford life if I radically change my profession?\nAfter three years working at the university, I felt like it was time for a change. I did not feel that same spark I had in the beginning. I still liked teaching, but the atmosphere at the university had changed negatively, which meant I did not want to stay there. On top of that, since I started a masters degree in Interaction Design, I became more interested in front-end development in combination with user interface design.\nTime for a change\nIn early 2020, I decided to apply for jobs in front-end development and got hired by Luminis as a junior front-end developer! I agreed to work a 32h week, so I had one day off for my masters degree. It all happened during the COVID-19 lockdowns, when the housing prices were sky high. So, how could it be possible to start a new job, continue my masters degree, and move to a new city, which was not even close to where I used to live? And how wouldn\u2019t afford everything?\nAfter I found out I got the job at Luminis, I had to find a place to live. It was difficult to find something, but I was never worried I wouldn\u2019t afford it. Also, I did not easily pick an apartment just so I had a place to live in. I have my needs, and the apartment needed to fit those needs. Eventually, I found the apartment that checked off all my boxes. Even though renting is expensive, I knew I made the right decision, because I would be excited to live there and enjoy my job as well.\nIs it possible to study, while also having a part-time job (32h)?\nShort answer: YES! But I guess you would like a more thorough explanation. I will start at the beginning, because I had to get a master\u2019s degree for my previous lecturer job. The university gave me two paid days to study. Halfway through my studies, I decided to\nchange jobs. How would that work? Because I would not get two paid days off for that job. I decided to work part-time, a 32h workweek. It meant I had one less day to work on my studies.\nMy solution:\nMake a PLAN! For the study, I knew what courses I had to take during a semester, which means I could easily plan out my semester. In my case, Google Classroom showed which deadlines I had on which days. I wrote down everything I had to do to meet that specific deadline. Each small task I divided over every Friday per deadline. I knew exactly what I had to do each Friday to stay on route towards meeting my deadlines.\nWorking at Luminis\nLuminis has always supported me from the beginning. In my application process, I let them know that I was studying for a master\u2019s and wanted to work a 32h workweek. Because there was clear communication, it was never a problem.\nIt takes effort to create a life that you are satisfied with, but if you do not take that leap of faith, you will not know if that one job opportunity will create the life you want to live! It worked out for me. I recently graduated with a master\u2019s degree in Interaction Design, and I have a job that I love that I can change to my satisfaction. You do not need to listen to what society wants you to do. If you did that, you would live the life society wants you to live instead of the life you want to create!\nTo end this blog post, I would like to shake my top three tips that can help you on your path to happiness job\u00a0wise:\n#1: Be excited about going to your workplace.\n#2: Money \u2260 Happiness. Take a leap of faith to become the person you want to become and have the life you want to live. Do not let money hold you back.\n#3: You are never too old to study. If you want to go back to school and change or upgrade your career, do it. It is never too late. Communicate, plan, and make it work the way you want it to work!\n", "tags": [], "categories": ["Blog", "Working at Luminis"]}
{"post_id": 38388, "title": "Develop and Test your Github Actions workflow locally with \u201cact\u201d", "url": "https://www.luminis.eu/blog-en/develop-and-test-your-github-actions-workflow-locally-with-act/", "updated_at": "2023-07-05T12:21:30", "body": "At work, I regularly train people on the subject of Continuous Integration and Continuous Delivery, where I predominantly utilize GitHub Actions for the workshop assignments. This choice is motivated by GitHub\u2019s extensive adoption within the developer community and the generous offering of approximately 2000 minutes or 33 hours of free build time per month.\nDuring one of my recent workshops, a participant raised a question regarding the possibility of locally testing workflows before pushing them to GitHub. They pointed out the inconvenience of waiting for a runner to pick up their pipeline or workflow, which negatively impacts the developer experience. At that time, I was unaware of any local options for GitHub Actions. However, I have since come across a solution called \u201cact\u201d that addresses this issue.\nWhat is \u201cact\u201d?\n\u201cact\u201d is a command-line utility that emulates a Github Actions environment and allows you to test your Github Actions workflows on your developer laptop instead of in a Github Actions environment. You can install \u201cact\u201d by using for instance brew on the Mac.\n$ brew install act\nRunning Workflows Locally\n\u201cact\u201d enables you to execute and debug GitHub Actions workflows locally, providing a faster feedback loop during development. Running the \u201cact\u201d command line will pick up the workflows in your .github/workflows folder and try to execute them. Using \u201cact\u201d can be as simple as:\n$ act\n\u201cact\u201d uses Docker to create an isolated environment that closely resembles the GitHub Actions execution environment. This ensures consistency in the execution of actions and workflows. If you don\u2019t have Docker installed you can use Docker Desktop or use Colima, an easy way to run container runtimes on macOS.\nRunners\nWhen defining the workflow you can specify a runner based on a specific virtual machine/environment when performing your steps.\njobs:\r\n  Build:\r\n    runs-on: ubuntu-latest\r\n    steps:\r\n      ...\r\n\nBy default, \u201cact\u201d has a mapping to a specific docker image when you specify the ubuntu-latest runner. When running \u201cact\u201d for the first time it will ask you to pick a default image for ubuntu-latest. You can choose from 3 types of base images that can be mapped to ubuntu-latest:\n\nMicro Docker Image (node:16-buster-slim)\nMedium Docker Image (catthehacker/ubuntu:act-latest)\nLarge Docker Image (catthehacker/ubuntu:full-latest)\n\nDon\u2019t worry if you\u2019re not happy with the one you selected, you can always change the default selection by changing the following file in your users home directory ~/.actrc.\nThe large docker image is around 18GB!!, so I initially picked the medium-sized image as it should contain most of the commonly used system dependencies. I soon learned that it contains quite some libraries, but when I tried to run a Java + Maven-based project I learned that it did not contain Apache Maven, while the normal ubuntu-latest on GitHub does have that.\n[CI/Build] \u2b50 Run Main Build\r\n[CI/Build]   \ud83d\udc33  docker exec cmd=[bash --noprofile --norc -e -o pipefail /var/run/act/workflow/2] user= workdir=\r\n| /var/run/act/workflow/2: line 2: mvn: command not found\r\n[CI/Build]   \u274c  Failure - Main Build\r\n[CI/Build] exitcode '127': command not found, please refer to https://github.com/nektos/act/issues/107 for more information\r\n\nI didn\u2019t want to switch to an 18GB docker image to be able to just run Maven, so I ended up finding an existing image by Jamez Perkins. It simply takes the original \u201cact\u201d image ghcr.io/catthehacker/ubuntu:act-latest and adds Maven version 3.x to it. You can easily specify running your workflow with custom images by providing the platform parameter.\n$ act -P ubuntu-latest=quay.io/jamezp/act-maven\nAfter using that image my workflow ran without any errors.\nWorking with multiple jobs/stages\nYour GitHub actions workflow usually consists of one or more jobs that separate different stages of your workflow. You might for instance have a Build, Test and Deploy stage.\n\nUsually, you build your application in the build job and use the resulting artifact in the deploy job. Jobs can run on different runners, so in a GitHub Actions environment, you will probably be using the upload/download artifact action which will use centralized storage for sharing the artifacts between different runners. When using \u201cact\u201d and sharing artifacts you will need to be specific about where the artifacts need to be stored. You can do so by providing a specific parameter named --artifact-server-path.\n$ act -P ubuntu-latest=quay.io/jamezp/act-maven \\\r\n  --artifact-server-path /tmp/act-artifacts\r\n\nWorking with secrets\nIt\u2019s a best practice to always separate your secrets from your workflow definition and only reference them from a specific secret store. When using GitHub Actions you can store your secrets in the built-in secret management functionality.\nTo provide an action with a secret, you can use the secrets context to access secrets you\u2019ve created in your repository.\njobs:\r\n  staticanalysis:\r\n    runs-on: ubuntu-latest\r\n    steps:\r\n    - uses: actions/checkout@v3\r\n      with:\r\n        # Disabling shallow clone is recommended for improving relevancy of reporting\r\n        fetch-depth: 0\r\n    - name: SonarQube Scan\r\n      uses: sonarsource/sonarqube-scan-action@master\r\n      env:\r\n        SONAR_TOKEN: ${{ secrets.SONAR_TOKEN }}\r\n        SONAR_HOST_URL: ${{ secrets.SONAR_URL }}\r\n\n\u201cact\u201d does not have a UI in which you can specify secrets, so you will need to provide those values explicitly from the command line or store them in a .env formatted file when testing your workflow. If you only have a few secrets you can easily add them by just providing the secret from the command line by using the -s option.\n$ act -s SONAR_TOKEN=somevalue\r\n$ act --secret-file my.secrets\r\n\nWorking with environment variables\nSimilar to secrets you sometimes make use of environment variables inside your workflow. For a single environment variable you can use --env myenv=foo or if you have a set of environment variables you can create a dotenv file and provide a reference to the file from the CLI by providing the --env-file parameter.\n$ act --env-file my.env\nThe .env file is based on a simple standard file format which contains a set of key-value pairs divided by new lines.\nMY_ENV_VAR=MY_ENV_VAR_VALUE\r\nMY_2ND_ENV_VAR=\"my 2nd env var value\"\r\n\nEvent simulation\nEvents are a fundamental part of workflows. Workflows will start due to some specific event happening within Github like a push, creation of a pull request, etc. With \u201cact\u201d you can simulate such an event to trigger your workflow(s). You can provide the event as an argument.\n$ act pull_request\nEvents are usually more complex than just a simple string so if you want to be specific you can provide a reference to an event payload:\n$ act --eventpath pull_request.json\r\n{\r\n  \"pull_request\": {\r\n    \"head\": {\r\n      \"ref\": \"sample-head-ref\"\r\n    },\r\n    \"base\": {\r\n      \"ref\": \"sample-base-ref\"\r\n    }\r\n  }\r\n}\r\n\nBy providing your events from the command line you can test different scenarios and observe how your workflows respond to those events.\nSummary\nUsing \u201cact\u201d is straightforward and can significantly help in the initial phase of developing your workflow. \u201cact\u201d offers a significant advantage in terms of a swift feedback loop. It enables developers to perform tests locally and iterate rapidly until they achieve the desired outcome, eliminating the need to wait for GitHub\u2019s runners to finish the workflow.\n\u201cact\u201d additionally aids developers in avoiding resource wastage on GitHub\u2019s runners. By conducting local tests, developers can ensure the proper functioning of their workflows before pushing code changes to the repository and initiating a workflow on GitHub\u2019s runners.\nIf you\u2019re working with GitHub Actions I would recommend to asses \u201cact\u201d as a tool for your development team.\n", "tags": ["CI/CD", "git", "github", "testing"], "categories": ["Blog", "Development"]}
{"post_id": 38365, "title": "Yuma, a new digital transformation challenger", "url": "https://www.luminis.eu/blog-en/yuma-a-new-digital-transformation-challenger/", "updated_at": "2023-06-28T13:52:22", "body": "Yuma is a new digital transformation group in the BeNeLux. A group where digital and human transformation go hand-in-hand as it states there is no digital transformation without human transformation.\nAmsterdam, June 28th, 2023 \u2013 Yuma will combine a human, people-centric approach, with a hands-on mentality and best-in-class expertise. This best-in-class expertise is a result of the combination of XPLUS, Total Design, Luminis and BPSOLUTIONS, and more companies are expected to join the group.\n\nLot van Wegen, CMO Yuma:\n\u201cIn today\u2019s world companies are digitally reinventing themselves continuously. Many digital transformations fail because they don\u2019t pay enough attention to the human side of the transformation. We see the need for a more human approach. That\u2019s why we started Yuma: a combination of best-in-class experts with an empathetic, pragmatic, and people-centric way of working.\u201d\nAs of the 1st of July, Pascal Laffineur will be the CEO of Yuma. Coming from NRB, Altran (CapGemini), SFR, and Alcatel, Pascal has a strong tech and IT background. Building on Pascal\u2019s leadership skills in his previous CEO roles, he brings the necessary knowledge and expertise to position Yuma as the prominent player we aspire to be in Western Europe.\nPascal Laffineur, CEO Yuma:\n\u201cBeing at the start of Yuma is the best challenge I could dream about. I truly believe in a more human and holistic approach to make digital transformation successful. The group is all about teamwork, customer service, thought leadership, technology, and fun!\u201d\nYuma expects a revenue of about 100 million euros this year and works together with 400 employees located across BeNeLux.\nKeep an eye on the website: weareyuma.com\n", "tags": [], "categories": ["Blog", "News"]}
{"post_id": 38323, "title": "Introduction to Autonomous Agents in AI", "url": "https://www.luminis.eu/blog-en/machine-learning-ai-en/introduction-to-autonomous-agents-in-ai/", "updated_at": "2023-06-27T14:32:51", "body": "Over the past 12 months, AI tools have garnered significant attention and generated considerable hype within the tech industry. Initially, the focus was on the metaverse, which was seen as an extension of our existing reality, offering more immersive experiences. However, the emergence of powerful tools such as chatGPT and DALL-E 2 has marked the beginning of a new paradigm.\nWhile the initial excitement surrounding AI tools may have subsided, one area that continues to grow and gain traction is autonomous agents in AI. Although the term \u2018autonomous agents in AI\u2019 might not immediately ring a bell, you are likely familiar with many related concepts.\nWhat is an agent?\nTo begin with, the word agent has nothing to do with AI. An agent is defined as anything that can perceive its environment and act upon that. This concept has been used in different disciplines, like cognitive science, ethics, or social simulations, but it comes from economics, where a \u201crational agent\u201d is defined as anything with a goal. The Matrix (1999) already used this concept, playing with this double meaning as a govern agent and software agent, which can respond to inputs and operate on given instructions.\nThe Stanford Encyclopaedia of Philosophy defines artificial intelligence as \u201cthe study of agents that receive precepts from the environment and perform actions\u201d. Each such agent implements a function that maps percept sequences to actions, and we cover different ways to represent these functions\u2026\u201d (Russell & Norvig 2009, vii).\nSo if AI is the study of agents, this is an important topic.\nNow you have the idea, where is the hype?\nResearchers from Stanford and Google created an interactive environment with 25 AI agents that simulate human behavior. You can see the full paper here. It is not just a bunch of pixel dolls walking around and having bot conversations; they remember those conversations, join for coffee at a cafe, a walk in the park, etc. These bots were able to respond to the environment by splitting complex tasks and giving priority over time. They could even prepare a Valentine\u2019s Day party by themselves:\n\u201cFor example, starting with only a single user-specified notion that one agent wants to throw a Valentine\u2019s Day party, the agents autonomously spread invitations to the party over the next two days, make new acquaintances, ask each other out on dates to the party, and coordinate to show up for the party together at the right time.\u201d\n\nIt looks like some folks just put some LLMs to talk to each other, but that is just the surface. There are some cool and promising projects trying to implement the best Autonomous Agent in AI possible, like Camel, AutoGPT (from OpenAI), or BabyAGI, and each one has a different approach. Let\u2019s take for instance BabyAGI, which autonomously creates and performs tasks based on an objective, following their Github:\nThe script works by running an infinite loop that does the following steps:\n\nPulls the first task from the task list.\nSends the task to the execution agent, which uses OpenAI\u2019s API to complete the task based on the context.\nEnriches the result and stores it in Chroma/Weaviate.\nCreates new tasks and reprioritizes the task list based on the objective and the result of the previous task.\n\n\nAnother example is this new tool developed by HyperWrite that can connect to the internet and take control of your computer and order a pizza from Domino\u2019s for you:\n\n\nAutoGPT agents like Hyperwrite AI can do everything for you from generating creative poetry to ordering pizza \ud83e\udd2f  \nDiscover the AI way to interact with the internet!! pic.twitter.com/n9XrS9JZvA\n\u2014 Shubham Saboo (@Saboo_Shubham_) April 14, 2023\n\nWhat is next?\nSo, as you can see, autonomous agents in artificial intelligence are powerful tools able to perform tasks and make decisions independently, without human intervention. The uses of these agents have potential to revolutionize various industries and domains, like healthcare, transportation, finance and customer service. However, with their increasing integration into society, it becomes crucial to examine the implications of this technology. Several topics need to be discussed.\nEthical Concerns:\nAs autonomous agents become more sophisticated and capable, ethical considerations become paramount. Questions arise regarding the decision-making process of these agents and the accountability for their actions. Who should be held responsible if an autonomous agent makes a mistake or causes harm? Establishing ethical guidelines and legal frameworks is crucial to ensure that agents operate in a responsible and accountable manner.\nJob Displacement:\nSince this technology is designed to perform tasks traditionally done by humans, it raises questions about the future of employment. While some argue that AI will create new job opportunities, others fear that certain industries and professions may become obsolete. Preparing for this potential shift in the job market and finding ways to re-skill and up-skill the workforce are critical considerations.\nPrivacy and Data Security:\nAutonomous agents rely on vast amounts of data to make informed decisions and provide personalized experiences. However, this raises concerns about privacy and data security. As these agents interact with users and collect sensitive information, ensuring the protection of personal data becomes paramount. Robust security measures, data anonymization techniques, and transparent data usage policies must be implemented to address these concerns.\nBias and Fairness:\nAutonomous agents are trained on large datasets, which can inadvertently introduce biases present in the data. If these biases are not identified and mitigated, autonomous agents can perpetuate discriminatory practices or amplify existing societal inequalities. Developing algorithms that are fair, transparent, and free from bias is crucial to ensure equitable outcomes in the deployment of autonomous agents.\nHuman-Autonomous Agent Collaboration:\nWhile the autonomy of these agents is a defining feature, ensuring effective collaboration between humans and autonomous agents is essential. Building systems that allow cooperation between humans and agents can lead to better productivity, decision-making, and problem-solving. Designing user-friendly interfaces and fostering trust and transparency in human-agent interactions are important factors to consider.\nTake-away on Autonomous Agents in AI\nAre Autonomous Agents the next step in AI? Well\u2026 it\u2019s kinda soon to know. At this moment, all the projects use the ChatGPT API, since it\u2019s the LLM that gives the best performance, so building several agents talking to each other is relatively expensive. Also, since the agent needs a response from another agent to start processing the next request, it makes the process slow, and other problems like hallucinations can interfere in the process.\nRemember, we are still on the first steps on AI, but the adoption curve has been almost non-existent for a product like ChatGPT, so the projects above mentioned are just the first early steps to something bigger than what we currently have.\nWant to know more? Make sure to read some of our other blogs.\n", "tags": [], "categories": ["Blog", "Machine learning &amp; AI"]}
{"post_id": 38362, "title": "1-daagse workshop klantgericht werken, hoe organiseer je dat?", "url": "https://www.luminis.eu/blog-en/1-daagse-workshop-klantgericht-werken-hoe-organiseer-je-dat/", "updated_at": "2023-07-02T20:54:55", "body": "In de eerste blog van deze blogserie wordt o.a. de 1-daagse workshop genoemd als methode om klantgericht werken in je organisatie in te bedden. Leuk natuurlijk, maar hoe doe je dat nou, zo\u2019n workshop organiseren en faciliteren?!\nIn deze blog deel ik de belangrijkste ervaringen die ik de afgelopen jaren op heb gedaan bij het voorbereiden en uitvoeren van zo\u2019n 1-daagse workshop. Valkuilen en tips en trucs komen zeker aan bod.\nEen 1-daagse workshop klantgericht werken, waarom?\nMet een 1-daagse workshop klantgericht werken wil je een ontwerpuitdaging concretiseren, een oplossingsrichting defini\u00ebren en focus (aan een project) geven zodat je daarna weer snel aan de slag kunnen.\u00a0Bij een 1-daagse worden allerlei stakeholder betrokken, van opdrachtgever tot eindgebruikers. Een 1-daagse heeft een innovatief karakter en gebruikt Design Thinking methodes om een innovatieve mindset te stimuleren.\nNaast de link met klantgericht werken zijn er dus meer redenen om een 1-daagse workshop te houden.\nEen 1-daagse:\n\nkan een groep stakeholders op 1 lijn krijgen;\nhelpt bij het vlottrekken van een vastgelopen proces;\ngeeft hernieuwde energie.\n\nKenmerken van een 1-daagse\nEen 1-daagse is gebaseerd op Design Thinking. In 1 dag doorloop je een design cyclus, via Empathy, Define, Ideate en Prototype naar Test. Voor elke fase zijn er dan weer allerlei werkvormen die je kunt gebruiken om inzichten op te halen. Daarover later meer\u2026\nEen design cyclus past in mijn ogen binnen timebox die je zo groot en zo klein kunt maken als je wilt. En dus ook binnen 1 dag! Daarbij wel de aantekening dat test de presentatie aan verschillende stakeholders is aan wie je de uitkomst van de workshop graag wilt presenteren.\nBij een 1-daagse:\n\nwerken deelnemers met diverse achtergronden samen waardoor wederzijds begrip en enthousiasme ontstaat;\nzijn deelnemers \u2018slechts\u2019 1 dag kwijt (voorbereiden, begeleiden en uitwerken kosten meerdere dagen maar dat is vooral het werk van een facilitator);\nwordt het topje van de ijsberg uitgewerkt. Het is \u2018slechts\u2019 1 dag waarna vervolgstappen bijna altijd nodig zijn.\n\nWaarvoor kun je een 1-daagse workshop zoal inzetten?\nEnkele voorbeelden uit de praktijk:\n\nstrategiesessies\ncontentarchitectuur\nnavigatie tussen je portalen\nchatbot\napps\nhet uitwerken van nieuwe Design System componenten\n\nStappenplan c.q. planning\nOm een 1-daagse succesvol te laten verlopen heb je een facilitator en enthousiaste deelnemers nodig. De facilitator kan helpen met het voorbereiden van het materiaal dat nodig is. Daarbij hanteer ik grofweg de volgende planning:\n\nminimaal 2 weken voor de 1-daagse: intake;\nuiterlijk twee weken voor de 1-daagse: uitnodiging om deel te nemen versturen;\n1 week voor de 1-daagse: agenda, inleiding en verwachting verspreiden;\n1 week voor de 1-daagse: start met het verzamelen van informatie en evt. aan de genodigden vragen om \u2018huiswerk\u2019 te maken;\nde 1-daagse workshop zelf;\n1 week na 1-daagse: conclusies verspreiden en vervolgacties uitzetten.\n\n\nDe intake\nEen intake met je opdrachtgever kun je in max. 2 uur doen en tijdens de intake komen de volgende onderwerpen aan bod:\n\nuitdaging\nverwachting, wat willen we bereiken op 1 dag?\ncijfers\nfocus bepalen\ngenodigden\n\nEen uitdaging wordt als HKW geformuleerd. HKW staat voor Hoe Kunnen We er voor zorgen dat\u2026\nBij het bepalen van een HKW gaat het om een uitdaging en niet om een oplossing. Een HKW wil je \u2018open\u2019 houden, de deelnemer hebben nog de hele dag om oplossingen te bedenken! En natuurlijk werken we graag met cijfers, als je een ontwerpuitdaging kunt onderbouwen helpt dat bij het maken van keuzes en stellen van prioriteiten bij je oplossing(en).\nAgenda en werkvormen\nTijdens de workshop wordt de Design Thinking cyclus gevolgd. Samen met de werkvormen die je wilt gebruiken bepalen ze de agenda van de dag. Elke ontwerpuitdaging ziet er echter wel anders uit waarmee ik vind dat je de agenda op maat samen zult moeten stellen. Een 1-daagse rond eenzelfde ontwerpuitdaging, bijvoorbeeld een herhalende workshop met designers in de IP sprint (zie SAFe) kun je prima \u2018van de plank\u2019 halen.\nDe indeling van de dag ziet er voor veel* 1-daagses ongeveer zo:\n\n09:00\u00a0 Deelnemers druppelen binnen\n09:30\u00a0 Start, koffie en introductie deelnemers\n09:45\u00a0 Uitdaging en wat weten we zoal?\n10:15\u00a0 \u00a0Koffie, niet onbelangrijk\n10:30\u00a0 Heatmappen (meeschrijven op stickies en dot-voten)\n10:45\u00a0 Scope bepalen\n11:15\u00a0 \u00a0Lightning demo\u2019s voorbereiden\n11:45\u00a0 \u00a0Pauze\n12:30\u00a0 Demo\u2019s\n13:00\u00a0 User test flows\n13:30\u00a0 Conceptualisatie\n15:00\u00a0 Uitwerken en presentabel maken concepten\n16: 00 Demo en actiepunten\n\nFase: Uitdaging, Define, Ideate, Prototype en test.\n*voor strategiesessies zal het tweede gedeelte van de agenda niet op demo\u2019s maar meer op uitwerking, beslissen en afspraken maken gericht zijn.\nEen dag begint altijd met een gesprek rond de uitdaging, wat er bekend is en het stellen van vragen om je in te leven. Op basis van je ontwerpuitdaging ga je verder met het programma waarbij je de volgende werkvormen toe kunt passen:\n\nontwerpvraag\nsituatieschets omschrijven/huidige situatie + knelpunten\ninzicht in data\ndesk research\nlightning Demo\u2019s (inspiratie)\npersonas\nempathymap\nuserflow + dot-voting\nflow uitwerken in scenario based sketchboard / solution sketch (notes, ideas, cray 8, scenario sketch)\n1 of meerdere schermen of idee\u00ebn uitwerken\nvervolgstappen benoemen\ndemo/oplevering\n\nVeel van deze werkvormen kun je online vinden. Kijk bijvoorbeeld eens op:\n\nWhiteboard templates for brainstorming, team management, strategy and mapping.\nDesign thinking template\nService design toolkit\nService design tools\n\nWie betrekken?\nOmdat ik geloof in Design Thinking en het collectief, nodig ik graag een multidisciplinaire groep uit. Tijdens een intake (zie stappenplan) komt de samenstelling van de groep aan bod. Standaard worden mensen vanuit business (haalbaar), eindgebruiker (wenselijk) en techniek (mogelijk) uitgenodigd. Dit betekent zoiets als opdrachtgever, eindgebruiker, engineer en helpdesk medewerkers, die zijn er dus sowieso bij.\n\nMijn checklist voor de deelnemers:\n\nEindgebruiker(s)\n(Realisatie)team\nOpdrachtgever\nKeten- of stuurgroepleden/management\nTechniek\nChat/social/\u2026\nHelpdesk\n\nVergeet ook niet een facilitator een uitnodiging te sturen, en te bedenken wie uit te nodigen voor de demo aan het einde van de dag.\nDe overige deelnemers worden aan de hand van de uitdaging bepaald.\u00a0 De invulling van Team kan realisatieteam zijn maar ook managementteam. Zoals gezegd, dit verschilt per uitdaging, het gaat om de kennis van de uitdaging die in wordt gebracht.\nValkuilen en tips\nDe workshop zelf heb ik nooit \u2018mis\u2019 zien gaan. Hoe meer materiaal je verzameld hebt hoe makkelijk de workshop verloopt. Dan nog, tijdens zo\u2019n dag haal je veel informatie op en ga er maar vanuit dat je echt niet alles in het voren weet. Berust in de known unknowns. Het voorbereiden opvolgen van een 1-daagse kost wel tijd en vooral daar moet je rekening mee houden.\nPunten van aandacht\n\nVoorbereidingstijd \u2192 neem je tijd om onderzoek te doen en materiaal te verzamelen\nOpvolging \u2192 als de 1-daagse voorbij is begint het pas, je hebt een richting en het is waarschijnlijk dat je die uit wilt gaan werken. Reserveer tijd voor het schrijven van een rapport en communicatie ervan.\n\nTips\n\nZoals je in de globale planning kunt zien vindt er een intake plaats. Bespreek tijdens de intake de volgende vragen:\n\nWelke uitdaging(en) speelt er?\nWat willen we bereiken op 1 dag?\nWelke cijfers of feiten zijn beschikbaar? \u2192 als je het niet weet, doe je huiswerk!\nWaar zou de focus op moeten liggen?\nWie moet er worden uitgenodigd?\n\n\nZorg voor een demo aan het einde van de dag waarin je de resultaten aan management en overige stakeholders presenteert. Dit brengt de energie van de dag over en geeft meer kans dat er ruimte komt voor vervolgstappen.\nOver de rol van een facilitator\u2026 Hoewel het vaak makkelijk is om als facilitator inhoudelijk betrokken te raken denk ik dat een facilitator mensen activeert door ze de uitdaging zelf te laten ervaren en een oplossing te laten bedenken. Een facilitator is niet verantwoordelijk voor de kwaliteit van het product. Hij geeft anderen de verantwoordelijkheid en begeleidt het proces en de ontwikkeling van de organisatie.\nHet helpt om een 1-daagse op een locatie te organiseren waar iedereen het gevoel heeft \u2018er uit\u2019 te zijn en aan een onderwerp te werken wat aandacht krijgt.\n\nEn nu?\nIk hoop dat je enthousiast bent over deze werkwijze en er mee aan de slag kan gaan. Gewoon doen en ervaring opdoen om te leren zou ik zeggen!\nHulp nodig?\nWe kunnen vanuit Luminis natuurlijk ook volgende dingen voor je regelen:\n\nMeewerken om een programma samen te stellen wat past bij jouw vraagstuk;\nFaciliteren van de 1-daagse workshop;\nVerzorgen van workshopmethodes en materialen die nodig zijn voor de 1-daagse.\n\nNeem contact met ons op voor meer informatie.\n\n\n\n", "tags": [], "categories": ["Blog", "Concepting &amp; UX"]}
{"post_id": 38298, "title": "Data Quality Series, part 2: Data Quality Testing with Deequ in Spark", "url": "https://www.luminis.eu/blog-en/data-quality-testing-with-deequ-in-spark/", "updated_at": "2023-08-13T12:12:10", "body": "In this blog, we explore how to ensure data quality in a Spark Scala ETL (Extract, Transform, Load) job. To achieve this, we leverage Deequ, an open-source library, to define and enforce various data quality checks.\nIf you need a refresher on data quality and its importance in data processing pipelines, you can refer to my previous blog post in this series, \u201cIntroduction to Data Quality\u201d. To recap, data quality is essential for accurate data analysis, decision-making, and achieving business objectives. It involves maintaining clean and standardized data that meets expectations. Ensuring data quality requires measuring and testing the data at different stages of the data pipeline. This may include unit testing, functional testing, and integration testing. A few testable properties of data are schema, freshness, quality, and volume, which we focus on in this blog.\nTo illustrate these concepts, we use a mock dataset based on the Iowa Liquor Sales Dataset as a running example. The dataset and the complete code for this blog can be found in the following GitHub repository.\nThe rest of the blog is structured as follows:\n\nTechnologies\nSetup\nThe Dataset\nBuilding Schema Checks\nProfiling Your Data\nAdding Data Quality Checks\nCollecting Data Quality Metrics\nGracefully Handling Data Changes\nAnomaly Detection\nConclusion\n\n1. Technologies\nEnsuring data quality in Spark can be achieved using various tools and libraries. One notable option is Deequ, an open-source library developed by AWS. It is a simple, but featureful tool that integrates well into AWS Glue or other Spark run times. By incorporating Deequ into our pipeline, we can perform schema checks, validate quality constraints, detect anomalies, collect quality metrics for monitoring, and use data profiling to gain insights into the properties of our data. Deequ effectively translates high-level rules and metrics into optimized Spark code, using the full potential of your Spark cluster.\nOther popular choices for data quality testing are tools like Great Expectations and Soda Core. These tools are rich in features, but also require additional configuration and setup, which may be explored in future blogs. For users already working within an AWS Glue ecosystem, exploring options that are tightly integrated with Glue, such as Deequ, can be more convenient and seamless.\nFor brevity, we focus on adding data quality to bare-bones Spark ETL scripts. While the implementation is similar, if you are using AWS Glue, we won\u2019t cover it in this blog. Instead, you can find an example glue script in the code repository.\n2. Setup\nTo begin, you need a working Scala development environment. If you don\u2019t, install Java, Scala, and sbt (Scala Build Tool). For Linux x86 the installation would look as follows:\n\n# Install Java (on Debian)\r\nsudo apt install default-jre\r\n# Install Coursier (Scala Version Manager)\r\ncurl -fL https://github.com/coursier/coursier/releases/latest/download/cs-x86_64-pc-linux.gz | gzip -d >> cs &&chmod +x cs &&./cs setup\r\n\r\n# Install Scala 2.12 and sbt\r\ncs install scala:2.12.15 &&cs install scalac:2.12.15\n\nNext, download a compatible Apache Spark distribution (version 3.3.x is recommended) and add the bin folder to your system path. If you can run spark-submit, you are all set.\n\n# Download Spark\r\ncurl https://dlcdn.apache.org/spark/spark-3.3.2/spark-3.3.2-bin-hadoop3.tgz --output hadoop.tgz\r\ntar xvf hadoop.tgz\r\nmv spark-3.3.2-bin-hadoop3 /usr/local/spark\r\n\r\n# Add the following line to your .bashrc (adds Spark to PATH)\r\nexport PATH=\"$PATH:/usr/local/spark/bin\"\n\nSample Script\nIf you haven\u2019t already, clone the example project and open it in your editor of choice.\ngit clone git@github.com:EgorDm/deequ-spark-example.git\nYou find an empty example Spark script that reads a CSV file and writes it in parquet format to the output path. It takes the input path, output path and a path for metric storage as command line arguments.\n\ndef main(sysArgs: Array[String]): Unit = {  \r\n    // Parse job arguments  \r\n    val args = Map(  \r\n        \"input_file_path\" -> sysArgs(0),  \r\n        \"output_file_path\" -> sysArgs(1),  \r\n        \"metrics_path\" -> sysArgs(2)  \r\n    )  \r\n      \r\n    // Read the CSV input file  \r\n    val rawDf = spark.read  \r\n        .option(\"header\", \"true\")  \r\n        .csv(args(\"input_file_path\"))  \r\n      \r\n    logger.info(s\"Do some preprocessing\")  \r\n      \r\n    // Write the result to S3 in Parquet format  \r\n    rawDf.write  \r\n        .mode(\"overwrite\")  \r\n        .parquet(args(\"output_file_path\"))  \r\n}\n\nCompile the script with the following command, which outputs the jar as target/scala-2.12/glue-deequ_2.12-0.1.0.jar.\nsbt compile && sbt package\nRunning this Spark job is straightforward:\nspark-submit \\\r\n    --class EmptyExample \\\r\n    ./target/scala-2.12/glue-deequ_2.12-0.1.0.jar \\\r\n    \"./data/iowa_liquor_sales_lite/year=2022/iowa_liquor_sales_01.csv\" \\\r\n    \"./outputs/sales/iowa_liquor_sales_processed\" \\\r\n    \"./outputs/dataquality/iowa_liquor_sales_processed\"\nInclude Deequ Library\nSince we use the Deequ library, it must be added as a dependency to our project. While the library is already included in the project\u2019s dependencies, it is deliberately not bundled into the compiled jar. Instead, you can use the following command to extract it to the target/libs folder, or you can download it yourself from the maven repository.\nsbt copyRuntimeDependencies\nPass the --jars option to the Spark job, so the library is loaded at runtime:\n\nspark-submit \\\r\n    --jars ./target/libs/deequ-2.0.3-spark-3.3.jar \\  \r\n    --class ExampleSpark \\  \r\n    ./target/scala-2.12/glue-deequ_2.12-0.1.0.jar \\  \r\n    \"./data/iowa_liquor_sales_lite/year=2022/iowa_liquor_sales_01.csv\" \\  \r\n    \"./outputs/sales/iowa_liquor_sales_processed\" \\  \r\n    \"./outputs/dataquality/iowa_liquor_sales_processed\"  \n\nAfter running the command, the output parquet files are stored in outputs/sales/iowa_liquor_sales_processed and can be inspected with Spark, Pandas, or data tools like tad.\n3. The Dataset\nNow that our example ETL script works, let\u2019s look at the dataset. The mock dataset is based on the Iowa Liquor Sales dataset, which is simplified and modified to contain various data issues representative of the real world.\nThe dataset is partitioned by year, where each partition introduces schema and/or distribution changes.\ndata/iowa_liquor_sales_lite/\r\n    year=2020/iowa_liquor_sales_*.csv\r\n    year=2021/iowa_liquor_sales_*.csv\r\n    year=2022/iowa_liquor_sales_*.csv\nAssuming we already conducted exploratory data analysis, we start building our data quality checks by using the 2022 partition, and consider at the end how the other partitions impact our solution.\nPreview of Iowa Liquor Sales dataset.\n4. Building Schema Checks\nThe first step is validating the schema of our dataset. A schema defines the structure and organization of the data, including the names and types of columns. By performing schema checks, we can ensure that our data conforms to the expected structure and identify any inconsistencies or missing columns.\nTo define the schema, we use Deequ\u2019s RowLevelSchema class. Here, each column and its properties are defined using methods like withStringColumn, withIntColumn, withTimestampColumn, or withDecimalColumn. For our dataset, the schema is as follows:\n\nval schema = RowLevelSchema()  \r\n    .withStringColumn(\"Invoice/Item Number\", isNullable = false)  \r\n    .withStringColumn(\"Date\", isNullable = false)  \r\n    .withStringColumn(\"Store Name\", isNullable = false)  \r\n    .withStringColumn(\"Zip Code\", isNullable = false)  \r\n    .withStringColumn(\"Vendor Name\", isNullable = false)  \r\n    .withIntColumn(\"Item Number\", isNullable = false)  \r\n    .withIntColumn(\"Bottles Sold\", isNullable = false)  \r\n    .withDecimalColumn(\"Sale\", isNullable = false, precision = 12, scale = 2)  \r\n    .withDecimalColumn(\"Volume Sold (Liters)\", isNullable = true, precision = 12, scale = 2)\n\nAfter defining the schema, it can be validated against the data (rawDf) using the RowLevelSchemaValidator.validate method.\n\nval schemaResult = RowLevelSchemaValidator.validate(rawDf, schema)  \r\nif (schemaResult.numInvalidRows > 0) {  \r\n    logger.error(  \r\n    s\"Schema validation failed with ${schemaResult.numInvalidRows} invalid rows. Results: ${schemaResult}\")  \r\n    schemaResult.invalidRows.show(10, truncate = false)  \r\n    sys.exit(1)  \r\n}\r\n\r\nval validDf = schemaResult.validRows\n\nThe result (schemaResult) contains two Data Frames, specifically the valid rows that conform to the schema and invalid rows that do not. In some cases, data quarantining can be applied by preserving invalid rows and moving forward. Here, we break and display faulty data in the console instead.\n5. Profiling Your Data\nThe next step is data profiling, which is an essential step for understanding the characteristics and properties of your dataset. It provides insights into the structure, content, and statistical properties of the data, enabling you to identify potential issues or anomalies, and make informed decisions about data cleansing or transformation.\nDeequ provides a convenient way to profile your data using the ConstraintSuggestionRunner. Based on the analyzed data, it collects various statistics and suggests constraints using predefined rules.\n\nConstraintSuggestionRunner()  \r\n    .onData(validDf)  \r\n    .useSparkSession(spark)  \r\n    .overwritePreviousFiles(true)  \r\n    .saveConstraintSuggestionsJsonToPath(\r\n        s\"${args(\"metrics_path\")}/suggestions.json\")  \r\n    .saveColumnProfilesJsonToPath(\r\n        s\"${args(\"metrics_path\")}/profiles.json\")  \r\n    .addConstraintRules(Rules.DEFAULT)  \r\n    .run()\n\nIn the metrics folder, profiles.json is created as output. It contains extracted statistics in a semi-structured format, which can be useful for data quality checks creation, as well as data monitoring.\n\n\"columns\": [  \r\n    {  \r\n        \"column\": \"Vendor Name\",  \r\n        \"dataType\": \"String\",  \r\n        \"isDataTypeInferred\": \"true\",  \r\n        \"completeness\": 1.0,  \r\n        \"approximateNumDistinctValues\": 166  \r\n    },  \r\n    {  \r\n        \"column\": \"Item Number\",  \r\n        \"dataType\": \"Integral\",  \r\n        \"isDataTypeInferred\": \"false\",  \r\n        \"completeness\": 1.0,  \r\n        \"approximateNumDistinctValues\": 1469,  \r\n        \"mean\": 59981.83674981477,  \r\n        \"maximum\": 995530.0,  \r\n        \"minimum\": 567.0,  \r\n        \"sum\": 2.42866457E8,  \r\n        \"stdDev\": 104855.01628803412,  \r\n        \"approxPercentiles\": []  \r\n    },  \r\n    {  \r\n        \"column\": \"Volume Sold (Liters)\",  \r\n        \"dataType\": \"Fractional\",  \r\n        \"isDataTypeInferred\": \"false\",  \r\n        \"completeness\": 0.8992343788589775,  \r\n        \"approximateNumDistinctValues\": 97,  \r\n        \"mean\": 11.238700906344382,  \r\n        \"maximum\": 1512.0,  \r\n        \"minimum\": 0.05,  \r\n        \"sum\": 40920.1099999999,  \r\n        \"stdDev\": 40.87384345937876,  \r\n        \"approxPercentiles\": []  \r\n    },\r\n    ...\n\nThe suggestions.json includes a list with some basic data quality rule suggestions based on the profiled metrics. Some suggestions are more useful than others. I noticed that sometimes columns with medium cardinality are mistaken for categorical variables, suggesting value constraints. Having tight checks is valuable, but be wary of over-fitting your tests.\n\n\"constraint_suggestions\": [  \r\n    {  \r\n        ...\r\n        \"column_name\": \"Invoice/Item Number\",  \r\n        \"current_value\": \"Completeness: 1.0\",  \r\n        \"rule_description\": \"If a column is complete in the sample, we suggest a NOT NULL constraint\",  \r\n        \"code_for_constraint\": \".isComplete(\\\"Invoice/Item Number\\\")\"  \r\n    },\r\n    {  \r\n        ...\r\n        \"column_name\": \"Volume Sold (Liters)\",  \r\n        \"current_value\": \"Minimum: 0.05\",  \r\n        \"rule_description\": \"If we see only non-negative numbers in a column, we suggest a corresponding constraint\",  \r\n        \"code_for_constraint\": \".isNonNegative(\\\"Volume Sold (Liters)\\\")\"  \r\n    },\n\n6. Adding Data Quality Checks\nNow that we identified expectations for our data, we write the data quality checks to help us identify and address any issues or inconsistencies present in the dataset.\nThe checks are defined in groups with associated description and severity. Under the hood, the checks are translated to metric calculations and predicates that indicate success or failure based on the result of said metric.\nThe checks address different types of issues and may operate on both column and dataset level. See this file for an overview of all supported checks. If you can\u2019t find the right check, a custom check can be written in Spark SQL with the satisfies() method.\nHere is an example of relevant data quality checks to our business case.\n\nval checks = Seq(  \r\n    Check(CheckLevel.Error, \"Sales base checks\")  \r\n        .hasSize(_ >= 0, Some(\"Dataset should not be empty\"))  \r\n        .isComplete(\"Invoice/Item Number\")  \r\n        .isComplete(\"Date\")  \r\n        .isComplete(\"Store Name\")  \r\n        .isComplete(\"Zip Code\")  \r\n        .isComplete(\"Vendor Name\")  \r\n        .isComplete(\"Item Number\")  \r\n        .isComplete(\"Bottles Sold\")  \r\n        .isComplete(\"Sale\")  \r\n        .isUnique(\"Invoice/Item Number\")  \r\n        .hasPattern(\"Invoice/Item Number\", \"^INV-[0-9]{11}$\".r)  \r\n        .hasPattern(\"Date\", \"^[0-9]{4}-[0-9]{2}-[0-9]{2}$\".r)  \r\n        .hasPattern(\"Zip Code\", \"^[0-9]{5}$\".r)  \r\n        .isNonNegative(\"`Bottles Sold`\")  \r\n        .isNonNegative(\"`Sale`\")  \r\n        .isNonNegative(\"`Volume Sold (Liters)`\")\r\n)\n\nThe data quality checks can be executed using the VerificationSuite:\n\nvar verificationSuite = VerificationSuite()  \r\n    .onData(validDf)  \r\n    .useSparkSession(spark)  \r\n    .overwritePreviousFiles(true)  \r\n    .saveCheckResultsJsonToPath(s\"${args(\"metrics_path\")}/checks.json\")  \r\n    .addChecks(checks)\r\n\r\nval verificationResult = verificationSuite.run()\r\nif (verificationResult.status == CheckStatus.Error) {  \r\n    logger.error(s\"Data quality checks failed. Results: ${verificationResult.checkResults}\")  \r\n    sys.exit(1)  \r\n}\n\nRunning the checks as it is, results in a failure. The generated report (e.g., checks.json) generally provides enough information to determine which checks fail and why. By examining the report, we see the following error, implying that ~1.1% of our zip codes don\u2019t follow the five-digit format.\n\n...\r\n{  \r\n    \"check_status\": \"Error\",  \r\n    \"check_level\": \"Error\",  \r\n    \"constraint_status\": \"Failure\",  \r\n    \"check\": \"Validity checks\",  \r\n    \"constraint_message\": \"Value: 0.9898740429735737 does not meet the constraint requirement!\",  \r\n    \"constraint\": \"PatternMatchConstraint(Zip Code, ^[0-9]{5}$)\"  \r\n},\r\n...\n\nThis is correct, as the zip code column in the dataset may contain some straggling characters. This can be fixed by either reducing the check sensitivity or addressing the issues before the checks are run:\n\nval validDf = schemaResult.validRows  \r\n    .withColumn(\"Zip Code\", F.regexp_extract(F.col(\"Zip Code\"), \"[0-9]{5}\", 0))\n\n7. Collecting Data Quality Metrics\nMetrics provide valuable insights into the health and quality of our data. They can help us see trends, make improvements, and find anomalies in our data. Some metrics are necessary for configured checks and are computed automatically, while others may be needed for external systems, such as monitoring dashboards or data catalogs, and need to be specified manually.\nThe additional metrics need to be added manually as analyzers:\n\nprivate def numericMetrics(column: String): Seq[Analyzer[_, Metric[_]]] = {\r\n    Seq(  \r\n        Minimum(column),  \r\n        Maximum(column),  \r\n        Mean(column),  \r\n        StandardDeviation(column),  \r\n        ApproxQuantile(column, 0.5)  \r\n    )\r\n}  \r\n  \r\nprivate def categoricalMetrics(column: String): Seq[Analyzer[_, Metric[_]]] = {  \r\n    Seq(  \r\n        CountDistinct(column),  \r\n    )  \r\n}\n\nBelow, we create analyzers to generically compute the distribution of numeric columns.\n\nval analysers = (  \r\n    numericMetrics(\"Bottles Sold\")  \r\n    ++ numericMetrics(\"Sale\")  \r\n    ++ numericMetrics(\"Volume Sold (Liters)\")  \r\n    ++ categoricalMetrics(\"Store Name\")  \r\n    ++ categoricalMetrics(\"Vendor Name\")  \r\n    ++ Seq(  \r\n        Completeness(\"Bottles Sold\"),  \r\n    )  \r\n)\n\nSimilar to quality checks, the metrics are computed using the VerificationSuite.run() method:\n\nvar verificationSuite = VerificationSuite()  \r\n    ... \r\n    .saveSuccessMetricsJsonToPath(s\"${args(\"metrics_path\")}/metrics.json\")  \r\n    .addRequiredAnalyzers(analysers)\r\n    ...\n\nThe collected metrics are written to metrics.json file, which can be loaded by external tools. Alternatively, Deequ defines a concept of metric repositories as an interface for saving the metrics to other systems in a generic manner. You can write your own repository to store the metrics in, for example, Prometheus or AWS Cloud Watch.\nAnother useful feature is KLL Sketches which supports approximate, but highly accurate metric calculation on data by sampling.\nIncremental Computation of Metrics\nIn the realm of ETL workloads, it is rare for data engineers to reprocess the entire dataset. Typically, pipelines are designed to be incremental, processing only new data. However, if your data quality checks rely on metrics computed over the entire dataset, this can lead to a continuous increase in load on your Spark cluster.\nInstead of repeatedly running the batch computation on growing input data D, incremental computation is supported hat only needs (t) to consume the latest dataset delta \u2206D and a state S of the computation. Source: technical paper.\n\u00a0\nTo address this challenge, Deequ introduces a concept of \u201cAlgebraic states.\u201d These states store calculated metrics and the corresponding data, enabling their aggregation across multiple pipeline runs. Consequently, only the incremental data needs to be processed, significantly reducing the computational burden.\nWe demonstrate this by adding complete dataset checks within our incremental ETL script. The first step is to record incremental metrics in a temporary in-memory state provider:\n\nval incrementalStateProvider = InMemoryStateProvider()\r\n\r\nval verificationResult = VerificationSuite()\r\n    ...\r\n    .saveStatesWith(incrementalStateProvider)  \r\n    ...\n\nTo load the aggregated state from a persistent provider, a persistent state provider is needed. Additionally, we check if the state already exists to determine whether it should be included in the aggregation, specifically necessary for the first pipeline run:\n\n// Initialize state for incremental metric computation  \r\nval completeStatePath = s\"${args(\"metrics_path\")}/state_repository/\"\r\nval completeStateProvider = HdfsStateProvider(spark, s\"${completeStatePath}/state.json\", allowOverwrite = true)\r\n\r\n// Determine if the complete state already exists\r\nval fs = FileSystem.get(spark.sparkContext.hadoopConfiguration)  \r\nval aggregateStates = try {  \r\n    fs.listFiles(new Path(completeStatePath), false).hasNext  \r\n} catch {  \r\n    case _: FileNotFoundException => false  \r\n}\n\nNow, once again, we can run VerificationSuite, but this time we use the providers to load state data. Consequently, the checks and metrics are computed and merged over the aggregated state, which, in this case, represents the complete dataset:\n\n// Merge incremental metrics with complete metrics, and run data quality checks  \r\nval completeChecks = Seq(  \r\n    Check(CheckLevel.Error, \"Sales complete checks\")  \r\n        .hasSize(_ >= 0, Some(\"Dataset should not be empty\"))  \r\n)\r\n\r\nlogger.info(\"Running complete dataset checks\")  \r\nval completeVerificationResult = VerificationSuite.runOnAggregatedStates(  \r\n    validDf.schema,  \r\n    completeChecks,  \r\n    if (aggregateStates) Seq(completeStateProvider, incrementalStateProvider)  \r\n    else Seq(incrementalStateProvider),  \r\n    saveStatesWith = Some(completeStateProvider)  \r\n)\r\nif (completeVerificationResult.status == CheckStatus.Error) {  \r\n    logger.error(s\"Complete data quality checks failed. Results: ${completeVerificationResult.checkResults}\")  \r\n    sys.exit(1)  \r\n}\n\nThis feature provides granular control over metric computation and therefore supports a series of implementations. For instance, you may choose to save the state only when the entire pipeline succeeds, or you may want to perform anomaly detection on the complete dataset.\n8. Gracefully Handling Data Changes\nWhen working with external data sources, it\u2019s common for changes to occur, which can lead to failed checks if not properly handled. To ensure backward compatibility and smooth data processing, there are two options you can consider:\nFilterable Constraint Checks: these are conditional checks that are only executed if a specific condition is satisfied, such as when the input data is from an older dataset version. This allows you to accommodate changes in the data structure while still maintaining compatibility.\n\nval checks = Seq(  \r\n    Check(CheckLevel.Error, \"Sales base checks\")\r\n        ...,\r\n    Check(CheckLevel.Error, \"Legacy checks\")\r\n        .hasPattern(\"Date\", \"^[0-9]{2}/[0-9]{2}/[0-9]{4}$\".r)\r\n        .where(\"year < 2022\")\r\n)\n\nSplitting by Data Version: Unfortunately, conditional checks are not applicable for schema checks. Cases such as column addition or deletion need to be addressed separately. In such cases, it\u2019s recommended to keep your data versions close at hand and use them as a discriminator to run various checks for different versions. Splitting by version enables you to granular control the checks, while still keeping the code reusability.\n9. Anomaly Detection\nAnomaly detection is a crucial aspect of testing data quality that helps identify unexpected or unusual patterns in the data based on historical observations. Deequ provides several anomaly detection strategies that can be applied to different aspects of the data.\nBefore applying anomaly detection, it is important to store the metrics in a persistent repository. This ensures that historical metrics are available for comparison and trend analysis. In the code snippet below, we use a FileSystemMetricsRepository to store the metrics in a file system location:\n\nval metricsRepository: MetricsRepository =\r\n      FileSystemMetricsRepository(spark, s\"${args(\"metrics_path\")}/metrics_repository.json\")\r\n\r\nvar verificationSuite = VerificationSuite()  \r\n    ... \r\n    .useRepository(metricsRepository)\r\n    ...\n\nOnce at least one data point is collected and stored in the metrics repository, we can apply anomaly detection strategies.\nOne useful application of anomaly detection is keeping the data volume in check. If your dataset is expected to grow at a predictable pace or remain stationary, you can add anomaly detection on the row count. This helps identify unexpected changes introduced by external systems or transformation scripts.\n\nvar verificationSuite = VerificationSuite()  \r\n    ... \r\n    .addAnomalyCheck(\r\n        RelativeRateOfChangeStrategy(maxRateIncrease = Some(2.0)),\r\n        Size(),\r\n        Some(AnomalyCheckConfig(\r\n            CheckLevel.Warning,\r\n            \"Dataset doubling or halving is anomalous\"\r\n        ))\r\n    )\r\n    ...\n\nSimilarly, anomaly detection can be applied to specific columns where you know the expected distribution or behavior of the data.\nWhen an anomaly is found, you can handle it based on the severity of the issue. If the anomaly is critical, you can stop the pipeline to avoid propagating the issue further, or if the anomaly is not severe, you can emit a warning to your monitoring systems for further investigation.\nBy incorporating anomaly detection into your data quality checks, you can proactively identify and address unexpected or unusual patterns in your data, ensuring the quality and reliability of your data pipelines.\n10. Conclusion\nIn this blog, we set up a data quality checking solution for our Spark ETL pipeline by incorporating the open-source library Deequ. We discussed how to use Deequ for schema checking, data profiling, quality constraints testing, quality metric collection, and anomaly detection.\nIf you prefer writing scripts in Python (i.e., PySpark), then PyDeequ can help, which is a Python library for Deequ. At the time of writing this blog, this library is a bit behind and doesn\u2019t yet support some features we discussed.\nCheck out the first part of this blog series \u201cIntroduction to Data Quality\u201d if you haven\u2019t yet. It gives you ideas on how to implement your data quality checks.\nMore Resources\n\nTest data quality at scale with Deequ | AWS Big Data Blog\nBuilding a serverless data quality and analysis framework with Deequ and AWS Glue | AWS Big Data Blog\nAutomating Large-Scale Data Quality Verification \u2013 Original Deequ Technical Paper\nAWS is currently building an integrated solution for data quality checking in AWS Glue. It still lacks many features of Deequ, but it is worth keeping an eye on this one, as it is in active development.\n\nData Quality \u2013 AWS Glue Data Quality\u2013 Amazon Web Services\n\n\nSee more examples on Deequ GitHub page\n\n", "tags": ["Data Quality"], "categories": ["Blog", "Data"]}
{"post_id": 38158, "title": "Using Machine Learning to Improve Search \u2013 Part 1 of 5", "url": "https://www.luminis.eu/blog-en/using-machine-learning-to-improve-search-part-1-of-5/", "updated_at": "2023-06-16T09:35:52", "body": "\nLarge Language Models and Generative AI\nMachine learning (ML) is a powerful tool to optimize search engines and natural language processing (NLP) capabilities. Using ML can result in more accurate and contextually relevant search results. ML algorithms can analyze vast amounts of data, including user queries, search patterns, and content, to improve search rankings and understand user intent. Another thing you can achieve with ML is searching images with text or even extracting information from images to enrich data.\nI recently started exploring the fascinating area of combining ML with search and decided to write blog posts to explain the possibilities.This first blog post is part of a series: Using Machine Learning to improve search. It consists of five parts. In this first part, I present how to leverage large language models and Generative AI to improve search.\nWhen integrated with search systems, NLP techniques improve the search experience by understanding and responding to user queries in a more sophisticated and context-aware manner. Large language models, such as GPT-4 from OpenAI, represent a significant breakthrough in the field of NLP. These models are designed to understand and generate human-like text by learning patterns and structures from vast amounts of training data.\nShort Introduction to LLMs\nLarge language models are advanced artificial intelligence systems designed to process and generate human-like text. They are built using deep learning techniques and consist of millions or even billions of parameters. These models are trained on massive amounts of text data to learn language patterns, grammar, and context. Once trained, they can perform a wide range of language-related tasks. This includes text completion, summarization, translation, and question-answering. LLMs have the ability to generate coherent and contextually relevant responses, mimicking human language with impressive fluency. Some of these cool things can also be applied to improve the search experience.\nIf you want to know more about LLMs, a lot has been written about it lately. I found this website helpful. It contains a lot of information about machine learning, and the particular page linked explains large language models.\nWays to Use LLMs to Improve Search\nReranking\nAfter obtaining the initial list of documents, we can further improve the search results using a technique called reranking. Reranking involves taking the initial list of documents, generally produced by a more straightforward retrieval model, and applying a more sophisticated model to reorder the documents based on relevance. In this context, the LLM can be used to understand the semantic and contextual relevance of each document to the query and reorder the documents accordingly.\nI recently came across a research paper where the OpenAI GPT-4 model was benchmarked against other reranking models and it seemed to have outperformed them in most areas. This means LLMs have real potential for reranking. If you\u2019re interested in this research, the paper can be found here. There are downsides to using OpenAI or similar APIs depending on the use case, like latency and costs. More on this I explain in the caveats section below.\nThis is a visual representation of how reranking can be implemented.\n\nMatch Explanation\nA search engine works by examining the terms you entered and then comparing these to its index. The engine uses complex algorithms to determine the relevance of each indexed document to your search terms. Sometimes it is unclear to the user why a specific result matched the query, for example, if the result doesn\u2019t contain any search terms. Understanding why a particular search result was ranked highly or presented to the user is crucial for building trust and providing transparent search experiences.\nWhen given a search query and a search result, a LLM can analyze the text of the query and the result, drawing on its extensive training data to identify the likely relevant features. Based on my experience so far, LLM can point out that the result contains many instances of the search terms, that the terms appear in important places like the title or first paragraph, or that the result\u2019s content is closely related to topics associated with the search terms.\nImportant here is how you phrase the prompt (prompt engineering). The way you frame a prompt can guide the model\u2019s responses in terms of length, detail, tone, context, or subject matter. A well-crafted prompt can help the model provide more useful and relevant responses. Because the user\u2019s screen is limited and you want to avoid long texts on your result page, you should instruct the model in such a matter that it returns a short and clear explanation.\nAnother option is to generate a list of the most semantically similar sentences in the document to the query and show these to the user.\nI\u2019ve created some sample code for you to check out and to easily try out match explanation.\nRelevancy Judgement Assistance\nA topic within the search community that is currently being discussed and experimented with is using LLMs to assist with relevancy judgments. Relevancy judgments are used in the field of information retrieval and search engines to evaluate and improve search results. This involves assessing the relevance of the results returned by a search engine in response to a particular query.\nUsually, a certain scale is used to rate the relevance. For instance, a binary scale (relevant, not relevant) or a multilevel scale (highly relevant, somewhat relevant, not relevant) might be employed. In many cases, these judgments are done by human reviewers who manually evaluate the relevance of each search result to the original query. This can be a time-consuming and costly process, but it is often necessary for optimizing the accuracy of search results.\nThis research paper explains automating this process with the use of LLMs. While completely automating this is not ready yet, LLMs can already be used to assist human reviewers with their judgments. Human reviewers struggle to see a pertinent connection when they are lacking world knowledge. LLMs can generate rationales that can explain such connections, similar to the match explanation above.\nContent Enrichment\nThis might not seem search related, but the opposite is true. Quality content is at the heart of every good search experience. This is why I also wanted to mention this part.\nGenerative AI models have the capability to generate high-quality content that can significantly enhance data enrichment. For instance, these models can generate pertinent summaries, comprehensive product descriptions, or contextual details for search queries. This offers considerable value when managing product fact sheets for example. The created summaries and descriptions can subsequently be employed in search functions.\nMoreover, generative AI models can offer assistance by expanding on existing content and synthesizing additional paragraphs, examples, or detailed explanations pertinent to a specific topic. This helps users understand better by providing them with lots of information and context, which expands their knowledge of the topic. Importantly, this wealth of information can also be incorporated into search operations.\nAs with match explanation, prompt engineering is important here too. In my experience, it\u2019s even more important in this part because your content should match the tone of voice of the company and should fit in the data structure (e.g. not too short or too long).\nEmbeddings\nOne of the key challenges in search is understanding the context and intent behind a user\u2019s query. Vector search is a technique used to find similar items based on their vector representations in a high-dimensional space. In the context of NLP, embeddings are vector representations of words, phrases, or documents. These embeddings capture semantic and contextual information. This allows similar items to be represented, as vectors are closer together in the embedding space.\nEmbeddings are typically created by ML models which are trained on datasets of specific domains. LLMs, on the other hand, are trained on large text corpora, enabling them to develop a robust understanding of language and context. This means they are often better at creating more meaningful and contextually aware embeddings than traditional models, resulting in more accurate search results.\nThat being said, it\u2019s important to note that the effectiveness of using LLMs for creating search embeddings depends on the specific use case and dataset. In some cases, simpler or more traditional models might perform just as well or even better. Especially when computational resources or data are limited or your data is niche.\nAn easy way to combine LLMs with vector search is to use the LangChain framework. My colleague Jettro has made a blog post about that, you can read it here.\nIn Part 2 of this blog series, I dive deeper into vector search and embeddings.\nCaveats\nThis all sounds very cool, but I feel it is important to also mention some things that should be taken into account before applying any of the above.\n\nOverfitting and irrelevant information: While the LLMs are designed to respond based on patterns it has observed during their training, they can occasionally generate outputs that include irrelevant or inaccurate information due to over-generalizing from the data it was trained on.\nNot up to date: LLMs were not designed for real-time learning or updating their knowledge. They are trained on a static dataset and do not have the ability to learn new information after training. This means they might not have information on recent events or developments.\nData privacy: There could be potential privacy concerns if a search engine built on an LLM is not designed with strong data privacy protections. Users would need to be assured that their queries are handled confidentially, and that the system isn\u2019t retaining or learning from their personal data.\nNiche data: LLMs can struggle with highly specialized terminology or context, as their understanding is based on patterns they\u2019ve observed in their training data. If a niche topic has unique contexts that were not adequately represented in the training data, the model might not respond accurately.\nLatency: Larger models require more computational power to process and generate responses. This can lead to longer response times, especially if the model is not optimized or if hardware resources are limited. The length of the generated responses and the number of requests the model has to handle concurrently can impact latency too. Longer responses take more time to generate and transmit. If a model serves a high volume of queries simultaneously, response times may increase.\nCost: If you\u2019re using a closed-source model, you probably have to pay for each request sent to the API wrapped around the model. Larger and more comprehensive models tend to be more expensive. Experiment with different models to see which one aligns best with your requirements, and whether it justifies the associated expenses.\n\nFinal Thoughts\nThe examples above are just a few of the capabilities of LLMs. Development in this area is progressing rapidly. Each day, these tools become more efficient, accurate, and easier to implement, signaling a transformative shift in search mechanisms.\nHowever, there are caveats, but I firmly believe with rigorous experimentation and refinement, we can navigate these hurdles. It\u2019s essential to know that LLMs are not futuristic constructs. They are here, now, and accessible for everyone to use. Now is the time to start experimenting!\n", "tags": ["Generative AI", "Large Language Models", "Machine Learning", "search"], "categories": ["Blog", "Search"]}
{"post_id": 38233, "title": "Sustainable Clouds: interview with Software Architect Steef", "url": "https://www.luminis.eu/blog-en/de-duurzame-cloud-interview-met-software-architect-steef/", "updated_at": "2024-03-13T13:09:03", "body": "Luminis helps organizations innovate successfully by empowering them with modern software technologies. However, we cannot separate the work we do from the impact we have on the world. We are working on that collectively in various ways, both in terms of social engagement, sustainability, and diversity & inclusion. In this article, we dive into the sustainability of IT architectures.\nWith the expertise we have, we can and want to work hard for a better world. We like to give our colleagues the freedom to contribute their own ideas and projects. For this interview, we spoke with Steef Burghouts about his experiences and ambitions regarding software development & sustainability.\n\nSteef is a Software Architect at Luminis. Two years ago, he started with Accelerate, a training program to fast-track the tech leaders of the future. During this program, his ambition to make social impact was sparked. Since then, Steef has a variety of interesting projects to his name. His interest lies in sustainability and climate in combination with technology.\nClimate change issues in IT\nSteef\u2019s interest in social impact projects has grown over the past two years. When asked why he has a greater focus on this, he answered:\n\u201cAt the beginning of my career, I mostly went from project to project, without really thinking about the subsequent impact of what I was working on. During the Accelerate program, you work on your personal growth. Part of this includes conversations with coaches who ask questions about your ambitions and dreams. I realized that I also want to commit myself to social projects, for example concerning climate change issues. Climate change is one of the biggest challenges of our generation. I like to be part of the solution.\u201d\nSteef\u2019s mission? To create awareness about the CO2 emissions of IT and the effect it has on the climate. The CO2 emissions of the IT industry are less visible and less measurable than for example aviation. As a result, IT\u2019s impact on the climate sometimes doesn\u2019t get enough attention during climate discussions. However, research indicates that the global IT industry is responsible for two to four percent of global CO2 emissions. By 2040, it is estimated to be between six and 14 percent.[1] This amount is comparable to the total emissions from aviation, so it is about time for some changes.\nOne reason that the IT has such high CO2 emissions is because of the continuous running of entire IT landscapes. This is where a major gain can be made, both in terms of CO2 emissions and financially. To achieve this, Steef is working on a tool for mapping the climate impact of IT architectures. Within IT, security gets attention in almost every project and conversation, so why not talk more often about sustainability?\nObviously, it\u2019s easier said than done: starting the conversation with each client about sustainability and the impact of their IT landscape. This takes time and money. Fortunately, we have more colleagues like Steef who see the importance of this development. They test different methods during their work with customers, with the objective to pragmatically come up with a new approach.\nMore impact using smart systems at Milieudefensie\nIn 2022, Steef introduced Milieudefensie (Dutch for \u201cEnvironmental Defense\u201d) as a customer to Luminis. Milieudefensie has a big impact on the climate policy of the Netherlands, for example by filing lawsuits against big players like Shell. For these lawsuits, Milieudefensie makes many WOO (Wet Open Overheid, Dutch for \u201cOpen Government Act\u201d requests to the Dutch government. Milieudefensie receives the answers to these requests as images, of which certain parts cannot be shared publicly and are taped off. The requests sometimes consist of thousands of pages of images, which are not easily searchable. Milieudefensie had to search these requests page-by-page, which took an awful lot of time.\nWhat have our colleagues done for Environmental Defense? Luminis developed a document system that converts the images of WOO requests into text and makes them searchable. This makes it possible to search through the thousands of pages of images in a targeted manner through which useful information is found sooner. Milieudefensie saves more than 1 FTE per month by doing this. They can now use this time to make even more impact.\nWhat Luminis has done for Milieudefensie is in line with the role we want to play more often. Milieudefensie showed us the value of making an impact. Steef explains why he finds this customer so important:\n\u201cI think Milieudefensie is a good organization because they are constantly looking very pragmatically at how they can make an impact. Moreover, it is very clever how they have filed and won various cases against, for example, the Dutch government and powerful companies. I sent Milieudefensie an email and asked what their challenges are and how we can help them. Their focus is not on IT, but good IT facilities do make their work easier. So it was a natural choice to use our knowledge and expertise there.\u201d\nA tool for every customer: the Cloud Cleanup\n\nSustainability and climate chance are present in Steef\u2019s work every day. To start the conversation about sustainability with all our customers, he is now developing the Cloud Cleanup. Its purpose is to map the emissions of an IT landscape and offer suggestions on how to reduce them. Currently, IT emissions are not included in calculating an organization\u2019s carbon footprint. However, measuring and knowing emissions is essential for creating awareness to reduce emissions step-by-step.\nThe question is why should our customers want this? Research shows that it is sometimes difficult to consider sustainability when making technology decisions.[1] All too often the emphasis is still on speed or continuity. Because security and reliability are paramount, overcapacity is often built in, for example by using extra servers. This not only costs a lot of money, but also causes extra CO2 emissions. As part of the Climate Agreement, the Dutch IT sector has stated its ambition to be climate neutral by 2030. An ambitious goal, to which everyone must contribute: from organizations and suppliers to developers.\nTherefore, at Luminis, we are busy testing and further developing the Cloud Cleanup. We started this initiative in 2022, with which we also mapped our own emissions. Something we also like to make possible for our customers. Steef explains how the Cloud Cleanup works:\n\u201cThe first step is a conversation with the customer to see what their IT landscape currently looks like. Once that is mapped out, we calculate the carbon footprint of that IT landscape. The next step is figuring out how this can be reduced using tools from cloud providers like AWS and Azure.\u201d\nA tangible example of applying the Cloud Cleanup is analyzing an organization\u2019s running servers. It is common knowledge that many companies, under the pretense of continuity and availability, regularly operate with too many or too heavy servers. A less heavy server is a more sustainable solution in terms of climate as well as financially. However, you need to know the customer very well to advise on this.\n\u201cA company like Ticketmaster needs very heavy servers at times, but not when there are no tickets being sold. One solution could be to deploy additional servers at peak times and switch back at quiet times.\u201d\nThese kinds of insights and desires are included in the Cloud Cleanup and ultimately provide each client with appropriate advice. Curious about the Cloud Cleanup? Contact Steef Burghouts at <steef.burghouts@luminis.eu>.\n[1] ABN-AMRO. 2021. Verduurzaming van IT.\n", "tags": [], "categories": ["Blog", "Cloud"]}
{"post_id": 38195, "title": "Accelerate Craftsmanship, opleidingstraject van Thales, De Belastingdienst en Luminis vliegend van start!", "url": "https://www.luminis.eu/blog-en/accelerate-craftsmanship-opleidingstraject-van-thales-de-belastingdienst-en-luminis-vliegend-van-start/", "updated_at": "2023-05-31T17:16:45", "body": "Apeldoorn, 31 mei 2023 \u2013 Op vliegveld Teuge start vandaag Accelerate Craftsmanship met een feestelijke kick-off. Dit opleidingstraject, waarvoor Thales, de Belastingdienst en Luminis de handen voor de derde keer ineenslaan is een op maat gesneden traject voor toptalenten van de drie organisaties. Het centrale thema is Software Craftsmanship, belicht vanuit zowel technisch inhoudelijk vlak maar vooral in combinatie met persoonlijke ontwikkeling.\n\n\u00a0\nWaarom Accelerate Craftsmanship?\nIn een wereld waarin software steeds belangrijker en zelfs onmisbaar wordt voor organisaties die zich willen onderscheiden van de concurrentie, zijn vakmensen nodig. Zij die de kennis en skills hebben om organisaties naar de toekomst te leiden. De hiervoor benodigde expertise ontwikkel je niet zomaar, daar zijn intensieve trainingen op gebied van technologie en soft skills voor nodig. Dat is precies wat Accelerate Craftsmanship biedt.\nIn samenwerking met How Company worden sessies gegeven over communicatie en persoonlijk leiderschap. Daarnaast komt ook technologie uitgebreid aan bod in sessies over onderwerpen als OO en clean coding principes, service design, refactoring, database ontwerp en best practices voor het ontwikkelen van veerkrachtige systemen en services. De combinatie van intensieve begeleiding vanuit coaches, de stuurgroep van het traject en How Company helpt deelnemers hun potentieel optimaal te benutten en hun persoonlijke doelstellingen te behalen.\n\u201cOndanks dat er geen compressiemechanisme voor ervaring bestaat, halen we in dit traject alles uit de kast om deelnemers in 10 maanden tijd een ontwikkeling te laten doormaken waarover ze anders jaren zouden doen. Met wat ze leren in Accelerate worden deelnemers niet alleen betere engineers maar zullen ze ook vooral op persoonlijk vlak grote stappen zetten. Iets waar ze de rest van hun leven van kunnen profiteren.\u201d Vertelt Bert Ertman, VP Technology bij Luminis en initiatiefnemer van het Accelerate concept.\n\u00a0\nEen traject vol life-changing inzichten\nAccelerate Craftsmanship is de derde in een succesvolle reeks Accelerate-trajecten. Thales en Luminis startten in 2020 met dit initiatief om tech-talent uit hun eigen organisaties klaar te stomen voor een toekomst waarin software steeds belangrijker wordt. De voorgaande trajecten, waarin ook al werd samengewerkt met de Belastingdienst, vormen de basis van deze verkorte Accelerate versie (10 maanden in plaats van 18 maanden red.), waarin de meest relevante onderwerpen en sessies uit eerdere trajecten aan bod komen.\n\n\u00a0\nDe impact van Accelerate op deelnemers is groot. De intensiviteit en vorm van het traject bieden deelnemers bijzondere groeikansen en inzichten op persoonlijk ontwikkelvlak. Zowel op werkgebied als priv\u00e9 heeft Accelerate oud-deelnemers veel gebracht. De deelnemers vervullen nu soms een rol als coach in nieuwe trajecten, of zijn doorgestroomd naar invloedrijke posities binnen de deelnemende organisaties.\nMarleen Hurenkamp, Productmanager vakontwikkeling IV bij de Belastingdienst vertelt over hun deelname aan Accelerate: \u201cIn het kader van de strategische doelstelling van de directie Informatievoorziening van de Belastingdienst om te gaan behoren tot een van de top IT werkgevers geven we onze medewerkers alle ruimte om bezig te zijn met hun ontwikkeling om zo te groeien in hun vakmanschap, zowel op inhoud als op vaardigheden en competenties. We willen talent laten bloeien. Het Accelerate-programma in de variant van Craftmanschap voor onze medioren past naadloos bij deze doelstelling en geeft onze talentvolle software engineers de stuwkracht en ruimte om versneld door te groeien tot het niveau van een senior.\u201d\nEn ook voor mede-initiatiefnemer Thales biedt dit traject veel, aldus Henk van Steeg, Head Software Engineering bij Thales: \u201cThales heeft de wind mee, we groeien enorm. Naast het aantrekken van veel nieuwe mensen investeren wij ook fors in de groei van onze collega\u2019s. We hebben interne trainingsprogramma\u2019s en werken intensief samen met de andere partijen uit het Accelerate-traject. Dit stelt onze mensen in staat hun kennis te verbreden en te sparren met peers.\u201d\nWe wensen de deelnemers, coaches en stuurgroep veel succes en kijken uit naar een intensieve en leerzame samenwerking de komende 10 maanden.\nMeer weten over het Accelerate-programma of IT-trainingen? Bekijk de website van de Luminis Academy of neem contact op met Louis Pouwels, contactpersoon van de Luminis Academy (academy@luminis.eu).\n", "tags": [], "categories": ["Blog", "News"]}
{"post_id": 38179, "title": "Vector search using Langchain, Weaviate and OpenSearch", "url": "https://www.luminis.eu/blog-en/vector-search-using-langchain-weaviate-and-opensearch/", "updated_at": "2023-05-30T09:53:04", "body": "\nWith the popularity of ChatGPT and Large Language Models (LLM), everybody is talking about it. On my Linkedin home page, about 90% of the posts seem to speak about ChatGPT, AI, and LLM. With my experience in search solutions and interest in everything related to Natural Language Processing and Search, I also had to start working on solutions.\nA few weeks ago, I visited the Haystack conference in Charlottesville. Listened to good talks and had interesting conversations with like-minded people. I learned about a framework called Langchain. LangChain is a framework for developing applications powered by language models, making working with other products like vector databases and large language modes much more straightforward. OpenSearch and Weaviate are among the integrations which I know well. I decided to experiment with a similarity search using both products.\nThis blog post discusses the sample. It shows the different steps to accomplish the following task:\n\nRead content using a Langchain loader.\nStore data in OpenSearch and Weaviate using the Langchain VectorStore interface.\nPerform a similarity search using the Langchain VectorStore interface\nPrint the results, including the score used for sorting\n\nRunning the project yourself\nYou can find the source code of the project on GitHub:\ngit clone git@github.com:jettro/MyDataPipeline.git\r\ngit checkout blog-langchain-vectorstores\nYou need to set up Python to run the sample. Next to Python, you need a running OpenSearch instance. I have provided a docker-compose file in the infra folder. For Weaviate, I advise using a sandbox environment. You must also create a .env file in your project\u2019s root folder. You can use the env_template file as a template for your .env file.\npython3 -m venv .venv\r\nsource .venv/bin/activate\r\npip install -r requirements.txt\nNow you can run the file run_langchain_ro_vac.py, but before you do, change the properties do_load_content at the bottom to True. That way, you will load the content into OpenSearch and Weaviate. After one successful run, you can switch them back to False.\nLoad the content using LangChain.\nLangChain uses Loaders to fetch data. It comes with a lot of different loaders to load data from databases, csv files, remote json files. It does however not support remote XML out of the box. The dataset we are using is coming from the Dutch government. It contains frequently asked questions, called \u201cVraag Antwoord Combinaties\u201d. The default format is XML, you can request json if you want, but I stuck with XML. Therefore I had to create a customer XML loader. The code for the loader is in the code block below.\nimport xml.etree.ElementTree as ET\r\nfrom urllib.request import urlopen\r\n\r\nfrom langchain.document_loaders.base import BaseLoader\r\nfrom langchain.schema import Document\r\nclass CustomXMLLoader(BaseLoader):\r\n    def __init__(self, file_path: str, encoding: str = \"utf-8\"):\r\n        super().__init__()\r\n        self.file_path = file_path\r\n        self.encoding = encoding\r\n\r\n        def load(self) -> list[Document]:\r\n        with urlopen(self.file_path) as f:\r\n\r\n        tree = ET.parse(f)\r\n        root = tree.getroot()\r\n\r\n        docs = []\r\n        for document in root:\r\n            # Extract relevant data from the XML element\r\n            text = document.find(\"question\").text\r\n            metadata = {\"docid\": document.find(\"id\").text, \"dataurl\": document.find(\"dataurl\").text}\r\n            # Create a Document object with the extracted data\r\n            doc = Document(page_content=text, metadata=metadata)\r\n            # Append the Document object to the list of documents\r\n            docs.append(doc)\r\n\r\n        return docs\nBelow is the function that makes use of the XML loader. The specific VectorStore is passed into the method.\ndef load_content(vector_store: VectorStore) -> None:\r\n    custom_xml_loader = CustomXMLLoader(file_path=\"https://opendata.rijksoverheid.nl/v1/infotypes/faq?rows=200\")\r\n    docs = custom_xml_loader.load()\r\n\r\n    run_logging.info(\"Store the content\")\r\n    vector_store.add_documents(docs)\nBefore inserting documents, you have to provide the schema for the class. Weaviate does create a default schema for your content, but that does not work with similarity search. You need a field that stores embedded versions of the content. Since loading the schema has nothing to do with LangChain, I will not post all the code. Please look at the provided repository. I want to show you part of the schema that works for the LangChain similarity search. In the schema, I configure the class RijksoverheidVac and the field text, which is the default field for LangChain. We can change this if we want.\n{\r\n  \"class\": \"RijksoverheidVac\",\r\n  \"description\": \"Dit is een vraag voor de Rijksoverheid \",\r\n  \"vectorizer\": \"text2vec-openai\",\r\n  \"properties\": [\r\n    {\r\n      \"dataType\": [\r\n        \"text\"\r\n      ],\r\n      \"moduleConfig\": {\r\n        \"text2vec-openai\": {\"skip\": false, \"vectorizePropertyName\": false}\r\n      },\r\n      \"name\": \"text\"\r\n    }\r\n  ]\r\n}\nCreate Weaviate VectorStore and execute a similarity search\nNext, we can construct the Weaviate client, the Weaviate VectorStore, and the VectorStoreIndexWrapper. Notice in the code below that:\n\nI use a wrapper around the Weaviate client. This wrapper makes interacting with Weaviate easy. LangChain uses its\u2019 own wrapper within the VectorStore.\nUsing the parameter do_load_content, you can control a fresh load of the content.\nIn the additional field, we pass the field certainty (I added this feature in a pull request :-))\nI use the field certainty in the _additional field of metadata to print the Weaviate certainty, which is used as a score.\n\ndef run_weaviate(query: str = \"enter your query\", do_load_content: bool = False) -> None:\r\n    weaviate_client = WeaviateClient()\r\n    vector_store = Weaviate(\r\n        client=weaviate_client.client,\r\n        index_name=WEAVIATE_CLASS,\r\n        text_key=\"text\"\r\n    )\r\n\r\n    if do_load_content:\r\n        load_weaviate_schema(weaviate_client=weaviate_client)\r\n        load_content(vector_store=vector_store)\r\n\r\n    index = VectorStoreIndexWrapper(vectorstore=vector_store)\r\n    docs = index.vectorstore.similarity_search(\r\n        query=query,\r\n        search_distance=0.6,\r\n        additional=[\"certainty\"])\r\n\r\n    print(f\"\\nResults from: Weaviate\")\r\n    for doc in docs:\r\n        print(f\"{doc.metadata['_additional']['certainty']} - {doc.page_content}\")\r\n\r\n\nThis method gives you the power to do a similarity search against Weaviate.\nCreate OpenSearch VectorStore and execute similarity search.\nNext, I show you that working with OpenSearch is similar to Weaviate. It is not the same, but similar. Managing the index does work from LangChain. So there is no need to create it by ourselves. The following code block should now be self-explanatory. Notice that we have to provide our own embedding here. Weaviate uses the schema to determine how to create the embeddings. With OpenSearch, we provided our own embeddings. In the end, we use OpenAI for both embeddings.\ndef run_opensearch(query: str = \"enter your query\", do_load_content: bool = False) -> None:\r\n    auth = (os.getenv('OS_USERNAME'), os.getenv('OS_PASSWORD'))\r\n    opensearch_client = OpenSearchClient()\r\n\r\n    vector_store = OpenSearchVectorSearch(\r\n        index_name=OPENSEARCH_INDEX,\r\n        embedding_function=OpenAIEmbeddings(openai_api_key=os.getenv('OPEN_AI_API_KEY')),\r\n        opensearch_url=\"https://localhost:9200\",\r\n        use_ssl=True,\r\n        verify_certs=False,\r\n        ssl_show_warn=False,\r\n        http_auth=auth\r\n    )\r\n\r\n    if do_load_content:\r\n        opensearch_client.delete_index(OPENSEARCH_INDEX)\r\n        load_content(vector_store=vector_store)\r\n\r\n    docs = vector_store.similarity_search_with_score(query=query)\r\n    print(f\"\\nResults from: OpenSearch\")\r\n    for doc, _score in docs:\r\n    print(f\"{_score} - {doc.page_content}\")\r\n\r\n\nTesting the integration\nNow you can test the example. You should see similar results using the code below.\nif __name__ == '__main__':\r\n    run_logging.info(\"Starting the script Langchain Rijksoverheid Vraag Antwoord Combinaties\")\r\n\r\n    query_str = \"mag ik een groen zwaai licht\"\r\n\r\n    run_weaviate(query=query_str,\r\n                 do_load_content=False)\r\n\r\n    run_opensearch(query=query_str,\r\n                   do_load_content=False)\r\n\r\n\nThis is the output.\nResults from: Weaviate\r\n0.9312953054904938 - Mag ik een zwaailicht of een sirene gebruiken op mijn auto?\r\n0.9251135289669037 - Hoe kan ik mijn duurzame initiatief voor een Green Deal aanmelden?\r\n0.9233253002166748 - Wanneer moet ik mijn autoverlichting gebruiken?\r\n0.9228493869304657 - Wat is groen sparen of beleggen?\r\n\r\nResults from: OpenSearch\r\n0.78848106 - Mag ik een zwaailicht of een sirene gebruiken op mijn auto?\r\n0.7636849 - Wat is groen sparen of beleggen?\r\n0.755817 - Hoe kan ik mijn duurzame initiatief voor een Green Deal aanmelden?\r\n0.75559974 - Wanneer moet ik mijn autoverlichting gebruiken?\nConcluding\nI hope you learned that it is not hard to start working with vector-based similarity search using OpenAI, LangChain, Weaviate, and OpenSearch. These are exciting times. We can improve search results using vector-based search. We can start using Hybrid Search\u2014more on these topics in another blog.\n", "tags": ["LangChain", "OpenAI", "OpenSearch", "weaviate"], "categories": ["Blog", "Search"]}
{"post_id": 38149, "title": "The Evolution of AWS from a Cloud-Native Development Perspective: Data, Sustainability, Builder Experience, and Serverlessification", "url": "https://www.luminis.eu/blog-en/the-evolution-of-aws-from-a-cloud-native-development-perspective-data-sustainability-builder-experience-and-serverlessification/", "updated_at": "2023-05-26T12:32:04", "body": "AWS doubles down on its initiatives announced at re:Invent 2021. Data, Sustainability, and a lower barrier to integrating services and onboarding developers seemed to be the recurring themes for AWS re: Invent 2022. Now that 5 months have passed, it seems like a good moment to re:Cap and evaluate what happened in the world of AWS post re:Invent \u201822.\nIn this blog post, we will walk you through the most important movements announced by AWS at re:Invent 2022 and discuss the impact on AWS cloud-native builders.\n\nLingering impressions\nLike the \u201821 edition, the \u201822 edition of the AWS annual conference didn\u2019t feature any game-changing announcements, but quite some very nice enhancements on existing services. Next to that, AWS seems to pivot into developing verticals: Services that cover a specific industry. Verticals can be really beneficial to customers that don\u2019t have the capital to invest in developing their own solution or don\u2019t want to be distracted by developing services that are not their core business. However, the need to spin up the innovation flywheel to find product-market-fits is more important than ever, because with the announcement of AWS Supply Chain, AWS has proven that it can and will take over your commodity business. The AWS cloud is maturing at a pace that could make a lot of businesses irrelevant.\nNowadays we have many ways to store data in the cloud. The next step is to have tools and knowledge to make use of this data. AWS is focusing on this next step with their announcements at re:Invent 2022.\nA lot of effort is being put into improving the builder experience. AWS wants you to be able to build as much as you can as fast as possible on their platform, whoever you might be. It does not matter if you are a seasoned developer or have absolutely no experience, there is an entry point for you. Or at least, that is the goal.\nMaking more use of the term \u2018serverless\u2019 has also been on the agenda of AWS since 2021. We have seen new serverless versions of existing services arrive, even though not all of them can scale down to 0 and some have even an entry cost. While on the one hand, it is always nice to have options, the other side of the story is that we need to have a discussion about what the term serverless means nowadays.\nAnother topic that AWS builds upon from the previous year is sustainability. Where in 2021 the major announcement was the addition of the sustainability pillar to the Well-Architected Framework, this year it is about concrete ways of having your workload consume less energy and also a promise to be water positive by 2023.\nIn the rest of this blog, we will dive deeper into these topics.\n\nBuilding a future-proof data foundation\nBesides the several announcements of new services like Amazon DataZone or enhancements of existing services like\u00a0 AWS Glue Data Quality, AWS has come to realize that education is necessary for us to make sure all the data we have available is put to work.\nInstead of collecting dust inside our data warehouses, AWS wants to build a future-proof data foundation by educating current and future generations and providing them with the skills necessary to leverage all the information that is hidden in the vast mountains of data. They provide over 150 courses online courses, low-code, and no-code tools to enable people to become data literate.\nServices that don\u2019t incur the need for re-architecting your system or add technical debt when future requirements change are key characteristics of the announcements done regarding data-related services. One concrete example of this is easier integration of services to prevent unnecessary moving around of data like Amazon red shift auto-copy from s3.\n\nOngoing serverlessification\nRelated to this is the ongoing serverlessification of the AWS services. Originally serverless was used only for services that utilized a pay-as-you-go model, however, AWS seems to move away from this paradigm with the announcement of other \u2018serverless\u2019 services.\nAn example is Amazon OpenSearch Serverless. The serverless nature of this service is debatable since you already have a hefty bill of $700 for only enabling it. Not really pay-as-you-go anymore, is it? However, AWS seems to be serious about the serverlessification of the AWS services. Last re:Invent several enhancements of services were announced that provide a higher level of abstraction onto services to lower the barrier-of-entry (Amazon OpenSearch Serverless), make it more cost-efficient (efforts around a zero-ETL world), or remove the necessity of makeshift solutions to bend services to your business needs (Amazon EventBridge Pipes).\nAWS is changing the meaning of serverless to \u2018a service that entails a higher abstraction\u2019 so that customers don\u2019t have to be bothered with all kinds of technical stuff under the hood. More focus on adding value for customers, faster time to market, and less distraction caused by implementing and managing commodities.\n\nA more efficient AWS builder\nAs we mentioned last year in our recap of re:Invent, AWS aims to improve business agility for their users by reducing complexity.\nLooking at the announcements made during pre:Invent and re:Ivent it becomes clear that AWS still tries to remove as much of the undifferentiated heavy lifting done by builders on their platform. Every line of code can be considered a liability and if AWS can take care of a piece of (glue) code for us, I would always opt for that. As a developer or engineer you just want to get business value out to the customer as soon as possible and it clearly shows that AWS is listening to the developers using their platform. The announcements of Eventbridge pipes, the new filtering SNS payloads feature and more advanced filtering capabilities in Amazon Eventbridge are an example of that and will reduce the amount of code being written by developers while working on the AWS platform. This in turn will let the builders focus more on their actual business logic.\nAnother great example of this is a recent improvement in AWS Lambda. From our experience, a lot of enterprises are standardized on Java and/or .Net, and as more and more companies are migrating their workloads to the cloud we see that Java developers are also trying to adopt new compute models. We sometimes see some of these Java and .Net developers skip services like AWS Lambda and directly go running their application in containers due to the fact that Java-based applications suffer from slower start times a.k.a. \u2018cold start\u2019 issues. This is mainly due to the fact that Java is a statically compiled language and the JVM and popular frameworks like Spring require more memory and take seconds before being able to serve traffic compared to dynamic interpreted languages like Python or Javascript.\nWithin the Java ecosystem, \u2018new\u2019 frameworks like Quarkus and Micronaut aim to solve some of these issues and also be more lightweight. These frameworks tightly integrate with new innovations like GraalVM, which compiles the applications to native code, resulting in faster startup times and a much lower memory footprint for Java-based applications.\nHowever, learning new frameworks like Quarkus, Micronaut or GraalVM takes time and it takes time away from delivering business value. To help builders spend more time on building on the platform instead of for the platform, AWS introduced AWS Lambda Snapstart. With Lambda Snapstart you can get up to 10x improvement on your Java Lambda cold start. SnapStart takes a memory snapshot of the state of the JVM after it has launched and initialized. Lambda Snapstart is a clear example of what can be done if you own the entire stack. It makes use of the snapshot functionality of the underlying FireCraker VM and a checkpoint and restore feature of the JVM.\n\nOne other change AWS seems to make is taking a suite approach in supporting builders during the entire software development lifecycle (SDLC), from feature request to deployment. Up until now, Amazon offered a lot of separate tools (CodeCommit, CodeBuild, CodePipeline, etc) that could be combined to support developers, but some important parts were missing and teams had to find those outside the AWS environment. We see that bigger enterprises have adopted suite-based solutions like (Microsoft) Github + Github Actions and Azure DevOps. With the announcement of Amazon CodeCatalyst that might be about to change for organizations working extensively with AWS. Amazon CodeCatalyst is a product offered outside of the well-known AWS Console as a standalone product. It allows you to integrate or connect to familiar products like JIRA and GitHub and the syntax for pipelines is based on the GitHub actions workflow model. This is an interesting move and we hope to see the product evolve over the next couple of years.\nOne Cloud for All\nWhen you walk around re:Invent or even watch some of the sessions online you will notice the word \u201cbuilder\u201d come up a lot. Not engineers, not developers, but builders. With the maturity of the foundation of the AWS platform, one focus of AWS has been to lower the entry barrier for building solutions on AWS and increase the target audience to extend beyond the technical-apt.\nWe have seen this previously with tools like AWS CDK and AWS Amplify, where the complexity of AWS CloudFormation has been abstracted away behind a more user-friendly and user-specific layer. The latter is more prevalent when looking at AWS Amplify, which caters specifically to frontend engineers that want to build AWS solutions, allowing them to quickly spin up an AWS backend and easily hook it up to their frontend framework of choice without having to know too much about what is going on under the hood.\nHowever, these tools seem to be only the first stop down this avenue that AWS is moving through. The steps AWS is making indicate a goal of getting everyone and their parents to be able to build AWS solutions. This strategy makes sense from a competitive point of view. Making AWS a household name where the creatives of the world can easily bring their ideas to life will introduce a new revenue stream previously blocked by the technology barrier.\nOnce building AWS solutions become \u201ceasy\u201d enough, it will allow a new wave of disruptors to reach the market with a development speed and cost-effectiveness never seen before. And if they are going to change the world anyway, why not have it all run on the AWS infrastructure? Going this route will be a win-win for AWS and the innovators of tomorrow.\nOne of the new announcements this year at AWS re:Invent is the launch of AWS Application Composer, which allows you to visually design serverless applications, including APIs, background batch processing, and event-driven workloads. You can focus on what you want to build and let Application Composer handle the how. While this launch moves AWS closer to its world domination goals, because compared to CDK you don\u2019t even need to code anymore, it still requires a knowledge of the building blocks to be able to create a working and useful solution. Nevertheless, this is a step in the right direction.\n\nAnother development in this area is the announcement of AWS Code Whisperer, which will allow users to generate code by simply writing something similar to \u201cgenerate a function that uploads a file to s3 using server-side encryption\u201d. No doubt an answer to GitHub\u2019s (Microsoft) Co-Pilot, it brings AI into the mix to assist the AWS Builder in creating their solutions.\nWhile Code Whisperer requires even more low-level knowledge than App Composer to get your solution running, the interesting property of Code Whisperer is the use of natural language as the human interface. We have seen what the combination of natural language and AI can do with the rise of ChatGPT. If this is the way people are going to \u201cdo things\u201d moving forward, it would be in the best interest of AWS to jump on the bandwagon and provide a ChatGPT-like interface to build AWS solutions. We have no doubt this is already being looked at.\n\nAll cloud for one\nAWS is not only targeting \u201ceveryone\u201d, they are also targeting \u201cyou specifically\u201d. AWS has released high-level services over the years that cater to very specific/vertical use cases. One example is AWS Connect, which was featured in one of the customer\u2019s highlights during a keynote this year. AWS Connect is an AWS service specifically designed to improve customer service by adding a cloud contact center and using AI to improve agent efficiency.\nA new service of this type announced this year is AWS Supply Chain, which is an AWS-managed service that, as the name implies, helps you manage your supply chain by leveraging AWS services and machine learning to provide visibility and help make decisions. Amazon as an e-commerce company has many years of experience in this domain, so it stands to reason that they have packaged many relevant best practices into this service. This is not only AWS providing a service, it is also them sharing their knowledge through this service.\n\nContinuing this trend of offering functionality derived from their own experience, AWS has released AWS Ambit scenario designer, an open-source set of tools to create 3D content. AWS has been functioning as a game studio for the last 10 years. With the release of this toolset, this again focuses on a completely new set of builders.\nWhat these developments aim to do is invite new builders to AWS that are trying to achieve very specific goals. AWS will offer them a user-friendly, end-to-end solution to do this, backed by the power of the AWS cloud. These managed services make it possible for users with minimal AWS knowledge to get things up and running for their vertical use cases. However, in our experience, as soon as you want to extend beyond what the managed service has to offer or if you should run into an error, a more advanced level of knowledge becomes necessary, where the gap in knowledge between the two scenarios is unexpectedly large. This is where the AWS expert builders come into play and why having a deeper level of AWS knowledge will be beneficial.\nWe expect AWS to continue this trend and announce even more of these vertical services related to their own expertise. One of their key areas of focus is sustainability. They have pledged to become \u201cwater-positive\u201d by 2030. It wouldn\u2019t surprise us to see a service appear down the line that allows you to receive actionable insights about your water and energy usage for your facilities, thus helping other companies also become more sustainable.\n\u00a0\n\n\u00a0\nBeing more sustainable\nA topic that is hot and happening: Sustainability and future-proof computing. Something we cannot ignore, given the ongoing change of our climate and all the coverage it gets in the media.\nThis is not a new topic for AWS, as last year they released the Carbon footprint tool and introduced the sustainability pillar to name a few actions.\nOne of the causes of this climate change can be attributed to our energy hunger and seemingly lack of efficiency in producing and consuming that energy. And let\u2019s be fair: Our electronics have become more and more efficient over the last decades, but our demand also grew exponentially. The computing power of my iPhone has increased significantly compared to one from 2013, while my battery doesn\u2019t last any longer. While efforts to increase power efficiency remain, we don\u2019t back off on the demand of workloads.\nThis re:Invent AWS announced several improvements in performance and efficiency: Smaller-sized hardware or compute resources performing the same workload, which results in less energy consumption.\nAWS\u2019 driver to being more sustainable seems uncertain, however: it could be caused by laws or the expectation that laws regarding sustainability will be created in the near future, or it\u2019s just marketing (greenwashing). Regardless of the reason that drives them, AWS seems to be serious about this, because of the number of announcements that have some relation with sustainability.\nManaged services increase the sustainability of your cloud workloads\nAWS is heavily advising customers to use managed services over EC2 instances. Besides the fact that managed services increase business agility and make experimentation easier and cheaper, it also contributes to more sustainable workloads: AWS has a strong driver to increase the efficiency of their services to be able to handle more workloads on equal or less hardware. So, designing and building your workloads in AWS using managed services automatically contributes to a more sustainable architecture. This is nicely pointed out in the shared responsibility model of the sustainability pillar in the well-architected framework.\nZero ETL\nIn this keynote, Adam Selipsky promised a \u2018zero ETL\u2019 future. ETL stands for \u2018Extract, Transform, and Load\u2019 and is a data pipeline that is applied to almost all data warehouse ingestion flows. This seems a somewhat technical announcement, but don\u2019t be mistaken: Data pipelines are subject to require a lot of compute, let alone the labor to design and implement them, to get the data into shape before adding them into the destination data warehouse. So, a zero ETL future would be great. This re:Invent a new integration was announced that should be the premise of a zero ETL future: A managed way to ingest data from Aurora into Redshift, without the need for a separate data pipeline to setup and maintain.\nWhen you listen carefully to the keynotes of this reinvent, one can discover that a lot of efforts have been made to let people think that AWS is boarding the sustainability train, full throttle. It seems that everything they develop has somehow to do with improved efficiency, doing more with equal or fewer resources, and higher-level abstractions in the form of managed services.\nAs far as we are concerned this is a strategy AWS may double down on since it will be a win-win outcome.\nTake advantage of the future\nTo stay relevant is getting more and more important for businesses now that the cloud seems to be maturing and cloud vendors like AWS are starting to pivot into developing verticals. Before you know it AWS has taken over your business because it demoted your core business to commodity. There is no denying anymore that the cloud is hype and will pass when you look at the economic size of the cloud platform. Rather sooner than later It will look like AWS will surpass the $100 billion in revenue mark this year. So, get on that train if you haven\u2019t done so, and learn to leverage everything the cloud has to offer to speed up your business innovation to stay relevant and bring value to your customers! And if so, we\u2019d love to help!\n", "tags": [], "categories": ["Blog", "Cloud", "News"]}
{"post_id": 38138, "title": "Klantgericht werken (in een grote organisatie), hoe doe je dat?", "url": "https://www.luminis.eu/blog-en/klantgericht-werken-in-een-grote-organisatie-hoe-doe-je-dat/", "updated_at": "2023-09-04T14:33:56", "body": "In de media lees je met enige regelmaat over wat er niet goed gaat bij dienstverlening van grote overheidsinstanties zoals de Belastingdienst. Vanuit mijn rol als Digitaal Strateeg richt ik me op de dienstverlening richting burgers en bedrijven, en de digitale producten die daarbij komen kijken.\nHet klantgericht werken of centraal zetten (sorry, beroepsdeformatie) van de eindgebruiker kan en moet beter. Dat laatste komt voort uit mijn eigen ervaring en blijf ik me voor in zetten. Waarvoor precies? Het ontwerpen en leveren van betere diensten en producten waar eindgebruikers makkelijk en met vertrouwen hun zaken met de Belastingdienst kunnen regelen. Die dus.\nMijn ervaring\nAls Digitaal Strateeg werk ik vanuit Luminis op dit moment voor de Belastingdienst. De omschrijving van de opdracht: heeft het voor IV (Informatie Voorziening, het ICT bedrijf van de Belastingdienst) waarde als we Design Thinking in ons proces toepassen? Bij Design Thinking werk je met een multidisciplinair team aan innovaties waarbij de eindgebruiker centraal staat. Geen dikke documenten vanuit een ivoren toren maar echt een eindgebruiker betrekken en samen met deze eindgebruiker, business en IV oplossingen bedenken. Creatief oplossen van uitdagingen met nadruk op prototypen en zo snel mogelijk testen.\n3 pijlers van SAFe\nNaast het aanbieden van een cursus Design Thinking wordt inmiddels ook SAFe binnen de organisatie uitgerold. Dit framework stelt je in staat om op grote schaal agile te werken. SAFe richt zich op 3 belangrijk pijlers: voorspelbaar, wendbaar \u00e8n klantgericht. Top! Customer Centricity en Design Thinking hebben een plek binnen het framework en \u2013 hoewel SAFe naar mijn mening snel neigt naar de \u2018release\u2019 kant \u2013 ben ik met de uitspraak zeker blij. Het centraal zetten van een eindgebruiker en empathie wordt gezien, en daarmee ook het belang om te kijken naar deze eindgebruiker en zijn uitdagingen. Ik gebruik bewust het woord \u201cuitdagingen\u201d omdat ik in allerlei projecten (ook bij Luminis en haar opdrachtgevers) te vaak meemaak dat de oplossing al bedacht is en er gebouwd wordt zonder eerst goed te kijken wat en voor wie er iets opgelost moet worden. Daar hebben we binnen onze Agile Release Train (ART in SAFe: een groep van Agile-teams dat stapsgewijs een of meer oplossingen in een waarde stroom ontwikkelt, levert en exploiteert) ervaring mee opgedaan. Ook steeds meer buiten deze ART trouwens. Er is dus beweging, ik zie dat we de goede kant op gaan. In deze blog deel ik de belangrijkste ervaringen die ik de afgelopen jaren (bij de Belastingdienst) op heb gedaan.\nIn vogelvlucht\nMaar eerst even terugkijken\u2026 Tot zo\u2019n 15 jaar geleden bedachten digitaal productontwerpers, na een briefing door opdrachtgevens, wat een eindgebruiker voor product wilt hebben. Ik hoorde vaak:\n\u201cJij bent de creatieveling, ik heb een idee maar ik kan dat niet \u2018vertalen\u2019, verzin jij het maar!\u201d Waarbij ik dan altijd voelde: \u201cga aan de slag ontwerper, bedenk iets slims en kom je hok uit als je het doordacht en ontworpen hebt. Ps. Zorg wel dat het te maken is\u2026\u201d.\nHoewel met deze methode ook vandaag de dag nog steeds prima producten ontworpen worden, werkt het voor mij tegenwoordig gelukkig anders. Daar probeer ik ook altijd op te sturen. Waarom? Omdat ik denk dat je samen betere producten kunt maken. Eindgebruikers en opdrachtgevers betrekken waardoor de producten in mijn ogen beter en dus succesvoller zijn.\nDe designcyclus\nMet de ontwikkeling en toepassing van Design Thinking (sinds 1970, en in 1990 bij een grotere groep bekend) en Service Design staan eindgebruiker en samenwerken met opdrachtgevers steeds centraler. Empathie (inleven in de eindgebruiker en zijn behoeftes) is iets wat in deze beide methodes in verschillende vormen terugkomt. Een designcyclus start met Empathy en eindigt via Define, Ideate en Prototype bij Test. Voor elke fase hebben zijn er dan weer allerlei werkvormen die je kunt gebruiken om tot inzichten te komen. Op basis van een uitdaging kijk je bij een designcyclus eerst in de breedte (divergeren) om vast te leggen waarvoor je gaat ontwerpen (convergeren) om vervolgens hetzelfde te doen bij het bedenken van idee\u00ebn en het maken en testen van een prototype.\n\n* Divergeren en convergeren in de \u2018double diamond\u2019 die gebruikt wordt om het design thinking proces te visualiseren.\n** Empathy heb ik begrijpen genoemd. Wat mij betreft gaat het in deze fase om behoeftes, beperkingen en data.\nVeel opdrachtgevers verwachten inmiddels dat er een proces gebruikt wordt waarin omschreven staat hoe en wanneer eindgebruikers gesproken worden en wie (van de opdrachtgever) we hierbij betrekken. Dat had ik 15 jaar geleden niet kunnen bedenken. Want op dat moment gaf ik zelf aan dat ik graag eindgebruikers wilde interviewen en via een (usability) test feedback over het product op wilde halen. Daarbij heb ik meerdere keren het volgende te horen gekregen:\n\u201cJij bent toch de ontwerper en dan weet je wat goed is, daar huur ik je toch voor in? Hoezo dan nog testen met eindgebruikers? Daarbij, dat is ook duur en verstoort/vertraagt het voortbrengingsproces. We willen nu gaan bouwen!\u201d\nMijn argument \u201cmaar wat nou als ik mis zit en we iets opnieuw moeten bouwen, dat kost toch meer tijd en geld?!\u201d veranderde helaas weinig aan de mening van opdrachtgevers. In die zin is niet de hele wereld veranderd, we werken namelijk nog steeds voor opdrachtgevers die het lastig en spannend vinden om eindgebruikers te betrekken en onderzoek duur vinden. In mijn ogen klopt mijn argument overigens nog steeds, misschien eens onderbouwen met cijfers?\nWaarom eindgebruikers betrekken?\nDe pitch (een gedeelte van) die we bij Luminis gebruiken om ons verhaal uit te leggen\u2026\nDigitale producten die er toe doen!\nLuminis maakt voor haar opdrachtgevers en eindgebruikers digitale producten die er toe doen. We ontwerpen en maken digitale producten waar eindgebruikers blij van worden en onze opdrachtgevers succesvol. Hierbij willen we vanuit business, gebruikers en technologie de volgende vragen beantwoorden in ons werk om de juiste digitale producten te kunnen maken:\n\n\n\nWat is wenselijk vanuit het perspectief van de gebruikers?\nWat is waardevol vanuit het perspectief van de opdrachtgever?\nWat is mogelijk vanuit het perspectief van technologie?\n\n\n*3 perspectieven op productontwikkeling om succesvolle producten te maken.\nBegin bij de eindgebruiker\nAls je een product wilt maken waar eindgebruikers blij van worden, zul je bij hen moeten beginnen. Vandaar de volgorde wenselijk, waardevol en mogelijk. Over blij valt in de context van een overheidsinstantie nog wel wat te zeggen. Vaak heeft de eindgebruiker het maar te doen met dit ene product en is er geen keuze. Daarom is een eindgebruiker wellicht eerder tevreden door soepel en makkelijk proces. Zonder gedoe en lang wachten aan de telefoon\u2026 dat je weet waar je aan toe bent en wat je moet doen om compliant te zijn.\nEn dan die eindgebruiker, wat wil die (echt)? Door je in te leven kom je daar achter, dit is eerder benoemd als empathie. Een collega schreef:\n\u201cAls professionele ontwerpers weten we dat het essentieel is om in de schoenen van onze doelgroep te kruipen om de wereld te kunnen beleven zoals zij het ervaren.\u201d\nHelemaal mee eens. En ik weet heus wel dat er meer nodig is dan alleen het resultaat uit een gebruikersonderzoek (waar op wordt gehint) om een overtuigend keuze te kunnen maken om een uitspraak te kunnen doen over de impact en het (verwachte) effect van een concept. Het aspect \u201cmogelijk\u201d bepaalt natuurlijk veel, maar realiseer je: als je weet wat een gebruiker wilt, hoef je dat niet pers\u00e9 (in 1 keer) te maken. Het maken van een ontwerp zie ik dan ook als richtinggevend en in gesprek aan te passen. Maar wel met een eindgebruiker in gedachte.\nEen antwoord geven op de vraag \u201cWat wilt een eindgebruiker echt?\u201d is voor onze opdrachtgevers vaak lastig. Ook voor mij als ontwerper trouwens. Dat heeft meerdere redenen want een eindgebruiker:\n\nzal je al snel willen \u2018pleasen\u2019 en wenselijke antwoorden geven;\nweet vaak niet precies wat hij wil;\nzegt wat hij wil maar wat hij wil zal niet helemaal overeenkomen met de doelen die je (stiekem) al in je achterhoofd hebt.\n\nNaast het feit dat het voor opdrachtgevers daarmee maar de vraag is hoe betrouwbaar zo\u2019n gesprek met een eindgebruiker is ontstaat er ook vaak weerstand om met een eindgebruiker in gesprek te gaan want opdrachtgevers:\n\nweten niet hoe en wanneer ze klanten kunnen betrekken om inzicht te krijgen;\ndenken dat het lang duurt en duur is om met eindgebruikers onderzoek te doen;\ndenken dat het hun eigen doelen schaadt als ze (te)veel naar eindgebruikers luisteren.\n\nHoe vind je een eindgebruiker?\nVoor het geval je niet weet hoe je een eindgebruiker kunt betrekken, de simpelste stap is rondvragen. Vrienden, familie etc. zijn in mijn ervaring snel bereid te helpen. Door een paar vragen te stellen krijg je snel reactie op je idee. Ook bestaande klanten kun je benaderen en er is niks leerzamer (en leuker) dan naar ze toe te gaan. Gewoon bellen, bijna iedereen zegt \u2018ja\u2019. De leukste en meest leerzame gesprekken en testen heb ik in de steenfabriek, de garage of op de boot uitgevoerd.\nJe kunt het werven van respondenten ook uitbesteden, dan gaat het je misschien meer kosten maar vaak heb je dan wel eerder een meer diverse groep waar je mee kunt testen. En het is natuurlijk gemakkelijk iemand te laten werven op basis van een opgesteld profiel. De test wordt serieuzer en \u2013 hoewel het een veel gehoorde opmerking dat je niet uit kunt gaan van een enkele testen \u2013 5 tot 6 respondenten is echt genoeg voor de eerste inzichten die je op wilt halen. Gebruik kwalitatief onderzoek als je iets wilt begrijpen (concepten, gedachten of ervaringen).\nOver punt 2 kan ik zeggen: je kunt het zo klein en zo groot maken als je wilt (van 1 uur tot meerdere weken) maar als je door 1 uur te besteden de inzichten ophaalt dat het anders, moet heb je snel je geld verdiend. Over punt 3, ach \u2013 je bent er zelf bij, je hoeft heus niet (alles) te maken wat een klant wil.\nAls je kijkt naar de verschillende frameworks, waar SAFe er eentje van is, zou je kunnen zeggen dat het mooi is dat Customer Centricity en Design Thinking genoemd wordt maar in mijn ervaring ben je er dan nog niet. Ook omdat deze frameworks niet beschrijven hoe je dit soort, voor de meeste mensen, ongrijpbare dingen toe kunt passen. Een veelgehoorde opmerking in dit verband: \u201d\nIk doe dat al, ik denk toch al vanuit de klant?!\u201d Stel dan de tegenvraag: wanneer heb je die voor het laatst gesproken? Het antwoord? \u201cJa nee, dat niet maar ik weet het wel wat goed voor ze is\u2026\u201d\nDat is niet wat ik met klantgericht werken bedoel.\nIn mijn ervaring kun je nog zoveel over de theorie vertellen maar is het vooral een kwestie van doen. Bijvoorbeeld deelnemers van workshops laten ervaren wat dat klantgericht werken is. Bij het uitleggen van Design Thinking heb ik een aantal keer meegemaakt dat ik het licht in de ogen van mensen zag doven. Terwijl als je een workshop doet en daar eindklanten bij betrekt (zonder precies je beweegredenen uit te leggen) worden medewerkers enthousiast en geven ze aan dat \u2018we\u2019 \u2018dit\u2019 vaker zouden moeten doen. Ha, een stapje gemaakt! Dan nog steeds, hoe pas je dat nou in de praktijk toe?\nKlantgericht werken (in een grote organisatie), hoe doe je dat?\nIn deze blog is het al subtiel aangestipt: dat is best lastig. De belangrijkste oorzaak hiervan is dat het een organisatie niet zomaar lukt om van de ene op de andere dag om anders te gaan werken en er medewerkers met weerstand overtuigd moeten worden. Een werkwijze aanpassen lukt niet van de ene op de andere dag, daarvoor heb je een lange adem nodig. Als je klantgericht wilt gaan werken, realiseer je dan dat je hier zomaar eens langer dan twee jaar mee bezig kunt zijn voordat je op een niveau bent gekomen dat je klantgericht(er) werkt.\nAls je klantgericht wilt gaan werken is het handig een aantal manieren of methodes te hebben waar klantgericht werken in zit. De belangrijkste workshopvormen die (met multidisciplinaire teams) in mijn ervaring werken:\n\n1-daagse workshop\nDesign sprints (zie de whitepaper Design Sprints die ik geschreven heb)\nService design trajecten\nKlantreis trajecten\n\nIn mijn ogen betekent een klant centraal stellen dat je op verschillende momenten met die klant moet gaan praten. In alle bovenstaande werkvormen zit het ingebakken dat je dit doet. Het zit dus in de werkwijze met als voordeel dat iedereen het na verloop van tijd normaal gaat vinden. Dan kun je wanneer je bijvoorbeeld een bestaand product wilt verbeteren de volgende (open) vragen aan een eindgebruiker stellen:\n\nWelk onderzoek heb je voordat je het product ging gebruiken gedaan?\nWat kun je me over dit formulier vertellen? Hoe heb je dit formulier ingevuld?\nWaar liep je tegenaan tijdens het invullen?\nWat wordt er met deze regeling gevraagd?\nHoe heb je dat opgelost?\n\nBij de projecten waar we interviews afnemen proberen we zoveel mogelijk stakeholders mee te laten luisteren. Omdat het voor een businessowner, domeinexpert, architect, etc. net zo goed is om te weten wat er bij een klant speelt. Dit is geen ontwerp feestje. Scheelt voor de designer ook weer tijd om anderen van het juiste product of dienst te overtuigen.\nDe genoemde werkvormen kun je inzetten om een oplossing voor kleine en grote uitdagingen uit te werken. In het geval van SAFe: als je elk increment (5 tot 6 sprints van twee weken binnen een kwartaal) een design sprint rond een belangrijke (klantgerichte) feature doet, ben je al klantgericht met je team aan het werk. In een design sprint praat je aan de start met eindgebruikers en test je aan het eind ook met eindgebruikers. En meteen realiseren zodat je kunt meten en naar klantwaarde kunt kijken. Het is wellicht eng om succes te meten (heb ik wel de juiste investering gedaan?) maar hoe eerder je test des te eerder je je product aan kunt gaan passen en meer (klant)waarde kunt leveren.\nTot slot\nIk denk dat klantgericht werken voor elke organisatie iets anders betekent. De markt, tijd, budget, teamsamenstelling, mindset, etc. allemaal factoren die van invloed zijn. Een vastomlijnd plan ligt er dus niet maar denk aan de volgende takeaways:\nAls je meer klantgericht wilt gaan werken, denk dan aan:\n\nhet betrekken van eindgebruikers;\nhet gebruiken van (design thinking) werkvormen waarmee je multidisciplinair werkt en teamleden zich in de gebruiker in kunnen leveren;\nhet testen met eindgebruikers.\n\nzodat:\n\nhet juiste probleem op wordt gelost;\nhet voor eindgebruikers makkelijker wordt om in 1 keer goed hun zaken kunnen regelen (met als bijkomend voordeel minder belasting van supportmedewerkers);\nrework achteraf voorkomen wordt (1st time right);\nje blije gebruikers hebt, klantwaarde levert en succesvoller met je product bent!\n\n\n\n", "tags": [], "categories": ["Blog", "Concepting &amp; UX"]}
{"post_id": 38095, "title": "Data Quality Series, part 1: Introduction to Data Quality", "url": "https://www.luminis.eu/blog-en/introduction-to-data-quality/", "updated_at": "2023-08-13T12:11:33", "body": "We\u2019ve all heard the phrase \u201cgarbage in, garbage out\u201d which highlights the importance of quality data for data-driven systems. Here, quality data can be interpreted in two ways: firstly as clean and well-standardized data that meets expectations, and secondly, as well-thought-out data that fits a particular business case. Although the latter is typically determined during the research or data strategy phase, in this blog we will focus on the former interpretation.\nMotivation\nMaintaining high-quality data is critical for accurate data analysis, decision-making, and achieving business objectives. Real-world data is often noisy and subject to constant changes, which makes maintaining data quality a challenging task. Therefore, it\u2019s crucial to identify data quality issues early on and address them before they have any effects on downstream analytics tasks or decision making processes. One of the responsibilities of Data and MLOps Engineers is to ensure that quality is maintained throughout its lifecycle.\nMeasuring Data Quality\nTo ensure data quality throughout the data pipeline, it\u2019s important to measure and test it at different stages. A typical testing workflow in data engineering would involve several types of tests:\n\nUnit testing: focuses on testing separate components of your pipeline in isolation. For example, testing whether (a part of) an SQL or a Spark script does what it is supposed to do.\nFunctional Testing: includes data flow validation such as transformation logic validation based on business rules, as well as data integrity, which validates data based on constraints and schema checks. This type of testing occurs frequently at different stages of the pipeline (think ingestion, processing, and storage).\nIntegration Testing: ensures that the data pipeline meets the business requirements. Generally, this is done by running fake data through the pipeline and validating the result.\n\nAlthough we have covered different types of tests, it\u2019s worth noting that traditional software engineering practices only cover these points to a certain extent. For this reason, let\u2019s focus on functional testing of data and take a look at testable properties attributed to quality data.\n\nCompleteness: checks whether all expected data is present and accounted for in the data pipeline. Simple checks may test for null values, while more complex checks may also condition based on value or other columns.\nUniqueness: verifies if there are no duplicate records in the data pipeline. Duplicate records can cause issues with aggregation and analysis, leading to incorrect results.\nDistribution: focuses on closely examining the validity of column values. This may involve checks to ensure that the data falls within an accepted range or that the units used in a given column are consistent.\nValidity: enforces known invariants that should always hold true, regardless of the input data. These may be defined based on data standards or business rules. For example, the price column may never be negative, or the total column should equal to the sum of pre-tax subtotal and tax amount.\nAccuracy: measures the level to which data reflects the real world by using a verifiable source. For example, a customer phone number can be validated.\nIntegrity: takes into account relationships of data with other systems within an organization. It involves limiting changes or additions to the data that may break connections and generate orphaned records.\nConsistency: ensures that the data is accurate, and aligned with the organization\u2019s attributes. By having consistent attribute values on can building relationships between different data systems, prevent data duplication, and inconsistencies.\n\nData Observability\nHaving proper testing mechanisms set up is only the first step, as it is only natural that the data keeps evolving and may change unexpectedly. This aspect of data is not easy to tame, since you don\u2019t always have control over the source systems. That\u2019s why it\u2019s crucial to continuously test and monitor data quality, which is where Data Observability comes into play.\nThe five pillars of data observability[1] provide good guidance criteria you would want to include in your testing and\nmonitoring.\n\nFreshness: Does the asset use the most recent data? This is a critical aspect of data quality, as outdated or stale data can lead to incorrect decisions being made. Depending on the use case, you may need to validate that the data is fresh within a certain time window, such as the past hour or day.\nQuality: The quality checks are vital in verifying the quality of data, as they ensure that the data is in the correct format and within acceptable limits. These checks are useful for ensuring that the transformation pipeline can handle the input data, as well as, validating the output data, as is commonly applied when writing data contracts[2].\nVolume: Did all the data arrive? How many rows were changed? Did the dataset decrease in size? These are important questions to answer when monitoring the volume of your data pipeline. Sudden spikes or drops in volume could indicate issues with the pipeline or changes in the underlying data sources.\nSchema: The schema of a dataset defines the structure and type of each field in the data. It is often used in contracts between producers and consumers of the data. Especially when working with raw data sources, schema validation checks can help catch issues such as missing or incorrectly formatted fields, and ensure that any changes to the schema are properly communicated to downstream consumers.\nLineage: Lineage refers to the record of the origins, movement, and transformation of data throughout the data It can answer questions about upstream sources that the data depends on and downstream assets that would be impacted by any change. The data lineage is a critical component during compliance auditing and root cause analysis.\n\nLineage of Continent Statistics table visualized in Dagster\nYou can not test for everything, and as things inevitably break, you may be unknowingly making decisions on bad data. There is an exponential relation between lost revenue and how far down the line data issues are diagnosed.\nWhen you write tests for your data, you are testing for \u201cknown unknowns\u201d. The next step you can take is testing for \u201cunknown unknowns\u201d, which on contrary are not apparent during the creation of the data systems. Detecting these issues is typically done through health checks and anomaly detection on collected metrics through simple thresholding or forecasting-based methods.\nMonitoring the percentage of faulty rows or checking whether the number of days since the last update does not exceed the historical average duration are good examples of proxy measures that can detect \u201cunknown unknowns\u201d.\nAnomaly Detection on Row Count quality metric in Soda Cloud [3]Performing data profiling by defining rule-based sets of metrics to be computed on all columns within your dataset can give you a good starting point when writing tests. Some data processing tools like OpenRefine and AWS DataBrew have built-in data profiling to aid in building cleaning transformations. Similarly, it can also be a powerful tool when combined with anomaly detection for building automated monitoring systems.\nData Profiles in AWS DataBrew [4]Presenting the data profiles, quality information, and schema as part of a dashboard or data catalog can provide a lot of value for your business[5]. Similarly, setting the right governance structure where the issues and alerts reach the appropriate team is an important aspect of maintaining high data reliability.\nFor additional guidelines on improving data reliability, consider reviewing AWS Well-Architected Data Analytics Lens [6].\nData Quality Score Cards in Monte Carlo\u2019s Data Reliability Dashboard [7]When it comes to designing reliable data systems, it\u2019s essential to handle errors gracefully both during data quality testing and transformation. Depending on your environment, there are various testing approaches you can take. A common first concern is determining when and where to run data quality checks.\nMany cases such as ETL pipelines prefer on-demand execution where the quality of raw data is evaluated at the source or the destination after loading the data. This approach ensures that the transformation step can handle the data before actual processing is applied. Both approaches have their benefits; testing before load requires query access to the source database and may put excessive load on the application database, while loading data beforehand may result in additional latency.\nSimilarly, scheduled execution periodically tests data quality in source tables and reports if any issues arise. This approach is typically found in data warehouse solutions, where transformation is postponed until query evaluation using views.\nA notable benefit of on-demand execution is that one can immediately act on it. As such, the circuit breaker pattern is utilized to break off pipeline execution if (batch) data does not pass the error checks or an anomaly is detected. The tradeoff is that the rest of the system keeps using stale or partial data until the issue is resolved.\nTo expand on this methodology, data quarantining is another related pattern that defines a flow where faulty data is set aside. The quarantined data can be used to fix the issues and reprocessed at a later date to ensure that no data loss occurs. This approach works particularly well for incremental processing pipelines or pipelines without idempotency property (i.e., processing data multiple times results in a different dataset).\nSelf-healing pipelines combine none or multiple of the mentioned properties to gracefully recover from failure. This may be as simple as retrying data submission, reprocessing the full dataset, or waiting until prerequisite data is in the system.\nChoosing Your Tools\nWe evaluated several open-source data quality tools (aside from AWS Glue) to use in our ETL pipelines. Our evaluation criteria included features, integrations, and compatibility with our existing pipeline architecture.\nGreat Expectations (GX): is the tool of choice for many data workloads. It has a large collection of community-made checks and a large collection of features. Supported integrations include some common data tools, cloud analytics (including Amazon Athena, AWS Glue, and AWS Redshift), and pandas dataframes.\n\nCodified data contracts and data docs generation\nData profiling\nThe Quality metrics are limited to what checks calculate.\nOn-demand execution\n\nAWS Deequ: is an open-source library built by AWS that covers a wide range of data quality needs. Deequ is based on the concept of data quarantining and has the functionality to filter out and store bad data at various stages of the process.\nThe tool is built in Scala on top of Apache Spark, but it has a Python bindings library which unfortunately lags quite far behind. If you don\u2019t use these tools in your stack, you will find them of limited use.\n\nAnomaly detection\nSchema checks\nData profiling\nQuality metrics calculation and merging\nOn-demand execution\n\nAWS Glue Data Quality Rules: Recently, AWS introduced a variety of data quality tools as part of their serverless computing platform Glue. The tool itself uses Deequ under the hood and provides excellent interoperability with the rest of AWS stack, such as AWS CloudWatch and result storage.\nAs of writing this article, the functionality is still in public beta, does not offer a way to store quality metric results for anomaly detection nor has a way to run the checks outside AWS glue environment (closed source). Similarly, many of the features included in deequ are not yet supported, such as quality metrics calculation or custom checks.\n\nConfiguration-based tests\nWell integrated with AWS infrastructure\n\nSoda Core: is a modern SQL-first data quality and observability tool. Similar to GX it includes a wide range of integrations. While Soda Core by itself is only for collecting metrics, a full-fledged data observability platform in form of Soda Cloud (proprietary) is provided with automatic monitoring of data quality results, data contracts, and anomaly detection.\n\nWide range of integrations\nSimple configuration\nSchema checks\nQuality measure calculation\n\nDBT Unit Tests: comes as part of the DBT which is an SQL-first tool for managing and modeling your data in data warehouses. The integrations are not limited to data sources, but also other data quality tools. The tool itself is meant for unit testing and therefore runs separately from the data flow.\n\nCustom metric calculation.\nCommunity support (resources, plugins, and integrations).\n\nApache Griffin: As a complete data quality platform, it provides an integrated dashboard for data quality analysis, and monitoring data quality over time. The quality testing runs are conducted within the tool but separate from the data flow. The integrations are limited to the Apache Stack (Kafka, Spark, Hive), and a select few other tools.\n\nStreaming data processing support\nDashboard for data quality analysis\nAnomaly detection\n\nAll listed tools have their use cases and as such there is no clear winner. For simple ETL workloads, you might want to try Deequ. In a data warehouse setting, dbt in combination with Soda or GX might prove useful. When working in a data science setting or with streaming data, GX and Apache Griffin respectively might be good choices. If your infrastructure runs on AWS, it\u2019s worth keeping an eye on developments in their Glue-based data quality tools.\nConclusion\nIn conclusion, maintaining high-quality data is essential for accurate data analysis, decision-making, and achieving business objectives. Data quality testing is a huge part of the testing process for data systems, and there are many options on how this can be implemented. In this blog, we have covered a few fundamentals, which I hope give you a starting point for exploring more on the topic and applying it in your projects. Stay tuned for part two, where we will use deequ for data quality testing within an ETL pipeline on AWS.\nCitations\n1. What Is Data Observability? 5 Key Pillars To Know In 2023\n2. Fine, let\u2019s talk about data contracts \u2013 by Benn Stancil\n3. Time Series Anomaly Detection with Soda | Soda Data\n4. AWS Launches Visual Data Prep Tool\n5. Build a data quality score card using AWS Glue DataBrew, Amazon Athena, and Amazon QuickSight | AWS Big Data Blog\n6. Data Analytics Lens 1 \u2013 Monitor the health of the analytics application workload\n7. Announcing Monte Carlo\u2019s Data Reliability Dashboard, A Better Way Understand The Health Of Your Data\n", "tags": [], "categories": ["Blog", "Data"]}
{"post_id": 37946, "title": "The Amplify Series, Part 7: Track app usage with Amplify Analytics", "url": "https://www.luminis.eu/blog-en/the-amplify-series-part-7-track-app-usage-with-amplify-analytics/", "updated_at": "2023-05-08T12:55:21", "body": "In the previous part of this series, we added the Amplify Predictions category, which allowed us to use the power of AI and Machine learning to identify text in images, convert text to speech and interpret sentiment of the text. In this part of the blog series, we will add functionality to be able to track the usage of our application in order to learn how our users use the application and be able to improve the application user experience based on this information. We will do this by adding the Amplify category called Amplify Analytics.\nWe will start by adding this category to the project. We will create a new page for the Amplify Analytics category and add some interactable elements to that page so that we can track the usage of these elements. We will also be updating the previous pages when needed in order to track the usage of those pages. In the end, you will have a better understanding of this category and be able to track the usage of your Amplify application.\nWe will continue in the repository where we left off in the previous blog post.\nAdding the Analytics category\nWe will start by generating the backend resources needed for the tracking functionality.\u00a0\nWe will run amplify add analytics with the following options:\n\nAnalytics provider: Amazon Pinpoint\nResource name: <<use default>>\nUnauthenticated users: No\n\n\r\n? Select an Analytics provider Amazon Pinpoint\r\n\u2714 Provide your pinpoint resource name: \u00b7 theamplifyapp\r\n\u26a0\ufe0f Auth configuration is required to allow unauthenticated users, but it is not configured properly.\r\n\u26a0\ufe0f Adding analytics would add the Auth category to the project if not already added.\r\n? Apps need authorization to send analytics events. Do you want to allow guests and unauthenticated users to send analytics events? (we recommend you a\r\nllow this when getting started) No\r\n\u26a0\ufe0f Authorize only authenticated users to send analytics events. Use \"amplify update auth\" to modify this behavior.\r\n\u2705 Successfully updated auth resource locally.\r\n\u2705 Successfully added resource theamplifyapp locally\r\n\nThis command will add a new directory called analytics to our\u00a0 amplify/backend directory, which will contain information needed to create the resources in AWS to support the new functionality.\nWe will run amplify push to create these resources in AWS. The AWS service used here are Amazon Pinpoint.\u00a0\nNote that using this service will cost you money. The first 100 million events recorded per month are free. After that, you pay $0.000001 per event you collect. A piece of general advice:\u00a0 configure a budget with billing alerts for your AWS account so you don\u2019t get surprised by high costs.\u00a0\nAfter running the amplify push command you will not be charged since the application must be configured to send events to AWS Pinpoint. Once we configure this, however, the application will automatically start sending a minimal set of events such as page visits. This can be turned off and will be covered in the next section.\nOnce we have created the resources, we can run amplify console analytics to open the Amazon Pinpoint console. Initially, all charts should be empty:\n\nIn the next step, we will configure the frontend application to start sending data to Amazon Pinpoint to fill these graphs.\nConfiguring the frontend for Analytics\nTo start, we will update our main.ts to include the following:\nimport { Predictions, Analytics } from 'aws-amplify';\nBy simply importing this dependency,running the application again and visiting it in the browser, we can see that the charts are being updated:\n\nWe can also click on the demographics to see information about the users:\n\nWith this small addition to the frontend application, we can now see how much the application is being used and from what type of devices. If you want to have more information about the users of the application, it is possible to customize the information sent about a user. We will not cover that part in this blog, however, a complete example can be found on this page.\nSession tracking\nThe session metric in AWS Pinpoint provides information about how often your application has been opened. However, once a session is registered for a device (mobile, browser, etc.) the session counter does not go up if the same device visits the site again within a certain time period. Refreshing the page or closing the browser and opening it again is not a new session. This is because the Amplify SDK creates a unique identifier for your device. \nAfter the page has not been active on the device for some time or if your application manually calls Analytics.stopSession the session will terminate. Opening the page again after this will register as a new session.\nThe session is tracked by default, however, you can turn it off by adding the following code in the main.ts:\nAnalytics.autoTrack('session', {\r\n\u00a0 \u00a0 enable: false\r\n});\nPage view tracking\nWe can also configure the application to track which pages are visited. We will update the main.ts with the following:\nAnalytics.autoTrack('pageView', {\r\n\u00a0 enable: true,\r\n\u00a0 type: 'SPA',\r\n\u00a0 attributes: {\r\n\u00a0 \u00a0 url: window.location.origin + window.location.pathname\r\n\u00a0 },\r\n})\nNow you can run your application and visit some of the pages. If you visit the events page in AWS Pinpoint you can see how much each page is viewed:\n\nDisclaimer: It can take up to 30 minutes for the events to show up in Pinpoint.\nRecording events\nWe can record events such as clicks on the pages with Amplify Analytics. This section describes two ways to do that.\nManual event recording\nThe first way is to do it manually. We will update our text-to-speech.component.ts to send an event to AWS Pinpoint every time the user uses the text-to-speech functionality. The event will contain the text that the user entered:\n// Other imports\r\nimport { Predictions, Analytics } from 'aws-amplify';\u00a0 // <--- Add import\r\n\r\n@Component({\r\n\u00a0 selector: 'app-text-to-speech',\r\n\u00a0 templateUrl: './text-to-speech.component.html',\r\n\u00a0 styleUrls: ['./text-to-speech.component.css']\r\n})\r\nexport class TextToSpeechComponent implements OnInit {\r\n\u00a0 // Other existing code\r\n\r\n\u00a0 convertToAudio = async () => {\r\n\u00a0 \u00a0 if (!this.textInput) {\r\n\u00a0 \u00a0 \u00a0 return;\r\n\u00a0 \u00a0 }\r\n\r\n\u00a0 \u00a0 // Add this \r\n\u00a0 \u00a0 Analytics.record({\r\n\u00a0 \u00a0 \u00a0 name: 'convertedTextToSpeech',\r\n\u00a0 \u00a0 \u00a0 attributes: { textInput: this.textInput },\r\n\u00a0 \u00a0 \u00a0 immediate: true\r\n\u00a0 \u00a0 });\r\n\r\n\u00a0 \u00a0 // Other existing code\r\n\u00a0 };\r\n\r\n\u00a0 // Other existing code\r\n}\nThis will now show up in our events every time a user uses this functionality:\n\nIn a real application you might not want to set an attribute value as we did here for an input field. Since the users of the application are free to enter any value they want, the attribute value list in the filters will explode. However, this example gives an impression of what is possible with the minimal setup we have so far.\nAutomatic event recording\nThe other way to record events is to set up an event tracker that will activate when an element contains certain properties. We will update the main.ts to include the following:\nAnalytics.autoTrack('event', {\r\n\u00a0 enable: true,\r\n\u00a0 events: ['click'],\r\n\u00a0 selectorPrefix: 'data-amplify-analytics-'\r\n});\nThis will track all click events for HTML elements that have properties with the prefix data-amplify-analytics-.\nWe will now update our comments.component.html to make use of this so that an event is sent to AWS Pinpoint every time someone adds a comment to the post. \n\u00a0\u00a0\u00a0\u00a0<!-- Other existing code -->\r\n\r\n\u00a0 \u00a0 <div class=\"col-lg-4\">\r\n\u00a0 \u00a0 \u00a0 <button\r\n\u00a0 \u00a0 \u00a0 \u00a0 class=\"aws-button\"\r\n\u00a0 \u00a0 \u00a0 \u00a0 data-amplify-analytics-on=\"click\"\r\n\u00a0 \u00a0 \u00a0 \u00a0 data-amplify-analytics-name=\"commentAdded\"\r\n\u00a0 \u00a0 \u00a0 \u00a0 (click)=\"addComment()\"\r\n\u00a0 \u00a0 \u00a0 >\r\n\u00a0 \u00a0 \u00a0 \u00a0 Add Comment\r\n\u00a0 \u00a0 \u00a0 </button>\r\n\u00a0 \u00a0 </div>\r\n\u00a0 <!-- Other existing code -->\nThis event will also show up similar to the previous one in our events dashboard:\n\nWe now have enough tools to be able to track the usage in our application.\nThe following code changes have been made to our repository in this article:\n\nAdding the backend resource\nUpdating the frontend\n\nAmplify Analytics vs Google Analytics\nWhen deciding to track app usage, you can also use Google Analytics, which is unarguably more feature-rich and streamlined than Amplify Analytics.\u00a0\nAmplify Analytics uses AWS Pinpoint, which has analytics capabilities but is more focused on user interaction, such as sending emails and notifications for campaigns.\u00a0\nGoogle Analytics is the industry standard when it comes to Analytics as it provides many features and is very user-friendly compared to AWS Pinpoint in my opinion. So how do you choose between the two?\nMy advice would be to choose AWS Pinpoint if you want to get a quick view of the usage of your application. If you already have an Amplify application, you can get up and running with analytics with just a few changes to your application as we have seen in this blog post.\u00a0\nIf you want to go further than seeing the general usage, event usage, and demographics, then you should consider using Google Analytics, especially if you or someone on your team already has experience with it.\u00a0\nUp next: Location functionality with Amplify Geo\nIn this blog we have used Amplify Analytics to track the app usage of our application. As with the previous categories, there is more that you can do with this category than what is covered in this article, so be sure to check the documentation for more information. In the next article, we will take a look at how to use Amplify Geo to add location-based functionality.\n", "tags": ["amplify", "angular", "aws", "cloud"], "categories": ["Blog", "Cloud"]}
{"post_id": 37914, "title": "Introduction of Frank and Amy at HaystackConf USA", "url": "https://www.luminis.eu/blog-en/introduction-of-frank-and-amy-at-haystackconf-usa/", "updated_at": "2023-05-02T15:04:11", "body": "Last week, at the end of April 2023, I attended the HaystackConf in Charlottesville, USA. HaystackConf is the conference to participate in if you are a search relevance engineer. With over a hundred people attending on-site and more than a hundred online, this is a good representation of the almost 4000 members at the time of writing in the search relevance community on Slack.\nAfter presenting at the online HaystackConf due to covid, this was the first time I joined the conference as a speaker. This was also my first speaker experience outside of Europe, and what a great experience it has been. This is the conference where I introduced Frank and Amy to the big public. This blog post will take you with me on my journey to the USA.\nSunday morning, we took the plane to Washington, DC. We, because I traveled with my Buddy and old colleague Byron Voorbach from Weaviate. We rented a car to drive to Charlottesville. After arrival, we did a walk through town, ate a hamburger, and took a drink.\nMonday was Jetlag day. Besides a bit of work, we explored the environment. We went to a park to shoot some pictures, and in the evening, we attended a local meetup at the Center for Open Science: Pre-Haystack US Meetup at Center for Open Science. The meetup had two talks.\nMatt Clark on COS, whose mission is to increase openness, integrity, and reproducibility of research, and their SCORE project: Systematizing Confidence in Open Research and Evidence.\nRen\u00e9 Kriegler, Director, E-commerce Search at Open Source Connections: A path to understanding and adopting evolving technologies in search.\nThe evening was a good warm-up for the conference\u2014excellent discussions about the relevance of Vector databases and ChatGPT for the search relevance domain.\nLego at the park\nTuesday, Conference Day 1\nCharlie Hull welcomed everybody on-site as well as online. Next was the keynote by Trey Grainger. Always a good speaker. I liked his overview of approaching search before and moving from Sparse to Dense information retrieval. It was clear that vector search is here; having a sparse search alone is not the future. Is hybrid search just a step from sparse to dense because dense still has some issues, or is hybrid the future? Will large language models be the silver bullet? The one clear thing is that search and information retrieval are going through an exciting time.\nThe next talk I attended has a summary in its title: \u201cLearning to hybrid search: combining BM25, neural embeddings and customer behavior into an ultimate ranking ensemble\u201d. Roman Grebennikov presented his thoughts on the current hype around neural search. Roman is the primary author of the project Metarank. An interesting project if you want to use learning to rank.\nThe next talk was by Karel Bergmann: \u201cCreating Representative Query Sets for Offline Evaluation\u201d. He explained how they used offline evaluation at Getty Images. Interesting to see the scale at which they use these evaluations to optimize searching for images.\nLunch at HaystackConf is going into town and selecting a restaurant. Everybody can find something they like with the number of choices available in town. After lunch, I visited Ohad Levi. His talk is titled: \u201cBreaking Search Performance Limits with Domain-Specific Computing\u201d. He discussed using domain-specific hardware to improve search performance with a factor of 100. Impressive numbers that work well with specific instances of AWS machines.\nFrank and Amy talk about search relevance.\nThe following presentation was my own. The title of my talk is \u201cTop 8 search topics to teach your team members\u201d. I introduced the main characters of my presentation, Frank and Amy. During the talk, I used Frank, the search relevance expert, to explain search features to Amy, a seasoned Python developer. It was fun for me to do the presentation. I got good feedback from the audience. They liked the presentation with all the drawings and recognized the topics. Not all of them explained these topics to their team members, which was a good lesson for them.\nAfter my talk, lightning talks followed by a reception and dinner at the Kardinal Hall. Good beer, food, and a game of Bocce. A relaxed atmosphere to talk about search topics and get to know each other.\nNot everybody returns to the hotel after dinner. Charlottesville has a lot of bars to offer, and drinks are diverse, as are the people from Charlottesville.\nAfter a satisfying day, time to go to bed.\n\u00a0\nWednesday, Conference Day 2\nThe second day of the conference started off with an Ask me anything session with the authors of the book \u201cAI-Powered Search\u201c. Participants were Trey Grainger, Doug Turnbull, and Max Irwin. Most of the exciting questions were again about vectors and Large Language Models\u2019 role in search. These three guys together have an incredible amount of knowledge in the search area. If you like this topic, you should read their book.\nNext, I attended the talk by Jay Flack titled \u201cEnterprise Search Relevance at Box: Simplicity\u201d. You should not be surprised this talk is about the experience they gained at Box to supply a search feature over all the documents they have. Security and speed are very important as the number of documents is enormous. They used Solr with an exterior ranking model built with Tensorflow running on AWS Sagemaker.\nLunchtime again, and waiting for the talk by Erika Cardenas. Erika is a colleague of Byron; this is her first talk in public. Yes, she was a bit nervous. But you step into a cinema room and present a keynote plus a short presentation about the women in search. Erika\u2019s talk was about \u201cBuilding Recommendation Systems with Vector Search.\u201d Erika did a great job. Of course, she used Weaviate for the demo application. An interesting talk about the ref2vec module for Weaviate. Go try it out if you are interested.\nNext up for me was the presentation from Chris Morley. His talk is titled \u201cPopulating and leveraging semantic knowledge graphs to supercharge search.\u201c\u00a0He was funny and had some interesting ideas, but it did not work for me. I did not really understand the point he was making. He mentioned it was more of a try-out for him; it needs some polishing if you ask me. The content is there, but the story is harder to follow.\nThe last presentation was from Colin Harman. The title of his talk is Stop \u201cHallucinations and Half-Truths in Generative Search\u201d. I really liked his talk. He showed examples of where ChatGPT produces wrong answers. His examples using simple calculations in your question to break the results were interesting. One example where he asked ChatGPT only to consider a provided document to answer a question was wrong. Also, an example about medicine, where the answer varied the code for the medicine a bit and could even give a dangerous answer. Great closing talk for me.\nThe closing event for the conference was a dinner sponsored by Weaviate. I sat next to Erika and Colin. Had really good conversations. It was an inspiring evening again.\nJettro on stage\nThat is where the conference ended, but not the learning. The following two days, we had fun driving to Washington through the mountains and visiting Washington. We also took time to work on a sample application using Weaviate and OpenAI. Using a cross-encoder as a re-ranker for a hybrid search in Weaviate. Feeling more confident now working with Weaviate.\nCall me if you are reading this and want to know more about search, vector databases, and Large Language Models. We can help you create AI-powered search solutions that help your user become effective searchers.\nStay tuned for announcements if you want to experience HaystackConf, but live in Europe and feel the USA is too far away. HaystackConf is coming to Europe again this year. Maybe to the Netherlands.\n", "tags": ["chatgpt", "haystackconf", "weaviate"], "categories": ["Blog", "Search"]}
{"post_id": 37425, "title": "Improve AWS security and compliance with cdk-nag", "url": "https://www.luminis.eu/blog-en/cloud-en/improve-aws-security-and-compliance-with-cdk-nag/", "updated_at": "2023-04-26T09:26:45", "body": "AWS Cloud Development Kit (AWS CDK) is a powerful tool that allows developers to define cloud infrastructure in code using familiar programming languages like TypeScript, Python, and Java.\nHowever, as with any infrastructure-as-code tool, it\u2019s important to ensure that the resulting infrastructure adheres to security and compliance best practices. This is where cdk-nag comes in.\nWhat is cdk-nag ?\ncdk-nag is an open-source tool that provides automated checks for AWS CDK code and the resulting Cloudformation templates to help ensure that they adhere to security and compliance best practices.\nAfter adding cdk-nag to your project it checks for a variety of known security and compliance issues including overly-permissive IAM policies, missing access logs and unintended public s3 buckets. cdk-nag also checks for common mistakes that can lead to security vulnerabilities, such as the use of plain text passwords and the use of default security groups.\nThe great thing about cdk-nag is that it allows you to catch mistakes at a very early stage in the process. Ideally, you can catch them while developing your infrastructure as code in CDK on your local machine. As an alternative, you can add cdk-nag to your CI/CD pipeline and make the build fail in case of any issues.\nAdding cdk-nag to your project\nUsing cdk-nag is simple. First, add it as a dependency to your AWS CDK project. If you\u2019re using Java you can add it to your pom.xml file.\n<dependency>\r\n  <groupId>io.github.cdklabs</groupId>\r\n  <artifactId>cdknag</artifactId>\r\n  <version>2.25.2</version>\r\n</dependency>\r\n\nAfter you\u2019ve added the dependency you will need to explicitly enable cdk-nag utilizing a CDK aspect. You can apply cdk-nag in the scope of your entire CDK application or just in the scope of a single CDK stack.\ncdk-nag works with rules which are defined in packs. Those packs are based on AWS Config conformance pack. If you\u2019ve never looked at AWS Config, the Operational Best Practices for HIPAA Security page is a nice page to look at in the context of these cdk-nag conformance packs. By default, cdk-nag comes with several rule packs out of the box.\n\nAWS Solutions\nHIPAA Security\nNIST 800-53 rev 4\nNIST 800-53 rev 5\nPCI DSS 3.2.1\n\nBased on your requirements you can enable one or more rule packs. Let\u2019s take a look at how to apply such a rule pack.\npublic class AwsCdkNagDemoApp {\r\n    public static void main(final String[] args) {\r\n        App app = new App();\r\n\r\n        new AwsCdkNagDemoStack(app, \"AwsCdkNagDemoStack\", \r\n            StackProps\r\n                .builder()\r\n                .env(Environment.builder()\r\n                .account(System.getenv(\"CDK_DEFAULT_ACCOUNT\"))\r\n                .region(System.getenv(\"CDK_DEFAULT_REGION\"))\r\n                .build())\r\n            .build()\r\n        );\r\n\r\n         Aspects.of(app)\r\n           .add(\r\n                AwsSolutionsChecks.Builder\r\n                .create()\r\n                .verbose(true)\r\n                .build()\r\n           );\r\n        app.synth();\r\n    }\r\n}\nAs you can see in the above code fragment we\u2019ve enabled the AwsSolutionsChecks rules for the scope of the entire CDK app. In this example, we\u2019ve explicitly enabled verbose mode as it will generate more descriptive messages.\nNow let\u2019s take a look at an example stack and see how cdk-nag responds to that. The stack below is a very simple stack which contains an AWS Lambda function processing messages from an SQS queue.\npublic AwsCdkNagDemoStack(final Construct scope, \r\n  final String id, final StackProps props) {\r\n      \r\n  super(scope, id, props);\r\n\r\n  final Queue queue = Queue.Builder.create(this, \"demo-queue\")\r\n                 .visibilityTimeout(Duration.seconds(300))\r\n                 .build();\r\n\r\n  final Function function = Function.Builder\r\n    .create(this, \"demo-function\")\r\n    .handler(\"com.jeroenreijn.demo.aws.cdknag.FunctionHandler\")\r\n    .code(Code.fromAsset(\"function.jar\"))\r\n    .runtime(Runtime.JAVA_11)\r\n    .events(List.of(\r\n      SqsEventSource.Builder.create(queue).build())\r\n    )\r\n    .build();\r\n\r\n  queue.grantConsumeMessages(function);\r\n}\r\n\nAnalyzing results\nNow when you run cdk synth from the command-line, it will trigger cdk-nag and it will automatically scan your resources in the resulting templates and check them for security and compliance issues. Once the scan is done, cdk-nag will either return successfully or return an error message and output a list of violations in a format that is easy to understand. After running cdk synth we will get the following messages in our output.\n[Error at /AwsCdkNagDemoStack/demo-queue/Resource] AwsSolutions-SQS3: The SQS queue is not used as a dead-letter queue (DLQ) and does not have a DLQ enabled. Using a DLQ helps maintain the queue flow and avoid losing data by detecting and mitigating failures and service disruptions on time.\r\n\r\n[Error at /AwsCdkNagDemoStack/demo-queue/Resource] AwsSolutions-SQS4: The SQS queue does not require requests to use SSL. Without HTTPS (TLS), a network-based attacker can eavesdrop on network traffic or manipulate it, using an attack such as man-in-the-middle. Allow only encrypted connections over HTTPS (TLS) using the aws:SecureTransport condition in the queue policy to force requests to use SSL.\r\n\r\n[Error at /AwsCdkNagDemoStack/demo-function/ServiceRole/Resource] AwsSolutions-IAM4[Policy::arn::iam::aws:policy/service-role/AWSLambdaBasicExecutionRole]: The IAM user, role, or group uses AWS managed policies. An AWS managed policy is a standalone policy that is created and administered by AWS. Currently, many AWS managed policies do not restrict resource scope. Replace AWS managed policies with system specific (customer) managed policies.This is a granular rule that returns individual findings that can be suppressed with 'appliesTo'. The findings are in the format 'Policy::' for AWS managed policies. Example: appliesTo: ['Policy::arn::iam::aws:policy/foo'].\r\n\r\n\r\nFound errors\r\n\n\u00a0\nAs you can see cdk-nag spotted some errors and explains what we can do to improve our infrastructure. Usually, it\u2019s quite easy to fix these errors. Level 2 CDK constructs already incorporate some of the best practices, so when using them you will probably find fewer errors compared to using Level 1 constructs.\nThe messages depend on the rule pack you select. For instance, when we switch to the HIPAASecurityChecks rule pack we will get some duplicates but also some additional error messages.\n[Error at /AwsCdkNagDemoStack/demo-function/Resource] HIPAA.Security-LambdaConcurrency: The Lambda function is not configured with function-level concurrent execution limits - (Control ID: 164.312(b)). Ensure that a Lambda function's concurrency high and low limits are established. This can assist in baselining the number of requests that your function is serving at any given time.\r\n\r\n[Error at /AwsCdkNagDemoStack/demo-function/Resource] HIPAA.Security-LambdaDLQ: The Lambda function is not configured with a dead-letter configuration - (Control ID: 164.312(b)). Notify the appropriate personnel through Amazon Simple Queue Service (Amazon SQS) or Amazon Simple Notification Service (Amazon SNS) when a function has failed.\r\n\r\n[Error at /AwsCdkNagDemoStack/demo-function/Resource] HIPAA.Security-LambdaInsideVPC: The Lambda function is not VPC enabled - (Control IDs: 164.308(a)(3)(i), 164.308(a)(4)(ii)(A), 164.308(a)(4)(ii)(C), 164.312(a)(1), 164.312(e)(1)). Because of their logical isolation, domains that reside within an Amazon VPC have an extra layer of security when compared to domains that use public endpoints.\r\n\r\n...\r\n\n\u00a0\nThe HIPAASecurityChecks also finds issues related to Lambda function concurrency and running your Lambda function inside a VPC. As you can see different packs look at different things, so it\u2019s worthwhile to explore the different packs and see how they can help you improve. It\u2019s worth mentioning that cdk-nag does not implement all rules defined in these AWS Config conformance packs. You can check which rules are excluded in the cdk-nag excluded rules documentation.\nSummary\nOverall, cdk-nag is a powerful tool for ensuring that your AWS CDK code and templates adhere to security and compliance best practices. By catching security issues early in the development process, cdk-nag can help you build more secure and reliable infrastructure. I\u2019ve used it in many projects over the last couple of years and it\u2019s adding value. Especially if you work in a team that does not have a lot of AWS experience it shines. If you\u2019re using AWS CDK, I highly recommend giving cdk-nag a try. The example code in this post and a working project can be found on GitHub.\n", "tags": ["aws", "aws cdk", "cloud security", "compliance"], "categories": ["Blog", "Cloud", "Development", "Security"]}
{"post_id": 37344, "title": "Ronald Voets new Managing Director Luminis", "url": "https://www.luminis.eu/blog-en/ronald-voets-new-managing-director-luminis/", "updated_at": "2023-03-30T09:36:05", "body": "M80 accelerates international ambitions with the appointment of Ronald Voets as new Managing Director Luminis\nBrussels, March 30, 2023 \u2013 To further fulfill M80\u2019s ambitions of becoming a best-in-class international player in the field of digital transformation, Ronald Voets will join the team in the role of Managing Director Luminis as of April 1, 2023.\nDutch technology company Luminis, which provides solutions and services in the Cloud and data field, has been part of M80\u2019s digital transformation platform since 2022 with the aim to accelerate its focus abroad. With Ronald Voets\u2019 previous experience at Visma, Raet, PinkRoccade and Exact, he has the strategic capabilities to shape Luminis\u2019 future professionalization and internationalization.\nHans Bossenbroek, Group CEO:\n\u201cWith Ronald we bring on board highly relevant knowledge and experience, which will contribute to our ambitions. I am confident that Ronald will take Luminis to the next level with a customer and employee centric approach.\u201d\nRonald Voets, Managing Director Luminis:\n\u201cM80 aims to create a portfolio of best-in-class specialists in the field of digital transformation. A European IT group at Champions League level. That appeals to me enormously and I look forward to taking Luminis and the software development within the rest of our portfolio to the next level to realize the international ambitions. Together I want to build a future in which our customers are central and Luminis remains an attractive employer for talent.\u201d\nAbout M80\n\nM80 Partners is the management company of M80 Capital, a private equity fund founded in 2018 that invests in companies in Belgium, France, the Netherlands, and Luxembourg.\nThe investment team consists of seasoned private equity professionals, as well as entrepreneurs, former CEOs, and digital pioneers.\nThe company focuses on growth companies in healthcare, consumer, business services and manufacturing. The M80 team invests in companies it can help digitally transform to accelerate revenue and improve operations.\nM80\u2019s digital transformation platform currently includes XPLUS, Luminis, BPSOLUTIONS and Total Design.\nMore information: https://m80partners.com/\n\nAbout Luminis\n\nFounded in 2002, Luminis provides customers with high-quality Cloud and Data solutions. Luminis has partnerships with Amazon Web Services (AWS) and Microsoft, among others.\nIn addition, Luminis is initiator and powerhouse of IT training programme Accelerate with Bosch and the Dutch Tax Authority, among others.\nLuminis has 150 employees and offices in Amersfoort, Amsterdam, Rotterdam, Arnhem, and Apeldoorn and provides its services to, for example, Thales, Alliander, Huuskes, BDR Thermea, bol.com and The Learning Network.\nMore information: https://www.luminis.eu\n\n", "tags": [], "categories": ["Blog", "News"]}
{"post_id": 37210, "title": "The Amplify Series, Part 6: Using the power of AI and Machine Learning with Amplify Predictions", "url": "https://www.luminis.eu/blog-en/the-amplify-series-part-6-using-the-power-of-ai-and-machine-learning-with-amplify-predictions/", "updated_at": "2023-05-08T12:58:26", "body": "In the last part of this series, we added the Amplify Storage category, which allowed us to retrieve and upload files to S3 directly from our application. In this part of the blog series, we will add even more functionality, such as identifying text from an image, converting text to speech, and interpreting the sentiment of text. We will do this by adding a new Amplify category called Amplify Predictions which will allow us to use the power of AI and machine learning to access powerful functionality out of the box.\nAs usual, we will start by adding the category to the project. We will then create a new page for the Amplify Predictions category and add sections to that page per functionality available. This page will serve as a showcase for all the possibilities within Amplify Predictions. In the end, you will have a better understanding of this category and be able to appreciate how easy it is to get this functionality working in your applications.\nAmplify app repository update\nWe will continue in the repository where we left off in the last blog post.\nThere have been 2 extra commits to this repository since the last blog post. These are:\n\nChanging the href to routerLink in the header so that the application behaves as a Single Page Application\nUpgrading Amplify CLI to the newest version (10.8.1)\u00a0 as of the point of writing\n\nNow we are ready to get started and add more functionality to our application.\nIdentify text from uploaded image\nIn this section, we will add functionality that will allow us to upload an image and retrieve the identified text inside the image.\nGenerating the backend resources\nWe will run amplify add predictions with the following options:\n\nCategory: identify\nWhat would you like to identify: Identify Text\nFriendly name: <<use default>>\nIdentify documents: yes\nAccess: Auth users only\n\nThe Amplify CLI output will look similar to this:\n\r\nEvertsons-MacBook-Pro:theamplifyapp evertsoncroes$ amplify add predictions\r\n\u2714 Please select from one of the categories below \u00b7 Identify\r\n\u2714 What would you like to identify? \u00b7 Identify Text\r\n\u2714 Provide a friendly name for your resource \u00b7 identifyTextd230b04a\r\n\u2714 Would you also like to identify documents? (y/N) \u00b7 yes\r\n\u2714 Who should have access? \u00b7 Auth users only\r\nSuccessfully added resource identifyTextd230b04a locally\r\n\nAs with the previous category, we want only to allow access to authenticated users. This command will add a new directory called predictions to our amplify/backend directory, which will contain information needed to create the resources in AWS to support the new functionality.\nBug: Before continuing, we need to add some manual changes to the generated output since there is a bug in the 10.8.1 version of the Amplify CLI. To fix the issue, open the amplify/backend/predictions/identifyText<<id>>/parameters.json file and add the following three key-value pairs to it:\n\n\u201cformat\u201d: \u201cPLAIN\u201d\n\u201caccess\u201d: \u201cauth\u201d\n\u201cidentifyDoc\u201d: \u201cdocument\u201d\n\nWe will run amplify push to create these resources in AWS. The AWS services used here are Amazon Rekognition for image recognition and Amazon Textract for document analysis.\u00a0\nNote that using these services will cost money. Amazon Rekognition will cost around $0.001 per image processed, while Amazon Textract costs around $0.0015 per document processed. As a general rule, be sure to set up a budget with billing alerts for your AWS account so you don\u2019t get surprised by large costs.\u00a0\nThese commands will add the following changes to our repository.\nAdding the predictions page\nThe first thing we need to do is add a new component to our frontend application that will contain all the predictions functionality:\nng generate component components/categories/predictions\nThis will generate the expected files for our component. We will then link up routing to be able to render this component. Refer to this commit for the details so far.\u00a0\nAdding Text identification functionality to the Predictions page\nWe will add a new frontend component that will contain the functionality to upload an image and identify the text in that image:\nng generate component components/categories/predictions/identify-text\nInside our predictions.component.html we must make sure to add the newly generated identify-text component:\n<app-identify-text></app-identify-text>\nInside the identify-text.component.html, we will add:\n<input\r\n\u00a0 type=\"file\"\r\n\u00a0 id=\"imageUpload\"\r\n\u00a0 name=\"imageUpload\"\r\n\u00a0 accept=\"image/png, image/jpeg\"\r\n\u00a0 (change)=\"imageSelected($event)\"\r\n/>\nSimilar to the last blog, this will give us an input that we can use to select images from our device. We have to add logic to react to the image that is selected in our identify-text.component.ts:\n\r\nimport { Component, OnInit } from '@angular/core';\r\n\r\n@Component({\r\n  selector: 'app-identify-text',\r\n  templateUrl: './identify-text.component.html',\r\n  styleUrls: ['./identify-text.component.css']\r\n})\r\nexport class IdentifyTextComponent implements OnInit {\r\n  selectedFile: File | undefined = undefined;\r\n\r\n  constructor() {}\r\n\r\n  ngOnInit(): void {}\r\n\r\n  imageSelected = (e: Event) => {\r\n    const input = e.target as HTMLInputElement;\r\n\r\n    if (!input.files?.length) {\r\n      return;\r\n    }\r\n\r\n    this.selectedFile = input.files[0];\r\n  };\r\n}\r\n\nWe will now be able to select a file and it will be stored in the selectedFile variable. Normally it would be best practice to add this logic to a separate component so that we can reuse this code. However, to keep the blog shorter we will allow duplicate code.\nNow that we have the functionality to upload an image, we need to add a button that does something with the selected image. We will also show the identified text and also add some CSS to identify-text.component.html:\n<div class=\"container-fluid card-background\">\r\n\u00a0 <h2>Identify Text</h2>\r\n\u00a0 <input\r\n\u00a0 \u00a0 type=\"file\"\r\n\u00a0 \u00a0 id=\"imageUpload\"\r\n\u00a0 \u00a0 name=\"imageUpload\"\r\n\u00a0 \u00a0 accept=\"image/png, image/jpeg\"\r\n\u00a0 \u00a0 (change)=\"imageSelected($event)\"\r\n\u00a0 />\r\n\r\n\u00a0 <button class=\"aws-button\" (click)=\"identifyText()\">\r\n\u00a0 \u00a0 Identify Text\r\n\u00a0 </button>\r\n\u00a0 <div class=\"identified-text\" *ngIf=\"identifiedText\">\r\n\u00a0 \u00a0 Identified words:\r\n\u00a0 \u00a0 <div *ngFor=\"let word of identifiedText.text.words\">\r\n\u00a0 \u00a0 \u00a0 {{ word.text }}\r\n\u00a0 \u00a0 </div>\r\n\u00a0 </div>\r\n</div>\nOnce this button is clicked, the identifyText function is called. We will define this function in the following way in our identify-text.component.ts:\n\r\nimport { Component, OnInit } from '@angular/core';\r\nimport { Predictions } from 'aws-amplify'; // <---NEW\r\nimport { IdentifyTextOutput } from '@aws-amplify/predictions'; // <---NEW\r\n\r\n@Component({\r\n  selector: 'app-identify-text',\r\n  templateUrl: './identify-text.component.html',\r\n  styleUrls: ['./identify-text.component.css']\r\n})\r\nexport class IdentifyTextComponent implements OnInit {\r\n  selectedFile: File | undefined = undefined; // <---NEW\r\n  identifiedText: IdentifyTextOutput | undefined = undefined; // <---NEW constructor() {} ngOnInit(): void {} identifyText = async () => {\r\n    if (!this.selectedFile) {\r\n      return;\r\n    }\r\n\r\n    //ADD THIS FUNCTION\r\n    Predictions.identify(\r\n      {\r\n        text: {\r\n          source: {\r\n            file: this.selectedFile\r\n          }\r\n        }\r\n      },\r\n      {}\r\n    )\r\n      .then(response => (this.identifiedText = response))\r\n      .catch(err => console.log({ err }));\r\n  };\r\n\r\n  //OTHER CODE\r\n}\r\n\nIn these changes we import some components we need related to Amplify Predictions. We add two properties to our components, the selectedFile which will hold the latest uploaded image and the identifiedText which will hold the latest results of identified text we received from AWS. We will then call the identify function with the selected image to be sent to AWS. The response will be set to the identifiedText field and the words will show up on the screen.\nThere is one more step we need to take. The Predictions component needs to be supplied with a Provider. This can be done in the main.ts file:\n\r\n#other imports\r\n\r\nimport { Predictions } from 'aws-amplify';\r\nimport { AmazonAIPredictionsProvider } from '@aws-amplify/predictions';\r\nAmplify.configure(aws_exports);\r\nPredictions.addPluggable(new AmazonAIPredictionsProvider());\r\n\r\n#other code\r\n\nOnce this is all done, we can run our application, upload an image, identify the text and see the results on the screen. When I used this\u00a0image: \n\nI got the following result:\n\nThere are more options to play around with, including identifying entities and labels in images and many ways to finetune the results. For more information on this, checkout the Amplify Predictions documentation.\nThe changes for this section can be found in this commit, including the changes needed for the CSS.\nConvert text to speech\nIn this section we are going to add functionality to convert text to speech using Amplify Predictions.\nGenerating the backend resources\nWe will first generate the backend resources needed. We will run amplify add predictions with the following options:\n\nCategory: Convert\nWhat to convert: Generate speech audio from text\nFriendly name: <<use default>>\nSource language: US English\nSpeaker: Kevin \u2013 Male\nAccess: Auth users only\n\nThe Amplify CLI output will look similar to this:\n\r\nEvertsons-MacBook-Pro:theamplifyapp evertsoncroes$ amplify add predictions\r\n\u2714 Please select from one of the categories below \u00b7 Convert\r\n\u2714 What would you like to convert? \u00b7 Generate speech audio from text\r\n\u2714 Provide a friendly name for your resource \u00b7 speechGenerator11c4cfca\r\n? What is the source language? ...  (Use arrow keys or type to filter)\r\n\u2714 What is the source language? \u00b7 US English\r\n\u2714 Select a speaker \u00b7 Kevin - Male\r\n\u2714 Who should have access? \u00b7 Auth users only\r\nSuccessfully added resource speechGenerator11c4cfca locally\r\n\nThis is very similar to what we previously did for the text identification. We can run amplify push again to create the resources in AWS. The AWS service that will be used for this functionality is Amazon Polly. The costs for using Amazon Polly is around $4.00 per 1 million characters.\u00a0\nThese commands will add the following changes to our repository. \nAdding text-to-speech functionality to the Predictions page\nSimilar as we did for the text-identification, we will add a component that will handle all of the text-to-speech functionality:\nng generate component components/categories/predictions/text-to-speech\nInside our predictions.component.html we will make sure to add the newly generated text-to-speech component:\n<app-text-to-speech></app-text-to-speech>\nInside the text-to-speech.component.html, we will add a text input and a button to play the text:\n<div class=\"container-fluid card-background\">\r\n\u00a0 <h2>Text to speech</h2>\r\n\u00a0 <input\r\n\u00a0 \u00a0 type=\"text\"\r\n\u00a0 \u00a0 id=\"textInput\"\r\n\u00a0 \u00a0 name=\"textInput\"\r\n\u00a0 \u00a0 (change)=\"textInputUpdated($event)\"\r\n\u00a0 />\r\n\r\n\u00a0 <button class=\"aws-button\" (click)=\"convertToAudio()\">\r\n\u00a0 \u00a0 Play\r\n\u00a0 </button>\r\n</div>\nNow we need to hook up these elements to our text-to-speech.component.ts and call the Predictions component to do the conversion from text to an audio buffer for us. Finally, we play the audio:\nimport { Component, OnInit } from '@angular/core';\r\nimport { Predictions } from 'aws-amplify';\r\nimport { TextToSpeechOutput } from '@aws-amplify/predictions';\r\n\r\n@Component({\r\n  selector: 'app-text-to-speech',\r\n  templateUrl: './text-to-speech.component.html',\r\n  styleUrls: ['./text-to-speech.component.css']\r\n})\r\nexport class TextToSpeechComponent implements OnInit {\r\n  textInput: string | undefined = undefined;\r\n\r\n  constructor() {}\r\n\r\n  ngOnInit(): void {}\r\n\r\n  convertToAudio = async () => {\r\n    if (!this.textInput) {\r\n      return;\r\n    }\r\n\r\n    Predictions.convert({\r\n      textToSpeech: {\r\n        source: {\r\n          text: this.textInput\r\n        },\r\n        voiceId: 'Amy'\r\n      }\r\n    })\r\n      .then(async result => {\r\n        this.playAudio(result);\r\n      })\r\n      .catch(err => console.log({ err }));\r\n  };\r\n\r\n  playAudio = async (audio: TextToSpeechOutput) => {\r\n    const context = new AudioContext();\r\n    const buffer = await context.decodeAudioData(audio.audioStream);\r\n    const source = context.createBufferSource();\r\n    source.buffer = buffer;\r\n    source.connect(context.destination);\r\n    source.start();\r\n  };\r\n\r\n  textInputUpdated = (e: Event) => {\r\n    const input = e.target as HTMLInputElement;\r\n    this.textInput = input.value;\r\n  };\r\n}\r\n\nBug: There is currently, at the time of writing, a bug in Amplify that does not allow us to use the voiceId \u201cKevin\u201d, which we selected when creating the backend resources. Selecting the voiceId \u201cAmy\u201d works, so we will use that.\nIn the code snippet above we create a field that will hold the text in the textInput. We have a method that is called to convert the text to audio using the Predictions component. The convert function will send an http request to Amazon Polly and an audioBuffer will be returned. This can be given to an AudioContext component to play to audio in the browser.\n\nThe changes made in these steps can be found in this commit. \nInterpret the sentiment of text\nThe final set of functionality we are going to add is the ability to interpret the sentiment of text.\u00a0\nGenerating the backend resources\nWe will add the backend resources by again running amplify add predictions with the following options:\n\nCategory: Interpret\nFriendly name: <<use default>>\nKind of interpretation: ALL\nAccess: Auth users only\n\nThe Amplify CLI output will look similar to this:\nEvertsons-MacBook-Pro:theamplifyapp evertsoncroes$ amplify add predictions\r\n\u2714 Please select from one of the categories below \u00b7 Interpret\r\nOnly one option for [What would you like to interpret?]. Selecting [Interpret Text].\r\n\u2714 Provide a friendly name for your resource \u00b7 interpretText9001208a\r\n\u2714 What kind of interpretation would you like? \u00b7 All\r\n\u2714 Who should have access? \u00b7 Auth users only\r\nSuccessfully added resource interpretText9001208a locally\r\n\nAnd now we can run amplify push to create the resources in AWS. The AWS service that will be used for this functionality is Amazon Comprehend. The pricing for this service can be found here.\u00a0\nThese commands will add the following changes to our repository. \nAdding text-interpret functionality to the Predictions page\nWe will first create a component that will handle the text-interpret functionality:\nng generate component components/categories/predictions/text-interpret\nInside our predictions.component.html we will make sure to add the newly generated text-interpret component:\n<app-text-interpret></app-text-interpret>\nInside the text-interpret.component.html, we will add a text area input and a button to trigger the interpretation of the text and a text to show what the sentiment is:\n<div class=\"container-fluid card-background\">\r\n\u00a0 \u00a0 <h2>Interpret text</h2>\r\n\u00a0 \u00a0 <textarea\r\n\u00a0 \u00a0 \u00a0 \u00a0 id=\"textAreaInput\"\r\n\u00a0 \u00a0 \u00a0 \u00a0 name=\"textAreaInput\"\r\n\u00a0 \u00a0 \u00a0 \u00a0 rows=\"5\"\r\n\u00a0 \u00a0 \u00a0 \u00a0 cols=\"66\"\r\n\u00a0 \u00a0 \u00a0 \u00a0 (change)=\"textInputUpdated($event)\"\r\n\u00a0 \u00a0 ></textarea>\r\n\r\n\u00a0 \u00a0 <button class=\"aws-button interpret-button\" (click)=\"interpretText()\">\r\n\u00a0 \u00a0 \u00a0 \u00a0 Interpret\r\n\u00a0 \u00a0 </button>\r\n\u00a0 \u00a0 <div *ngIf=\"interpretation\">\r\n\u00a0 \u00a0 \u00a0 \u00a0 Interpretation = {{ interpretation.textInterpretation.sentiment?.predominant }}\r\n\u00a0 \u00a0 </div>\r\n</div>\nNow we will update the text-interpret.component.ts to hook up the functions defined here and call the Predictions component to interpret the text:\n\r\nimport { Component, OnInit } from '@angular/core';\r\nimport { Predictions } from 'aws-amplify';\r\nimport { InterpretTextCategories, InterpretTextOutput } from '@aws-amplify/predictions';\r\n\r\n@Component({\r\n  selector: 'app-text-interpret',\r\n  templateUrl: './text-interpret.component.html',\r\n  styleUrls: ['./text-interpret.component.css']\r\n})\r\nexport class TextInterpretComponent implements OnInit {\r\n  textInput: string | undefined = undefined;\r\n  interpretation: InterpretTextOutput | undefined = undefined;\r\n  constructor() { }\r\n\r\n  ngOnInit(): void {\r\n  }\r\n\r\n  interpretText = async () => {\r\n    if (!this.textInput) {\r\n      return;\r\n    }\r\n\r\n    Predictions.interpret({\r\n      text: {\r\n        source: {\r\n          text: this.textInput\r\n        },\r\n        type: InterpretTextCategories.ALL\r\n      }\r\n    }).then(result => this.interpretation= result)\r\n    .catch(err => console.log({err}))\r\n  }\r\n\r\n\r\n  textInputUpdated = (e: Event) => {\r\n    const input = e.target as HTMLInputElement;\r\n    this.textInput = input.value;\r\n  };\r\n}\r\n\nWe can now try our entering text in our text area and doing a sentiment check. I Googled \u201chappy poems\u201d\u00a0 and entered the first one I found:\n\nTry adding different types of text to check the interpretation. Furthermore, the response to the interpret function also contains more information related to the interpretation of the text. Check the documentation for more information and possibilities.\u00a0\nThe changes made in these steps can be found in this commit\nUp next: Tracking app usage with Amplify Analytics\nIn this blog we have used Amplify Predictions to identify text in images, convert text to speech and interpret the sentiment of text. There are more possibilities in this category, however, these examples should give you an idea. In the next article, we will look at using Amplify Analytics to collect analytics data for your application.\n", "tags": ["amplify", "angular", "aws", "cloud"], "categories": ["Blog", "Cloud"]}
{"post_id": 37069, "title": "Hosting a static react website on Amazon S3 with CDK", "url": "https://www.luminis.eu/blog-en/hosting-a-static-react-website-on-amazon-s3-with-cdk/", "updated_at": "2023-02-27T11:15:31", "body": "I recently migrated an existing website to the cloud and decided to share the process.\nThe cloud platform I used for this task was Amazon Web Services (AWS). In AWS there is a service called S3 (Simple Storage Service). S3 \u201cBuckets\u201d, which are similar to file folders, store objects, which consist of data and descriptive metadata. Why are we using S3 Buckets to host our website you ask? Because it is cheap!\nAWS charges customers for storing objects in a bucket and for transferring objects in and out of buckets, this is perfect for a static website that doesn\u2019t have to change much. The average price of hosting a website on an S3 bucket is $0.50 per month when using the free tier. When outside the free tier, it will cost up to $1-3 per month.\nSo hosting a website on Amazon S3 is a simple and cost-effective way to make your website available to the world. In this blog post, we\u2019ll discuss the steps of hosting a React website on Amazon S3 with the use of the AWS Cloud Development Kit (CDK), which is a software development framework to define cloud infrastructure as code.\nYou can check out the result here\nWhy use AWS CDK?\nThe main reason for using the AWS CDK is to enable developers to define and manage their cloud infrastructure using familiar programming languages and tools, rather than having to write templates in JSON or YAML. This provides several benefits, including improved efficiency, increased agility, better collaboration, and improved security. Overall, the AWS CDK provides a flexible, powerful, and efficient way to define, provision, and manage cloud infrastructure, helping organizations to be more productive and successful in their cloud computing efforts.\nRequirements\nThe requirements of the migrating assignment were:\n\nThe site redirects from HTTP to HTTPS\nusing a CloudFront distribution\nand ACM certificate\nmove the existing domain name to AWS\n\nLet\u2019s get started\nBefore you start, ensure you have an AWS account and a React app you want to host. The code uses the AWS Cloud Development Kit (CDK), which is a software development framework to define cloud infrastructure as code.\nWe start by importing the necessary constructs, services, and utilities from the AWS CDK library.\nimport { Construct } from \"constructs\";\r\nimport * as s3 from \"aws-cdk-lib/aws-s3\";\r\nimport * as iam from \"aws-cdk-lib/aws-iam\";\r\nimport * as cloudfront from \"aws-cdk-lib/aws-cloudfront\";\r\nimport { RemovalPolicy, Stack, StackProps } from \"aws-cdk-lib\";\r\nimport * as s3deploy from \"aws-cdk-lib/aws-s3-deployment\";\r\nimport { HttpMethods } from \"aws-cdk-lib/aws-s3\";\nThe StaticSite class extends the Stack class from the AWS CDK library, which is used to create a CloudFormation Stack. The class takes three parameters: the scope of the construct, the ID of the construct, and the properties of the Stack (in this case, StaticSiteProps).\nThe StaticSiteProps interface extends the StackProps interface and includes an additional parameter: cloudfrontCertArn. This parameter is used to configure the CloudFront distribution, which serves as the primary access point to the static site.\nexport interface StaticSiteProps extends StackProps {\r\n  cloudfrontCertArn?: string;\r\n}\r\n\r\nexport class StaticSite extends Stack {\r\n  constructor(scope: Construct, id: string, props: StaticSiteProps) {\r\n    super(scope, id, props);\r\n   // put your infrastructure here\r\n\r\n}\nIn the constructor, we create an S3 bucket to store the site files. The bucket is set up to prevent public access and has a removal policy of DESTROY, which means that the bucket and its contents will be deleted when the Stack is deleted.\nconst websiteBucket = new s3.Bucket(this, \"WebsiteBucket\", {\r\n  bucketName: `my-website-bucket`,\r\n  publicReadAccess: false, // no public access, user must access via cloudfront\r\n  removalPolicy: RemovalPolicy.DESTROY,\r\n  autoDeleteObjects: true,\r\n  cors: [\r\n    {\r\n      allowedHeaders: [\"*\"],\r\n      allowedMethods: [HttpMethods.GET],\r\n      allowedOrigins: [\"*\"],\r\n      exposedHeaders: [],\r\n    },\r\n  ],\r\n});\nThen we create an Origin Access Identity (OAI), which is a special CloudFront user that is used to grant access to the objects in the S3 bucket. The OAI is granted access to the objects in the S3 bucket through an IAM policy statement.\nconst identity = new cloudfront.OriginAccessIdentity(this, \"id\");\r\n\r\nwebsiteBucket.addToResourcePolicy(\r\n  new iam.PolicyStatement({\r\n    actions: [\"s3:GetObject\"],\r\n    resources: [websiteBucket.arnForObjects(\"*\")],\r\n    principals: [\r\n      new iam.CanonicalUserPrincipal(\r\n        identity.cloudFrontOriginAccessIdentityS3CanonicalUserId\r\n      ),\r\n    ],\r\n  })\r\n);\nNext, we create the CloudFront distribution, which serves as the primary access point to the static site. The CloudFront distribution is set up to redirect all traffic from HTTP to HTTPS and is configured to use the ACM certificate specified by the cloudfrontCertArn property. This ACM certificate had to be made manually before using it in the code.\nconst distribution = new cloudfront.CloudFrontWebDistribution(\r\n  this,\r\n  \"cloudfront\",\r\n  {\r\n    originConfigs: [\r\n      {\r\n        s3OriginSource: {\r\n          s3BucketSource: websiteBucket,\r\n          originAccessIdentity: identity,\r\n        },\r\n        behaviors: [\r\n          {\r\n            viewerProtocolPolicy:\r\n              cloudfront.ViewerProtocolPolicy.REDIRECT_TO_HTTPS,\r\n            allowedMethods: cloudfront.CloudFrontAllowedMethods.GET_HEAD,\r\n            compress: true,\r\n            isDefaultBehavior: true,\r\n          },\r\n        ],\r\n      },\r\n    ],\r\n    viewerCertificate: {\r\n      aliases: [\"search-matrix.luminis.amsterdam\"],\r\n      props: {\r\n        acmCertificateArn: props.cloudfrontCertArn,\r\n        sslSupportMethod: \"sni-only\",\r\n      },\r\n    },\r\n    defaultRootObject: \"index.html\",\r\n    errorConfigurations: [\r\n      {\r\n        errorCode: 403,\r\n        responseCode: 200,\r\n        responsePagePath: \"/index.html\",\r\n      },\r\n    ],\r\n  }\r\n);\nFinally, the code deploys the site files to the S3 bucket using the BucketDeployment class from the AWS CDK. The site files are sourced from the ./front-end/build directory and are deployed to the websiteBucket created earlier in the code.\nnew s3deploy.BucketDeployment(this, \"DeployWebsite\", {\r\n    sources: [s3deploy.Source.asset(\"./front-end/build\")],\r\n    destinationBucket: websiteBucket,\r\n    distribution,\r\n    });\r\n}\nConclusion\nIn conclusion, this code sets up a static site infrastructure in AWS, using the AWS CDK. The infrastructure includes an S3 bucket to store the site files, a CloudFront distribution to serve the site, and an IAM policy to grant access to the objects in the S3 bucket. The site files are deployed to the S3 bucket using the BucketDeployment class from the AWS CDK.\nAnd that\u2019s it! You have your website up and running in the cloud, but what if you wanted to make a change to your website? Do you have to do everything over again? No! This is where Continuous Integration and Deployment (CI/CD) comes into play. Setting up a CI/CD pipeline in AWS will be explained next time!\n", "tags": [], "categories": ["Blog", "Cloud"]}
{"post_id": 36999, "title": "7 lessons about success in digital product development", "url": "https://www.luminis.eu/blog-en/7-lessons-about-success-in-digital-product-development/", "updated_at": "2023-02-20T20:01:30", "body": "In my first years as a UX designer, about 15 years ago, there was one question I feared: \u201cCan you\u202fprove\u202fthat your design will actually result in a good user experience?\u201d I did not know how to prove that my work was a success yet. My instincts told me my work would smoothly guide users in whatever they needed from the product, but I could not hand over any hard evidence.\nThe best I could do was to test a prototype with users; a time-consuming undertaking that most of the time did not even deliver any hard evidence but would uncover flaws in my design that called for a next design iteration. To be honest: I would rather stay with my instinct telling me I had done a good job, than face reality and discover I should have done better. Because that was how it felt: whenever a design flaw came to light, I thought I had failed as a designer. And I soooo needed that pat on my back.\nI do not fear those questions anymore and I even started to ask them to myself. Because there is no shame, but a lot of value in uncovering design flaws. And time spent on testing and experimenting is not wasted when it saves you from building the wrong thing.\nLesson 1: There is no such thing as the perfect design\nEventually I learned that designing (and building) complex digital products is a process of learning just as much as creating. There is no such thing as the perfect design; there is only a solid process to continuously improve a product\u2019s user experience. And that actually applies to digital product development in general.\nQuestion: When you\u2019re in a company developing a digital product or service, when do you feel you are doing a good job? Is your success about:\n\nthe lines of code written,\nstory points accomplished,\nproduct features added,\ndeadlines met,\n\u2026?\n\nOr is it about:\n\nrevenue increased,\nusers time spared,\ncustomer satisfaction improved,\ncost of ownership lowered,\n\u2026?\n\nIf your answer falls into the first category, I hope you have been feeling successful lately. But even if you have, chances are that the product you\u2019ve been working on has not been successful at all. The first category are output parameters; metrics of what you have\u202fproduced. Not about the\u202fvalue\u202fthat was delivered by your work. Measuring output instead of outcome is not wrong, but can be misleading. I\u2019ve seen well-oiled teams successfully burn through a backlog at a dizzying pace, absolutely convinced they were on top of their game. Only to discover 6 months later that the problem they thought they were solving for their customers, was not a big issue for them at all. Great work, but no value.\nThe second category, which are the outcomes, can be measured as well, but not as easy to measure as the first category. They represent value delivered by your work on the product, for either the business, the customer or the user. And that is what you as a developer, designer or manager of digital products should be looking at to know whether you are successful: did your work result in the value as intended?\n\nLesson 2: Separate output from outcome\nIt is not wrong to keep track of your production with output parameters; it gives you solid insights in your development process and helps you and your team monitor, tweak and optimize. Output parameters do in fact impact outcome in many ways (just think about the number of bugs found). But meeting your target on output parameters does not guarantee a positive outcome for the business, customer or user. Though\u202fnot\u202fmeeting your target on your output mostly\u202fwill\u202fhave a negative effect on the outcome.\nBest is to have data on both output and outcome. But\u202fwhat\u202fshould you be measuring? On the output side the DORA metrics (deployment rate, lead time for changes, change failure rate and time to restore service) give a good indication of your product team\u2019s performance. But measuring success on the outcome side is different. To know what you should be measuring, you first need to know what you are trying to accomplish for the business, customer or user. And this is where things often get merky, because: why are we actually building this thing?\nLesson 3: Make the business everybody\u2019s business\nAt a lot of companies I encounter a disconnect between the business people and the people that are developing the product or service. I\u2019ve often seen one of these things happening:\n\na lack of vision and/or strategy on the business side, leaving it up to the development team (and not getting the right thing built);\na lack of communication between business and development, both being frustrated (and not getting the right thing built);\na lack of understanding between business and development, leading to a misinterpretation of the business\u2019 intent (and not getting the right thing built);\na lack of strategic focus, with the business prioritizing all things that come up and seem important (and not getting the right thing built);\n\nDeveloping a product or service ultimately serves a function for the customer and user, but also should add value for the company developing it. Product teams should know what success looks like for both the customers and users as well as for the company, and how their work contributes. How else could you expect them to make the right decisions?\nA lot has been written about how to bridge the gap between business and development. An excellent read on this is \u201cEscaping the Build Trap\u201d, by Melissa Perri. In her book she stresses the importance of aligning strategy throughout the company, in order to keep developing your product effectively towards delivering the intended value. She gives the example of having three different recurring meetings to review progress towards strategic intents and to make strategic decisions on product level:\n\nBusiness review: Financial outcomes like revenues and costs, progress towards strategic intents and how product initiatives are contributing to this progress (and adjust product strategy accordingly)\nProduct initiative review: Progress made on the initiatives and how experiments and options are contributing to the initiative (and adjust initiative strategy accordingly)\nRelease review: Functionality that will be shipped, success metrics and roadmap updates (so marketing, sales and executive teams are aware).\n\nNot all strategic decisions will be made in these meetings. But they do help to keep everybody in the company aligned strategically.\nLesson 4: Focus on value instead of features\nProducts are delivering value for the company by delivering value for the customers and users. So, before thinking of any features, product teams should be exploring and answering these questions:\n\nWhere is the value for your customers and users (fast delivery, better service, \u2026)\nWhere is the value for your business (increase revenue, reduce costs, \u2026\n\nYou want to measure this value in order to determine how successful your product is and know if your efforts are paying off. Vision and strategy on how the company can deliver this value should be shared throughout the company. Also with the product team, so that product goals and initiatives can be aligned with company goals and strategy. Strategy maps and roadmaps help, both at company as product level (I will cover that in another blog, so stay tuned).\nThere are many ways to measure the value of a product, it depends on the specific goals and objectives of the product which ones are the most useful. Some common examples of metrics used to measure the success of a product are:\n\nUser Engagement: Metrics such as active users, time spent on the product, and frequency of use can be used to measure how engaged users are with the product.\nConversion Rates: Measuring the number of users who convert from free to paid plans or the percentage of website visitors who become customers can help determine if the product is delivering value.\nCustomer Satisfaction: Feedback from users, such as Net Promoter Scores or surveys, can provide insights into how satisfied customers are with the product.\nBusiness Metrics: Revenue, profit margins, and other financial metrics can help determine if the product is delivering value to the business.\nUser Retention: Measuring the number of users who return to the product over time can help assess the stickiness of the product and how well it meets user needs.\n\nLesson 5: Ask yourself what is preventing you to deliver (more of) this value\nAfter settling what value(s) the product should deliver, you can define success indicators on your product and figure out the way to measure these. To get the moving numbers (like those mentioned in the examples above) in the right direction, you need to explore the problem or opportunity and discover solutions:\n\nWhat is preventing us to deliver (more of) this value?\nWhich of these problems should we be solving first?\nWhat should we measure to know the impact of a solution to this problem?\n\nLet\u2019s take an example: Customer satisfaction is one of our core values and we are using Net Promoter Scores to measure this. We are seeing that our scores are dropping, so before thinking of measures, we start by looking for the cause: what is causing this lower customer satisfaction? Because in this example our customer is also our user, we are conducting user research to find out. Our quantitative research shows a lower task completion rate since last quarter\u2019s release. Subsequent qualitative research then points out that many users don\u2019t understand the new filter feature that we added in that release.\nOnce we have targeted the problem, we can explore possible solutions:\n\nWhat could be our options to solve the problem?\nWhat are direct success indicators in our solution and how could we measure them?\nWhat option emerges as our best bet, after preliminary experiments?\n\nIn our example, options to solve the filter problem could be to remove the filter altogether, to improve the usability of the feature or to make filtering optional instead of mandatory. We run some A-B tests with these options and find out that task completion increases most when the filter is removed. But this filter was introduced after previous research pointed out that users wanted it, so removing it might not be our best option after all. The usability improvement of the feature (a short tutorial explaining the filter feature to the user) did also result in slightly higher task completion rates, but not as much as making the filtering optional instead of mandatory. This last option seems to be our best bet in our example.\nBy first discovering what is preventing us to deliver more value, and experimenting with multiple solution options, we can be confident that our selected solution will effectively solve a real problem and therefore will lead to value increase.\n\nLesson 6: Experiments are meant to learn first and to scale later\nRunning experiments might seem like a waste of resources, because it means throwing away the less-optimal solutions. But as you\u2019ve seen in our example, it also teaches us about the users\u2019 behavior and preferences, and it eliminates most of the risk of releasing the wrong thing. Still, we should keep the costs of these experiments as low as possible. Sometimes that means using paper prototypes or other ways to experiment with little or no coding. The most reliable experiments are the ones conducted in a live production environment though. Luckily, cloud technology enables us to run these live experiments more easily and to scale the right solution faster.\nTo know whether you really are successful after scaling the right solution, you need to keep measuring:\n\nInitiative-level success indicators, like Task Completion rates.\nProduct-level outcomes, like Customer Satisfaction;\nCompany-level results, like Revenue;\n\nResults at company level are so-called lagging success indicators; it will take some time to notice the effect of your product initiatives, and it provides only indirect evidence of your success; results at this level are affected by many things. Outcomes at product level are also lagging indicators, but provide more direct evidence for your success developing the product. Lastly, success indicators at initiative level are the most direct way of measuring success, because they show whether your solution is working.\n\nLesson 7: Not wrong long\nIt is kind of scary to measure success, because you might find your investment and work has not been paying off. Nobody likes hearing that, but let\u2019s be realistic: the sooner you know you took a wrong turn, the sooner you can correct your course towards delivering value. Experimenting and measuring success doesn\u2019t completely eliminate the risk of being wrong, but it does make sure you are not wrong long.\n", "tags": [], "categories": ["Blog", "Concepting &amp; UX"]}
{"post_id": 36848, "title": "Learn Elasticsearch from my liveProject at Manning", "url": "https://www.luminis.eu/blog-en/learn-elasticsearch-from-my-liveproject-at-manning/", "updated_at": "2023-02-16T10:02:43", "body": "I like to share my knowledge. I have written 100s of blogs, performed 10s of talks at conferences, and trained 100s of people during onsite training programs. This week I reached a new milestone in my knowledge-sharing endeavor. This week I finalized my liveProject at Manning called Elasticsearch for a Search API. You can read my experiences creating this liveProject in this blog post. If you only want to know what it is, jump to the end of this post.\nHow I became an author for Manning\nI had the idea of writing a book for a long time. How do you start with such an endeavor? To gain experience, I began reviewing books from Manning and Packt publishing. A few years ago, Manning asked me to help with the book Elasticsearch in Action. The author could not finish some chapters of the book, and I had the chance to rewrite them. The book has my name on the cover as a technical editor, which makes me proud. I never got to writing a complete book, but in May 2022, Manning contacted me and asked if I would like to create a liveProject. I liked the idea and checked with Luminis if it was ok, which of course, was no problem. On the contrary, they loved the idea.\nHow does it work\nI started with writing an overview of the project. The outline gave an idea about what the students learn, the different projects in the series, and the tools to be used by the students. After approval of the concept, the second step is a detailed plan for the complete series. For a liveProject to succeed, students must feel connected. I came up with an online shoe store called \u201cSneakers To The Max\u201d. The student is a search relevance engineer responsible for the search service used by customers through the website and the mobile app. Some experts in the domain reviewed my proposal; they were all very positive. After approving the detailed plan, I signed the contract and got started.\nI started writing the boilerplate code to get students underway. I wanted the student to lose as little time as possible and begin learning by running the code.\nThe next step was breaking up the solution into multiple assignments grouped into four projects, projects into milestones, and milestones into steps. Each milestone ends with a short examination. Besides the code, the questions, and the texts, I created a few images. Which were then transformed by aa design artist, and I like the sneaker created based on an elementary sketch from me. Infographics are available for each project, and I had to record the audio descriptions for these infographics.\nThe sketch I send the designer\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 Logo for Sneakers to the Max\nTesting the project\nAt first, one person acted as an alfa tester. This tester is the first person that gives feedback about the project. My alfa tester had some excellent suggestions. In general, he was positive about the project. After some corrections, it was time for the beta testing phase. During beta testing, more people get to try out the project. Communication through the discussion board, lots of review comments. I was lucky with the beta testers. Again, there is a lot of positive feedback and excellent suggestions for improvement. I learned that one of the projects was too easy. I took a few weeks to create a new milestone for one of the projects.\nNow it is your turn\nWith the liveProject finished, it is time for students to start working on it. Try out the series if you want to sharpen your Elasticsearch skills and are not afraid of a bit of python. For now, the first project is free to try. I hope to see you in the discussion group or during the online call to pass the certification exam after completing all projects.\nhttps://www.manning.com/liveprojectseries/elasticsearch-for-a-search-api-ser\nCover image for the series at Manning\nQuestions about search, Elasticsearch, AWS OpenSearch, Solr\nIf you do not want to learn Elasticsearch yourselves but have questions about your search solution, feel free to contact us. We have the knowledge and experience to create the best search solution for all situations\n", "tags": [], "categories": ["Blog", "News", "Search"]}
{"post_id": 36764, "title": "Resolving the paradox of cloud-native solutions: more agility without total control", "url": "https://www.luminis.eu/blog-en/resolving-the-paradox-of-cloud-native-solutions-more-agility-without-total-control/", "updated_at": "2023-04-20T13:45:49", "body": "Being cloud-native enables organizations to implement changes at the speed of light, but leveraging cloud technology also means giving away control. That sounds like a paradox, but it resolves once you realize that you don\u2019t need to give away control completely. But what do you keep in-house, and what power do you give to your cloud technology and third-party software vendors? Sneak peek: focus your efforts on your core competitive edge while smartly integrating with the non-differentiating stuff.\nDon\u2019t feel like reading? Watch this post (and more) as a video\nLast week, I joined an XPLUS Breakfast Session to talk about how organizations can leverage cloud technology to shorten their time-to-value. Our client Ctac, personified by Ivo van der Raad, started the webcast by presenting their cloud plans for their XV Retail Suite. Exciting stuff, so feel free to come back here later and watch the session first.\nI used my ten minutes to look at this typical retail challenge from a technical point of view. This blog post is an extended transcript of that presentation, with the upside that you don\u2019t have to look at my face the whole time. Win-win.\nNow, back to resolving the paradox.\nThe need for speed\nBefore I get into the technical stuff, let me briefly explain the problem in business terms first.\nConsumer-facing software is moving faster than ever, with customers expecting increasingly safer, better, and faster solutions. Evolving technology is a significant enabler and driving force behind this acceleration. Successful solution providers combine tailor-made software, third-party packages, and SaaS offerings to craft the stuff their customers crave. That\u2019s impressive! But it also sounds like a magic trick. An illusion, if you will. How is it possible to hand over so much control to external parties and maintain the agility needed to compete in the market?\nSmart integration, that\u2019s how.\nThe integration challenge: from central management to distributed control\nThree focus areas emerge if we compare this current reality to the world of a decade or more ago.\n\u00a0\n\n\u00a0\nThe first is a shift from centrally managed, self-hosted application landscapes to distributed solutions. Let\u2019s say an organization has a few systems containing customer data and many custom-made services and clients that integrate with them. A decade ago, their IT department would have self-hosted this custom and third-party software combination, employing a variety of service buses, API endpoints, and direct messaging solutions. They would have slowly evolved the landscape whenever they had to build new functionality or add a new system.\nThanks to the advent of Software as a Service, IaaS, PaaS, and high-level cloud services, organizations need to move much faster to keep up. Their landscapes are fragmenting. Keeping expertise in-house is costly and reduces innovation budgets.\nHere emerges the second challenge. The technology market\u2019s shift to SaaS and managed cloud services, combined with the never-ending desire for better customer functionality, forces companies to gain traction and move more dynamically.\nThat can only mean, whether they want to or not, that builders need to let go of some of the control they used to have. But they still need to integrate everything. They must learn how to leverage new ways of integrating their building blocks, and I propose they use those giant vendors\u2019 shoulders and shift a lot of the heavy lifting onto the public cloud and SaaS providers\u2019 offerings.\nThe cloud-native integration paradox: more agility, less control\nAll right, now we are looking at what seems to be a paradox. Organizations need to be in total control, but at the same time, they need to hand over some control to external parties. What gives?\nOrganizations need to be able to move fast to respond to the changing needs of their customers. They must deliver changes into production quickly, see what works, and continually adapt based on what they learn. To do so, they require the technology and skills to precisely determine what their customers want and give it to them. They must practice deploying changes rapidly but safely. And they need to develop a software delivery process that is solid and improving over time. In other words, organizations need to be in complete control.\n\u00a0\n\n\u00a0\nHowever, I claim that they hand over some control to external parties. That means they no longer have total power over all the underlying infrastructure or the services and software they need to combine into a solution.\nSo here we have the paradox of delivering value, end-to-end, through software using the cloud and software as a service: the need for more agility while simultaneously leveraging the offerings of external vendors.\nLuckily, this is not an actual paradox. It is possible to increase agility while giving up control, but the trick is to be smart about it. Let me explain.\nBuild, buy, outsource, integrate? All of the above (but: pick your battles)\n\n\u00a0\nLet\u2019s look at the choices the fictional organization behind this highly successful (but fictional, like I said) app has made.\nIn the image above, you see an example architecture of an AI-powered photo retouching app, broken down into its components and plotted on a Wardley Map. The y-axis shows us how valuable elements of the solution are to end users. The higher, the more visible a component is to them, and the more valuable they deem it: a user generally does not care if you use platform X or Y as long as their app works reliably. The x-axis is about the evolutionary stage of the components. Just-discovered technology, for example, will appear on the left, but the older it gets, the more it will shift to the right. Over a decade ago, container platforms were relatively new, so they would have been plotted much farther left than they are now.\nAt the top left, we see the mobile application users interact with daily. It uses custom algorithms running somewhere in the backend or the app itself. This functionality is by far the most customer-visible and valuable thing they own; that\u2019s why it\u2019s so high on the map. So, they want complete control: they design, develop, and deploy the app and algorithms themselves.\nTo further evolve the algorithms, their data scientists and business stakeholders need to gain continuous insight into their users\u2019 behavior. The blue dots are the systems and services that help them collect, combine, transform, and finally visualize their user\u2019s behavioral data using managed services, third-party tooling, and a hosting platform. They don\u2019t need complete control here, so they opt to hand off its management to service providers. However, they do need control over integrating this part of their landscape.\nThen there is the so-called boring stuff, at least from a software creator\u2019s point of view, at the bottom right in orange: the critical infrastructure. Their customers do not care one bit if they manage this themselves. But like water and electricity, it is fundamental for everything built on top to work. So, they want to exert less control here: they don\u2019t want to spend costly resources managing all this highly available, virtually infinitely scalable stuff. They want to leverage the years of experience of a party that knows how to run critical infrastructure while paying only for what they use. So they outsource this.\nHere we have the first part of the answer to the so-called paradox of less control and more speed. Now let\u2019s solve the integration part of this puzzle.\nIntegrating the solution: less control = looser coupling\nHere is the same solution from an integration perspective.\n\u00a0\n\n\u00a0\nWe are still looking at a mix of self-hosted applications integrated with managed, third-party services and externally hosted SaaS. At the infrastructure level, there is a mixture of public cloud, private cloud, or a private data center, and the magical SaaS infrastructure.\nThe (still fictional) AI-powered photo retouching app company smartly combines integration solutions to make something of these mixed ingredients. The general rule of thumb here is simple: the more control they have over solution lifecycles, the tighter they can couple them. And vice versa: less control, looser coupling.\nOn the left-hand side, there is all the stuff they control entirely. They integrate using direct connections, push notification services, and message queues.\nIn the middle are the managed services they employ. They use a central event bus, so they decouple the integration slightly more than on the left by introducing a highly configurable, flexible, but fast integration solution in the middle. It could be a serverless event router like Amazon Eventbridge or Azure Event Grid.\nAt the far end, on the right-hand side, in orange, are the external parts of our landscape: SaaS offerings, maybe another cloud, or externally hosted service. Since the photo app company has no control over them, they decouple these elements using data transfer services or an event bus at the edge of their specific hosting platforms. This way, the organization can still evolve its end-to-end solution, and the SaaS and external service providers can continuously update their offerings independently. In the case of an API or contract change, the AI photo app developers only need to change the data transformation logic, while the rest can keep running as before.\nNow the speed-up-with-less-control paradox is resolved. Again, we need to realize that our landscape consists of groups of subsystems, grouped by their properties \u2014 in this case, the amount of evolutionary control the organization has over each solution piece. From this perspective, they can then integrate them using fit-for-purpose integration solutions. They still control their total solution, just not all its internals.\nExcellent, but the story may not convince you entirely yet \u2014 the company, its users, and its success are fictional, after all. Let me try to convince you with a real-world example where I was closely involved.\nOHRA: cloud-native development and integration from the trenches\nOHRA, one of the Netherlands\u2019 biggest directly writing insurance companies, asked Luminis to help migrate their application landscape from their on-premise data center to the AWS cloud. Their application landscape was large, with almost 1,000 integrations between internal and external services, clients, and databases. Did I mention the fixed deadline? About a year from the start. We helped them make it, partly thanks to the abovementioned integration strategy.\nMost of OHRA\u2019s moving parts are under their control, so they are integrated using a mix of direct messaging and message queues. When moving to AWS, we helped them mostly replatform the applications from application servers running Java applications to a managed Kubernetes cluster running containers. Much of OHRA\u2019s data flows through third-party packages spread over their application landscape. So we employed the tactic we saw on the previous slide: loose coupling for SaaS integrations using data transfer logic at the edge and asynchronous communication with third-party apps installed in their application landscape.\nA big help in making OHRA\u2019s cloud migration deadline was their existing service-oriented architecture, an excellent stepping stone towards a more event-driven and, thus, cloud-native architecture.\nFurther reducing complexity and costs, improving agility\nAfter migrating, the cloud provided OHRA with new opportunities to evolve its solutions. One example is the modernization of batch applications.\nPreviously, scheduled jobs were deployed on self-managed servers, running 24/7, even though most of them only actively did something for a few minutes to a couple of hours a day. In the public cloud, you should pay only for what you use, so I helped OHRA envision a more cloud-native and cost-effective way of creating short-running jobs.\n\u00a0\n\n\u00a0\nDuring two weeks, I led a team that re-architected an existing solution and delivered a modern, lean batch application: a distributed data processor made from a combination of serverless building blocks like Amazon EventBridge rules, Amazon SQS queues, and AWS Lambda Functions. As you can see in the image above, this drastically reduced the amount of code needed (and thus its complexity), the resources used, and the costs accrued.\nOHRA can use this cloud-native way of working moving forward and enable their teams to free up time and resources, which they can then spend on stuff that will help the business grow its competitive edge.\nOne more time: the paradox resolved\nSo, now you have a complete, high-overview answer to the faux paradox of gaining speed while not having complete control. Allow me to recap.\nFirst off, from a market perspective, companies are forced to move away from centrally controlled, neatly contained solutions running in their data centers towards integrating their locally managed custom building blocks with third-party software and externally hosted SaaS solutions.\nThere is a two-part solution to this problem.\nThe first part concerns being smart about spending valuable time and resources. Organizations require complete control of the software development and deployment lifecycle for the user-facing features they develop. Everything that powers this end-to-end solution \u2014 but is less visible or even completely invisible to the customer \u2014 can be hired as a service or bought as a product from a cloud provider.\nThe other half of the answer concerns being smart about integrating a fragmented landscape. I identified three groups, from fully controllable and evolvable elements to managed services and non-transparent SaaS solutions. Tight coupling is a good solution for distributed applications under a company\u2019s control, but managed services and SaaS need looser coupling. Organizations can implement this solution by leveraging a combination of message queues, event buses, and data transfer services.\nAnd thus, the paradox is dissolved!\n", "tags": ["cloud", "devops", "innovation"], "categories": ["Blog", "Cloud"]}
{"post_id": 36736, "title": "Traceable tests", "url": "https://www.luminis.eu/blog-en/traceable-tests/", "updated_at": "2023-01-30T15:50:23", "body": "Have you ever spent hours or days trying to figure out why some API test is failing? Whenever something like that happens to me, my immediate thought is: what could have helped me find this problem faster?\nUnit tests are much better at this, when they fail, you just run them again locally, set a breakpoint if you have to, and find out what failed. But with tests that run against your API (yes, you need those too!) it can be much harder to figure out exactly what code got executed. There are all sorts of boundaries that make this hard, especially when those tests run as part of your pipeline (yes, you need to do that!). You can\u2019t just set a breakpoint in your pipeline and even if you have logging in your backend, how do you know which log entries resulted from which test scenario?\nI\u2019ve been playing with the idea of linking backend logs to tests for some time. Years back, I had a plan to add the name of the test to a custom HTTP header when calling the API from tests. In the backend I could add that value to the MDC context that we attach to log entries. The result would be that for each log entry, you could see which test triggered it.\nD\u00e9j\u00e0 vu\nSomehow I never got around to implementing that idea. Last year though, I helped out a team working on an API-only service that ran into the very same problem. Their system is comprised of a set of microservices and some of the API calls go through several of these services to handle the request. Next to unit tests, one of their other automated validations is a regression test suite that runs against the API to validate expected behavior.\nSo far, so good, and that test suite was really valuable. It allowed the team to quickly implement new functionality without spending a lot of time checking that existing functionality kept working.\nYou know what\u2019s coming now\u2026 whenever a test would fail, it was a nightmare to figure out why.\nLuckily, I had just finished hooking up al the services to an Elastic Cloud environment, with APM-powered tracing and all the good stuff. The traces gave excellent insight into what happened inside and between the micro services. However, it was still really hard to link a specific test failure to a set of traces.\nOpenTelemetry FTW \ud83d\ude03\nI still had that idea from before in my head, and when I saw the team struggling with these failing tests, a new idea popped in: what if I linked the traces to tests?\nThe typical method to pass trace ids across service calls is by using a trace header. OpenTelemetry provides the traceparent header for that, and the Elastic APM agent being used in the services picks that up out-of-the-box.\nMy first idea was to return the traceparent header in the API responses and then log them with each test. However, that\u2019s not really how the OpenTelemetry traceparent header is supposed to be used. You\u2019re supposed to include the header in the calling request. So that\u2019s what I tried next.\nShow me the code!\nFirst thing I did was locate how the test suite was making the API calls. Turned out this was done with HttpClient, which offers a nice way to inject additional headers\nimport static TraceInstrumentationInit.openTelemetry;\r\nimport static TraceInstrumentationInit.tracer;\r\n\r\npublic class HttpClientTraceInstrumentation implements HttpProcessor {\r\n  @Override\r\n  public void process(HttpRequest request, HttpContext context) {\r\n    injectTraceHeaders(request);\r\n  }\r\n\r\n  @Override\r\n  public void process(HttpResponse response, HttpContext context) {\r\n  }\r\n\r\n  private static void injectTraceHeaders(HttpRequest request) {\r\n    // TODO\r\n  }\r\n\r\n  public static HttpClient registerTraceInstrumentation(\r\n        DefaultHttpClient httpClient) {\r\n    HttpClientTraceInstrumentation traceInstrumentation = \r\n      new HttpClientTraceInstrumentation();\r\n    httpClient.addRequestInterceptor(traceInstrumentation);\r\n    httpClient.addResponseInterceptor(traceInstrumentation);\r\n    return httpClient;\r\n  }\r\n}\nOk, so that gives us a way to inject the traceparent header. First I figured I could generate the trace id myself. However, it turns out the traceparent value is not just a trivial UUID, it has a very specific structure and I wouldn\u2019t advise trying to generate it yourself.\nLuckily, OpenTelemetry has excellent SDKs that make it easy to generate and inject these headers. So we need a dependency on the OpenTelemetry API and SDK.\n<dependencyManagement>\r\n  <dependencies>\r\n    <dependency>\r\n      <groupId>io.opentelemetry</groupId>\r\n      <artifactId>opentelemetry-bom</artifactId>\r\n      <version>1.18.0</version>\r\n      <type>pom</type>\r\n      <scope>import</scope>\r\n    </dependency>\r\n  </dependencies>\r\n</dependencyManagement>\r\n<dependencies>\r\n  <dependency>\r\n    <groupId>io.opentelemetry</groupId>\r\n    <artifactId>opentelemetry-api</artifactId>\r\n  </dependency>\r\n  <dependency>\r\n    <groupId>io.opentelemetry</groupId>\r\n    <artifactId>opentelemetry-sdk</artifactId>\r\n  </dependency>\r\n</dependencies>\r\n\nNext, we had to initiate a trace for each scenario, and a sub-span for each API call being made.\nAfter looking at the codebase, we saw we could add that to an already present base class used by all tests to report scenario results to TestRail. Personally I\u2019m not a big fan of using base classes for tests as it can be hard to maintain a sensible hierarchy and the use of inheritance creates tight coupling between tests, which we don\u2019t want. In JUnit for example, I would use @ExtendWith to keep such responsibilities separated and easier to re-use and to avoid the use of static instances as seen here. But we didn\u2019t want to overhaul the complete test suite just to introduce this, better not to mix a big refactoring like that. The main goal at this point was to validate that traces could work.\nAs you see, we also passed the traceId to TestRail, more on that later.\npublic class Hooks extends BaseSteps {\r\n  private static final TestRailReporter testRailReporter =\r\n    TestRailReporterFactory.get();\r\n  private static final ScenarioTraceInstrumentation instrumentation =\r\n    new ScenarioTraceInstrumentation();\r\n\r\n  @Before\r\n  public void setup(Scenario scenario) {\r\n    instrumentation.startTrace(scenario);\r\n  }\r\n\r\n  @After\r\n  public void teardown(Scenario scenario) {\r\n    String traceId = instrumentation.endTrace();\r\n    testRailReporter.reportResultToTestRail(scenario, traceId);\r\n  }\r\n}\r\n\nFor every scenario we start a new span, including attributes to link it to the scenario. We also do some rudimentary logging for every scenario, making it easy to spot in the logs which scenario ran at which point and what the trace id was for that scenario run.\nimport static TraceInstrumentationInit.tracer;\r\n\r\npublic class ScenarioTraceInstrumentation {\r\n  private final ThreadLocalSpan spanState = new ThreadLocalSpan();\r\n\r\n  public void startTrace(Scenario scenario) {\r\n    Span span = spanState.start(\r\n      tracer.spanBuilder(scenario.getName())\r\n            .setSpanKind(SpanKind.CLIENT)\r\n    );\r\n    span.setAttribute(\"test.scenario_id\", scenario.getId());\r\n\r\n    System.out.println(\"|---------\\n\"\r\n      + \"| SCENARIO: \" + scenario.getId() + \" - \" + scenario.getName() + \"\\n\"\r\n      + \"| trace.id: \" + span.getSpanContext().getTraceId());\r\n  }\r\n\r\n  public String endTrace() {\r\n    String traceId = spanState.traceId();\r\n    spanState.end();\r\n    return traceId;\r\n  }\r\n}\nTo keep track of span state, I used a ThreadLocal, exposing a simple API to start and end the span that we could use from the ScenarioTraceInstrumentation and HttpClientTraceInstrumentation. As you see I did get lazy a bit here, using a Pair to store both the Span and Scope, using a record for that would make the code easier to read.\nclass ThreadLocalSpan {\r\n  private final ThreadLocal<Pair<Span, Scope>> state = new ThreadLocal<>();\r\n\r\n  Span start(SpanBuilder spanBuilder) {\r\n    Span span = spanBuilder.startSpan();\r\n    Scope scope = span.makeCurrent();\r\n    state.set(Pair.of(span, scope));\r\n    return span;\r\n  }\r\n\r\n  String traceId() {\r\n    return state.get().getLeft().getSpanContext().getTraceId();\r\n  }\r\n\r\n  void end() {\r\n    Pair<Span, Scope> pair = state.get();\r\n    pair.getRight().close();\r\n    pair.getLeft().end();\r\n  }\r\n}\r\n\nThe OpenTelemetry SDK also needs some initialization. Keeping it simple for this first experiment, and since some of the other code was static already, we just created two package scoped statics to be used from the ScenarioTraceInstrumentation and HttpClientTraceInstrumentation.\nclass TraceInstrumentationInit {\r\n  static final OpenTelemetry openTelemetry = initOpenTelemetry();\r\n  static final Tracer tracer = openTelemetry.getTracer(\"regressionTest\");\r\n\r\n  private static OpenTelemetry initOpenTelemetry() {\r\n    SdkTracerProvider sdkTracerProvider = SdkTracerProvider\r\n        .builder().build();\r\n    OpenTelemetrySdk sdk = OpenTelemetrySdk.builder()\r\n        .setTracerProvider(sdkTracerProvider)\r\n        .setPropagators(ContextPropagators.create(\r\n          W3CTraceContextPropagator.getInstance()\r\n        ))\r\n        .build();\r\n    Runtime.getRuntime().addShutdownHook(\r\n      new Thread(sdkTracerProvider::close)\r\n    );\r\n    return sdk;\r\n  }\r\n}\nAnd finally, we could put all the the pieces together in the HttpClientTraceInstrumentation, see below.\nWe create an additional span for the request [1] and then inject the traceheader into the request [2].\nThe OpenTelemetry SDK code is very generic, allowing you to inject headers into pretty much anything, from HTTP requests to Messages with all sorts of client libraries. The extremely generic code does make it a bit hard to read, but essentially you need to pass it a method that accepts two string arguments, the header name and header value. The HttpRequest from HttpClient has exactly such a method: setHeader [3].\nLast but not least, we also log every request being made [4], giving a nice overview in the test logs of what API calls are being made for each scenario.\nimport static TraceInstrumentationInit.openTelemetry;\r\nimport static TraceInstrumentationInit.tracer;\r\n\r\npublic class HttpClientTraceInstrumentation implements HttpProcessor {\r\n  private static final String W3C_TRACEPARENT_HEADER = \"Traceparent\";\r\n\r\n  private static final TextMapPropagator textMapPropagator =\r\n    openTelemetry.getPropagators().getTextMapPropagator();\r\n\r\n  private static final TextMapSetter setter = \r\n    HttpRequest::setHeader; // [3] http client setHeader method\r\n\r\n  private final ThreadLocalSpan spanState = new ThreadLocalSpan();\r\n\r\n  @Override\r\n  public void process(HttpRequest request, HttpContext context) {\r\n    Span span = spanState.start(\r\n      tracer.spanBuilder(\"/\")\r\n          .setSpanKind(SpanKind.CLIENT)\r\n    );\r\n    try (Scope ignored = span.makeCurrent()) {\r\n      // [1] add span details\r\n      span.setAttribute(HTTP_METHOD, request.getRequestLine().getMethod());\r\n      span.setAttribute(HTTP_URL, request.getRequestLine().getUri());\r\n      span.setAttribute(\"component\", \"http\");\r\n\r\n      injectTraceHeader(request);\r\n      logRequest(request);\r\n    }\r\n  }\r\n\r\n  @Override\r\n  public void process(HttpResponse response, HttpContext context) {\r\n    spanState.end();\r\n  }\r\n\r\n  private static void injectTraceHeader(HttpRequest request) {\r\n    // [2] let opentelemetry sdk propagate any required headers\r\n    textMapPropagator.inject(Context.current(), request, setter);\r\n  }\r\n\r\n  private static void logRequest(HttpRequest request) {\r\n    System.out.println( // [4]\r\n      \"|- \" + request.getRequestLine() + \"\\n\" +\r\n      \"|  ^- \" + request.getLastHeader(W3C_TRACEPARENT_HEADER));\r\n  }\r\n}\r\n\nEverything coming together\nNow when tests are run, their logs clearly show each of the scenarios, their trace id and the API calls being invoked. And for every API call, the complete traceparent header is shown. With these details being propagating it to our backend, we can now lookup all the calls for a scenario in Elastic, or even search there for the span id to locate the trace for a specific API call.\n|---------\r\n| SCENARIO: T2748705 - Validate if method PUT was added to controller and it's possible to use it\r\n| trace.id: d28a64332015de5d324cb3e0f0380eba\r\n|- PUT http://.../api/...\r\n| ^- traceparent: ...\r\n...\r\n...\r\n|---------\r\n| SCENARIO: ...\r\n| trace.id: ...\r\n|- GET http://.../api/...\r\n| ^- traceparent: ...\r\n...\r\n...\r\n\nIntegrate existing tools\nAt this client, the team used TestRail to centrally view reports of all test runs. To find the cause for a failing test, wouldn\u2019t it be nice if we could skip spitting through logs. As you saw before, we had the trace id available when we posted results to TestRail, so we added a link there to take you directly to the corresponding trace in Elastic.\n\nResults!\nWhen you follow that link to Elastic, it shows you all the API calls (transactions) that were part of that test scenario (trace).\nFrom there you can navigate to each API call and look at individual traces. Elastic shows a lot of useful info, like a latency distribution chart that makes it super easy to quickly select all slow calls or all failing calls.\n\nThere is also a dedicated errors section, which allows you to quickly locate details for any errors that occurred as part of the scenario traces.\n\nTip: If you want to take a closer look at what\u2019s in these kind of traces, there is a nice free Elastic demo environment where you can play around with traces in the APM section.\nConclusion\nWith just a little bit of code, we managed to reduce the amount of time needed to pinpoint test failures dramatically. Which shows again that optimizing for quick work loops and short feedback cycles are crucial for improving software deliver performance.\n", "tags": [], "categories": ["Blog", "Cloud", "Development"]}
{"post_id": 34178, "title": "Top 8 tips for visiting re:Invent 2023", "url": "https://www.luminis.eu/blog-en/top-8-tips-for-visiting-reinvent-2023/", "updated_at": "2022-12-13T14:50:57", "body": "AWS re:Invent 2022 will be an event I will never forget. From the sheer size of the event to the high-quality sessions, conversations, and swag, re:Invent has a lot to offer if you are at all working with AWS.\n\nThe event took place in Las Vegas, spanning multiple venues on the strip. AWS did their best to ensure that the event\u2019s magnitude was manageable for the visitoran app that shows everything that is happening that week and \u00a0by introducing helps you navigate between the venues.\u00a0\nEven with this app handy and all of the re:Invent employees ready to help you at any moment, there are still some things that are good to know before going to the event. In this blog, I will share 8 tips for re:Invent 2023 that I learned from my visit this year.\n#1: Reserve your sessions\nThe sessions scheduled for re:Invent will be made available much earlier than the event, and you will have the option to reserve your seat at these sessions. I highly recommend you take the time to do this and reserve the sessions that you want to go to.\n\nWhen you finally visit re:Invent, all sessions will have two lines. One for the reserved visitors and the other for the unreserved (walk-up) visitors. Having a reserved seat will not only guarantee you a spot at the session, but it also allows you to do other things and show up 15 minutes before the session and still enter. The people in the walk-up line usually start to gather about 45 minutes before the session and have to stand, and sometimes sit, in line until 10 minutes before the session. Don\u2019t let this happen to you. Reserve your seat!\n#2: Get Certified\nThere are sessions, swag, events, and even a special lounge for visitors that are AWS certified.\n\nI had the most interesting interactions and the most fun at these events. I had conversations with AWS builders around the world. We had discussions about the certification process, how they use AWS at the companies they work, what kind of costs they have, which AWS services they use, and a lot more. We also played games and had some drinks, and eventually added each other on LinkedIn to keep in touch. Don\u2019t miss out on this. Get certified! Any official AWS certification will do.\n\n#3: Join the AWS Community builders\nThere are also special perks for visitors that are part of the AWS Community Builders group. This gives you access to events and special SWAG. There is an open application twice a year to join. To get into the AWS community builders, you need to:\n\nWrite blogs about AWS\nSpeak at conferences about AWS\nGive workshops on AWS technology\nAnswer questions online about AWS technology\nContribute to AWS open source projects\n\u2026anything else that spreads the word and helps others learn about AWS\n\nBy joining, you get several benefits next to all of the special access and SWAG at re:Invent:\n\n500$ AWS credit\n1 year subscription at CloudAcademy\n1 AWS certified exam voucher\nAccess to Slack which gives you access to AWS community builders and Hero\u2019s\nre:Invent discount of 50%\nMore SWAG\nLots of exclusive access to resources (webinars, beta previews)\n\n\n#4: Leave room for SWAG\nAWS re:Invent has SWAG, a lot of it! Once you visit the Expo, where all the stands are of the event sponsors, you will be overwhelmed with the number of companies that have shown up and are eager to talk to you about their products. Most, if not all stands have some sort of SWAG to give to you. These come in all shapes and sizes. Leave some room in your suitcase!\n\n#5: Vary the type of session you visit\nThere are several types of sessions to choose from at re:Invent, ranging from typical breakout sessions to workshops. One fact to note is that a breakout session is recorded and can be viewed later on, while sessions such as Chalk or builders sessions do not. If you are struggling to decide which sessions to go to and some of them overlap, consider the session type and choose the non-breakout sessions. You can always watch the breakout sessions later.\nFor example, at one of the \u201cbuilder sessions\u201d I was sitting at a table with a Senior AWS architect and other re:Invent visitors to discuss how we can monitor costs on AWS. It was very insightful to see which tools were being used, either AWS native or third party, and also to see the massive number of costs some people are dealing with. One of the tools they discussed in this session is the CUDOS dashboard, which you can deploy via a CloudFormation template that can monitor various aspects of your costs and give you recommendations.\u00a0\nAnother type of session was the \u201cChalk\u201d session, which was very interactive. In one of the sessions, we designed a solution that included using Lambda extensions to debug them from your local development environment, decreasing the feedback loop during Lambda development.\nI also visited some breakout sessions, which are available now on Youtube. Some of my favorites include:\n\nUnleash developer productivity with infrastructure from code\n\nAn introduction to Infrastructure FROM code from the CEO of Ampt AWS services.\n\n\nA close look at AWS Fargate and AWS App Runner\n\nA walkthrough of the evolution from EC2 instances to AWS App Runner, given by a principal engineer at AWS that has been there from the start.\n\n\nA day in the life of a billion requests\n\nA look into how AWS optimized their authentication mechanism to handle half a billion requests per second to IAM. (spoiler: they use a lot of HMAC\u2019s)\n\n\nAre you integrating or building distributed applications?\n\nAn overview of choices you have to make when building distributed applications and makes clear that the words you use are important. Recommended for anyone in an Architect role.\n\n\nThe architect elevator: Connecting the boardroom and IT\n\nThis talk describes the impact you can have as an architect within a company and gives you tips on how to do this. Also highly recommended for anyone in an Architect role.\n\n\n\n#6: Take the venue\u2019s locations into account\nAs mentioned previously, re:Invent takes place at multiple venues on the Las Vegas strip. Take a moment to check where all the venues are and how long it takes to walk to each venue. AWS offers shuttles between venues, and in my experience, these work well. However, even with the shuttles, if you have to get from the Wynn to the Mandalay Bay, then you are probably going to miss the mark.\n\n#7: Book a hotel close to the Expo\nThe expo was held at the Venetian. This is the place you want to be when you are not in any other sessions or events. There is a lot to do and see there, such as speaking to people at the several stands about their AWS-related products, collect SWAG, play several types of games, win prizes, attend lightning talks, view product demo\u2019s and get some snacks.\nBooking a hotel close to the Expo gives you more opportunities to experience everything it offers.\n\n#8: Register and attend receptions\nThere is breakfast and lunch included on most days of re:Invent. However, try registering for as many receptions as possible for dinner. These will be announced in the AWS re:Invent app and will be hosted by several of the sponsors. This gives you a good chance to network with different kinds of people. Some of these will offer buffet dinners which is a nice plus.\u00a0\nFor some of these events you have to register on time or you won\u2019t get in, and in some cases, the registration starts well before re:Invent, so be sure to keep an eye out online and in your email inbox. If you get an invitation, register immediately as they are quickly full.\nConclusion\nI had a great time at re:Invent, and I hope these tips will help improve your re:Invent experience if you choose to attend in person. Of course, you can attend re:Invent virtually as well, for free. Some of these tips, such as getting certified or joining a community, will require you to take action now. It might seem like a lot of work. However, you will be happy once you reap the benefits at re:Invent next year.\n", "tags": ["aws", "AWS re:Invent 2022", "AWS re:Invent 2023"], "categories": ["Blog", "Cloud"]}
{"post_id": 34120, "title": "Cybersecurity Awareness | Phishing", "url": "https://www.luminis.eu/blog-en/cybersecurity-awareness-phishing/", "updated_at": "2022-11-30T11:59:53", "body": "In October 2004 U.S. Department of Homeland Security and the National Cybersecurity Alliance launched Cybersecurity Awareness month in effort to create awareness and helping individuals to protect themselves. Since 2012 this idea has been adopted by ENISA (European Union Agency for Cybersecurity) with an annual campaign dedicated to promoting cybersecurity among EU citizens and organizations through awareness raising activities and sharing of good practices.\nIn 2022 the theme is \u201cThink Before U Click!\u201d #ThinkB4UClick, which addresses two of the most common threads, namely Phishing and Ransomware.\nIn this blog we will dive into the thread called Phishing. What is it, how to recognize a phishing attack, prevention and what to do when receiving a phishing email and why it is important to create awareness within your organization.\nWhat is phishing?\nPhishing is a social engineering method, where an attacker\u2019s goal is to trick a person to reveille personal and/or sensitive information like credentials, PIN, phone numbers, financial information or any other information that could be used to perform the next step in the attack chain, like gaining access to systems.\nOne of the commonly used methods is email phishing. With this method the attacker sends an email to a person or organization and mostly requires the receiver to login into a website that looks familiar, but in fact is fraudulent.\nHow to recognize email phishing?\nEmail phishing attacks are evolving and become harder to recognize. Still there are some \u2018red flags\u2019 that could help you recognize a phishing email or at least raise suspicion when receiving one.\nA brief list of \u2018red flags\u2019 when comes to recognizing a phishing email;\n\nRequest to login\nThe email contains a link to a website where you need to login\nPoor grammar\nAlthough not always the case, a phishing email containing poor grammar and spelling errors should at least raise suspicion\nUrgency\nThe message contains some urgency. \u201cYou have limited time to click on the link\u201d\nMessage is addressed to a generic recipient\nPhishing emails are often addressed to a generic recipient. \u201cDear sir/mom\u201d\nFrom name and address do not match\nThe \u2018from\u2019 name does not match the address that is used in the email\nSubject unclear\nThe subject of the message is unclear. What is the purpose of the message you received?\nContact information missing\nThe contact information you could use to verify the purpose of the message is missing\n\nAn image of a suspicious email containing some of the red flags mentioned above\n\nPreventing phishing within your organization\nImplementing proper technology that automates the process of detection and prevents the delivery of suspicious emails to your users is the first (and necessary) step. Equally important is to keep your organization informed and \u2018up-to-date\u2019 through education and awareness on this subject. When an automated process fails to recognize a malicious email, the receiver is the next in the \u2018line of defense\u2019 and they should be able to recognize it based on some red flags. And this can only be achieved by creating awareness and training.\nWhat to do when receiving a suspicious message\nWhen you are suspicions about a message you received or identified one as phishing and it has any attachments, it is Important not to open it. An attachment may contain harmful software (known as malware) that could supply access to the attacker or even harm your system.\nA general rule to confirm if the sender intended to send you the message, is simply to contact the sender (if possible) and verify it. If contact information is missing, that should be a red flag and you should not take any actions requested in the message.\nIf you are not reassured after the previous step, you should always report the message as phishing with your mail client. This information is used by automatic detection processes to avoid further spreading of the message through the organization.\nIn case you (accidently) opened the attachment or followed up the instruction in the mail, then you should report this to your security department. Proper actions should be taken by the security department to prevent this spreading further within an organization.\nIn most cases a phishing attack is executed on several users within an organization. Therefore, it is important to inform others within your organization about a (possible) phishing attack, and it is recommended to use the internal communication channel.\nTo summarize the actions, you should (or should not) take as a user when a suspicious message reaches you:\n\nDo not open any attachments\nAn attachment may contain harmful software (known as malware) that could provide access to the attacker or even harm your system.\nContact the sender\nIf possible, contact the sender in a separate message to verify if they did send a message and request information about the purpose of the message\nReport it as phishing\nMost mail clients offer functionalities to report phishing. This helps (automated) systems to recognize phishing attacks and prevents them from spreading further within an organization.\nUse internal communication channels to inform others about a (possible) attack\n\nWhy it is important to create awareness\nPhishing and other attacks have increased, especially during the Covid period. Educating your employees and increasing awareness within your entire organization has become important, since the employees are \u2018the first line of defense\u2019 if an automatic process fails. Recognizing malicious attacks and taking proper actions to prevent an attacker from gaining access to sensitive data and increases the security maturity within your organization.\nIn conclusion\nEvery organization should be aware of a (possible) attack and ready to act if one occurs. Setting up processes, testing these and using the proper security tooling to mitigate and/or prevent attacks and create cybersecurity awareness through training and attack simulations. And of course, #ThinkB4UClick\n", "tags": [], "categories": ["Blog", "Security"]}
{"post_id": 33652, "title": "Using Azure Active Directory for your web apps: Thoughts from a software developer", "url": "https://www.luminis.eu/blog-en/using-azure-active-directory-for-your-web-apps-thoughts-from-a-software-developer/", "updated_at": "2022-09-27T13:58:49", "body": "With the emergence of identity as a service (iaaS) from Cloud service providers such as Azure and AWS, managing authorization and authentication to your web application in a secure manner has become much easier and less work for developers.\nFor most scenarios we no longer need to implement ASP.NET Core Identity user management ourselves, nor setup our own identity server (for single sign-on). Instead we can make use of the identity service provider called Azure Active Directory (AAD) which is maintained and developed by Microsoft. This is a good thing, as correctly implementing such software patterns is non-trivial and may incite insecure practices (such as the password-credentials flow) unless the developer knows exactly what he or she is doing.\nHowever, I have found that properly understanding and managing iaaS via Azure can be a confusing topic in itself and it took me some time to properly understand how to use and configure it. I would like to explain how you can use it for you own applications and clarify some concepts that were originally unclear to me in hopes of providing some illumination for others. For the scope of this article, I\u2019ll focus on one of the most basic scenarios for using Azure active directory as a software developer: How to retrieve access tokens for your SPA (Single Page Application) which communicates with your web API, and subsequently protect your web API against unauthorized access.\nFor the scenario in which you have a SPA that targets a web API, I have found that this documentation best covers the steps you need to take.\nThe specific example here is registering the API and the SPA with Azure AD B2C (which is AAD for customer defined identities), but the process should be the same for normal Azure AD (which is AAD for Microsoft identities).\nFor the scope of this article, I would like to expand the following topics in that piece of documentation.\nSpecifically, I\u2019ll cover three topics that I feel require additional emphasis beyond the official Microsoft documentation\n\nApp registration and MSAL.\nScopes and admin consent.\nId tokens vs Auth tokens.\n\nI\u2019ll additionally provide the official links that cover how to implement these things technically for convenience of the reader.\nApp registration and MSAL\nPicture the basic scenario that users will be logging into our SPA using a Microsoft AD account of employees from a single tenant. Our SPA needs to retrieve application specific information from a ASP.Net Core web API. We want to protect this API so that only a specific subset of the users within our tenant are allowed to retrieve data from the web API.\nSchematically we arrive at the following idea:\n\nReceiving and verifying the token is a bit more complex than this schema suggests but this suffices to convey the basic idea for now.\nLet\u2019s first look at how the SPA registers with AAD.\nOn the Azure portal side, you will have to create an app registration. The app registration is an entry inside Azure that contains all identity related information of an application that you register with AAD. Perhaps somewhat counter-intuitively, this registration typically has no relationship with the deployed instance of the website on Azure itself. Instead, the front-end code of the SPA uses a Javascript library called MSAL (\u201cMicrosoft Authentication Library\u201d) and a configuration file containing a TenantId, ClientId and redirect URL to find the app registration on Azure. Where the TenantId is the identifier of the organisation/tenant under which the app registration is created, the ClientID is the identifier of app registration instance on Azure itself and the redirect URL is the web address at which the SPA requests the access token to be returned. Upon successful retrieval of the token by the SPA via the logic from the MSAL library, the same token will be added as HTTP header to each request from your SPA to your web API. Note that the successful retrieval of the token is also generally the clue that the website user has successfully logged in for your front-end application logic.\nMSAL isn\u2019t just available as a JavaScript library, but also for .NET applications, Java applications, mobile applications and a few others. The beauty of it is that the library manages interaction with AAD for fetching tokens, fetching refresh tokens and expiring the token. Additionally, you don\u2019t have to search for a discovery document, specify the token endpoint or manage storage of the token in the web browser (etc.) yourself as the MSAL library will do these things for you. You only have to follow the implementation instructions for your specific platform and configure a settings file generally.\nOn the app registration side, we need to specify which root domains we white-list to request an access token. We also specify what type of consent the user requesting the token must give, what scopes the token is for and what type of tokens are handed out. The above-mentioned Microsoft documentation explains clearly how to do these things, but in order to understand what you are actually doing I feel some extra explanation is very welcome. In the rest of this blog, I\u2019ll elaborate on these things.\nNow that I have given an overview of how the SPA will interact with AAD, that leaves the topic of the web API. With each request to the web API from our SPA, the authorization token, which is usually a Json Web Token (JWT), is passed along in the authorization headers. Therefore, the process for integrating the web API with AAD is a bit simpler than for the SPA. The only thing we really need to do is register our web API with AAD via a separate app registration and subsequently verify the JWT with the AAD service upon each request entering the API. You\u2019ll usually want the Web API and the SPA to each have their own app registration, so you can finetune their scopes separately. The API can be granted admin consent for example, where the SPA is not (I\u2019ll discuss consent and scopes in the next section). In a typical .NET Core webapp, the entire implementation could be something as simple as this (when using the Microsoft.AspNetCore.Authentication Nuget package):\n\n\nFuther reading on code configuration\n\nScopes and admin consent\nThe concept of scope for a JWT may be a little confusing at first. Scopes are not part of the JWT itself but are sent alongside a request towards the authorization service to specify what type of information the application would like access to. If the requested Scope and previously granted access (either previously by the user, or from admin consent), \u00a0are not the same for the user identity matching to the requested JWT, the user will have to give consent for the app to fetch the additional information. Alternatively, if that user is not allowed to give this consent for a specific resource, he/she may be prompted to ask an admin within the organisation for consent. When the JWT is subsequently verified by the Web API, the scope that was agreed upon by that user, is again be checked via AAD (the previously granted scopes are stored on the AAD side for that User identity), to see if it encompasses the specific API information that the front-end tries to access.\n\nFurther reading on scopes\nFurther reading on consent prompts\n\nOftentimes you\u2019ll find your SPA will need a complex set of information from your web API and you don\u2019t want to bother the employees using your app with a consent pop-up. In these cases, AAD offers the option to provide tenant-wide admin consent. Basically, you login as an administrator to give an application or group of users in your tenant permanent consent for a specific scope without user interaction. This should be handled with some care and you should make sure the application is only granted access for the scopes that it actually needs. Granting admin consent can be done in the following Azure pane:\n\nIt is a powerful tool but the developer should take care to not abuse it in scenarios where user-consent would suffice.\nID tokens versus Auth tokens\nPerhaps one of the most obscure options the AAD portal interface provides to the developer that tries to setup AAD single sign on for one of his or her app for the first time is the concept of ID tokens and access tokens:\n\nThe wording here is extremely confusing so let me explain when and how to use this. Apparently AAD distinguishes between Access tokens and ID tokens. These are the two types of JWT that the authorization endpoint can hand out. Note that the Azure portal speaks about the AUTHORIZATION endpoint and specifically not the TOKEN endpoint. To understand the difference between the two, we need to cover the two OAuth based authorization flows that are commonly used. Namely the authorization code flow and the implicit flow.\nImplicit grant flow\nA popular authentication method that has been used for single page web applications (SPA\u2019s) in recent years, is the implicit flow (a.k.a. the \u00a0OpenId Connect (OIDC) implicit grant flow). However, the most commonly used way to implement this type of authentication and authorization nowadays, is via the so called authorization code flow. (Implicit flow has fallen out of favor since browsers started supporting cross origin resource sharing).\nBecause it\u2019s chronologically the more dated one, let\u2019s start by discussing the implicit flow:\n\nImplicit grant flow\n\nThe authentication library signals the browser to navigate to the access (bearer) token endpoint of the identity provider*.\nBecause the user is not authenticated, the browser is redirected to login screen for the authentication service.\nThe user enters his credentials and these are verified by the authentication service.\nThe browser is redirected to the access token endpoint.\nThe access token is returned as a URL/query parameter to a redirect URL that was pre-configured at the Authorization Server, ensuring that only the designated domain can request the token. Because the token is passed in the URL, it is extra important to always communicate over Https when using the implicit flow.\nThe access token is held in the browser storage by the authentication library. It is retrieved and added to the request headers by your SPA or mobile app.\nRequests made to your web API can now be accompanied by the access token.\nThe web API can now verify whether these requests are authentic with the identity provider, using this token.\n\n* Azure Active Directory in this specific case.\nOn a side-note: The Azure Active Directory Authentication Library (MsAdal) was the popular predecessor to MSAL for JavaScript applications until about a year ago. It was therefore an attractive option for implementing an authentication system for your front-end apps. However, this predecessor exclusively supported the implicit grant flow.\nAuthorization code flow\nThe authorization code flow is the currently recommended way to manage tokens from AAD and I\u2019ll try to describe it the best way I can with the following figure and step-by-step explanation.\n\nAuthorization code flow\n\nThe authentication library signals the browser to navigate to the authorization code endpoint of the identity provider*.\nBecause the user is not authenticated, the browser is redirected to login screen for the authentication service.\nThe user enters his credentials and these are verified by the authentication service.\nThe browser is redirected back to the Auth code endpoint. This time the authorization code can be returned.\nThe authorization code is returned as a URL/query parameter to a redirect URL that was pre-configured at the authorization server ensuring that only the designated domain can request the authorization code.\nThe authentication library does a POST request to the /token endpoint of the identity provider, providing the (PKCE protected) authorization code.\nThe access (bearer) token is retrieved to the return URL.\nThe access token is held in the browser storage by the authentication library. It is retrieved and added to the http request headers by your SPA or mobile app. (Refresh token can be fetched from the identity provider once needed and replace the access token).\nRequest made to your web API can now be accompanied by the bearer token.\nThe web API can now verify whether these requests are authentic with the identity provider, using this token.\n\n* Azure Active Directory in this specific case.\nAs we can see, the authorization code flow expands on the implicit grant flow by first retrieving an authorization code, before getting the access token with the help of that authorization code. The significant advantages of using the authorization code flow over the implicit grant flow are that (A) the access token is not as exposed and (B) the availability of refresh tokens. These advantages are possible because (A) the authorization code is short-lived and single-use, and that (B) the access token cannot be easily intercepted (due to PKCE and because it is never passed in the URL).\nBack to the Azure portal\nIn short, we can now understand that we would like to use the auth code flow in most scenarios. How to configure the auth code flow in AAD depends on whether additional user claims are needed. If we do not need additional user claims in our front-end, we only need an authorization code from the /authorize endpoint of AAD and therefore both the Access token and ID token checkboxes can be left unchecked. We get the access token from the /token endpoint of AAD and not from the /authorize endpoint after all. If we need more info for our front-end than just the token to send with the request headers to our web API (for example a set of claims that control whether our front-end does or does not show certain features and buttons) we should still use the auth code flow, but enable the checkbox for ID Tokens. This causes an ID Token to be sent alongside the authorization code to the redirect URL. This configuration is what Microsoft calls the \u2018hybrid\u2019 flow. Unlike the auth code and the access (bearer) token, the ID token contains additional information like user claims that the front-end may need for displaying purposes.\nIn modern web-apps you should basically never have to retrieve access tokens directly from the authorization endpoint, because all the MSAL variations now support the authorization code flow. Therefore, the access token checkbox should always be left empty unless you need to work specifically with the implicit grant flow for backward compatibility reasons.\nConclusion\nIn summary, I\u2019d recommend the following things in most scenarios when implementing AAD authentication for your web applications:\n\nUse MSAL.\nUse scopes.\nAlways use the authorization code flow.\nUse tenant-wide admin consent sparingly and responsibly.\n\nI hope this paper has provided the reader some insight on how and why and has contributed to a better understanding of how to implement OAuth based authentication with Azure Active Directory.\n", "tags": ["Azure", "Azure directory", "Webapi"], "categories": ["Blog", "Cloud"]}
{"post_id": 33528, "title": "Developing a (Cloud) Security strategy\u00a0", "url": "https://www.luminis.eu/blog-en/developing-a-cloud-security-strategy/", "updated_at": "2022-09-07T11:44:18", "body": "In an increasingly digital world, security has become an important subject for every organization, regardless of its size. With more organizations adopting cloud as their main provider, the need to implement a thorough cloud security strategy has become essential to every business. An ad-hoc security control and response approach is not enough to secure your organization.\nBut what are the best practices you need to consider, when implementing (cloud) security in you organization? And what are the subjects, you need to be aware of when developing a security strategy?\nIn a series of blogs, we will show you how we at Luminis address these challenges with our clients and partners.\nWhy do I need a security strategy?\nA well-developed security strategy adds maturity to your organization, helps you detect and prevent threats that accrue now and in the future. It enables organizations to become agile in terms of security and address threats and issues faster. It also helps you prevent overspending on security. When dealing with security, it is important to remember: \u201cSpent the right amount, not more\u201d!\nThe landscape around (cloud) security changes rapidly. Once your security strategy is developed, you need to regularly review it and follow up on your findings. This is vital for your organization in order to gain maturity in security strategy and keep your workload and data secure.\nSome subjects you should consider when developing a cloud strategy:\n\nSecurity is the responsibility of the entire organization, not just the security department/team\nIt should be proactive and not just a periodic check when a project is pushed to production\nIt should be repeatable\nIt should be cost effective\nIt should minimize risk\nIt should be documented\n\nShared responsibility\nWhen developing a cloud strategy, you first need to be aware of the responsibility you have as a cloud service cust0mer. Alle major cloud service providers apply a \u201cshared responsibility\u201d model. As can be seen in the chart below (AWS shared resposibility model), this differentiation of responsibility is commonly referred to as Security \u201cof\u201d the Cloud versus Security \u201cin\u201d the Cloud.\n\nAWS shared responsibility model\nIn this model, the cloud provider is responsible for protecting the infrastructure that hosts all of the offered services. Whereas the customer is responsible for securely configuring the cloud services they select.\nWith this in mind, the next logical step is to map the existing infrastructure and architecture.\nMapping\nYou cannot secure what you cannot see or do not know! Mapping your existing infrastructure and architecture is a vital part of a healthy and solid (cloud) security strategy. Visualizing your workloads, (key) assets, business objectives, critical information and systems should give you a clear overview of the possible attack surface you need to address, what components are in use, the priorities within your roadmap, how the components are related to each other, what tools you should consider using, etc.\nOnce you have gathered all this information, it will help you create a balanced roadmap.\nAssessment\nOnce these steps are taken, our suggestion would be to start with an assessment of workloads. This helps you understand the current state of your infrastructure and architecture. This assessment will also contribute to the general security awareness within the organization, which is arguably the most important step in keeping your organization secure.\nPolicies and compliance\nIdentifying the necessary mission and business needs, laws, executive orders, guidelines, regulations, policies, standards, guidelines and regulatory compliance for your business helps you understand the current and upcoming challenges. It can serve as a guideline for making the right choices when it comes to choosing a cloud provider, tooling, (security) frameworks and methods.\nPeople, tooling and methodologies\nWhen developing and implementing a security strategy, next to tooling and methodologies, people are equally important! By involving people from your organization, your create ownership and awareness about security. Everybody should be aware of the risks and should be \u2018part\u2019 of the general security mindset.\nSome of the subjects to consider:\n\nCIO/CISO\nWell-Architected framework\nDevSecOps\nIaC\nSCA, SAST, DAST\nEncryption (on transit & at rest)\nZero trust\nOWASP\nEmployee education \u2013 recurring!\nBackup & DR\n\nRoadmap\nOnce you have gathered alle the necessary information and documentation, it is good start drawing up a roadmap. This doesn\u2019t have to be in detail. The roadmap is a living document and you should treat it as such.\nThink big, start small!\nStart with a highover view of the findings from the previous steps, for example on the policy level and then work out the details. Further down the road you start to implement controls, methodologies and tooling.\nIn the next blog of this security series we will discuss DevSecOps. What are the considerations you need to take into account and what you need to know before applying DevOps methodology for your security?\n", "tags": ["cloud", "cloud security", "security"], "categories": ["Blog", "Security"]}
{"post_id": 33506, "title": "Luminis achieves growth ambitions with Private Equity firm M80", "url": "https://www.luminis.eu/blog-en/luminis-achieves-growth-ambitions-with-private-equity-firm-m80/", "updated_at": "2022-09-02T10:38:31", "body": "Brussels \u2013 September 2nd, 2022 \u2013 Luminis announces today that the private equity fund M80 has acquired a majority stake in the company. The Belgian M80 offers software and technology company Luminis the opportunity to further expand its strategy and market position. Luminis becomes part of a platform that also includes XPLUS, a Belgian expert in enterprise IT architecture. M80 plans to make selective acquisitions in Europe in the coming months with the aim of building a portfolio of best-in-class digital transformation specialists.\nHans Bossenbroek, CEO Luminis about accelerating the strategy: \u201cDue to the tight labor market, organic growth of Luminis proved to be a challenge. With the help of the private equity fund M80 it is now possible to realize our growth ambitions.\u201d\nAfter acquiring XPLUS in April 2022, M80 has now taken the next step in its development of a full-service digital transformation provider in Europe. By becoming part of this group of companies, Luminis can accelerate the broadening of its services and products portfolio in the field of digital transformation.\nHans Bossenbroek will lead this new group of companies and will take responsibility for the further development of the digital transformation services. \u201cThe rapid developments in the market and technology require a multidisciplinary approach based on customer value and innovation. We strive to be the full-service provider for our customers in the field of Cloud and data.\u201d With this step, Hans hands over the baton to Jeroen Bouvrie, who will take on the task of Managing Director Luminis.\nJeroen Bouvrie, Managing Director Luminis: \u201cI am convinced that together with XPLUS and other companies that will join our platform in the future, we can support customers even better in their digital transformation. Bringing parties together fits exactly in the line of thought and strategy of Luminis; we do not look for synergy afterwards, but aim to add value for our customers as quickly as possible.\u201d\nCarl Annicq, M80 Partners: \u201cM80 aims to bring together companies with superior capabilities to save customers the frustration of working with mediocre IT vendors \u2013 we are thrilled to have convinced a quality player like Luminis to join this initiative alongside XPLUS. We look forward to further steps to strengthen our group internationally.\u201d\n\nAbout Luminis\n\nLuminis was founded in 2002 and offers customers high-quality solutions in the field of Cloud and Data. Luminis has partnerships with Amazon Web Services (AWS) and Microsoft, among others.\nIn addition, Luminis is the initiator and driving force of the IT training program Accelerate with, among others, Thales, Bosch and the Dutch Tax Authorities.\nLuminis has 150 employees and has offices in Amersfoort, Amsterdam, Rotterdam, Arnhem and Apeldoorn and provides its services to, for example, Thales, Alliander, Huuskes, BDR Thermea, bol.com and The Learning Network.\nMore information on https://www.luminis.eu\n\nAbout M80\n\nM80 Partners is the management company of M80 Capital, a private equity fund established in November 2018, investing in companies in Belgium, France, the Netherlands and Luxembourg.\nFounded by Peter Maenhout, the investment team consists of seasoned private equity professionals, but also entrepreneurs, former CEOs and digital pioneers.\nThe company focuses on growth companies in healthcare, consumer, business services and industry. The M80 team invests in companies it can help digitally transform to accelerate sales and improve operations.\nMore information on\u202fhttps://m80partners.com/\n\nAbout XPLUS\u202f\n\nFounded in 2010 by Wim Vochten, XPLUS provides consulting services focused on digital transformation through the implementation of business-critical enterprise IT architecture and digital solutions in the BeNeFraLux.\nAmong its 100+ highly experienced consultants, 75% worked earlier as a principal consultant or partner with a top 5 consultancy firm.\nXPLUS serves blue chip clients in a variety of sectors, mainly in banking, insurance, telco and retail.\nMore information on\u202fhttps://www.xplus.eu\n\nContact:\nHans Bossenbroek \u2013 CEO: +31 622801230, hans.bossenbroek@luminis.eu\nCarl Annicq \u2013 M80 Partners: +32 495 58 10 27 \u2013 carl.annicq@m80partners.com\n", "tags": [], "categories": ["Blog", "News"]}
{"post_id": 33242, "title": "Invoking an AWS Lambda function during a CDK deployment", "url": "https://www.luminis.eu/blog-en/invoking-an-aws-lambda-function-during-a-cdk-deployment/", "updated_at": "2022-07-25T16:35:43", "body": "In general, AWS Lambda functions are triggered by some sort event. Most common use cases are an event from EventBridge, SQS, or an event created by a call to API Gateway in case you have a REST/HTTP API based on an AWS Lambda function. However, the other day I was looking for an option to execute my Lambda function immediately after it was created and/or updated while deploying my Infrastructure as Code with AWS CDK. I wanted it to work without manually executing a CLI command or calling an HTTP endpoint. It needed to be based on the CDK / CloudFormation deployment. A couple of use cases we had was triggering an import process or running a liquibase/ flyway script to populate a database.\nLooking for options\nWhile researching options, I initially looked for a method on the Function CDK Construct. I wondered if had specific lifecycle methods, but that did not seem the case. Secondly I started looking at an EventBridge rule that could listen to AWS CloudFormation events, but it seems there are almost no events are coming out of CloudFormation into EventBridge.\nAWS CDK is based on CloudFormation, so I searched within the documentation for both technologies to see what kind of hooks or lifecycle events were available. First thing I found was CloudFormation Hooks, however that only seemed related to proactive validation and automatic enforcement at the pre-deployment phase. While searching I did find a suggestion to look into using a Custom Resource and that seemed like a good solution.\nUsing Custom Resources in CDK\nWhat are Custom Resource in CDK?\nAWS CloudFormation custom resources are extension points to the provisioning engine. When CloudFormation needs to create, update or delete a custom resource, it sends a lifecycle event notification to a custom resource provider.\nWith custom resources you can hook into the provisioning engine and create a handler for the create, update and delete events. This will allow you to:\n\nCreate AWS resources that are not (yet) supported by CDK/CloudFormation\nCreate Non AWS resources (remote managed databases like ElasticCloud or MongoDB Atlas)\nPerform all kinds of other operations as you can write your own custom logic ( database seeding, database migrations, API calls, SDK calls)\n\nAWS CDK supports Custom Resources and gives you two options to implement them:\n\nLeverage the Custom Resource Provider Framework \u2013 Create your own lambda functions to handle the cloud formation events\nLeverage the Custom Resources for AWS APIs \u2013 Use the AWSCustomResource construct and provide a single AWS SDK API call\n\nUsing a custom resource provider\nSo what is a custom resource provider in CDK / CloudFormation?\nWhen CloudFormation needs to create, update or delete a custom resource, it sends a lifecycle event notification to a custom resource provider. The provider handles the event (e.g. creates a resource) and sends back a response to CloudFormation. Providers are implemented through AWS Lambda functions that are triggered by the provider framework in response to lifecycle events.\nThe CDK documentation on Custom Resources has some extensive documentation on implementing such a Lambda function as a custom provider. At the minimum, you will need to define the onEvent handler, which is invoked by the provider framework for all resource lifecycle events (create, update and delete) and you need to return a result which is then submitted to CloudFormation.\nThe framework offers a high-level API which makes it easier to implement robust and powerful custom resources and includes the following capabilities:\n\nHandles responses to AWS CloudFormation and protects against blocked deployments\nValidates handler return values to help with correct handler implementation\nSupports asynchronous handlers to enable operations that require a long waiting period for a resource, which can exceed the AWS Lambda timeout\nImplements default behavior for physical resource IDs.\n\nThe following code shows how the Provider construct is used in conjunction with a CustomResource and a user-provided AWS Lambda function which implements the actual handler.\nFunction onEvent;\r\nFunction isComplete;\r\nRole myRole;\r\n\r\nProvider myProvider = Provider.Builder.create(this, \"MyProvider\")\r\n  .onEventHandler(onEvent)\r\n  .isCompleteHandler(isComplete) // optional async \"waiter\"\r\n  .logRetention(RetentionDays.ONE_DAY) // default is INFINITE\r\n  .role(myRole)\r\n  .build();\r\n\r\nCustomResource.Builder.create(this, \"Resource1\").serviceToken(myProvider.getServiceToken()).build();\r\n\nWhen writing such an eventHandler you can use the AWS Lambda PowerTools for Java Custom Resources utility library.\nA skeleton of such a function when used with Lambda PowerTools will look like:\nimport com.amazonaws.services.lambda.runtime.Context;\r\nimport com.amazonaws.services.lambda.runtime.events.CloudFormationCustomResourceEvent;\r\nimport software.amazon.lambda.powertools.cloudformation.AbstractCustomResourceHandler;\r\nimport software.amazon.lambda.powertools.cloudformation.Response;\r\n\r\npublic class ProvisionEventHandler extends AbstractCustomResourceHandler {\r\n\r\n    @Override\r\n    protected Response create(CloudFormationCustomResourceEvent createEvent, Context context) {\r\n        doProvisioning();\r\n        return Response.success();\r\n    }\r\n\r\n    @Override\r\n    protected Response update(CloudFormationCustomResourceEvent updateEvent, Context context) {\r\n        return null;\r\n    }\r\n\r\n    @Override\r\n    protected Response delete(CloudFormationCustomResourceEvent deleteEvent, Context context) {\r\n        return null;\r\n    }\r\n}\r\n\r\n\nAs you can see from the code snippet, Lambda power tools adds a level of abstraction for you so you don\u2019t have to handle events and offers direct methods for create, update and delete of a resource. The solution itself looks very powerful and flexible for a lot of different use cases. You can add multiple operations inside such a block which makes it a powerful solution for complex operations. Creating the code for such a Lambda function looks straight forward, but still it\u2019s quite a bit of work and more code to maintain, so after reading about the AWSCustomResource construct, I had the gut feeling it was all I needed and it looks much simpler to achieve my goal.\nUsing the AWSCustomResource construct\nSo what does the AWSCustomResource construct do?\nDefines a custom resource that is materialized using specific AWS API calls. These calls are created using a singleton Lambda function.\nYou can specify exactly which calls are invoked for the \u2018CREATE\u2019, \u2018UPDATE\u2019 and \u2018DELETE\u2019 life cycle events.\nThat sounds pretty cool! Besides the AWS CDK code it sounds like we don\u2019t have to write any code to be able to leverage this. So we don\u2019t have to write the Lambda function or manage the IAM policies. All we need to do is provide de Sdk call. The rest seems to be handled by the construct. Sweet!\nLet\u2019s first define the Lambda Function that will run our own business logic and needs to be triggered during the deployment.\nFunction function = new Function(this, \"java-based-function\", FunctionProps.builder()\r\n            .runtime(Runtime.JAVA_11)\r\n            .code(Code.fromAsset(\"../app/target/app.jar\"))\r\n            .handler(\"com.jeroenreijn.aws.samples.lambdatrigger.FunctionHandler\")\r\n            .memorySize(512)\r\n            .timeout(Duration.seconds(10))\r\n            .logRetention(RetentionDays.ONE_WEEK)\r\n            .build());\r\n\nNow that our business logic function is defined, we will need to define which AWS SDK call we want to make. In our case we want to invoke a Lambda function from inside our custom resource. Let\u2019s create the SDK call to the AWS Lambda service and provide our parameters.\nPhysicalResourceId physicalResourceId = PhysicalResourceId.of(\r\n        LocalDateTime\r\n                .now()\r\n                .toString()\r\n);\r\n\r\nAwsSdkCall lambdaExecutionCall = AwsSdkCall.builder()\r\n        .service(\"Lambda\")\r\n        .action(\"invoke\")\r\n        .physicalResourceId(physicalResourceId)\r\n        .parameters(Map.of(\r\n                \"FunctionName\", function.getFunctionName(),\r\n                \"InvocationType\", \"Event\",\r\n                \"Payload\",\r\n                \"{\" + \"\\\"body\\\":\\\"{\\\\\\\"message\\\\\\\": \\\\\\\"Hello World\\\\\\\"}\\\"\" + \"}\"\r\n        ))\r\n        .build();\r\n\nIf we look at the above snippet, we can see an example of how to invoke a specific lambda function by name. The Payload parameter is optional, so if your function is not expecting a payload you can leave that out.\nWith the AWS SDK call in place we wil need to create our AwsCustomResource construct. Since we want our function logic to happen when we create our update our CDK stack we will need to add our AWS SDK call to the onCreate and onUpdate handlers.\nLast but not least, to follow the least privilege principle we make sure that our Custom Resource can only call our specific function by adding it to the Policy.\nAwsCustomResourcePolicy policy = AwsCustomResourcePolicy.fromStatements(List.of(\r\n        PolicyStatement.Builder\r\n                .create()\r\n                .actions(List.of(\"lambda:InvokeFunction\"))\r\n                .effect(Effect.ALLOW)\r\n                .resources(List.of(function.getFunctionArn()))\r\n                .build()));\r\n\r\nAwsCustomResource lambdaTriggerResource = AwsCustomResource.Builder.create(this, \"custom-resource\")\r\n        .logRetention(RetentionDays.FIVE_DAYS)\r\n        .onCreate(lambdaExecutionCall)\r\n        .onUpdate(lambdaExecutionCall)\r\n        .timeout(Duration.minutes(1))\r\n        .policy(policy)\r\n        .installLatestAwsSdk(false)\r\n        .build();\r\n\nTo make sure our business function is deployed before making the call we can add an explicit dependency. By doing so, CDK / CloudFormation will know there is a specific order in which it needs to create our resources.\nlambdaTriggerResource.getNode().addDependency(function);\nWhen you deploy the above solution AWS CDK / CloudFormation will actually create a second Lambda function for us containing the SDK call to the function that holds our actual business logic. That was exactly what I was trying to do and CDK seems to make it really simple to implement this.\n\nSummary\nAs you can see Custom Resources in AWS CDK are quite powerful. It gives you a lot of flexibility and when you need more than a single API call you can leverage the Provider framework. For single API calls using the AwsCustomResource is quite straightforward and it allowed me to invoke my lambda function on deployment.\n", "tags": ["aws", "aws cdk", "lambda", "serverless"], "categories": ["Blog", "Cloud"]}
{"post_id": 33120, "title": "AWS Lambda Provisioned Concurrency AutoScaling with AWS CDK", "url": "https://www.luminis.eu/blog-en/aws-lambda-provisioned-concurrency-autoscaling-with-aws-cdk/", "updated_at": "2022-07-19T09:26:38", "body": "A couple of weeks ago I was working on some AWS CDK based code and I was trying to figure out how to configure auto-scaling for the provisioned concurrency configuration of an AWS Lambda function. We wanted to run some performance tests on our service and were wondering how scaling provisioned concurrency would impact our overall latency. We tried with the default configuration but also wanted to experiment with a bit more aggressive scaling policy so we had to provide our own metric configuration. In this post, I will explain what provisioned concurrency is and how to set up an auto-scaling configuration for it using AWS CDK. We\u2019ll be looking at using predefined metric configurations, but also how to do it with a custom metric.\nWhat is provisioned concurrency and how does it relate to latency and cost?\nProvisioned concurrency is a feature for AWS Lambda that got introduced in 2019. It helps you keep one or more AWS lambda function instances in a \u2018warm\u2019 state in case of sudden bursts of traffic. This is particularly useful if you have functions that take a bit of time to initialize while traffic is increasing and you want to keep the latency as low as possible. Without provisioned concurrency (and without autoscaling) you might end up having a cold starts when traffic increases, which will result in a high latency for your end consumer. This can happen with all languages, but in particular with Java or .Net based applications. As you\u2019re paying for these \u2018warm\u2019 execution environments you don\u2019t want to have too many of them running when they\u2019re not in use. To prevent this there are two ways of adjusting your concurrency setting:\n\nScheduled (very useful if you have very predictable load patterns)\nTarget tracking (scale based on the increase or decrease of load)\n\nWhen you want to calculate the required concurrency you can use the following formula Transactions per second * Function Duration (in seconds). The shorter your function runs, the less concurrency you will need. Sometimes you have unpredictable traffic patterns, so a target tracking auto-scaling strategy is the best option. With the target tracking policy attached to application auto-scaling it will make sure that the number of concurrently available lambdas will stay in line with the number of requests.\n\nNow let\u2019s see what this looks like from an AWS CDK perspective.\nConfiguring our Infrastructure as Code with AWS CDK\nNow in CDK, we will need to define and configure our Lambda function. To do so you can use the Function construct and reference the jar that contains your actual code.\nFunction javaBasedFunction =\r\nnew Function(this, \"java-based-function-id\", FunctionProps.builder()\r\n.runtime(Runtime.JAVA_11)\r\n.code(Code.fromAsset(\"../app/target/java-app.jar\"))\r\n.handler(\"com.jeroenreijn.aws.samples.scaling.FunctionHandler\")\r\n.memorySize(1024)\r\n.timeout(Duration.seconds(10))\r\n.logRetention(RetentionDays.ONE_WEEK)\r\n.build());\r\n\nProvisioned concurrency for a lambda function can only be configured if the lambda function has an alias or version. In this example, we will create an alias for our function and map that alias to the latest version of our function. When deploying the CDK stack a new version will be created and the alias will be referenced to the latest version. For our function, we will start with a single provisioned lambda function.\nAlias alias = Alias.Builder.create(this, \"auto-scaling-lambda-alias-id\")\r\n.aliasName(\"auto-scaling-lambda-alias\")\r\n.provisionedConcurrentExecutions(1)\r\n.version(function.getCurrentVersion())\r\n.build();\nSo far so good for defining our function and function alias.\nRegistering Lambda functions as scalable targets with Application Auto Scaling\nNow that we have everything in place for our Lambda function we will need to register our function as a scalable target.\nThrough the AWS CDK, there are two ways of configuring autoscaling for the provisioned concurrency configuration of our function.\n\nConfiguring scaling options directly on the alias\nConfiguring scaling options via application autoscaling\n\nSo let\u2019s explore both options.\nConfiguring scaling options directly on the alias\nThe Function Alias has a short-hand method for configuring provisioned concurrency scaling. You can do this by calling the .addAutoScaling method on the Alias.\nAutoScalingOptions autoScalingOptions = AutoScalingOptions.builder()\r\n.minCapacity(0)\r\n.maxCapacity(10)\r\n.build();\r\n\r\nIScalableFunctionAttribute iScalableFunctionAttribute =\r\nalias.addAutoScaling(autoScalingOptions);\r\n\r\niScalableFunctionAttribute.scaleOnUtilization(\r\nUtilizationScalingOptions\r\n.builder()\r\n.utilizationTarget(0.7)\r\n.build()\r\n);\nAdding a scaling strategy on the alias is pretty straight forward. You can use both scaling on utilization and scale by schedule. However, it does not seem to allow for scaling by a custom metric configuration.\nConfiguring scaling options via application autoscaling\nIn CDK we can leverage the constructs available for the application autoscaling service. In our case, we will be primarily using the ScalableTarget construct to define the target that we want to scale. It requires a couple of configuration options like; min and max capacity, the id of the resource we want to scale, and the dimension we want to scale. In Java this looks like this:\nScalableTarget scalableTarget =\r\nScalableTarget.Builder.create(this, \"auto-scaling-lambda-target-id\")\r\n.serviceNamespace(ServiceNamespace.LAMBDA)\r\n.minCapacity(1)\r\n.maxCapacity(10)\r\n.resourceId(String.format(\"function:%s:%s\", function.getFunctionName(), alias.getAliasName()))\r\n.scalableDimension(\"lambda:function:ProvisionedConcurrency\")\r\n.build();\nNow that we\u2019ve defined the target we still need to provide the scaling strategy. In our case, we want to scale on the utilization of our provisioned concurrency setting. CDK, by default, has a predefined setting to scale on the ProvisionedConcurrencyUtilization metric. Besides the metric, we also need to specify a target value for which application autoscaling will trigger.\nscalableTarget\r\n.scaleToTrackMetric(\"PCU\", BasicTargetTrackingScalingPolicyProps.builder()\r\n.predefinedMetric(\r\nPredefinedMetric.LAMBDA_PROVISIONED_CONCURRENCY_UTILIZATION\r\n)\r\n.targetValue(0.7)\r\n.build()\r\n);\nThe generated scaling policy will result in two Cloudwatch alarms:\n\nAn alarm for scaling up that requires 3 data points over 1 minute each\nAn alarm for scaling down that requires 15 data points over 15 minutes\n\nBoth of these alarms will use the ProvisionedConcurrencyUtilization metric in combination with the Average statistic by default, which means that on average over these 3 data points (minutes) the value needs to be above 0.7 to trigger a scale-up.\n\nSometimes you might have sudden spikes which only happen in the first minute and not in the second or third, which will result in no scaling action. Depending on your use case this could be a problem. An alternative could be to make this a bit more of an aggressive scaling policy and use the Maximum statistic, which causes the alarm to trigger if the maximum utilization was above 0.7 for only one out of three data points. To do that we need to define the metric ourselves and specify the Maximum statistic.\nMetric maximumMetric = Metric.Builder.create()\r\n.namespace(\"AWS/Lambda\")\r\n.metricName(\"ProvisionedConcurrencyUtilization\")\r\n.statistic(\"Maximum\")\r\n.dimensionsMap(\r\nMap.of(\r\n\"FunctionName\", function.getFunctionName(),\r\n\"Resource\", function.getFunctionName() + \":\" + alias.getAliasName()))\r\n.unit(Unit.COUNT)\r\n.period(Duration.minutes(1))\r\n.build();\nTake a good look at the .dimensionsMap method as I got this wrong the first time and my scale up and down did not happen. It\u2019s important to get these values right. To be able to use the custom metric we can specify it on our TargetTracking scaling policy.\nscalableTarget\r\n.scaleToTrackMetric(\"PCU\", BasicTargetTrackingScalingPolicyProps.builder()\r\n.customMetric(maximumMetric)\r\n.targetValue(0.7)\r\n.build()\r\n);\nTesting the application autoscaling configuration\nIt\u2019s important to validate that we\u2019ve set up everything in the right way. I can tell you from experience when it comes to writing CDK code you sometimes miss predefined static values for specific services that contain the correct format and pattern for instance like with the .dimensionsMap as you can see above. If you\u2019re using the AWS console, the UI will take care of these dimensions for you, but when you manually have to specify them you need to make sure to validate that they contain the correct values for application autoscaling to do its job.\nWe can test autoscaling on our custom metric with a simple load testing tool like Artillery or Apache Benchmark. In this example, I\u2019m using Apache Benchmark as it\u2019s quite simple to run from the command line and I already had it installed on my machine.\n$ ab -n 20000 -c 20 https://someid.execute-api.eu-west-1.amazonaws.com/test/\nIf everything is set up correctly the scaling policy should trigger application autoscaling to scale up the number of provisioned concurrent lambda functions.\nValidating application autoscaling\nYou should be able to see the value change in the AWS console under the Provisioned concurrency configuration, but I always prefer to use the AWS command-line interface to see if it\u2019s doing its job. To see the scaling activities for the AWS Lambda functions you can call the application autoscaling service and ask for scaling activities that happen within the Lambda namespace.\n$ aws application-autoscaling describe-scaling-activities --service-namespace=lambda\nNow if you did that correctly it should result in a response that describes all the scaling activities going on for your AWS Lambda functions. You can see specify --resource-ids=someid if you only want to see the activities for a specific function.\n{\r\n\"ScalingActivities\": [\r\n{\r\n    \"ActivityId\": \"52873ef2-2a0d-4af4-8f58-62467cd7b4ee\",\r\n    \"ServiceNamespace\": \"lambda\",\r\n    \"ResourceId\": \"function:AwsApigatewayLambdaStack-HelloWorldHandler30C22324-E6HowYd9st0b:hello-world-lambda-alias\",\r\n    \"ScalableDimension\": \"lambda:function:ProvisionedConcurrency\",\r\n    \"Description\": \"Setting desired concurrency to 5.\",\r\n    \"Cause\": \"monitor alarm TargetTracking-function:AwsApigatewayLambdaStack-HelloWorldHandler30C22324-E6HowYd9st0b:hello-world-lambda-alias-AlarmHigh-34b6d7bb-2ab4-4a9b-9d80-7110edcfe768     in state ALARM \r\ntriggered policy AwsApigatewayLambdaStackhelloworldlambdaasgPCU388B92B5\",\r\n    \"StartTime\": \"2022-07-02T11:57:31.400000+02:00\",\r\n    \"EndTime\": \"2022-07-02T11:59:13.059000+02:00\",\r\n    \"StatusCode\": \"Successful\",\r\n    \"StatusMessage\": \"Successfully set desired concurrency to 5. Change successfully fulfilled by lambda.\"\r\n},\r\n....\r\n]\r\n}\nIf everything is working as expected you should see the currency value change based on the increase or decrease of utilization.\nSome lessons learned while testing\nWe\u2019ve tested our function and have seen that the provisioned concurrency configuration for our lambda is scaling properly, so now is a good time to talk about some gotchas. While testing this it\u2019s important to keep in mind that if you run a very big load test against your lambda and your test is finished, you need to make sure that you either manually scale down in your dev/test environment or send some slow traffic to your services over a period of time. While testing this myself I noticed in our dev/test environment that after we did our tests, the provisioned concurrency number was not going down. As we\u2019ve seen in a previous section of the article, scaling up and down is based on a CloudWatch Alarm. The alarms, however, are by default configured with \u201cTreat missing data as missing\u201d, which means that if there is no traffic no alarms will be triggered and you can have quite some warm functions in your account which quickly starts to add up in unnecessary costs.\nSummary\nIn this post we looked at scaling the provisioned concurrency settings for lambda functions by means of configuring them via AWS CDK. As you\u2019ve seen it\u2019s relatively simple to configure and it can help you save cost and keep latency down.\n", "tags": ["aws", "aws cdk", "java", "lambda", "serverless"], "categories": ["Blog", "Cloud", "Development"]}
{"post_id": 33036, "title": "The Amplify Series, Part 5: Uploading and retrieving images with Amplify Storage", "url": "https://www.luminis.eu/blog-en/cloud-en/the-amplify-series-part-5-uploading-and-retrieving-images-with-amplify-storage/", "updated_at": "2023-05-08T12:58:50", "body": "With our application in place, it is now time to start adding more functionality using some of the other Amplify categories. In this article, we will be adding Amplify Storage to upload and retrieve images from AWS in just a few steps.\nWe will start off by adding the Amplify Storage category to our project. We will follow up by using the Storage component to be able to upload and list all of our images. Finally, we will create some UI that will make use of this functionality. At the end of this article, you will have a better understanding of Amplify Storage and can use it in any scenario regarding file uploads and retrieval.\nAdding Amplify Storage to our project\nWe will continue in the repository where we left off in the last blog post. From this point, we will run amplify add storage with the following options:\n\nService: content.\nFriendly name: amplifyappimages.\nBucket name: amplifyappimages\nAccess: create, update, read, and delete, for authorized users only.\n\nThe Amplify CLI output will look similar to this:\n\r\nEvertsons-MBP:theamplifyapp evertsoncroes$ amplify add storage\r\n\r\n? Select from one of the below mentioned services: Content (Images, audio, video, etc.)\r\n\r\n\u2714 Provide a friendly name for your resource that will be used to label this category in the project: \u00b7 amplifyappimages\r\n\r\n\u2714 Provide bucket name: \u00b7 amplifyappimages\r\n\r\n\u2714 Who should have access: \u00b7 Auth users only\r\n\r\n\u2714 What kind of access do you want for Authenticated users? \u00b7 create/update, read, delete\r\n\r\n\u2714 Do you want to add a Lambda Trigger for your S3 Bucket? (y/N) \u00b7 no\r\n\r\n\u2705 Successfully added resource amplifyappimages locally\r\n\r\n\u26a0\ufe0f If a user is part of a user pool group, run \"amplify update storage\" to enable IAM group policies for CRUD operations\r\n\u2705 Some next steps:\r\n\"amplify push\" builds all of your local backend resources and provisions them in the cloud\r\n\r\n\"amplify publish\" builds all of your local backend and front-end resources (if you added hosting category) and provisions them in the cloud\r\n\nWe want users to be able to only see their own uploads. Therefore it is important that we select that only authenticated users can make use of this resource and that we give them all permissions so that they can upload, view and delete images. This is yet another useful example of how the Amplify Auth category automatically links with other categories to provide authentication and authorization functionality.\u00a0\nThis command adds some changes to our repo. In the next step, we will start using the new\u00a0category.\nWe will separate this functionality into three components:\n\nImage uploader: A component responsible for uploading the image and giving the user feedback about the upload.\n\n\nImage album: A component responsible for listing all uploaded images.\n\n\nImage viewer: A component that will show only one image in a modal.\n\nAdding an image overview page\nWe will first start by adding a new Angular component that will be used to show all images our users have uploaded::\nng generate component components/categories/storage\nThis will generate the expected files for our component. We will then link up routing to be able to render this component. This is all pretty standard Angular stuff that we have done in the previous blog, please refer to this commit for details.\nAdding image upload functionality\nWe will now add a component that will contain the functionality to upload an image:\nng generate component components/categories/storage/image-upload\nThis will generate the expected Angular files. Inside our storage.component.html we will make sure to add the newly generated image-upload:\n<app-image-upload></app-image-upload>\r\n\r\n\nInside the image-upload.component.html, we will add:\n<input\r\n\u00a0 type=\"file\"\r\n\u00a0 id=\"imageUpload\"\r\n\u00a0 name=\"imageUpload\"\r\n\u00a0 accept=\"image/png, image/jpeg\"\r\n\u00a0 (change)=\"imageSelected($event)\"\r\n/>\nThis will give us an input that we can use to select images from our device. We have to add logic to react to whenever a user selects an image in our image-upload.component.ts:\nimport { Component, OnInit } from '@angular/core';\r\n\r\n@Component({\r\n\u00a0 selector: 'app-image-upload',\r\n\u00a0 templateUrl: './image-upload.component.html',\r\n\u00a0 styleUrls: ['./image-upload.component.css']\r\n})\r\nexport class ImageUploadComponent implements OnInit {\r\n\u00a0 selectedFile: File | undefined = undefined;\r\n\r\n\u00a0 constructor() {}\r\n\r\n\u00a0 ngOnInit(): void {}\r\n\r\n\u00a0 imageSelected = (e: Event) => {\r\n\u00a0 \u00a0 const input = e.target as HTMLInputElement;\r\n\r\n\u00a0 \u00a0 if (!input.files?.length) {\r\n\u00a0 \u00a0 \u00a0 return;\r\n\u00a0 \u00a0 }\r\n\r\n\u00a0 \u00a0 this.selectedFile = input.files[0];\r\n\u00a0 };\r\n}\nWith this change we set the variable selectedFile to contain the contents of the file we selected. Now that we have this, we can add an upload button that uploads the selected image to the AWS cloud, which we previously configured.\u00a0\nWe will start off by adding a button in image-upload.component.html:\n<input\r\n\u00a0 type=\"file\"\r\n\u00a0 id=\"imageUpload\"\r\n\u00a0 name=\"imageUpload\"\r\n\u00a0 accept=\"image/png, image/jpeg\"\r\n\u00a0 (change)=\"imageSelected($event)\"\r\n/>\r\n\r\n<button class=\"aws-button\" (click)=\"uploadImage()\"> <!---NEW!-->\r\n\u00a0 Upload\r\n</button>\nOnce this button is clicked the uploadImage function is called. We will define this function in the following way in our image-upload.component.ts:\nimport { Component, OnInit } from '@angular/core';\r\nimport { Storage } from 'aws-amplify'; // <--- add this\r\n\r\n@Component({\r\n\u00a0 selector: 'app-image-upload',\r\n\u00a0 templateUrl: './image-upload.component.html',\r\n\u00a0 styleUrls: ['./image-upload.component.css']\r\n})\r\nexport class ImageUploadComponent implements OnInit {\r\n\u00a0 selectedFile: File | undefined = undefined;\r\n\r\n\u00a0 constructor() {}\r\n\r\n\u00a0 ngOnInit(): void {}\r\n\r\n\u00a0 //Add this function!\r\n\u00a0 uploadImage = async () => {\r\n\u00a0 \u00a0 if (!this.selectedFile) {\r\n\u00a0 \u00a0 \u00a0 return;\r\n\u00a0 \u00a0 }\r\n\u00a0 \u00a0 try {\r\n\u00a0 \u00a0 \u00a0 await Storage.put(this.selectedFile.name, this.selectedFile, {\r\n\u00a0 \u00a0 \u00a0 \u00a0 contentType: 'image/*',\r\n\u00a0 \u00a0 \u00a0 \u00a0 level: 'private'\r\n\u00a0 \u00a0 \u00a0 });\r\n\u00a0 \u00a0 } catch (error) {\r\n\u00a0 \u00a0 \u00a0 console.log('Error uploading file: ', error);\r\n\u00a0 \u00a0 }\r\n\u00a0 };\r\n\r\n\u00a0 //other code\r\n}\nWe first import the Storage component from aws-amplify. We then define the uploadImage function that is used by our new button. This function uses the Amplify Storage component and calls the put method, which gets the file name, the file contents, and some properties.\nIn this case, the options include the type of file we want to accept and the authorization level. Setting the level to private means that only the user that uploaded this image will have access to it.\u00a0\nOnce we have this in place we can select an image with our input and upload it using the new button. While we don\u2019t see any feedback yet, if we go into our AWS console to S3 and search for our bucket, we should see that there is a new directory called \u201cprivate\u201d which contains a directory with the cognitoID of the user we used to upload the image. Inside this directory we can see the image that was uploaded:\n\nAdding file upload progress feedback\nIn this step we want to show the user some feedback about the image upload process. We will start by adding an enum for the possible upload states and will set a variable in our component to have the correct state given the situation of the upload process. On our UI we will then show some text based on this state.\n\u00a0\nWe will first make these changes to our image-upload.component.ts:\nimport { Component, OnInit } from '@angular/core';\r\nimport { Storage } from 'aws-amplify';\r\n\r\n//Add this!\r\nenum UploadState {\r\n\u00a0 IDLE,\r\n\u00a0 UPLOADING,\r\n\u00a0 UPLOAD_COMPLETE,\r\n\u00a0 ERROR\r\n}\r\n\r\n@Component({\r\n\u00a0 selector: 'app-image-upload',\r\n\u00a0 templateUrl: './image-upload.component.html',\r\n\u00a0 styleUrls: ['./image-upload.component.css']\r\n})\r\nexport class ImageUploadComponent implements OnInit {\r\n\u00a0 selectedFile: File | undefined = undefined;\r\n\u00a0 //Add these 3:\r\n\u00a0 uploadStates = UploadState;\r\n\u00a0 uploadState: UploadState = UploadState.IDLE;\r\n\u00a0 progressText: string = '';\r\n\r\n\u00a0 constructor() {}\r\n\r\n\u00a0 ngOnInit(): void {}\r\n\r\n\u00a0 uploadImage = async () => {\r\n\u00a0 \u00a0 this.uploadState = UploadState.UPLOADING; // <--- Add this\r\n\u00a0 \u00a0 if (!this.selectedFile) {\r\n\u00a0 \u00a0 \u00a0 return;\r\n\u00a0 \u00a0 }\r\n\u00a0 \u00a0 try {\r\n\u00a0 \u00a0 \u00a0 await Storage.put(this.selectedFile.name, this.selectedFile, {\r\n\u00a0 \u00a0 \u00a0 \u00a0 contentType: 'image/*',\r\n\u00a0 \u00a0 \u00a0 \u00a0 level: 'private',\r\n\u00a0 \u00a0 \u00a0 \u00a0 progressCallback: this.progressCallback // <---- Add this\r\n\u00a0 \u00a0 \u00a0 });\r\n\u00a0 \u00a0 \u00a0 this.uploadState = UploadState.UPLOAD_COMPLETE; // <--- Add this\r\n\u00a0 \u00a0 } catch (error) {\r\n\u00a0 \u00a0 \u00a0 this.uploadState = UploadState.ERROR; // <---- Add this\r\n\u00a0 \u00a0 \u00a0 console.log('Error uploading file: ', error);\r\n\u00a0 \u00a0 }\r\n\u00a0 };\r\n\r\n\u00a0 //Add this function!\r\n\u00a0 progressCallback = (progress: any) => {\r\n\u00a0 \u00a0 this.progressText = `Uploaded: ${(progress.loaded / progress.total) *\r\n\u00a0 \u00a0 \u00a0 100} %`;\r\n\u00a0 };\r\n\r\n\u00a0 imageSelected = (e: Event) => {\r\n\u00a0 \u00a0 this.uploadState = UploadState.IDLE; // <---- Add this\r\n\u00a0 \u00a0 const input = e.target as HTMLInputElement;\r\n\r\n\u00a0 \u00a0 if (!input.files?.length) {\r\n\u00a0 \u00a0 \u00a0 return;\r\n\u00a0 \u00a0 }\r\n\r\n\u00a0 \u00a0 this.selectedFile = input.files[0];\r\n\u00a0 };\r\n}\nThe upload state will start in IDLE, switch to UPLOADING and will either go into UPLOADING_COMPLETE or ERROR depending on how the call to Amplify Storage goes. The Storage.put function also allows for the definition of a progressCallback function in order to track the progress of the upload.\u00a0\nOnce these are set in place, we add the following to our image-upload.component.html to be able to see the feedback:\n<!---Existing code-->\r\n\r\n<p\r\n\u00a0 *ngIf=\"\r\n\u00a0 \u00a0 uploadState === uploadStates.UPLOADING ||\r\n\u00a0 \u00a0 uploadState === uploadStates.UPLOAD_COMPLETE\r\n\u00a0 \"\r\n>\r\n\u00a0 {{ progressText }}\r\n</p>\r\n<p *ngIf=\"uploadState === uploadStates.UPLOAD_COMPLETE\">\r\n\u00a0 Upload complete!\r\n</p>\r\n\r\n<p class=\"error-text\" *ngIf=\"uploadState === uploadStates.ERROR\">\r\n\u00a0 Something went wrong with the file upload\r\n</p>\nWe will now get some feedback when uploading images.\nThe complete set of changes for this step can be found in this commit.\nAdding an image overview\nNow that we can upload images, we want to view all our uploaded images. We start by creating an image-album component:\nng generate component components/categories/storage/image-album\nNow we are going to update the image-album.component.ts to contain logic to get all of the images for our user:\nimport { Component, OnInit } from '@angular/core';\r\nimport { Storage } from 'aws-amplify';\r\n\r\nexport interface Image {\r\n\u00a0 key: string;\r\n\u00a0 url: string;\r\n}\r\n\r\n@Component({\r\n\u00a0 selector: 'app-image-album',\r\n\u00a0 templateUrl: './image-album.component.html',\r\n\u00a0 styleUrls: ['./image-album.component.css']\r\n})\r\nexport class ImageAlbumComponent implements OnInit {\r\n\u00a0 images: Image[] = [];\r\n\r\n\u00a0 constructor() {}\r\n\r\n\u00a0 ngOnInit(): void {\r\n\u00a0 \u00a0 this.getAllImages();\r\n\u00a0 }\r\n\r\n\u00a0 getAllImages = () => {\r\n\u00a0 \u00a0 Storage.list('', { level: 'private' })\r\n\u00a0 \u00a0 \u00a0 .then(result => {\r\n\u00a0 \u00a0 \u00a0 \u00a0 result.forEach(async imageObject => {\r\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 const objectKey = imageObject.key;\r\n\r\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 if (objectKey !== undefined) {\r\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 const signedURL = await Storage.get(objectKey, {\r\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 level: 'private',\r\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 download: false\r\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 });\r\n\r\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 this.images.push({ key: objectKey, url: signedURL });\r\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 }\r\n\u00a0 \u00a0 \u00a0 \u00a0 });\r\n\u00a0 \u00a0 \u00a0 })\r\n\u00a0 \u00a0 \u00a0 .catch(err => console.log(err));\r\n\u00a0 };\r\n}\nSome important things to note here is to set the levels to private in both the Storage.list and Storage.get functions. The reason we need to use two functions is because the Storage.list only gives us some information about the objects, such as the unique key in S3. However it does not give use a URL with which we can actually view the object.\nFor this we need a signed URL. Because we are logged in and we call the Storage.get function with the private level, the Amplify Storage component will check which user is logged in and verify that this user may view this object and create a signed url.\nWe also created an Image interface which we will use to contain the image key and signed url retrieved from S3. We will store these in an array on the component and loop through these to show the images in image-album.component.html:\n<div class=\"container\">\r\n\u00a0 <div class=\"row\" *ngFor=\"let image of images\">\r\n\u00a0 \u00a0 <div class=\"col-lg-2\"></div>\r\n\u00a0 \u00a0 <div class=\"col-lg-8\">\r\n\u00a0 \u00a0 \u00a0 <img class=\"album-image\" src=\"{{ image.url }}\" />\r\n\u00a0 \u00a0 </div>\r\n\u00a0 \u00a0 <div class=\"col-lg-2\"></div>\r\n\u00a0 </div>\r\n</div>\nThis simply loops through the imageUrls and shows an image for each. We also add a bit of styling to add the width and a bit of margin in our image-album.component.css:\n.album-image {\r\n\u00a0 width: 100%;\r\n\u00a0 margin: 8px;\r\n}\nThis results in the images being viewed on the page like this:\n\nFor all the code changes, including the wiring the new component, see this commit.\nAdding image removal functionality\nIn this section we are going to add a button to remove images that we have uploaded. We will first add a removeImage function in our image-album.component.ts:\nremoveImage = async (key: string) => {\r\n\u00a0 \u00a0 await Storage.remove(key, { level: 'private' });\r\n\u00a0 \u00a0 this.images = [];\r\n\u00a0 \u00a0 this.getAllImages();\r\n\u00a0 };\nWe reassign the images array here to force Angular to rerender the image-album component so that we can see that our image as indeed been removed. We then update our image-album.component.html to contain an \u201cx\u201d button for removal next to each image:\n<div class=\"container\">\r\n\u00a0 <div class=\"row\" *ngFor=\"let image of images\">\r\n\u00a0 \u00a0 <div class=\"col-lg-2\"></div>\r\n\u00a0 \u00a0 <div class=\"col-lg-8\">\r\n\u00a0 \u00a0 \u00a0 <img class=\"album-image\" src=\"{{ image.url }}\" />\r\n\u00a0 \u00a0 </div>\r\n\u00a0 \u00a0 <div class=\"col-lg-2\"> <!--NEW! -->\r\n\u00a0 \u00a0 \u00a0 <button class=\"remove-image-button\" (click)=\"removeImage(image.key)\">\r\n\u00a0 \u00a0 \u00a0 \u00a0 x\r\n\u00a0 \u00a0 \u00a0 </button>\r\n\u00a0 \u00a0 </div>\r\n\u00a0 </div>\r\n</div>\nAnd we add a bit of styling to the button:\n.remove-image-button {\r\n\u00a0 border: 0px;\r\n\u00a0 background: transparent;\r\n}\nAnd now we have a button that allows us to remove images, which will look something like this:\n\nAs always, the changes for this step can be found in this commit.\nSetting the maximum number of uploads per user\nThe final step we want to add for our image upload is to add a maximum number of images per user so that a user does not flood our S3 buckets. We don\u2019t have a property for this in the Amplify Storage component, so we need to build this ourselves. Also, note that the solution here is only a frontend check, there is no actual check on the bucket itself per user.\u00a0\nWe make the following changes to our image-upload.component.ts:\nimport { Component, OnInit } from '@angular/core';\r\nimport { Storage } from 'aws-amplify';\r\n\r\nconst MAX_NUMBER_OF_IMAGES_PER_USER: number = 10;\r\n\r\nenum UploadState {\r\n\u00a0 IDLE,\r\n\u00a0 UPLOADING,\r\n\u00a0 UPLOAD_COMPLETE,\r\n\u00a0 ERROR,\r\n\u00a0 MAX_REACHED // <--- add this!\r\n}\r\n\r\n@Component({\r\n\u00a0 selector: 'app-image-upload',\r\n\u00a0 templateUrl: './image-upload.component.html',\r\n\u00a0 styleUrls: ['./image-upload.component.css']\r\n})\r\nexport class ImageUploadComponent implements OnInit {\r\n\u00a0 //Existing code...\r\n\r\n\u00a0 uploadImage = async () => {\r\n\u00a0 \u00a0 this.uploadState = UploadState.UPLOADING;\r\n\u00a0 \u00a0 if (!this.selectedFile) {\r\n\u00a0 \u00a0 \u00a0 return;\r\n\u00a0 \u00a0 }\r\n\u00a0 \u00a0 try {\r\n\u00a0 \u00a0 \u00a0 //Add this part!\r\n\u00a0 \u00a0 \u00a0 const userImages = await Storage.list('', { level: 'private' });\r\n\u00a0 \u00a0 \u00a0 if (userImages.length >= MAX_NUMBER_OF_IMAGES_PER_USER) {\r\n\u00a0 \u00a0 \u00a0 \u00a0 this.uploadState = UploadState.MAX_REACHED;\r\n\u00a0 \u00a0 \u00a0 \u00a0 return;\r\n\u00a0 \u00a0 \u00a0 }\r\n\r\n \u00a0 \u00a0 //Existing code...\r\n\u00a0 \u00a0 } catch (error) {\r\n\u00a0 \u00a0 \u00a0 //Existing code ...\r\n\u00a0 \u00a0 }\r\n\u00a0 };\r\n\r\n\u00a0 //Existing code...\r\n}\nWe first add a new state called MAX_REACHED which we will set when this is the case. Inside the uploadImage function we will add a new check where we will first list the objects of the user and check if the length is equal or greater than the max. If this is the case we set the upload state to MAX_REACHED and stop the upload.\nIn our image-upload.component.html, we add a new error message that is shown in this upload state:\n<!---Existing code...-->\r\n\r\n<p class=\"error-text\" *ngIf=\"uploadState === uploadStates.MAX_REACHED\">\r\n\u00a0 Reached maximum amount of uploads. Please remove an image before trying again.\r\n</p>\nNow when we try to add the 11th image, we get the following message:\n\nChanges for this step can be found here.\nUp next: AI and ML with Amplify Predictions\nIn this blog, we have used the Amplify Storage category to update our existing application to contain image upload, viewing, and removal functionality per user. In the next article, we will look at using the power of AI and Machine Learning with Amplify Predictions.\n\u00a0\n", "tags": ["amplify", "aws", "cloud", "infrastructure as code"], "categories": ["Blog", "Cloud"]}
{"post_id": 30665, "title": "Pull requests for the designer: How to implement the workflow for a multi-platform application on Azure DevOps", "url": "https://www.luminis.eu/blog-en/development-en/pull-requests-for-the-designer-how-to-implement-the-workflow-for-a-multi-platform-application-on-azure-devops/", "updated_at": "2023-01-27T12:41:32", "body": "Enabling designers to review design implementations early on in the development process is a great way to improve the flow of getting work done in a project. Plus, it increases the collaboration of designers, engineers, testers, and business stakeholders. In this post, I\u2019ll explain how to technically implement this workflow, using a typical multi-platform application project as an example.\nThis blog post is the technical counterpart to the article explaining the conceptual side of the pull-requests-for-the-designer equation. If you want to know how to improve user experience and project flow by involving designers early in the review process, go read that one first. Then, come back here and learn how to set up the workflow yourself. Onto the implementation details!\nProject and workflow setup\nWhile we will use a Xamarin-powered multi-platform application \u2014 specifically: the FlowSuite app we built for our client Bronkhorst \u2014 and Azure DevOps as an example, the principles of the project and workflow setup can be applied to basically any user-facing software application setup.\nThe idea is simple: developers work on a single codebase that contains generic business and data binding logic for all platforms (in our case: iOS, Android, and Windows), and platform-specific code (mostly view-related). This enables us to build several functionally equivalent applications with the minimum amount of code duplication and effort.\nAt the project workflow level, a designer user interface designs and user experience descriptions aimed at the specific platforms, related to user stories. Engineers get to work on the implementation, trying their hardest to follow the specifications. Once ready for review, they submit their code, create a pull request for it, and wait for review. At that point, a build pipeline picks up the changes, builds all application variants and the implemented design is ready for review by the designer, who can leave comments, optionally ask for rework, and finally approve the change.\n\nSetting up the build pipeline\nTo enable the described workflow, we have set up an Azure Pipeline in Azure DevOps, tied to our trunk-based code repository. For every commit on a pull-request, release or main branch, a pipeline is created, and for every change to it, a pipeline run is triggered, resulting in several application builds.\n\nBelow, you can see the high-level overview of our pipeline definition, written in YAML. It contains the build trigger, necessary build meta information and the build stages. As you can see, we have a non-platform specific build stage for the application foundation, and three separate build stages, one for each platform. Each successful build is then released to our designers via AppCenter, enabling quick delivery and short process times for the review (and optional rework) stage.\ntrigger:\r\n  - main\r\n  - release/*\r\n\r\nvariables:\r\n  isRelease: $[startsWith(variables['Build.SourceBranch'], 'refs/heads/release')]\r\n  androidArtifactName: \"drop_android\"\r\n  iosArtifactName: \"drop_ios\"\r\n  uwpArtifactName: \"drop_uwp\"\r\n\r\nname: $(GITVERSION.FULLSEMVER)\r\n\r\nstages:\r\n  - stage: \"Build\"\r\n   ...\r\n\r\n  - stage: \"BuildAndroid\"\r\n    dependsOn: Build\r\n    jobs:\r\n      - template: /build-android.yml\r\n\r\n  - stage: \"BuildIos\"\r\n    dependsOn: Build\r\n    jobs:\r\n      - template: /build-ios.yml\r\n\r\n  - stage: \"BuildUwp\"\r\n    dependsOn: Build\r\n    jobs:\r\n      - template: /build-uwp.yml\r\n\r\n  - stage: \"Production_Releases\"\r\n    ...\r\n\nEnabling iOS builds and distributions\nFor iOS, some specifics are worth mentioning. First of all, we use two separate provisioning profiles to build our iOS app: one for review builds distributed via AppCenter (the scope of this article), and one for end-user release builds ending up in the Apple App Store. Secondly, we need to be able to identify builds so we can tie them to change requests. For this, we use GitVersion, but any uniquely ID that is traceable to the commit or pull request level would suffice.\nIn our pipeline configuration we use a simple \u2018isRelease\u2019 variable to trigger the applicable build and release task:\njobs:\r\n  - job: Build_iOS\r\n    pool:\r\n      vmImage: \"macos-11\"\r\n    steps:\r\n      - task: GitVersion@5\r\n        ..\r\n      - task: NuGetToolInstaller@1\r\n\r\n      - task: NuGetCommand@2\r\n        ...\r\n\r\n      - task: InstallAppleCertificate@2\r\n        inputs:\r\n          certSecureFile: \"BronkhorstDistri.p12\"\r\n          certPwd: \"*****\"\r\n          keychain: \"temp\"\r\n\r\n      - task: DownloadSecureFile@1\r\n        name: BronkhorstDistri # The name with which to reference the secure file's path on the agent, like $(mySecureFile.secureFilePath)\r\n        inputs:\r\n          secureFile: BronkhorstDistri.p12\r\n\r\n      - task: InstallAppleProvisioningProfile@1\r\n        condition: eq(variables.isRelease, false)\r\n        inputs:\r\n          provisioningProfileLocation: \"secureFiles\"\r\n          provProfileSecureFile: \"Ad_Hoc_Bronkhorst_FlowControl.mobileprovision\"\r\n\r\n      - task: InstallAppleProvisioningProfile@1\r\n        condition: eq(variables.isRelease, true)\r\n        inputs:\r\n          provisioningProfileLocation: \"secureFiles\"\r\n          provProfileSecureFile: \"Bronkhorst_Flowcontrol_distribution.mobileprovision\"\r\n\r\n      - task: UpdateiOSVersionInfoPlist@1\r\n        ...\r\n\r\n      - task: CopyFiles@2\r\n        ...\r\n\r\n      - task: XamariniOS@2\r\n        inputs:\r\n          solutionFile: \"**/*.sln\"\r\n          configuration: \"Release\"\r\n          # This value is automatically set by the InstallAppleCertificate task\r\n          signingIdentity: $(APPLE_CERTIFICATE_SIGNING_IDENTITY)\r\n          # This value is automatically set by the InstallAppleProvisioningProfile task\r\n          signingProvisioningProfileID: $(APPLE_PROV_PROFILE_UUID)\r\n          packageApp: true\r\n          args: /p:IpaPackageDir=\"$(Build.ArtifactStagingDirectory)\"\r\n\r\n      - task: PublishBuildArtifacts@1\r\n        inputs:\r\n          PathtoPublish: \"$(Build.ArtifactStagingDirectory)\"\r\n          ArtifactName: \"$(iosArtifactName)\"\r\n          publishLocation: \"Container\"\r\n\r\n      - task: AppCenterDistribute@3\r\n        condition: eq(variables.isRelease, false)\r\n        displayName: iOS group release\r\n        inputs:\r\n          serverEndpoint: \"BronkhorstAppCenter\"\r\n          appSlug: \"Luminis/Bronkhorst.FlowControl.App.iOS\"\r\n          appFile: \"$(Build.ArtifactStagingDirectory)/Bronkhorst.FlowControl.iOS.ipa\"\r\n          buildVersion: \"$(GITVERSION.FULLSEMVER)\"\r\n          releaseNotesOption: \"input\"\r\n          releaseNotesInput: \"$(GITVERSION.FULLSEMVER)\"\r\n          destinationType: \"groups\"\r\n          distributionGroupId: \"XXXX-XXXX-XXXX-XXXX-XXXX\"\r\n\nBefore this can run we need to setup a few things in AppCenter, most importantly the distributionGroupId that is used in the AppCenterDistribute task.\n\nA few more things of note: to be able to distribute the app to our designers and other testers, we need to set up a few things in AppCenter. Most importantly, we need to have a \u2018distributionGroupId\u2019 and use that in our\u00a0 \u2018AppCenterDistribute\u2019 task. Next, we\u2019ve setup a Feature Tester group that will automatically resign the app for new devices; the certificate used in the build should match the one that is used to build the app in Azure DevOps. Finally, any tester that is added to our group must set up their device once using AppCenter, after which they will be able to receive subsequential updates.\nEnabling Android builds and distributions\nOur Android Pipeline setup looks very similar: we have separate build tasks for testing and production, which will either produce an Android Package (APK) that is distributed using AppCenter, or an Android App Bundle (AAB) that can be released to Google Play. Our configuration looks something like this:\njobs:\r\n  - job: Build_Android\r\n    pool:\r\n      vmImage: \"macos-latest\"\r\n    steps:\r\n      - task: GitVersion@5\r\n        ...\r\n\r\n      - task: NuGetToolInstaller@1\r\n\r\n      - task: NuGetCommand@2\r\n        ...\r\n\r\n      - task: DownloadSecureFile@1\r\n        name: \"androidKeystore\"\r\n        inputs:\r\n          secureFile: \"bronkhorst.flowcontrol.keystore\"\r\n          \r\n      - task: PowerShell@2\r\n        displayName: Set the variable of the Android version Code\r\n        ...\r\n\r\n      - task: UpdateAndroidVersionManifest@1\r\n        ...\r\n\r\n      - task: Bash@3\r\n        name: \"BuildAndroid_aab\"\r\n        condition: eq(variables.isRelease, true)\r\n        inputs:\r\n          targetType: \"inline\"\r\n          script: \"msbuild $(Build.SourcesDirectory)/Bronkhorst.Flowcontrol.Droid/*.csproj /t:SignAndroidPackage -p:AndroidPackageFormat=aab /p:OutputPath=$(Build.ArtifactStagingDirectory) /p:Configuration=release /p:JavaSdkDirectory=$(JAVA_HOME_8_X64) -p:AndroidKeyStore=True -p:AndroidSigningKeyStore=$(androidKeystore.secureFilePath) -p:AndroidSigningStorePass=$(androidKeystore.password) -p:AndroidSigningKeyPass=$(androidKeystore.releaseKeyPassword) -p:AndroidSigningKeyAlias=release\"\r\n\r\n      - task: Bash@3\r\n        name: \"BuildAndroid_apk\"\r\n        condition: eq(variables.isRelease, false)\r\n        inputs:\r\n          targetType: \"inline\"\r\n          script: \"msbuild $(Build.SourcesDirectory)/Bronkhorst.Flowcontrol.Droid/*.csproj /t:SignAndroidPackage -p:AndroidPackageFormat=apk /p:OutputPath=$(Build.ArtifactStagingDirectory) /p:Configuration=release /p:JavaSdkDirectory=$(JAVA_HOME_8_X64) -p:AndroidKeyStore=True -p:AndroidSigningKeyStore=$(androidKeystore.secureFilePath) -p:AndroidSigningStorePass=$(androidKeystore.password) -p:AndroidSigningKeyPass=$(androidKeystore.releaseKeyPassword) -p:AndroidSigningKeyAlias=release\"\r\n\r\n      - task: PublishBuildArtifacts@1\r\n        inputs:\r\n          PathtoPublish: \"$(Build.ArtifactStagingDirectory)\"\r\n          ArtifactName: \"$(androidArtifactName)\"\r\n          publishLocation: \"Container\"\r\n\r\n      - task: AppCenterDistribute@3\r\n        displayName: Android group release\r\n        condition: eq(variables.isRelease, false)\r\n        inputs:\r\n          serverEndpoint: \"BronkhorstAppCenterAndroid\"\r\n          appSlug: \"Luminis/Bornkhorst.FlowControl.App.Android\"\r\n          appFile: \"$(Build.ArtifactStagingDirectory)/com.bronkhorst.flowcontrol-Signed.apk\"\r\n          buildVersion: \"$(GitVersion.FULLSEMVER)\"\r\n          symbolsOption: \"Android\"\r\n          releaseNotesOption: \"input\"\r\n          releaseNotesInput: \"$(GITVERSION.FULLSEMVER)\"\r\n          destinationType: \"groups\"\r\n          distributionGroupId: \"be0bc1dc-26d2-40c4-90e9-c8ac1fdfae05\"\r\n\nAfter building the APK we can publish it to the AppCenter Test group.\n\nWe once again use an AppCenter Test group to onboard our designers and other reviewers. Unlike iOS, signing of the app is not needed, as long as our reviewers allow installations from unknown sources. The AppCenter Android app can be used to easily install new versions.\nEnabling Windows builds and distributions\nIn our case, we also release a Windows application. The process is once again very similar. To prevent unnecessary wait times, we opted to only test the x86 release version of our app, reducing our build time by half compared to building for more targets. Our setup, roughly:\njobs:\r\n  - job: Build_UWP\r\n    pool:\r\n      vmImage: \"windows-latest\"\r\n    steps:\r\n      - task: GitVersion@5\r\n        ...\r\n\r\n      - task: NuGetToolInstaller@1\r\n\r\n      - task: NuGetCommand@2\r\n        ...\r\n\r\n      - task: DownloadSecureFile@1\r\n        name: \"uwpCert\"\r\n        inputs:\r\n          secureFile: \"Bronkhorst.FlowControl.UWP.pfx\"\r\n\r\n      - task: PowerShell@2\r\n        displayName: \"Set Package.appxmanifest version to GitVersion\"\r\n        ...\r\n\r\n      - task: VSBuild@1\r\n        condition: eq(variables.isRelease, false)\r\n        name: \"BuildReleaseTestVersion\"\r\n        displayName: \"Build Release Test Version\"\r\n        inputs:\r\n          platform: \"x86\"\r\n          solution: \"$(Build.SourcesDirectory)/Bronkhorst.Flowcontrol.UWP/*.csproj\"\r\n          configuration: \"Release\"\r\n          msbuildArgs: '/p:AppxBundlePlatforms=\"x86\"\r\n                        /p:AppxPackageDir=\"$(Build.ArtifactStagingDirectory)\"\r\n                        /p:AppxBundle=Always\r\n                        /p:UapAppxPackageBuildMode=StoreUpload\r\n                        /p:AppxPackageSigningEnabled=true\r\n                        /p:PackageCertificateThumbprint=\"\"\r\n                        /p:PackageCertificateKeyFile=\"$(uwpCert.secureFilePath)\"\r\n                        /p:PackageCertificatePassword=\"$(uwpCert.password)\"'\r\n\r\n      - task: VSBuild@1\r\n        condition: eq(variables.isRelease, true)\r\n        name: \"BuildReleaseVersion\"\r\n        displayName: \"Build Release Version\"\r\n        inputs:\r\n          platform: \"x86\"\r\n          solution: \"$(Build.SourcesDirectory)/Bronkhorst.Flowcontrol.UWP/*.csproj\"\r\n          configuration: \"Release\"\r\n          msbuildArgs: '/p:AppxBundlePlatforms=\"x86|x64|ARM\"\r\n                        /p:AppxPackageDir=\"$(Build.ArtifactStagingDirectory)\"\r\n                        /p:AppxBundle=Always\r\n                        /p:UapAppxPackageBuildMode=StoreUpload\r\n                        /p:AppxPackageSigningEnabled=true\r\n                        /p:PackageCertificateThumbprint=\"\"\r\n                        /p:PackageCertificateKeyFile=\"$(uwpCert.secureFilePath)\"\r\n                        /p:PackageCertificatePassword=\"$(uwpCert.password)\"'\r\n\r\n      - task: CopyFiles@2\r\n        displayName: \"Copy Output Files to: $(Build.ArtifactStagingDirectory)\"\r\n        inputs:\r\n          SourceFolder: \"$(system.defaultworkingdirectory)\"\r\n          Contents: '**\\bin\\Release\\**'\r\n          TargetFolder: \"$(Build.ArtifactStagingDirectory)\"\r\n\r\n      - task: PublishBuildArtifacts@1\r\n        inputs:\r\n          PathtoPublish: \"$(Build.ArtifactStagingDirectory)\"\r\n          ArtifactName: \"$(uwpArtifactName)\"\r\n          publishLocation: \"Container\"\r\n\r\n      - task: AppCenterDistribute@3\r\n        displayName: \"UWP group release $(GitVersion.AssemblySemVer)\"\r\n        condition: eq(variables.isRelease, false)\r\n        inputs:\r\n          serverEndpoint: \"BronkhorstAppCenterUWP\"\r\n          appSlug: \"Luminis/Bronkhorst-FlowSuite-UWP\"\r\n          appFile: \"$(Build.ArtifactStagingDirectory)/Bronkhorst.FlowControl.UWP_$(GitVersion.AssemblySemVer)_Test/Bronkhorst.FlowControl.UWP_$(GitVersion.AssemblySemVer)_x86.msixbundle\"\r\n          buildVersion: \"$(GITVERSION.AssemblySemVer)\"\r\n          symbolsOption: \"UWP\"\r\n          releaseNotesOption: \"input\"\r\n          releaseNotesInput: \"$(GITVERSION.FULLSEMVER)\"\r\n          destinationType: \"groups\"\r\n          distributionGroupId: \"XXXX-XXXX-XXXX-XXXX-XXXX\"\r\n\nUnsigned app bundles cannot be installed on Windows, so we need to sign our app using a certificate. If you use one that is not trusted by Windows, your reviews must first install the certificate as a root certificate before they can install the test application versions.\nConclusion\nI hope that Mike and I have managed to convince you that improving user experience and project flow is possible with the right attitude, process, and technology. In our case, Azure DevOps and AppCenter helped us a great deal in reducing lead times and thus delivering customer value early and often. Our setup is specific, but the principles of our solutions should be easily applicable to other technology stacks and similar use cases. We\u2019d love to hear how you did it!\n", "tags": ["Android", "AppCenter", "Azure DevOps", "CI/CD", "Design", "iOS", "Mobile", "Windows"], "categories": ["Blog", "Development"]}
{"post_id": 30750, "title": "Pull requests for the designer: How to improve the development process", "url": "https://www.luminis.eu/blog-en/concepting-ux-en/pull-requests-for-the-designer-how-to-improve-the-development-process/", "updated_at": "2023-01-27T12:43:49", "body": "At Luminis we see user experience and usability as crucial factors in developing successful mobile and web apps.\u00a0We are always looking for ways to improve the way we as a team collaborate and work towards delivering those great applications. One of those improvements is putting the designer into the technical workflow of the developers.\nOur approach\nIn short, our approach goes something like this. After the product definition phase, the designer starts creating the various screens and interactions to accommodate the use cases as described on the backlog. This is done in collaboration with the Product Owner (PO) and the development team. In doing so, we design the right solution (checked by PO) and the design is implementable (checked by the developers).\nWhen the designs are done, the development team can still involve the designer for assistance during implementation, sometimes resulting in updated designs. This flexibility and close contact helps us as a team to implement the best possible solution within 1 sprint.\nThe problem\nThis close contact between the PO, the designer and the developers makes us agile and helps us to quickly catch possible issues and lose little time by preventing rework. But what did not work well enough for us, is the moment after the developers are done with implementing those designs.\u2028\u2028The developer finishes the functionality, sets the backlog item to done (after being reviewed by a colleague developer of course), and that functionality is then presented in the demo at the end of sprint. In our experience, this process was lacking for two reasons. \u2028One, during this demo the designer sometimes would notice a detail or interaction that would not work well or as intended. This can be the result of a mistaken implementation of the design or an unexpected issue caused by something else. As a consequence, this issue would be put in the next sprint to fix. \u2028The second reason is that the demo shows the functionality in a limited way, often as one specific use case as described in the original user story. That is quite different from experiencing the app on your own device, going through several use cases. You\u2019ll experience stuff like loading times, and how an interaction feels in the bigger picture of the whole application.\nThe fix\nSo there was a weak link between finishing the code by the developers, and demoing this work at the end of the sprint. How did we fix this? We involve the designer as soon as possible, when the functionality has been built but before this is shown at a demo. Using the existing development process with its tools, it is quite easy for the designer to test the new functionality.\u00a0And that is done by involving the designer in approving pull requests as well. (What is a pull request, you ask? It is basically a developer asking the team to review his or her code, before it become part of the main code.)\u00a0The developer is then not only sending a pull request to the developers but also the designer. The designer can download the right build of the app to experience and review this new functionality.\n\nThis was the fix we needed in our process. As a designer I get notified when a PR is available, I download the build and test the functionality. I am able to verify if the design and the PO\u2019s wishes have been implemented well, and I could act quickly (with suggestions or updated designs) if something was off. For our development process, this gives us multiple advantages:\n\nA designer can check the functionality and help to improve it before the end of the sprint, so the functionality is definitely finished at that point. And that results in a satisfied PO at the demo;\nWith a review of the PR when it becomes available, the developer has the code and solution still fresh in his or her memory. So discussing or changing the solution, takes less time and effort for the developer, in comparison to fixing an issue in the next sprint.\u00a0This will more then compensate the extra time the designer puts in to do the PR work;\nA designer becomes a semi-tester, by putting the new functionality to the test and trying various inputs and interactions.\n\nIn a recent project, we applied this process when developing a mobile app. My colleague Frank explains in this post how you could set this is up for your own projects.\nConclusion\nIf user experience and usability of an application are important, it makes sense to involve the designer sooner and more often during the development process to review newly built functionalities. The best way to do this is to use the already existing process of pull requests, and sending the designer a PRs as well. Then not only the code will be reviewed, but the user-facing solution will be tested as well, resulting in better and more user-friendly applications.\n", "tags": ["Design"], "categories": ["Blog", "Concepting &amp; UX", "Development"]}
{"post_id": 32408, "title": "Creating a simple API stub with API Gateway and S3", "url": "https://www.luminis.eu/blog-en/cloud-en/creating-a-simple-api-stub-with-api-gateway-and-s3/", "updated_at": "2022-05-20T10:27:46", "body": "A while ago my team was looking to create a stub for an internal JSON HTTP based API. The to-be stubbed service was quite simple. The service exposed a REST API endpoint for listing resources of a specific type. The API supported paging and some specific request/query parameters.\nGET requests to the service looked something like this:\n/items?type=sometype&page=2\nWe wanted to create a simple stub for the service within our AWS account which we could use for testing. One of the tests we wanted to perform is to test if the service was down. If so our application would follow a specific code path. We could of course not easily test this with the real services. of the remote API, we tried to keep things as simple as possible.\nCreating the API with API Gateway, Lambda, and S3\nAs most of the services within our domain are based on Amazon API Gateway and AWS Lambda, we started looking into that direction at first. As our stub was read-only and we didn\u2019t have to modify the items, we chose to create an initial export of the dataset from the remote API into JSON files, which we could store in S3. For storing the files we chose to use a file name pattern that resembled our type and page parameter.\n{TYPE}_{PAGE_NUMER}(.json)\nThis resulted in a bucket like this:\n\nSo the simplified design of the API stub was going to be as follows:\n\nAn example version of what our code looked like was something similar to this:\n\nAs you can see in the above snippet, we\u2019re essentially calculating the path to the object in S3 based on some request parameters. When the path is resolved we just fetch the file from S3, and convert it to a string for the reply to API Gateway, which in turn returns the file to the consumer. Our lambda function was just acting as a simple proxy and we were wondering if we could get rid of the lambda function at all. Maintaining code, dependencies, etc takes a burden, so if we don\u2019t need it we would like to get rid of it.\nCreating the API with just API Gateway and S3\nAPI Gateway has great support for direct integration with other AWS services, so we started exploring our options. The solution we hoped for was something similar to the image below.\n\nWhile going through the documentation for API Gateway we found a pretty good example of how to use API Gateway as a proxy for S3. The provided solution lets API Gateway mirror the folder and file structure(s) in S3. Useful, but it did not cover our use case.\nOne of the other options that looked promising was configuring mapping templates. We had used that before to transform an incoming request body to a different format for the remote backend. In case you\u2019re unfamiliar with mapping templates in API Gateway:\nA mapping template is a script expressed in Velocity Template Language (VTL)and applied to the payload using JSONPath expressions.\nAfter digging through the API Gateway documentation we also discovered that mapping templates can be used to alter the query string, headers, and path.\n\nModifying the path was exactly what we wanted, so we tried that and it worked like a charm. Let\u2019s take a look at the resulting setup for our API Gateway GET request.\n\nAs you can see in the above section we\u2019ve added a GET method to the root of our API. The GET method has a method request which defines both query parameters; page and type.\n\nFor the integration request, we define the remote service we want to integrate with and we specify the bucket and a dynamic fileName.\n\nIn the path override of the integration request there are two important things to notice:\n\nAt the start of the Path override parameter, we provide the S3 bucket name\nAs the second argument, we provide a dynamic value named fileName\n\nThe path override will therefor be {bucket-name}/{fileName}.\nNow in our mapping template, we will fill the fileName parameter so that API Gateway knows which file to get from S3.\nLet\u2019s take a look at the mapping template.\n\nAs you can see we\u2019ve set up a template for requests for content-type application/json. Now when a GET request arrives with the type and page query parameters, it will assemble the resulting fileName variable in the path override.\n\nWhen the file in that specific bucket is found it will return the corresponding JSON. When the file is not found it will throw a 404 response body. We can also map the response code with a mapping template to produce some nice-looking error messages.\nSome last thoughts\nWhile researching this solution I also came across a post by Tom Vincent. Tom wrote a nice post about what he calls Lambdaless. Lambdaless is in essence an integration from API Gateway to another AWS Service without the use of AWS Lambda. I like the term and it resonates well with what we also tried to achieve in this post. In essence it\u2019s similar to what Peter wrote when he wrote his articles on creating a REST API on top of DynamoDB.\nI think this post shows a nice way of using Mapping Templates in API Gateway to transform the path and create a simple stub. Do keep in mind we use this for testing only and we don\u2019t run production workloads with this setup. I also think that if the API would have been more complex we would not have taken this approach. Nevertheless, it was simple to set up and use for our stub.\n", "tags": ["aws", "cloud"], "categories": ["Cloud", "Development"]}
{"post_id": 32359, "title": "Production-Ready CDK \u2013 Bootstrapping", "url": "https://www.luminis.eu/blog-en/production-ready-cdk-bootstrapping/", "updated_at": "2023-01-27T12:55:25", "body": "\nIn the previous posts, we set up the project, wrote our first construct, and implemented a CI/CD Pipeline using CDK Pipelines. If you haven\u2019t already done so, Bootstrapping AWS CDK is one of the first things we need to do in our AWS Accounts.\n\n\n\n\n\nWhat is AWS CDK Bootstrapping?\nTo use CDK and deploy our CDK app to our AWS environment(an AWS account plus a region), we need to provision the resources required for deployment. These resources are mainly an S3 bucket to store files and IAM Roles to have permissions related to deployment. We call the process of setting these resources bootstrapping.\nAccording to the documentation, you need bootstrapping if you use Assets and if your generated app template file size is more than 50 kilobytes. However, you will almost always cover these two cases, so you can start doing it without thinking more.\nWhy is it worth discussing?\nYou might be wondering why even I write about bootstrapping. Because in many tutorials, it says you need to use the cdk bootstrap command, and you are good to go, which is right most of the time.\nBut when you are using a company AWS account, the command will probably fail because of the permissions boundary. In that case, you need to consider Custom Bootstrapping, which might take some time if you don\u2019t know how to do it. The following information will help you do it in a few minutes.\nIf the cdk bootstrap command works on the first try, you most likely don\u2019t have the permissions boundary setup(you should seriously consider implementing it).\n\n\n\n\nSide note: CDK v2 uses the modern bootstrapping template by default, unlike CDK v1. In CDK v1, the default template is the legacy one, and the modern one is optional as you need to use the context flag:\n\u201c@aws-cdk/core:newStyleStackSynthesis\u201d: \u201ctrue\u201d.\nIf you have the legacy template in your environment, I recommend updating it and this post might be relevant to you. You can check the differences here.\n\n\n\n\nAlright, how do we\u00a0proceed?\nHere is the documentation for the bootstrapping. Again, using CDK v2, I would try cdk bootstrap with my local credentials and profile (example: including --profile dev), and it would fail(hopefully)!\nHaving purposeful guardrails in the cloud is essential. It might first seem they are restricting us, developers. But, if we use the guardrails well, they will save us from future headaches.\nNow, let\u2019s say we have already tried the command and it didn\u2019t work, because a role creation failed due to the permissions boundary. We need to think of Custom Bootstrapping, which is straightforward.\nAs the next step, you can download the bootstrapping template using the command cdk bootstrap --show-template > bootstrap-template.yaml. Then use the command cdk bootstrap --template bootstrap-template.yaml to deploy it. But it will fail again because we haven\u2019t changed anything related to the permissions boundary.\nPermissions Boundary\u00a0Changes\nWe need to provide a permissions boundary to every role in the bootstrapping template to resolve the problem. To achieve that quickly, we can get the Amazon Resource Name(ARN) of the boundary as an input parameter and then use it.\nIn the example below, I omitted and dotted some parts for brevity. We can ignore those lines.\nYou can see the boundary parameter with the name PermissionsBoundaryArn. After defining the input parameter, we use it in the IAM Role declaration on line 54.\n\r\nDescription: This stack includes resources needed to deploy AWS CDK apps into this\r\n  environment\r\nParameters:\r\n  .\r\n  .\r\n  .\r\n  .\r\n  .\r\n  PublicAccessBlockConfiguration:\r\n    Description: Whether or not to enable S3 Staging Bucket Public Access Block Configuration\r\n    Default: 'true'\r\n    Type: 'String'\r\n    AllowedValues: ['true', 'false']\r\n  PermissionsBoundaryArn:\r\n    Description: ARN of the Permissions Boundary\r\n    Type: 'String'\r\nConditions:\r\n  HasTrustedAccounts:\r\n    Fn::Not:\r\n      - Fn::Equals:\r\n          - ''\r\n          - Fn::Join:\r\n              - ''\r\n              - Ref: TrustedAccounts\r\n  .\r\n  .\r\n  .\r\n  .\r\nResources:\r\n  .\r\n  .\r\n  .\r\n  .\r\n  FilePublishingRole:\r\n    Type: AWS::IAM::Role\r\n    Properties:\r\n      AssumeRolePolicyDocument:\r\n        Statement:\r\n          - Action: sts:AssumeRole\r\n            Effect: Allow\r\n            Principal:\r\n              AWS:\r\n                Ref: AWS::AccountId\r\n          - Fn::If:\r\n              - HasTrustedAccounts\r\n              - Action: sts:AssumeRole\r\n                Effect: Allow\r\n                Principal:\r\n                  AWS:\r\n                    Ref: TrustedAccounts\r\n              - Ref: AWS::NoValue\r\n      RoleName:\r\n        Fn::Sub: cdk-${Qualifier}-file-publishing-role-${AWS::AccountId}-${AWS::Region}\r\n      PermissionsBoundary: !Ref PermissionsBoundaryArn\r\n      Tags:\r\n        - Key: aws-cdk:bootstrap-role\r\n          Value: file-publishing\r\n\nHere is the complete bootstrapping template. If you want to deploy from the CloudFormation Console, you don\u2019t need to change anything(the template version is v12 by the time I wrote this). You only need to provide the ARN as input.\nTo deploy from your CLI, you need to provide the ARN value for PermissionsBoundaryArn in the template. Then you can use the command cdk bootstrap --template bootstrap-template.yaml.\nMore Customization\nDepending on the industry & size of your company, your organization might have specific Compliance & Security requirements, such as adding Tags to the resources or having encryption for S3 Buckets. Depending on those requirements, you can keep modifying the template and deploy it in the same way.\nFor the other CLI command options, you can recheck the documentation. I recommend using flags:\n\n\u2012 \u2012 tags: it is a best practice to tag all resources you can.\n\u2012 \u2012 termination-protection: and have it enabled. Because you hardly need to delete the bootstrapping stack, you better have it.\n\u2012 \u2012 trust: in case you need to deploy from your dev environment to higher environments.\n\n\n\n\n\nIn this part of the series, we focused on a shorter topic, bootstrapping, and especially how to do this in a custom way.\nIn the next chapter, we will continue with Aspects to overcome the Permissions Boundary. This time, it is for our CDK Pipeline roles and all the other roles/resources we will provision.\nSee you in the next ones. Ciao!\n\n\n\n\n", "tags": ["aws", "aws cdk", "cloud", "devops"], "categories": ["Blog", "Cloud"]}
{"post_id": 32212, "title": "Luminis partners with AWS in Cloud Lab at Cupola XS", "url": "https://www.luminis.eu/blog-en/luminis-partners-with-aws-in-cloud-lab-at-cupola-xs/", "updated_at": "2022-05-05T13:18:16", "body": "Haarlem \u2013 Luminis and AWS are working together to accelerate the adoption of cloud technology by SMEs. Luminis has joined the Cloud Lab set up by Amazon Web Services (AWS) at the Cupola XS innovation center to help organizations innovate faster and scale their innovations more easily. As an AWS Advanced Consulting Partner, Luminis offers direct access to its knowledge and expertise in cloud and data technology in the domed building that used to be Haarlem prison.\nIn partnering at the Cloud Lab, AWS and Luminis share the ambition of accelerating the adoption of cloud technology by SMEs and their employees. AWS is a global pioneer in cloud services and has set up the Cloud Lab on the third floor at the Cupola XSinnovation center. Netherlands-based cloud-native consultancy CloudNation has also joined the lab.\nSharing knowledge and expertise\nThe Cloud Lab will actively contribute to knowledge events in an inspiring setting that used to be Haarlem prison. Luminis will share its knowledge and expertise in business and digital transformation at these events at least four times a year. Together with AWS, Luminis shows how cloud technology enables innovation.\u00a0\n\u201cLuminis is looking forward to actively contributing to the Cupola XS community. Besides attending the knowledge events and presentations, my Luminis colleagues and I will be present in person at the Cloud Lab to assist organizations and their employees with their digital needs and innovations,\u201d said Richard M. de Wolf, commercial partner lead at Luminis.\nLuminis as AWS Partner\nLuminis is an AWS Advanced Consulting Partner . Luminis spends a considerable amount of time gaining and maintaining knowledge and experience of AWS products and services. Luminis colleagues work with AWS technology daily and use this knowledge to assist the company\u2019s customers. This joint expertise will also be available at the physical Cloud Lab as part of the intensive collaboration between Luminis and AWS.\n", "tags": ["aws", "cloud labs", "cupola", "news"], "categories": ["Blog", "News"]}
{"post_id": 31828, "title": "The Amplify Series, Part 4: Developing and deploying a cloud-native application with AWS Amplify", "url": "https://www.luminis.eu/blog-en/the-amplify-series-part-4-developing-and-deploying-a-cloud-native-application-with-aws-amplify/", "updated_at": "2023-05-08T12:59:24", "body": "In this article, we will be using AWS Amplify to create a cloud-native application. We will cover the Authentication, GraphQL API, and Hosting categories of AWS Amplify. We will be using Angular for the frontend. However, the steps taken here should be similar for other frameworks.\nWe have learned what Amplify is, how Amplify works behind the scenes, and Amplify use cases throughout this series. It is time to see this in action and get hands-on experience using\u00a0 Amplify. At the end of this article, you will have a cloud-native application.\nPrerequisites\nBefore getting started, we need to make sure we have some things set. You need to have the following to be able to follow these instructions and build your application:\n\nAn active AWS account.\nNodeJS: I used version 14.17.5.\nAngular CLI: I used version 13.1.4.\nGit: Necessary to check out the example project.\nAmplify CLI: I used version 7.6.22.\n\nConfiguring Amplify CLI\nBefore we can start using the CLI, we need to configure the AWS Amplify CLI to use our AWS account. Once you have configured your CLI, we can get started.\nInstalling dependencies & launching the app\nWe have created an example AWS Amplify Angular app that you can clone. Check out the start_here branch and start from there. To ensure everything is working, run npm run start. You should see a simple application called \u201cThe Amplify App\u201d:\n\nThis app is currently a simple Angular app with no associated backend yet. From this point, we will start using the CLI to initialise our project, generate backend resources, and connect them to the frontend.\nInitialising the backend\nThe first command we will run is amplify init. You should see output similar to this:\nEvertsons-MBP:theamplifyapp evertsoncroes$ amplify init\r\nNote: It is recommended to run this command from the root of your app directory\r\n? Enter a name for the project theamplifyapp\r\nThe following configuration will be applied:\r\n\r\nProject information\r\n| Name: theamplifyapp\r\n| Environment: dev\r\n| Default editor: Visual Studio Code\r\n| App type: javascript\r\n| Javascript framework: angular\r\n| Source Directory Path: src\r\n| Distribution Directory Path: dist/theamplifyapp\r\n| Build Command: npm run-script build\r\n| Start Command: ng serve\r\n\r\n? Initialize the project with the above configuration? Yes\r\nUsing default provider awscloudformation\r\n? Select the authentication method you want to use: AWS profile\r\n\r\nFor more information on AWS Profiles, see:\r\nhttps://docs.aws.amazon.com/cli/latest/userguide/cli-configure-profiles.html\r\n\r\n? Please choose the profile you want to use default\r\n\u2834 Initializing project in the cloud...\r\n\nOnce this is done, our Amplify project is created, and we can see several new files in our repository. If you need a refresher of what these files are and how Amplify works, check out part 2 of this blog series: how does AWS amplify work?\nIf you are creating a public project, it is recommended to add the amplify/team-provider-info.json to the .gitignore, as it contains identifiers to the resources we are using for Amplify. This does not create a security risk since your project is secured with your credentials. If multiple developers are working on the same project, they all need this file.\nIf everything is working, you should see about 7 changed files.\nConfiguring the frontend\nNow we need to install the relevant Amplify frontend libraries. In this case, we run:\nnpm install --save aws-amplify @aws-amplify/ui-angular\nThis will install the amplify library and Angular specific UI elements that we will be using later. After this, we want to configure the Amplify library with our aws-exports.js by adding the following to our src/main.ts:\nimport Amplify from 'aws-amplify';\r\nimport aws_exports from './aws-exports';\r\nAmplify.configure(aws_exports);\nYour IDE might complain about the ./aws-exports not having a type definition. An easy for now is to add allowJs:true\u00a0to our tsconfig.json.\nFinally, we also need to add the following to src/polyfill.ts:\n(window as any).global = window;\r\n(window as any).process = {\r\nenv: { DEBUG: undefined }\r\n};\nWe need to add these since they are used by Amplify but are not present by default since Angular 6.\nYou should see around 5 file changes after this step.\nAdding Amplify Authentication\nWe are now going to add user authentication functionality to our application.\u00a0\nGenerating backend resources for authentication\nThe first thing we will do is generate backend resources that will help us with authentication by running amplify add auth. You should see output similar to this:\nEvertsons-MBP:theamplifyapp evertsoncroes$ amplify add auth\r\nUsing service: Cognito, provided by: awscloudformation\r\n\r\nThe current configured provider is Amazon Cognito.\r\n\r\nDo you want to use the default authentication and security configuration? Default configuration\r\nWarning: you will not be able to edit these selections.\r\nHow do you want users to be able to sign in? Username\r\nDo you want to configure advanced settings? No, I am done.\r\n\u2705 Successfully added auth resource theamplifyappfdeaa7e5 locally\r\n\r\n\u2705 Some next steps:\r\n\"amplify push\" will build all your local backend resources and provision it in the cloud\r\n\"amplify publish\" will build all your local backend and frontend resources (if you have hosting category added) and provision it in the cloud\nWe should now see some new files added to our project, such as files under amplify/backend/auth, which represent our authentication choices. However, nothing has happened in the backend yet. We need to run amplify push to push our changes to AWS and have CloudFormation create the resources for us:\nEvertsons-MBP:theamplifyapp evertsoncroes$ amplify push\r\n\r\n\u2714 Successfully pulled backend environment dev from the cloud.\r\n\r\nCurrent Environment: dev\r\n\r\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\r\n\u2502 Category \u2502 Resource name \u2502 Operation \u2502 Provider plugin \u2502\r\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\r\n\u2502 Auth \u2502 theamplifyappfdeaa7e5 \u2502 Create \u2502 awscloudformation \u2502\r\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\r\n? Are you sure you want to continue? Yes\r\n\u280b Updating resources in the cloud. This may take a few minutes...\nAdding the authentication component to the frontend\nWe will be using the Amplify Authenticator UI component to implement authentication in the frontend. We already installed this previously however now, we still need to add it to src/app/app.module.ts:\n//Other imports\r\nimport { AmplifyAuthenticatorModule } from '@aws-amplify/ui-angular'; // <-- Add this\r\n\r\n@NgModule({\r\ndeclarations: [\r\n//Existing declarations\r\n],\r\nimports: [\r\n//other imports\r\nAmplifyAuthenticatorModule // <-- Add this\r\n],\r\n// Other stuff\r\n})\r\nexport class AppModule {}\nWe are going to start off by simply protecting our entire application with the Amplify authenticator. To do this, add the following to src/app/app.component.html:\n<amplify-authenticator><!-- <--Add this -->\r\n<app-header></app-header>\r\n<div class=\"content-container\">\r\n<router-outlet></router-outlet>\r\n</div>\r\n<app-footer></app-footer>\r\n</amplify-authenticator> <!-- <--Add this -->\nAnd we also need to import the default styling in our src/app/styles.css:\n@import \"@aws-amplify/ui-angular/theme.css\";\r\n\r\n/* other styles */\nIf you run your application now, you should see the Amplify authenticator:\n\nYou can use this to create an account and log in to see the application. It is a fully-featured authenticator with registration, sign-in, forgot password, and email activation. It also gives feedback when errors occur.\u00a0\nThere are several options to customize this UI. You can find out more about the Amplify Authenticator in the documentation.\nOnce you have created an account, you should be able to log in to the AWS console, navigate to the Cognito service and see a user pool with the name of your project. Inside that user pool, you should see your new account:\n\nAdding signout functionality\nWe also want to be able to sign out of our application. First we add the actual button in header.component.html:\n<nav>\r\n    <!-- other existing code -->\r\n    <button class=\"aws-button\" (click)=\"signOut()\">Sign out</button> <!-- \u00a0<-- Add this -->\r\n</nav>\nAnd then, we add our sign out logic, which we get directly from Amplify, in our header.component.ts:\nimport { Auth } from 'aws-amplify'; // <-- Add this\r\n\r\nexport class HeaderComponent implements OnInit {\r\n\r\n// Other existing code\r\n\r\n    signOut() { // <-- Add this\r\n      Auth.signOut();\r\n    }\r\n}\nAnd now, we have a sign-out button which we can use to sign out and go back to the Amplify Authenticator.\nUser data\nWe will add a Welcome <<username>> text in the header. We can use the Amplify Auth library to get this data. We will begin by creating an Angular service which we will call UserService, which will handle all of our logged-in user\u2019s needs.\u00a0\nIn the terminal, run the following at the root of your project:\nng generate service services/shared/user\nWe will update the new service in the following way:\nimport { Injectable } from '@angular/core';\r\nimport { Auth } from 'aws-amplify';\r\n\r\nexport interface UserInfo {\r\n\u00a0 email: string;\r\n\u00a0 username: string;\r\n}\r\n\r\n@Injectable({\r\n\u00a0 providedIn: 'root'\r\n})\r\nexport class UserService {\r\n\u00a0 constructor() {}\r\n\r\n\u00a0 async getCurrentUserInfo(): Promise {\r\n\u00a0 \u00a0 const userInfoResponse = await Auth.currentUserInfo();\r\n\u00a0 \u00a0 return {\r\n\u00a0 \u00a0 \u00a0 email: userInfoResponse.attributes.email,\r\n\u00a0 \u00a0 \u00a0 username: userInfoResponse.username\r\n\u00a0 \u00a0 };\r\n\u00a0 }\r\n}\nIt makes use of the Amplify Auth library to get information about the current user. We then make some changes in our header.component.ts file to call this function:\nimport { UserService, UserInfo } from '../../services/shared/user.service'; //<--- Add this\r\n\r\nexport class HeaderComponent implements OnInit {\r\n\u00a0 //other code\r\n\u00a0 currentUserName: string = ''; //<--- Add this\r\n\r\n\u00a0 constructor(public router: Router, private userService: UserService ) { //<--- Add userService\r\n\u00a0 \u00a0 //other code\r\n\u00a0 }\r\n\r\n\u00a0 ngOnInit(): void { //<--- Add this \u00a0 \u00a0 this.userService \u00a0 \u00a0 \u00a0 .getCurrentUserInfo() \u00a0 \u00a0 \u00a0 .then((userInfo: UserInfo) => {\r\n\u00a0 \u00a0 \u00a0 \u00a0 this.currentUserName = userInfo.username;\r\n\u00a0 \u00a0 \u00a0 })\r\n\u00a0 \u00a0 \u00a0 .catch(error => {\r\n\u00a0 \u00a0 \u00a0 \u00a0 console.log('Error while obtaining user: ', error);\r\n\u00a0 \u00a0 \u00a0 });\r\n\u00a0 }\r\n}\nAnd now that we have variable in the header component with the current username, we can display this in the header.component.html:\n<nav>\r\n<!-- other code -->\r\n    <span class=\"welcome-text\">Welcome {{ currentUserName }}</span> <!-- <--- add this -->\r\n    <button class=\"aws-button\" (click)=\"signOut()\">Sign out</button>\r\n</nav>\nAnd finally we add some styling in header.component.css so that we can actually see the text on the dark background:\n.welcome-text {\r\n  color: white;\r\n  margin-right: 16px;\r\n}\nAnd now we have a text that shows us the username of the user that is logged in:\n\nCheck out the Amplify Auth documentation\u00a0for details.\nWe\u2019ve changed several things now:\n\nWe added the Amplify Authenticator UI.\nWe added a signout button.\nWe added user details to the header.\n\nConnecting the backend with GraphQL\nIn this section, we will add a GraphQL API for our application, which will replace the current in-memory mock data being used for the API page in our application. If you click through the API tab, you will see that we have a list of Posts that can all have a list of Likes and Comments. We will be modeling this in GraphQL.\nGenerating backend resources for the API\nTo start, we need to add a resource endpoint:\nEvertsons-MBP:theamplifyapp evertsoncroes$ amplify add api\r\n? Select from one of the below-mentioned services: GraphQL\r\n? Here is the GraphQL API that we will create. Select a setting to edit or continue Authorization modes: API key (default, expiration time: 7 days from now)\r\n? Choose the default authorization type for the API Amazon Cognito User Pool\r\nUse a Cognito user pool configured as a part of this project.\r\n? Configure additional auth types? No\r\n? Here is the GraphQL API that we will create. Select a setting to edit or continue Continue\r\n? Choose a schema template: One-to-many relationship (e.g., \u201cBlogs\u201d with \u201cPosts\u201d and \u201cComments\u201d)\r\n\r\n\u26a0\ufe0f WARNING: Some types do not have authorization rules configured. That means all create, read, update, and delete operations are denied on these types:\r\n- Blog\r\n- Post\r\n- Comment\r\nLearn more about \"@auth\" authorization rules here: https://docs.amplify.aws/cli/graphql/authorization-rules\r\nGraphQL schema compiled successfully.\r\n\r\nEdit your schema at /Users/evertsoncroes/Documents/development/private/theamplifyapp/amplify/backend/api/theamplifyapp/schema.graphql or place .graphql files in a directory at /Users\r\n/evertsoncroes/Documents/development/private/theamplifyapp/amplify/backend/api/theamplifyapp/schema\r\n\u2714 Do you want to edit the schema now? (Y/n) \u00b7 yes\r\n\u2705 Successfully added resource theamplifyapp locally\r\n\r\n\u2705 Some next steps:\r\n\"amplify push\" will build all your local backend resources and provision it in the cloud\r\n\"amplify publish\" will build all your local backend and frontend resources (if you have hosting category added) and provision it in the cloud\nIt is important to change the authentication method from API key to Amazon Cognito User Pool to use the Auth resources we created in the last section.\nWe now have some new files generated for us by the Amplify CLI. We are going to start by editing the amplify/backend/api/schema.graphql to look as follows:\ntype Post\r\n@model\r\n@auth(rules: [{ allow: owner }, { allow: private, operations: [read] }]) {\r\nid: ID!\r\ntitle: String!\r\ndescription: String\r\nlikes: [Like] @hasMany\r\ncomments: [Comment] @hasMany\r\n}\r\n\r\ntype Like\r\n@model\r\n@auth(rules: [{ allow: owner }, { allow: private, operations: [read] }]) {\r\nid: ID!\r\npost: Post @belongsTo\r\n}\r\n\r\ntype Comment\r\n@model\r\n@auth(rules: [{ allow: owner }, { allow: private, operations: [read] }]) {\r\nid: ID!\r\npost: Post @belongsTo\r\ncontent: String!\r\n}\nIn this example, we have defined 3 GraphQL Object types for Post, Likes, and Comments. We don\u2019t specify a date field because we get a createdAt and updatedAt field for each type by default.\u00a0\nThe Amplify specific parts of this schema are the directives, which you can notice by the @ prefix. The directives used here are:\n\n@model: This creates a DynamoDB table to back this model. In this case, 3 tables will be created.\n@auth: These are authorization rules for the type. The rules described here are:\n\nThe owner (creator) of a record is allowed to perform all operations on that record\nAn \u201cowner\u201d field will be added to each item in the DB, which contains the username of the logged-in user that created the item\nOnly logged in users (private) can read the records for this type\n\n\n@hasMany: This creates a one-to-many relationship with another type. In our example above, Posts have many comments and likes.\n@belongsTo: This creates a many-to-one relationship with another type. In our example above, comments belong to one post, and likes belong to one post\n\nYou can learn more about directives or authorization rules in the Amplify documentation.\nAfter customizing our schema we run amplify push and select yes. We are then asked a few questions related to graphql:\n? Do you want to generate code for your newly created GraphQL API Yes\r\n? Choose the code generation language target angular\r\n? Enter the file name pattern of graphql queries, mutations and subscriptions src/graphql/**/*.graphql\r\n? Do you want to generate/update all possible GraphQL operations - queries, mutations, and subscriptions Yes\r\n? Enter maximum statement depth [increase from default if your schema is deeply nested] 2\r\n? Enter the file name for the generated code src/app/API.service.ts\nThis will generate a lot of front-end code specific to our schema so thatuse we can easily \u00a0our GraphQL backend.\nConnecting the frontend to the API\nA few files have been generated that are important to cover. First of all, there is the API.service.ts. This file contains all generated types and statements that can be used with the GraphQL API and injected as an Angular Service into Angular components and services.\nSecondly, we also have a directory now called graphql in the root of our project. This file contains generated GraphQL Queries, Mutations and Subscription, and a JSON representation of our GraphQL schema. These files are used as input to generate API.service.ts. We will look at these files later in the article.\nFilling the data source\nWe will start by updating our post.service.ts to save new posts to our Amplify backend instead of in the mockObject. We will make the following changes:\n//Other imports\r\nimport { APIService } from 'src/app/API.service'; // <--- Add this\r\n\r\n@Injectable({\r\n\u00a0 providedIn: 'root'\r\n})\r\nexport class PostService {\r\n\u00a0 //other code\r\n\r\n\u00a0 constructor(private api: APIService) { // <--- Add api to constructor\r\n\u00a0 }\r\n\r\n\u00a0 addPost(title: string, description: string) { // <--- replace current addPost with new code\r\n\u00a0 \u00a0 this.api.CreatePost({\r\n\u00a0 \u00a0 \u00a0 title,\r\n\u00a0 \u00a0 \u00a0 description\r\n\u00a0 \u00a0 });\r\n\u00a0 }\r\n\r\n\u00a0 //other code\r\n}\nHere we import the generated API service and use one of the generated functions, CreatePost, to create a new post item to our backend. If you search for the definition of the CreatePost function in src/app/API.service you can see which parameters it expects. In this case adding a title and description is enough.\u00a0\nNow when we add posts, they will be persisted.\u00a0\nRetrieving data from the backend\nThe next step is to retrieve all our posts from the backend. We will make the following changes to our post.service.ts:\n// other imports\r\nimport { APIService, Post as AmplifyPost } from 'src/app/API.service'; // <--- Add this new import\r\n\r\n@Injectable({\r\n\u00a0 providedIn: 'root'\r\n})\r\nexport class PostService {\r\n\u00a0 private posts: BehaviorSubject<Post[]> = new BehaviorSubject<Post[]>([]); // <--- remove mockposts and its usage completely\r\n\u00a0 private postsData: Post[] = [];\r\n\r\n\u00a0 constructor(private api: APIService, private userService: UserService) {\r\n\u00a0 \u00a0 this.setOnPostCreateSubscription();\r\n\u00a0 }\r\n\r\n\u00a0 // Other code\r\n\u00a0 \r\n\u00a0 //Update this function\r\n\u00a0 getAllPosts(): Observable<Post[]> {\r\n\u00a0 \u00a0 this.api.ListPosts().then(response => {\r\n\u00a0 \u00a0 \u00a0 const responsePosts: Post[] = [];\r\n\u00a0 \u00a0 \u00a0 response.items.forEach(item => {\r\n\u00a0 \u00a0 \u00a0 \u00a0 const post = this.convertToPost(item as AmplifyPost);\r\n\u00a0 \u00a0 \u00a0 \u00a0 responsePosts.push(post);\r\n\u00a0 \u00a0 \u00a0 });\r\n\r\n\u00a0 \u00a0 \u00a0 this.posts.next(responsePosts);\r\n\u00a0 \u00a0 });\r\n\r\n\u00a0 \u00a0 return this.posts.asObservable();\r\n\u00a0 }\r\n\r\n\u00a0 //Add this function\r\n\u00a0 private convertToPost(amplifyPost: AmplifyPost): Post {\r\n\u00a0 \u00a0 const {\r\n\u00a0 \u00a0 \u00a0 id,\r\n\u00a0 \u00a0 \u00a0 title,\r\n\u00a0 \u00a0 \u00a0 description,\r\n\u00a0 \u00a0 \u00a0 owner,\r\n\u00a0 \u00a0 \u00a0 createdAt,\r\n\u00a0 \u00a0 \u00a0 likes,\r\n\u00a0 \u00a0 \u00a0 comments\r\n\u00a0 \u00a0 } = amplifyPost;\r\n\r\n\u00a0 \u00a0 const likesItems = likes ? likes.items : undefined;\r\n\u00a0 \u00a0 const commentsItems = comments ? comments.items : undefined;\r\n\r\n\u00a0 \u00a0 return {\r\n\u00a0 \u00a0 \u00a0 id,\r\n\u00a0 \u00a0 \u00a0 title,\r\n\u00a0 \u00a0 \u00a0 description: description ? description : '',\r\n\u00a0 \u00a0 \u00a0 author: owner ? owner : '',\r\n\u00a0 \u00a0 \u00a0 date: new Date(createdAt),\r\n\u00a0 \u00a0 \u00a0 likes: likesItems ? likesItems.length : 0,\r\n\u00a0 \u00a0 \u00a0 comments: commentsItems ? commentsItems.length : 0\r\n\u00a0 \u00a0 };\r\n\u00a0 }\r\n}\nWe are doing a few things here:\n\nImporting the generated Post interface and giving it an alias since Post already exists and is used for our Post component UI model\nRemoving the mock data completely since this service will now be serving data from the Amplify backend\nUsing the listPosts generated function to get all posts in our backend.\nConverting the posts retrieved from the amplify backend to the Post model we want to use in our UI. A significant difference here is that we want to count the number of likes and comments and return numbers.\u00a0\n\nIf we run our app now, we should now see new data retrieved from the backend. If we add a new post and refresh the page ,we should see the latest post in the list.\nSubscribing to data\nIt would be nice to have the new post show up in the list immediately after it is added. One way to achieve this is to call getAllPosts after the addPosts was successful. There is a better way, though: Amplify GraphQL also allows us to subscribe to certain events.\u00a0\nIn this case, we can subscribe to the event of a post being created by making the following changes to our post.service.ts:\nimport { UserService, UserInfo } from '../shared/user.service'; // Add this import\r\n\r\n@Injectable({\r\n\u00a0 providedIn: 'root'\r\n})\r\nexport class PostService {\r\n\u00a0 private postsData: Post[] = []; // <-- add this\r\n\r\n\u00a0 constructor(private api: APIService, private userService: UserService) {\r\n\u00a0 \u00a0 this.setOnPostCreateSubscription(); // <-- add this\r\n\u00a0 }\r\n\r\n\u00a0 //Add this function\r\n\u00a0 private async setOnPostCreateSubscription() {\r\n\u00a0 \u00a0 const userInfo: UserInfo = await this.userService.getCurrentUserInfo();\r\n\r\n\u00a0 \u00a0 if (userInfo) {\r\n\u00a0 \u00a0 \u00a0 this.api.OnCreatePostListener(userInfo.username).subscribe(response => {\r\n\u00a0 \u00a0 \u00a0 \u00a0 const responseData = response.value.data;\r\n\u00a0 \u00a0 \u00a0 \u00a0 if (responseData && responseData.onCreatePost) {\r\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 const post = this.convertToPost(\r\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 responseData.onCreatePost as AmplifyPost\r\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 );\r\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 this.postsData.push(post);\r\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 this.posts.next(this.postsData);\r\n\u00a0 \u00a0 \u00a0 \u00a0 }\r\n\u00a0 \u00a0 \u00a0 });\r\n\u00a0 \u00a0 }\r\n\u00a0 }\r\n\r\n\u00a0 getAllPosts(): Observable<Post[]> {\r\n\u00a0 \u00a0 this.api.CustomListPosts().then(response => {\r\n\u00a0 \u00a0 \u00a0 const responsePosts: Post[] = [];\r\n\u00a0 \u00a0 \u00a0 response.items.forEach(item => {\r\n\u00a0 \u00a0 \u00a0 \u00a0 const post = this.convertToPost(item as AmplifyPost);\r\n\u00a0 \u00a0 \u00a0 \u00a0 responsePosts.push(post);\r\n\u00a0 \u00a0 \u00a0 });\r\n\r\n\u00a0 \u00a0 \u00a0 this.postsData = responsePosts; // <-- add this\r\n\u00a0 \u00a0 \u00a0 this.posts.next(responsePosts);\r\n\u00a0 \u00a0 });\r\n\u00a0 \u00a0 return this.posts.asObservable();\r\n\u00a0 }\r\n}\nWe made these changes:\n\nImport the UserService we created in the last section. This is needed because of a limitation in Amplify GraphQL subscription (see below).\nCreated an array that holds the state of the latest data retrieved. We need this since the subscription only returns the new post added. We need to add the newly created post to this list and return it to the component.\nWe call the onCreatePostListener with the username of the currently logged in user and we convert the response of this subscription to our Post model object, similarly to how we did this for the query.\n\nAmplify GraphQL limitation\nCurrently it is not possible to have the owner auth rule in your GraphQL schema and have a subscription on an object without supplying the owner username. If you want to be able to have a subscription on all objects of a type, regardless of who created it, you need to remove the owner auth rule in your schema. However, this means that you will no longer get the owner field in all of your items in the database and that any logged in user can also update and delete posts that don\u2019t belong to them.\u00a0\nFor the sake of having a working subscription example, we have created a subscription for the current user, meaning you will only get updated for posts that you post yourself. Hopefully in the future we will be able to have more flexibility regarding subscriptions. Nevertheless, this example should still show how easy it is to setup subscriptions and there are still several use cases where this might be useful.\u00a0\nLikes and comments\nFor the Like and Comment services we followed steps similar to those followed for the Post. You can view all of these steps in the Git commits we will list at the end of this section. However, there are some key points we still want to cover that we encounter when migrating likes and comments to the Amplify backend.\u00a0\nGraphQL query with filters and pagination\nWhen we want to query the likes or comments, we only want to query them if they belong to the Post that we are currently looking at in the frontend. Luckily, the generated functions we use to query our backend come with built-in filtering and pagination functionality.\u00a0\nIf we look at the definition of the ListLikes function in the API.service.ts, we can see the following signature:\nasync ListLikes(\r\n\u00a0 \u00a0 filter?: ModelLikeFilterInput,\r\n\u00a0 \u00a0 limit?: number,\r\n\u00a0 \u00a0 nextToken?: string\r\n\u00a0 ): Promise<ListLikesQuery> {\r\n\r\n //code\r\n\r\n }\nThe ModelLikeFilterInput contains options to filter the results based on several conditions. Click through them to see all of the possibilities. We can also see a limit and a nextToken parameters which are used for pagination. Since we did not build pagination in this application yet, we will refer to the pagination documentation.\nTo query only the likes that belong to a certain post, we use the following code:\ngetLikesForPostId(postId: string) {\r\n\u00a0 \u00a0 this.api.ListLikes({ postLikesId: { eq: postId } }).then(response => {\r\n\u00a0 \u00a0 //other code\nCustom GraphQL Queries, Mutations and Subscriptions\nIf you follow the patterns we have shown up to now and have posts, likes and comments using the Amplify backend, you will notice that all posts still show 0 likes and comments on the post overview. This is a bug, and has to do with the values that are retrieved in the generated GraphQL query used in the listPosts function.\u00a0\nIf we check the definition of the query in the src/graphql/queries.graphl we will see the following:\nquery ListPosts(\r\n\u00a0 $filter: ModelPostFilterInput\r\n\u00a0 $limit: Int\r\n\u00a0 $nextToken: String\r\n) {\r\n\u00a0 listPosts(filter: $filter, limit: $limit, nextToken: $nextToken) {\r\n\u00a0 \u00a0 items {\r\n\u00a0 \u00a0 \u00a0 id\r\n\u00a0 \u00a0 \u00a0 title\r\n\u00a0 \u00a0 \u00a0 description\r\n\u00a0 \u00a0 \u00a0 likes {\r\n\u00a0 \u00a0 \u00a0 \u00a0 nextToken\r\n\u00a0 \u00a0 \u00a0 }\r\n\u00a0 \u00a0 \u00a0 comments {\r\n\u00a0 \u00a0 \u00a0 \u00a0 nextToken\r\n\u00a0 \u00a0 \u00a0 }\r\n\u00a0 \u00a0 \u00a0 createdAt\r\n\u00a0 \u00a0 \u00a0 updatedAt\r\n\u00a0 \u00a0 \u00a0 owner\r\n\u00a0 \u00a0 }\r\n\u00a0 \u00a0 nextToken\r\n\u00a0 }\r\n}\nWe can see that for the likes and comments, only the nextToken field is being retrieved. The items field is not being retrieved at all, which is why our frontend is defaulting to 0. Thankfully, we can define our own queries.\nWe can create a new file called src/app/custom-queries.graphql with the following content:\nquery CustomListPosts(\r\n\u00a0 $filter: ModelPostFilterInput\r\n\u00a0 $limit: Int\r\n\u00a0 $nextToken: String\r\n) {\r\n\u00a0 listPosts(filter: $filter, limit: $limit, nextToken: $nextToken) {\r\n\u00a0 \u00a0 items {\r\n\u00a0 \u00a0 \u00a0 id\r\n\u00a0 \u00a0 \u00a0 title\r\n\u00a0 \u00a0 \u00a0 description\r\n\u00a0 \u00a0 \u00a0 likes {\r\n\u00a0 \u00a0 \u00a0 \u00a0 items {\r\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 id\r\n\u00a0 \u00a0 \u00a0 \u00a0 }\r\n\u00a0 \u00a0 \u00a0 }\r\n\u00a0 \u00a0 \u00a0 comments {\r\n\u00a0 \u00a0 \u00a0 \u00a0 items {\r\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 id\r\n\u00a0 \u00a0 \u00a0 \u00a0 }\r\n\u00a0 \u00a0 \u00a0 }\r\n\u00a0 \u00a0 \u00a0 createdAt\r\n\u00a0 \u00a0 \u00a0 updatedAt\r\n\u00a0 \u00a0 \u00a0 owner\r\n\u00a0 \u00a0 }\r\n\u00a0 \u00a0 nextToken\r\n\u00a0 }\r\n}\nWe now query the items for likes and comments and only retrieve the ids, since we only want to count them. Once we add this file, we can run the following in the terminal:\namplify codegen\nThis will generate new code that you can use in your frontend without having to run amplify push. Now we can update our post.service.ts to make use of this new query:\ngetAllPosts(): Observable<Post[]> {\r\n\u00a0 \u00a0 this.api.CustomListPosts().then(response => { // <-- updated here\r\n\u00a0 \u00a0 \u00a0 //code\nIf we run the application now we should see the correct counters for likes and comments.\nCustom GraphQL Resolvers\nIn our current example we retrieve likes and comments and we count them in the frontend to show the counters. We could also update our GraphQL schema for Post to include a likesCount and commentsCount fields and define custom logic for these fields.\u00a0\nThis would let the backend calculate these numbers for us and return them to the frontend. A demo of this would be too long for this article, so we will refer to the documentation for AWS Lambda resolver configuration, which explains how to set this up.\nWe\u2019ve introduced quite some changes:\n\nWe added an API endpoint.\nWe updated the GraphQL schema.\nWe pushed the changes to the backend.\nWe added Posts to the GraphQL backend.\nWe retrieved Posts from the GraphQL backend.\nWe added subscriptions for Posts.\nWe added Likes \u00a0to the GraphQL backend.\nWe retrieved Likes from the GraphQL backend.\nWe \u00a0fixed Posts subscriptions.\nWe created a custom query to list Posts.\nWe added a subscription for likes.\nWe added Comments to the GraphQL backend.\nWe retrieve Comments from the GraphQL backend.\nWe added subscriptions for Comments.\n\nHosting our application\nNow that we have our application working locally, it is time to host it on AWS so that it is available online. We are going to do this by adding the Amplify hosting category. Before we do that, we need to do these steps:\n\nLog in to the AWS console with your default browser.\nPush all your code to Git.\nUpdate the initial budget in angular.json to 2mb.\n\nWe will run the following command to add hosting:\nEvertsons-MBP:theamplifyapp evertsoncroes$ amplify add hosting\r\n? Select the plugin module to execute Hosting with Amplify Console (Managed hosting with custom domains, Continuous deployment)\r\n? Choose a type Continuous deployment (Git-based deployments)\r\n? Continuous deployment is configured in the Amplify Console. Please hit enter once you connect your repository\nThis will open the AWS console that will enable us to connect our repository to the CI/CD pipeline:\n\nClick on the Hosting environments tab and select your Git repository:\n\nClick on connect branch. Once you are authenticated with your Git repository, select the repository and branch where the latest version of your code exists and click on Next.\nOn the next page it is important that you select an environment to use for this deployment. In this case we have only created a dev environment, so we will use that:\n\nIn case you don\u2019t have an existing service role for Amplify projects, click on Create new role to generate one and click on the refresh button to see it appear as an option.\nIf you scroll to the bottom you will see the default build settings. For now these will work, however if in the future you want to extend the build to include more phases or steps you can always change this.\nWhen we click on next we will get a summary of the options we selected. Review them and click on Save and deploy.\nFrom this point on, everytime you push to the develop branch a build will start and the live application will get updated. This is what it looks like when it is building:\n\nYou can click on any of the phases, such as provision to see the detailed logging of what is happening:\n\nWhen it is done building and deploying, we can see that all steps have passed and that there is a link to test out our app:\n\nConfiguring redirects for our SPA\nIf you click on the link you should see your application where you can log in and should see the main page. However, if you click on API you will get an access denied error. For single page applications we need to add an extra setting in the Amplify console in our project related to rewrites and redirects:\n\nThe settings are:\n\nSource address: </^[^.]+$|\\.(?!(css|gif|ico|jpg|js|png|txt|svg|woff|woff2|ttf|map|json)$)([^.]+$)/>.\nTarget address: /index.html.\nType: 200 (Rewrite)\nCountry: (leave empty).\n\nFor details, refer to the Amplify documentation for single page application redirects.\n(Extra) Linking a domain name to your deployment\nOne extra step you could take is to link a domain name to one of your deployments. In this case we are going to create a new environment for our project, link it to the master branch and then link the domain name \u201ctheamplifyapp.com\u201d to that deployment.\nTo start off, I will run the following command in the terminal:\nEvertsons-MBP:theamplifyapp evertsoncroes$ amplify init\r\nNote: It is recommended to run this command from the root of your app directory\r\n? Do you want to use an existing environment? No\r\n? Enter a name for the environment prod\r\n? Choose your default editor: Visual Studio Code\r\nUsing default provider\u00a0 awscloudformation\r\n? Select the authentication method you want to use: AWS profile\r\n\r\nFor more information on AWS Profiles, see:\r\nhttps://docs.aws.amazon.com/cli/latest/userguide/cli-configure-profiles.html\r\n\r\n? Please choose the profile you want to use default\r\nAdding backend environment prod to AWS Amplify app: d1ekyrd95b627y\r\n\u2834 Initializing project in the cloud...\nOnce that is done we can run amplify push and follow the steps we made for the dev environment.\nAfter that, we can go into the Amplify console, click on General, scroll to the bottom and click on connect branch. There we will select the master branch and the prod environment:\n\nThen we click on next\u201d and save and deploy and wait for the build to be successful.\nAfter that is done, we can go to Route53 in the AWS console and register a domain name. Note that this will cost money! In this case I have registered the domain theamplifyapp.com.\nBack in the Amplify console, click on the Domain management tab and on Add domain. We should find our registered domain in the available domains and select it. Then we click on Configure domain and set it to the master branch build:\nOnce we click on Save the process should get started. When it is complete, we can visit our application on our brand new domain!\nWe made two major changes:\n\nWe\u2019ve added Amplify hosting.\nWe\u2019ve updated our Angular budget to 2Mb.Updated Angular budget to 2mb\n\nUp next\u2026\nWith just a few Amplify commands and code changes we have:\n\nCreated a cloud-native application\u2026\nProtected with authentication\u2026\u00a0\nBacked by a GraphQL backend\u2026\nWith a fully configured CI/CD pipeline\u2026\nHosted on the AWS cloud\u2026\nConnected through a registered domain name\n\nFor each of the categories taken there are still plenty of customization options to explore. However, hopefully it has become clear how powerful Amplify can be.\u00a0\nIn the next installment of this series we will continue working on our application and we will add the Amplify Storage category. We will use this to create an image library where we can upload images.\nLearn from our experts:30 Jan 2021-Evertson CroesTraining: AWS AmplifyHave you always wanted to create a Cloud native app but you don\u2019t know where to begin? The Cloud can be an intimidating concept. In this training you will get a good introduction to the Cloud and you are going...\n", "tags": ["amplify", "aws", "cloud", "infrastructure as code"], "categories": ["Blog", "Cloud"]}
{"post_id": 31593, "title": "The Evolution of AWS from a Cloud-Native Development Perspective: Serverless, Event-Driven, Developer-Friendly, Sustainable", "url": "https://www.luminis.eu/blog-en/the-evolution-of-aws-from-a-cloud-native-development-perspective-serverless-event-driven-developer-friendly-sustainable/", "updated_at": "2023-04-20T13:42:59", "body": "Recent AWS announcements feature fewer service additions and instead underline the efforts the cloud giant is undertaking to increase the strength of its massive portfolio of building blocks. AWS CEO Adam Selipsky has mapped out a course that intensifies AWS\u2019s focus on serverless technologies, event-driven architectures, an improved developer experience, and sustainability.\n\nIn this blog post, we outline the direction cloud-native engineering is heading in, how you can learn to leverage AWS\u2019s portfolio of managed services and primitives to your advantage, and how cloud technology is reshaping the field of software engineering.\nIf you prefer watching a video, we got you covered! We recently broadcasted a cloud-native development focussed AWS re:Invent re:Cap featuring most of this article\u2019s themes. You can find the recording at the end of this post.\nWhat AWS is telling us\nWith so many services in its portfolio already, this year\u2019s AWS re:Invent didn\u2019t hold any massively surprising announcements. With the yearly conference reaching its 10-year milestone \u2014 and AWS being around for 15 years \u2014 the AWS platform is maturing while at the same time evolving to meet market demands.\nAWS leadership\u2019s re:Invent keynotes\nFreshly at the helm of AWS, Adam Selipsky held his first re:Invent Keynote. Selipsky\u2019s primary message: AWS is shifting from merely offering infrastructure primitives to embracing the idea of being a so-called platform of platforms. Therefore, AWS\u2019s offerings will increasingly become an interesting piece of organizational value chains.\nWhile Amazon CTO Werner Vogels took the stage after an introduction referencing Fear and Loathing in Las Vegas, his keynote wasn\u2019t as spectacular. Dr. Vogels took the time to make a case for Well-Architected applications, which we interpret as a good signal for builders. Other highlights were the release of Construct Hub and the CDK Patterns library.\n\nPrimitives, not frameworks\nAn important theme these last few months was the expansion and refinement of AWS as a cloud-native platform. You can view AWS as a data center containing compute resources and seemingly limitless storage at the other end of the internet, or maybe as a big bunch of infrastructural primitives. The way we look at the platform is by grouping its services into three categories:\n\nInfrastructural primitives: storage, networking, and compute (a.k.a. Infrastructure as a Service, IaaS).\nMiddleware-oriented services: web servers, container orchestrators, databases, message queues (a.k.a. Platform as a Service, PaaS).\nServerless: take what you need, pay-as-you-go.\n\nBuilding something on a platform containing 200+ services might seem a bit overwhelming. Therefore, from the perspective of cloud-native development, we use this rule of thumb: start building from the most modern collection of services first and, if needed, complement the solution using more primitive building blocks.\nFrequently, you will end up with a serverless solution that is rapidly and almost infinitely scalable and highly cost-effective. However, sometimes there are missing pieces of the puzzle, and you can\u2019t complete it with serverless services. We can then turn to platform primitives or infrastructure services like containers, RDS, virtual machines, or even complete SaaS solutions. Don\u2019t forget to update your solutions from time to time, as the platform never stops evolving.\nReduce risk, improve performance, grow revenue, increase efficiency\nEffectively adopting the cloud means making the cloud work for your money. It\u2019s not just a virtualized data center, or at least: that\u2019s not the mindset that enables you to accelerate your organization towards its goals. Cloud-native development requires understanding cloud-native best practices in terms of performance, resiliency, cost optimization, and security. Luckily, AWS keeps investing in adoption frameworks, its Well-Architected Framework, and certification and training programs.\n\nThe ongoing evolution of serverless: commoditize all the things\nYou can immediately tell the serverless category from above is something special. Its services allow you to focus on functional value without worrying about what is needed to run it. You can compare a serverless service to a commodity, like electricity: you use it anytime you need it, without much second thought. For a while, serverless was synonymous with functions (FaaS) or Lambda, but its reach rapidly extends beyond compute to data stores and application integration.\nServerless technology keeps gaining traction, as we can see in its continuous evolution. Let\u2019s look at AWS\u2019s current serverless portfolio and see how it evolves.\nThree categories of building blocks\nRoughly speaking, we can divide AWS\u2019s serverless building blocks into three categories:\n\nCompute: running code without provisioning servers (Lambda, Fargate).\nData Storage: storing objects, documents, relational and other data (Simple Storage Service (S3), DynamoDB, Relational Database Service Proxy, Aurora Serverless).\n(Application) integration: EventBridge, Step Functions, Simple Queue Service (SQS), Simple Notification Service (SNS), API Gateway, AppSync.\n\nCombining these building blocks, you can quickly create valuable solutions at scale.\n\nServerless expansion areas\nWhile SaaS is powerful and often offers excellent value for money, it has significant constraints in terms of flexibility. To win in the marketplace, we need more flexibility for our differentiating technology. These are the lines along which serverless is evolving. Its journey started at compute, but the serverless philosophy quickly moved into other AWS categories.\nBig data and streaming\nAmazon MSK will get a serverless counterpart, which is great news for organizations running Kafka workloads. The same goes for AWS\u2019s big data platform Amazon EMR and its data warehouse offering Amazon Redshift. Amazon Kinesis is also evolving its data streaming service, exemplified by the new Data Streams On-Demand feature. Last but certainly not least: AWS Data Exchange for APIs makes it a breeze to use third-party data while leveraging AWS-native authentication and governance.\nApplication integration\nApart from getting extra ephemeral storage, AWS Lambda now supports partial batch responses for SQS and event source filtering for Kinesis, DynamoDB, and SQS. EventBridge added Amazon S3 Notifications. On the surface, that might sound like a small change. But as Matt Coulter puts it: code is a liability, not an asset. Anything you can push to the cloud vendor gains you some security and an increased ability to focus on value. You can now control Amazon Athena workflows using AWS Step Functions in that same vein.\nAre applications, infrastructure, and integration separable?\nNow that serverless is moving beyond compute into the data and integration layers, we might ask ourselves: are applications, infrastructure, and integration separable? Is programming still mostly about functions and logic, and where does it bleed into the domain of operations? The advent of serverless is pushing boundaries everywhere and blurring previously clear and stable lines.\nThat\u2019s excellent news, in our opinion: the more control and flexibility our teams have, the faster we can innovate. Gregor Hohpe, an Enterprise Strategist at AWS, says: \u201cI firmly believe that we are just starting to realize the true potential of cloud automation. Modern cloud automation isn\u2019t just about reducing toil. (\u2026) it can help us blur the line between application and automation code.\u201d We agree wholeheartedly.\n\nThe future (of cloud) is event-driven\nAnother direction cloud technology developments are pointing towards is that of event-driven architectures. Not only is the cloud itself highly event-driven, but applications built on cloud platforms also tend to be fitting most naturally when using the power of events. Why is that?\nAgility in architecture\nMarket forces are driving organizations to become more agile. As Jeff Bezos puts it: \u201cThe only sustainable advantage you can have over others is agility, that\u2019s it.\u201d Organizations must learn to leverage technology to respond to and drive change to fulfill this business requirement. Event-driven architectures present the necessary characteristics:\n\nLoosely coupled: heterogeneous services and share information effectively while hiding implementation details.\nScalable: multiple types of consumers can work in parallel, massively if needed.\nIndependent: evolution, scaling, and failing can happen in isolation, especially when using buffers like event routers.\nAuditable: if events are routed and stored centrally, we can quickly implement auditing, policy enforcement, and information access control.\nFrugal: if we push changes when available, we don\u2019t need to waste resources continuously polling for changes.\n\nEvent-driven systems enable us to focus on system behavior and temporality within a business domain, freeing us from the chains of rigid structures incapable of dealing with changing needs and requirements. There is a trade-off (of course): agile systems are complex and thus force us to learn to manage \u2014 seeming, but not actual \u2014 chaos.\n\nAWS building blocks for event-driven architectures\nNow, how is AWS helping us compose these systems? Whether you are building event-sourced systems, leverage CQRS, or are just notifying components, AWS has the building blocks you need:\n\nProducers: most AWS primitives, especially serverless ones, produce events. For example, we can feed Lambda with events from API Gateway, CloudWatch, DynamoDB, S3, Kinesis, and many more services. Besides that, we can use custom apps, SaaS solutions, and microservices as event producers.\nConsumers: notable consumers are AWS Lambda, Amazon SQS, Amazon Kinesis Data Firehose, and Amazon SNS. We can also use APIs from SaaS offerings and custom apps as event consumers.\nRouters: we can fulfill most integration needs using Amazon EventBridge and Amazon SNS.\nWell-Architected: the AWS Well-Achitected Framework describes best practices, which we can assess and monitor using the AWS Well-Architected Tool.\n\nCombining these AWS services and using them to integrate SaaS and self-built products gives us the leverage we miss when only using off-the-shelf products.\nHow are event-driven cloud architectures evolving?\nThe answer to this question is short and straightforward: along the same lines as serverless. Last December, AWS mostly announced incremental improvements. But they quickly add up, accumulating in an ever-more powerful platform to build and evolve our solutions on. And the announcements don\u2019t stop when re:Invent is done: AWS is working to improve its event-driven primitives year-round.\nAmazon Eventbridge is an excellent example of this continuous investment. It was introduced in mid-2019 and has gained a lot of features since then: a schema registry, archiving and replaying of events, and a long list of event destinations. Its evolving event pattern matching, filtering, and routing options embody the \u2018code is liability\u2019 philosophy, enabling us to focus on value delivery. Recently, S3 Event Notifications were added to EventBridge\u2019s features, giving us more direct, reliable, and developer-friendly options when responding to S3 object changes. For the web-scale organizations among us, AWS introduced cross-region event routing.\nWe mentioned Lambda\u2019s new message filtering features above but want to highlight them again since they underline AWS\u2019s continued investment in this area. Lastly, AWS IoT TwinMaker deserves some attention: it utilizes events as data sources and enables developers to create digital twins of real-world systems, which is nothing less than fantastic.\n\nImproving the developer experience\nAWS provides us with many building blocks, but using them effectively is not straightforward. Luckily for us, AWS seems to understand this problem and is steadily improving in that area.\nProgrammable everything: APIs, SDKs, and the AWS CDK\nIn 2002, Jeff Bezos\u2019s so-called API Mandate forced all Amazon teams to expose their data and functionality through service interfaces. Amazon has built AWS around the same principles: every service is programmatically controllable, from starting a virtual machine to accessing a satellite. While this is an essential property of an effective cloud platform, it is not necessarily developer-friendly.\nBy now, AWS has incrementally and significantly improved in this space. Besides using their APIs, we can control services and infrastructure with a unified Command Line Interface (CLI), Software Development Kits (SDK), CloudFormation, and, since July 2019, the AWS Cloud Development Kit (CDK).\nThe CDK had a massive impact on developer productivity and satisfaction. While teams could already access and control services and infrastructure using the AWS SDK and their favorite programming language, infrastructure was primarily defined using incredible amounts of mostly punctuation marks and whitespace, also known as YAML or JSON. CDK \u2014 initially only flavored TypeScript and Python \u2014 finally gave developers an AWS-native means to define Infrastructure as actual Code. Since its introduction, AWS has added support for more languages, like Java, C#, and Go.\nMetrics, toolsets, and communities\nThe lines between infrastructure and logical code have thus been blurred, creating more opportunities to increase business agility. And AWS provides more tools to increase development productivity, like metrics and purpose-built toolsets. New developments in this space are:\n\nCommunity support platforms like the newly announced re:Post and Construct Hub.\nMonitoring improvements like CloudWatch Real-User Monitoring and Metrics Insights.\nA Secrets Detector addition to the software engineer\u2019s best non-human programming buddy, Amazon CodeGuru.\n\n\nReducing complexity\nAnother way to increase (business) agility is by reducing complexity. AWS seems very aware of the complexity we add when developing cloud-native solutions. It has released several offerings and improvements to its platform over the last period in reaction to this.\nLess handcrafted code\nWe can reduce the amount of hand-crafted code we deploy in several areas (once more: code is a liability!). Business analysts can kickstart machine learning workloads using Amazon SageMaker Canvas, while others can visually create full-stack apps using Amplify Studio. More incremental improvements are enhanced dead-letter queue management for SQS and the Lambda and S3 enhancements mentioned earlier.\nFeature flags and dark launches\nReducing operational complexity is a way to increase productivity. Modern organizations have learned to leverage feature flags and dark launches to test changes without introducing much operational overhead. Our colleague Nico Krijnen took the time to write down how Amazon CloudWatch Evidently and AWS AppConfig Feature Flags can help us in this regard.\nMigration automation\nLastly, a powerful \u2014 but often not so straightforward \u2014 way to reduce complexity is to migrate existing workloads from the old data center to the cloud. AWS has been investing a lot of effort in this space and continues to do so, as evidenced by the recent releases of AWS Migration Hub Refactor Spaces, AWS Mainframe Modernisation, and AWS MicroService Extractor for .NET.\n\nSustainability of the cloud, sustainability in the cloud\nSoftware engineering has more dimensions than speed and complexity. With the advent of DevOps, FinOps, and the increased awareness of IT infrastructure\u2019s impact on our environment, teams are increasingly responsible for more than just developing and deploying code.\nAdrian Cockcroft, Amazon\u2019s newly appointed VP of Sustainability Architecture, recently headed a very insightful talk on architecting for sustainability. Amazon has committed to net-zero carbon by 2040, and AWS aims to use 100% renewable energy by 2025. AWS needs their customer\u2019s help to decrease their footprint, so AWS has introduced several sustainability tools and improvements to their platform.\nThe Customer Carbon Footprint Tool was released earlier this month, enabling AWS users to calculate the carbon emissions their workloads produce now and in the future. Reducing them is the next logical step, which is why AWS added a new Sustainability Pillar to their Well-Architected Framework. More concretely, teams optimize resource usage by choosing several new CPU instances for EC2 or Lambda or by analyzing the enhanced AWS Compute Optimizer infrastructure metrics and acting accordingly.\n\nSurvival of the fittest\nAWS\u2019s cloud platform is becoming more mature, stable, developer-friendly, and sustainable while at the same time evolving rapidly to meet emerging business needs. By closely following the needs of its users and experimenting and growing with them, AWS keeps delivering the platform of the future.\nFor us cloud-native developers, that\u2019s excellent news. Increasing business agility by creating and evolving event-driven, serverless, sustainable systems that can turn on a dime in response to customer needs: it\u2019s what we need now and going forward.\n\nLearn from our experts:25 Jan 2021-Training: AWS overviewIn this training we will take you through the main services of AWS (including Lambda, EC2, S3, Elastic Beanstalk). Not only do we explain what these services mean, but also in short demos we show what possible applications are and...\n", "tags": ["aws", "cloud", "evolution", "future"], "categories": ["Blog", "Cloud"]}
{"post_id": 31377, "title": "Production-Ready CDK \u2013 CDK Pipelines", "url": "https://www.luminis.eu/blog-en/production-ready-cdk-cdk-pipelines/", "updated_at": "2023-01-27T12:55:14", "body": "We initiated our AWS CDK project in the previous chapter and focused on the project structure. Now, we can leverage CI/CD to speed up the technical value stream. Besides, as the project gets bigger, it becomes more challenging to automate; therefore, why not do it initially?\n\nThe most powerful feature of Cloud Development Kits is abstracting complex cloud applications and, as a result, making the cloud more straightforward. AWS CDK does this also for CI/CD pipelines by offering a module called CDK Pipelines.\nCDK Pipelines save us from writing a lot of code, configuration, and wiring. I have used and tried many CI/CD tools, and CDK Pipelines way is one of the most painless ways of implementing CI/CD. For a more detailed introduction, you can check the documentation. We won\u2019t recap it, but we will focus on real-life scenarios in production-like environments.\nIf you tried CDK Pipelines in the past and did it with the old ways, you might disagree with this statement. I should say: I agree with you. And to clarify, there was another construct that was not the most intuitive in the past. But that one is deprecated. So from now on, we only use this CodePipeline construct from the CDK Pipelines library, which I think works as it should be. The code is compact and opinionated in a beautiful way.\nLet\u2019s start by taking a step back and thinking about the positioning before jumping into the code.\n\u00a0\nDon\u2019t use \u2018cdk deploy\u2019 in your pipelines\n\u201ccdk deploy command is convenient, I always use it for my local CDK code, and I can also use it at my CI/CD pipelines. First, I use the CI/CD tool I want and do the CI part, then just deploy with a single command, easy peasy.\u201d\u200a\u2014\u200aprobably someone who doesn\u2019t care about CI/CD\nWell, you can, but should you?\nI see this in many CI/CD pipelines used for CDK projects, from simple PoCs to production environments in enterprises. cdk deploy command is quick and straightforward, but the intended purpose is fast development, not CI/CD pipelines. Why? Because as your application gets more extensive, you will have more CI/CD steps, more CloudFormation stacks, and cross-account or multi-region deployments. It might seem easy, but it is the dirty way in the long run.\nSecond, when using cdk deploy in a CI/CD pipeline, you must give deployment-related IAM permissions. What happens if your CI/CD tool is compromised? They will have access to your AWS account to deploy stacks or, maybe even if you don\u2019t handle permissions right, destroy existing stacks. It sounds improbable, but it is not the well-architected way. We should be reducing permissions continuously.\nI am not arguing that we shouldn\u2019t be using the command at all, only saying it has a specific purpose which is not secure or complex pipeline scenarios.\n\u00a0\nRestricting to the\u00a0Minimum\nOkay, cdk deploy is out of the equation. So what is the right way?\nWe need to give the least privileges for controlling AWS resources from the CI/CD tooling. One way to do that is by providing only an S3 file upload permission to the IAM role used by the CI/CD tool and creating a deployment pipeline on AWS for the deployment step. We can implement more sophisticated controls in this way, like adding more checks or having manual approvals at different stages.\nFurthermore, since only the S3 file upload action can be compromised, it becomes harder to deploy/destroy resources.\nWe can achieve this in two ways:\n\nUse Git to upload artifacts to S3, then do CI and CD on AWS using CDK Pipelines.\nBuild artifacts with a CI tool, then upload artifacts to S3, finally do CD part on AWS using CDK Pipelines.\n\nLet\u2019s see what the first one looks like:\nWay 1: CDK Pipeline for both Continuous Integration and Continuous Deployment\n\u00a0\nIf you are not starting your software development from scratch for yourself or your company, you should already have at least one CI/CD tooling in use. As a result, you will have CI steps already figured out and implemented before. Good news and this takes us to the second way. We can use the CI tooling and still use CDK Pipelines for CD. This way is the way I use most at my projects:\nWay 2: CDK Pipeline for only Continuous Deployment\n\u00a0\nIntegrating Github and\u00a0AWS\nWe discussed keeping things at a minimum with S3 upload permission. For simplicity, we will skip the CI and go similar to the first way. We will push changes from Git to AWS directly and then deploy them as CloudFormation Stacks.\nLuckily, we can simplify more and skip having an S3 Bucket part. Since we use Github, we can utilize the Github-AWS integration, namely CodeStar Connection. In this way, we use only CodeStar permission to deploy stacks (instead of S3 upload permission). It looks like this at the end:\nWay 3: Git-CDK Pipeline integration using CodeStar Connections\nYou can do it in a minute by using the API for it. Or even easier, you can go to one of the Developer Tooling Services of AWS like CodePipeline, click Settings, click Connections, and finally click Create Connection. Follow the steps and give the permissions you need.\n\n\u00a0\nVoil\u00e0, ready for the CDK Pipeline!\n\u00a0\nAdding CDK Pipeline to the\u00a0Project\nIf the stack from the previous article still exists, you should start over by destroying it using cdk destroy or npx projen destroy as I explained before.\nLet\u2019s start by separating the Lambda Stack from src/main.ts. We create a new file with the path src/lambda-stack.ts:\nimport { Stack, StackProps } from 'aws-cdk-lib';\r\nimport * as lambda from 'aws-cdk-lib/aws-lambda';\r\nimport { Construct } from 'constructs';\r\n\r\n\r\n// example cdk app stack\r\nexport class LambdaStack extends Stack {\r\n  constructor(scope: Construct, id: string, props?: StackProps) {\r\n    super(scope, id, props);\r\n\r\n    new lambda.Function(this, 'ExampleFunction', {\r\n      functionName: 'example-lambda',\r\n      code: lambda.Code.fromAsset('lambda'),\r\n      handler: 'hello.handler',\r\n      runtime: lambda.Runtime.NODEJS_14_X,\r\n    });\r\n  }\r\n}\r\n\r\n\nThen implement the pipeline at a new file with the path src/cdk-pipeline-stack.ts.\nimport { Stack, StackProps, Stage } from 'aws-cdk-lib';\r\nimport { CodePipeline, CodePipelineSource, ShellStep } from 'aws-cdk-lib/pipelines';\r\nimport { Construct } from 'constructs';\r\nimport { LambdaStack } from './lambda-stack';\r\n\r\n// 3a. We define a Lambda Stage that deploys the Lambda Stack. \r\nexport class LambdaStage extends Stage {\r\n  constructor(scope: Construct, id: string) {\r\n    super(scope, id);\r\n    new LambdaStack(this, 'LambdaStack');\r\n  }\r\n}\r\n\r\nexport class CdkPipelineStack extends Stack {\r\n  constructor(scope: Construct, id: string, props?: StackProps) {\r\n    super(scope, id, props);\r\n\r\n    // 1. We import the CodeStar Connection for Github-CDK Pipeline integration. Therefore, \r\n    // you only need to provide the ARN of the Connection.\r\n    const codePipelineSource = CodePipelineSource.connection('cagingulsen/prod-ready-cdk','main', { \r\n      connectionArn: 'arn:aws:codestar-connections:eu-west-1:YOUR_ACCOUNTI_D:connection/YOUR_CONNECTION_ID'\r\n      },\r\n    );\r\n\r\n    // 2. We define the CDK Pipeline using the source from the first step and \r\n    // use three commands for the synth step. We install dependencies from the yarn.lock file \r\n    // with yarn install --frozen-lockfile command to have deterministic, fast, and repeatable builds. \r\n    // The following two lines, we already know.\r\n    const cdkPipeline = new CodePipeline(this, 'CdkPipeline', {\r\n      pipelineName: 'lambda-stack-cdk-pipeline',\r\n      synth: new ShellStep('Synth', {\r\n        input: codePipelineSource,\r\n        commands: [\r\n          'yarn install --frozen-lockfile',\r\n          'npx projen build',\r\n          'npx projen synth',\r\n        ],\r\n      }),\r\n    });\r\n\r\n    // 3b. Then we add this to the CDK Pipeline as a pipeline stage.\r\n    cdkPipeline.addStage(new LambdaStage(this, 'dev'));\r\n  }\r\n}\r\n\nHere we see three things happening; please check the comments in the code above.\nThen, of course, we also need to change the src/main.ts, because we moved the Lambda Stack to a separate file, and the starting stack of the CDK App is from now on the CDK Pipeline Stack.\nimport { App } from 'aws-cdk-lib';\r\nimport { CdkPipelineStack } from './cdk-pipeline-stack';\r\n\r\n// for development, use account/region from cdk cli\r\nconst devEnv = {\r\n  account: process.env.CDK_DEFAULT_ACCOUNT,\r\n  region: process.env.CDK_DEFAULT_REGION,\r\n};\r\n\r\nconst app = new App();\r\n\r\nnew CdkPipelineStack(app, 'CdkPipelineStack', { env: devEnv });\r\n\r\napp.synth();\r\n\r\n\nAnd finally, we need to update the only test by renaming main.test.ts to lambda-stack.test.ts without changing the test. But again, we are testing if our Lambda Stack has exactly one Lambda Function.\nimport * as cdk from 'aws-cdk-lib';\r\nimport { Template } from 'aws-cdk-lib/assertions';\r\nimport { LambdaStack } from '../src/lambda-stack';\r\n\r\ntest('Lambda created', () => {\r\n  const app = new cdk.App();\r\n  const stack = new LambdaStack(app, 'LambdaStack');\r\n  const template = Template.fromStack(stack);\r\n\r\n  template.resourceCountIs('AWS::Lambda::Function', 1);\r\n});\r\n\r\n\nWe need to run cdk deploy or npx projen deploy only once to deploy our stacks. Then, it will deploy the CDK Pipeline, and we can see the pipeline at the CodePipeline service. From now on, for every commit you have on the main branch, the CDK pipeline will pick it up. No more deploy commands. We only push to the main branch to deploy.\nNeat, isn\u2019t it?\n\u00a0\nCDK Pipeline deployed and working\nHere is the code with the CDK Pipeline.\n\u00a0\nOther Cool\u00a0Features\nAs you saw, we only used the most basic way to use CDK Pipelines. We can always configure it more by:\n\nAdding more stacks. We could have a different stack like API Gateway Stack and deploy it in the same pipeline. Or use Lambda Stack again but deploy another version of it with a different configuration.\n\ncdkPipeline.addStage(new LambdaStage(this, 'dev'));\r\ncdkPipeline.addStage(new APIGatewayStage(this, 'dev'));\r\n\nor\ncdkPipeline.addStage(new APIGatewayStage(this, 'dev'));\r\ncdkPipeline.addStage(new APIGatewayStage(this, 'acceptance'));\r\n\n\u00a0\n\nDeploying stacks to multiple regions or accounts:\n\nexport class LambdaStage extends Stage {\r\n  constructor(scope: Construct, id: string, appRegion: string) {\r\n    super(scope, id);\r\n    new LambdaStack(this, 'LambdaStack', {\r\n      env: {\r\n        account: process.env.CDK_DEFAULT_ACCOUNT,\r\n        region: appRegion,\r\n      },\r\n    });\r\n  }}\r\ncdkPipeline.addStage(new LambdaStage(this, 'dev1', 'eu-west-1'));\r\ncdkPipeline.addStage(new LambdaStage(this, 'dev2', 'us-east-1'));\r\n\n\u00a0\n\nAdding other types of stages, like a ShellStep or CodeBuildStep:\n\ndeclare const cdkPipeline: pipelines.CodePipeline; \r\nconst preprod = new APIGatewayStage(this, 'PreProd');\r\ncdkPipeline.addStage(preprod, {   \r\n  post: [     \r\n    new pipelines.ShellStep('Validate Endpoint', {       \r\n      commands: ['curl -Ssf https://my.webservice.com/'],     \r\n    }),\r\n   ],\r\n });\r\n\n\u00a0\n\nRunning pipeline stages in parallel using Waves.\n\ndeclare const cdkPipeline: pipelines.CodePipeline;\r\n  \r\nconst wave = cdkPipeline.addWave('MyWave'); \r\nwave.addStage(new APIGatewayStage(this, 'Stage1')); \r\nwave.addStage(new APIGatewayStage(this, 'Stage2'));\r\n\n\u00a0\n\nAdding manual approvals between pipeline stages:\n\ndeclare const cdkPipeline: pipelines.CodePipeline; \r\nconst preprod = new APIGatewayStage(this, 'PreProd'); \r\nconst prod = new APIGatewayStage(this, 'Prod');\r\ncdkPipeline.addStage(preprod, {   \r\n  post: [     \r\n    new pipelines.ShellStep('Validate Endpoint', {       \r\n      commands: ['curl -Ssf https://my.webservice.com/'],     \r\n    }),\r\n   ],\r\n });\r\ncdkPipeline.addStage(prod, {\r\n   pre: [\r\n     new pipelines.ManualApprovalStep('PromoteToProd'),\r\n   ],\r\n });\r\n\n\u00a0\n\nUsing the (default) self mutation feature. If you add new application stages in the source code or new stacks to LambdaStage, the pipeline will automatically reconfigure itself to deploy those new stages and stacks.\n\n\u00a0\n\n\n\n\nAwesome!\nWe will use some of the features we mentioned here in the following chapters and improve our pipeline.\n\n\nIn this blog, we continued building our project by adding a CI/CD pipeline using the CDK Pipelines module of AWS CDK. The next topics are Bootstrapping and Aspects. We will tackle a few problems we see when we try to use AWS CDK in AWS platforms. See you soon in the next one, cheers!\n\n\n", "tags": ["aws", "aws cdk", "cdk pipelines", "CI/CD", "cloud", "devops"], "categories": ["Blog", "Cloud"]}
{"post_id": 31138, "title": "Production-Ready CDK \u2013 Project Structure", "url": "https://www.luminis.eu/blog-en/cloud-en/production-ready-cdk-project-structure/", "updated_at": "2023-01-27T12:55:00", "body": "In my last post, I announced I am starting a new blog series on Cloud Development Kits. You can find more about the purpose & plan here.\nIn the first chapter of this series, we will begin building our cdk project hands-on while explaining the tooling and the decisions used.\n\nThe primary tool of this post is Projen! In the CDK community, it is a popular tool these days. But for others, it might be the first time they hear about it.\nProjen is a project configuration management tool. To make it more concrete, what AWS CDK is to AWS is, Projen is to your Git Project. So as they call it: it is a CDK for software projects. We can manage all project configurations from a simple, single Javascript file and synthesize project files such as package.json, .gitignore, .eslintrc.json.\nThe concept sounds familiar. We have all seen and used other utility tools like Cookiecutter or Yeoman before. The main issue with those tools is they are for templating, only for one use. After weeks/months of using those tools, your projects will look very different, and there is nothing to do about it. Whereas with Projen, we can create the project and keep managing and configuring it actively since it is not one-time use.\nWhy is it popular with the CDK community, and how did the project start? Because Mr. Elad Ben-Israel, the leading creator of AWS CDK, started Projen and showcased it\u00a0at the first CDK Day in 2020. Then the project grew quickly and almost became the new standard for the CDK projects. While writing this, I saw that it even became an AWS project.\nMy personal experience and why I prefer it\nI have observed that after people start using AWS CDK, the number of AWS CDK projects usually increases sharply after some time. I once worked in an environment where we had +50 AWS CDK repositories. The configurations, pipelines, versioning were all over the place. We fixed it and made them look similar, but not all projects were developed or maintained at the same rate. As time passed, we had the same issue, and we didn\u2019t have a clever and consistent way of managing our projects.\nI also tried templating engines, mostly Cookiecutter, years ago. But unfortunately, the template becomes obsolete rapidly, and almost always, people are not on the same page regarding the project configuration. Besides that, I tried the Bedrock Pattern but didn\u2019t find it applicable to my projects.\nPlus, it is hard to a correct and consistent project structure. There are so many things to think about. To make it more concrete, here is the list of files/features we usually need from a Typescript project, which is a lot:\n\nThe heart of the project: package.json\nTypescript Compiler configuration: tsconfig.json\nDependencies\nLinter\nUnit testing & coverage\nVersion bumps & changelog\nCI builds\nAutomated releases\nSecurity patches\nLicense\nNpm workflow scripts\n\nLuckily the opinionated projects that come ready with Projen contain months of experience, trial, and error. For me, Projen solved the problems I mentioned and made our configuration management much more straightforward, thanks to these.\nLastly, although I highly recommend it, I should warn you it might sometimes be challenging to fix errors because it is a pretty new tool, and the community is not at its peak yet. So we need a bit of patience, that\u2019s all.\nImplementation\nEnough with the story; let\u2019s start with the implementation. Here are the prerequisites to be able to use AWS CDK and Projen:\n\nAWS Account & IAM User or Role that you can assume\nAWS CLI\nNode.js: recommend version 16; version 14 should also be fine\nIDE of your choice\nGit\n\nI assume you configured all and AWS CLI & git & npm(from Node.js installation) working as expected. So let\u2019s execute the following commands to create the project:\n$ mkdir prod-ready-cdk && cd prod-ready-cdk\r\n$ git init\r\n$ npx projen new awscdk-app-ts\nProjen file\nNow, we have a project ready to be detailed. First, we will be working with the\u00a0.projenrc.js file to configure the project. It should look like this first:\nconst { AwsCdkTypeScriptApp } = require('projen');\r\nconst project = new AwsCdkTypeScriptApp({\r\n  cdkVersion: '1.95.2',\r\n  defaultReleaseBranch: 'main',\r\n  name: 'prod-ready-cdk',\r\n\r\n  // cdkDependencies: undefined,  /* Which AWS CDK modules (those that start with \"@aws-cdk/\") this app uses. */\r\n  // deps: [],                    /* Runtime dependencies of this module. */\r\n  // description: undefined,      /* The description is just a string that helps people understand the purpose of the package. */\r\n  // devDeps: [],                 /* Build dependencies for this module. */\r\n  // packageName: undefined,      /* The \"name\" in package.json. */\r\n  // release: undefined,          /* Add release management to this project. */\r\n});\r\nproject.synth();\nSee, it comes up with AWS CDK v1. We need to change the CDK version to v2 and provide more fields. (Warning: I needed to change the first two lines as well)\nconst { awscdk } = require('projen');\r\nconst project = new awscdk.AwsCdkTypeScriptApp({\r\n  authorAddress: 'kemal.gulsen@luminis.eu',\r\n  authorName: 'Kemal Cagin Gulsen',\r\n  cdkVersion: '2.8.0',\r\n  defaultReleaseBranch: 'main',\r\n  name: 'prod-ready-cdk',\r\n  description: 'A CDK project for my blog posts',\r\n  repositoryUrl: 'https://github.com/cagingulsen/prod-ready-cdk.git',\r\n  keywords: [\r\n    'AWS CDK',\r\n    'projen',\r\n    'Typescript',\r\n    'Deployment',\r\n  ],\r\n\r\n  // cdkDependencies: undefined,  /* Which AWS CDK modules (those that start with \"@aws-cdk/\") this app uses. */\r\n  // deps: [],                    /* Runtime dependencies of this module. */\r\n  // description: undefined,      /* The description is just a string that helps people understand the purpose of the package. */\r\n  // devDeps: [],                 /* Build dependencies for this module. */\r\n  // packageName: undefined,      /* The \"name\" in package.json. */\r\n  // release: undefined,          /* Add release management to this project. */\r\n});\r\nproject.synth();\nFor changes to take effect, we need to rerun Projen:\n$ npx projen\nYou can see the changes in the\u00a0package.json, mainly for the AWS CDK dependencies. With CDK v2, we don\u2019t need to add dependencies per AWS Service,\u00a0\u201caws-cdk-lib\u201d:\u00a0\u201c^2.8.0\u201d\u00a0is all we need.\nFurthermore, you might need to bootstrap AWS CDK again using the\u00a0cdk bootstrap\u00a0command since CDK v2 uses the modern bootstrap stack. This modern way will help us because the modern bootstrap stack is a prerequisite for CDK Pipelines.\nNext, we synthesize the CDK app using the command:\n$ npx projen synth\ninstead of\u00a0cdk synth. But, we see that it doesn\u2019t work since we changed the CDK version. So, let\u2019s update the dependencies and add a Hello World Lambda while on\u00a0src/main.ts.\nTip: instead of using npx projen \u2026\u00a0every time, we can have an alias like\u00a0alias pj=\u201dnpx projen\u201d to make it shorter.\nAWS CDK App: A Hello World Lambda Function\nLet\u2019s use src/main.ts for our Lambda stack for now and refactor it in the next episodes. After I fix the imports and add the lambda function, it looks like this:\nimport { App, Stack, StackProps } from 'aws-cdk-lib';\r\nimport * as lambda from 'aws-cdk-lib/aws-lambda';\r\nimport { Construct } from 'constructs';\r\n\r\nexport class LambdaStack extends Stack {\r\n  constructor(scope: Construct, id: string, props: StackProps = {}) {\r\n    super(scope, id, props);\r\n\r\n    new lambda.Function(this, 'ExampleFunction', {\r\n      functionName: 'example-lambda',\r\n      code: lambda.Code.fromAsset('lambda'),\r\n      handler: 'hello.handler',\r\n      runtime: lambda.Runtime.NODEJS_14_X,\r\n    });\r\n  }\r\n}\r\n\r\n// for development, use account/region from cdk cli\r\nconst devEnv = {\r\n  account: process.env.CDK_DEFAULT_ACCOUNT,\r\n  region: process.env.CDK_DEFAULT_REGION,\r\n};\r\n\r\nconst app = new App();\r\n\r\nnew LambdaStack(app, 'lambda-stack-dev', { env: devEnv });\r\n// new LambdaStack(app, 'lambda-stack-prod', { env: prodEnv });\r\n\r\napp.synth();\nAnd of course, add our Lambda source code,\u00a0lambda/hello.js.\nexports.handler = function(event, context) {\r\n  console.log('Hello, Cloudwatch!');\r\n  context.succeed('Hello, World!');\r\n};\nThen finally, after we\u00a0npx projen synth (or shorter, pj synth), we will have the smallest AWS CDK App ready to be deployed. You know the drill; then we do\u00a0npx projen deploy\u00a0and check the lambda created on AWS.\nTesting\nFinal touches, let\u2019s add our first unit test.\nimport * as cdk from 'aws-cdk-lib';\r\nimport { Template } from 'aws-cdk-lib/assertions';\r\nimport { LambdaStack } from '../src/main';\r\n\r\ntest('Lambda created', () => {\r\n  const app = new cdk.App();\r\n  const stack = new LambdaStack(app, 'LambdaStack');\r\n  const template = Template.fromStack(stack);\r\n\r\n  template.resourceCountIs('AWS::Lambda::Function', 1);\r\n});\nTo run tests, we can use the command\u00a0npx projen test.\nWatch mode\nWatch mode is a feature that can be very handy when writing CDK code. AWS CDK Team introduced this mode with AWS CDK v2. Every time we save a file and change the synthesized cdk output, the watch mode calls cdk deploy command. Therefore, our stacks deployed on AWS reflect our code, and we don\u2019t need to use cdk/projen commands every time we deploy. As a result, we save time, and deployments are faster for the development environment.\nTo enable it, we can use\u00a0npx projen watch.\nFor other commands, you can check\u00a0package.json. And for the list of options, you can check the API Reference. However, these days we have a more good-looking option in the Construct Hub.\nGithub Project\nAfter configuring the Projen file and AWS CDK App, we can create a new repository on Github and push the code to the repository. I have mine: cagingulsen/prod-ready-cdk. Then used the following commands:\n$ git remote add origin https://github.com/cagingulsen/prod-ready-cdk.git\r\n$ git push -u origin main\nThen we can commit the latest changes and push them to Github. You can check the code here.\nWe made the introduction for our CDK journey and mainly focused on Projen. But of course, this is not the final version of our cdk project configuration. We will add more settings and use more Projen features in the future to ramp up. And indeed, we will have more CDK Constructs than just a Lambda Function.\nThank you for your time, and see you in the upcoming post on CDK Pipelines. Cheers!\nReferences:\nhttps://github.com/projen/projen\nhttps://youtu.be/SOWMPzXtTCw\nhttps://aws.amazon.com/blogs/developer/increasing-development-speed-with-cdk-watch/\n\u00a0\n", "tags": ["aws", "aws cdk", "cdk", "cloud", "infrastructure as code", "projen"], "categories": ["Blog", "Cloud"]}
{"post_id": 30000, "title": "Cross-account AWS resource access with AWS CDK", "url": "https://www.luminis.eu/blog-en/cloud-en/cross-account-aws-resource-access-with-aws-cdk/", "updated_at": "2023-06-16T08:21:19", "body": "So here is the case: you have S3 buckets, DynamoDB tables, relational tables on several AWS accounts and want to share the data with other AWS accounts. To create a data lake for example. And you are not using the AWS Lake Formation, which provides cross account usage out of the box.\nOr, you are in the middle of a migration from accounts.\nOr, you want to have communication over multiple accounts, for example an API Gateway which needs to be used by different lambdas over multiple accounts.\nOr, you want to deploy with code pipeline to multiple environments which exists in multiple accounts.\nWhere to start\nAlthough AWS describes this topic quit nicely, I want to demonstrate how to do it with CDK.\nFor this example we will have two accounts, the original, source Account ID is 11111 and the new, target Account ID is 22222.There are actually two ways of using resources in cross accounts, namely by identity-based policy and resource-based policy. What is the difference then?\nWell, the first (and for us most important) difference is, not all resources do support a resource-based policy. For example, DynamoDB does not, as can be found in these tables right here.\n\nWith an identity-based policy, you will kind of create a \u201cproxy role\u201d to get to the other account and resources. For a resource-based policy, a policy will be directly attached to the resource itself, where you can attach the account IDs you want to give access to. For an identity-based policy the new account will need to assume a role temporarily, which then only gives permissions for that specific role instead of the original permissions, while a resource-based policy will give both permissions at the same time.\nGiven that not all resources support identity-based policy, we will provide a solution for both below.\n\u00a0\nResource-based policy cross account usage\nWe are going to add some code in our existing CDK script for the source account (11111):\nconst bucket = new s3.Bucket(this, 'SourceBucket', {\r\n    bucketName: 'source-bucket'\r\n});\r\n\r\nbucket.addToResourcePolicy(new iam.PolicyStatement({\r\n    actions: ['s3:Get*', 's3:List*'],\r\n    resources: [bucket.arnForObjects('*')],\r\n    principals: [new iam.AccountPrincipal('22222')]\r\n}))\r\n\nHere we are defining the resource-based policy for the new target account, 22222 for our original bucket, so that it will have direct access. The service in the target account just has to reference the bucket (by arn, most of the times) and it will work! At least, the actions that you gave permissions for. \ud83d\ude42\nThe target CDK script could contain the bucket if needed like so:\nconst bucket = s3.Bucket.fromBucketName(this, 'SourceBucket', 'source-bucket');\nBut more likely is it, that you will have to use it in your application, for example with the Java S3Client (v2):\nvar s3Client = S3Client.builder().build();\r\nvar getObjectRequest = GetObjectRequest\r\n   .builder()\r\n   .bucket(\"source-bucket\")\r\n   .key(\"filename\")\r\n   .build();\r\nvar response = s3Client.getObject(getObjectRequest);\nA bit more advanced approach which you will probably need when you have > 10 accounts that need to use the S3 bucket, is by using AWS Organizations. Because if you will have to add all those accounts separately, you can miss the overview quite fast!\nThe CDK script will change slightly, but still is easy configurable as long as you are managing your accounts correctly by Organizations:\nbucket.addToResourcePolicy(new iam.PolicyStatement({\r\n   actions: ['s3:Get*', 's3:List*'],\r\n   resources: [bucket.arnForObjects('*')],\r\n   principals: [new iam.OrganizationPrincipal('organizationId')]\r\n}));\nAll accounts under this organizationId will get the access you just defined.\nIdentity-based cross account usage\nIf you want to enable cross account usage for DynamoDB for example, we are going to need to use the identity-based policy option. This will involve some more steps than for the resource-based policy.\nFirst, we need to ensure that the original account will allow the new account to perform the action of assuming a role. This is done by creating a role in the 11111 account with a policy to allow this actions. Because we trust the new account fully, we will use 22222:root as principal. You could always put this to a specific user if you want to.\n//create role to assume the new principal account to\r\nconst role = new iam.Role(this, 'CrossAcountRole', {\r\n    assumedBy: new iam.ArnPrincipal('arn:aws:iam::22222:root'),\r\n    roleName: 'cross-account-role'\r\n});\r\n\r\n//add statement to allow assumerole action for this account\r\nconst assumeStatement = new iam.PolicyStatement();\r\nassumeStatement.addActions('sts:AssumeRole')\r\n\r\n//add under the principal policy\r\nrole.addToPrincipalPolicy(assumeStatement);\r\n\r\n//add statement to allow reading and putting into dynamodb table in original account\r\nconst resourceStatement = new iam.PolicyStatement();\r\nresourceStatement.addResources('arn:aws:dynamodb:eu-west-1:11111:table/tableName');\r\nresourceStatement.addActions('dynamodb:DescribeTable', 'dynamodb:GetItem', 'dynamodb:PutItem');\r\n\r\n//add as new policy\r\nrole.addToPolicy(resourceStatement);\r\n\nThis will create a role with an arn. Keep this arn for the next step.\nAfter this, we can go on to the CDK part of the new account. Here, we need to allow the task, lambda or any computing service to let it assume a role to the original account; the changing to the proxy role. This is done by adding a policy to the related role of the service. Here we need the arn of the role we just created.\n//for an ecs task\r\nnew ecs.TaskDefinition(this, 'TaskDefinition', {...})\r\n\t.addToTaskRolePolicy(\r\n    new iam.PolicyStatement({\r\n        resources: ['arn:aws:iam::111111:role/cross-account-role'],\r\n        actions: ['sts:AssumeRole'],\r\n    })\r\n);\r\n//for a labmda function\r\nnew lambda.Function(this, 'Function', {...})\r\n\t.addToRolePolicy(\r\n    new iam.PolicyStatement({\r\n        resources: ['arn:aws:iam::111111:role/cross-account-role'],\r\n        actions: ['sts:AssumeRole'],\r\n    })\r\n);\r\n\nIn CDK that\u2019s it!\nNow, we need to get the credentials correctly set up in the application itself (we are using a Java application as example, which uses AWS SDK v1); meaning that we get temporary credentials (the proxy role) we can use to hook into the original account, instead of the actual account, which will normally happen when you are creating a resource client like for DynamoDB.\nWhen looking into the STSAssumeRoleSessionCredentialsProvider class, it becomes clear we can use this to keep a temporary role for the original account we want. In order to use this, we also need the ARN of the role defined in the original account.\nvar stsAssumeRoleSessionCredentialsProvider = StsAssumeRoleCredentialsProvider\r\n   .builder()\r\n   .refreshRequest(\r\n      AssumeRoleRequest\r\n         .builder()\r\n         .roleArn(\"arn:aws:iam::111111:role/cross-account-role\")\r\n         .roleSessionName(\"Name for session role\")\r\n         .build())\r\n   .build();\r\n\nAnd then we need to ensure our client(s) use these credentials.\nvar amazonDynamoDBClient = DynamoDbClient\r\n   .builder()\r\n   .credentialsProvider(stsAssumeRoleSessionCredentialsProvider)\r\n   .build();\r\n\nAfter that, we will get a connection between the two accounts. We can still also define a separate client, which connects to a resource in its own account, by not providing the credentials.\nThat was quite easy! Now your target account has access to resources of the original account, as long as the role exists and the resources are available.\nSources:\nhttps://aws.amazon.com/premiumsupport/knowledge-center/lambda-function-assume-iam-role/\nhttps://docs.aws.amazon.com/IAM/latest/UserGuide/tutorial_cross-account-with-roles.html\nhttps://docs.aws.amazon.com/IAM/latest/UserGuide/id_roles_compare-resource-policies.html\nhttps://docs.aws.amazon.com/IAM/latest/UserGuide/access_policies_identity-vs-resource.html\nhttps://docs.aws.amazon.com/IAM/latest/UserGuide/reference_aws-services-that-work-with-iam.html\nhttps://jayendrapatil.com/aws-iam-roles-vs-resource-based-policies/\nhttps://stackoverflow.com/questions/60310575/how-to-add-s3-bucketpolicy-with-aws-cdk\nhttps://aws.amazon.com/blogs/security/iam-share-aws-resources-groups-aws-accounts-aws-organizations/\n", "tags": ["aws", "cdk", "cloud", "iam"], "categories": ["Cloud", "Development"]}
{"post_id": 31125, "title": "Production-Ready CDK \u2013 Prelude", "url": "https://www.luminis.eu/blog-en/cloud-en/production-ready-cdk-prelude/", "updated_at": "2023-01-27T12:53:40", "body": "I am starting a new blog series on Cloud Development Kits(CDK), first with AWS CDK. Then later, planning to jump to CDK for Terraform (CDKtf) and combine it with CDK for Kubernetes(CDK8s). Let\u2019s see how far we will come.\nWhat is a Cloud Development Kit, and how to start using it?\nI won\u2019t discuss what Cloud Development Kits are or why to use them compared to the other Infrastructure as Code tools. To learn more about CDKs and particularly AWS CDK, I recommend watching this video from AWS re:Invent 2018, especially if you don\u2019t have much experience with Infrastructure as Code tools. Although some things changed since 2018, the first part of the video will be relevant for many years because it explains the main concept behind CDKs.\n\nAfter watching the video, I highly recommend getting hands-on and following the CDK Workshop. Afterwards reading this blog series will make a lot more sense. Finally, the CDK Book can help if you want to dive in a bit deeper.\nWhat is my experience with CDKs?\nI work as a Cloud Architect/Consultant, and I have been using AWS CDK for more than two years. I used it mainly for serverless applications, CI/CD pipelines, and platform-level solutions at four different companies.\nHowever, I don\u2019t use it 8 hours a day, and there are people in this field who more actively develop the tools I use. So I will keep learning as I write, and meanwhile, I will share my knowledge in every detail to make your CDK journey as easy and smooth as possible. I hope you will enjoy it!\nWho is the audience?\nMainly anyone who is a Cloud Enthusiast. But providing some titles: Cloud Engineers, Software Engineers using Cloud, Cloud Architects, and DevOps Engineers.\nWhy \u201cproduction-ready CDK\u201d?\nI am doing the \u201cProduction-ready\u201d way because most of the resources and articles I find online focus on fundamental use cases. I often need to code my constructs very differently. The main reason is that those examples are not fit for the production level or enterprise level I work on.\nIn this series, I will share insights for production-grade CDK apps. So the focus is not explaining concepts shortly and quickly. Instead, we will do the opposite and try to get into the details that will make your applications secure, scalable, and available. Which doesn\u2019t mean the code I will be sharing will be ready to use in your production environment. You still need to make it your own, but it will give you ideas for real life cases.\nWhich tools are we going to use?\n\u2022 AWS\n\u2022 AWS CDK v2\n\u2022 Typescript\n\u2022 Projen\n\u2022 Github\n\u2022 CDK Pipelines\n\u2022 and others in the future\nIf you haven\u2019t decided to use AWS CDK as your Infrastructure as Code tooling for AWS:\nWell, for sure you need to compare and decide. But, if you are considering it for personal projects, I can promise you won\u2019t regret it. It will be fun, and you will learn valuable skills for both the present and the future.\nAWS CDK is a higher-level IaC tool compared to CloudFormation or Terraform\nIf you are considering using AWS CDK for your team: there are multiple factors you need to think about, like tools already in use, people\u2019s expertise, existing enterprise tooling, etc.\nBut to give you some ideas, you can read the following:\n\u2022 Cloudformation terraform or cloud development kit guide to iac on AWS\n\u2022 Terraform vs cdk vs cloudformation\n\u2022 Adopting aws cdk\nIf you are not sure which programming language you should use for AWS CDK:\nWe all have different tastes in programming languages, but I suggest using Typescript because of jsii. Jsii is a tool that enables some other programming languages to import and use Javascript classes. In other words, if we write our constructs in Typescript, we can package them for other languages, and for example, when using Python, we can import those constructs! But beware, it doesn\u2019t work the other way. If you write your constructs in Python, you cannot import them in Typescript. Also, most of the AWS CDK examples are for Typescript Apps.\nThe second language I can recommend is Python. It suits CDK programming very well; you can compactly write your constructs.\nImportant tip: we shouldn\u2019t pick our programming language thinking of our frontend or backend programming experience. Coding CDK apps is very straightforward, we use Object Oriented Programming much less, and we never use design patterns (well, you can, but why should you). Considering these, Typescript and Python-like languages suit better compared to Java or C#.\nFinally, I will be sharing the code on Github and keep updating and maintaining it.\nProjen \u2013 #TemplatesAreEvil\nOur first topic is Project Structure for AWS CDK Apps using Projen. See you in the next ones.\n", "tags": ["aws", "aws cdk", "cdk", "cloud", "infrastructure as code"], "categories": ["Blog", "Cloud"]}
{"post_id": 31111, "title": "My experience becoming a FinOps Certified Practitioner (FOCP)", "url": "https://www.luminis.eu/blog-en/my-experience-becoming-a-finops-certified-practitioner-focp/", "updated_at": "2022-06-24T09:22:55", "body": "Recently, I sat down for the official FinOps Certified Practitioner exam. Now the experience is still fresh, I want to share my thoughts on getting this certification.\nFirst a bit of background, I\u2019m a software developer and architect working fulltime on cloud projects. I do not have much experience with the financial side of business but do have a healthy curiosity for things outside my direct line of work. In the Cloud, certifications are a well-established way to demonstrate proficiency. They are never proof of mastery, just evidence that you are familiar with the subject. For myself, I like certifications, they give you a tangible goal and something to show for your efforts. So, when I discovered the FinOps Certified Practitioner certification, a seed was planted.\nWhen you embark on such an endeavor, it\u2019s always good to start with the why. For me, one thing I always liked about the Cloud is that you have this extra dimension of cost, next to the usual suspects like compute, memory, storage, (and engineering effort of course). When you\u2019re working on a solution you try to balance all these dimensions and try to come up with the most optimal outcome. And what really makes it interesting is that the flexibility and speed of the cloud put all these dimensions in overdrive. This balancing act can (and maybe even should) be done continuously.\nWhat is FinOps?\nThe official website defines FinOps as follows:\n\nFinOps is shorthand for \u201cCloud Financial Operations\u201d or \u201cCloud Financial Management\u201d or \u201cCloud Cost Management\u201d. It is the practice of bringing financial accountability to the variable spend model of cloud, enabling distributed teams to make business trade-offs between speed, cost, and quality.\n\nIt\u2019s an area where two worlds collide, the technical world of IT and Cloud, and the business world of Finance and accounting. The Cloud has brought with it an enormous boost both in speed and unpredictability of IT cost, one that every serious user must come to grips with sooner or later. FinOps is an answer that will help you deal with this. Not by trying to mold the Cloud in an old form, controlling it, choking it to death, but by embracing all its benefits and capabilities. Another quote that I really like is:\n\nFinOps is not about saving money, it is about making money.\n\nHow? That\u2019s where the FinOps framework comes into play and what the certification is all about. It consists of a set of principles, phases, capabilities, and domains, all worked out in detail. They have a nice poster on the official website that gives a clear overview of this framework.\nImportant as all these components of the framework are, it\u2019s good to realize that at its core, FinOps is a cultural change. In that sense it is like the DevOps movement, which was probably a huge inspiration for the FinOps foundation. It\u2019s not just about tools and processes, but also about collaboration and establishing a shared language.\nFinOps Foundation\nThe FinOps movement is coordinated and organized by the FinOps Foundation, a program of the Linux Foundation. Their mission is to build and foster a community of people and companies practicing financial cloud management. Their members come with a wide variety of backgrounds, from technical, engineering type of backgrounds to finance or procurement professionals.\nOne way in which they share their knowledge is via training and certification. The best known and most achieved certification is the FinOps Certified Practitioner certification, and that is the one I had set my eyes on.\nFinOps Certified Practitioner\nThe certification:\n\nallows individuals in a large variety of cloud, finance and technology roles to validate their FinOps knowledge and enhance their professional credibility. The certification covers FinOps fundamentals and an overview of key concepts in each of the three sections of the FinOps lifecycle: Inform, Optimize and Operate.\n\nIt\u2019s a foundational level course, meant to ensure a common grounding between practitioners and a good introduction to the FinOps practices. There is also a Professional level certification, targeted at experience FinOps practitioners.\nPreparing for the exam\nThe most important source is the highly recommended Cloud FinOps book. This book covers all the material for the exam, and some more. It\u2019s a good read for anybody interested in this area, not just for wannabe certified FinOps practitioners.\nThere\u2019s also the official training from the FinOps foundation itself, either led by a virtual instructor, or in a self-paced format. Because I did not know what to expect from the exam, I decided to go for the self-paced training. It came with a hefty price of $499, which they bumped to $599 as of beginning this year. It includes the exam price (of $300), but even then, it\u2019s very expensive for an online training course that takes less than 6 hours.\nWas it worth the money? I\u2019m inclined to say no. It\u2019s basically a slimmed down version of the book, with an occasional quiz. If you\u2019ve read the book, made some notes, thoroughly went through the website, you can save yourself 300$. But it might also depend on your learning preference, if you\u2019re more a listener then a reader, it might make sense to splurge on the course. In that case I would recommend first investigating unofficial, third-party, online training courses which are usually a lot cheaper.\nWhat I also recommend is some practical experience. What I mean is, really going through the cloud bill and cost reporting of you cloud provider, to get some real-life understanding of the theory.\nThe exam\nThe exam is pretty easy, provided you went through all the material. It\u2019s only 60 minutes long, but you have plenty of time. It took me about 35 minutes and did not feel rushed in any way. It\u2019s a remote exam, unproctored, meaning you can complete it in your own browser, without having to install special surveillance software or somebody watching remotely. If you\u2019re familiar with the AWS certification space, it\u2019s more like an accreditation as opposed to a certification, although the difficulty is similar to the AWS Cloud Practitioner certification. The questions, 50 in total, are all multiple-choice and do not involve extensive reading.\nThe good part is also that you get 3 attempts available out of the box, so you can try it out as soon as you think you have the material covered and see if you\u2019re at the necessary level.\nNormally, when I study for an examination, I like to do one or more practice exams, to ensure I\u2019m at the required level. But with the 3 attempts available, I decided to first try one out before I\u2019d purchased some kind of practice exam. Luckily for me, the first try was successful (with 84% score, where 75% is required for passing), so I didn\u2019t have to go back to the drawing board and readjust my studies.\nFinal thoughts\nI thought it was a nice and interesting experience that really gave me good introduction into the subject.\nIn the end, it was easier than I expected. Maybe it was that hefty price tag that subconsciously made it appear bigger than it was. And when you expect things to be difficult, they often turn out easier than you thought they would be. Not to say it\u2019s a walk in the park, you do need to study and take it seriously.\nWould I recommend going for the FOCP certification? Well, it depends, of course. It depends on what your goals are.\n\nIf you want a job in this area, then the cost if probably small compared to what you can get out of it, and it is well worth your time and money.\nIf you\u2019re just interested in the field, then I would probably say no. The price/value ratio was not great. Reading the book is probably your best and most cost-effective approach.\nIf you don\u2019t have to pay fully for the exam yourself, and certification aligns with your learning preferences, then the required time and effort make sense\n\nMe personally, I fall into the last category and am happy with both the journey and the destination. It brought me practical tools for my day-to-day job and a stronger appreciation for the subject.\nAre you curious or do you want to know more? Get in touch.\nLearn from our experts:8 Feb 2021-Mark van KesselTraining: AWS Cost ControlOne of the big promises of the Cloud is that it makes IT cheaper. This is certainly a possibility, but not a given. Running the same workload as before in the exact same way usually only increases the cost. Cost...\n", "tags": ["FinOps"], "categories": ["Blog", "Cloud"]}
{"post_id": 31061, "title": "Getting more value out of your data\u00a0", "url": "https://www.luminis.eu/blog-en/data-en/getting-more-value-out-of-your-data/", "updated_at": "2023-09-06T13:53:56", "body": "\u201cData is eating the world\u201d, is a well-known variant on Mark Andreesen\u2019s famous quote \u201cSoftware is eating the world\u201d. In our current output driven society everything is measured by an interpretation of a set of data, even by the people and businesses that were not traditionally driven by data.\nThis has led to an increasing demand for, and thus increased value of data. But not just any data, it is data that provides us with insights that we value most. Whether data is insightful depends on many facets, most of which vary between different groups of stakeholders. But an overall rule of thumb is that for the data to be insightful, it must have a certain level of quality. To reach this level of quality there are four things that play a key role: collection, storage, availability and analysis. For the data to have decent quality these four facets need to be coordinated correctly.\nCollection\nThe first is collection: Not only do you have to decide which data is relevant for the insights you want to gain, but you also must decide the right method to collect this data. Depending on the data you want to collect, you might have to select reliable sources. Do your users provide the data, is it something collected by an (IoT) device, or is it an existing dataset from a third party? Different types of data come with different measures for determining reliability.\nYou also must define if there is a minimal amount of data required and if so, what that minimal amount is. In case of benchmarking for example, you want to compare a sample against a bigger population. To make reliable assumptions you need to have enough data for each of the benchmarked groups. But whether that is ten or ten million items per group depends on the use case. Finally, you need to know whether your data needs some form of preparation, transformation or optimization. You might want to train some AI models, transform some binary data into something more readable or distill only the useful pieces of data out of a larger set, so it can be retrieved faster and more easily. Depending on the use case data collection can be a repetitive process, so there is not one moment to collect data, but this is done throughout time.\nStorage\nThen there is storage: You must decide how to store the collected data in a secure manner. Think about which regulations apply (GDPR for example) and where you want to store it. Do you want to store your data in the cloud or on premise, and does the physical location matter, or is it OK that it is stored in a datacenter on the other side of the world?\nWhich type of storage is best for your data? Will a relational database suffice or is your data (partially) free format, in which case a document store will be the more obvious choice, while a graph database is more likely when it is important how parts of your data relate to each other. And is it possible to combine different types of storage if it is required by your data?\nDo you want the data to contain state, or does it need to be a set of state changes? While the former is less storage consuming, the latter makes it possible to keep a data trail and recreate the state of the data at any moment in time. In case a data trail is important, event sourcing might be something to look into.\nAnalysis\nAt this point you have collected (some) data and stored it somewhere. But does it already provide the insights you thought it would? In many cases raw data is not really that insightful. In that case Analysis of the data can be a solution. There are many ways of analyzing a set of data. It is possible to use a predefined algorithm to deduct the insightful information from a larger set, or you can use machine learning to make classifications or predictions for you. In this case analysis provides new views on the data. But you can also analyze the data to check the quality or find out if you collected the right data. With the feedback from the latter analysis, you can improve your data and tweak the way you collect data in the future. In this way you improve the data you have already collected while you make sure that the data you collect in the future is less flawed, win-win.\nAvailability / Accessibility\nAfter you have gotten to the point where your data contains the insights you wanted, it is time to share it with others. The concept of availability might seem contradictory because it is about making your data easily available, but only to the people that are eligible. For the \u201cmaking your data easily available\u201d part you need to know some things about your users. Who are they? Can you distinguish user groups with different needs? How are you going to visualize data? Do your users want dashboards, or do they want an API to communicate with? In case of dashboards, you need to think about which visualization is best to capture the insights for your users. In case of an API, what is your API strategy? How do you make sure that it is well documented for your users, and do you want to make it easily accessible for applications as well?\nFor the \u201conly to the people that are eligible\u201d part you must think about how to restrict the access to your data. Do you want people to login first, and do you want to have different access levels? In some cases you want to keep track of who accessed which part of your data, access logs might come in handy. In that case you want to think about how to store this metadata and the regulations that apply when storing it.\nAs you can see there is way more than meets the eye when it comes to creating more valuable data for your organization. This blog might have left you with more questions than answers it has given you. Partially because these answers vary for each use case, but also because it makes you rethink about choices you have made regarding your own data and the process of making it more valuable.\nAt Luminis we have developed a set of methods and frameworks that help you find the optimal data solution for your use case. From helping you with answering the questions in this blog on a more functional level, to technical solutions that help you implement your optimal data solution. Over the coming months we will publish more blogs that dive deeper into these methods and frameworks.\n", "tags": ["data", "structure"], "categories": ["Blog", "Data"]}
{"post_id": 31008, "title": "Luminis achieves the status of AWS Advanced Consulting Partner", "url": "https://www.luminis.eu/blog-en/luminis-achieves-the-status-of-aws-advanced-consulting-partner/", "updated_at": "2022-01-21T13:50:44", "body": "Amersfoort, 20 January 2022. Tech company Luminis has achieved the status of Advanced Consulting Partner in the Amazon Web Services (AWS) Partner Network. This achievement is another milestone in the collaboration between AWS and Luminis, and means that customers with issues related to AWS cloud services are in safe hands with Luminis.\nWith this new partnership level, Advanced Consulting Partner, it becomes even more clear to customers that Luminis is a recognized expert in the field of AWS. Examples of services that Luminis offers include: advice, integration, migration and the lifecycle management of applications in the AWS Cloud. With more than 60 different employees who have obtained one or more AWS certifications, Luminis joins the select group of AWS Advanced Consulting Partners in the Netherlands.\nLuminis as Advanced Consulting Partner\nTo become an Advanced Consulting Partner, an extensive set of requirements must be met in terms of knowledge, based on AWS certifications, and experience, based on projects and results. In addition, a long-term partner plan has also been drawn up that sets out how AWS and Luminis can strengthen and use each other. An important part of this plan also fits in well with Luminis\u2019 knowledge-sharing strategy. In 2021 Luminis organized various events for customers, colleagues and the AWS community in the Benelux. As a result, Luminis employees have been accepted into AWS\u2019 prestigious Community Builder program. The AWS Community Builders program provides a variety of materials and networking opportunities for AWS thought leaders with the goal of growing the AWS community in the Benelux.\nCloud Proficiency by Gamification\nLuminis has expanded its knowledge in the field of Cloud in a very special way: with Gamification. A number of enthusiastic Luminis colleagues have created a development program based on Gamification. that helps people get acquainted with the possibilities of Cloud technology. Bert Ertman, VP Technology explains: \u201cTo work on our Cloud Proficiency, a group of colleagues has started \u2018Cloud the Game\u2019. This is a platform with many gamification elements such as badges, pools and mini-challenges. Anyone who achieves something in the field of Cloud technology can share this performance with their colleagues. The platform and the challenges we organize challenge our employees to increase their Cloud Proficiency.\u201d\nLuminis as partner for Cloud technology\nCloud technology is both an opportunity as well as a challenge for many organizations. Luminis has been actively applying Cloud technology for both its customers and its own products since 2009. The gained knowledge and experience are the foundation for all our services and customers. These services focus on helping organizations with concepts such as \u2018Cloud-the-Game\u2019 and \u2018Accelerate\u2019, an intensive training for tomorrow\u2019s leaders in field of technology, as well as on the technological challenges with services in the field of migrations, software development and data strategy.\n", "tags": [], "categories": ["Blog", "Cloud", "News"]}
{"post_id": 30660, "title": "The Amplify Series, Part 3: Why should you use AWS Amplify?", "url": "https://www.luminis.eu/blog-en/the-amplify-series-part-3-why-should-you-use-aws-amplify/", "updated_at": "2023-05-08T13:08:08", "body": "Now that you know what AWS Amplify is and have a broad overview of how it works, we will look at several reasons you might consider using it. We will also look at scenarios where Amplify might not be the best choice. AWS Amplify is a powerful tool. However, you should always try to use the right tool for the job! Whether the right choice is Amplify or another tool.\nAmplify has evolved into a useful and mature toolset for creating cloud-native web and mobile applications. Having Amplify in your developer toolbelt will increase your productivity in different scenarios. Be it developing your next application, creating a quick prototype, or learning/teaching AWS. Let\u2019s look at why you might want to use AWS Amplify.\nIncrease speed, reduce risk: write less hand-crafted code\nThe primary reason to use AWS Amplify is that it increases your business speed and agility. This is achieved by greatly reducing the amount of code you need to write and manage to achieve cloud-native functionality. In part 1, we already listed all of the functionality available in AWS Amplify. However, that was just a brief overview. In this section, we want to zoom in on some of the functionality and advantages that impress us the most and end up using the most in our projects.\nAuthentication\nOne of the most valuable categories in the Amplify suite is the Auth category. It provides us with rich authentication functionality backed by AWS Cognito. After you use this for the first time, you will never want to write your own authentication mechanisms ever again. This category even comes with an authentication UI that you can optionally use to speed up development even more, and it is completely customizable:\n\nOnce the auth category is set up, it will create an AWS Cognito user pool to register your users. The Amplify SDK will provide several helper functions to register, sign in and sign out your users. There is also forgot-password and email activation functionality out-of-the-box.\u00a0\nBesides creating a user using an email and password combination, you can easily set up federated sign-in with Google, Facebook, Apple, or any provider that supports Open-ID connect and SAML.\nOne other nice feature to notice in this category is that it will handle authentication for the other categories such as API and Storage once you have it set up. This means that you do not need to handle any token management yourself.\nGraphQL\nWhile the REST version of the Amplify API category also provides a lot of functionality, including a REST API, Lambda function, and DynamoDB, we have decided to highlight the GraphQL version as it is a prime example how much AWS Amplify can help you with.\nIf you are familiar with GraphQL, you know that you can define your API in a GraphQL schema. What Amplify adds to this is several annotations called GraphQL directives that generate functionality. The following is an example of such a schema:\n\nJust by creating this schema, we will get the following generated for us:\n\n@model\n\nDynamoDB tables for Show and Review generated.\nGraphQL Query, Mutation, and Subscription client stub code for Show and Review\u00a0\n\n\n@auth\n\n403 http status codes when the auth rules are broken. In this example, only users that are logged in (\u201cprivate\u201d) are allowed to create, read, update and delete shows. There is also the possibility of making more complex authorization rules, such as allowing only users of a particular group to perform actions.\n\n\n@connection\n\nReferences from Show to Review and vice versa\n\n\n@function\n\nGraphQL queries for \u201creviewScore\u201d for \u201cshow\u201d routes to a separate Lambda function instead of going directly to the database. In this way, you can still customize the logic in the backend instead of just building CRUD.\n\n\n\nVisit the AWS Amplify docs to find out more about Amplify GraphQL directives.\nHosting and CI/CD\nThe final part we want to highlight is the entire hosting and CI/CD that you can set up for your project in just a few commands. By simply running \u201camplify add hosting\u201d and following a few instructions, you can set up an entire CI/CD pipeline for your repo that deploys the backend and hosts the frontend on a public URL.\n\nEvery part of the pipeline is customizable, and setting up redirects and HTTPS with a domain name is a piece of cake. You can even spin up different environments per branch to test them.\nEmpower front-end engineers: JavaScript front- and back-ends\nWe have noticed while using Amplify that it has allowed our front-end engineers to expand their reach in development by being able to contribute to the backend since it is also written in JavaScript.\n\nWhile Amplify supports Lambda functions in other languages such as Java, choosing for JavaScript while also building your frontend in JavaScript means that you develop your entire application with the one language all frontend engineers know. This will allow Frontend engineers to pick up whole vertical issues in your project to handle changes to the entire stack.\nNote that you can choose to ignore JavaScript completely and build an Android mobile app with Java Lambda\u2019s in the backend in Amplify. The advantage mentioned here does not hold in this case.\nInnovate at speed: prototype to reduce time-to-value\nPrototyping is one of the best ways to determine if your product/solution will solve the problems you are trying to solve. If you can test your ideas and get feedback as fast as possible, you will innovate quickly.\n\nWhile using paper prototypes or interactive mockups is a fast way to get user feedback, you can only reach the number of people you have time to test with. If you want to get an actual prototype up and running in no time so that it can be shared via social media and tested by several users, then Amplify can be used to skip all of the setup and let you focus on building the prototype.\nIf you combine this with analytics and surveys, you will get information from a more extensive user base should that be necessary.\nAccelerate cloud proficiency: leverage AWS best practices and patterns\nAWS has almost 200 services available at the moment. This can be very intimidating for someone trying to start their cloud journey. AWS Amplify only uses a subset of these services and guides you in creating and configuring the services.\nOne of the strengths of Amplify is that you can create an entire cloud-native application without having to know what is happening under the hood. However, learning what happens under the hood of AWS Amplify is an excellent way to start learning about cloud computing since the scope of services is much smaller, and you can take it step-by-step based on the categories.\nWhen you are done building your first cloud-native Amplify application and know what is happening in the background, you will have a sound base of knowledge and experience to build upon.\nKeep options open: extend Amplify with CDK\nOne of the most common questions is when we should use Amplify or CDK when starting up a cloud-native AWS project. By now, you have an idea of what AWS Amplify is. If you have never heard of CDK before, our colleague wrote an introduction blog post about it.\u00a0\n\nThe primary objective of CDK is to allow developers to write infrastructure-as-code in a programmer-friendly language instead of CloudFormation. Amplify wants to abstract as much as possible from the backend infrastructure and Cloudformation, while CDK has nothing to do with the frontend. Since they have different goals, they have different scenarios where you should pick one over the other.\nHowever, since the introduction of Amplify Extensibility, your options have become somewhat simpler. You can build your frontend using Amplify, extend it with CDK and even export your Amplify project as a CloudFormation templates to use in your existing CDK projects. \u200b\u200bPicking one over the other is no longer an binary exercise. Instead, we should explore when to leverage which option.\nWhen should you not use AWS Amplify?\nBefore Amplify Extensibility, which allows you to use AWS CDK to define AWS services you want to use in your Amplify application that is not present in the several Amplify categories, my advice was always:\n\u201cIf you can map your main success use cases to the Amplify categories, then you should use Amplify. Otherwise, you should avoid using it.\u201d\nHowever, this advice has become obsolete since we can now use any AWS service in our Amplify applications thanks to Extensibility. There are still some scenarios where you shouldn\u2019t use Amplify:\n\nNon-AWS or multi-cloud projects: AWS Amplify can only target the AWS cloud platform.\nNon-fullstack projects: if you creating pure front-end or back-end projects, Amplify might not be the most effective choice.\nMisfit with existing architecture: if fitting an AWS Amplify project into your existing landscape would require massive effort or does not align with existing architectural principles, it might not be the most logical choice. Maybe you should start with convincing your enterprise architect of Amplify\u2019s value first.\nNo clear advantage: if large parts of Amplify\u2019s functionality are already in place (e.g., slick build and deploy pipelines, or a library of similar building blocks), then the added value of Amplify might not be so great. But that\u2019s good news, right?\n\nUp next\u2026\nHopefully, this blog has given you even more reason to start working with AWS Amplify. Now that we have the high-overview and background information out of the way, it is time to get our hands dirty! In the following blog posts, we will be creating an AWS Amplify application, and we will take a closer look at several Amplify categories.\n\u00a0\nLearn from our experts:30 Jan 2021-Evertson CroesTraining: AWS AmplifyHave you always wanted to create a Cloud native app but you don\u2019t know where to begin? The Cloud can be an intimidating concept. In this training you will get a good introduction to the Cloud and you are going...\n", "tags": ["amplify", "aws", "cloud", "infrastructure as code"], "categories": ["Blog", "Cloud"]}
{"post_id": 30467, "title": "The Amplify Series, Part 2: How does AWS Amplify work?", "url": "https://www.luminis.eu/blog-en/the-amplify-series-part-2-how-does-aws-amplify-work/", "updated_at": "2023-05-08T13:08:44", "body": "In the previous blog, we got a high-level overview of AWS Amplify. In this article I will examine how AWS Amplify works and explain the \u201cmagic\u201d behind the scenes. While one of the advantages of Amplify is that you don\u2019t need to know any of this to develop a cloud-native application, it is essential to understand its inner working if you want to do more than just quick scaffolding.\nI have worked on several Amplify projects without digging deep to get effective. So if you want to get to just building a cloud-native application, you can skip this one. However, for those of you who stay, we will be covering the foundation on which Amplify is built upon, the concept and technologies used, and describe what actions happen in the background when you run specific commands in the Amplify CLI. It is important to note that the examples we give in these articles are very web app-oriented to keep the articles shorter. However, Amplify also supports mobile development with Android, iOS, and hybrid frameworks, and for the most part, the information here is still relevant.\nInfrastructure as Code (IaC)\n\nTo explain how AWS Amplify works, we first need to discuss the concept of \u201cInfrastructure as Code\u201d.\nBefore IaC, we would probably log in on a cloud console or web portal of some sort and start configuring our infrastructure by clicking around. Additionally, the cloud provider might have a CLI or another tool to help us provision our infrastructure. After many clicking and cli commands, we finally have our first version up and running.\nAfter testing this setup, we are ready to go to production. Here is where we encounter one of the first problems that IaC solves: How do we replicate the test environment we provisioned and tested? One option is to follow all the steps we did to create a replica. However, as you can already tell, this is very error-prone, and missing even one step might make a big difference.\nWith IaC, we solve this issue by describing these steps we made into some form of code. Once we have this in code, we can easily create a copy of the infrastructure using a provider that understands the code and can provide us with resources. There are also other benefits to IaC. To learn more about it, check out this blog post or this short video.\nMany tools provide IaC functionality, such as Terraform, Puppet, Google Cloud Deployment Manager, and Azure resource manager, to name a few. When it comes to AWS, we use AWS Cloudformation.\nAWS CloudFormation\nAWS Cloudformation is the Infrastructure as Code solution provided by Amplify. We can define our cloud environment, meaning the resources and their dependencies, in so-called \u201cCloudFormation templates\u201d. These templates are written in JSON or YAML. CloudFormation can take such a template and create a CloudFormation Stack, a set of related resources. For CloudFormation to find the template, it must be uploaded to S3.\n\nHowever, the good news is that Amplify will do all of the steps above for you. You just need to answer a few questions about what kind of functionality you need for your cloud-native app. Amplify will generate CloudFormation templates based on these answers, deploy them to S3 and use CloudFormation to create, update or delete the stack. We can log in to AWS and go to CloudFormations to look at our stacks at any time. Here is an example of a CloudFormation template generated by Amplify:\n\nSome items in the example above are minimized, but we can see that we are declaring an API resource with a path \u201c/players\u201d and \u201c/achievements\u201d. The entire file is 2200 lines long, only for the Amplify API category. Learning to do this by hand is a lot of work and error-prone. That is why several tools generate CloudFormation, such as Amplify, AWS CDK, and AWS SAM. Which tool you should use will be covered in the next blog. For now, it is essential to understand the basics of CloudFormation.\nConfiguring AWS Amplify CLI\nBefore you start using the Amplify CLI, you need to configure it to know which account it can use to create AWS resources using S3 and CloudFormation. You can do this by running \u201camplify configure\u201d in the command-line terminal. This will open a browser window for you to log in to the AWS console and walk you through the steps of creating an IAM account that has enough access to do everything that the Amplify CLI needs to do. \u00a0At the end of all of the steps, you must give the profile a name. This name will be used in the CLI moving forward with Amplify, and at some points, the CLI will ask you which profile you want to use for which commands.\nOnce this is done, the CLI will use these credentials to communicate with AWS to generate cloud resources via CloudFormation. Here is a diagram showing the actions that happen when you first configure your CLI and how the credentials are used to access CloudFormation and S3:\n\nInitialising an Amplify project\nWhen your CLI is configured, you can start creating your Amplify project. This is done by navigating to the root of your project and running \u201camplify init\u201d. The CLI will ask several questions, one of which is related to the credentials we configured. Once we select the correct profile, Amplify will work and perform several actions to create the Amplify project.\nChanges in AWS\nAfter running the amplify init command, the CLI will create an S3 bucket to store the CloudFormation templates, and it will create a CloudFormation stack that will point to the S3 bucket. Here is an example of a stack built after running amplify init:\n\nAs we can see, the DeploymentBucket is present, which will hold our CloudFormation templates. There are also two IAM roles created for the project. One for authorized users and one for unauthorized users. Once we begin adding more Amplify functionality and setting up authorization rules, these IAM roles will be updated to match what we configure via Amplify.\nAnother action that is taken by the CLI when running Amplify init is the creation of an Amplify project in the Amplify console:\n\nThis will become relevant once we start looking into hosting and adding CI/CD for our project.\nChanges in the project repository\nThe CLI also generates and updates several files in our repository, reflecting our choices made in the Amplify CLI. An extensive list of files generated by Amplify can be found here; however,\u00a0here is a summary of the files changed if you init for an Angular application:\n\namplify/cli.json: Feature flags for the CLI\namplify/README.MD: A list of links to helpful resources\namplify/team-provider-info.json: Information needed so that other team members can use Amplify. This file can be added to git if running a private application. However, it is advised to remove it or add it to the gitignore if it is a public project. Note: Even if someone gets the information in this file, they would still need the correct credentials to use the information.\namplify/.config/..: Configuration options set during the \u201camplify configure\u201d command\namplify/backend/backend-config.json: Information about resources and how they connect.\namplify/hooks/\u2026: Command hooks that can be used during the Amplify lifecycle, such as \u201cpre-push\u201d, \u201cpost-add-function\u201d etc.\nsrc/aws-exports.js: This file only exists for JS projects and is used by the JS libraries to know which resources they should communicate with.\n.gitignore: This file is updated by the Amplify CLI to ignore certain generated files and should not be stored in Git.\n\nThese changes should all be stored in Git to have a good starting point for your project. In general, changes made by the Amplify CLI should be committed to Git because those are the changes in your Infrastructure as Code setup that Amplify is updating for you based on your CLI choices.\nAdding backend functionality to project\nOnce the project is initialized with Amplify, we can add backend functionality.\nBackend functionality\nWhen we talk about backend functionality, we mean AWS Cloud services that will be generated by the CLI and consumed by our frontend application. For example, adding the Amplify (REST) API category will make use of AWS API Gateway. Adding the Amplify Auth category will make use AWS Cognito. Here is a diagram that shows a deployment of a REST backend created with AWS Amplify:\n\nHere we can see that the CLI generates a set of AWS resources. Each resource is part of an Amplify category. On the right side we can see that our frontend application will make use of the Amplify SDK to use these resources.\nUsing the CLI\nWe can add backend functionality by running \u201camplify add <<name of category>>\u201d in the CLI. In this case, if I run \u201camplify add auth\u201d I will get the following questions:\n\nNote that this is one of the more straightforward functionalities to add. When adding API, for example, you will have to answer many more questions. Once we are done running this command, there will be changes in our project:\n\namplify/team-provider-info.json: It will now contain a \u201ccategories\u201d object containing the \u201cauth\u201d category with identifiers of the resource.\namplify/backend/backend-config.json: It will now contain an object called \u201cauth\u201d with information related to the AWS service and how it needs to be configured, in this case, AWS Cognito.\namplify/backend/auth/<<some identifier>>/cli-inputs.json:\u00a0This file contains the choices you made via the CLI for the auth category. This is the file that will be committed in Git with the state of the auth category for this project. This is much better than adding the CloudFormation templates themselves to Git, as that would make fixing merge conflicts harder.\namplify/backend/auth/<<some identifier>>/build/<<some identifier>>-cloudformation-template.json: This is the actual CloudFormation template which will be used. This file is already added to the .gitignore by Amplify and will be regenerated when there are relevant changes.\namplify/backend/auth/<<some identifier>>/build/parameters.json: Contains environment specific parameters for the CloudFormation template. It should also be ignored in Git.\n\nIt is important to note that we have not yet done anything in AWS. These changes all remain local until the next step.\nPushing backend changes to AWS\nAfter adding and updating functionality that we need in our application via the CLI, we can push the local changes to AWS running \u201camplify push\u201d. This is where we will be making changes in AWS and generating the resources that will be needed to support the functionality we want. With this command, we will be generating several resources into our CloudFormation stack by just answering questions in a CLI.\nWhen running the command, the CLI will compare the state of the infrastructure in the S3 deployment bucket and the local changes and give a preview of the changes that will happen if the push continues. In the case of the auth category we added in the last section; the preview would look as follows:\n\nWe can see that new resources will be created for the auth category. If we choose \u201cyes\u201d then the CLI will update the S3 bucket with the new CloudFormation templates and use CloudFormation to create/update the stack for our Amplify project. One other important thing to note is that once the CLI starts pushing the changes to AWS, it will also update the aws-exports.js file with the new references so that the frontend libraries can make use of this.\nAt any point,\u00a0we can run amplify status to see the state of your current changes. After pushing the changes and running amplify status, we can see that we no longer have any changes:\n\nWe can also check out our CloudFormation stack to see all the new resources created.\nThis is the primary development cycle for working with AWS Amplify using the CLI. In short, we:\n\nMake local changes by running \u201camplify add <<category>>\u201d or \u201camplify update <<category>>\u201d\nWe run \u201camplify push\u201d and check if the changes make sense\nWe click on \u201cYes\u201d to let the push continue\n(Optional) We check CloudFormation to see if the stack is in the \u201cCREATE_COMPLETE\u201d or \u201cUPDATE_COMPLETE\u201d status\n\nConnecting backend and frontend\nBesides the CLI, Amplify also provides libraries so that your frontend application can make use of the generated resources in an easy way. We need to inform the frontend libraries of the backend resources we have generated and how to find them.\nTo get this working, we first need to install the libraries. You can find tutorials for all of the integrations here. For Angular, we can run:\n\u201cnpm install \u2013save aws-amplify @aws-amplify/ui-angular@1.x.x \u201c\nFurthermore, we need to configure the Amplify library with the aws-exports.js to know which resources it can use for which category of Amplify functionality. This is usually done somewhere in your frontend code in the earliest part of the app lifecycle. For Angular, we add it in the src/main.ts:\n\nOnce this is done, we can run functionality such as \u201cAmplify.Auth.register(userInfo)\u201d to register a new user. We will look into more concrete examples in the following blogs.\nThe complete flow of adding Amplify functionality and using it can be illustrated in the following way:\n\nUp next\u2026\nIn this blog we looked at how AWS Amplify works behind the scenes. If you are considering using Amplify for real life projects, this information will help you have the confidence to make changes and fix issues. In the next blog, we will be looking at why you should consider using AWS Amplify, when to use it, when not to use it and when you should look at alternatives. See you in the next one!\nLearn from our experts:30 Jan 2021-Evertson CroesTraining: AWS AmplifyHave you always wanted to create a Cloud native app but you don\u2019t know where to begin? The Cloud can be an intimidating concept. In this training you will get a good introduction to the Cloud and you are going...\n", "tags": ["amplify", "aws", "cloud", "infrastructure as code"], "categories": ["Blog", "Cloud"]}
{"post_id": 30110, "title": "Creating your serverless web-application using AWS CDK \u2013 part 2", "url": "https://www.luminis.eu/blog-en/creating-your-serverless-web-application-using-aws-cdk-part-2/", "updated_at": "2021-12-21T10:58:40", "body": "Welcome to part two of my blog about creating a serverless web-applications using AWS CDK. In the first part of this blog I explained why I prefer to use infrastructure as code and AWS CDK.\nA short recap: we use infrastructure as code because it gives us the opportunity to maintain our infrastructure with greater ease, as opposed to configuring our resources manually using the AWS console. My infrastructure tool of choice in this is AWS CDK. I chose AWS CDK because I can use it to define infrastructure in a programming language I already know, and because it offers me a great balance between flexibility when I need it and great defaults so I do not have to do too much work myself.\nIn the first part, we also created the basic setup for our serverless web-application with AWS CDK. However it is not quite ready for us to use in production.\nTo get the application production ready, we need to address the following:\n\nThe url to the application is something.s3-website-region.amazonaws.com. This does not help users easily reach the application.\nThe application does not offer a HTTPS connection for the frontend, so access to the application is not secure.\nThe current application has no user authentication. You can access the data of any user by simply entering the correct username.\n\nLeveraging the cloud\nIn addressing the topics I just mentioned, we can see some of the true potential of serverless applications built in the cloud. Setting up HTTPS with certificates, DNS or user authentication can be a lot of work. Even if you decide to use existing services to save you from doing all the work yourself, you will probably still wind up with more manual steps than you would care for. Is that not what we set out to prevent in the first place? Using Amazon\u2019s Web Services you really can use the hard work of others who have come before you. Join me as I show you how adding three new services with just a few lines of codes can bring all we need to get our application ready for production.\nOur production setup will look like this:\n\nWe will use Route53 to link a custom domain we own to the application. This step also allows us to create certificates that we need to enable HTTPS for the frontend. To get HTTPS going for our frontend we also need CloudFront. Finally we add Cognito for user management and authentication. The frontend will redirect users to Cognito to login and receive authentication tokens. These tokens will be validated by API Gateway using a new Lambda function we add.\nDomain\nTo allow users easy access to our application and to create HTTPS certificates, we need to have a custom domain. We also need to have access to the public hosted zone for that domain. A hosted zone is a concept used in Route53. It represents a collection of DNS records that can be managed together, belonging to a single parent domain name. The easiest way to get a custom domain that is ready to use with AWS is to register one through Route53 in the AWS portal. If you register a domain this way, AWS automatically creates a public hosted zone for that domain for you.\nAll we need to get access to it in the TodoApplicationStack is install and import @aws-cdk/aws-route53 and add this code:\n\nconst hostedZone = route53.HostedZone.fromLookup(this, 'TodoApplicationHostedZone', {\r\n  domainName:'tomhanekamp.com'\r\n});\nWhat we do here is a lookup of a hosted zone based on the domain name. For my to do list application I registered tomhanekamp.com as a domain. A condition for this lookup is that the hosted zone is registered in the same AWS account that you use to deploy the CDK stack. Also, the account and region need to be passed into the TodoApplicationStack, which is done by using environment variables. For my to do list application I chose to specify the environment by putting the following code into my bin/todo_application.ts:\n\n#!/usr/bin/env node\r\nimport 'source-map-support/register';\r\nimport * as cdk from '@aws-cdk/core';\r\nimport { TodoApplicationStack } from '../lib/todo_application-stack';\r\n\r\nconst app = new cdk.App();\r\nnew TodoApplicationStack(app, 'TodoApplicationStack', {\r\n  env: {\r\n    account: process.env.CDK_DEFAULT_ACCOUNT,\r\n    region: process.env.CDK_DEFAULT_REGION\r\n  }\r\n});\nBy doing this, I make CDK take the default account and region from my AWS CLI configuration.\n\n\nCertificates\nThe next thing we do for the production set-up is get some certificates for the HTTPS connections to the frontend and the API. You might wonder why we need a certificate for the API as well since it already allows HTTPS connections. While this is true, the API can currently only be accessed on a amazonaws.com domain. Since we are setting up a custom domain anyway I also want the API to be accessible on this domain. As the domain name is embedded in the certificate, this means we need to create a new one.\n\nTo create the certificates, we need @aws-cdk/aws-certificatemanager installed and imported in the TodoApplicationStack. We also add this code:\n\nconst frontendCertificate = new acm.DnsValidatedCertificate(this, 'TodoApplicationFrontendCertificate', {\r\n  domainName: 'todoapplication.tomhanekamp.com',\r\n  hostedZone: hostedZone,\r\n  region: 'us-east-1'\r\n});\r\n\r\nconst apiCertificate = new acm.DnsValidatedCertificate(this, 'TodoApplicationApiCertificate', {\r\n  domainName: 'todoapplication-api.tomhanekamp.com',\r\n  hostedZone: hostedZone,\r\n});\nWith the code above we create two certificates. These certificates will be automatically validated using DNS validation. To make the validation succeed, we specify the Route53 hosted zone we looked up earlier. Apart from the hosted zone we also specify the domain names for the certificates. I use the subdomain todoapplication.tomhanekamp.com for the frontend and todoapplication-api.tomhanekamp.com for the API. If you wonder why the frontend certificate has a third property where we set the region to \u201cus-east-1\u201d, well spotted. This is due to a requirement of CloudFront that certificates requested through ACM for usage with CloudFront should be requested for this region.\nCloudFront\nNow that we have the certificates we need. We can set up the CloudFront distribution. To do this, we install and import @aws-cdk/aws-cloudfront and the following to TodoApplicationStack:\n\n\n\nconst distribution = new cloudfront.CloudFrontWebDistribution(this, 'SiteDistribution', {\r\n  originConfigs: [\r\n    {\r\n      s3OriginSource: {\r\n        s3BucketSource: frontendBucket\r\n      },\r\n      behaviors : [ { isDefaultBehavior:true } ],\r\n    }\r\n  ],\r\n  viewerCertificate: {\r\n    aliases: [ 'todoapplication.tomhanekamp.com' ],\r\n    props: {\r\n      acmCertificateArn: frontendCertificate.certificateArn,\r\n      sslSupportMethod: \"sni-only\",\r\n      minimumProtocolVersion: \"TLSv1.2_2021\"\r\n    }\r\n  }\r\n});\nThe first setting we enter into the CloudFront web distribution, is a set of origin configs. These configurations specify the location of the source files for the distribution as well as some configuration for how to handle the origin. To set up our web application we need only a single origin, which is the S3 bucket hosting the frontend. In terms of behaviour we do not actually need to change anything from the defaults. However we must specify at least one default behaviour, so that is what we do.\nThe other thing we set onto the distribution is that we enable HTTPS. This is after all the main reason we included a CloudFront distribution in our deployment in the first place. We do this by specifying a viewer certificate. Our viewer certificate has a single alias, which in my case is todoapplication.tomhanekamp.com, the hostname for the frontend.\nWe also give it a couple of properties. First we specify the certificate ARN to be the one of the frontend certificate that we created earlier. The next setting is pretty important if you want to avoid a high bill from Amazon. The SSLSupportMethod setting offers two options, sni-only and vip. The first option makes the distribution accept HTTPS only from clients that support server name indication. SNI is supported by most browsers and clients and therefore should suit most use cases just fine. The second option, \u201cVIP\u201d, makes AWS reserve a dedicated IP address for your endpoint, and they will charge you for that. This option comes at a $600 a month prorated surcharge. Finally we also specify the minimum security protocol version. I choose the highest available and recommended option here.\nWith the CloudFront distribution in place, we now have HTTPS support for our application completely finished.\nDNS records\nEarlier in this blog we registered our custom domain and we looked up the public hosted zone that came with it. We now complete the process of adding a custom domain by creating alias records for the frontend and API. To learn more about alias records and when to use them, I recommend this page.\nThe first thing we do to start is install and import @aws-cdk/aws-route53-targets.\nFrontend\nLinking our custom domain to the frontend requires little work. We already configured the CloudFront distribution with the right settings. Now all we do is add the following code to the stack:\n\n\nconst websiteARecord = new route53.ARecord(this, \"TodoApplicationAPIRecord\", {\r\n  recordName: 'todoapplication.tomhanekamp.com',\r\n  zone: hostedZone,\r\n  target: route53.RecordTarget.fromAlias(new route53Targets.CloudFrontTarget(distribution))\r\n});\n\nThis adds a DNS \u201cA record\u201d to the stack. To set the domain name for the record we specify it as record name and we link it to the hosted zone we looked up earlier. Finally we set a target. The target we use is a so called AliasRecordTarget.\u00a0 To create it, we set the CloudFront distribution we created earlier as input. This points the record to the distribution. At the same time, because we used an AliasRecordTarget, it turns the record from a regular \u201cA record\u201d into an \u201calias record\u201d.\nAPI\nTo link our custom domain to the API, we first need to add some configuration to the API Gateway we added in the basic setup. Back then we added a RestApi construct and we only set the name of the API Gateway. We now change that code to the following:\n\nconst apiGateway = new apigateway.RestApi(this, 'TodoApplicationApiGateway', {\r\n  restApiName: 'TodoApplicationApi',\r\n  domainName: {\r\n    domainName: 'todoapplication-api.tomhanekamp.com',\r\n    certificate: apiCertificate,\r\n    securityPolicy: apigateway.SecurityPolicy.TLS_1_2\r\n  }\r\n})\nThe additions we make here configure a custom domain for the API Gateway. We set the domain name that will be used to reach the API Gateway, in my case todoapplication-api.tomhanekamp.com. We also configure the certificate for HTTPS on the API to be the apiCertificate we created earlier. Finally we set the minimum required TLS version for the HTTPS connection to the API Gateway. I set this to TLS 1.2 which is the safest available option at this time.\nAfter making this change to the API Gateway, we add another alias record to the stack using the following code:\n\n\nconst apiARecord = new route53.ARecord( this, \"TodoApplicationAPIRecord\", {\r\n  recordName: 'todoapplication-api.tomhanekamp.com',\r\n  zone: hostedZone,\r\n  target: route53.RecordTarget.fromAlias(newroute53Targets.ApiGateway(apiGateway))\r\n});\n\nYou will notice that this code is very similar to that for the frontend DNS record. The only changes are the record name and the target. We still create an AliasRecordTarget as the target, but this time we set the API Gateway as input.\nUser authentication\nThe last bit of functionality we add to make the application ready for production is user authentication. We do this by adding a Cognito user pool to our deployment where users can sign up and log in to retrieve tokens. We also need to add a functionality to our API that allows it to verify these tokens.\nUser pool\nTo add a user pool, we need to import @aws-cdk/aws-cognito. We can then add the following to TodoApplicationStack:\n\nconst userPool = new cognito.UserPool(this, \"TodoApplicationUserPool\", {\r\n  selfSignUpEnabled: true,\r\n  signInAliases: { email: true },\r\n  autoVerify: { email: true },\r\n  removalPolicy: cdk.RemovalPolicy.DESTROY\r\n});\r\nconst userPoolClient = userPool.addClient(\"TodoApplicationUserPoolClient\", { \r\n  oAuth: { \r\n    flows: { \r\n      authorizationCodeGrant:true, \r\n    }, \r\n    scopes: [ cognito.OAuthScope.OPENID ], \r\n    callbackUrls: [ `https://todoapplication.tomhanekamp.com/` ], \r\n    logoutUrls: [ `https://todoapplication.tomhanekamp.com/` ] \r\n  } \r\n});\r\nuserPool.addDomain(\"TodoApplicationCognitoDomain\", {\r\n  cognitoDomain: {\r\n    domainPrefix: \"todo-application\",\r\n  },\r\n});\r\n\n\nThe first piece of code adds the user pool. In the case of my application, I want anybody to be able to sign up for it using their email addresses without the need for manual verification. This reflects in the settings I pass into the user pool.\nFirst of these settings is selfSignUpEnabled. If you set this to false, users can only be created by an administrator of the user pool. The second setting I add is signInAliases. This setting defines the methods users can use to sign up and sign in to the user. As I mentioned earlier, I want user to use their email address. The third setting to the user pool is autoVerify. When users sign up to the user pool, they account will need to be verified. You can choose to allow this only as a manual action by an administrator. You can also opt to have Cognito automatically verify users by sending them a verification link via email, which is what I did here. Finally I set the removalPolicy for the user pool to destroy, as the default is \u201cretain\u201d.\nWith the user pool added, we register our application as app client to the user pool. This allows it to call the API\u2019s of the user pool to register and sign in users. When registering the application as client we specify the OAuth settings for this client. How to best set up OAuth for your application is topic that deserve a blog post of its own and has little to do with using CDK. You can find some explanation in the AWS documentation for configuring an app client and even more detail here.\nThe last bit of code configures the user pool domain. In this case I opted to use an Amazon Cognito hosted domain with todo-application as domain prefix. This means the urls for the sign-up and sign-in pages of my user pool are in a domain from Amazon Cognito, such as amazoncognito.com.\nVerifying tokens\nIn order to verify the tokens that are sent to the API, I use the Cognito user pool that we created as authorizer. I configure this authorizer for the methods in the API. Every time these methods are called, this authorizer is called upon to verify the token.\nWe add this authorizer to the TodoApplicationStack with the following code:\n\n\nconst authorizer = new apigateway.CognitoUserPoolsAuthorizer(this, 'TodoApplicationAuthorizer', {\r\n  cognitoUserPools: [userPool]\r\n});\n\nWith this code we first add the authorizer itself. We do not need to configure a lot of settings for it, all it needs is a reference to the user pool where the tokens will be verified.\nThe second thing we do is that we add this authorizer to the API. We do this by altering a piece of code we added in the first part of the blog:\n\n\nitemResource.addMethod('PUT', new apigateway.LambdaIntegration(addItemLambda), {\r\n  authorizer: authorizer\r\n})\r\n\r\nitemResource.addMethod('GET', new apigateway.LambdaIntegration(getItemsLambda), {\r\n  authorizer: authorizer\r\n})\n\nIn the first part of this blog we added the HTTP GET and PUT methods to the API and we specified the Lambda Function to call for these methods. Back then we did not specify any options for the method. Now however we specify one option, the authorizer.\nConfig JS\nWith all the resource in place for our production set up, we have one thing left to do before we can deploy it. In the previous blog we added code to upload a config.js file with the API url to S3. In this production setup, our frontend also needs to information about the user pool client we added. To get this information to the frontend we add it to the contents of the config.js file.\nReplace the code for the frontendConfig with the following:\n\nconst frontendConfig = {\r\n  serverUrl: `https://todoapplication.tomhanekamp.com/`,\r\n  region: 'eu-west-1',\r\n  cognitoClientId: userPoolClient.userPoolClientId,\r\n  cognitoDomain: 'todo-application',\r\n  itemsApi: 'https://todoapplication-api.tomhanekamp.com/',\r\n  lastChanged: newDate().toUTCString()\r\n};\n\nYou will see that we added four new properties to the configuration. These properties are all used by the CognitoService in the frontend to send requests to the user pool. You might notice that only one of the properties, the cognitoClientId, that are now in the frontend configuration comes from CDK instance in our stack. The other properties are static values. This means we could also just hard code them somewhere in the frontend. However, even though the values of these properties are static now, when we change the domain or region where the application is deployed they would still change. I therefore keep these properties in the frontendConfig. This also helps me if I decide to configure multiple environments for my application in the future as all the environment specific code is in TodoApplicationStack.\nBecause we added a property to the frontenConfig that relies on userPoolClient, we should also add a dependency for that:\n\ns3Upload.node.addDependency(userPoolClient);\n\nDeploying\nNow that all the code is complete, we can deploy our application. Once again let\u2019s make sure the compile the code we deploy to AWS. In my project I do this with:\nnpm install\r\nnpm run build\non the frontend and Lambda functions as they are NodeJS based. With everything compile we deploy the stack with:\ncdk deploy\nIf that passes, you can go to the custom you registered to open your production ready application.\nMy Todo list application now looks like this:\n\nIt doesn\u2019t look all that different from what I showed at the end of the previous blogs. However if you look closely you will see signs of all that we have accomplished in this blog.\nFirst of all, the hostname I used to reach it is now todoapplication.tomhanekamp.com\u00a0because of the custom domain we added. A big improvement from todoapplicationstack-todoapplicationfrontend8b34e-65bjgh3il4ru.s3-website-eu-west-1.amazonaws.com that it used to be when it comes to being easy to find for my users.\nIf you look to the left of the hostname in the address bar of my browser, you will also see a padded lock where it previously said \u201cNot Secure\u201d. This means I am now connected to the application using HTTPS and the certificate is considered valid by my browser.\nFinally you might notice that the application no longer has an input field for the username and that a logout button was added instead. When I first opened the application after deploying it, it actually looked like this:\n\nWhen you click on the Login button, the application directs you to Cognito. Here you can sign up for an account that you verify via email. After you sign in, the email address you used to sign up is also your username.\nWith these changes we have finished what we set out to do. Our application is now ready to be used in production.\nYou want to have a closer look at some of the code, you can find the complete solution on GitHub.\n", "tags": ["aws", "cdk", "cloud"], "categories": ["Blog", "Cloud"]}
{"post_id": 30557, "title": "Move faster with new feature flag capabilities in AWS", "url": "https://www.luminis.eu/blog-en/cloud-en/move-faster-with-new-feature-flag-capabilities-in-aws/", "updated_at": "2021-12-16T12:16:27", "body": "I always get excited about things that allow you to bring new ideas to production faster. And it seems we are in luck! During the yearly AWS re:Invent conference that took place in Las Vegas, AWS launched several services that help you do just that!\nBusiness around us is changing more and more rapidly. Year by year, the State of Devops research confirms that high performing companies excel in adapting to those changes by optimizing their ability to experiment, deliver quickly and gather feedback directly. You need to try many ideas and quickly find out which are successful and which are not. That ability to make many small changes reduces risk and increases the ability to innovate.\nThe new AWS services around feature flags enable organizations to achieve these goals. I will explain these briefly and show you how you can leverage them.\n\nMoving fast\nSo what ingredients do you need to go fast? First, you have to optimize your organization and teams so they can come up with diverse ideas. Make sure you enable them to try those out and get feedback quickly without having to go through committees to get approval.\nSecond, on the engineering side, you need to learn to do everything in small steps. Integrate all your changes with the rest of your team often, at least daily. This way of working is called trunk-based development. The key here is never letting go of high quality and keeping your software in a constant state of being able to deploy it to production. To achieve that, you need a (continuous) delivery pipeline that automates all the steps needed to get from commit to deploy. As excellently explained by Dave Farley, the key responsibility of that pipeline is not to ensure that the software is 100% correct \u2013 that is not realistic. Instead, the pipeline needs to do all that it can to figure out if something might be broken or misbehaving. That means you\u2019ll need at least a solid automated test suite that provides you with confidence that on the functional side everything works. Next to that, you can add validations to your pipeline for things like performance, code quality, and any other aspects that are important for your application.\nIf you do everything in small steps, does that mean you cannot have big innovations? No, of course it doesn\u2019t. Big innovations never happen in one go. They are the combination of smaller increments, even the failed ones that you learned from. Like Edison\u2019s light bulb, which was the result of thousands of failed experiments.\nFeedback, as early as possible\nA common trick is to separate the deployment of changes from the release of a feature. By using feature flags, you can include the new code you are writing without the feature showing up or being active. With all the code being deployed continuously, the act of \u201creleasing\u201d a feature simply becomes toggling the feature flag.\nThis way of working helps greatly in catching mistakes or wrong assumptions in your ideas early, while it is still easy to try another approach. It is much better to discover that your idea does not work after a few hours than finding out weeks later, when you\u2019ve already invested a lot of time. You also make it much easier to experiment with new features much earlier in their development, maybe just exposing them to a few interested beta users and receive feedback before you make the feature generally available.\n\nEnter feature flags\nI always like keeping things simple. The easiest way to implement feature flags is to just add some booleans and \u2018if statements\u2019 in your code and allow these to be toggled between the different environments you have. For example, showing the new feature you are working on in your staging environment, but hiding and deactivating it in production. To be honest, in many cases, a simple mechanism like this is more than sufficient.\nDo realize there is a tradeoff when using feature flags. Having these branches in your code means you add complexity and you make your application less predictable. In many cases the benefits greatly outweigh the downside, as long as you keep it under control. Don\u2019t let hundreds of feature flags linger around. Be responsible, be a good craftsman and keep your codebase clean. Once a feature is released, clean up all traces of the flag. That way you minimize complexity and keep your codebase lean and mean and easy to change. After all, you want to keep doing more experiments and keep delivering value in a sustainable way.\nDepending on how long your build pipeline takes, having to change booleans in code may mean that it takes a while to toggle a feature in production. This may be ok in some cases, but when it turns out that a feature doesn\u2019t play nice on production yet, you want to toggle it off again quickly. What if you could toggle it in real-time and let someone else handle part of the complexity around feature flags?\nLaunch 1: AWS AppConfig \u2013 Feature Flags\nAWS AppConfig has been around since 2019 and allows you to roll out configuration changes to your applications in real-time, without having to deploy code or take your application out of service. It can validate and then roll out configuration changes gradually while monitoring your application and rolling back changes in case of errors, minimizing impact to users.\nRecently AWS launched AWS AppConfig Feature Flags, which builds on AppConfig and makes it easier to roll out new configurations of your feature toggles. You can now setup and toggle feature flags directly from the AWS Console. Versions of your feature flags configuration are tracked and as with config changes, you can manage and monitor the gradual deployment to your environments.\n\nAs feature flags are just booleans, there is no validation around them. You can also add custom attributes to flags. An example of what I would use these for is creating flags that are only enabled for specific beta tenants. For these attributes you can set up some simple validation rules in the UI. That may not be as powerful as writing a validation lambda as you can do for normal AppConfig validation, but it sure is a lot simpler and probably sufficient. And don\u2019t forget, less handcrafted code means you reduce risk.\n\nApart from feature flags, you can also use this feature to toggle operational aspects of your application quickly. For example, in case of an incident you can temporarily switch off non-critical functionality or behavior to reduce load and keep the essentials of your service working, allowing your team to focus on resolving the incident.\nThat all sounds fantastic, but it does mean that releases of features are no longer tracked in your central git history, instead they are changed through the AWS Console. Luckily, the Console does track versions of your configurations and shows you the history of feature flags and configuration deployments that have been done to each of your environments. So, it may not be in one place anymore, but you can still audit when features went live or were rolled back. Just be aware that you are making this tradeoff.\n\nLaunch 2: AWS CloudWatch \u2013 Evidently\nWhere AppConfig Feature Flags is geared towards simplifying the technicality of managing feature flags and their deployment, Evidently is all about connecting the data you collect for experiments and A/B tests around feature roll outs that you do with smaller sets of users before making the feature generally available.\nEvidently allows you to set up multiple variants of a feature and test these variants on different subsets of users at the same time. Once you have set up and are running such an experiment, Evidently also helps in statistically analyzing which variant is performing better, based on data collected during the experiment, giving you confidence in making the right choices.\n\nIn today\u2019s fast paced world, you have to continuously reinvent your business to remain relevant. To do so you have to innovate and Evidently really helps you with tools to apply that mindset of running experiments and basing your decisions on the data that you gather.\n\nLaunch 3: Amazon CloudWatch RUM (Real User Monitoring)\nNow obviously, to make a service like Evidently work well, you need to collect the right data. Data that helps you prove the effectiveness and desired behavior of the features you build. To help with this, AWS launched RUM, another service under the CloudWatch umbrella.\nCloudWatch is all about monitoring, but so far that was mostly oriented towards raw infrastructure and backend metrics of apps running inside AWS. RUM enhances that with capabilities to monitor web application performance, aimed at user experience. You inject some JavaScript into your web application frontend which then allows you to anonymously collect page load and layout performance metrics, JavaScript errors and client-side experienced HTTP errors.\n\nWhere traditional CloudWatch metrics are somewhat raw, RUM feels more like Google Analytics, giving you out-of-the-box dashboards that provide visibility into your application\u2019s reliability, performance, and end user satisfaction. The service performs analysis on the collected data and breaks it down so you can gain insights about which areas of your application\u2019s user experience can be improved.\n\nOn the observability side, CloudWatch RUM integrates with AWS X-Ray, so you can view traces and segments for end user requests. You can even see your clients in CloudWatch ServiceLens service maps.\n\nPricing\nAs usual with AWS, pricing for these services are based on usage, which means you can try these out on a small scale and then evaluate if the benefits they bring you are worth the cost. Once you use these services in production, always put up some cost monitoring so you remain aware of what you are spending. Below you will find an overview of the pricing for these services. For details, look at their pricing page.\nAWS AppConfig Feature Flags are charged by the number of times you request and receive config changes:\n\n$0.0000002 per configuration request via API Calls\n$0.0008 per configuration received\n\nAWS CloudWatch Evidently has a first time free trial that includes 3 million Evidently events and 10 million Evidently analysis units per account. After that you pay:\n\n$5 per 1 million events (user actions & assignment events)\n$7.50 per 1 million analysis units\n\nAWS CloudWatch RUM allows you to control costs by configuring it to only analyze a limited percentage of user sessions. There is a first time free trial that includes 1 million RUM events per account. After that you pay:\n\n$1 for every 100,000 events collected (page view, JavaScript error, HTTP error)\n\nAlternatives\nCompared to products that specialize in this field like LaunchDarkly or Optimizely, these AWS services may feel somewhat bare bones. This is no coincidence, as Werner Vogels said: AWS services are designed to be building blocks, not frameworks.\n\nAWS may not be as feature rich as some of these others but having it all integrated into the rest of your service stack can be a big benefit. The low barrier to entry means you can start using it today and get value out of it quickly, or switch to something else later if you discover that your needs have grown.\nAnd don\u2019t forget, if you don\u2019t need all the experimentation statistics or real-time flag controls, just keep it simple and add a few booleans in your code.\nConclusion\nIt is great to see that AWS is serious about giving their users the keys they need for innovation. What I personally find most exciting is that with this combination of services, AWS makes it easier for all of us to adopt that mindset of doing experiment-based and data-driven roll outs.\nIf you need help to introduce that mindset of experimentation, don\u2019t hesitate to reach out to us!\nTo learn more about these AWS services, check out the official blogs:\n\nIntroducing AWS AppConfig Feature Flags In Preview\nNew \u2013 Amazon CloudWatch Evidently \u2013 Experiments and Feature Management\nNew \u2013 Real-User Monitoring for Amazon CloudWatch\n\n", "tags": ["aws", "AWS re:Invent 2021", "cloud"], "categories": ["Blog", "Cloud"]}
{"post_id": 30355, "title": "The Amplify Series, Part 1: What is AWS Amplify?", "url": "https://www.luminis.eu/blog-en/cloud-en/the-amplify-series-part-1-what-is-aws-amplify/", "updated_at": "2023-05-08T13:09:15", "body": "The way we develop software is rapidly and increasingly changing with the world around us. One of the more exciting developments is the advent of serverless technology, which, when applied correctly, opens doors to organizational agility and speed \u2014 and thus innovation and happy users.\nOne of the quickest ways to get familiar with this way of working is using AWS Amplify, a set of powerful tools and features that can help you quickly and easily build cloud-powered, extensible, and scalable full-stack applications.\nAmplify has selected a subset of all AWS services and default settings to help you quickly get started with building your Cloud Native application and thus provide value at a higher pace. This might seem like you are losing control over your application and that you will be limited somehow; however, with AWS Amplify, you can still override any defaults and access any AWS service if you need to.\nBlog series\nIn this blog series I will introduce AWS Amplify, explain why you would want to use it, how it works, and give detailed examples of the several categories of functionality available out-of-the-box with AWS Amplify.\nThe entire series will cover the following topics:\n\nWhat is AWS Amplify?\nHow does AWS Amplify work?\nWhy should you use AWS Amplify?\nDeveloping and deploying a Cloud Native application with AWS Amplify\nUploading and retrieving images with Amplify Storage\nUsing the power of AI and Machine Learning with Amplify Predictions\nTrack app usage with Amplify Analytics\nLocation functionality with Amplify Geo\nChatbots with Amplify Interactions\nDo everything with AWS Amplify! (Extensibility)\nLow Code with Amplify Studio\n\nWe hope you will enjoy this series and extend your AWS Amplify knowledge.\n\u00a0\nWhat is AWS Amplify?\nIn this blog, we will give a high-level overview of the functionalities and tools provided by Amplify.\nFunctionality categories\nWith Amplify, we are more concerned with what we want to build instead of how it will work with AWS. Amplify offers the possibility to generate AWS services based on a set of functionality categories. At the moment of writing, these categories are:\n\nAuthentication: Fast and easy authentication and authorization out-of-the-box\nAPI: Create and use a REST or GraphQL API as a gateway to other services in AWS, such as Lambda functions or databases\nStorage: Used for storing and retrieving user-generated content on AWS such as photos, videos, or other files\nGeo: Location-based functionality such as maps and location search\nHosting:\u00a0Hosting your app on AWS and setting up a CI/CD pipeline\nInteractions: Create chatbots for your application\nPubSub: Connectivity with cloud-based message-oriented middleware\nDataStore: Use on-device persistent storage to be able to use your application when offline\nFunctions: Create Lambda functions that are linked to an API or create CRON jobs\nAnalytics: Add tracking of user behavior of your apps\nAI/ML Predictions: Use AI/ML to add functionality such as text translation, speech-to-text, and entity recognition in images\nPush Notifications: Send messages to your customers to improve engagement\nExtensibility: Leverage any other AWS service that is not covered by the categories above by using AWS CDK\n\nAs you can see, the functionality categories already cover many everyday use cases for web and mobile apps while still offering flexibility by leveraging CDK.\nTools\n\nAWS Amplify consists of 4 tools that help you develop cloud-native applications. These are:\n\nCLI: The Command-line interface that you can use to initialize your Amplify project and generate AWS resources\nLibraries/SDK: The set of libraries used by your frontend of any kind to more easily be able to communicate with the AWS resources created with Amplify\nConsole: The AWS service where you can keep track of your Amplify projects and configure many settings related to the deployment and hosting of your AWS Amplify app\nAmplify Studio: Amplify Studio allows you to configure your AWS resources using the console and visual modeling. Everything you change here is still compatible with the CLI, and you can even use the CLI to download the resources you designed in the studio.\n\nCLI\nWith the command-line interface (CLI), you can run a few commands, you will be asked specific questions about the functionality you want to achieve, and it will generate the AWS resources based on your answers. Here is an example of such an exchange with the Amplify CLI:\n\nIn this example, we add a REST API to my project using the Amplify CLI. Depending on what functionality you want to add, you will need to answer different questions.\nIn the following blogs in this series, we will be using the CLI a lot and show you everything from starting a project to adding several functionalities. For now, it is important that you have a broad idea of how the CLI works.\nLibraries (SDK)\nWe can use the Amplify libraries to connect our app to AWS resources we created using the CLI. There are libraries for all of the most popular frameworks to develop web and mobile apps:\n\nThe combination of the CLI and libraries makes it possible for your app to use the AWS resources instantly, without knowing the IP or ARN for every resource you created. Here is an example of making a REST call to an endpoint created with the AWS Amplify in JavaScript:\n\nThere are helper functions for most functionality categories covered by Amplify that help you use the AWS resources you have created via the CLI.\nConsole\nJust like services such as API Gateway or AWS Lambda, AWS Amplify is can now also be considered a first-class citizen and can be found as a service in the AWS console. Here is an example of what it looks like when you have active projects:\n\nThe following is a list of things you can do on the Amplify console:\nCI/CD Pipeline\nYou can set up a CI/CD pipeline via the Amplify CLI and track the progress via the Amplify Console. Here is a screenshot of what it looks like:\n\nThe default CI/CD pipeline setup has these four steps. However, it is fully configurable either in the Amplify console or by adding a .yml file in the root of your Amplify project. If the project builds and is deployed successfully, you will even see a preview of your app, and you will be provided with a link to the hosted app.\nConnect Git Branch\nYou can connect git branches from your projects to an AWS Amplify environment you have created with the Amplify CLI. Once connected, your app can be built every time you push to that branch, and the frontend and backend environment it is related to will be deployed.Domain management\nBy default, you will get a cloudfront.net URL on which your application is hosted when you first start. However, in the Amplify Console, linking a domain you have registered to a specific git branch of your project is possible. In the case shown above, every time something is merged to develop, a build is run, and once it is deployed, it is available at https://cloudthegame.com.\nConfigure build settings\n\nYou can change the build steps in the console as previously mentioned. You can configure custom webhooks to trigger builds for specific branches, and you can configure the settings for the Amazon Linux container that will run the build. Setting up environment variables is also possible.\nView metrics\n\nWhile services created with the Amplify CLI, such as API Gateway and Lambda, have their metrics, the Amplify console shows you metrics related to your hosted application. You can see metrics such as viewer requests, bytes downloaded, and errors.\nAmplify Studio\nThe Amplify Studio allows you to achieve the same functionality you could using the CLI. The difference is that instead of answering questions in a CLI, you are clicking in the console. You will see that all functionality categories have tabs that you can configure. Once you are done configuring your backend, you can \u201cpull\u201d the backend to your local Amplify project and start using it in your application in the same way you would if you generated the backend using the CLI.\nHere is a screenshot of the studio where we designed a data model for our pizza restaurant:\n\nThere is also a set of functionalities that is exclusive to the Amplify Studio, such as:\n\nUI Library: You can make component UI designs in Figma and generate ReactJS component code. You can even link the components to backend logic in the studio\nData and files:\u00a0You can manage your database (CMS), files, and user/groups.\nAuthentication:\u00a0You can give users access to your Amplify studio environment without creating an AWS account. This can be useful if you have clients who want to manage the data for their application.\n\nIn short, Amplify Studio aims to be a visual development environment. While it is low-code, it is still backed by human-readable code and AWS CloudFormation, the infrastructure-as-code solution on AWS. We will talk more about CloudFormation in the next blog.\nUp next\u2026\nWe hope this blog post has given you a clear overview of Amplify and has gotten you excited and interested in learning more. We can achieve a lot with Amplify out-of-the-box to create a cloud-native application using the most popular Javascript and hybrid mobile frameworks and native Android and iOS. In the next blog, we will look at how AWS Amplify works to achieve all of this functionality. In the subsequent blogs, we will be using Amplify to develop and deploy a cloud-native application. See you there!\nLearn from our experts:30 Jan 2021-Evertson CroesTraining: AWS AmplifyHave you always wanted to create a Cloud native app but you don\u2019t know where to begin? The Cloud can be an intimidating concept. In this training you will get a good introduction to the Cloud and you are going...\n", "tags": ["amplify", "aws", "cloud", "infrastructure as code"], "categories": ["Blog", "Cloud"]}
{"post_id": 30493, "title": "Top Takeaways from AWS re:Invent: 2021 Keynotes and Announcements", "url": "https://www.luminis.eu/blog-en/cloud-en/top-takeaways-from-aws-reinvent-2021-keynotes-and-announcements/", "updated_at": "2021-12-08T09:57:23", "body": "\n\nAs a cloud enthusiast, I like this time of the year. Early December, just before the Christmas holiday, the anticipation of \u2018getting presents\u2019 from my favorite cloud provider with announcements and extraordinary sessions.\nThis year\u2019s re:Invent took place in Vegas, in-person. Of course, fewer people were on-site because of the Covid measures. I participated virtually and followed all the keynotes, announcements, and leadership & tech sessions. I digested the event, collected my notes and thoughts, then came up with my key takeaways from the keynotes and the announcements. I would like to share these with you today:\n\n\n\n\n\nNo game-changer announcements but improvements\nI have to admit I missed Andy Jassy. This year\u2019s event was the 10th, and the first time we didn\u2019t have his usual speech. Instead, the new CEO of AWS, Adam Selipsky, did the main keynote given in high spirits and with top customer stories. But the speech lacked any significant announcements.\nA reason for this is that we are getting to the point that technology is not as new anymore. Instead, it is mature, as this year was the 15th anniversary of AWS. Or, as Dr. Werner Vogels said in his keynote, much of the innovation this year was going on behind the scenes, as the company worked to simplify operations for AWS customers.\nLet\u2019s go through the key announcements quickly.\nSelipsky announced many new services and features: the new Graviton3 chips, C7g instances powered by the Graviton3 chip, EC2 Trn1 instances, Mainframe Modernization service, Private 5g, Lake Formation updates, Serverless analytics for Redshift, EMR, MSK and Kinesis, SageMaker Canvas, IoT Twinmaker, and IoT Fleetwise.\nThis year\u2019s Infrastructure Keynote, Peter DeSantis, focused on AWS Nitro SSD and Graviton 3, AWS Tranium, and Inferentia Processors. He wrapped up his keynote with sustainability and\u00a0the climate pledge of Amazon.\n\n\n\n\nAWS re:Invent 2021 Infrastructure Keynote by Peter DeSantis\nNext up was Dr. Swami Sivasubramanian with his Machine Learning Keynote. He announced Amazon DevOps Guru for RDS, AWS Database Migration Service (DMS) Fleet Advisor, Amazon Kendra Experience builder, and new SageMaker features.\nLastly, my favorite one this year\u00a0wasDr. Werner Vogels\u2019 keynote on the closing day. After an amazing intro video, he started with an Amazon M1 Mac instance and 30 new Local Zones announcements. Then he introduced maybe the most critical networking-related update of the event, AWS Cloud WAN. Finally, some Cloud Development Kit (CDK) love with Matt Coulter. Highly recommend watching this keynote.\nFan-favorite keynote\n\n\n\n\nYou can check Top Announcements of AWS re:Invent 2021\u00a0here. But here are my top takeaways from the announcements:\nEasier, Better, Faster, Stronger\nAlthough we don\u2019t have a new and shiny service this year that will change our daily work-life, many improvements have been implemented in the background. First, AWS worked on core infrastructure improvements with chip & instance additions to make our computing resources run faster and cheaper with Graviton3 & Trainium chips and M1 Mac Instances.\nWe also see AWS is really focusing on networking improvements and even started competing with network and security companies and ISPs. For me, the two most exciting announcements of the event were on the networking topic.\u00a0The first exciting news was the announcement of\u00a0AWS Private 5G. AWS primarily focused on software development services in the past, such as S3, SQS, EC2, ECS, Lambda, but is currently offering a private 5G networking service. This managed, pay-as-you-go service makes it easy to deploy, operate, and scale your own private cellular network, with the required hardware and software provided by AWS. Of course, this one is not for everyone, but it is a fantastic option for some AWS customers.\nThe\u00a0second most exciting news\u00a0is\u00a0AWS Cloud WAN. With the help of its partners, AWS is offering a Wide Area Network service.\u00a0AWS Cloud WAN provides an easy way to connect your data centers, branch offices, and cloud resources into a centrally managed network and dramatically reduces operational cost and complexity. Doing this on AWS without a third-party WAN product simplifies networking for many companies that need a WAN solution.\nAWS Cloud WAN \u2014 \u201cEasily build, manage, and monitor global wide area networks.\u201d\n\n\n\n\nAs we all know, AWS has already accomplished providing cloud services with its infrastructure, and services and in the past couple of years, they moved to bring their services to customers\u2019 on-premise using\u00a0AWS Outposts. Today they continue with an \u2018uncharted\u2019 area: the mainframe, and mainframe modernization.\nReducing the Complexity\nThere already are over 200 AWS services, and the number keeps growing every year. As a result, the cloud is getting more complex, and it is becoming more and more impossible to know and use all of the services. Because of this, it is crucial to reduce complexity when introducing new technologies.\nAWS reduces the complexity by introducing more serverless offerings. This year the focus was the analytics services: Amazon Redshift, Amazon Elastic MapReduce(EMR), Amazon Managed Streaming for Apache Kafka(MSK), and Amazon Kinesis. Some would argue about this, but to me, the increasing number of serverless services is another sign that Serverless is the next \u201cbig thing\u201d, especially compared to Kubernetes.\nSecurity: Secrets Management can be challenging at the beginning. However, the code review tool Amazon CodeGuru now has Secrets Detector\u00a0to help us. Not a significant feature, but nice to have.\nFor frontend development with minimal complexity:\u00a0AWS Amplify Studio, \u201ca visual development environment that offers frontend developers new features to accelerate UI development with minimal coding,\u201d is available. It can be handy for small applications.\nA final service to mention in terms of reducing complexity, a no-code Machine Learning capability: SageMaker Canvas. This new capability enables people with less technical knowledge to use Machine Learning services.\nNew Pillar: Sustainability\nAWS announced extending AWS Well-Architected Framework by adding a new Sustainability Pillar. Sustainability is becoming one of the hottest topics of the cloud infrastructure industry and we get more and more questions about this topic. I am pleased to see AWS is continuously putting effort into environmental issues and thinking about energy efficiency and sustainability.\nAs we know, the Well-Architected Framework is a list of questions aiming at different pillars to improve the design, architecture, and implementation of our systems and resources. The new pillar helps customers evaluate their workloads in order to reduce energy consumption and improve efficiency.\nAWS\u2019s shared responsibility model applies to sustainability\n\n\nCheck out the AWS blog post about the Sustainability pillar\u00a0here. To learn more or to have AWS Well-Architected Reviews for your workloads, you can always reach out to me.\nEven Better Developer Experience\nAWS has always been developers\u2019 choice when it comes to cloud providers because it offers so much flexibility and ease of use when it comes to its services. AWS, by far, has the best cloud services portfolio to enable DevOps teams in any task. Again this year, many announcements were about improving the developer experience.\nThe major announcement on the Infrastructure as Code tooling was the introduction of AWS Cloud Development Kit(CDK) Version 2. In addition, we can now use\u00a0Construct Hub: it is the home of all CDKs; AWS CDK, CDK8s, CDKtf. Both AWS CDK v2 and Construct Hub are\u00a0currently generally available(GA). Of course, we already knew about these two for a year, but it is good to see AWS actively promote its new developer tooling. Soon AWS CDK will eclipse CloudFormation and become the new standard. If you haven\u2019t tried a CDK yet, I strongly recommend trying it.\nSharable & repeatable cloud constructs thanks to Construct Hub\n\n\n\n\nMore positive news is that AWS offers new SDKs for the Swift, Kotlin, and Rust programming languages. Currently, these SDKs are on the public preview.\nInteresting note, AWS converted an internal tool to external:\u00a0AWS re:Post. It is a community tool to find answers, answer questions, and join groups.\nAfter all, this years re:Invent was another excellent learning experience. I am looking forward to the future ones. See you all at the following AWS events!\n\n\n", "tags": ["Architecture", "aws", "AWS re:Invent 2021", "cloud", "Well-Architected Framework", "Well-Architected Reviews"], "categories": ["Cloud", "News"]}
{"post_id": 30362, "title": "Silver Linings, No Silver Bullets: 3 Perspectives on Innovating with Cloud", "url": "https://www.luminis.eu/blog-en/silver-linings-no-silver-bullets-3-perspectives-on-innovating-with-cloud/", "updated_at": "2023-04-20T13:41:38", "body": "We\u2019ve heard the phrase, seen the cynical stickers: \u201cThere is no cloud; it\u2019s just someone else\u2019s computer.\u201d That might have been true 15 years ago, but the thing we call the cloud has evolved significantly since. We live in times of accelerating change; organizations that can leverage the cloud effectively will survive them, even thrive in them. How well prepared are you?\nTaking great advantage of the cloud requires a shift in thinking: technology no longer belongs to the IT department (if it ever did). To become cloud natives, organizations need to learn to streamline their effort around everything the cloud offers.\nLet\u2019s look at this paradigm shift from several perspectives: operations, development, and leadership. Each of these perspectives starts with describing the ineffective way of working. We use the term cloud naive (as in: inexperienced or lacking knowledge). Or as the State of DevOps report classifies it: the way of the low performers. Then, we cross the chasm and look at it from the cloud-native perspective, as used by companies that use cloud technology to accelerate their business continuously.\n\nOperations: embrace speed to reduce risk\nLow performance: cloud naive operations\nIn simpler times, developers delivered complete software packages to operations people, who in turn installed and ran them on the appropriate infrastructure. Preferably not too often and with as little change needed as possible. A similar rule determined a snail\u2019s pace of change for platforms: standardization before innovation. Why? Because every change introduced risk and operations were held responsible for it, often having to clean up the mess when things went wrong.\nThis model led operations departments to push back hard on changes and reduce the speed of change\u2014developers adjusted by increasing release sizes and time between releases.\nThe result: slow change, little cooperation, suffering users.\nElite performance: cloud-native operations\nThe way out of this standoff is well-known by now but not always well understood or effectively implemented: DevOps. Making agile teams responsible for business outcomes forces down the classical barriers between the business, developers, and operations, leading to better customer outcomes. Cloud platforms have evolved by adding layer upon layer of abstraction onto the classic data center, leading to hundreds of managed services that teams can continuously leverage to decrease their time-to-value. Perhaps counterintuitively, this increased rate of change enables teams to manage risk better than ever.\nThink about it: if time-to-value is the most critical metric for success, change size logically also gets smaller. Minor changes are easier to manage in case of failure, and deploying multiple times a day yields teams totally in control of their technological solutions.\nThis cloud-native way of doing operations yields precisely what these teams aimed for in the first place: less risk, more control.\n\nDevelopment: focus on outcomes, not technology\nLow performance: cloud naive development\nBefore the emergence of the cloud, servers had to be installed by hand, as did the software running on them. Installing and testing releases was a time-consuming process, mostly done by other teams. So, developers made sure their sparsely released work contained as many features as possible. Any infrastructural requirements and other dependencies had to be carefully communicated and planned with IT operations, dictating long lead times between innovative ideas and customer value.\nAs a result, developers preferred working with stable, minimally variable environments simulated on their development machines. Their products were massive monoliths often written using a single programming language, running on single application container instances. Development feedback might have been fast; customer feedback certainly wasn\u2019t. Developers were often kept totally in the dark about the impact of their changes in production until nightly calls from frustrated IT operations, who spoke in an entirely different IT dialect.\nAgain: slow change, little cooperation, suffering users.\nElite performance: cloud-native development\nThe step-by-step virtualization and containerization of servers, platforms, and runtime environments have enabled development teams to master what was previously the sole job of operations. And more importantly, the marriage of Dev and Ops has given birth to beautiful things like infrastructure as code, CI/CD, and SRE. Managing complex systems this way has opened up opportunities for creating massively scalable but well-observable systems. The old monoliths have been split into small, modular services or even single functions that typically communicate asynchronously using a mind-boggling number of events.\nNow the customer wins. Changing, replacing, or radically rewriting a single, small module and testing and deploying it in the blink of an eye has become a reality. Experimenting \u2014 seeing if an idea for a new job to be done holds up in production \u2014 has become cheap as chips using the pay-as-you-go managed services as provided by cloud platforms like AWS, Azure, and GCP.\n\nLeadership: get out of the way of innovation\nLow performance: cloud naive leadership\nDuring less volatile times, markets were predictable, transformations were carefully planned and budgeted, and users were okay with incremental changes every so often. Software, and the hardware it ran on, were complex and expensive beasts, mastered only by tech masochists stuffed away in cost centers called IT departments. Nothing got built until it was very meticulously specified in requirements documents. Even then, the delivered result was often wildly different from what the market specialists and innovators defined months or years before, if not already outdated.\nOnce more: slow change, little cooperation, suffering users.\nElite performance: cloud-native leadership\nSoftware has been eating the world for a while now. Organizations need to adapt or accept a slow and painful decline into irrelevance. Users now expect to get what they need before realizing they do: a new Netflix series, a recommended product, or even an improved driving experience. Understanding users is key to market success and requires continuous experimentation and software development mastery. The cloud is a massive enabler: it enables organizations to focus on user needs by handing the management of undifferentiated technology over to the cloud provider.\nBut while technical skill development is essential, it is just one part of the innovation puzzle. Another is culture: leaders need to focus everyone\u2019s attention on the importance of time-to-value and agility as the metrics for success. To create empowered, self-sufficient, and effective teams, people need to come together and shatter the walls previously separating them. Lastly, leaders need to take the long view and realize that short-term risk aversion can be a major innovation blocker.\n\nTo boldly go, together\nTransforming an organization from cloud naiveness into cloud-native isn\u2019t for the faint of heart, but very necessary nonetheless. Luckily, Luminis has traveled this path many times before and is a very effective guide on the path to cloud-native. For example, we helped OHRA, one of the largest Dutch digital insurers, migrate its complete data center to the cloud. Another example is the digital transformation of The Learning Network (TLN) we helped shape and implement.\nGet in touch and let us help you kickstart your cloud-native transformation.\nRelated post:cloudwhitepaperWhite paper Cloud migrationCloud migration: 5 effective steps towards a successful result. Download our free white paper. Do you want to migrate your organisation\u2019s systems to the cloud? A cloud migration provides speed and efficiency, among many other advantages. This white paper is...\n", "tags": ["cloud", "devops", "innovation"], "categories": ["Blog", "Cloud"]}
{"post_id": 29809, "title": "Creating your serverless web-application using AWS CDK \u2013 part 1", "url": "https://www.luminis.eu/blog-en/creating-your-serverless-web-application-using-aws-cdk-part-1/", "updated_at": "2021-11-02T09:44:22", "body": "So you are looking to build a web-application. You want it to be serverless and hosted on AWS, what do you do? In this blog post I will tell you why I like to use CDK for this. I will also explain how to use CDK to set up your own web-application with ease.\nInfrastructure as code\nCreating a serverless application in AWS means that you will be configuring certain AWS resources to behave the way you want them to. By doing this, you set up the infrastructure for your application. A quick and easy way to set up this infrastructure would be to navigate to the AWS portal using your browser and manually configure everything there. But what will you do if you need to deploy your application a second time? You would have to painstakingly document every step you take and every field you fill so that you can repeat it at a later time. Also if you make any changes you need to make sure to update your documentation. Sounds exhausting right?\nThis is where infrastructure as code solutions come in to save the day. By defining your infrastructure as code, just as you would with other code in your application, you can maintain it in a version control system. Any change you make in the infrastructure is being tracked and at any time you can checkout the latest version of the infrastructure and deploy it immediately. You can even automate such deployments to start automatically whenever you check in your changes.\nWhy I use CDK\nNowadays there are plenty of options for you to choose from when it comes to infrastructure as code tooling. Each of these tools comes with its own pro\u2019s and con\u2019s. It is hard to pick one tool that is best for every single situation. CDK however has two strong points which for me, makes it the tool I choose.\nThe first is that as a developer, CDK feels very familiar. CDK allows you to define your infrastructure setup using Typescript, Javascript, Python, Java, C#/.Net and Go. This means that if you are a developer with experience with one of these languages, you do not have to spend time learning a syntax first. Right away you can put all your focus on learning about AWS resources and how to set them up.\nThe second is that CDK offers a very nice balance between helping you deploy resources with little effort and giving you the flexibility where you need it. It has a bunch of higher level constructs with reasonable defaults that you can easily use to quickly get started. However you can override the defaults on these constructs, or use lower level constructs in the same framework to create whatever you need. This means you are always in control.\nWhat do you need?\nThe first thing you need is to decide on the language you are going to use with CDK. In this case I chose to use Typescript.\nApart from picking a language, in order to get started with AWS CDK you need:\n\nAn AWS account\nTo install and bootstrap the AWS CDK\nA code editor, I tend to use Visual Studio Code for this\n\nThe basic setup\nWe will start building our web-application with a simple basic setup which contains the minimum you would need to see something work. After that is set up, we will expand upon this basic setup until we have something that we could put into production.\nOur basic setup will look like this:\n\nThe static web resources of our application frontend such as the HTML, CSS and Javascript will be hosted in an S3 bucket. We will use DynamoDB to store the persistent data of our application and Lambda and API Gateway to offer an API over HTTPs that allows access to this data.\nThe first thing we will need to do is create a new directory for our application. In my case I am creating a simple Todo list application, so I create a directory called TodoApplication. Move into this directory and execute the command:\ncdk init app --language typescript\nThis will give us the files and folders we need to get started with our application.\nIn order to keep the length of this blog somewhat limited I am only showing the CDK code in this blog, with a few exceptions here and there. If you want to see the full solution however it is available on GitHub. For the basic setup, you have to checkout the basic_setup\u00a0tag.\nThe frontend\nThe next thing we will do is set up the frontend. We create a directory called application and another directory called frontend in that. In this directory we will put the code for the frontend application. I have used Angular to create my frontend, but feel free to use another frontend framework if you like.\nOne of the files that we created when we initialised our CDK application is libs/todo_application-stack.ts.\u00a0This is where we define all AWS resources our application uses. We do this by defining them in the constructor of the TodoApplicationStack class that was created for us.\u00a0 For our frontend application to be hosted in S3, we add the following:\nconst frontendBucket = new s3.Bucket(this, 'TodoApplicationFrontend', {\r\n  websiteIndexDocument: 'index.html',\r\n  publicReadAccess: true,\r\n  removalPolicy: cdk.RemovalPolicy.DESTROY,\r\n  autoDeleteObjects: true\r\n});\r\n\r\nconst bucketDeployment = new s3deploy.BucketDeployment(this, 'DeployTodoApplicationFrontend', {\r\n  sources: [s3deploy.Source.asset(`application/frontend/dist/todo-application`)],\r\n  destinationBucket: frontendBucket\r\n});\r\nbucketDeployment.node.addDependency(frontendBucket);\nThis code adds two CDK constructs to the CDK stack. The first construct is an S3 bucket that we create. By setting the websiteIndexDocument property we also enable static website hosting for this bucket and provide the name for the index document of the website. The second setting we set on the S3 bucket is that we enable public read access on the bucket. The last two settings dictate what happens to this bucket when the CDK stack is destroyed.\nThe removal policy tells CDK what to do with the bucket itself. The default setting \u201cretain\u201d orphans the S3 bucket, leaving it disconnected from the CDK stack but does not remove the bucket or the files in it. I want the bucket and its contents to be deleted along with the rest of the CDK stack which is why I set it to destroy. In the case of an S3 bucket, this action will fail if there are still any files left in the bucket. To allow these files to be removed along with the bucket, I set autoDeleteObjects to true.\nThe other construct is a deployment of resources to S3. In this case, we deploy everything in\u00a0application/frontend/dist/todo-application\u00a0to the S3 bucket we just created. This directory is where Angular stores the frontend distribution after running npm run build. If you use a different framework to create your frontend you may have to change this path.\nYou might notice that if you paste the above code in the CDK stack, it will not compile. This is because we are missing the @aws-cdk/aws-s3 and @aws-cdk/aws-s3-deployment node modules. We can install those using:\nnpm install @aws-cdk/aws-s3 @aws-cdk/aws-s3-deployment\nWe also need to import them into the stack:\n\nimport * as s3 from '@aws-cdk/aws-s3';\r\nimport * as s3deploy from '@aws-cdk/aws-s3-deployment';\nNow we can deploy this stack to AWS using:\ncdk deploy\nWhen this is done, you will have a publicly accessible S3 bucket that hosts your frontend.\nDatabase\nNow that we have our frontend, we shall set up the DynamoDB database we use to store our data.\nFirst we need to add the\u00a0@aws-cdk/aws-dynamodb\u00a0node module to our project and import it just like we did earlier. Then secondly we add the following to the TodoApplicationStack:\n\n\nconst todoItemsTable = new dynamodb.Table(this, 'TodoApplicationTodoItemsTable', {\r\n  partitionKey: {\r\n    name: 'who',\r\n    type: dynamodb.AttributeType.STRING,\r\n  },\r\n  sortKey: {\r\n    name: 'creationDate',\r\n    type: dynamodb.AttributeType.STRING\r\n  },\r\n  removalPolicy: cdk.RemovalPolicy.DESTROY\r\n});\n\nThis code adds a new construct to our CDK stack that creates a DynamoDB table. The table will have a partition key of type String named \u201cwho\u201d. It\u2019s value will contain the username of the user the Todo item was created by. The table will also have an additional sort key, also of type String, that contains the creation date of the Todo item. This allows us to retrieve the Todo items from the table ordered by their creation date. The last property that I added to my table construct is again an override of the removal policy. By default the removal policy for a DynamoDB table is \u201cretain\u201d. Like with the S3 bucket I want the table to be removed when I delete the CDK stack, so I set it to destroy.\nAPI\nIt is a good thing to have our database table set up, however at the moment the frontend has no way to access it. To solve this, we need to set up our API. Setting up the API that allows the frontend to reach the database is the largest part of our basic setup. To keep things clear and orderly, we will do this in a couple of steps.\nShared code layer\nWe start with a Lambda layer that houses shared code that can be used by multiple Lambda functions. In my Todo list application I have a TodoItem class that I want to reuse. Another reason why I want to have this shared code layer is that I use NodeJS for my Lambda functions. This means my functions use node modules, but I do not want to include those in every Lambda function when I deploy them to AWS. Instead I put all the node modules in this shared code layer. This way I keep my functions nice and clean with only function specific code.\nTo create this shared code layer, we start by creating a directory called\u00a0functions in the\u00a0application directory we created earlier. Here we keep the code for our Lambda functions. Within that directory we then create another directory called shared-code and finally a directory called nodejs in that. In this directory we place a package.json for the node modules as well as the TodoItem.ts and the tscompile.json and tsconfig.json files to compile it.\nTo use Lambda for the shared code layer in the TodoApplicationStack, we need to import it just as we did earlier on with S3 and DynamoDB:\nimport * as lambda from '@aws-cdk/aws-lambda';\n\nNext, to deploy our shared code layer we add the following code to the stack:\n\nconst sharedCodeLayer = new lambda.LayerVersion(this, 'TodoApplicationSharedCode', {\r\n\u00a0 code: lambda.Code.fromAsset('application/functions/shared-code'),\r\n\u00a0 compatibleRuntimes: [lambda.Runtime.NODEJS_14_X]\r\n});\n\nThis adds a LayerVersion construct for us. The settings we give it are not all the special. We simply point to the location of the code to include and specify that that we want to use the NodeJS 14.X runtime.\nLambda functions\nTime to make a Lambda function that uses the shared code layer we created just now. In this case I have two Lambda functions, one to retrieve Todo items from the DynamoDB table and one to create items in the table. Within the existing directory application/functions\u00a0we create the directories\u00a0get-items and\u00a0add-item. The code for our Lambda functions goes into these directories.\nMost of the code in our Lambda function is like in any other Lambda function. I will therefore not be going into the code of the Lambda functions here, if you want you can check them out here. However there is one thing I do want to point out. Because we use a Lambda layer to house some shared code, any class that is in this layer must be imported as follows:\n\nimport { TodoItem } from '/opt/nodejs/todoItem';\n\n\nThis in turn will make the compiler complain when you compile the Typescript on your local machine, because this path cannot be found in the local repository. To counteract this, add the following to the tsconfig.json of the Lambda functions:\n\n\"paths\": {\r\n  \"/opt/nodejs/*\": [\"../shared-code/nodejs/*\"]\r\n}\n\nNow we can add the Lambda function to the TodoApplicationStack with the following code:\n\n\n\nconst getItemsLambda = new lambda.Function(this, 'TodoApplicationGetItemsFunction', {\r\n  runtime: lambda.Runtime.NODEJS_14_X,\r\n  handler: 'index.handler',\r\n  code: lambda.Code.fromAsset('application/functions/get-items', {exclude: [\"node_modules\", \"*.json\"]}),\r\n  environment: {\r\n    TODO_ITEMS_TABLE_NAME:todoItemsTable.tableName,\r\n    ALLOWED_ORIGINS:'*'\r\n  },\r\n  layers: [\r\n    sharedCodeLayer\r\n  ]\r\n})\r\ntodoItemsTable.grantReadData(getItemsLambda)\n\n\n\nWith this code we add another CDK construct, this time for a Lambda function. In this case the function shown here is get-items. Apart from some names and paths the add-item function is exactly the same.\nIn settings we set the runtime for the function to NodeJS 14.X and set the handler for the function to the exported handler function of Index.ts. We also direct CDK to the location of the code for this function. From this path, we exclude everything under node_modules and everything that has the extension json. The node modules we exclude because they are in our shared code layer. The JSON files in our function we exclude because we only use them at compile time before pushing the code to AWS. For these two functions I also included two environment variables that can be accessed within the function. This keeps me from having to hard code certain values in the function code itself. Finally I pointed this Lambda function to the shared code layer so that it can access the code there.\nThe last line of this code instructs the DynamoDB table construct to allow read access to the Lambda function. This creates the correct IAM access rights for the Lambda function.\nAPI Gateway\nThe last component needed for the basic setup is an API Gateway that exposes our lambda Functions to the frontend. To set up this API Gateway we add the @aws-cdk/aws-apigateway\u00a0node module to our project and import it into the TodoApplicationStack. Next we add the following code to the stack:\n\nconst apiGateway = new apigateway.RestApi(this, 'TodoApplicationApiGateway', {\r\n  restApiName: 'TodoApplicationApi'\r\n})\r\n\r\nconst itemResource = apiGateway.root.addResource('item')\r\nitemResource.addCorsPreflight({\r\n  allowOrigins: [ '*' ],\r\n  allowMethods: [ 'GET', 'PUT' ]\r\n});\r\nitemResource.addMethod('PUT', new apigateway.LambdaIntegration(addItemLambda), {})\r\nitemResource.addMethod('GET', new apigateway.LambdaIntegration(getItemsLambda), {})\nThe first thing this code does is add the CDK RestApi construct for the API Gateway. Other than setting the name, we currently do not need to change any settings on this construct from their default setting.\nWith the RestApi construct in place, we add a resource to the root endpoint of this API called item. To this resource we then add the CORS preflight OPTIONS method. We set this resource to allow all origins since we want to create a public API. We also set it to allow the GET and PUT methods as those are the ones our Lambda functions will listen to.\nFinally we add the GET and PUT methods to the item resource by creating two new LambdaIntegration instances. Into their constructor we pass the two Lambda functions that were created earlier in the stack as the handler functions for these methods.\nConfig JS\nWith the API Gateway in place we have all the components that we require for the basic setup. We have one problem left to solve however before the basic setup is fully functional. Because we do not yet use a custom domain for the API we created, the url to invoke this API is one in the amazonaws.com domain and generated when we deploy the API Gateway. This means we cannot simply hard code this url into the code we deploy for the frontend.\nI solve this by creating a config.js file and uploading this to the frontend S3 bucket at the end of the deployment. At this stage of the deployment the API Gateway construct is in place. This means I can use the property url on the construct to get access to the url and write that into config.js. As my frontend is created with Angular, it can read out this config.js file to access the url to the API Gateway.\nTo create this custom resource and upload it, I need to add the following import to the TodoApplicationStack:\n\nimport * as customResources from '@aws-cdk/custom-resources';\nThen I add the following code to the stack:\n\nconst frontendConfig = {\r\n  itemsApi: apiGateway.url,\r\n  lastChanged: newDate().toUTCString()\r\n};\r\n\r\nconstdataString = `window.AWSConfig = ${JSON.stringify(frontendConfig, null, 4)};`;\r\n\r\nconst putUpdate = {\r\n  service: 'S3',\r\n  action: 'putObject',\r\n  parameters: {\r\n    Body: dataString,\r\n    Bucket: `${frontendBucket.bucketName}`,\r\n    Key: 'config.js',\r\n  },\r\n  physicalResourceId: customResources.PhysicalResourceId.of(`${frontendBucket.bucketName}`)\r\n};\r\nconst s3Upload = new customResources.AwsCustomResource(this, 'TodoApplicationSetConfigJS', {\r\n  policy: customResources.AwsCustomResourcePolicy.fromSdkCalls({ resources: customResources.AwsCustomResourcePolicy.ANY_RESOURCE }),\r\n  onUpdate: putUpdate,\r\n  onCreate: putUpdate,\r\n});\r\ns3Upload.node.addDependency(bucketDeployment);\r\ns3Upload.node.addDependency(apiGateway);\nThis code creates an object that contains the API url and a last changed date. It then turns that object into a data string. Lastly it makes an AwsCustomResource instance. We instruct this instance to perform the action putObject on S3 with the specified parameters whenever it is either created or updated. We make CDK generate the correct IAM policy statements based on the SDK calls made by the actions performed. Because we give the AwsCustomResource instance a dependency on the bucketDeployment and apiGateway, we ensure that it is deployed after those were created.\nDeploying\nBefore we deploy everything, make sure to compile the frontend and all Lambda functions. In my project this means I use:\nnpm install\r\nnpm run build\non all of them as they are all NodeJS based. With the code compiled, we use:\ncdk deploy\nto deploy the stack. In the AWS portal you can find the S3 bucket. In its properties under static web hosting you will find the url where your web application is now running.\nMy Todo list application now looks like this:\n\nWe have now completed the basic setup for our serverless web-application with AWS CDK. We have a running frontend to serve to our users. To store the data of our users we have created a DynamoDB database. Finally we created a HTTP API that connects to frontend to the database.\nThis basic setup however is not quite ready for us to use in production. To get the application production ready, we need to address the following:\n\nThe url to the application is somethingsomething.s3-website-region.amazonaws.com. This does not help users easily reach the application.\nThe application does not offer a HTTPS connection for the frontend, so access to the application is not secure.\nThe current application has no user authentication. You can access the data of any user by simply entering the correct username.\n\nWe solve these issues in the second part of this blog.\n\n\n\n", "tags": [], "categories": ["Blog", "Cloud"]}
{"post_id": 29954, "title": "Moving your iOS app development and release process to the cloud using Azure DevOps", "url": "https://www.luminis.eu/blog-en/cloud-en/moving-your-ios-app-development-and-release-process-to-the-cloud-using-azure-devops/", "updated_at": "2021-12-16T11:43:41", "body": "Developing your applications in the cloud has many advantages, including cost reduction, scalability, and ease of use for developers. In this blog I will explain the basics of what is needed to move the build and release process of an iOS app to the cloud using Azure DevOps.\nWe can roughly divide this process in the following steps:\n\nMoving your code repository to Azure DevOps;\nSetting up a build pipeline;\nUsing a Service Connection to automatically push app builds to App Store Connect.\n\nI will describe each step in more detail below.\n\u00a0\nNote: this guide assumes you have an Azure DevOps Project for your repository and build pipelines. Microsoft\u2019s own documentation on how to create such a project can be found here. For more information on the advantages of moving to the cloud, read Luminis\u2019\u00a0 whitepaper on the subject via this link.\n\u00a0\nStep 1: Moving your code repository to Azure DevOps\n\u00a0\nOur first step will be to move our app\u2019s source code from any existing repository to an Azure DevOps repository. It is of course also possible to have our Azure DevOps pipeline pull code from an external repository, but that is beyond the scope of this blog.\nIf you are creating a new repository in your Azure DevOps Project, you can simply use the import repo feature during the creation of your repository as described here. If you want to use an existing (empty) Azure DevOps repository for your code, you can manually import your source code from your current repository into the existing Azure DevOps repository using this guide.\nAssuming your source code was successfully pushed to your Azure DevOps repo, we can continue and create a pipeline to actually build our app.\n\u00a0\nStep 2: Setting up a build pipeline\n\u00a0\nInitial pipeline creation\n\u00a0\nA pipeline is created by navigating to Pipelines > Pipelines from the left bar and selecting Create Pipeline. On the Connect tab, we will select \u2018Azure Repos Git\u2019 as our source code location:\n\n\u00a0\nOn the Select tab, we will select the repository that we moved our app\u2019s source code to in Step 1. Then, on the Configure tab we can select \u2018Starter pipeline\u2019. On the \u2018Review\u2019 tab we can see that an azure-pipelines.yml file will be created in our repository. This file contains the instructions that Azure DevOps needs to successfully build and publish our app. On this page we can edit its contents before the file is actually created:\n\nEven though we can change the contents of this file later on, we might as well make some changes for the initial commit.\nRemove the comment header and the script parts of the file and change the vmImage to \u2018macOS-latest\u2019. Your file should now look like this:\ntrigger:\r\n- develop\r\n\r\npool:\r\n\u00a0 vmImage: 'macOS-latest'\r\n\r\nvariables:\r\n\r\n- group: \r\n\r\nsteps:\r\n\nThe trigger key here describes that every commit to the \u2018develop\u2019 branch will trigger an automatic build. Change this to whatever branch you want, or remove it entirely if you do not want to trigger automatic builds. Builds can also be triggered manually from the Pipelines screen. The \u2018macOS-latest\u2019 image is needed here because we are building an iOS app which requires Xcode on our build agent.\nWe will now add tasks under our steps key to further define our build process. Add the following tasks under the steps key:\nsteps:\r\n\u00a0 - task: InstallAppleCertificate@2\r\n\u00a0 \u00a0 inputs:\r\n\u00a0 \u00a0 \u00a0 certSecureFile: ''\r\n\u00a0 \u00a0 \u00a0 certPwd: ''\r\n\u00a0 \u00a0 \u00a0 keychain: 'temp'\r\n\u00a0 \u00a0 \u00a0 deleteCert: true\r\n\r\n\u00a0 - task: InstallAppleProvisioningProfile@1\r\n\u00a0 \u00a0 inputs:\r\n\u00a0 \u00a0 \u00a0 provisioningProfileLocation: 'secureFiles'\r\n\u00a0 \u00a0 \u00a0 provProfileSecureFile: ''\r\n\u00a0 \u00a0 \u00a0 removeProfile: true\r\n\r\n\u00a0 - task: Xcode@5\r\n\u00a0 \u00a0 inputs:\r\n\u00a0 \u00a0 \u00a0 actions: 'build'\r\n\u00a0 \u00a0 \u00a0 scheme: ''\r\n\u00a0 \u00a0 \u00a0 sdk: 'iphoneos'\r\n\u00a0 \u00a0 \u00a0 configuration: 'Release'\r\n\u00a0 \u00a0 \u00a0 xcWorkspacePath: '**/YourProject.xcworkspace'\r\n\u00a0 \u00a0 \u00a0 xcodeVersion: 'default'\r\n\u00a0 \u00a0 \u00a0 packageApp: true\r\n\u00a0 \u00a0 \u00a0 signingOption: 'manual'\r\n\u00a0 \u00a0 \u00a0 signingIdentity: '$(APPLE_CERTIFICATE_SIGNING_IDENTITY)'\r\n\u00a0 \u00a0 \u00a0 provisioningProfileUuid: '$(APPLE_PROV_PROFILE_UUID)'\r\n\r\n\u00a0 - task: CopyFiles@2\r\n\u00a0 \u00a0 inputs:\r\n\u00a0 \u00a0 \u00a0 contents: '**/*.ipa'\r\n\u00a0 \u00a0 \u00a0 targetFolder: '$(build.artifactStagingDirectory)'\r\n      flattenFolders: true\r\n\u00a0 \u00a0 \u00a0 overWrite: true\r\n\r\n\u00a0 - task: PublishBuildArtifacts@1\r\n\u00a0 \u00a0 inputs:\r\n\u00a0 \u00a0 \u00a0 pathtoPublish: '$(build.artifactStagingDirectory)' \r\n\u00a0 \u00a0 \u00a0 artifactName: 'drop' \r\n\u00a0 \u00a0 \u00a0 publishLocation: 'Container'\r\n\nIn order, these tasks do the following: installing an App Store certificate, installing a provisioning profile, running xcodebuild and copying and publishing the created artifacts. As you can see, some values are currently empty. For now, make sure the scheme key under the \u2018Xcode@5\u2019 task contains the scheme name of your app and the xcWorkspacePath key contains the correct path to the .xcworkspace file in your repository. After this, select the \u2018Save\u2019 sub option:\n\nEnter the commit message and select the branch you want to commit the initial pipeline file to. As the pipeline file is not final in its current form, I recommend committing it to a temporary branch and merging it into your main branch later on.\n\u00a0\nConfiguring the certificate and provisioning profile\n\u00a0\nAs you may have noticed, the \u2018InstallAppleCertificate@2\u2019 and \u2018InstallAppleProvisioningProfile@1\u2019 tasks still have some empty keys. These tasks require us to upload a .p12 certificate file and a .mobileprovision file to our secure files. Please refer to this guide on how to obtain these files. Note: the linked guide refers to a Cordova project (phonegap), but the part about certificate file generation applies here as well.\nIf you have added a password to your .p12 file (which is recommended), we will need to store it in our DevOps pipeline. For security reasons, the p12 password will be added to a variable group, which our pipeline file can refer to. To create a variable group, from the sidebar go to Pipelines > Library and select \u2018+ Variable group\u2019. The following screen will look like this:\n\n\u00a0\nAdd the p12 password as a variable and select the lock icon to hide it. This will prevent us from having to store our p12 password in our repository. Name your group and save it.\nThe .p12 and .mobileprovision files can be stored as secure files under the Secure files tab on the same page (Library):\n\nNow, we can go back to our azure-pipelines.yml file and fill in the missing keys. Go to Pipelines > Pipelines using the sidebar, select your pipeline and then press the \u2018Edit\u2019 button to modify the pipeline file.\nFirst of all, we can change the variable group to refer to our newly created group:\nvariables:\r\n- group: luminis-blog-group\r\n\nSecondly, we can now fill in the blank keys in the \u2018InstallAppleCertificate@2\u2019 and \u2018InstallAppleProvisioningProfile@1\u2019 tasks:\nsteps:\r\n\u00a0 - task: InstallAppleCertificate@2\r\n\u00a0 \u00a0 inputs:\r\n\u00a0 \u00a0 \u00a0 certSecureFile: 'certificate.p12'\r\n\u00a0 \u00a0 \u00a0 certPwd: '$(p12Password)'\r\n\u00a0 \u00a0 \u00a0 keychain: 'temp'\r\n\u00a0 \u00a0 \u00a0 deleteCert: true\r\n\r\n\u00a0 - task: InstallAppleProvisioningProfile@1\r\n\u00a0 \u00a0 inputs:\r\n\u00a0 \u00a0 \u00a0 provisioningProfileLocation: 'secureFiles'\r\n\u00a0 \u00a0 \u00a0 provProfileSecureFile: 'myApp.mobileprovision'\r\n\u00a0 \u00a0 \u00a0 removeProfile: true\r\n\n\u00a0\nWith these final blanks filled, our template can be tested! Save the file and select \u2018Run\u2019. If all goes well, you can download your artifact on the result summary page after your build completes:\n\n\u00a0\nNow that we have a working pipeline, there is one remaining step: pushing the artifact automatically to App Store Connect, so that it can be released to testers or pushed to the App Store.\n\u00a0\nStep 3: Using a Service Connection to automatically push app builds to App Store Connect\n\u00a0\nEven though we have now fully automated building our artifact, we still have to manually upload our artifact to App Store Connect. We can automate this process as well by expanding our pipeline configuration to push our artifact to Apple using a so-called Service Connection. We will however first need to create this Service Connection before we can use it in our pipeline.\n\u00a0\nCreating a Service Connection\n\u00a0\nTo create a service connection, we require an API token to authenticate ourselves with the App Store Connect API each time we want to push a build automatically. Details on how to generate such a token can be found here. Make sure the token is generated with the developer role.\nWe also need to install the \u2018Apple App Store\u2019 pipeline extension, located here.\nWith the extension installed, we can proceed to actually create the Service Connection. In your Azure DevOps Project, click on the \u2018Project settings\u2019 button located on the bottom of the sidebar. On this page, from the second sidebar, select Service Connections > Create service connection. From the list, select \u2018Apple App Store\u2019 and then fill in the details required. Save the connection.\n\n\u00a0\nUsing the Service Connection in our pipeline file\n\u00a0\nNow that we have configured a Service Connection, we can use it in our azure-pipelines.yml file. This can be done by adding the \u2018AppStoreRelease@1\u2019 task to the bottom of the file, below the \u2018PublishBuildArtifacts@1\u2019 task:\n\u00a0- task: PublishBuildArtifacts@1\r\n \u00a0 inputs:\r\n \u00a0 \u00a0 pathtoPublish: '$(build.artifactStagingDirectory)/output/iphoneos/Release' \r\n \u00a0 \u00a0 artifactName: 'drop' \r\n \u00a0 \u00a0 publishLocation: 'Container'\r\n\r\n - task: AppStoreRelease@1\r\n \u00a0 inputs:\r\n \u00a0 \u00a0 serviceEndpoint: 'Luminis Blog Service Connection'\r\n \u00a0 \u00a0 appIdentifier: eu.luminis.blog\r\n \u00a0 \u00a0 ipaPath: '$(build.artifactstagingdirectory)/**/Luminis Blog.ipa'\r\n \u00a0 \u00a0 shouldSkipWaitingForProcessing: true\r\n \u00a0 \u00a0 shouldSkipSubmission: true\r\n\nSave the file and run the pipeline. Provided our Service Connection is working, our pipeline will now push the IPA artifact to App Store Connect at the end of the build, where it can be rolled out to testers or released to the App Store.\nCongratulations! You have successfully moved your iOS app build process to the cloud.\n\u00a0\nFinal words\nAs developing software in the cloud is the new way of working, we need to be able to move our development processes to cloud environments. In this post I have explained the basics of getting an iOS app built on an Azure DevOps environment. If you have any questions or comments regarding this guide, don\u2019t hesitate to contact me at ejnar.kaekebeke@luminis.eu.\n", "tags": [], "categories": ["Blog", "Cloud", "Development", "No category"]}
{"post_id": 29843, "title": "Easily configure Azure API Management with bicep and az-cli", "url": "https://www.luminis.eu/blog-en/cloud-en/easily-configure-azure-api-management-with-bicep-and-az-cli/", "updated_at": "2021-12-16T12:15:45", "body": "A while ago Tom Meulenberg wrote a blogpost about API Management and the challenges he faced. With this blogpost I will help him make things easier next time by splitting managing the configuration of API Management from defining the api\u2019s.\nOne of his observations is the verbosity of the arm-template. That is mainly caused by all the operations he added in the template. Using a new (but still under development) az-cli subcommand (apim) these operations can be removed from the template, making the template a lot cleaner. This requires a clean separation between your api definition (all endpoints) and your api configuration allowing you to update your api-spec (using open-api yaml) without adjusting the configuration (ie. policies).\nAPI Management concepts\nIn the first version of this blog I took knowledge about API Management for granted and left my reviewers puzzled. Let\u2019s take one step back and explain some concepts before diving into details.\nOperations, APIs, Products and subscriptions\n\nOperations define actions on a resource (like HTTP POST /user to create a user)\nAPIs group operations that belong together\nProduct can be linked to one or more API\u2019s\nSubscription (the right to consume an API) are applied to a product\n\nAPI Management concepts\nPolicies\nAs Microsoft states here, policies are a collection of statement that change the API behavior through configuration. Popular statements include:\n\nFormat conversion from XML to JSON\nRate limiting to restrict the number of incoming calls\nAdd an http-header (like an API key) on the request to the backend\nForward authentication data (like bearer tokens) to the backend\nRemove sensitive information from the response\n\nPolicies can be configured globally (on all APIs) or at the scope of a product, api or operation. The superset of policies are applied to sequentially to a request or response. API Management allows for deterministic ordering of the combined policy statements via the base\u00a0element.\n<policies>\r\n  <inbound>\r\n    <rate-limit-by-key \r\n             calls=\"150\" \r\n             renewal-period=\"60\" \r\n             counter-key=\"@(context.Subscription?.Key ?? \"anonymous\")\" />\r\n    <base />\r\n    <set-header name=\"X-Forwarded-Uri\" exists-action=\"override\">\r\n      <value>@(context.Request.OriginalUrl.ToString())</value>\r\n    </set-header>\r\n  </inbound>\r\n  <backend>\r\n    <base />\r\n  </backend>\r\n  <outbound>\r\n    <base />\r\n  </outbound>\r\n  <on-error>\r\n    <base />\r\n  </on-error>\r\n</policies>\r\n\nWhen you do not require any policy at operation-level, you are ready to configure your API by importing OpenApi-specs.\nLet\u2019s take a look at the code\nFor readability and of course to learn something new, I switched to bicep for defining my deployment. I\u2019ll skip the obvious parts like creating the Api Management-instance and configuring logging, but you can find those in the complete bicep-file linked at the end of this post. I will configure API Management with the commonly used Petshop sample that can be found at github.\nDefine the API\nIn the code below, I define the Petshop-api and it\u2019s policies. These policies are defined as xml and will generate a simple default inheriting policy with some comments on top.\n\nThis will result in the following api definition in API Management.\nAzure API Management Petshop API\nDefine the products\nNext I define the products. In the sample I define a basic product with rate-limiting and an advanced product without limitations. Both products are linked to the petshop api.\n\nThis will create the following policies on the basic product.\nAzure API Management Basic Product Policies\nDeploy the template\nThat\u2019s all that\u2019s required for configuring api management. To deploy this template, run the following command:\naz deployment group create --name demoapim --resource-group <rgname> --template apimanagement.deploy.bicep\nImport the api definition\nWith this basic configuration, it\u2019s now time to import the operations. As mentioned, I am using the `openapi.yaml` downloaded from github.\naz apim api import --resource-group <rgname> --service-name thijdemoapis --api-id petshop --path petshop --display-name 'Petshop API' --specification-format OpenApi --specification-path openapi.yml\nThe parameter api-id should be the same as the name defined here in the bicep file: Microsoft.ApiManagement/service/apis@2020-12-01. Otherwise you will add a new api to API Management and miss the configuration you made.\nThe path and display-name parameters overwrite the settings configured in the api. So make sure they are the same as in the bicep file.\nWhen running the command, you\u2019ll see the disclaimer \u2018Command group \u2018apim\u2019 is experimental and under development.\u2019 After a few seconds, the operations and definitions are available in API Management.\nAzure API Management List of operations\nAutomating deployment in your release pipeline\nOne final consideration: when you decorate the functions in your function-app with OpenApi-attributes, you can generate the OpenApi-spec after the release of your function app and import it into API Management using the az apim command.\nConclusion\nAlthough the az apim command is still under development, I would certainly choose it to configure api\u2019s in API Management. It forces you to think about your policies by removing the possibility to create them at operation level, making it easier to maintain it. And for bicep? Well, I am loving it! The 0.4-release I used is production ready and intellisense support in VSCode helps creating correct templates quickly.\nLinks\nWhat is bicep\naz apim api import\nThe complete bicep-file\nPetshop OpenApi spec\n", "tags": ["Api Management", "az cli", "Azure", "bicep"], "categories": ["Blog", "Cloud", "Development"]}
{"post_id": 29680, "title": "Bluetooth Low Energy logging by placing a Mac-in-the-middle", "url": "https://www.luminis.eu/blog-en/bluetooth-low-energy-logging-by-placing-a-mac-in-the-middle/", "updated_at": "2021-09-09T15:53:52", "body": "In the past two years or so I have been delving into Bluetooth Low Energy (BLE) for a project I did for one of our customers at Luminis Arnhem. In this project we have been tasked with implementing mobile applications for Android and iOS that used BLE to communicate with various products made by our customer. Because of this, a lot of my focus has gone to the BLE stacks that Apple and Google have created on their platforms for application developers to use.\nThe need for logging\nThese stacks take care of the lower levels of the BLE protocol for developers and in very many ways this is a good thing. I don\u2019t know if it is even possible, without first having to hack your own device, to completely create your own BLE stack for these platforms and use that on iOS and Android devices instead but I certainly would not recommend you to try it. One issue with using these stacks however, is that they make it hard as a developer to know what is actually being communicated at the protocol level. The API\u2019s that are offered are pretty high level so that they are easy to use, which is good. However the documentation provided with the API\u2019s is not very descriptive or transparant about how calling those API\u2019s translates to actual communication in the BLE protocol. So when your connection does not act as you expected it would and chances are that it will, what you really want to do is take a look at the lower level packets that are actually being transferred.\nCool, how do I do this?\nDepending on the platform that you are developing, for there may already be options available that you can use to get access to such logging without having to put in a lot of effort. For iOS devices with an OS version higher than 13 for example, Apple already offers a solution using PacketLogger that is fairly easy to set up. I have not actually tried this solution myself yet, but as the solution I am about to propose builds on some of the same concepts I can say with some confidence that this will work. In fact if iOS is the only target you are focussed on I would probably advise you to just use this instead and forget about my solution (Keep reading though! It never hurts to learn).\nIf you are also targeting Android, the challenge becomes a lot bigger. If you google logging ble on android\u00a0you will find plenty of blogs that explain how to enable\u00a0Bluetooth HCI snoop logging to achieve this, however I have a few problems with this. The main issue that I have with this solution is that it simply logs everything your device does over Bluetooth to a file and leaves you with the task of figuring out where that one bit of communication you were actually interested in went. There is no option to view the logs during the connection and any filtering that you may want to do on the traffic completely relies on you to find a tool or way to do that. Secondary, getting this logging to work on your device can be quite difficult as the steps required also tend to differ based on your device and Android version.\nAnother option you could consider is to get what is called a packet sniffer, one that supports BLE. Packet sniffers are devices that allow you to listen in on traffic. Usually you can plug them into your computer and they come with software that allow you to view the traffic and filter it. The problem with this option is that there are actually two types of such devices for BLE, singleband and multiband sniffers. You can buy a singleband packet sniffer for around 50 euros which is affordable enough, but these do not really work well for our purposes. A connection between two devices over Bluetooth Low Energy tends to jump between bands multiple times during the connection. As singleband sniffers by their design are only able to listen in on one band at a time you will find it difficult if not impossible to capture a complete connection. Multiband packet sniffers on the other are able to listen in on multiple bands at the same time and should therefore not have these difficulties. Multiband packet sniffers however are a lot harder to come by and when you do find them they can easily cost ten times as much as a singleband sniffer.\nWhat\u2019s Mac got to do with it?\nSo the Mac then? Earlier when talking about logging BLE traffic for iOS I already mentioned PacketLogger. PacketLogger is a tool that comes with the\u00a0Additional Tools for XCode. It allows you to view a live log of all the Bluetooth traffic going to and from your Macbook. Various BLE protocol layers and message types are coloured differently to easily detect certain messages and you can also filter the traffic in a few ways and save log files to view them later on. It is a great tool and I have used it a lot to analyse problems in BLE connections. But PacketLogger can only log traffic to and from your Mac, how will this help us log traffic between our application and another device? Well XCode allows you to create applications that run on your Mac and by using CoreBluetooth, the BLE stack that Apple has created for iOS and MacOS applications, it is not that hard to create a small application that acts as a proxy between the peripheral and central in your BLE connection. Which is exactly what I did and you can use it too!\nHow do I use it?\nThe first thing you need is some basic information about the peripheral you want to use this on. The information you need to know is:\n\nThe name the peripheral advertises with.\nThe password for the peripheral, if it uses a secure BLE connection.\nThe Services offered by the peripheral, or at least the ones that you wish to use in your app.\nThe Characteristics on the Services offered by the peripheral. Once again you would at least need to know about the ones you wish to use in your app.\n\nIf you plan on developing an application that communicates with your peripheral then you will probably have all of this information already anyway.\nThe second step to take it to check out the\u00a0BLEProxy Git repo. Within the repository you will find a project that you can open using XCode. When you do so, open up the BLEConstants.swift file. In this file are a few constants that you need to fill:\n\nDEVICE_NAME: The name the peripheral advertises with.\nSERVICES_AND_CHARACTERISTICS: UUID\u2019s of the services and characteristics on the peripheral.\n\nAfter filling these values with the correct values, you can start the project using XCode to run it on your local Mac.\nWant to know how to build your own version of this application? Check out the README.\n", "tags": [], "categories": ["Blog", "Internet of Things"]}
{"post_id": 29342, "title": "Deploying Spring Boot applications to AWS App Runner with AWS CodePipeline", "url": "https://www.luminis.eu/blog-en/cloud-en/deploying-a-spring-native-application-to-aws-app-runner/", "updated_at": "2022-06-24T09:23:49", "body": "In a previous post, we looked at AWS App Runner. AWS App Runner is a container service that lets you focus on your application and allows you to deploy your application in minutes without setting up any infrastructure.\nAWS App Runner supports two source options for your App Runner service:\n\nBy pointing to a Git(Hub) repository that contains your application source code\nFrom an existing container image stored in ECR (public or private is both possible)\n\nThe code-based source option only supports two languages at the moment: Node.js and Python. Your source also has to be stored on Github, but I expect AWS to add CodeCommit as a valid option pretty soon though. Now in my case, I would like to deploy a Java / Spring Boot based application to AWS App Runner, so my best bet, for now, is to use a container image based deployment. Now we could go for a pre-build image, but what\u2019s the fun in that right? So in this post, we will take a look at a setup that you can leverage to deploy a Spring Boot based application to AWS App Runner.\nThe overall setup\nOur source code will be hosted in Github and we are leveraging CodePipeline + CodeBuild to build and test our Java application. If the build and test are successful we push the resulting container image to a private container registry in ECR. App Runner can then pick up the container image for deployment.\n\nNow that we have a clear overall idea of what we need, let\u2019s create the build and deployment stack with AWS CDK.\nSetting up the basics\nThe first thing we will need is a private container image repository in ECR. This will allow us to store our container image and can be used later by App Runner to get our application as a container.\n\nWith the image repository in place, we can continue with the next step, creating a CI pipeline for our project.\n\nThe build pipeline makes sure that any code change will result in an update of our service. As you might have seen in the code snippet above we only build from the master branch, so any PR request, merge or commit to master will trigger a new build and will result in a new container image. In the App Runner service, which we will define later on, we can choose if we want App Runner to automatically deploy a new version of our application once it\u2019s available. To instruct CodeBuild to build our maven based project and create a docker image out of that we can do so with a custom buildspec.yml file.\n\nAs you can see we\u2019ve split the build into 3 separate phases:\n\nWe log into ECR and we create a hash for the image tag that we will create later on.\nWe build the maven project and leverage the Spring Native maven plugin to create an a native image. For tagging with the correct container registry, we inject the repository location as an environment variable into our build process. As a final step, we tag the created docker image.\nWe push the docker image to our ECR repository.\n\nTo make sure the App Runner service can fetch the docker image from our ECR repository, we will need to create and assign a role that has the permissions to do so.\n\nCreating the App Runner service\nNow for the last and final part of our setup, we will need to create the AWS App Runner service via CDK.\n\nIf we look at the above code snippet we\u2019ve setup five configuration options for App Runner:\n\nThe health check endpoint.\nThe role required to access the image in ECR\nThe image source and the Port that our container will listen on\nThe service name\nThe type of resources (memory/cpu) required to run our service\n\nNow we have all the code we need for CDK to create the entire stack and create the service for us. It\u2019s just a matter of running cdk deploy and you will have the entire stack up and running.\n$ cdk deploy\nIf we want to know the URL of our new service, we can leverage a CDK `CnfOutput` construct in which you can request the URL of the service and it will be printed out once the stack is finished deploying.\n\nWhen CDK is done with the deployment you will be able to find the URL to your service in the output of the CDK deploy.\n\r\nDo you wish to deploy these changes (y/n)? y\r\napprunner-runtime-stack: deploying...\r\napprunner-runtime-stack: creating CloudFormation changeset...\r\n\r\n\u2705 apprunner-runtime-stack\r\n\r\nOutputs:\r\napprunner-runtime-stack.serviceUrl = https://someid.eu-west-1.awsapprunner.com\r\n\n\u00a0\nSummary\nEven if you\u2019re not using a language supported by AWS App Runner, it\u2019s still pretty straightforward to deploy your service to AWS App Runner. You can simply use your existing pipeline or create a new build pipeline in AWS CodePipeline that will result in an image in ECR, from which App Runner can do the rest. For this service, I\u2019ve chosen to use Spring Native. Spring Native will create a native image for your Spring Boot application, which results in a much faster application startup. In my case, for a simple application, the time it takes for the application to start is about 500ms instead of 3 seconds (non-native image). When you expect your application to retrieve traffic spikes that might trigger app runner to scale out , this improvement can help for sure.\nLearn from our experts:30 Jan 2021-Bj\u00f6rn LammersTraining: Spring CloudTo be able to fully take advantage of the benefits of the Cloud, the Cloud environment needs to be taken into consideration while applications are developed. Applications should be designed to be cloud Native. Description This workshop will explain the...\n", "tags": ["aws", "cdk", "cloud", "containers", "java"], "categories": ["Blog", "Cloud", "Development"]}
{"post_id": 28220, "title": "A first impression of AWS App Runner", "url": "https://www.luminis.eu/blog-en/cloud-en/a-first-impression-of-aws-app-runner/", "updated_at": "2022-06-24T09:25:14", "body": "About three months ago AWS released a new service named\u00a0AWS App Runner. After reading the introduction blog post, I got pretty excited to check it out. AWS App Runner is a new service that provides organizations with a fast, simple, and secure way to deploy containerized applications on the AWS platform without managing any infrastructure. AWS already offers a wide range of container based services like AWS Fargate, ECS, Elastic BeanStalk, and AWS EKS, so why did they come up with App Runner?\nWhat makes App Runner different from the other services?\nLet\u2019s see how AWS describes App Runner.\nAWS App Runner is a fully managed service that makes it easy for developers to quickly deploy containerized web applications and APIs, at scale and with no prior infrastructure experience required. Start with your source code or a container image. App Runner automatically builds and deploys the web application and load balances traffic with encryption. App Runner also scales up or down automatically to meet your traffic needs. With App Runner, rather than thinking about servers or scaling, you have more time to focus on your applications.\nTo me the key selling point for App Runner is the fact that it\u2019s very easy to use and a secure and\u00a0fully managed service. It\u2019s an opinionated architecture that makes it really easy to run containerized applications in AWS. App Runner offers another level of abstraction for the complex ecosystem of container runtime and orchestration options. Compared to the existing AWS container services there is almost no learning curve as you only need to point to a GitHub repo or a container image in ECR and provide\u00a0some configuration settings for security, a default number of instances and required resources for a single instance, and it will create a secure, fully load-balanced and autoscaling service.\nScreenshot of the AWS Console showing the different source and deployment options for an App Runner service.\nScreenshot of the AWS Console showing the different service settings for an App Runner service.\nWhat\u2019s hiding under the hood of AWS App Runner?\nApp Runner is built upon a wide range of different AWS services:\n\nAWS CodeBuild \u2013 For building, testing, and packaging the application. Only used if you choose to build from source. Supported languages are Node.js and Python.\nAWS Fargate & AWS ECS \u2013 Used for the underlying managed container orchestration platform.\nAWS Auto Scaling \u2013 Makes sure that the application scales based on the number of concurrent requests.\nAWS Elastic Load Balancing \u2013 Makes sure that the load is evenly distributed amongst the different instances of the service.\nAmazon CloudWatch \u2013 Used for storing App Runner Logs (events, deployments, and application) and metrics.\nAWS Certificate Manager \u2013 Used to provide out-of-the-box SSL/TLS certificates for the service endpoint.\nAWS KMS \u2013 Used to encrypt copies of the source repository and service logs.\n\nNext to that, you can also configure/register your own (sub-) domain for the service, which presumably is using Route53 (could not find that anywhere).\nAs you can see that\u2019s a whole bunch of services, configuration, provisioning, and management you don\u2019t have to think about. AWS App Runner hides/abstracts this from you and lets you focus on your application and business problems. Pretty neat, huh?\nSo what\u2019s missing?\nIt\u2019s a new service so some features are not (yet) available which you might find in similar services. While experimenting with App Runner I noticed a couple of them, so let\u2019s take a look.\n\nNo native support for Parameter Store or Secrets Manager when configuring an App Runner service. You can still use those services via the SDK within your application, but you can\u2019t use them for instance to configure some environment variables for your container.\nAs far as I could see App Runner does not (yet) allow you to work with resources inside a private VPC (for instance an RDS instance). UPDATE: App runner now supports resources inside your private VPC (https://aws.amazon.com/blogs/containers/deep-dive-on-aws-app-runner-vpc-networking/)\nIf you\u2019re a fan of AWS CDK, you will have to keep in mind that CDK only offers L1 constructs for creating an App Runner service for now. No L2 support yet, but it seems that\u2019s on the roadmap. Update: As of CDK version 1.126.0, CDK also has L2 support for App Runner.\nApp Runner only supports the blue/green deployment model, so other options like rolling, canary or traffic split are not an option right now.\nNo other container repositories besides ECR are supported.\nNo other git repositories besides GitHub are currently supported. I expect them to add CodeCommit pretty soon.\nAs with most new services App Runner is currently only available in a few regions: Europe (Ireland),\u00a0Asia Pacific (Tokyo),\u00a0US East (N. Virginia),\u00a0US East (Ohio),\u00a0US West (Oregon).\nThere is no support yet for languages like Java, Go, Rust, Ruby, etc. So if one of those is your favorite programming language you will have to create a container image before you can launch the service in App Runner.\nApp Runner does not allow you to scale to zero instances. You will always have a single instance running and will be charged for the allocated resources.\n\nPricing\nWhen you look at the pricing mentioned on the AWS App Runner page you might think it\u2019s pretty cheap with $ 5 dollar a month, but there is more to it. They mention that it costs about $ 5 a month, but while reading the small letters it says that that\u2019s the case for an app running a single instance that is paused for about 22 hours a day. With App Runner, you are charged for the compute and memory resources used by your application. From what I noticed while running a service is that from a CPU perspective you seem to be only charged for the actual CPU resources spent. If your application is not being used, you are not consuming CPU and therefore are not billed for CPU usage. Memory on the other hand will stay reserved for your application and you will be billed accordingly. You will also be charged for additional App Runner features like building from source code or automating your deployments. Pricing therefore might not be straightforward at first, but also not too complicated to figure out. Before you start migrating your apps to App Runner be sure to try and estimate your bill as it might not be worthwhile to migrate from a micro EC2 instance to App Runner.\n\nFinal thoughts\nThe first time I read the announcement about App Runner it made me think about Google Cloud Run. I see App Runner as the AWS response to Google Cloud Run. It has a strong opinionated architecture and built on top of other great AWS services. The ease of use is really great and without having a lot of experience with containers you can get started really quickly. I\u2019ve tested App Runner with a bunch of Spring Boot applications and it was really easy to get an application up and running within a couple of minutes. I think App Runner can be very useful for creating small applications with use cases such as REST APIs or web applications. I think it\u2019s great for rapid prototyping and deploying PoCs and MVPs.\nThere is a public roadmap that starts to take shape with some great features. Popular tools used by the AWS community are actively adding support for App Runner.\u00a0I\u2019m looking forward to seeing where App Runner will be in a year from now.\nAdditional resources\n\nAWS App Runner Workshop \u2013 Great if you want to get started with some hands-on exercises.\nAWS App Runner Deep Dive (video) \u2013 Really nice overview of the services and gives some good insights into how features like auto-scaling works.\n\nFeel free to leave a comment or tweet at @jreijn or @luminis_eu!\nLearn from our experts:8 Feb 2021-Mark van KesselTraining: AWS SecurityCloud computing offers unrivaled possibilities with its vast array of services and features allowing an unprecedented speed of development. But using it in the wrong way can lead to serious risks. Hackers, data-leaks, DDoS attacks, bad actors, the internet is...\n8 Feb 2021-Mark van KesselTraining: AWS MonitoringWith the rise of Devops and the \u201cyou build it, you run it\u201d mentality, it is more important than ever for teams to have a thorough understanding of how their applications are running and performing. With the ever-increasing complexity of...\n", "tags": ["aws", "cloud", "containers"], "categories": ["Blog", "Cloud", "Development"]}
{"post_id": 24847, "title": "Flash around your AWS badge of honor.", "url": "https://www.luminis.eu/blog-en/flash-around-your-aws-badge-of-honor/", "updated_at": "2021-11-11T16:58:46", "body": "\n\u00a0\nThis blog will go to detail into:\n\nWhy one should or should not certify.\nMy experience with AWS.\nPractical tips to study for the exam.\nThe result I achieved.\n\n\u00a0\nWhy certify?\nThe title might sound like I am immediately going to advocate getting certified on AWS but the first question to ask is always why?\nWhy would someone need a certification/badge? How does a standardized test make you as a developer more knowledgeable?\n\nThe short answer is that you probably don\u2019t need to get certified in order to get the knowledge that you need to do your job right.\nTo me, the main benefit is that someone else (e.g. employer) can quickly validate that you have a certain level of knowledge required to handle their systems in AWS. Other than that, it\u2019s a nice validation to yourself that you have the skills required to work with AWS.\nThis does not mean that it\u2019s entirely useless to pursue a certification, it is the exact opposite. Regardless of the current amount of knowledge that you currently possess, the journey of getting AWS certified will give you a lot more information and some experience. By having to answer most of the questions correctly you will learn about topics you might not have touched before your journey of getting certified. I experienced it as a moment of \u201cUltra Learning\u201d where I learn so much that I can apply it in my daily work immediately after.\n\u00a0\nMy journey into AWS\nIn October 2019 I started working at Luminis as my first full-time job. The project I started with required the use of AWS IoT. This is a specialized service for building internet of things services.\nAs a front-end engineer, I was tasked to build an Angular application directly on top of the HTTP API that Amazon Web Services provides. In order to access the AWS services, we needed temporary credentials, which let met to using Cognito through AWS Amplify. To host the website we used CloudFront as a CDN and S3 as the storage.\nThis project got me interested in AWS and to learn more about it in a theoretical manner and it was also a huge help in studying for the first exam. I chose to take the developer exam before doing the cloud practitioner exam because I was already experienced.\n\u00a0\nHow do you get more knowledge and experience for the AWS exam?\nFirst of all, it is great to review the official AWS page: https://aws.amazon.com/certification/certification-prep/\nIf you like to read more about AWS to get a broad overview first, I suggest you look at some of these cheat sheets to get some more information about all the services and concepts related to AWS:\nhttps://tutorialsdojo.com/aws-cheat-sheets/\nTo get more knowledge for the AWS developer associate exam that I took, I followed the Udemy course AWS developer associate. Although this was a great way to get a guided tour through several AWS services, this would not have been enough for me to pass the exam.\nFor that I have a few more tips listed below.\n1. Look at some of the product (services) FAQs\nStudying for the exam, you mainly have to learn about AWS services. A good way to learn about them is by reading their FAQs. This will list all the common questions others have asked about the services.\nThis is not something I did for ALL of the services that AWS is offering. However, I found it really useful to get an idea of what these services are for. Examples I looked at are:\n\nIAM\nKMS\nAWS X-Ray\n\nMore advanced products:\n\nKinesis data firehose\n\n2. Take practice exams.\nPractice exams will get you familiar with the way AWS asks you questions and what kind of knowledge you need to possess in order to pass the exam. They can also tell you whether or not you are ready to schedule a real exam.\nThe practice exam service that I used is called\u00a0Whizlabs for which you can buy exams through a one-time purchase. They also give one free exam for you to try them out first. That one has less questions, so it is not representative of the other practice exams. You could also purchase an official AWS practice exam, but that one is more expensive.\nWhen you already completed an exam successfully you will get a free official practice exam for your next certification and you will also get a 50% discount towards your next exam. These can be found on https://www.certmetrics.com/amazon/candidate/benefit_summary.aspx\nThere are five practice exams that you can make:\n\nThis is what the results look like:\n\nThe added benefit of learning through the practice exams is that you can check out extra information and even buy labs to get hands-on experience based on a question you got wrong or right:\n\n3. Saving links as resources that you didn\u2019t know about when making practice exams.\nFor me one of the links I saved was:\n\nParameter store\n\nThis is something I wanted to do more before taking the exam, but I didn\u2019t get around to it because I had already planned my exam in advance. Although it is very helpful to do in the beginning while studying.\nI also made notes from the Udemy course that I took, so I only saved one link from my Whizlabs journey. For my next exam, I will save more links to resources based on Whizlabs and I will not be taking a Udemy course. This because a video-based course makes me learn slower than being confronted with exam questions and further references after making a mistake.\n4. Doing the exam preparation training created by AWS themselves.\nJust a few days before taking your exam you have to follow the following course: https://explore.skillbuilder.aws/learn/course/internal/view/elearning/42/exam-readiness-aws-certified-developer-associate-digital\nThis course will go into detail on how AWS will ask questions and it will give an interactive way of stressing the important details that exist in the question.\nFor example, you will notice that usually the first sentence of a question is not informative for answering the question.\nThe course is free to do and takes a few hours to complete. For me this brought my practice exam score from 67% up to 84%.\n5. Schedule your exam before you think you are completely finished studying.\nIn order to actually go through with your plans of getting certified, I recommend planning the exam before you think you are fully ready for it. This will make sure that you put some more effort into getting your practice exam score above the required 80% to be ready for the real exam. My experience was that I put in 3x more studying efforts in the last two weeks leading up to the exam.\n\u00a0\nWhat you will get in return for the effort you put in\nIn the end, this is what you gave all that effort for:\n\nOn the 1000 point scale, I got an 880 grade. The minimum to pass the exam is 720. Click here to get more information on the scoring system.\nFor the cost of $150 (~\u20ac135) I wouldn\u2019t personally spend this amount of money on the exam, especially since there is a tax portion included which would be more beneficial to pay through your own company or an employer for write-off benefits.\n\u00a0\nEnding notes\nAt the company I am working for, we are continuously working to develop our cloud knowledge. In the year 2021 we started participating in \u201ccloud the game\u201d, a game to learn cloud technologies made by colleagues to be used by other colleagues.\nIf this seems like something fun for you to partake in, you can always have a look at the\u00a0careers page\u00a0and be sure to mention my name once you decide to apply.\nMy professional goals for 2021 are to learn more about security in AWS. I will be doing this by getting the\u00a0AWS Certified Security \u2013 Specialty\u00a0certification.\n", "tags": [], "categories": ["Blog", "Cloud"]}
{"post_id": 27259, "title": "Untangle your micro-service driven application with Azure API Management", "url": "https://www.luminis.eu/blog-en/cloud-en/untangle-your-micro-service-driven-application-with-azure-api-management/", "updated_at": "2024-02-07T15:49:32", "body": "In this blog I will introduce you with the basics of Azure API Management. Firstly, I will start by introducing an use case for API management: a merger of the webservices of two online web shops. Additionally, I will show you how to create the APIM instance, a product, operations and policies. This is preferably all done using ARM templates.\nAZ-204 & Azure API Management\nIn my last blog I wrote about the renewed Azure AZ-204 exam. I had the intention to write some more blogs. I used the blogs as a way to learn for the exam, while sharing knowledge at the same time. Well, that did not go as planned.\nI found that writing and studying did cost way more time than only studying. Who would have thought so.. Nonetheless, I have completed the AZ-204 exam, and I am going to continue sharing information about the topics.\nIn this blog I will show you Azure API Management, as it is part of the exam, but also because of the added value it offers to your micro-service driven application.\nMotorcycleParts.com\nWe are building an online shop for a motorcycle parts dealer, called MotorcycleParts. It consists of multiple Azure Functions, including:\n\nProduct-service\n(Shopping) Basket-service\nProduct-stock-service\n\nThe company recently bought a car part dealer called CarPartz. The CarPartz site is going to be incorporated into the MotorcyleParts site. CarPartz recently updated their whole backend for their shop, so the requirement is to reuse their backend in the current state.\nAPI Management is going to help us to organize the calls to our functions, as well as incorporating the CarPartz backend into the MotorcycleParts site.\n\u00a0\n\nAPI management\nAPI management, in short APIM, is placed in front of our backends. It gives us the possibility to present a single endpoint to our website, instead of multiple URLs for different Azure Functions. This way the website does not have to know about all the different backend service that may exist. All the routing is done by APIM.\nIt also allows us to switch one of the backends, for a totally different implementation. Due to APIM there is a low coupling between the front-end and the backends. APIM can also help us with IP restriction, rate-limiting, and more.\nFirst off, we are going to create an APIM instance. This is possible via multiple ways: ARM templates, Azure CLI, Portal and Powershell. I am going to create the instance using an ARM template. I always choose ARM in favor of the other options since it give us the ability to use version control and it is less manual work, in other words, less error prone. The same can be said about Powershell and the Azure CLI when used in a script.\n{\r\n    \"$schema\": \"https://schema.management.azure.com/schemas/2019-04-01/deploymentTemplate.json#\",\r\n    \"contentVersion\": \"1.0.0.0\",\r\n    \"parameters\": {\r\n        \"publisherEmail\": {\r\n            \"type\": \"string\",\r\n            \"minLength\": 1,\r\n            \"metadata\": {\r\n                \"description\": \"Email owner service\"\r\n            }\r\n        },\r\n        \"publisherName\": {\r\n            \"type\": \"string\",\r\n            \"minLength\": 1,\r\n            \"metadata\": {\r\n                \"description\": \"Name owner service\"\r\n            }\r\n        },\r\n        \"location\": {\r\n            \"type\": \"string\",\r\n            \"defaultValue\": \"[resourceGroup().location]\",\r\n            \"metadata\": {\r\n                \"description\": \"Location to deploy to\"\r\n            }\r\n        }\r\n    },\r\n    \"variables\": {\r\n        \"apimInstanceName\": \"apim-poc-prod-01\",\r\n        \"apimProductName\": \"motorcyle-parts\",\r\n        \"apimApiProductServiceName\": \"product-service\",\r\n        \"getProductsOperation\": \"[concat(variables('apimInstanceName'), '/', variables('apimApiProductServiceName'), '/get')]\",\r\n        \"getProductByIdOperation\": \"[concat(variables('apimInstanceName'), '/', variables('apimApiProductServiceName'), '/get-by-id')]\"\r\n    },\r\n    \"resources\": [\r\n        {\r\n            \"type\": \"Microsoft.ApiManagement/service\",\r\n            \"apiVersion\": \"2019-12-01\",\r\n            \"name\": \"[variables('apimInstanceName')]\",\r\n            \"location\": \"[parameters('location')]\",\r\n            \"sku\": {\r\n                \"name\": \"Consumption\",\r\n                \"capacity\": 0\r\n            },\r\n            \"properties\": {\r\n                \"publisherEmail\": \"[parameters('publisherEmail')]\",\r\n                \"publisherName\": \"[parameters('publisherName')]\"\r\n            },\r\n            \"resources\": []\r\n        }\r\n    ]\r\n}\r\n\n\nAs you can see above, I have created an ARM template that uses the bare minimum to create the APIM instance. The first parameter, publisherEmail, is required to receive notifications about your APIM instance. This email address is for example used to notify you when the creation of the instance is finished. The creation of an instance can sometimes take up to one hour.\nFor this blog I chose to create APIM in a consumption plan, since all the Azure Functions are also on a pay-per-use plan. The consumption plan has, in contrary to a normal subscription, some usage limits and, custom domain names are not supported. The documentation shows all the differences and the limitations that apply.\nYou can deploy this template, and after a while the APIM instance will be available. But without API definitions, APIM is not going to do anything. We can add API definitions through the portal. When you navigate to the APIM Resource -> APIs, it should look like this. But again, I like to automate as much as possible, so I will use ARM templates.\n\n\nAdding APIs to APIM\nAs described in the introduction, the APIM instance should make four endpoints of the backend services available. Three new Azure Functions and an external system which is, for the sake of the difference, running on an Azure Kubernetes cluster. We are going to add the endpoints using the ARM template.\nI like to start by creating a product in APIM. Using a product you can group one or more APIs. It is possible configure some stuff on product-level. In the example below you can see the product definition. For the product I only defined a name and a reference to the API that is going to be part of the product. The API on the contrary, contains more properties. The path defines the route we can call from the APIM perspective. The service-url defines the base url of the Azure Function.\n\n\n{\r\n    \"type\": \"Microsoft.ApiManagement/service\",\r\n    \"apiVersion\": \"2019-12-01\",\r\n    \"name\": \"[variables('apimInstanceName')]\",\r\n    \"location\": \"[parameters('location')]\",\r\n    \"sku\": {\r\n        \"name\": \"Consumption\",\r\n        \"capacity\": 0\r\n    },\r\n    \"properties\": {\r\n        \"publisherEmail\": \"[parameters('publisherEmail')]\",\r\n        \"publisherName\": \"[parameters('publisherName')]\"\r\n    },\r\n    \"resources\": [\r\n        {\r\n            \"name\": \"[variables('apimProductName')]\",\r\n            \"type\": \"products\",\r\n            \"apiVersion\": \"2019-12-01\",\r\n            \"properties\": {\r\n                \"displayName\": \"[variables('apimProductName')]\"\r\n            },\r\n            \"dependsOn\": [\r\n                \"[resourceId('Microsoft.ApiManagement/service', variables('apimInstanceName'))]\"\r\n            ],\r\n            \"resources\": [\r\n                {\r\n                    \"name\": \"[variables('apimApiProductServiceName')]\",\r\n                    \"type\": \"apis\",\r\n                    \"apiVersion\": \"2019-12-01\",\r\n                    \"dependsOn\": [\r\n                        \"[resourceId('Microsoft.ApiManagement/service', variables('apimInstanceName'))]\",\r\n                        \"[concat(resourceId('Microsoft.ApiManagement/service', variables('apimInstanceName')), '/products/', variables('apimProductName'))]\",\r\n                        \"[concat(resourceId('Microsoft.ApiManagement/service', variables('apimInstanceName')), '/apis/', variables('apimApiProductServiceName'))]\"\r\n                    ]\r\n                }\r\n            ]\r\n        },\r\n        {\r\n            \"name\": \"[variables('apimApiProductServiceName')]\",\r\n            \"type\": \"apis\",\r\n            \"apiVersion\": \"2019-12-01\",\r\n            \"dependsOn\": [\r\n                \"[resourceId('Microsoft.ApiManagement/service', variables('apimInstanceName'))]\"\r\n            ],\r\n            \"properties\": {\r\n                \"displayName\": \"[variables('apimApiProductServiceName')]\",\r\n                \"path\": \"product-service\",\r\n                \"serviceUrl\": \"https://apim-poc-test.azurewebsites.net/api\",\r\n                \"protocols\": [\r\n                    \"http\",\r\n                    \"https\"\r\n                ]\r\n            },\r\n            \"resources\": []\r\n        }\r\n    ]\r\n}\r\n\n\n\nOperations\nOne key ingredient is still missing, operations. Operations are the actual API calls that will be available from APIM. I have created two operations. One for retrieving all products, and a second to filter by id. The defined url template is appended to the API service-url we defined earlier.\nMost properties of the operation are self-explanatory. The templateParameters, are the parameters that we expect to receive at the APIM endpoint. We can also define required or optional query parameters and headers. For POST requests we define an expected request body.\n\n\n\n{\r\n    \"name\": \"[variables('apimApiProductServiceName')]\",\r\n    \"type\": \"apis\",\r\n    \"apiVersion\": \"2019-12-01\",\r\n    \"dependsOn\": [\r\n        \"[resourceId('Microsoft.ApiManagement/service', variables('apimInstanceName'))]\"\r\n    ],\r\n    \"properties\": {\r\n        \"displayName\": \"[variables('apimApiProductServiceName')]\",\r\n        \"path\": \"product-service\",\r\n        \"serviceUrl\": \"https://apim-poc-test.azurewebsites.net/api\",\r\n        \"protocols\": [\r\n            \"http\",\r\n            \"https\"\r\n        ]\r\n    },\r\n    \"resources\": [\r\n        {\r\n            \"name\": \"get-all\",\r\n            \"type\": \"operations\",\r\n            \"apiVersion\": \"2019-12-01\",\r\n            \"dependsOn\": [\r\n                \"[concat(resourceId('Microsoft.ApiManagement/service', variables('apimInstanceName')), '/apis/', variables('apimApiProductServiceName'))]\"\r\n            ],\r\n            \"properties\": {\r\n                \"displayName\": \"get_products\",\r\n                \"method\": \"GET\",\r\n                \"urlTemplate\": \"/products\"\r\n            }\r\n        },\r\n        {\r\n            \"name\": \"get-by-id\",\r\n            \"type\": \"operations\",\r\n            \"apiVersion\": \"2019-12-01\",\r\n            \"dependsOn\": [\r\n                \"[concat(resourceId('Microsoft.ApiManagement/service', variables('apimInstanceName')), '/apis/', variables('apimApiProductServiceName'))]\"\r\n            ],\r\n            \"properties\": {\r\n                \"displayName\": \"get_product_by_id\",\r\n                \"method\": \"GET\",\r\n                \"urlTemplate\": \"/products/{id}\",\r\n                \"templateParameters\": [\r\n                    {\r\n                        \"name\": \"id\",\r\n                        \"type\": \"number\",\r\n                        \"required\": true\r\n                    }\r\n                ]\r\n            },\r\n            \"resources\": []\r\n        }\r\n    ]\r\n}\r\n\n\n\n\nNow we can call APIM, which will redirect our calls to the Azure Function. You can test your APIM implementation using any REST client, but also via the Azure Portal. Once you have deployed your ARM template, the defined operations are shown in the portal. Using the test tab, you can test your operation. For APIM development I prefer this over any other rest client, since it shows you an extensive trace of the retrieved request and how it is passed on to the backend service. In my case the call to retrieve all products is working just fine. Below you can see how the APIM url is translated to the correct Azure Function url.\n\n\nPolicies\nAs I tried to retrieve a single product, I came to the conclusion that the endpoint is not working yet. I forgot that the Azure Function expects the product id via a query parameter. The APIM get-by-id operation defines the product id as a route parameter. Since the path of the APIM request is just appended to the service url of the API, I am getting a 404 response. The Azure Function only has an endpoint on /products, \u00a0there is no endpoint /products/{id}.\nIn order to redirect the get-by-id request to the Azure Function, we need to introduce something to rewrite the calls to APIM. This is where policies come into play. Policies allow you to change the behavior of APIM. APIM policies are defined in XML. There are multiple types of policies:\n\n\n\nPolicy Type\nUsage\n\n\nInbound\nStatements to apply to the request\n\n\nBackend\nStatements to apply before the request is forwarded to the backend service\n\n\nOutbound\nStatements to apply to the response of the backend\n\n\nOn-error\nStatements to apply in case of an error\n\n\n\n\u00a0\nThere are lots of policies you can create. For this example I am going to create a rewrite inbound policy. This means we are going to alter the requested path to another path. Below you can see how I have defined the policy. It is really basic. You can implement more advanced stuff, like IP-filtering and rate-limiting, as well. Keep in mind that it is possible to create policies in operation scope, but also in global, product and API scope.\n\n\n\n\n\n\n<policies>\r\n    <inbound>\r\n        <base />\r\n        <rewrite-uri template=\"/products?id={id}\"/>\r\n    </inbound>\r\n    <backend>\r\n        <base />\r\n    </backend>\r\n    <outbound>\r\n        <base />\r\n    </outbound>\r\n    <on-error>\r\n        <base />\r\n    </on-error>\r\n</policies>\r\n\n\n\n\n\n\n\n\nThe last thing to do, is to add this policy to the ARM template. In order to add the policy, you need to join the lines so it becomes a single line, and escape the quotes. Now we can add another resource to the get-by-id operation, a policy type resource. In the value property we can provide the xml of the policy.\n\n\n\n{\r\n    \"name\": \"get-by-id\",\r\n    \"type\": \"operations\",\r\n    \"apiVersion\": \"2019-12-01\",\r\n    \"dependsOn\": [\r\n        \"[concat(resourceId('Microsoft.ApiManagement/service', variables('apimInstanceName')), '/apis/', variables('apimApiProductServiceName'))]\"\r\n    ],\r\n    \"properties\": {\r\n        \"displayName\": \"get_product_by_id\",\r\n        \"method\": \"GET\",\r\n        \"urlTemplate\": \"/products/{id}\",\r\n        \"templateParameters\": [\r\n            {\r\n                \"name\": \"id\",\r\n                \"type\": \"number\",\r\n                \"required\": true\r\n            }\r\n        ]\r\n    },\r\n    \"resources\": [\r\n        {\r\n            \"name\": \"policy\",\r\n            \"type\": \"policies\",\r\n            \"apiVersion\": \"2019-12-01\",\r\n            \"dependsOn\": [\r\n                \"[concat(resourceId('Microsoft.ApiManagement/service', variables('apimInstanceName')), '/apis/', variables('apimApiProductServiceName'), '/operations/get-by-id')]\"\r\n            ],\r\n            \"properties\": {\r\n                \"value\": \"<policies> <inbound> <base /> <rewrite-uri template=\\\"/products?id={id}\\\"/> </inbound> <backend> <base /> </backend> <outbound> <base /> </outbound> <on-error> <base /> </on-error> </policies>\",\r\n                \"format\": \"xml\"\r\n            }\r\n        }\r\n    ]\r\n}\r\n\n\n\n\n\nConclusion\nWe have just created an APIM instance and added the definition of two endpoints of one service. In the same way you can add APIs for the remaining Azure Functions and the Kubernetes cluster.\nI have had some troubles fixing errors in my ARM template. I managed to resolve them using the specification for the ARM template which you can find here. Other great examples for the ARM templates are available in the Microsoft Azure Quickstart Github repo.\n\nIn retrospect, I did not like how verbose the ARM template became. I found the definition of the policy in the ARM template to verbose and error prone. I will look for a cleaner way to do so.\n\n", "tags": ["Api Management", "Azure", "Azure Function", "Micro-services"], "categories": ["Blog", "Cloud"]}
{"post_id": 29324, "title": "Luminis Reaches New Milestone in AWS Partnership", "url": "https://www.luminis.eu/blog-en/news-en/luminis-reaches-new-milestone-in-aws-partnership/", "updated_at": "2021-08-10T16:07:51", "body": "Software and technology company Luminis has achieved another milestone in its AWS partnership this month. With more than 50 AWS certifications, Luminis joins a select group of organisations in the Netherlands.\n\nA strategic choice\nIn recent years, Luminis has invested heavily in cloud knowledge and expertise, with a major focus on Amazon Web Services (AWS). Because of this Luminis is now increasingly being asked for challenging cloud assignments.\n\nWorking with the latest cloud technology is not something that has only happened in recent years. Luminis has followed the rise of the Cloud since the start of AWS in 2006, and had already done several projects with AWS solutions before 2010.\nBert Ertman, VP Technology at Luminis, says: \u201cOver the past 5 years, the Cloud has become mainstream. Many companies now use cloud solutions, from hosting to cloud applications. Yet we encounter major differences among companies. Particularly in fields like software development and data analytics, solutions from before the cloud era are still widely used. As a result, organisations often lag behind in terms of speed and innovation. Understanding cloud technology in its entirety and applying it at all levels gives you a head start. That is why access to the best cloud knowledge and partnerships is of strategic importance to Luminis.\u201d\nInvesting in Cloud Proficiency\nA lot has happened in the field of cloud at Luminis in recent years. Bert Ertman cites some examples: \u201cTo work on our cloud proficiency, a group of enthusiastic colleagues started \u2018Cloud the Game\u2018. A competition with many gamification elements such as badges, groups and mini-challenges. Anyone who achieves something in the field of cloud can log their achievements via our \u2018Cloud the Game\u2019 application. Achievements include obtaining a certification, giving a presentation, writing a blog or reading one. It is wonderful to see our colleagues started doing this en masse and that considerable competition arose. And not only the engineers took part, our sales people, marketers and directors also gained a lot of knowledge about the Cloud. What you achieve with something like Cloud the Game is that everyone becomes self-sufficient in the cloud area at their own pace and finds their way. At Luminis, we\u2019re working with the Cloud every single day.\u201d\nLuminis also organised \u2018AWS Game Day\u2018 with AWS. During this day, teams from different companies competed against each other. The goal was to keep an AWS infrastructure up and running as best as possible, while things were going wrong. This requires not only broad knowledge of all kinds of AWS services, but also problem-solving ability and collaboration. In the end, the Planon team took home the first prize. AWS Game Day is a great example of how learning goes hand in hand with fun and a healthy dose of competition.\nBeneficial for customers\nLearning new things is fun, but the ultimate goal is for our customers to benefit from the best AWS knowledge available in the market. Bert Ertman notices that not only more and more customers are finding us in this area: \u201cIn recent years we have invested a lot in our relationship and cooperation with AWS. Because we focus on AWS across the board, we see that we have easier access to specialists from AWS as well. We are even being asked by AWS to help expand their own professional services department, AWS Professional Services. That to me is the ultimate proof that our knowledge and experience is at the right level. In addition, more cloud engineers and architects are interested in working at Luminis, because they can work with the latest technology, which is very nice in the current market!\u201d\nAbout Luminis\nLuminis is a software and technology company with offices in Amsterdam, Apeldoorn, Arnhem, Eindhoven, Rotterdam and the UK. From these offices, 200 professionals work on high-tech solutions for customers, including major brands such as KLM, Nike and Bol.com. Luminis distinguishes itself by demonstrably leading the way in the field of (cloud) technology and innovation.\nWant to know more about AWS? Check out our AWS consultancy page, or contact us.\n", "tags": [], "categories": ["Blog", "News"]}
{"post_id": 28683, "title": "Enable B2B users on an Azure AD B2C tenant", "url": "https://www.luminis.eu/blog-en/cloud-en/enable-b2b-users-on-an-azure-ad-b2c-tenant/", "updated_at": "2021-08-10T16:05:44", "body": "For one of our customers, we created a set of B2B applications using Azure Active Directory (B2B) and Azure App Service Authentication (EasyAuth). Now they want to enable customers on one of the applications while still enabling single sign-on for (external) business users on all applications. To accomplish this, we used Azure AD B2C and linked it with the existing Azure AD tenant that already contains all business users. In this blog I will describe the individual steps and missteps we made. I will focus solely on the Azure AD-configuration and leave out any other changes.\nLet\u2019s assume the Azure AD tenant and the AD B2C tenant including the app registration and user flows are created and focus on linking the active directory tenants.\nCreate an identity provider\nConfigure the custom identity provider\nFirst create an app registration in the Azure AD tenant. This app registration will function as the Identity Provider for the Azure AD B2C tenant. The redirect uri is the authentication response endpoint of the B2C tenant. This uri has the format https://<b2c-tenant-id>.b2clogin.com/<b2c-tenant-id>.onmicrosoft.com/oauth2/authresp.\nNext configure a client secret. Remember the client id and the secret value since they are needed when configuring the identity provider in the B2C tenant.\nSwitch to the B2C-tenant to create a new identity provider. Make sure it has a nice and recognizable name, since that name will be displayed on the sign-in and sign-up pages.\nProvide here the client id and client secret of the app we just created and build up the metadata url based on the following template: https://login.microsoftonline.com/<b2b-tenant-id>/v2.0/.well-known/openid-configuration.\nSign-in flow with corporate account\nThe last step is enabling the identity provider in the user flows. This is nothing more than selecting the OpenID Connect Identity provider in the required user flows. Do not forget to enable the identity provider in the sign up flow. See the \u2018issues I ran into\u2019 for more details. When testing the sign-in flow the \u2018corporate\u2019 sign-in button will be displayed.\nIssues I ran into:\nDeleting an identity provider\nWhile preparing the blogpost, I removed an identity provider that was in use by a user flow. This caused the user flow to be hidden and it was almost impossible to get it back again.\nAdmin consent for B2B user\nWhen signing in with a B2B account, the user is asked to consent. When the user has no admin privileges it will not be possible consent. Either an admin of his organization has to sign in first or you should give admin consent in the B2B App Registration on the Api Permissions-tab.\nSigning in with B2B user\nWhen signing in with a B2B user, I end-up with a 401-error on the \u2018callback\u2019-request to my application. When looking closer at the request in the network tab in the developer tools I discovered the following description:\nUser does not exist\nAlthough it\u2019s a business user registered in the B2B tenant, the user needs to sign up first. This will create the link between the B2C tenant and the B2B tenant. When using the combined sign-in and sign-up flow, the user will be registered automatically in the B2C-tenant.\n", "tags": ["Azure AD", "Azure AD B2C"], "categories": ["Blog", "Cloud", "Security"]}
{"post_id": 28443, "title": "How to use Java CDK to define a DynamoDB-backed REST API with only AWS API Gateway \u2013 part 2", "url": "https://www.luminis.eu/blog-en/how-to-use-java-cdk-to-define-a-dynamodb-backed-rest-api-with-only-aws-api-gateway-part-2/", "updated_at": "2021-08-10T16:05:35", "body": "In the previous blog we discussed how you can define a no-code or low-code REST-API on top of a DynamoDB table. We used a high level construct from the AWS Solutions Constructs library, that defines all in one go. The limitation we ran into was that we couldn\u2019t customise the output of the GET method in order to return only the (stored) JSON document. In this blog, we\u2019ll replace the solution based on AWS Solutions Constructs library by a solution build with the standard CDK elements, that gives all the freedom we need.\nAs a starting point, we take the source code developed in part 1. Our first step is to refactor the code by replacing the \u201cAWS Solutions Constructs\u201d by constructs from the AWS Construct Library, and create exact the same deployment as in part 1. From there, we\u2019ll improve the response of the GET method by adding a response template. Note that the complete sample code can be found on github.\nDefining the major elements\nFor this refactoring, we only need to replace the content of the DynamoRestStack class. Just remove all code from the constructor, except for the call to super. We start with defining the major elements: the REST API and the DynamoDB table. Defining the REST API is pretty simple:\nRestApi dynamoRestApi = new RestApi(this, \"DynamoRest\", RestApiProps.builder()\r\n    .deployOptions(StageOptions.builder()\r\n    .loggingLevel(MethodLoggingLevel.INFO)\r\n    .build())\r\n    .build());\nActually, it\u2019s not much more then setting a meaningful name (\u201cDynamoRest\u201d) and enabling logging (in the previous version, the AWS Solution Construct enabled logging by default).\nDefining the table is almost identical as before:\nTableProps tableProps = TableProps.builder()\r\n    .partitionKey(Attribute.builder()\r\n         .name(\"id\")\r\n         .type(AttributeType.STRING)\r\n         .build())\r\n    .tableName(\"exerciseStats\")\r\n    .build();\r\nTable dynamoDbTable = new Table(this, \"exerciseStats\", tableProps);\nwe only have to instantiate the Table explicitly (last line in the sample).\nAllow access to db\nNext, we need an IAM Role, in order to allow the API Gateway to access the DynamoDB table. This is typically something a higher level construct can arrange for us, which is why we didn\u2019t have to bother while using the AWS Solution Constructs library in part 1. This is how the Role is defined in Java code:\nrole = new Role(this, \"dynamorest\", RoleProps.builder()\r\n    .assumedBy(new ServicePrincipal(\"apigateway.amazonaws.com\"))\r\n    .build());\r\n\r\nrole.addToPolicy(new PolicyStatement(PolicyStatementProps.builder()\r\n    .actions(List.of(\"dynamodb:Query\", \r\n                     \"dynamodb:PutItem\", \r\n                     \"dynamodb:UpdateItem\"))\r\n    .effect(Effect.ALLOW)\r\n    .resources(List.of(dynamoDbTable.getTableArn()))\r\n    .build()));\nNote how the second part adds ALLOW permissions on the DynamoDB table (identified by its ARN) for the Query, PutItem and UpdateItem actions.\nDefining the REST resource\nThis brings us to the interesting parts: setting up the resources and methods that make up the REST API. As before, we want the resource to be created by sending a POST request to root (\u201c/\u201d). Translated to CDK code: get the root resource and add a POST method:\nIResource rootResource = dynamoRestApi.getRoot();\r\nrootResource.addMethod(\"POST\", createIntegration, \r\n                       MethodOptions.builder()\r\n    .methodResponses(List.of(MethodResponse.builder()\r\n        .statusCode(\"200\")\r\n        .build()))\r\n    .build());\nTo make this compile, we first need to define how this method integrates with the back-end service (i.e. we need to define createIntegration). This is a bit more complicated, lets review each line of the following fragment:\nIntegration createIntegration = AwsIntegration.Builder.create()\r\n    .action(\"PutItem\")\r\n    .service(\"dynamodb\")\r\n    .integrationHttpMethod(\"POST\")\r\n    .options(IntegrationOptions.builder()\r\n        .credentialsRole(role)\r\n        .requestTemplates(Map.of(\"application/json\", createRequestTemplate))\r\n        .integrationResponses(List.of(IntegrationResponse.builder()\r\n            .statusCode(\"200\")\r\n            .build()))\r\n        .build())\r\n    .build();\n\naction (PutItem): the command that is sent to the back-end service\nservice: definition of the back-end service. Use lowercase; even though the AWS console shows \u201cDynamoDB\u201d, that doesn\u2019t work in CDK!\nintegrationHttpMethod: the HTTP method that is used to communicate with the back-end service; always POST for DynamoDB (do not confuse with the HTTP method used for in the REST call itself)\ncredentialsRole: the IAM role that will give the integration the necessary permissions\nrequestTemplate: the template that defines the command being send to DynamoDB\nintegrationResponses: the response(s) the REST call might return.\n\nThe (createRequestTemplate) is exactly the same template we used in part 1.\nWe\u2019re nearly there. For the PUT method, the recipe is almost the same (with action = \u201cUpdateItem\u201d), except for the resource. To update the resource, we don\u2019t want to PUT to root, but to the resource to be updated of course (e.g. /2021-05-31). So, instead of retrieving and using the root, we create the document resource first:\nResource doc = dynamoRestApi.getRoot().addResource(\"{id}\");\nThe rest (pun intended) is pretty similar. The same holds for the GET method: in this case the action is \u201cQuery\u201d. Refer to this commit for the complete solution at this stage.\nBack where we started\nThe code is now functionally equivalent to the version created in part 1: if you run cdk deploy you can check the result in AWS Console or test it by \u201ccurling\u201d a POST request.\nThe final part\nFinally, we get at the part that made us start. Our aim is to write a template that will transform the response from DynamoDB into a proper REST API response. Let\u2019s recall what DynamoDB serves us when issuing the Query command:\n{\"Count\":1,\r\n\"Items\":[ {\r\n    \"content\": {\r\n        \"S\": \"{ date: \\\"2021-05-25\\\", exercise: 42}\"\r\n    },\r\n    \"id\": {\r\n        \"S\": \"2021-05-25\"\r\n    }\r\n} ],\r\n\"ScannedCount\":1}\nWhat we want to return is the content, but with the \u201cS\u201d (type indicator) removed. This transformation is achieved by this piece of code:\n#set($inputRoot = $input.path('$'))\r\n#if(!$inputRoot.Items.isEmpty())$inputRoot.Items[0].content.S\r\n#end\nThe syntax is from the velocity templating language, which has been used in multiple Java web frameworks in the past. See API Gateway Developer guide for more info. It\u2019s a bit cryptic, but you probably can guess what it does: if the \u201cItems\u201d element of the response is non-empty, it navigates to the first item, takes the content part and and then the \u201cS\u201d part. If the \u201cItems\u201d element is empty, it returns nothing. The template rendering is quite literal: you can add spaces, but they\u2019ll end up in the result at the same spot; that\u2019s why we skipped all spaces, even though spaces would improve readability ;-).\nDone\nJust expand the Integration element with this template:\n.integrationResponses(List.of(IntegrationResponse.builder()\r\n    .statusCode(\"200\")\r\n    .responseTemplates(Map.of(\"application/json\", \r\n        \"#set($inputRoot = $input.path('$'))\\n\" +\r\n        \"#if(!$inputRoot.Items.isEmpty())\" +\r\n        \"$inputRoot.Items[0].content.S\\n\" +\r\n        \"#end\\n\"))\r\n    .build()))\r\n\nand cdk deploy the solution. To see the result, copy-paste the production URL from the CDK output (or look it up in the AWS Console) and query an item with\ncurl https://xxx-api.eu-west-1.amazonaws.com/prod/2021-05-25\nWrap up\nNow we have transformed the code to use only standard CDK constructs (from the AWS Construct Library, it becomes even more clear that higher level constructs like the one we used in part 1, make life easier. However, as with all abstractions, it also hides details that would actually give us more insights how things work under the hood. I think the takeaway is that knowing both solutions improves your understanding and ensures you can always pick the right tool for the job. In this case, we had to use the lower level API to complete our solution in a proper way and avoid that a GET request would return implementations details about the underlying persistence mechanism.\nDo not forget the cdk destroy the sample to avoid unwanted costs.\n", "tags": [], "categories": ["Blog", "Cloud", "Development"]}
{"post_id": 28369, "title": "Kotlin Multiplatform \u2013 A remedy for all mobile app development?", "url": "https://www.luminis.eu/blog-en/development-en/kotlin-multiplatform-a-remedy-for-all-mobile-app-development/", "updated_at": "2021-08-10T16:05:27", "body": "Ever since mobile apps were built there was a desire to have a single codebase that can run upon multiple platforms. Way more solutions and technologies saw the light of day than I can think and can keep track of and with Kotlin Multiplatform (Mobile) there is a (fairly) new player in town. Is this new player in town worth to invest your valuable time in? In this article, we\u2019ll explore Kotlin Multiplatform, what it is, how it is different from what already is available, how it works, and finally arrive at a conclusion as to why you should at least try it in one of your next mobile projects.\n\u00a0\nSo, what is Kotlin Multiplatform Mobile?\n\nSome say Kotlin Multiplatform Mobile is an SDK, a mechanism, an extension or even a way of living. In my opinion it doesn\u2019t really matter what it is, although Jetbrains, the company behind Kotlin, says it\u2019s an SDK, as long as it does what it says it does. Kotlin Multiplatform Mobile is a subset of Kotlin Multiplatform and allows you to write business logic code for both iOS and Android from within a single codebase. While the superset, Kotlin Multiplatform, enables you to include other platforms like Linux, Windows and the web (JavaScript). Kotlin Multiplatform Mobile projects are still considered alpha, which means features and tooling may change in future Kotlin releases, but nonetheless it is getting more mature fast, with almost all its components already considered beta.\n\u00a0\nOkay, but how is it different than what is already known?\nIt really is not, at least if you compare the goals of most \u2013 if not all \u2013 other solutions with each other. Most \u2013 if not all \u2013 solutions aim to make developing apps easier and more maintainable. They differ by how they are achieving that goal and, in my opinion, most compelling difference between Kotlin Multiplatform Mobile and most other solutions is that Kotlin Multiplatform Mobile allows you to run compiled code straight onto native platforms, without the code being run using a virtual machine or in a webview-like solution. The second big difference is that Kotlin Multiplatform Mobile guarantees a full native user experience, because (for now) it doesn\u2019t support sharing UI logic across platforms, which means iOS developers have no other option but to use Apple\u2019s frameworks. With Xamarin a similar approach can be used. Solutions like React Native (and other webview-like solutions) need to bridge code between the native OS platforms and JavaScript. This causes performance issues and on top of that the animations are not as satisfactory as within native apps. Flutter solves many of these issues common to React Native, and thus it also doesn\u2019t guarantee a full native user experience. The table below displays a few of the main differences between a couple of widely adopted other solutions.\n\n\u00a0\nNow, how does it work?\n\nKotlin Multiplatform (Mobile) enables you to write common (Kotlin) code for your business logic, which is then usable on every platform supported, because all code gets compiled to bytecode for Android and native for iOS. When you need to write code that needs to access platform-specific APIs you can use the Kotlin mechanism of expected and actual declarations. With this mechanism, a common source set defines an expected declaration, and platform source sets must provide the actual declaration that corresponds to the expected declaration as seen below in the small code example. One of the key benefits is that you can use code from platform-dependent libraries like Foundation and UIKit for iOS.\n// Common\r\nexpect fun randomUUID(): String\r\n\n// Android\r\nimport java.util.*\r\nactual fun randomUUID() = UUID.randomUUID().toString()\r\n\n// iOS\r\nimport platform.Foundation.NSUUID\r\nactual fun randomUUID(): String = NSUUID().UUIDString()\r\n\nWith this knowledge it is possible to leverage the advantage of this mechanism and move as much code to the shared library as possible and thus make the actual native app as \u2018dumb\u2019 as possible. Ideally the native app is only used to link the UI with the actual shared library. Using Kotlin Multiplatform I think that it should be possible to share 70-80% of code across mobile platforms and even up to maybe 90% on \u201csimple\u201d apps. Adding a new platform will solely be a matter of implementing UI code.\n\u00a0\nWhy should I choose Kotlin Multiplatform Mobile for my next project?\n\nIt has no overhead as it compiles to bytecode for Android and native for iOS.\nMany Android developers are already familiar with Kotlin and the Gradle build system, it\u2019s a little to no change for them.\nNot sharing the UI can be a curse or a blessing. While reducing code by writing UI once, this can come at the expense of reduced user experience as every platform has its own way / method of interaction, guidelines and so on, this can either be a curse or a blessing: Everyone bakes his cake as he likes to eat it.\nDue to the interop with the other platform specific languages Kotlin Multiplatform allows you to write Java, Swift or Objective-C code and use Kotlin code within those languages and vice-versa.\nIt is also possible to use platform-specific libraries and even choose a totally different library for let\u2019s say logging where each platform defines a complete other interface to log (you need to define what functions should be called using the expect/actual-mechanism, but this opens up for tons of possibilities).\n\n\u00a0\nAre there any downsides?\nOf course. If there are advantages there are probably also disadvantages, which is also the case for Kotlin Multiplatform (Mobile).\n\nKotlin Multiplatform (Mobile) projects are considered alpha, so in future Kotlin releases features and or tooling may change.\nIt has a relatively smaller community as compared to React Native or Flutter, which means it is possible there will not be an answer for all your questions or problems. I personally haven\u2019t experienced it yet, but it is possible.\nFor iOS-only developers, with no Kotlin or Gradle experience, it means a new build system and/or language should be learned.\nObjective-C interop (still) has its limitations. Inline classes and custom classes implementing standard Kotlin collection interfaces aren\u2019t fully supported.\n\n\u00a0\nHow do I get started?\nI won\u2019t get into too much detail about how you can get started. This isn\u2019t supposed to be a how-to guide, but merely an article informing you about what Kotlin Multiplatform Mobile is. Nevertheless, I\u2019ve found some great resources that I think you should most definitely check if you want to try it out.\n\nFirst, I would suggest visiting its website.\nAfter that I would take a look at the docs.\nRay Wanderlich always has a lot of good hands-on tutorials.\nLast but not least; I would suggest joining the Kotlin slack or get otherwise involved with the community.\n\n\u00a0\nFinal Thoughts\nNow that we\u2019ve explored Kotlin Multiplatform Mobile, we\u2019ve learned that it is an SDK available to share as much code as possible between platforms, that it guarantees a full native user experience, while compiled code is being run straight onto the native platforms. We\u2019ve also explored how Kotlin Multiplatform Mobile works and that we can access platform-dependent APIs with the help of the expected and actual declarations. All in all, when weighing the advantages with the disadvantages, I think it is safe to say Kotlin Multiplatform Mobile is worth your valuable time (especially if you\u2019re an app developer like me, even if you\u2019re a iOS-only developer) and thus you should most definitely try it out on one of your next projects to become just as enthusiastic as I am.\n", "tags": [], "categories": ["Blog", "Development"]}
{"post_id": 28160, "title": "How to use Java CDK to define a DynamoDB-backed REST API with only AWS API Gateway", "url": "https://www.luminis.eu/blog-en/cloud-en/rest-api-on-dynamodb-with-aws-api-gateway-in-java-cdk/", "updated_at": "2021-08-10T16:05:11", "body": "A step by step tutorial to make AWS API Gateway implement a REST API on top of a DynamoDB table, deployed using a CDK script written in Java.\nIn \u201ctraditional\u201d Java development, a REST API for objects stored in a database would mostly be served by some Java code handling the HTTP requests and transforming the database model to the appropriate models used in the API. You probably wouldn\u2019t even consider to implement it in another way, for example in the HTTP proxy that connects your application to the internet. When architecting a Java application, it doesn\u2019t feel right to put business logic in an infrastructure component. And it would not be very convenient in the development stage either, because the HTTP proxy is often not even present in the development environment.\nEnter cloud\nWith cloud development, this is all a bit different. With infrastructure-as-code, the infrastructure has become part of the development and probably, its definition is even in the same source code repository as the application itself. Moreover, one of the major advantages of the cloud is that it provides a plentitude of services, or building blocks, that should make our (developer) live easier. Well, if it makes it easier is something we\u2019ll cover later on, but at least it should support the notion of writing less code and concentrate on business code in stead of code that moves data (\u201cwrite code that transforms, not transports\u201d).\nCDK\nIn this blog, we\u2019ll take a look at using AWS API Gateway to provide a REST interface on top of a DynamoDB table, as a low-code alternative to writing a Lambda function to implement a REST interface. Of course we\u2019ll be using CDK to define and deploy the cloud components (infrastructure-as-code) and we\u2019ll the Java variant of the CDK. Of course you can use CDK with different languages, and you could as well use TypeScript, but if support for different languages has the same maturity level, you should pick the language you are most familiair with, so that no doubts about how to express yourself in the language get in the way of what our focus is: get it deployed in the cloud.\n\nThe sample we\u2019ll use is storing a small and simple JSON document that records the distance you walked / biked / run everyday (i know there are great Apps for this, it is just a simple example ;-)). The tiny JSON document will have a format like this:\n{ date: \"2021-05-23\", distance: 38 }\nPlease note that the complete sample code developed in this blog, as well as the different stages of development presented in this blog, can be found on github.\nGetting started\nAssuming you\u2019ve already installed and bootstrapped the CDK (if not, check out this), we can generate a project template with cdk init app --language=java. This will even initialise a git repository and create a first commit with the generated sources; the name of the project and generated classes is based on the directory name. The first thing to do is to change the package name of the generated classes from com.myorg to something sensible.\nThe next thing to do is the edit the DynamoRestApp class and remove the call to the DynamoRestStack.Builder and replace it by a simple instantiation of the DynamoRestStack. Even though usage of a builder is very common in CDK, we remove it here because the referenced DynamoRestStack.Builder class is not generated at all. It will compile (thanks to inheriting a no-op implementation), but it won\u2019t do anything useful, which is of course very confusing and not something you would expect from generated code that is there to give you a quick start. So replace the contains of the generated main method by\npublic static void main(final String[] args) {\r\n    App app = new App();\r\n    new DynamoRestStack(app, \"DynamoRestStack\");\r\n    app.synth();\r\n}\nWe\u2019ll also remove (or truncate) the generated test case; we won\u2019t need it for this blog.\nNow it\u2019s time to start coding. The AWS Solutions Constructs library provides a solution construct that seems ideal for our use case: the aws-apigateway-dynamodb module.\nTo use it, we need to declare it as a maven dependency (in the pom.xml):\n<dependency>\r\n    <groupId>software.amazon.awsconstructs</groupId>\r\n    <artifactId>apigatewaydynamodb</artifactId>\r\n    <version>${cdk.version}</version>\r\n</dependency>\nTo construct our solution, we could do with this minimal piece of code\nApiGatewayToDynamoDBProps apiGatewayToDynamoDBProps = \r\n    ApiGatewayToDynamoDBProps.builder()\r\n\u00a0 \u00a0     .allowCreateOperation(true)\r\n\u00a0 \u00a0     .allowUpdateOperation(true)\r\n\u00a0 \u00a0     .allowReadOperation(true)\r\n\u00a0 \u00a0     .build();\r\nApiGatewayToDynamoDB apiGateway = \r\n    new ApiGatewayToDynamoDB(this, \"dynamogateway\", \r\n                             apiGatewayToDynamoDBProps);\nHowever, that would create an API with the rather meaningless name \u201cRestApi\u201d, so to avoid our AWS account gets messy with non-descriptive names, we\u2019ll fix that right away by explicitly defining the name:\nRestApiProps apiGatewayProps = RestApiProps.builder()\r\n    .restApiName(\"DynamoRest\")\r\n    .build();\nAlso, the default will use IAM authentication, which is very inconvenient for testing with for example a curl client, so we\u2019ll replace that by authentication type none. If you code along with this blog, make sure you remove the deployment when done. (Alternatively, you could secure the API with an API_KEY, which is much easier to use.)\nRestApiProps apiGatewayProps = RestApiProps.builder()\r\n    .restApiName(\"DynamoRest\")\r\n    .defaultMethodOptions(MethodOptions.builder()\r\n    .authorizationType(AuthorizationType.NONE)\r\n    .build())\r\n.build();\nWhen you deploy the solution with the cdk deploy command, it will ask for confirmation when IAM roles are affected. This can be very annoying, especially when deployment takes several minutes, and you spend the waiting time doing something else only to discover upon return that all the time, the deployment command has been waiting for your approval. To get rid of this confirmation, add the following line to the cdk.json file:\n\"requireApproval\": \"never\"\nNearly done!?\nIf you deploy what we have got so far and take a look at the API in the AWS console, you\u2019ll be disappointed to see the API defining only one HTTP method. The solution only has a GET method that can be used to read from DynamoDB, there are no create and update operations, even though we defined them in our code (e.g. with allowCreateOperation(true)).\n\nThis is because the create and update methods need a template that tells API Gateway how to convert the request body into a proper DynamoDB command. Let\u2019s start with the creation. The template we need is this:\n{\r\n    \"TableName\": \"exerciseStats\",\r\n    \"Item\": {\r\n        \"id\": {\r\n            \"S\": \"$input.params('id')\"\r\n        },\r\n        \"content\":\r\n            \"S\": \"$util.escapeJavaScript($input.body)\"\r\n        }\r\n    }\r\n}\nIf you read this template, it\u2019s probably obvious that the DynamoDB item will consist of a partition key (by default named \u201cid\u201d) and an attribute named content, that we will use to store our JSON data. The command syntax of DynamoDB requires the values to be indicated by type, the \u201cS\u201d stands for String (see Data Type Descriptors in DynamoDB developer guide)\nBecause the content is JSON, the quotes need to be escaped, which is why we need the $util.escapeJavaScript() function.\nThe update template is similar, but uses a different syntax as you must explicitly define what to update:\n{\r\n    \"TableName\": \"exerciseStats\",\r\n    \"Key\": {\r\n        \"id\": {\r\n            \"S\": \"$input.params('id')\"\r\n        }\r\n    },\r\n    \"UpdateExpression\": \"set content = :v1\",\r\n    \"ExpressionAttributeValues\": {\r\n        \":v1\": {\r\n            \"S\": \"$util.escapeJavaScript($input.body)\"\r\n        }\r\n    },\r\n    \"ReturnValues\": \"NONE\"\r\n}\nAfter deploying this version, you can POST, PUT or GET items with curl. To obtain the URL, open the AWS Console in your browser and go to API Gateway, select the \u201cDynamoRest\u201d API, select stages and click the \u201cprod\u201d stage; the invoke URL is displayed in the stage editor at the right.  To create or update an item, use curl commands like these:\ncurl -X POST -H 'Content-Type: application/json' -d '{ date: \"2021-05-25\", exercise: 42 }' https://xxx-api.eu-west-1.amazonaws.com/prod?id=2021-05-25\r\ncurl -X PUT -H 'Content-Type: application/json' -d '{ date: \"2021-05-23\", time: 38 }' https://xxx-api.eu-west-1.amazonaws.com/prod/2021-05-23\r\n\nNote that the POST has the id as query parameters, while the PUT has the id embedded in the URL.\nGET\nOf course, the GET is just curl https://xxx-api.eu-west-1.amazonaws.com/prod/2021-05-25. This returns\n{\"Count\":1,\"Items\":[{\"content\":{\"S\":\"{ date: \\\"2021-05-25\\\", exercise: 42}\"},\"id\":{\"S\":\"2021-05-25\"}}],\"ScannedCount\":1}\nwhich is not exactly what we want. What we get here is the raw DynamoDB result, not the response we would expect from a proper REST API, which is the JSON content returned in the same format as we used for inserting ({ date: \"2021-05-23\", time: 38 }).\nIf we would deploy this solution, it would leak implementation details: anyone could tell from the result that under the hood, DynamoDB is being used. From engineering point of view, this is a bad thing \u2013 and not because the fact that DynamoDB is being used should be kept secret. It is a signal that implementation and interface are not clearly separated. This is bad because once clients of there API exist, they will always expect this syntax and thereby depend on (the implementation detail that) DynamoDB is being used. If you would ever want to change implementation by using a different persistence mechanism, your API will change and clients will break. If the interface is cleanly separated from the implementation, this will never happen.\nTo make the GET request return the JSON document only, we would need to use custom response template: a script that defines how the output from DynamoDB is mapped to the response of the GET request. AWS API Gateway supports this out of the box, but unfortunately, the AWS solution construct we used here does not support this (yet). In order to finish this REST API properly, we need to return to using the constructs from the basic CDK library. This requires more coding, but it will enable us to customise every single piece of it, including the response template. How this is coded exactly will be covered in part 2.\n", "tags": [], "categories": ["Blog", "Cloud"]}
{"post_id": 27901, "title": "PIT Mutation Testing", "url": "https://www.luminis.eu/blog-en/resilience-en/pit-mutation-testing/", "updated_at": "2023-01-31T11:54:44", "body": "Real world mutation testing\nIt\u2019s been hard keeping up with everything else going around the world, even after eliminating the long travelling hours. But still, I do have something interesting to share from work. So far, the projects I have been assigned to, had an average testing coverage, ranging from 70-90ish%. But here at our client, we have\n\nJunit tests\nRegression tests\nPitests\n\nEverything should be at 100% before it is merged into develop branch.\nFor us at Luminis, Java, Junit test sit in the core, so I won\u2019t be using my word limit for that.\nRegression tests is a synonym for Cucumber tests at our client. Most of us are familiar with Cucumber tests too. It allows automation of functional validation in easily readable and understandable format (like plain English) to Business Analysts, Developers, Testers, etc.\nCucumber (created in 2008) executes tests specified written in language called Gherkin. It is a plaint-text natural language (for example, English or one of other 60+ languages supported by Cucumber) with a given structure.\nIn a Gherkin file, non-blank lines can start with a keyword, followed by text in natural language. The main keywords are the following:\n\nFeature: High-level description of the software feature to be tested. It can be seen as a use case description.\nScenario: Concrete example that illustrates a business rule. Scenarios follow the same pattern:\n\nExample of a feature file:\nScenario Outline: Successful login to Searchlight as a legit Searchlight user\u00a0\n\u00a0\u00a0\u00a0 Given I am on the \"Login\" page\u00a0\n\u00a0\u00a0\u00a0 When I fill in \"username\" field with \"foo@mydomain.com\"\u00a0\n\u00a0\u00a0\u00a0 And I fill in \"password\" field with \"CucumberIsMagic\"\u00a0\n\u00a0\u00a0\u00a0 And I click \"submit\" button\u00a0\n\u00a0\u00a0\u00a0 Then I should be logged in successfully as \"foo@mydomain.com\"\nPitests \u2013 Now this is the interesting part. At least for me. I never came across mutation testing, i.e., a software\u00a0testing\u00a0in which certain statements of the source code are changed/mutated to check if the\u00a0test\u00a0cases can find errors in source code. The goal of\u00a0Mutation Testing\u00a0is ensuring the quality of\u00a0test\u00a0cases in terms of robustness that it should fail the\u00a0mutated\u00a0source code.\nTraditional test coverage (i.e line, statement, branch, etc.) measures only which code is\u00a0executed by your tests. It does\u00a0not\u00a0check that your tests are actually able to\u00a0detect faults\u00a0in the executed code. It is therefore only able to identify code that is definitely\u00a0not tested. The most extreme examples of the problem are tests with no assertions. Fortunately these are uncommon in most code bases. Much more common is code that is only\u00a0partially tested\u00a0by its suite.\nPitest at work\nOur code:\npublic boolean isPositive(int number) {\n\u00a0 \u00a0 boolean result = false;\n\u00a0 \u00a0 if (number >= 0) {\n\u00a0 \u00a0 \u00a0 \u00a0 result = true;\n\u00a0 \u00a0 }\n\u00a0 \u00a0 return result;\n}\nPitest makes the following mutations of our code to test if they really do meaningful work\n#1 Mutation \u2013 Changed conditional boundary (mutator)\n\u00a0\u00a0\u00a0 public boolean isPositive(int number) {\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 boolean result = false;\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0// mutator - changed conditional boundary\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 if (number > 0) {\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 result = true;\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 }\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 return result;\u00a0\u00a0\u00a0\u00a0\n\u00a0\u00a0\u00a0 }\n#2 Mutation \u2013 Negated conditional (mutator)\npublic boolean isPositive(int number) {\n\u00a0 \u00a0 boolean result = false;\n\u00a0 \u00a0 // mutator - negated conditional\n\u00a0 \u00a0 if (false) {\n\u00a0 \u00a0 \u00a0 \u00a0 result = true;\n\u00a0 \u00a0 }\n\u00a0 \u00a0 return result;\n}\nA Good unit test should fail (kill) all the mutations #1,#2,#3.\n@Test\npublic void testPositive() {\n\u00a0 \u00a0 CalculatorService obj = new CalculatorService();\n\u00a0 \u00a0 assertEquals(true, obj.isPositive(10));\n}\nThe above unit test will kill the mutation #2 (unit test is failed), but the mutation #1 is survived (unit test is passed).\nReview the mutation #1 again. To fail (kill) this test (mutation), we should test the conditional boundary, the number zero.\n\u00a0\u00a0\u00a0 public boolean isPositive(int number) {\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 boolean result = false;\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0// mutator - changed conditional boundary\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 if (number > 0) {\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 result = true;\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 }\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 return result;\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\n\u00a0\u00a0\u00a0 }\nImproving the unit test by testing the number zero.\n\u00a0\u00a0\u00a0 @Test\n\u00a0\u00a0\u00a0 public void testPositive() {\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 CalculatorService obj = new CalculatorService();\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 assertEquals(true, obj.isPositive(10));\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0//kill mutation #1\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 assertEquals(true, obj.isPositive(0));\n\u00a0\u00a0\u00a0 }\nDone, 100% mutation coverage now.\nThere are other mutation testing systems for Java, but they are not widely used. They are mostly slow, difficult to use and written to meet the needs of academic research rather than real development teams.\nPIT is different. It\u2019s fast\u00a0\u2013 can analyse in\u00a0minutes\u00a0what would take earlier systems\u00a0days; and is actively developed & supported.\nThe reports produced by PIT are in an easy to read format combining\u00a0line coverage\u00a0and\u00a0mutation coverage\u00a0information.\n\n\nAnd it is quite easy to integrate with Ant, Maven, Gradle and others. There are eclipse and IntelliJ idea plugins available for use too. So, there is something new I learned at work.\n", "tags": [], "categories": ["Blog", "Development", "Resilience"]}
{"post_id": 27882, "title": "Gazar about Luminis; \u201cNo empty promises\u201d", "url": "https://www.luminis.eu/blog-en/gazar-over-luminis-geen-loze-beloftes/", "updated_at": "2021-08-10T16:10:01", "body": "Gazar Ajroumjan worked for another company for a while after college, dabled in midlancing, but eventually decided he could learn more in a permanent position. That is how he ended up at Luminis.\nTell us Gazar, what was your job interview at Luminis like?\nI almost canceled my job interview with Luminis. Two more companies were interested in me and I thought I would just choose one of them. In the end I went to see Luminis, and until this day I do not regret this decision. The conversation was fun, something instantly clicked. What particularly attracted me to Luminis is that there is a lot of room for personal development. At first I thought this was just a sales pitch, after all, every company makes new IT-ers those kinds of promises. But at Luminis things where different: there is no fixed training buget, if you can explain why you need a certain course, you will be able to partake.\n\nWhat kind of projects are you currently working on?\nThe first project I worked on was really fun: developing a lead motion controller. I had no idea that Luminis had such interesting, innovative projects, in that respect the website does not do the organisation justice. Although it was my first project, I was given a lot of freedom. I was allowed to choose which techniques I wanted to use and how I would implement everything with the help of a designer.\nAt the moment I work at VGZ, where we are designing a chatbot, which is also a very interesting project. In the future I want to focus on the business side. That is why I am doing an MA in Management at Nyenrode. Software is very interesting, but I do not want to limit myself by specialising in just one area of expertise. In addition to my work, I have been running my own company for about eight years, which is currently on the back burner, but I have always had an interest in entrepreneurship.\nLuminis and its future, how do you think we will evolve?\nI see a bright future for Luminis: the technical sector is growing rapidly and I expect that it will continue to grow in the coming years. Luminis is doing very well, but to keep up you also have to keep asking yourself critical questions and taking on new challenges.\nBesides all that, Luminis is a pleasant workplace. I get along well with my colleagues and we have a great relationship as a team!\n\n\n\n", "tags": [], "categories": ["Blog", "News"]}
{"post_id": 27852, "title": "Introducing Luminis", "url": "https://www.luminis.eu/blog-en/introducing-luminis/", "updated_at": "2021-08-10T16:08:59", "body": "I regularly conduct job interviews. Sometimes it only lasts an hour and sometimes it is the beginning of a friendship that continues for many years. At Luminis we leave every meeting in a positive way. I think it is important that you always look back at Luminis with a good feeling, whether it regards a short introduction or years of collaboration.\nOrdinary or not quite?\nThe aim of a first meeting is to get to know each other, have an open conversation, not just firing away way too many questions. When a candidate talks about his life, I pay special attention to the choices someone has made. Of course the \u201cwhat\u201d is important: what did you study? But more importantly is the \u201cwhy\u201d. How did you make the choices you made and what are the things that drive you daily?\nObviously such an introduction goes both ways. I also tell my interlocutor about working at Luminis and explain them why I love working here. I do not think much about the things I talk about during a first meeting, but every time I notice candidates think it\u2019s quite extraordinary. The time we take for colleagues, how we work together, our collective motivation, our passion for craftsmanship, the importance of humour, knowledge sharing and of course our focus to learn new things and develop every day. These are all topics we talk about a lot at Luminis, and that we have come to find normal conversation topics. These introductory meetings have made me realise that talking about such topics is actually not so common.\nWelcome!\nThat is why I started writing down said topics at the end of last year. Intended to introduce interested parties to our culture, and also to introduce colleagues who have just started at Luminis to our culture. When you start working at Luminis there are probably things that will surprise you. This special booklet will help you understand why your colleagues do what they do and say what they say. It helps you feel welcome at our company and dares you to be yourself.\n\nCommitting our culture to writing made me see a pitfall. Implicit manners become explicit by writing them down. In addition, it can be taken as a standard, something that new employees need to meet. I wanted to stay away from that. A corporate culture develops over time, it is not something that should be imposed. The goal instead is to record our experiences so that they can be shared. And of course I hope it will inspire everyone, new colleagues as well as ones who have worked at Luminis for years. There are always opportunities to further develop!\nWorking on the booklet together\nWhat started during my Christmas break with an empty Word document has now become a large stack of booklets, which were delivered to the office last week. In the months in between, many colleagues co-wrote and helped make my texts readable. We hired a cartoonist and a colleague, Laura designed the booklet. Many thanks to all! As far as I am concerned this joint effort was a good example of what our company culture can lead to.\nWould you like to receive the \u201cwelcome to Luminis booklet\u201d?\nWould you like to receive the \u201cwelcome to Luminis booklet? You can! Enter your details here and we will send it to you. Of course we would like to hear your reaction after reading it, because at Luminis we believe we have new things to learn every day.\nGreeting,\nRen\u00e9.\n", "tags": [], "categories": ["Blog", "News"]}
{"post_id": 27527, "title": "What does a Business Consultant do?", "url": "https://www.luminis.eu/blog-en/news-en/what-does-a-business-consultant-do/", "updated_at": "2021-08-10T16:09:05", "body": "Meet Robin! Robin van Kaathoven is business consultant at Luminis. Nice job title, but what do you actually do? What does being a business consultant at Luminis entail? We would like to give you an idea through an interview with Robin. However, if you end up a Business Consultant at Luminis, your workload could look different. At Luminis, we focus on people, not a pre-determined profile. We think it is important that you fit in with us. Only then will your qualities be able to shine and will you enjoy your work to the fullest.\nBelow you can read the interview with Robin. Do you recognise yourself in the things a Business Consultant does at Luminis? Do not hesitate to contact us!\nRobin, why did you choose Luminis?\nI chose Luminis because I am enthusiastic about innovations in IT, Cloud, and Data. The speed at which new services and digital products are introduced is increasing, as is the variety of revenue models associated with them. Modern software technology opens doors to create value faster, but how do you go about this? What product design do your customers like the most? Which business model fits and what is feasible in terms of software technology? Precisely these questions are at the core of the way Luminis helps customers take the next step.\n\u00a0\n\n\u00a0\nHow do you start with a new customer?\nWhile I also have my own network as a business consultant, I am usually introduced to customers by my commercial colleagues. My task is to gain the trust of clients and position Luminis at board-room level. A business design workshop, for example, is a great tool to establish the relationship between a company\u2019s strategy and the digital products that can contribute to it. After a few follow-up interviews, I usually start in an advisory role, rolling up my sleeves and getting to work. For example, in my last assignment I started as Product Owner. This was a great way to get to know the people and the domain better, and of course to set an example of how such a role can be fulfilled.\nSo as Business Owner you are a Product Owner as well?\nI only fill such roles temporarily. I prefer to work during the phase in which business concepts and project definitions have not yet been fully formed, that\u2019s where my strength lies. My focus is therefore not delivering projects, but transforming companies into agile organisations that are able to translate the possibilities of software technology into business benefits. I realise that I cannot do this alone and I consider myself fortunate to have my Luminis colleagues who can join me in various areas of expertise and who can help fill parts of the path that I envision for the customer. Such as, for example, a business analyst who is much stronger in concretising a concept to system requirements and a project definition.\n\nWhat do you do when the project is on its way?\nWhen we have reached that state, I am often already broadening my network within the customer organisation to see how Luminis can contribute more besides the original assignment. It is my job to strengthen the relationship between customers and Luminis in this way, and not fall into the trap of continuing to work full-time on one project. However, I will continue to monitor the relationship and make adjustments because I can, like no other, establish the relationship between where the customer wants to go and the role Luminis can play in this.\nDo you also have a commercial role as Business Consultant?\nWith regards to the long-term strategy for a customer, I work with my commercial colleague. For large accounts we make a long-term plan where the strategy of the company and their business model play an important role. Of course we share this strategy with said customer and adjust it based on their feedback. These are the moments when a customer notices that we are a true partner, not just a supplier.\nWhat really makes you happy?\nTransforming an organisation is not easy. I often work with people who are less willing or able to contribute to the necessary changes. That is part of my job. For me, digital leadership is also an ambassador\u2019s role, and I know that in addition to drive, patience and soft skills are also very important. I always keep the bigger goal and long term vision in mind. What I really enjoy is when people gradually become more enthusiastic about new technology and start using and applying it themselves.\nDo you also have an internal role at Luminis?\nEvery week is different for me, as I work for various clients at the same time. I am also working internally besides my work for our clients. For example: a brainstorm with colleagues, help with a quotation, to prepare a presentation or have a meeting with all colleagues who work for one of my clients. We then discuss developments and see whether we are on track. I am not only concerned with the development of our customers, but also with that of Luminis.\n", "tags": [], "categories": ["Blog", "News"]}
{"post_id": 27764, "title": "New business models for the installation industry", "url": "https://www.luminis.eu/blog-en/new-business-models-for-the-installation-industry/", "updated_at": "2021-08-10T16:09:16", "body": "In recent years, I have regularly attended technology fairs. I still remember a booth, where visitors with VR glasses gazed at large gears: products that, as far as I could judge, were no longer distinctive decades ago. I missed the link with the VR application, which seemed to have originated more from a marketing standpoint than from product management scope. Innovations can contribute to brand experience, but the road to adding business value is often very long. The reason for writing this article is that I have in recent years seen a shift that I would like to share with you.\nAt Luminis, we are passionate about digital products that matter: products that are valuable to users and that enable companies to make great strides. We are currently working for a number of customers in the installation industry that are seeing their business grow through digitisation because their innovations meet the real challenges their customers face.\nThe Challenges\nSpecialist knowledge\nThe biggest challenge for the installation industry is knowledge. Attracting well-trained technical personnel is not easy to say the least. As a result, installers need more guidance to properly perform their work at customer locations. Dutch language proficiency is sometimes another factor that increases this challenge. Suppliers often offer support in the form of third-line employees and notice that this increases the pressure on specialist knowledge. In addition, this is not a scalable solution, and there is usually no business model for third-line support, which increases costs. Furthermore, the lower level of available knowledge increases the amount of mistakes that are made, for example when a new installation is taken into use, and reduces the appreciation of end customers. Scenarios like this result in multiple service visits by the installer, which increases costs for the installation company, and makes them less inclined to recommend such an installation in the future.\nEase of operation\nSuppliers are responding to challenges by making the operation of devices simpler and less prone to errors. Often devices are provided with a user interface and software. Although this certainly increases the ease of use, the costs of devices also increase significantly, and it is a challenge to keep software up to date.\nLack of data\nA second challenge in the installation industry is that device suppliers usually do not have the required information about the use of their devices: who use them, where, when and most importantly: how? The installations of devices are often carried out by independent companies and the devices in fact disappear out of reach for the supplier. This is particularly problematic for R&D departments of suppliers, as this data would be invaluable for determining the product roadmap for these devices.\nThe Solutions\nLuminis works with various companies in the installation industry, such as BDR Thermea, Flamco and Bronkorst High-Teach. These companies all recognise the challenges described above. Through our collaboration we have been able to find fitting solutions for these challenges. As is often the case, the implementation of solutions differ for each company. For example, for Bronkhorst we started an online Design Sprint to arrive at a validated prototype for their specific problem. Yet the solutions also have some similarities.\nMobile\nIn all projects for clients in the installation industry, we have developed an app that communicates with the device via Bluetooth or WIFI. The screen of a smartphone or tablet opens up completely new possibilities for user support compared to hardware operation on the device. In addition, the smartphone is of course connected to the internet. This may not be the case when the installer is working, but even in that case the smartphone or tablet can download information at another time to help during installation. This information can be tailor-made. After all, the app communicates with the device and therefore only information is shown that relates to the specific device and the exact situation indicated by the device. By means of language settings, the instruction can be offered in the correct language and provided with images or videos to reduce the risk of errors.\nCloud environment\nAs is becoming clear, it is not just about an app that needs to be developed. A central content server is also necessary, and of course the content (instructions) for the installer must be up to date and structured in a way that allows it to be offered in accordance with the situation. We often see that the introduction of a solution like is the trigger that ensures that content is improved. The content thus grows and is enriched by the experiences gained through usage. This content does not only consist of instructions. We see that this is also used for the distribution of software on the various devices. An app is the perfect means to download and install this software on the device in a safe way.\nOne or two directions?\nThe data flow does not only have to go from a central cloud to the mobile device. If the user consents to this, the device can also send usage data to the cloud. In this way, insight is gained into where devices are located and which malfunctions occurred. This information helps suppliers to further improve devices and help the installer help customers faster and more focused, thereby reducing downtime.\nJifeline\nAnother Luminis customer who has taken these types of solutions one step further is Jifeline. Dealer and garage companies see that vehicles are increasingly equipped with complex software. Reading and installing this requires specialist knowledge. Consider resetting a reversing sensor after installing a towbar. Jifeline supports garages by performing these activities remotely. This means that garages no longer have to invest in acquiring this type of specialist knowledge, while still being able to offer solutions for customers. At Jifeline, this support service is not a service that is added to the delivery of a device, but the service is a business model in itself. A good example of how technological innovation creates value for our customers\u2019 customers.\nIntroducing digital products that really matter starts with user experience design. Only when the end customer immediately recognises the added value will the use of an app or web application increase explosively. The combination of cloud, data and connected devices make these types of solutions possible for many companies. Fortunately, nowadays it is no longer necessary to develop these solutions from scratch yourself. There are more and more semi-finished products that enable companies to experiment with this in an accessible and economically attractive way.\n", "tags": [], "categories": ["Blog", "Cloud", "Strategy &amp; Innovation"]}
{"post_id": 25604, "title": "Gradle: behind the basics", "url": "https://www.luminis.eu/blog-en/development-en/gradle-insights-for-when-you-want-more-than-defining-dependencies/", "updated_at": "2021-08-10T16:09:29", "body": "If you know the basics of Gradle but want to have a better understanding of how it works and what else is possible keep on reading. In this post I will talk about:\n\nThe Gradle build lifecycle\nGradle plugins\nUse of properties\nKotlin as DSL\n\n\u00a0\nUp until recently I only used Gradle when I needed to add a dependency or some basic plugin. I never really took the time to dig deeper into Gradle. I knew that Gradle had way more to offer than what I was using and recently I had some spare time and decided that this was the moment for me to go past the basics. I took a course on Gradle (https://www.udemy.com/course/gradle-masterclass/) and did some practicing and this blog post is the result of the most important bits that I\u2019ve learned.\n\u00a0\nThe Gradle build lifecycle\n\nBuild phases\n\n\nA Gradle build has three distinct phases.\n\n\n\nInitialization\nGradle supports single and multi-project builds. During the initialization phase, Gradle determines which projects are going to take part in the build, and creates a\u00a0Project\u00a0instance for each of these projects.\nConfiguration\nDuring this phase the project objects are configured. The build scripts of\u00a0all\u00a0projects which are part of the build are executed.\nExecution\nGradle determines the subset of the tasks, created and configured during the configuration phase, to be executed. The subset is determined by the task name arguments passed to the\u00a0gradle\u00a0command and the current directory. Gradle then executes each of the selected tasks.\n\n\n\n\nsource: https://docs.gradle.org/current/userguide/build_lifecycle.html\n\u00a0\nAs explained above the Initialization phase is responsible for determining what projects are going to be used. This is exactly where the settings.gradle file is for, telling Gradle what different projects/modules there are.\n\nBesides the settings.gradle file there\u2019s another file that\u2019s part of the Initialization phase: the init scripts or init.gradle file.\nThe init.gradle file (https://docs.gradle.org/current/userguide/init_scripts.html) allows you to define logic before the Gradle build has actually started. This allows you to for example change the way Gradle logs or add a mandatory company provided repository for dependencies.\nOne of the strange things about the init script is that the location of this file is inside of the .gradle folder which is excluded from git. An alternative would be to provide the script as an argument to Gradle using \u2013init-script init.gradle. However that is bound to go wrong at some point.\nThe most common approach that I\u2019ve seen is creating a custom distribution of Gradle with the init script included. This feels finicky for something that\u00a0 seems a bit of a niche. But apparently, people have had a proper use case for this for example: https://blog.mrhaki.com/2012/10/gradle-goodness-distribute-custom.html\n\u00a0\nAs for the Configuration and Execution phase, both phases make use of build.gradle files of which there can be multiple as opposed to the settings.gradle file of which there can only be one.\nin the Configuration phase Gradle walks through the script to find out what tasks there are. In a way this can be compared to asking people around you at work what their plans are for today, what input they need to get started and who is waiting for their work.\nThen in the Execution phase all of the work is executed in the right order starting with the tasks which have no dependencies on others etc.. until all work is done. As you can imagine if 2 tasks are in need of each other albeit directly or through other tasks, a circular dependency occurs. Gradle detects this part in the configuration phase and then refuses to perform the Execution phase.\nIn order to illustrate what I\u2019ve explained above, I created an example.\n\nHere I\u2019ve created a Gradle task \u2018HelloThere\u2019 which performs some setup on the newly created task such as setting the description and then defines what logic needs to be executed once the \u2018HelloThere\u2019 task is executed in the doLast part of the task (more about the doLast part in the next section).\n\nAs you see above there is a distinction between what is executed in the configuration phase vs the execution phase. This matters because the code in the configuration phase is executed for every task you execute(!). While the code in the doLast is only executed if you execute the specific task.\n\u00a0\nExtending the Gradle lifecycle\u00a0\nIn the example above I created a new task that I had to explicitly start by executing the command: ./gradlew HelloThere. However, ideally your new task is part of some existing lifecycle and started automatically whenever it is ready to perform its job.\nWhen a task is not dependent on any other task you can make the task part of the \u2018defaultTasks\u2019 method.\nHowever most of the time your task is dependent on some other task. In this situation, a better approach would be to make your new task directly depent on the other task.\n\nHere you can see an example where I\u2019ve introduced a new task \u2018goodbye\u2019 which is the defaultTask which means we can start this task by simply executing ./gradlew. However the \u2018goodbye\u2019 task first needs the \u2018HelloThere\u2019 task the be executed.\n\u00a0\nPlugins\nPlugins are basically nothing more than a set of tasks that hook into the existing Gradle tasks (using the same dependsOn logic as above) and/or provide standalone tasks. The java plugin for example (https://docs.gradle.org/current/userguide/java_plugin.html) extends the existing Gradle tasks with logic to compile java code, run tests, build a jar etc..\n\nIn the image above you can see what tasks the java plugins adds and their interdependency on each other\n\u00a0\nNoteworthy plugins\nBesides mandatory plugins to run your project like the Java plugin, there are also utility plugins that help make a developers life easier.\nAn example of this is the scan plugin built by Gradle itself (https://docs.gradle.com/enterprise/gradle-plugin/). It adds the option to scan your Gradle build process for all sorts of metadata such as how long tests took to run but also suggestions about how to improve the speed of you build and way more.\nNow that it is clear that plugins are nothing more than a bundle of new tasks or an extension of existing tasks it\u2019s easy to create your own. I won\u2019t go into the details here but if you want to dive deeper check out https://docs.gradle.org/current/userguide/custom_plugins.html.\n\u00a0\nThe use of properties\nEvery build script has some sensitive information that you don\u2019t want to share. Think of for example credentials for your repository manager, Git or if you\u2019re working with Android, the play store.\nIdeally you automate your entire build process so you don\u2019t have to type in passwords manually or define them hardcoded in your project ever. But at the same time you don\u2019t want to check your sensitive passwords into Git as part of your buildscript or code.\nThis is where the gradle.properties file comes into play. In this file you can define properties in a key=value manner and their contents can be used in your build.gradle and settings.gradle file. You can either create a gradle.properties file in the root of your project (by default part of your versioning system) or in your $GRADLE_USER_HOME. The advantage to the latter is that you credentials which you use over different projects in a single location.\n\nexample gradle.properties file\n\nHere\u2019s an example of how to use a Gradle property combined with the hasProperty() function. The hasProperty() does what the name implies checks if the property is there and is useful to find out why something isn\u2019t working.\n\u00a0\nKotlin as DSL\nInstead of Groovy which can take some time to get used to you can also use Kotlin (https://docs.gradle.org/current/userguide/kotlin_dsl.html)! What\u2019s better than using the same language for your project and build scripts. Besides, Kotlin is statically typed whereas Groovy is not making it easier to create build scripts!\nOne other advantage is that Intellij offers better support through things like autocomplete and fancy colors. Below you can see a side-by-side comparison of the two.\n\n\u00a0\nClosing thoughts\nLearning something is always fun I had fun in this process of doing so. As for how to keep up-to-date with Gradle in the future and continue learning I\u2019ve found the official documentation (https://docs.gradle.org/current/userguide/userguide.html) really useful. It\u2019s clear explicit and has lot\u2019s of examples and if you really don\u2019t know you can always try stackoverflow \ud83d\ude09\n\u00a0\nThanks for reading, I hope I\u2019ve helped you gain some new insights into Gradle and its possibilities!\n", "tags": [], "categories": ["Blog", "Development"]}
{"post_id": 26604, "title": "Java with Lambda does not have to be slow", "url": "https://www.luminis.eu/blog-en/cloud-en/java-with-lambda-does-not-have-to-be-slow/", "updated_at": "2021-08-10T16:09:42", "body": "As a seasoned Java programmer, usually with Spring Boot, it hurts to learn that most lambdas use JavaScript/TypeScript. What if you want to keep writing your Lambda in Java? A query in google teaches us that Quarkus is the way to go. This blog post introduces Quarkus, GraalVM, and we top it off with AWS CDK to deploy the sample.\nIntroducing Quarkus\nThe goal for Quarkus is to create a framework that makes Java applications ready for the cloud. It uses GraalVM to build Docker images running Java as native applications. The improvements to first response times are impressive, as is the memory consumption. This procedure does limit the libraries that you can use. However, more and more extensions become available. I got inspired to look into Quarkus (again) after reading this very cool blog post about running Lucene as a Lambda.\nThe trick with Quarkus is creating a native image with as little as possible reflection and, as much as possible build metadata processing upfront. That way, less processing is required at the start time. Classes only required during setup can be removed. That way, the image and the used resources are lower. Read more about it on the Quarkus page Container First.\nGenerating the Lambda\nIf there is one thing you have to like about Quarkus besides blazing-fast java applications, it is their documentation and tutorials. The tutorial Quarkus \u2013 Amazon Lambda\u00a0gives you a jump start to create your own Lambda. Some aspects that I like is that you can include multiple Lambdas in one package. Use an environment variable to choose the Lambda to run. The generated code also comes with a few scripts that help you deploy and interact with your Lambda. I don\u2019t use them; I prefer the CDK approach. Still, it is good to know they are available. The following code block shows the maven command to generate our project using the archetype provided by Quarkus.\n\r\nmvn archetype:generate \\\r\n       -DarchetypeGroupId=io.quarkus \\\r\n       -DarchetypeArtifactId=quarkus-amazon-lambda-archetype \\\r\n       -DarchetypeVersion=1.12.2.Final \\\r\n       -DgroupId=eu.luminis.aws \\\r\n       -DartifactId=aws-quarkus-lambda\r\n\nAfter an \u201cmvn clean package\u201d, the target folder is available with the mentioned shell scripts and a function.zip file. This function.zip file contains the optimized code for the Lambda. Later in this blog, we use this zip file in our CDK stack to upload the Lambda. We need to add one property to the maven config. For the native profile, we add the property \u201cquarkus.native.container-build\u201d. I removed the generated Lambdas and created my own SendMessageLambda and HelloLambda. If you have only one Lambda available, this is automatically selected. I like to experiment with two Lambdas in one distribution.\nUsing Java for a Lambda does require a specific interface. AWS does provide this. The Java class that receives the events had to implement the interface \u201cRequestHandler\u201d. You can specify the InputObject and OutputObject yourself. The following code block shows the java code for the Lambda-class.\n\r\n@Named(\"message\")\r\npublic class SendMessageLambda implements RequestHandler<InputObject, OutputObject> {\r\n    @Inject\r\n    ProcessingService service;\r\n\r\n    @Override\r\n    public OutputObject handleRequest(InputObject input, Context context) {\r\n        String process = service.process(input.getGreeting(), input.getName());\r\n        OutputObject out = new OutputObject();\r\n        out.setResult(process);\r\n        out.setRequestId(context.getAwsRequestId());\r\n\r\n        return out;\r\n    }\r\n}\r\n\nNotice the \u201c@Named\u201d annotation; you used this annotation to specify the Lambda currently in use. Use the application.properties file or a system environment parameter. The \u201c@Inject\u201d annotation configures the dependency injection.\nDeploying the Lambda using CDK\nWith the Lambda available as a zip file in the maven target folder, we can use CDK to deploy the Lambda to AWS. Within the project, we create a CDK folder and initialize the CDK project using TypeScript.\n$ cdk init app --language typescript\r\n\nThe configuration of the stack to include the Lambda looks like this.\n\r\nconst quarkusLambda = new lambda.Function(this, \"QuarkusLambda\", {\r\n  runtime: lambda.Runtime.PROVIDED_AL2,\r\n  handler: \"io.quarkus.amazon.lambda.runtime.QuarkusStreamHandler::handleRequest\",\r\n  code: lambda.Code.fromAsset('../target/function.zip'),\r\n});\r\n\nDo not change the handler; Quarkus provide this class. Notice the runtime that we use. As we create a native image, we use the provided runtime.\u00a0Before we run cdk deploy, we first have to build the java project using maven and the native profile.\n$ mvn clean package -Pnative\r\n$ cd cdk\r\n$ cdk deploy\r\n\nUsing the AWS console, we can test the Lambda. Open the Lambda, click on the test tab, and enter the following JSON document as input.\n\r\n{\r\n  \"name\": \"Hello\",\r\n  \"greeting\": \"readers\"\r\n}\r\n\nAfter the invocation, you see the following output.\n\nNotice the init duration (196.37 ms) and the duration (4.69 ms). On a second run, the init duration is gone, and the duration becomes 1 ms.\nUse environment variables to select the right Lambda\nUsing the console, we provide the following environment variable \u201cquarkus_lambda_handler\u201d. Go to the \u201cConfiguration\u201d tab, environment variables and add the one mentioned above. In our case, we give it the value \u201chello\u201d, now the output becomes \u201cHello World!\u201d, no matter the input. As we change the configuration, the Lambda requires a cold restart again.\nOf course, we also want to be able to configure this parameter using CDK. This configuration is straightforward; add an environment block to the lambda specification.\n\r\nconst quarkusMessageLambda = new lambda.Function(this, \"QuarkusMessageLambda\", {\r\n  runtime: lambda.Runtime.PROVIDED_AL2,\r\n  handler: \"io.quarkus.amazon.lambda.runtime.QuarkusStreamHandler::handleRequest\",\r\n  code: lambda.Code.fromAsset('../target/function.zip'),\r\n  environment: {\r\n    quarkus_lambda_handler: \"message\"\r\n  }\r\n});\r\n\nConcluding\nYes, we still can use our java knowledge to create high performing lambdas. Of course, we created a basic Lambda, but more advanced lambdas are possible as well. Quarkus is perfect for making these java Lambdas. I want to look at Quarkus as a modern rest endpoint combined with the AWS API Gateway in the next blog. So stay tuned for more Quarkus.\nReferences\n\nhttps://www.morling.dev/blog/how-i-built-a-serverless-search-for-my-blog/\nhttps://quarkus.io\nhttps://quarkus.io/guides/amazon-lambda\nhttps://github.com/luminis-ams/aws-cdk-examples\n\n\u00a0\n", "tags": ["cdk", "cloud", "java", "Quarkus"], "categories": ["Blog", "Cloud"]}
{"post_id": 25679, "title": "From legacy to the cloud: how modern software development speeds you up", "url": "https://www.luminis.eu/blog-en/naar-moderne-softwareontwikkeling/", "updated_at": "2022-02-04T11:47:11", "body": "These days, software is indispensable for most organizations. When talking to our customers, we often hear that although their existing software is well-tested and has been running fine for years, their system has reached its maximum lifespan. Moreover, a large chunk of their IT budgets is needed to keep existing systems up and running. Now, modernizing a running system is a complex process. At a certain point, it is more efficient to switch to a new generation of technology, and to slowly migrate legacy software. However, what does \u201cat a certain point\u201d mean for your organization? In this article, I\u2019ll show you when to switch to a new generation of technology, and how to reduce risks during a migration.\nModern software: what\u2019s in it for me?\nLet\u2019s start by understanding when a software system is outdated. This obsolescence is partly technical: software grows more complex over time by definition. Over time, it becomes too expensive to adapt software. Sometimes software becomes so complex that trying to understand it causes headaches. What we also notice is that in the rapidly changing digital world, there are fewer and fewer people on the market who still master older technology. Technical amortisation is not exclusive to the digital world, of course, but if you\u2019re not careful, software will be outdated quickly.\nAging can also be economic: new services are coming onto the market that you can no longer compete with. The world never stops turning; people expect new generations of digital technology that work well on mobile phones, or that just respond to their voice commands. The cloud accelerates innovation incredibly fast, which means that customers expect software to be always accessible, to work smoothly and to be fast even during peak traffic. Cloud technology Cloud vendors now have a proven track record and can guarantee the desired degree of availability.\u00a0\nWouldn\u2019t it be nice if your software could be easily adapted? If you can quickly develop an idea and implement a new feature? If said new feature can be immediately distributed to your customers to validate the new idea? With the evolution of the cloud, we now look very differently at software development than we did with technology of previous generations. Using cloud technology requires a smaller capital investment, as well. If it turns out that there\u2019s a better idea, your organization will have learned this with a small investment.\nHow does this work? Faster innovation with cloud technology is made possible by a lower capital investment, where you only pay for what you actually use. Developers do not have to build their own components, but can reuse proven, safe and efficient components other developers have made. Think of technology that you had to acquire knowledge for until not so long ago, such as load balancers, web servers and DNS changes. In addition, with cloud technology you can deploy updates to software several times a day and quickly roll back if and when problems arise. You no longer have to design a complex solution far in advance, so you can start validation quickly.\nFinally, cloud services make life easier for developers in your organization. Cloud vendors supply tools for every task you can think of. Example: with AWS, if you build apps for the Apple ecosystem, you can compile software in the cloud. Traditionally, developers had to buy Apple computers and build their software on those. Having computers around like this introduces headaches of its own. Think people tripping over power cords. To solve this, AWS has racked up actual Apple Minis, so developers no longer have these problems. Sometimes, developers spend an extra hour to get something to run by themselves. I know I\u2019m guilty. Developer time is precious, though, and might be better spent elsewhere.\nHow do you notice that your organization is at risk?\nThere are several signs that indicate you need to update software on the short term. For example, software sometimes only works on a company network. By now, people expect that they can access the their stuff at any time, on their own mobiles, without messing around with VPNs. Store confidential information securely in the cloud, so it can be distributed. In addition, privacy legislation made giant strides by now. In legacy systems, the disposal of data is not always thorough enough by current standards. Furthermore, we increasingly see organizations making their own system choices outside the all-powerful IT department. People want to buy their software tools directly.\nKeeping existing systems running is expensive, not only due to the growing complexity of this changing world, but also due to the scarcity of knowledge. It is therefore important to find out what the costs and benefits of the old system are. Finally, there are other reasons to switch, for example the end of support from a supplier or expiring maintenance contracts.\nFrom legacy to modern software: start small, start now\nWe believe that the best way to rebuild software is with an incremental way of working. To reduce the impact on the current business of the organization, we start small and gradually transfer parts of the technology landscape. After all, the more experience about cloud within the organization, the more projects can be started that help you benefit from cloud technology.\nIn the meantime, keep the legacy system running. This often means that data is stored in two places for a while, so that business-critical data is not lost. Existing systems are more often monolithic and, before things can be migrated, need changes to make chunks of data available. Define in your organization which data is leading: that of the new cloud systems or of the legacy systems. The larger the portion that resides in the cloud, the easier it will be to start new experiments. This way your organization can start doing things that were previously possible for tech giants only.\nWhen building software you have to choose what you want to be good at. You can choose to build everything yourself, including maintaining your own servers, for example. However, your organization is not going to make the difference with having their own servers; you would be better of just renting rackspace. Not so long ago, organizations had to manage their own server fleet, know everything about the management of operating systems and have all kinds of network knowledge, before they could only start building useful things. Only then, people could \u00a0see your organization. The cloud vendor does that work for you now, so you can focus on your strengths.\n\nNow your organization can focus on writing the application and lean on the cloud vendor for more technical matters. This requires less knowledge within the organization, which allows your development group to focus on what makes your organization unique.\u00a0\nHow do we do this?\nThere is a range of flavours between running in the cloud completely and maintaining a system on your own metal. Wherever you currently are on this spectrum, it is always important to reduce risks with a gradual transition. The first step of the transition is to determine the migration strategy. The first strategy is just keeping the existing technology; this way there is little risk, but also little reward. The second strategy, moving existing technology to the cloud, brings more rewards. Determine which parts of your technology landscape can be moved and which parts can be phased out. Finally, the strategy that can have the most impact is to rebuild using managed cloud services. Read more about this spectrum of strategies in the whitepaper by Piet van Dongen, our colleague who has the remarkable talent to give serious subjects such as cloud migrations a funny twist.\nOnce you have taken the first steps in the cloud, it is important to get to know the hundreds of managed cloud services better. There is such a wide range of services that finding the right service is a competence in itself. This is precisely where Luminis can offer a helping hand: as a partner of the major cloud providers, we can find the right services and proactively propose them if we spot possible improvements for the organization. Bert Ertman tells more about how these cloud services work in his blogpost. In this day and age where building and maintaining software happen at the same time, we stay in the loop after building.\u00a0\nWithout Luminis to help guide you, it is difficult to know what you can do with a cloud provider like AWS. AWS is the market leader because of their reliability and enormous range of product, but not because their cloud services are easy to grasp. Their cloud services have rather mysterious names and are introduced at such a pace that it is a day job to keep up with new developments. For example, say you have a lot of location data. How do you choose between AWS Kinesis, Timestream or just using Lambdas? In addition, the cloud bill is the only place where your organization gains complete insight into the costs, which can only be viewed after using services. We analyze which services entail excessive costs beforehand and, for example, migrate data to a different type of storage or use a different type of database if this can help save costs. This way we help your organization regains control of cloud costs. Finally, we analyse which usage we can predict: smart purchasing makes smart deals possible and the costs of the IT landscape will go down even further. Yet the ultimate goal is not to think cost-driven, but to be agile through technology. This makes IT fun again and we can make people happy with robust systems that make it possible to do what was previously impossible.\n", "tags": [], "categories": ["Blog", "Strategy &amp; Innovation"]}
{"post_id": 26378, "title": "AWS Lambda here, there and everywhere", "url": "https://www.luminis.eu/blog-en/cloud-en/aws-lambda-here-there-and-everywhere/", "updated_at": "2021-08-10T16:25:31", "body": "Everybody is talking about serverless, and with serverless comes serverless functions. Small pieces of code (in theory) that receive an event (input) and return a message (output). They do not need a server. Therefore they are serverless, and they scale on demand. On the AWS platform, a serverless function is called a Lambda. In theory, you write the code, and AWS takes care of the rest. There is a reason why I put \u201cin theory\u201d in some locations in the text. Even writing the hello world sample does need some authorization configuration, and when running complete Docker images as a lambda, you cannot talk about small pieces of code. Time to focus on typical usage patterns for lambdas.\nUsage patterns for Lambdas\nWhen thinking about Lambdas, think of small pieces of functionality. In your average MVC application, you can think of the request handlers, services, and repository methods. The big difference is, we do not create a class with multiple methods. Instead, we make separate Lamdas for each method. But what about reuse and libraries? That is where Layers come in, more on those things later on.\u00a0Example usage of lambdas, you require an API to be used by a mobile app. You can specify an API file using OpenApi or create the AWS API Gateway service\u2019s endpoints. Next, you write a lambda to handle the API call, get some data from DynamoDB and return the response. Another example, when deploying an AWS Elasticsearch cluster using CDK, you use a lambda to interact with Elasticsearch for configuration queries. You can read more about deploying elasticsearch in another blog post of mine, Deploying a secure AWS Elasticsearch cluster using CDK. A final example, when sending notifications on arrival of a new order in DynamoDB, create a lambda function triggered by the event that sends a message to AWS SNS.\nTalking about it is easy, time to create a few lamdas. The next sections give code samples of lambdas and how to manage them using AWS CDK. You can find the code in our Github repository.\nRunning examples \u2013 Hello world.\nAll good? Time for some running examples. To make setting up the examples easier, we use AWS CDK, and what is easier for a Lambda then creating a HelloWorld lambda? The following code block shows how to create a lambda that receives an event with two parameters. The first parameter, \u2018say,\u2019 is the message you want to bring. The second parameter, \u2018to\u2019, is the person to say the message to. In the lambda, we log the famous and customizable hello world message to the console. The final line uses the context to find the log stream\u2019s name to send the logs. The structure for lambda logs is first the log-group. This name is the name returned by deploying the stack using CDK. Within the log-group, there are multiple streams. We obtain the right stream using the context as mentioned before.\n\r\nconst helloLogsLambda = new lambda.Function(this, \"HelloLogsLambda\", {\r\n  runtime: lambda.Runtime.NODEJS_12_X,\r\n  handler: \"index.handler\",\r\n  code: lambda.Code.fromInline(`\r\n    exports.handler = async function (event, context) {\r\n      console.log(event.say + \" \" + event.to + \"!\");\r\n      return context.logStreamName;\r\n    };\r\n  `),\r\n});\r\n\nUse CDK to deploy the stack to AWS. Next, we want to execute the lambda, you can use the console, but I prefer to use the AWS command-line tool. We use two different modules, the lambda module for interacting with lambda functions and the logs module, to obtain the right log stream. Another thing to notice is the way to send a payload to a lambda through the command-line tool. We need to base64 encode the payload before sending it. With sed, we find the name for the log stream. The log-group is the same as the name of the lambda. The script is as follows.\n\r\n#!/bin/bash\r\nlambda_name=\"AwsLambdasStack-HelloLogsLambdaA3F63B77-130WFH89QU9AN\"\r\npayload=$(echo '{\"say\": \"Hello\", \"to\": \"Lambda\" }' | openssl base64)\r\naws lambda invoke --function-name $lambda_name out --payload \"$payload\"\r\nsed -i'' -e 's/\"//g' out\r\nsleep 15\r\naws logs get-log-events --log-group-name /aws/lambda/$lambda_name --log-stream-name $(cat out) --limit 5\r\n\nThe next block shows the output from the script. The output is verbose. There is a start event, the console log event, the end event, and a report.\n\r\n{\r\n    \"events\": [\r\n        {\r\n            \"timestamp\": 1616341610877,\r\n            \"message\": \"START RequestId: 7d75dba5-0c54-45b1-8378-7c8cfce71873 Version: $LATEST\\n\",\r\n            \"ingestionTime\": 1616341619926\r\n        },\r\n        {\r\n            \"timestamp\": 1616341610893,\r\n            \"message\": \"2021-03-21T15:46:50.878Z\\t7d75dba5-0c54-45b1-8378-7c8cfce71873\\tINFO\\tHello Lambda!\\n\",\r\n            \"ingestionTime\": 1616341619926\r\n        },\r\n        {\r\n            \"timestamp\": 1616341610895,\r\n            \"message\": \"END RequestId: 7d75dba5-0c54-45b1-8378-7c8cfce71873\\n\",\r\n            \"ingestionTime\": 1616341619926\r\n        },\r\n        {\r\n            \"timestamp\": 1616341610895,\r\n            \"message\": \"REPORT RequestId: 7d75dba5-0c54-45b1-8378-7c8cfce71873\\tDuration: 18.12 ms\\tBilled Duration: 19 ms\\tMemory Size: 128 MB\\tMax Memory Used: 64 MB\\tInit Duration: 148.96 ms\\t\\n\",\r\n            \"ingestionTime\": 1616341619926\r\n        }\r\n    ],\r\n    \"nextForwardToken\": \"f/36045622418351923997532045574640095047475813476767367171\",\r\n    \"nextBackwardToken\": \"b/36045622417950510583958494358092452118568142969659719680\"\r\n}\r\n\nRunning examples \u2013 Sending a message.\nWe now know how to create a lambda and how to call it using the command line. Time to upper the game and introduce the Simple Notification Service to send an email with the content coming from a lambda. We again call the lambda from the command line and send the message to the configured email address. The first part of the stack is creating the SNS Topic. The code block below shows the creation of the Topic as well as adding an email type subscription.\n\r\nconst snsTopic = new sns.Topic(this, 'LambdaSnsTopic', {\r\n    displayName: 'Lambda SNS Topic to send email',\r\n});\r\nsnsTopic.addSubscription(new subs.EmailSubscription(\"your_email@here.com\"));\r\n\nNext, we create a Lambda that sends a message to the Topic. This time we do it a little bit different. Instead of embedded JavaScript for the lambda implementation, we use a separate file. We place the file in the lambda folder. This location is important, as we need it for the CDK configuration. The lambda itself is easy, but now we have another way to store the lambda. The following code block shows the lambda.\n\r\nconst { SNSClient, PublishCommand } = require(\"@aws-sdk/client-sns\");\r\nexports.handler = async (event, context) => {\r\n    const sns = new SNSClient(process.env.REGION)\r\n\r\n    const publishCommand = new PublishCommand({\r\n        Message: event.message,\r\n        TopicArn: process.env.SNS_TOPIC_ARN,\r\n    });\r\n    await sns.send(publishCommand);\r\n    console.log(\"Send a message to the SNS Topic: \" + process.env.SNS_TOPIC_ARN);\r\n    return context.logStreamName;\r\n}\r\n\nNext, we create the lambda in AWS using CDK, and we grant publisher rights to the lambda on the created Topic.\n\r\nconst sendToSNSLambda = new lambda.Function(this, 'SendToSNSLambda', {\r\n    runtime: lambda.Runtime.NODEJS_12_X,\r\n    code: lambda.Code.fromAsset('lambda'),\r\n    handler: 'send-message.handler',\r\n    environment: {\r\n        SNS_TOPIC_ARN: snsTopic.topicArn,\r\n        REGION: this.region,\r\n    }\r\n});\r\nsnsTopic.grantPublish(sendToSNSLambda);\r\n\nOf course, we need a new script to execute the lambda from the command line. I only show the two lines that changed in the script compared to the previous bash script.\n\r\nlambda_name=\"AwsLambdasStack-SendToSNSLambdaE1D5DD6A-T6Q6W0KDYCTV\"\r\npayload=$(echo '{\"message\": \"Hi, hope you like this message from AWS.\" }' | openssl base64)\r\n\nNow when executing the lambda, I get the following email message:\n\nRunning examples \u2013 Provide a REST endpoint.\nThe AWS command-line tool is a good tool to execute a lambda. However, we prefer having a REST endpoint that a mobile app or a website can call\u2014time to meet the AWS API Gateway. The API Gateway configures an endpoint that, when called, executes the configured lambda. This example contains the API Gateway and a lambda that again posts a message to SNS like in the previous example. The request it performs is a POST with a JSON document in the body. The JSON document contains a field message. The contents of this field are sent to SNS and, in the end, to an email. Beware, the usage of the API Gateway here is elementary. Explaining all the nuts and bolts of the gateway is not the focus of this blog as we focus on the lambdas.\nThe code for exposing the lambda is the same as for the previous example. The lambda itself is not. Notice that we are dealing with strings in the request and the response. The lambda has to parse into and from JSON itself. The response is also worthy of checking out. Without a correct structure, the lambda is doing fine, but the gateway throws an error. The following code block shows the lambda code.\n\r\nexports.handler = async (event) => {\r\n    const sns = new SNSClient(process.env.REGION)\r\n    const jsonBody = JSON.parse(event.body);\r\n\r\n    const publishCommand = new PublishCommand({\r\n        Message: jsonBody.message,\r\n        TopicArn: process.env.SNS_TOPIC_ARN,\r\n    });\r\n\r\n    await sns.send(publishCommand);\r\n\r\n    return {\r\n        \"isBase64Encoded\": false,\r\n        \"statusCode\": 200,\r\n        \"headers\": {\r\n            \"Access-Control-Allow-Origin\": '*'\r\n        },\r\n        \"body\": JSON.stringify({\r\n            \"message\": \"OK\"\r\n        })\r\n    };\r\n}\r\n\nWhat about the gateway? As mentioned before, creating a simple gateway is also very simple in the CDK. Beware, we expose the root resource, which is not a good practice. Check the gateway documentation for more information.\n\r\nnew apigateway.LambdaRestApi(this, 'JettroLambdaApi', {\r\n    handler: gatewayLambda,\r\n});\r\n\nRunning a CDK deploy exposes the URL of the gateway:\nAwsLambdasStack.JettroLambdaApiEndpoint0A243061 = https://943pdmvxn7.execute-api.eu-west-1.amazonaws.com/prod/\r\n\nSend a POST request using any tool you like. I prefer using the Intellij HTTP client for this.\n\r\n### Send POST request with json body\r\nPOST https://943pdmvxn7.execute-api.eu-west-1.amazonaws.com/prod/\r\nContent-Type: application/json\r\n\r\n{\r\n  \"message\": \"Hello from API Gateway using HTTP client.\"\r\n}\r\n\nLayers\nWhen writing code in a lambda, you can still use libraries. You can add the library immediately to your lambda package. However, this means you have to redeploy it each time you deploy your lambda. Also, you cannot easily reuse the code among your lambdas. These challenges are why AWS provides layers. A layer contains libraries created by yourself but also offered by AWS or third parties. Layers are extracted in a language-specific path within your lambda. When creating your lambda using Nodejs, the path to the libraries in a layer is \u201c/opt/nodejs\u201d.\nOf course, we want to use a layer to improve our sample application. Our Layer provides a function to create a response in the correct format for the API Gateway.\nThe code for the Layer is straightforward. The folder layout of the Layer is essential. If you make a mistake, you get errors telling you the Layer could not be found. You have to adhere to the following structure:\n-nodejs\n\u2013response-layer\n\u2014responses.js\n\u2013package.json\nThe name in the package.json is \u201cnodejs\u201d, and the main is \u201cindex.js\u201d. Below is the code for the layer in the file \u201cresponses.js\u201d\n\r\nexports.createResponse = (message) => {\r\n    return {\r\n        \"isBase64Encoded\": false,\r\n        \"statusCode\": 200,\r\n        \"headers\": {\r\n            \"Access-Control-Allow-Origin\": '*'\r\n        },\r\n        \"body\": JSON.stringify({\r\n            \"message\": message\r\n        })\r\n    };\r\n}\r\n\nBelow is the definition of the Layer and the lambda in CDK. Notice the path to the Layer and adding the Layer to the layers property of the lambda.\n\r\nconst responseLayer = new lambda.LayerVersion(this, \"ResponseLayer\", {\r\n    code: lambda.Code.fromAsset(\"./layers/response-layer\"),\r\n    compatibleRuntimes: [lambda.Runtime.NODEJS_12_X],\r\n});\r\n\r\nconst gatewayLambda = new lambda.Function(this, 'GatewaySendToSNSLambda', {\r\n   runtime: lambda.Runtime.NODEJS_12_X,\r\n   code: lambda.Code.fromAsset('./lambdas/lambda-gateway'),\r\n   handler: 'index.handler',\r\n   environment: {\r\n      SNS_TOPIC_ARN: snsTopic.topicArn,\r\n      REGION: this.region,\r\n   },\r\n   layers:[responseLayer],\r\n});\r\n\nThe final piece of the puzzle is importing the \u201ccreateResponse\u201d method from the module, do notice the path used in the require part.\n\r\nconst { createResponse } = require(\"/opt/nodejs/response-layer/responses\");\r\n\nStep functions\nAs I mentioned in the title, Lambdas are everywhere. I cannot give examples of all different scenario\u2019s. But using Lambdas in step functions is an important one to understand. A single Lambda is ideal for doing small tasks, but you need orchestration capabilities when dealing with more advanced tasks. You want to create an error flow. You want to do different actions based on some state\u2014time to meet AWS Step Functions.\nWith AWS Step Functions, you create a workflow or state diagram. Our flow consists of a few steps:\n\nRetrieve parameters from the request\nCheck if the message contains the word \u201cbad.\u201d\n\nNO: Send a message to the Topic and send an email.\nYES: Don\u2019t do anything\n\n\n\nI am nog showing the CDK code for lamdas again. But for each step in the workflow that calls a lambda, we need to create a LambdaInvoke task. The following code block shows creating the task.\n\r\nconst parseRequest = new tasks.LambdaInvoke(this, \"ParseRequest\", {\r\n  lambdaFunction: parseRequestLambda,\r\n  outputPath: \"$.Payload\",\r\n});\r\n\nThe next block shows how to use these tasks to create a state machine definition and the state machine itself.\n\r\nconst definition = parseRequest\r\n  .next(checkBadWords)\r\n  .next(\r\n    new sfn.Choice(this, 'No Bad Words')\r\n      .when(sfn.Condition.booleanEquals('$.valid', false), sendMessageFailed)\r\n      .when(sfn.Condition.booleanEquals('$.valid', true), sendMessage)\r\n      .otherwise(waitX)\r\n    );\r\n\r\nconst stateMachine = new sfn.StateMachine(this, \"StateMachine\", {\r\n  definition,\r\n  timeout: cdk.Duration.minutes(5),\r\n});\r\n\nIn the console, you can find the workflow with all the steps. For each stage, the input and output are available. That makes it easier to debug. The sample code also shows how to call the step functions from an API Gateway.\n\nConcluding\nAs you can see, Lambdas are the glue of the AWS Cloud. You find them everywhere. They are also very powerful to create a high available and scalable system. I have shown a number of typical usage scenarios. And working with Lambdas together with AWS CDK makes it easy to deploy them. If you want to have a better look at the sample code, check out the link to our Github repo containing a number of CDK experiments. This blog post uses the aws-lambdas and the aws-step-functions modules. Please contact me if you have questions or improvements.\nhttps://github.com/luminis-ams/aws-cdk-examples\n\u00a0\n", "tags": [], "categories": ["Blog", "Cloud"]}
{"post_id": 26289, "title": "Health dashboard with Kibana Canvas", "url": "https://www.luminis.eu/blog-en/development-en/health-dashboard-with-kibana-canvas/", "updated_at": "2021-08-10T16:25:24", "body": "In this blog post, I want to show you how we\u2019ve set up a health dashboard for the integration landscape of a client in Rotterdam. The dashboard shows the\u00a0health\u00a0of some important servers in our production landscape and the health of some Spring Boot apps. Also, some extra information is displayed, like the\u00a0version of the app and its uptime.\u00a0Once the pandemic is over, you can display this dashboard on a big OLED screen in the office so everybody can see how good (or bad) you\u2019re doing. Also, users browsing to the dashboard URL can also click on the blocks to get more information on a detailed page.\nThe end result looks like this:\nExample \u2018traffic light\u2019 dashboard showing the health of servers and services. O, o, Infohub is down!\nYeah, I know: I\u2019m a developer, not a designer \ud83d\ude09\nEach square consists of:\n\nAn Image / Image reveal: showing a green or red block (image) based on the status of a monitored item (up or down). This will use data from\u00a0heartbeat\nMarkdown text with static and dynamic text (showing version and uptime of a monitored item). This will use data from\u00a0metricbeat\n\nTo get the health- and version data into Elasticsearch / Kibana , we must first setup 2 beats the Elastic stack provides: heartbeat and metricbeat. We\u2019re using Elastic 7.9 here but i think the description below works for slightly older and newer versions also.\nSetting up Heartbeat\nHeartbeat\u00a0is a lightweight shipper for uptime monitoring. You can configure several types of monitoring like ICMP (the so-called ping), TCP, or HTTP. See the\u00a0quick start\u00a0for instructions on how to install heartbeat on your system.\nNote: if you have setup heartbeat, you can also have a look at the Uptime dashboard included in Kibana.\nHeartbeat config for a Spring boot app\nCreate a monitor config and put in the monitors.d folder, e.g. /etc/heartbeat/monitors.d/cis-infohub-http.yml\n- type: http\r\n  id: cis-infohub\r\n  name: CIS Infohub\r\n  enabled: true\r\n  schedule: '@every 30s'\r\n  hosts: [\"yourapphost:12000/management/health\"]\r\n  ipv4: true\r\n  ipv6: false\r\n  mode: all\r\n  method: \"GET\"\r\n  check.response:\r\n    status: 200\r\n    json:\r\n    - description: Status must be UP\r\n      condition:\r\n        equals:\r\n          status: UP\nThis config will make sure your app living @ <yourapphost> port 12000 is polled every 30 seconds by sending an HTTP request to the\u00a0Spring Boot actuator health endpoint. The response is checked for the JSON string \u201cUP\u201d and the results will be stored in the heartbeat-* index in elasticsearch (by default). We will use the status in a query later (see below)\nHeartbeat config for a server\nTo check if a server is alive and kicking, a ping request can be sent to it using the ICMP protocol. Store the monitor definition in the monitors.d folder, e.g. /etc/heartbeat/monitors.d/cis-appserver-icmp.yml\n- type: icmp # monitor type `icmp` (requires root) uses ICMP Echo Request to ping\r\n  # ID used to uniquely identify this monitor in elasticsearch even if the config changes\r\n  id:  yourappserverid\r\n \r\n  # Human readable display name for this service in Uptime UI and elsewhere\r\n  name: Our App Server\r\n \r\n  # Enable/Disable monitor\r\n  enabled: true\r\n \r\n  # Configure task schedule using cron-like syntax\r\n  schedule: '@every 60s'\r\n \r\n  # List of hosts to ping\r\n  hosts: [\"yourappserver\"]\r\n \r\n  # Configure IP protocol types to ping on if hostnames are configured.\r\n  # Ping all resolvable IPs if `mode` is `all`, or only one IP if `mode` is `any`.\r\n  ipv4: true\r\n  ipv6: true\r\n  mode: any\r\n \r\n  # Total running time per ping test.\r\n  timeout: 20s\r\n \r\n  # Waiting duration until another ICMP Echo Request is emitted.\r\n  wait: 5s\nThis monitor will ping the server(s) specified in the\u00a0\u2018hosts\u2019\u00a0field every minute. The results will also be stored in the heartbeat-* index in elasticsearch (by default).\nMetricbeat\nMetricbeat\u00a0is a lightweight shipper for metrics. It comes with a lot of modules like \u2018system\u2018 for shipping CPU, memory, network, uptime data, and more, \u2018http\u2018 for querying HTTP endpoints, \u2018Jolokia\u2019 for querying queue statistics, etc If you need to install metricbeat on your system, just follow the\u00a0quick start guide.\nMetricbeat config for a Spring boot app\nWe\u2019ll use the actuator info endpoint to determine the uptime of, for example, our camel context. The metricbeat HTTP module gives us the possibility to fetch and return data from an HTTP endpoint. Since the info endpoint returns JSON, we use the JSON metric set. Put the metricbeat monitor in the modules.d folder, e.g. /etc/metricbeat/modules.d/cis-infohub-http.yml\n</code class=\"yaml\">- module: http\r\n  enabled: true\r\n  metricsets:\r\n    - json\r\n  period: 30s\r\n  hosts: [\"yourappserver:12000/management/info\"]\r\n  namespace: \"cis\"\r\n  fields:\r\n    metric_id: cis-infohub\nWhen the metrics are retrieved, the JSON data will be stored under the given module name + specified namespace. For example:\n\nmetricbeat data from http endpoint\n\nWe\u2019re interested in the version of our app and the uptime as we will see later.\nMetricbeat config for a server\n- module: system\r\n  period: 15m\r\n  metricsets:\r\n    - uptime\nPut this metricbeat monitor in the modules.d folder\u00a0on the system you are monitoring, e.g. /etc/metricbeat/modules.d/cis-server-uptime.yml\nThe system module is just one of the many modules you can use. See the\u00a0Elastic module page\u00a0for more info. Also, the \u2018uptime\u2019 metric set of the system module is just one of the many sets you can use. See the\u00a0Elastic system module page\u00a0for more info.\n\u00a0\nCreating the Kibana Canvas\nNow, we will finally use all those gorgeous data we collected in the previous steps. You can use the gathered data in Kibana Dashboards, but also in a Powerpointy like sheet thingie called Kibana Canvas. You can create multi-paged, CSS styled, pixel perfect presentations like the one shown here, at least: that\u2019s what they\u00a0say.\n\nClick on left menu in Kibana / Canvas. Create a workpad. A workpad is a workspace for your presentations. A workpad consists of one or more pages with elements.\nAdding the green/red box (image)\n\nAdd element: Image / Image Reveal.\nClick on the \u2018+\u2019 on the right (Reveal image) and select \u2018Background image\u2019. Now you can select 2 images representing the up and down state.\nYou can use any image you want, say a green traffic light image for the foreground image, and a red traffic light image for the background. To switch between the images or better: reveal the correct image, we must first go to the Data tab and configure it to use our heartbeat data.\nGo to the Data tab, select Demo data and replace it with Elasticsearch SQL (or another way if you don\u2019t like SQL; then you on your own ;-))\nThe point is to come up with a value between 0 and 1 (100%) to reveal the image. Since we only need to completely hide or reveal the image, the values 0 or 1 will suffice. 1 is up (green), 0 is down (red).\nPreview the data end if you\u2019re happy, Save!\nGo to the Display tab again, and set the value to the field containing the 0 or 1.\n\nIn one picture:\n\n\u00a0\nAdding the uptime / version info\n\nAdd element / Text\nYou can use data from a query in the markup text. Great stuff!\nExample of uptime info:\n\n\nThe complete SQL is added below\nESQL & MD FTW !\nESQL for determining the up or down status\nselect case when monitor.status = 'up' then 1 else 0 end as status\r\nfrom \"heartbeat-*\"\r\nwhere monitor.id='yourappserverid'\r\norder by \"@timestamp\" desc\r\nlimit 1\nESQL for selecting version & uptime\nSELECT http.cis.build.version as version, http.cis.camel.uptime as uptime\r\nFROM \"metricbeat-*\"\r\nwhere agent.type = 'metricbeat'\r\nand fields.metric_id='cis-infohub'\r\norder by \"@timestamp\" desc\r\nlimit 1\nESQL for determining uptime (if you only got millis)\nSELECT uptimeDays, FLOOR(uptimeInHours-uptimeDays*24) AS uptimeHours, FLOOR(uptimeInMins-uptimeInHours*60) AS uptimeMins\r\nFROM (\r\n  SELECT FLOOR(system.uptime.duration.ms/1000/60/60/24) AS uptimeDays,\r\n    FLOOR(system.uptime.duration.ms/1000/60/60) AS uptimeInHours,\r\n    FLOOR(system.uptime.duration.ms/1000/60) AS uptimeInMins\r\n  FROM \"metricbeat-*\"\r\n  WHERE event.dataset = 'system.uptime'\r\n  AND host.name = 'yourhostname'\r\n  ORDER BY \"@timestamp\" DESC\r\n  LIMIT 1\r\n)\nMarkdown for displaying title, version & uptime\n# Infohub\r\n \r\n{{#each rows}}\r\n#### {{version}}\r\n#### Up: {{uptime}}\r\n{{/each}}\r\n[See More](http://link-to-a-detailed-dashoard)\nEnjoy !\nPS: i also wrote a 3 part blog post about\u00a0MDC logging with Camel, Spring Boot & ELK. Make sure to check it out!\n", "tags": [], "categories": ["Blog", "Development"]}
{"post_id": 26285, "title": "MDC Logging with Camel, Spring boot & ELK (part 3)", "url": "https://www.luminis.eu/blog-en/mdc-logging-with-camel-spring-boot-elk-part-3/", "updated_at": "2023-01-31T11:51:27", "body": "This blog is part 3 of a 3-part blog series about Apache Camel, ELK, and (MDC) logging.\nPart 1\u00a0described how you can centralize the logging from Spring Boot / Camel apps into Elasticsearch using MDC and filebeat. In\u00a0part 2\u00a0we aggregated the logging from part 1 with help of logstash into a separate elasticsearch index, grouping messages and making it a bit more readable for everyone.\nThis 3rd part will use the aggregated logging from part 2 to create watches/alerts to notify you about errors or warnings (use case 1) and when some entry is missing in the log you expected to be there\u00a0(use case 2)\nWatcher\nNow that we got a nicely aggregated index, it is also very easy to create alerts on it. For example \u201cemail me when an error occurs in the logging\u201d, or \u201cemail me when the ECB exchange rate didn\u2019t come in on time\u201d. For this, we can use Watcher. As the\u00a0Elastic website\u00a0says: Watcher is an Elasticsearch feature that you can use to create actions based on conditions, which are periodically evaluated using queries on your data. Note that this a paid feature, available starting from the Gold subscription: see\u00a0https://www.elastic.co/subscriptions. You can however request a trial to try it out for 30 days.\nWatch definition\nA watch consists of a\u00a0trigger,\u00a0input,\u00a0condition< and\u00a0actions. The actions define what needs to be done once the condition is met. In addition, you can define\u00a0conditions\u00a0and\u00a0transforms to process and prepare the watch payload before executing the actions.\n\nTrigger: Determines when the watch is checked. A watch must have a trigger.\nInput: Loads data into the watch payload. If no input is specified, an empty payload is loaded.\nCondition: Controls whether the watch actions are executed. If no condition is specified, the condition defaults to\u00a0always.\nTransform: Processes the watch payload to prepare it for the watch actions. You can define transforms at the watch level or define action-specific transforms. Optional.\nActions: Specify what happens when the watch condition is met, e.g. mail, send to pager, slack, jira, etc\n\nYou can create or edit a watch from the Management / Watcher (under Elasticsearch) menu in Kibana or create one via the Dev tools menu calling the Watcher API\u2019s.\nFor example: create or update a watch with id \u2018acme-watch-banks-ecb\u2019 in the dev console:\nPUT _watcher/watch/acme-watch-banks-ecb\r\n{\r\n  \"trigger\": {...},\r\n  \"input\": {...},\r\n  \"condition\": {...},\r\n  \"transform\": {...},\r\n  \"actions\": {...}\r\n}\nOne powerful feature of the watch API is that you can test your watch right away, without waiting for the specified trigger to occur. For example, to test the above watch just do this:\nPOST _watcher/watch/acme-watch-banks-ecb/_execute\nUse case 1: Log and mail on WARNING / ERROR in the last hour\nWe can search the aggregated index for the Status field to indicate an error or warning. Full example:\n{\r\n  \"trigger\": {\r\n    \"schedule\": {\r\n      \"interval\": \"5m\"\r\n    }\r\n  },\r\n  \"input\": {\r\n    \"search\": {\r\n      \"request\": {\r\n        \"search_type\": \"query_then_fetch\",\r\n        \"indices\": [\"aggr*\"],\r\n        \"rest_total_hits_as_int\": true,\r\n        \"body\": {\r\n          \"query\": {\r\n            \"bool\": {\r\n              \"should\": [{\r\n                  \"match\": {\"Status.keyword\": \"UNKNOWN\"}\r\n                },{\r\n                  \"match\": {\"Status.keyword\": \"ERROR\"}\r\n              }],\r\n              \"minimum_should_match\": 1,\r\n              \"filter\": [{\r\n                  \"range\": {\r\n                    \"@timestamp\": {\"from\": \"now-1h\",\"to\": \"now\"}\r\n                    }\r\n              }]\r\n  }}}}}},\r\n  \"condition\": {\r\n    \"compare\": {\r\n      \"ctx.payload.hits.total\": {\r\n        \"gt\": 0\r\n      }\r\n    }\r\n  },\r\n  \"actions\": {\r\n    \"log\": {\r\n      \"logging\": {\r\n        \"level\": \"warn\",\r\n        \"text\": \"Warning or errors detected: {{ctx.payload.hits.total}}\"\r\n      }\r\n    },\r\n    \"email_it\": {\r\n      \"email\": {\r\n        \"profile\": \"standard\",\r\n        \"priority\": \"high\",\r\n        \"to\": [\"alerts@acme.com\"],\r\n        \"subject\": \"There are {{ctx.payload.hits.total}}  UNKNOWNs / ERRORs detected in last 1 hour!\",\r\n       }   \r\n    },\r\n   \"throttle_period_in_millis\": 3600000\r\n  }\r\n}\nExplanation:\n\nLine 2-5: Trigger: we look every minutes to see if an error or warning appeared in the logging\nLine 7-27: Search for ERROR or WARNING status in the last hour\nLine 28-34: Perform the action if we find any matches (> 0)\nLine 36-41: log the total number of errors & warnings in the log file\nLine 42-49: mail it also\nLine 50: don\u2019t do it more than once an hour (to prevent mail flood)\n\nUse case 2: Mail on missing log entry\nWe like to check if our ECB rate did come in every weekday. Complete example:\nPUT _watcher/watch/acme-watch-banks-ecb\r\n{\r\n  \"trigger\": {\r\n    \"schedule\": {\r\n      \"cron\": \"0 15 14 ? * MON-FRI\"\r\n    }\r\n  },\r\n  \"input\": {\r\n    \"search\": {\r\n      \"request\": {\r\n        \"search_type\": \"query_then_fetch\",\r\n        \"indices\": [\"aggregated*\"],\r\n        \"rest_total_hits_as_int\": true,\r\n        \"body\": {\r\n          \"query\": {\r\n            \"bool\": {\r\n              must\": [{\r\n                 \"match\": {\"Status.keyword\": \"COMPLETED\"}\r\n                }, {\r\n                  \"match\": {\"Scenario.keyword\": \"acme-banks-ecb\"}\r\n              }],\r\n              \"filter\": [{\r\n                  \"range\": {\r\n                    \"@timestamp\": {\r\n                      \"from\": \"now-30m\", \"to\": \"now\"\r\n                    }\r\n                  }\r\n              }]\r\n     } } } } }\r\n  },\r\n  \"condition\": {\r\n    \"compare\": {\r\n      \"ctx.payload.hits.total\": {\"not_eq\": 1}\r\n    }\r\n  },\r\n  \"actions\": {\r\n    \"email_it\": {\r\n      \"email\": {\r\n        \"to\": [\"alerts@acme.com\"],\r\n        \"subject\": \"ERROR: Acme @ TEST: did not receive ECB rates in time\",\r\n        \"body\": {\r\n          \"text\": \"ECB rates should be received by 16:05 CET / 14:05 UTC every day\"\r\n        }\r\n  } } }\r\n}\nExplanation\n\nLine 3-6: the watch will run Monday to Friday at 14:15. Note: the rates are triggered to be fetched at 14:00 or so, so by 14:15 the rates should be there in the aggregated logging.\nLine 8-30: We\u2019re searching in the aggregated index for STATUS=COMPLETED of the scenario \u2018acme-banks-ecb\u2019. We\u2019re looking back 30 minutes just to be sure we don\u2019t miss it.\nLine 31-35: if we don\u2019t find anything (#hits not equal to 1) then a mail will be send\nLine 36-44: mail the error to alerts@acme.com\n\nFor the mail action to work, you need to specify the SMTP credentials in /etc/elasticsearch/elasticsearch.yml\nWe\u2019re using Outlook, so it looks something like this:\nxpack.notification.email.account:\r\n    exchange_account:\r\n        profile: outlook\r\n        email_defaults:\r\n           from: 'The Watcher <alerts@acme.com>'\r\n        smtp:\r\n            auth: true\r\n            starttls.enable: false\r\n            host: smtp.acme.dummy.com\r\n            user: acme-mail-user\nNow, when the ECB rate did not came in on time, we will get an email of it.\nThere are a lot more things you can do with the watcher API; like aggregating on the Status field creating a count per different status, using other actions like sending a SMS or sending a text to a Slack channel.\nThat was it! Hope you find it usefull. Keep on searchin !\nBe sure to also checkout my blog post about\u00a0Health dashboard with Kibana Canvas!\nFurther reading\n\nhttps://www.elastic.co/guide/en/elasticsearch/reference/current/actions-email.html#configuring-email\nhttps://www.elastic.co/guide/en/elasticsearch/reference/current/actions-email.html\n\n", "tags": [], "categories": ["Blog", "Development"]}
{"post_id": 26282, "title": "MDC Logging with Camel, Spring boot & ELK (part 2)", "url": "https://www.luminis.eu/blog-en/mdc-logging-with-camel-spring-boot-elk-part-2/", "updated_at": "2021-08-10T16:25:08", "body": "This blog is part 2 of a 3-part blog series about Apache Camel, ELK, and (MDC) logging.\nPart 1\u00a0described how you can centralize the logging from Spring Boot / Camel apps into Elasticsearch using MDC and filebeat. In\u00a0part 2\u00a0we will aggregate the logging from part 1 with help of logstash into a separate elasticsearch index, grouping messages and making it a bit more readable for managers \ud83d\ude09\u00a0Part 3\u00a0will use the aggregated logging from part 2 to create watches/alerts to notify you about errors, warnings, etc.\nYes yes, very nice this centralized logging, but now I see 20+ messages in Kibana of my integration scenario.\nThat\u2019s too much and too technical for me! I just want one line saying \u2018Did it succeed or not and where was it delivered\u2019\nOur IT manager\nSo now all the logging of our apps is centralized in elasticsearch (see\u00a0Part 1) we can do all sorts of nice things with it. Search it (duh!), create dashboards, but also: keep our manager and functional support people happy by creating some sort of \u2018summary\u2019 of all logs of a certain integration scenario.\nAs an example, consider a simple integration scenario \u201cAt the beginning of each month, download exchange rates from the Myanmar bank and put them on a file share / queue / whatever\u201d. This scenario can contain multiple steps like:\n\nA quartz timer that fires when it\u2019s time to fetch the rates\nDownload the rates from the Myanmar website\nTransform the raw format (JSON, HTML, etc) to our internal format (CDM: canonical data model)\nSometimes: aggregate this with another source, enrich the data, split into multiple messages, etc\nPut the CDM message on the file share/queue / whatever\n\nAll this is nicely logged into elasticsearch, grouped together on camel\u2019s breadcrumbId but as you can see the number of messages of this and more complex integration scenario\u2019s can quickly become overwhelming.\n\nShowing 2 of the 28 log lines of our Myanmar exchange rate download route\n\nYou can use the power of the Elasticsearch API\u2019s to do a complex search, with filters, fuzzy matching, some aggregation, etc but this all seemed too complicated for my taste.\nWhy not creating a 2nd index in elasticsearch containing the \u2018summary\u2019, leaving the original index intact of course.\nEnter: logstash\u00a0(aggregation filter)\nRemember, the setup from part 1? Now we actually gonna use logstash to do the aggregation for us.\n\nWe use logstash to aggregate the messages of one breadcrumbId\n\nFirst, we have to tell filebeat to send it\u2019s output to logstash instead of elasticsearch.\nfilebeat.inputs:\r\n  \r\n- type: log\r\n  \r\n  enabled: true\r\n  \r\n  # Paths that should be crawled and fetched. Glob based paths.\r\n  paths:\r\n    - /opt/acme/app1/log/filebeat/*.log\r\n    - /opt/acme/app2/log/filebeat/*.log    \r\n  \r\n  # json: make it possible for Filebeat to decode logs structured as JSON messages.\r\n  json:\r\n    keys_under_root: true\r\n  \r\n  \r\n#output.elasticsearch:\r\n#   hosts: [\"11.22.33.44:9200\"]\r\n  \r\noutput.logstash:\r\n  hosts: [\"11.22.33.44:5044\"]\nLogstash can be configured in so-called \u2018pipelines\u2019. A pipeline is a simple (Ruby-like) description of the input, an optional filter, and the output. The 3 sections each can make use of the many plugins available. See for example the\u00a0input plugins,\u00a0filter plugins, and\u00a0output plugins. We\u2019re focussing on the\u00a0aggregate filter plugin. The pipeline can be dropped in the /etc/logstash/conf.d folder (assuming you are on an *nix box)\nThis is the complete pipeline for our aggregation. Explanation below!\ninput {\r\n    beats {\r\n        port => 5044\r\n    }\r\n}\r\n \r\nfilter {\r\n    # The following line will create 1 additional copy of each document\r\n    # Each copy will automatically have a \"type\" field added corresponding to the name given in the array.\r\n    clone {\r\n        clones => [ 'clone_for_aggregation' ]\r\n    }\r\n \r\n    if [type] == 'clone_for_aggregation' {\r\n        aggregate {\r\n            task_id => \"%{camel.breadcrumbId}\"\r\n            code => \"\r\n                map['Name'] ||= event.get('cis-name')\r\n                map['SourceSystem'] ||= event.get('cis_log_SourceSystem')\r\n                map['SourceSystemReference'] ||= event.get('cis_log_SourceSystemReference')\r\n                map['Scenario'] ||= event.get('cis_log_Scenario')\r\n                map['TargetSystem'] ||= event.get('cis_log_TargetSystem')\r\n                map['TargetSystemReference'] ||= event.get('cis_log_TargetSystemReference')\r\n                map['Status'] ||= 'UNKNOWN'\r\n                if event.get('cis_log_status') =~ /COMPLETED/\r\n                    map['Status'] = 'COMPLETED'\r\n                elsif event.get('cis_log_status') =~ /ERROR/\r\n                    map['Status'] = 'ERROR'\r\n                end\r\n                \"\r\n            push_map_as_event_on_timeout => true\r\n            timeout_task_id_field => \"camel.breadcrumbId\"\r\n            timeout => 120 # 2 minutes\r\n            timeout_code => \"event.set('[@metadata][type]', 'aggregated_event')\"\r\n        }\r\n    }\r\n}\r\n \r\noutput {\r\n    if [@metadata][type] == 'aggregated_event' {\r\n        elasticsearch {\r\n            hosts => [\"http://localhost:9200\"]\r\n            index => \"aggregated-filebeat-%{+YYYY.MM}\"\r\n        }\r\n    } else if [type] != 'clone_for_aggregation' {\r\n        elasticsearch {\r\n            hosts => [\"http://localhost:9200\"]\r\n            index => \"filebeat-%{+YYYY.MM}\"\r\n        }\r\n    }\r\n}\nExplanation:\n\nLine 1-5: tells logstash to listen on port 5044, for events send by filebeat\nLine 10-12: I want the original event to go to elasticsearch just as before. I make a clone of the event with the clone plugin. This will copy the even and the cloned copy will have a type field with the value \u2018clone_for_aggregation\u2019. This helps us later to discriminate between type of events.\nLine 14: only apply it to the cloned event.\nLine 15-16: use the aggregate plugin, and aggregate on the field \u2018camel.breadcrumbId\u2019. Effectively saying: group all messages with the same breadcrumbId together.\nLine 17-30: Ruby-ish code that runs for every event. We map the fields from the input to friendlier field names. For example, Line 18 says: get the cis-name value and put in into a new field called \u2018Name\u2019. Because of the \u2018||=\u2019 syntax, it only happens when Name is not already initialized.\nLine 24: initialize the Status to \u2018UNKNOWN\u2019. It could happen that, because of an exception/network problems/ etc, the final message never comes in in time. Than the Status stays \u2018UNKNOWN\u2019\nLine 31-34: when the timeout occurs (2 minutes) the new event is pushed. This is done because we could never be sure the last success or error message is received.\nLine 39-51: the aggregated event will be send to another index as the original event.\n\n\u00a0\nAfter a minute or 2 the aggregated events will start to trickle through. You will now see a much cleaner log, one-line per scenario like this:\n\nOne line per scenario. ECB FXRATE needs some extra attention!\n\nPro tip:\u00a0also see the article of my Luminis colleague Jettro Coenradie on how to create a robust Logstash configuration:\u00a0https://sharing.luminis.eu/blog/robust-logstash-configuration/\n\u00a0\nNice he!? In the last part of this blog series we will use this aggregated logging to set some alarms. Proceed to\u00a0part 3\nFurther reading\n\nhttps://www.elastic.co/logstash\nhttps://www.elastic.co/guide/en/logstash/current/filter-plugins.html\nhttps://www.elastic.co/guide/en/logstash/current/plugins-filters-aggregate.html\n\n", "tags": [], "categories": ["Blog", "Development"]}
{"post_id": 24350, "title": "MDC logging with Camel, Spring Boot & ELK", "url": "https://www.luminis.eu/blog-en/mdc-logging-with-camel-spring-boot-elk/", "updated_at": "2021-08-10T16:23:31", "body": "\n\nThis blog is part 1 of a 3-part blog series about Apache Camel, ELK, and (MDC) logging.\nPart 1\u00a0describes how you can centralize the logging from Spring Boot / Camel apps into Elasticsearch using MDC and filebeat. In\u00a0part 2\u00a0we will aggregate the logging from part 1 with help of logstash into a separate elasticsearch index, grouping messages and making it a bit more readable for managers \ud83d\ude09\u00a0Part 3\u00a0will use the aggregated logging from part 2 to create watches/alerts to notify you about errors, warnings, etc.\nLog files galore\nFor a large customer based in Rotterdam, we\u2019re building a new platform that will handle all the customer\u2019s integration needs. The platform is based on\u00a0Spring Boot\u00a0and we use\u00a0Apache Camel\u00a0as our integration Swiss army knife. Actually, there are several Spring Boot applications running at any given time each using SLF4j to log to files.\nThe #1 question we get asked is:\n\u201cWhat happened to my message XYZ? Is it already in system ABC or is it stuck somewhere ?\u201d\nunsure client\n\n\nTo keep track of the messages flowing through the systems, we like to store all the log data into Elasicsearch. This can be done relatively easily by using\u00a0Filebeat. Filebeat is a lightweight shipper for log and other data. To make it easy for ourselves (yes I\u2019m lazy, I know) we use a special encoder in Spring\u2019s log configuration. The net.logstash.logback.encoder.LogstashEncoder will output the log data as JSON, so we don\u2019t need any further (GROK) parsing to dissect the log message into separate fields for example. The basic flow will look like this:\n\nThe BLEK flow (logstash is optional; we use it in part 2)\n\n\u00a0\nLogback-spring.xml\nYou can use SLF4J / Log4j for logging. In your Spring app you can configure the logging format in an xml file. Default it\u2019s named logback-spring.xml\n<?xml version=\"1.0\" encoding=\"UTF-8\"?>\r\n<configuration scan=\"true\" scanPeriod=\"30 seconds\">\r\n \r\n    <appender name=\"Console\" class=\"ch.qos.logback.core.ConsoleAppender\">\r\n        <layout class=\"ch.qos.logback.classic.PatternLayout\">\r\n            <Pattern>\r\n                %black(%d{ISO8601}) %highlight(%-5level) [%t][ %blue(%X{camel.breadcrumbId}) ] | %yellow(%C{1.}): %msg%n%throwable\r\n            </Pattern>\r\n        </layout>\r\n    </appender>    \r\n \r\n   <appender name=\"FilebeatAppender\" class=\"ch.qos.logback.core.rolling.RollingFileAppender\">\r\n        <file>${LOG_PATH}/filebeat/normal-${LOG_FILE}</file>\r\n        <encoder class=\"net.logstash.logback.encoder.LogstashEncoder\">\r\n            <customFields>{\"app-name\":\"app1\"}</customFields>\r\n        </encoder>\r\n        <rollingPolicy class=\"ch.qos.logback.core.rolling.SizeAndTimeBasedRollingPolicy\">\r\n            <fileNamePattern>${LOG_PATH}/archived/normal-${LOG_FILE}-%d{yyyy-MM-dd}.%i.gz</fileNamePattern>\r\n            <maxFileSize>100MB</maxFileSize>\r\n            <maxHistory>10</maxHistory>\r\n            <totalSizeCap>1000MB</totalSizeCap>\r\n        </rollingPolicy>\r\n    </appender>\r\n \r\n    <root level=\"info\">\r\n        <appender-ref ref=\"FilebeatAppender\"/>\r\n        <appender-ref ref=\"Console\"/>\r\n    </root>\r\n \r\n</configuration>\r\n\nThe example above will log to console and to a json file in the specified folder. We also added a custom field \u2018app-name\u2019 to later distinguish which app this log came from. The log file will look like this (shortened for readability; and after applying some MDC magic i will discuss later on)\n{\"@timestamp\":\"2020-05-24T00:00:00.000Z\",\"@version\":\"1\",\"message\":\"Route started\",\"camel.breadcrumbId\":\"ID-xyz-1590162977230-0-33641\",\"app-name\":\"app1\", \"mdc_field1\":\"test\", ...}\r\n\nFilebeat\nFilebeat supports a lot of logging \u2018modules\u2019 like apache httpd, kafka, activemq, etc. There was no module for our situation, so we added our log paths to the main filebeat.yml file. This will do its work just fine.\nfilebeat.inputs:\r\n \r\n- type: log\r\n \r\n  enabled: true\r\n \r\n  # Paths that should be crawled and fetched. Glob based paths.\r\n  paths:\r\n    - /opt/acme/app1/log/filebeat/*.log\r\n    - /opt/acme/app2/log/filebeat/*.log    \r\n \r\n  # json: make it possible for Filebeat to decode logs structured as JSON messages.\r\n  json:\r\n    keys_under_root: true\r\n \r\n \r\noutput.elasticsearch:\r\n   hosts: [\"11.22.33.44:9200\"]\r\n \r\n# output.logstash:\r\n#   hosts: [\"11.22.33.44:5044\"]\nAs you can see, multiple logfiles are parsed and ingested directly into Elasticsearch. You can also choose to send it to Logstash for additional filtering / aggregation (as we will do in part 2 of this blog serie).\nMDC\nYou could be all done here without ever using MDC, but then all your logging / useful variables will end up in one big pile in the \u2018message\u2019 field of your json log message AND you will need to add everything in every .log() statement every time you want to log. Pff, exhausting and error prone right?\nEnter:\u00a0Mapped Diagnostic Contexts\nMDC primarily stores a key-value map of contextual data (strings mapping to\u00a0strings), and the context map key can be added to your logging format (logging.properties, or other logging configuration), like in the following example\u00a0(\u2018orderId\u2019\u00a0and camel.breadcrumbId are context keys):\n%d{HH:mm:ss,SSS} %-5p [%c] %X{camel.breadcrumbId} | %X{orderId} | (%t) %s%E%n\nNow every log line will have the camel.breadcrumbId and orderId in it!. Note: if you want the camel variables in your logging, you have to enable this in your Spring boot app config like so:\nMDC.put(\"orderId\", \"123456789\");\nAnd the actual UOW can extend from the Camel provided MDCUnitOfWork class:\ncamel.springboot.use-mdc-logging = true\nNow, if Spring can find your MyLogConfig class, then every camel route you create will use your (MDC) Unit of work class. Nice he!?\nProceed to\u00a0part 2\nFurther reading\n\nhttps://cwiki.apache.org/confluence/display/CAMEL/MDC+logging\n\n", "tags": [], "categories": ["Blog", "Development"]}
{"post_id": 26261, "title": "Speed up Angular unit test duration", "url": "https://www.luminis.eu/blog-en/development-en/speed-up-angular-unit-test-duration/", "updated_at": "2021-08-10T16:23:25", "body": "Unit tests are the first line of defense for developers when working on a large application. With a large application, comes great responsibility. The number of unit tests can quickly exploded into hundreds, or even thousands. These tests need to run before each build. It can become a real pain when the unit test duration becomes high.\nThere is a flaw in Angular TestBed, which makes it even more painful. So let me enlighten you, and point out the issue I\u2019ve addressed and fixed for some projects I\u2019ve been working on, which reduced the duration of running unit test by threefold.\nComponent style references\nDuring a unit test, when importing and compiling a component, styles are added to the head section of the iframe where the test is running. The problem is that the styles after the test aren\u2019t removed from the head. Since the head will always load before the DOM is ready, it will slow down the spec runner. This isn\u2019t that big of a deal when there aren\u2019t a lot of tests. But imagine when there are 200 components.\nImprove unit test duration\nInstead of imagining, I\u2019ve created an empty project using the Angular CLI tool, and copied the spec file of the app component 200 times. Each spec file contains 3 tests, so it\u2019s a total of 600. When running ng test, the duration of running all tests takes up to +/- 20 seconds. Not too bad, 20 seconds isn\u2019t that much. But when you clear the style sheet after each test, it can speed up almost 3 times!\n\nWhen adding the following code to your test.ts file within the src directory, every injected style rule will be removed after each describe block.\njasmine.getEnv().afterEach(() => cleanStylesFromDOM());\r\n\r\nconst cleanStylesFromDOM = () => {\r\n\u00a0 const head = document.getElementsByTagName('head')[0];\r\n\u00a0 const styles = head.getElementsByTagName('style');\r\n\r\n\u00a0 Array\r\n\u00a0\u00a0\u00a0 .from(styles)\r\n\u00a0\u00a0\u00a0 .forEach(style => head.removeChild(style));\r\n};\r\n\nThis is the result after running the same set again, with clearing the styles after each test:\n\nThe duration of the spec runner went from 20.682 tot 7.884 seconds!\nCurrent state\nThis issue has been reported to the Angular team in July 2019. But till today a fix hasn\u2019t been merged. In the description of this issue, the real problem is addressed.\nAngular appends inline styles associated with the component via the\u00a0SharedStylesHost\u00a0service. These inline styles are cleaned up when\u00a0the service is destroyed.\nUnfortunately, these inline styles are never cleaned up in Karma unit tests. This appears to be because each SharedStylesHost service is never destroyed, apparently because each associated NgModule is never destroyed between specs.\nUpdate\nIf force cleaning all the styles in the head section is causing problems, you could try to only remove styles added by the shared styles host service. In order to do so, the service needs to be destroyed after each test.\njasmine.getEnv().afterEach(() => getTestBed().inject(\u0275DomSharedStylesHost).ngOnDestroy());\nEnjoy all the spare time you and your team gain from this!\n", "tags": [], "categories": ["Blog", "Development"]}
{"post_id": 25724, "title": "How we configure our Azure DevOps build and release pipelines", "url": "https://www.luminis.eu/blog-en/how-we-configure-our-azure-devops-build-and-release-pipeline/", "updated_at": "2021-08-25T08:04:36", "body": "When you build software for the cloud, you want to release often. This is why you want to configure a good Azure DevOps pipeline.\nAzure DevOps provides multiple starter templates for configuring your build and release pipelines, but most of the times those do not completely satisfy your needs. Especially when you need to deploy to a DTAP environment. I will show a fully functional template step-by-step, based on the bare minimal \u2018Starter Template\u2019 that Azure DevOps provides. At the end of this blogpost, you will find a link to the sample templates.\nThe pipeline is stored in git\nPreviously the build and release pipelines lived in Azure DevOps and were not linked to any version of the software, which gave problems when you needed to build or release an old version of the code. Fortunately this is changed! The pipeline can now be stored in git to enable it to grow with development.\nI always create two folders in the root of my repo:\n\n.azure: in this folder, the ARM template and other files related to it are stored.\n.azuredevops: this folder contains the build and release pipeline.\n\nThe pipeline we use to build and release\nThe picture below shows the changes I made to the starter template. Below the picture I describe and explain the changes.\nTo make the pipeline short and clear, I hardcoded variables and removed the build and test-parts. Normally we use variables to support our naming conventions. But that can be a blogpost on its own.\n\n\u00a0\nStep 1: Introduce stages and jobs\nIn the first block, I wrapped the normal build template in a Build Stage. That allows me to add release stages later on.\nStep 2: Add ARM template validation to the build\nI don\u2019t want surprises while releasing the Azure, so I validate the ARM template during the build. To validate the ARM template, set the deployment-mode to Validation. When variables need to be passed to the template, you can pass \u2018dummy\u2019 values to the override parameters.\nAfter the validation is finished, the template is copied to the artifact staging directory.\nOne thing to note here is the azure subscription (here: azureSubscription: \u201c<spn4Test>\u201d). This is the name of the service connection as configured in Azure DevOps under Project Settings -> Pipelines -> Service Connections and serves as the credentials used to release to Azure.\nStep 3: Add the release stages\nWe can now add the first deployment stage at the end of the file. As the release stages will only differ in variables, we will introduce a deploy template for the release. I will describe the template next.\nThe deployment-stage contains the following elements that require some explanation\n\ncondition: only when this condition is true, the deployment is executed. So if the build is triggered by a pull request, the release is not executed since the branch-name does not match \u2018master\u2019.\ndependsOn: defines the preceding stage. If the condition states succeeded, that stage should have a success-outcome.\nvariables: you can link here a variable group defined in the Azure DevOps library. These variables can be referenced as $(variable-name), either here or in the deploy-template.\n\nStep 4: Add deploy.yml to git\nI have provided a very simple deploy.yml, but it can get as complex as you need. I first create, based on the values of the parameters, some variables that I can use later on.\nVariables in the pipeline require some special attention:\n\nVariables defined in azure-pipelines.yml are available here. That can be confusing. Especially when they have the same name as the parameters.\nDon\u2019t mess up with parameters and variables:\n\nParameters are referenced with ${{parameters.parameter-name}}\nVariables are referenced as $(variable-name)\n\n\n\nWhen the environment is set in the release stage, you can configure release approval. More details below in step 5.\n\nStep 5: Configure release approvals\nWhen you configure an Azure DevOps build and release pipeline that deploys to a production environment you want to add some gates. To accomplish this, I set the environment in the pipeline\u00a0(here at line 20: environment: \u201c${{parameters.environmentLong}}\u201d).\nAfter the first run, the environment is created automatically in Azure DevOps.\nYou can also manually create in Azure DevOps. Go to the Environments tab under Pipelines in Azure DevOps and create a new Environment.\nThe approvals can be set once the Environment is created. Go to the Environments tab under Pipelines in Azure DevOps, select the environment you just created. Press the dots, select \u2018Approvals and checks\u2019 and select \u2018Approvals\u2019.\n\n\nConfigure the approval\n\nI have now configured a nice Azure DevOps build and release pipeline that can grow with my development. By storing the pipeline in git, I have full version control on the pipeline and can introduce changes to it at the moment they are needed.\nThese templates can be found on GitHub.\n", "tags": [], "categories": ["Blog", "Cloud", "Development"]}
{"post_id": 25590, "title": "Motorsport Games acquires Luminis subsidiary Studio 397", "url": "https://www.luminis.eu/blog-en/news-en/motorsport-games-acquires-luminis-subsidiary-studio-397/", "updated_at": "2021-12-17T12:32:14", "body": "Amersfoort, The Netherlands, March 3, 2021. Luminis signed an agreement this week with Motorsport Games Inc. focused on the full acquisition of Studio 397, including high-end racing simulation software rFactor 2. The acquisition enables both companies to further expand their strong position in esports through the use of virtual racing events and competitions based on the rFactor 2 platform. For Luminis this means a new phase for its subsidiary, which was started by its own employees. The Studio 397 and rFactor 2 brands will continue to exist.\nIn 2020, Studio 397 and Motorsport Games organized the largest esports event in the world: the virtual 24 hours of Le Mans. At this globally televised event, watched by 63 million people, Motorsport saw the power of Studio 397 and rFactor 2. As a publicly traded publisher of sim racing events, games and publications, Motorsport Games saw opportunities for Studio 397 and rFactor 2 to offer viewers and users an even better experience. Motorsport Games intends to further invest in the rFactor 2 platform and use rFactor 2 as the standard for all new racing games and publications.\n\nStudio 397 was started in 2016 by Marcel Offermans, who has worked since the start of Luminis in various roles such as architect and director of Luminis Technologies. From a strong passion for sim racing, Marcel started a collaboration with Image Space Incorperated (ISI) in 2016. rFactor 2 is a high-end racing simulator, known worldwide for its realism in terms of tire dynamics, aerodynamics, weather, sound and the detailed race tracks. This ensures a realistic racing experience and challenges the drivers to perform better.\nIn recent years, the development of rFactor 2 has accelerated. The racing sim has been expanded with new features, cars, tracks, competition options and a new interface. This made the use of rFactor 2 an unavoidable option for professional esports leagues and competitions. Many well-known (former) Formula 1 drivers use rFactor 2 for training and for participating in esports competitions. Studio 397 worked closely with well-known car brands such as McLaren, Porsche and BMW. This is how Studio 397 built up a strong name in the racing sim world in a short time.\nMarcel Offermans, Managing Director of Studio 397, is looking positively at the development: \u201cFollowing the growth of rFactor 2 over the past five years, we are delighted to take the next step with Motorsport Games, driving rFactor 2 forward and our advanced simulation technology. to be integrated in future projects. We share a common ambition with Motorsport Games to be the best at what we do. \u201d\nHans Bossenbroek, CEO and co-founder of Luminis, has mixed feelings: \u201cStudio 397 has always had a special place with me. I always proudly said that Luminis collaborated from Studio 397 with companies such as McLaren, Porsche and BMW. For me, Studio 397 is about innovation, about getting the most out of technology. Despite their limited size, their talent and drive enabled them to conquer the hearts of many motorsport enthusiasts. Studio 397 is a confirmation for me that stimulating passion and entrepreneurship leads to wonderful independent companies that can grow and mature under the Luminis banner. Unfortunately, the moment will come when they can grow better at another organization. Despite the fact that this is the best for all parties, it feels double to me. \u201d\n\u201cThis planned acquisition is another clear sign of our intention to establish Motorsport Games as the leader in virtual racing,\u201d said Dmitry Kozko, CEO of Motorsport Games. We can bring our experience and knowledge to maximize the potential of the rFactor 2 platform, while also having exclusive access to the very best technology for our future projects. \u201d\nAbout Motorsport Games:\nMotorsport Games, a Motorsport Network company, combines innovative and engaging video games with exciting esports competitions and content for racing fans and gamers around the globe. The Company is the officially licensed video game developer and publisher for iconic motorsport racing series including NASCAR, 24 Hours of Le Mans and the British Touring Car Championship (\u201cBTCC\u201d). Motorsport Games is an award-winning esports partner of choice for 24 Hours of Le Mans, Formula E, BTCC, the FIA World Rallycross Championship and the eNASCAR Heat Pro League among others.\nFor more information about Motorsport Games visit: www.motorsportgames.com\nFor more information\nOfficial Motorsport Games press release: https://motorsportgames.com/motorsport-games-enters-into-binding-term-sheet-to-acquire-developer-studio397-and-its-rfactor-2-simulation-platform/\nStudio 397 news update: https://www.studio-397.com/2021/03/announcement-motorsport-games/\nFor questions and press contact: Martin van Mierloo \u2013 martin.vanmierloo@luminis.eu +31614841519\n", "tags": [], "categories": ["Blog", "News"]}
{"post_id": 25563, "title": "Deploying a secure AWS Elasticsearch cluster using cdk", "url": "https://www.luminis.eu/blog-en/cloud-en/deploying-a-secure-aws-elasticsearch-cluster-using-cdk/", "updated_at": "2023-04-17T10:55:43", "body": "If there is one thing that I have seen a lot with elasticsearch clusters in the wild, it is problems with security. The out of the box Elasticsearch installation does not have a user_id password protection like most databases do. Using the basic license these days does give you the option, but you still have to configure it yourself. When AWS introduced their AWS Elasticsearch service, it did not configure security out of the box. Therefore Elasticsearch clusters have been famous for exposed data. Check an article like this, but there are lots more. Does this mean Elasticsearch should not be used? No, but you must know what you are doing.\nWhen in need of an elasticsearch cluster, you can do everything yourself. You can read more about the security options here. Another approach would be to use a cloud service. As the title mentions, in this blog, I will discuss how you can create a secure cluster with Amazon AWS. When creating a cluster, the default approach from AWS is to allow VPC access; when exposing your cluster to the public, you should at least limit usage through certain IP Addresses or users. AWS uses Open Distro, which comes with fine-grained access control. There is an option to secure certain indexes, documents and even fields. Roles within Open Distro can be combined with back end roles coming from IAM. Using Cognito, users and user groups can be managed from AWS.\nCreating a fully configured secure AWS Elasticsearch cluster with Kibana using Cognito is doable using the AWS console. A blog post by the AWS team already gives a good overview and an example: \u201cLaunch Amazon Elasticsearch Service \u2026\u201c. Still, I wanted to use the latest and greatest AWS CDK to create an automated deployment. I also want to give you more information about the different components you need to fully secure your AWS Elasticsearch cluster.\nWhat are we creating?\nWe need an Elasticsearch cluster that is exposed through Kibana. By using a username/password combination, the Kibana environment is secured. Users are divided into two groups, admins and limited users. Limited users can still log in to Kibana but have only read access to specific indexes. Admins can do everything they want through Kibana. Besides access through Kibana, we also have a Java application that exposes an API that needs access.\nBelow you can find a schematic overview of the solution:\n\nAs you can see, there is more to it than just a simple installation of elasticsearch. When using the AWS Console, you can configure the same solution as we need; that does not make it repeatable, though. Let us break down into the different components:\n\nBrowser \u2013 This is used to connect to the Kibana GUI to interact with the Elasticsearch cluster data.\nAWS Infrastructure \u2013 This is used to deliver a highly scalable and secure environment for keeping and presenting your data.\nAWS Elasticsearch \u2013 An Elasticsearch (Open Distro) cluster accessible using Kibana. Comes out of the box with lots of features like logging, fine-grained security, backups, upgrades and all, of course, highly scalable.\nAWS Cognito \u2013 This is used to provide authentication of users and authorisation of requests. The user pool comes with an internal user and group collection. The identity pool can make use of the user pool but also of numerous other OpenId providers. Cognito is used to provide role-based authorization to Elasticsearch.\nAWS IAM \u2013 Is used by most components to enable security. It provides roles and service delegates, policies, all to make the security wishes of out components possible.\nAWS Lambda \u2013 These are used to execute calls to elasticsearch to configure open distro.\n\nThe sample code\nYou can find the sample code in this Github repository. The sample is based for the biggest part on a sample provided by AWS. When creating a project for yourself you can use the CDK init mechanism and we need to install a few additional dependencies.\ncdk init app --language typescript\r\nnpm install @aws-cdk/aws-custom-resources\r\nnpm install @aws-cdk/aws-elasticsearch\r\nnpm install @aws-cdk/aws-lambda\r\nnpm install @aws-cdk/aws-cognito\r\nnpm install @aws-cdk/aws-iam\r\n\nThe code repository contains the configuration of the stack. Check the file aws-es-CDK-stack.ts. The first step is configuring the user and the identity pool. The user pool has a lot of options. The sample code shows how to change the templates for the SMS and email messages used to communicate around new accounts. You can also change the logo\u2019s and colours to create login pages that match your corporate style. Do notice that we add the cognitoDomain to the user pool. The identity pool is basic in the beginning. However, we need to add a lot more configuration later when we have the required roles available.\n\r\n    private createUserPool(applicationPrefix: string) {\r\n        const userPool = new UserPool(this, applicationPrefix + 'UserPool', {\r\n            userPoolName: applicationPrefix + ' User Pool',\r\n            userInvitation: {\r\n                emailSubject: 'With this account you can use Kibana',\r\n                emailBody: 'Hello {username}, you have been invited to join our awesome app! Your temporary password is {####}',\r\n                smsMessage: 'Hi {username}, your temporary password for our awesome app is {####}',\r\n            },\r\n            signInAliases: {\r\n                username: true,\r\n                email: true,\r\n            },\r\n            autoVerify: {\r\n                email: true,\r\n            }\r\n        });\r\n\r\n        userPool.addDomain('cognitoDomain', {\r\n            cognitoDomain: {\r\n                domainPrefix: applicationPrefix\r\n            }\r\n        });\r\n        return userPool;\r\n    }\r\n\r\n    private createIdentityPool(applicationPrefix: string) {\r\n        return new CfnIdentityPool(this, applicationPrefix + \"IdentityPool\", {\r\n            allowUnauthenticatedIdentities: false,\r\n            cognitoIdentityProviders: []\r\n        });\r\n    }\nThe next step is creating the 4 roles that we need. We have two different roles for user groups\u2014one for admin users and one for limited users. We also have two different service roles. One used to configure Cognito in the elasticsearch cluster and one to execute lambdas. The function below configures user roles.\u00a0\n\r\n    private createUserRole(idPool: CfnIdentityPool, identifier: string) {\r\n        return new Role(this, identifier, {\r\n            assumedBy: new FederatedPrincipal('cognito-identity.amazonaws.com', {\r\n                \"StringEquals\": {\"cognito-identity.amazonaws.com:aud\": idPool.ref},\r\n                \"ForAnyValue:StringLike\": {\r\n                    \"cognito-identity.amazonaws.com:amr\": \"authenticated\"\r\n                }\r\n            }, \"sts:AssumeRoleWithWebIdentity\")\r\n        });\r\n    }\r\n\nWe create a FederatedPrincipal and use 2 conditions to assign the role to a logged-in user or not. If properties taken from the authenticated token claim match the conditions, the user can assume the role. In this case, we assume a token coming from a Cognito authentication and check that our own identity pool authenticates the user. More info on this topic here. The code for creating a service role is a bit shorter.\n\r\n    private createServiceRole(identifier: string, servicePrincipal: string, policyName: string) {\r\n        return new Role(this, identifier, {\r\n            assumedBy: new ServicePrincipal(servicePrincipal),\r\n            managedPolicies: [ManagedPolicy.fromAwsManagedPolicyName(policyName)]\r\n        });\r\n    }\r\n\nImportant here is the service principal to give access to and the policy to apply. The role to configure Cognito in Elasticsearch needs the policy AmazonESCognitoAccess to interact with the service es.amazonaws.com.\nWe can create our user groups with two user roles\u2014one for admins and one for limited users. The next code block shows how to create the admin user group. Notice that we refer to the user pool, the admin role through its arn. And we give it a name.\n\r\n    private createAdminUserGroup(userPoolId: string, adminUserRoleArn: string) {\r\n        new CfnUserPoolGroup(this, \"userPoolAdminGroupPool\", {\r\n            userPoolId: userPoolId,\r\n            groupName: \"es-admins\",\r\n            roleArn: adminUserRoleArn\r\n        });\r\n    }\r\n\nThe next step is what we have all been waiting for, creating the AWS Elasticsearch domain. There is a lot that comes together when in this step. We need a reference to the identity and user pool to configure Kibana and Cognito connection. We need the service role that is allowed to configure Elasticsearch for Cognito. We need the lambda service role to configure the fine-grained access in elasticsearch. We can configure the number of nodes, dedicated master nodes, sizing of the nodes, enabling logging, and encryption, using Cognito for Kibana and fine-grained access control. In the final bit, we configure that the limited user role and the lambda service role should have HTTP access to the domain.\n\r\n    private createESDomain(domainName: string,\r\n                           idPool: CfnIdentityPool,\r\n                           esServiceRole: Role,\r\n                           esLimitedUserRole: Role,\r\n                           lambdaServiceRole: Role,\r\n                           userPool: UserPool) {\r\n        const domainArn = \"arn:aws:es:\" + this.region + \":\" + this.account + \":domain/\" + domainName + \"/*\"\r\n\r\n        const domain = new es.Domain(this, 'Domain', {\r\n            version: es.ElasticsearchVersion.V7_9,\r\n            domainName: domainName,\r\n            enableVersionUpgrade: true,\r\n            capacity: {\r\n                dataNodes: 1,\r\n                dataNodeInstanceType: \"r5.large.elasticsearch\",\r\n            },\r\n            ebs: {\r\n                volumeSize: 10\r\n            },\r\n            logging: {\r\n                appLogEnabled: false,\r\n                slowSearchLogEnabled: false,\r\n                slowIndexLogEnabled: false,\r\n            },\r\n            nodeToNodeEncryption: true,\r\n            encryptionAtRest: {\r\n                enabled: true\r\n            },\r\n            enforceHttps: true,\r\n            accessPolicies: [new PolicyStatement({\r\n                effect: Effect.ALLOW,\r\n                actions: [\"es:ESHttp*\"],\r\n                principals: [new AnyPrincipal()],\r\n                resources: [domainArn],\r\n            })\r\n            ],\r\n            cognitoKibanaAuth: {\r\n                identityPoolId: idPool.ref,\r\n                role: esServiceRole,\r\n                userPoolId: userPool.userPoolId\r\n            },\r\n            fineGrainedAccessControl: {\r\n                masterUserArn: lambdaServiceRole.roleArn\r\n            }\r\n        });\r\n\r\n\r\n        new ManagedPolicy(this, 'limitedUserPolicy', {\r\n            roles: [esLimitedUserRole, lambdaServiceRole],\r\n            statements: [\r\n                new PolicyStatement({\r\n                    effect: Effect.ALLOW,\r\n                    resources: [domainArn],\r\n                    actions: ['es:ESHttp*']\r\n                })\r\n            ]\r\n        })\r\n\r\n        return domain;\r\n    }\r\n\nThe last bit is configuring Open Distro security. In the sample, I scratch the service of what is possible. But the mechanism should be easy to use to add more rules. The mechanism is to create a lambda that interacts with elasticsearch. The lambda is read from a file added to the repo, which is a shameless copy from the mentioned AWS sample. Notice we supply the lambda with the elasticsearch domain endpoint, the lamda service role and the current region. Then we use a custom resource to use a provider to send messages to. The event handler of the provider calls the lambda with the provided message. The send messages contain an HTTP method, a path and a body. The fine-grained access control starts with roles as well. Beware, these are not the same as the IAM roles we talked about earlier. In the role mapping, we map the fine-grained access control role to an IAM role, called the backend role. The next and final code block shows the lambda, the provider and the custom resource. I did remove some of the requests as the code block would go on and on. Check the git repo for the complete repo.\n\r\n    private executeOpenDistroConfiguration(lambdaServiceRole: Role, esDomain: Domain, esAdminUserRole: Role, esLimitedUserRole: Role) {\r\n        /**\r\n         * Function implementing the requests to Amazon Elasticsearch Service\r\n         * for the custom resource.\r\n         */\r\n        const esRequestsFn = new lambda.Function(this, 'esRequestsFn', {\r\n            runtime: lambda.Runtime.NODEJS_10_X,\r\n            handler: 'es-requests.handler',\r\n            code: lambda.Code.fromAsset(path.join(__dirname, '..', 'functions/es-requests')),\r\n            timeout: Duration.seconds(30),\r\n            role: lambdaServiceRole,\r\n            environment: {\r\n                \"DOMAIN\": esDomain.domainEndpoint,\r\n                \"REGION\": this.region\r\n            }\r\n        });\r\n\r\n        const esRequestProvider = new Provider(this, 'esRequestProvider', {\r\n            onEventHandler: esRequestsFn\r\n        });\r\n\r\n        new CustomResource(this, 'esRequestsResource', {\r\n            serviceToken: esRequestProvider.serviceToken,\r\n            properties: {\r\n                requests: [\r\n                    {\r\n                        \"method\": \"PUT\",\r\n                        \"path\": \"_opendistro/_security/api/roles/kibana_limited_role\",\r\n                        \"body\": {\r\n                            \"cluster_permissions\": [\r\n                                \"cluster_composite_ops\",\r\n                                \"indices_monitor\"\r\n                            ],\r\n                            \"index_permissions\": [{\r\n                                \"index_patterns\": [\r\n                                    \"test*\"\r\n                                ],\r\n                                \"dls\": \"\",\r\n                                \"fls\": [],\r\n                                \"masked_fields\": [],\r\n                                \"allowed_actions\": [\r\n                                    \"read\"\r\n                                ]\r\n                            }],\r\n                            \"tenant_permissions\": [{\r\n                                \"tenant_patterns\": [\r\n                                    \"global\"\r\n                                ],\r\n                                \"allowed_actions\": [\r\n                                    \"kibana_all_read\"\r\n                                ]\r\n                            }]\r\n                        }\r\n                    },\r\n                    {\r\n                        \"method\": \"PUT\",\r\n                        \"path\": \"_opendistro/_security/api/rolesmapping/kibana_limited_role\",\r\n                        \"body\": {\r\n                            \"backend_roles\": [\r\n                                esLimitedUserRole.roleArn\r\n                            ],\r\n                            \"hosts\": [],\r\n                            \"users\": []\r\n                        }\r\n                    }\r\n                ]\r\n            }\r\n        });\r\n    }\r\n\r\n\nOh, what a journey it has been. Finally, time to just run the deploy, create the admin and limited users and start using the elasticsearch cluster.\nIf you have questions, don\u2019t hesitate to contact me. If you don\u2019t want to create and maintain your own cluster, you can contact me as well. If you have a question about the best way to use Elasticsearch, monitor your search performance, learn about returning relevance results, you guessed it, you can contact me as well.\n", "tags": [], "categories": ["Blog", "Cloud", "Search", "Security"]}
{"post_id": 25363, "title": "Let\u2019s talk about the Elastic license change", "url": "https://www.luminis.eu/blog-en/search-en/lets-talk-about-the-elastic-license-change/", "updated_at": "2021-02-12T19:21:20", "body": "Is it a bomb under the open source model, or a genius move to protect their business from pirates?\nOn January 14, 2021 a blog post was published on Elastic\u2019s website, titled: licensing change. In this blog post, they announced a license change in the next release of all their tools (including Elasticsearch and Kibana). The purpose of this change is mainly to block AWS: Elastic wants to make it impossible for AWS to create its own competing managed Elasticsearch service. The announced changes were not very well received in the open source world. In this blog post I tried to give an overview of what happened, and what the consequences are for companies that use Elasticsearch and Kibana. Just a disclaimer, I am not a lawyer. Hence I will not go into the content of the new license model. The aim of this post is mainly to provide an overview of varying opinions on the internet and the possible consequences for users.\nThe Elastic company\nIf you do not know the company Elastic at all, it is the company behind one of the largest search engines called Elasticsearch. The company was founded in Amsterdam, so it is a company from Dutch origin. The company is worth billions on the stock exchange in the U.S. Elasticsearch is their main product, but many other products have emerged around it. Elasticsearch started as a fully open source product, just like some of their other projects such as Kibana, Logstash and Beats which also started as fully open sourced projects.\nThe change in licensing\nElasticsearch started as a full Apache 2.0 project. With the arrival of X-Pack, components were added that followed a closed model. In a blog post in 2018 called Doubling down on open, Elastic announced that it will also make the X-Pack code openly available. Mind you, open, not open source. From that moment on, a dual license was clearly used. Some of the source code was made available under the Apache 2.0 license, another part of the code was made available through the elastic license. The binaries were available through the elastic license.\nThe license model will change with the next release. The Apache\u00a0 2.0 license is being replaced by Server Side Public License (SSPL). The SSPL license was developed by MongoDB, through this license they tried to make it more difficult for AWS to offer Mongo as a service. Where do problems arise for the community? Well problems start with it no longer being an open source license. Why SSPL is not an open source license is clearly described here on the website of the Open Source initiative. The bottom line is that the source code will remain available, but not according to the ideas of the Open Source initiative (OSI). Another look at this case is explained very well in this post. The open source initiative can say that they do not recognize a license, but they have no right to say that something is not open source. The writer of this blog post Righteous, Expedient, Wrong ~ OSI swings at Elastic, misses and leaves a mess giving you yet another view of OSI\u2019s claims.\nThe reason for taking this step is to make it impossible for Amazon to make the AWS Elasticsearch managed service available. Elastic has drawn up agreements with a number of large cloud providers such as Google and Microsoft Azure about their service to offer Elastic products as a managed service. However the negotiations with AWS did not go anywhere. The consequences of this license change are significant for some companies that are reliant on Elasticsearch. This is explained in the blog of Bonsai, a company that has been offering Elasticsearch as a service for years. For many developers the whole ordeal is much more a matter of principle than anything else. This is well articulated by someone like Drew DeVault in his blog Elasticsearch does not belong to Elastic. He calls the license change a slap in the face for the 1,573 developers who contributed to the project by making their code available.\nThe disagreement between Elastic and AWS\nAnyway, back to those licenses. When launched, Elasticsearch was completely open source under the Apache 2.0 license. Previously, Elastic had already decided not to give away everything for free, so they added their own license to part of the code. Something that was not appreciated by a party like AWS, especially because it made it difficult to use the code yourself. This was when a new Elasticsearch distribution emerged called \u201cOpen Distro for Elasticsearch\u201d. This is a distribution with all code released under the Apache 2.0 license in conjunction with other open source products. According to Elastic there were some problems with it. The distribution uses a number of open source extensions. Elastic claims that the creators of one of these components copied code from Elastic that is not under the Apache license. Read more about this in Dear Searchguard users. Personally, I do not think it is very appropriate, but it does make sense when keeping the open source concept in mind. Not stealing code, of course, but making a fork. As Elastic itself points out, they have tried to make things move in a different direction with AWS, but their attempts have failed. Read the following article Why License Change AWS for more information on this. Incidentally, Elastic is not alone in their fight against AWS. There are many companies with Open Source products complaining about the way AWS deals with them. An extensive article appeared in the New York Times in 2019: How Amazon Wields Power in the Technology World. It describes how Amazon turns Open Source projects into managed services without benefiting the companies that try to make a living from the projects. In the article they mention the strip mining of open source projects. Examples are given about products such as MongoDB, Splunk, MariaDB, Redis. In the article by ZDNet AWS hits back at open-source software critics, the feeling of the open-source world about AWS is highlighted again.\nBack to AWS and Elasticsearch. Amazon is of course not keeping quiet either. They released a statement saying that they have done nothing wrong. They have made patches and extensions available to Elasticsearch. However, they say that this systematically led to nothing. This is why they started Open Distro, which they themselves say is not a fork yet because they have worked according to Open Source Upstream. In other words, they have followed the rules as a good Open Source user should. AWS\u2019s response to the latest license change is that they are now actually going to fork.\nAWS is not the only company that has problems with these latest licensing developments. There are plenty of articles about it available online. A good example is Logz.io\u2019s blog Open source Elasticsearch doubling down. Logz.io offers a cloud monitoring solution based on open source tools such as Elasticsearch and Kibana. They indicate that the license change feels like a slap in the face for the open source community, and are working with a number of other companies to start a fork. Yes indeed, one of these companies is AWS. This has been made public in the following article: Stepping up for a truly open source Elasticsearch. Another company that has also been offering Elasticsearch As A Service for a long time is Bonsai. They explain their options for continuing to provide this service in their blog Elasticsearch license changes. This is how they view the path to comply with the SSPL license. At first glance, the text of the license itself does not seem unambiguous enough to fully support it. Then they can choose between making an agreement with Elastic or going for an Open Source fork.\nImpact on those working with Elasticsearch daily\nIn my opinion, the announced changes are a bad thing for Elasticsearch users. How nice would it be if several companies worked on improvements to the Elasticsearch product, according to the Open Source idea. I understand very well that Elastic wants to earn money from their products, they are the ones putting the most hours into development. Just like MongoDB, which faced similar problems a few years ago, and therefore developed a new license. It would be nice if AWS would put some of their money back into the project. Of course they say they already do this by offering improvements, but if they had to buy some sort of license, are we still talking about Open Source? That is exactly what they say at the Open Source Initiative. Fine if you as a company want to earn money through a certain business model, but then you should not advertise with the fact that you offer an open source product. And you have to think about this in advance, not after you have benefited greatly from the open source community.\nWhat is the impact for you if you want to use Elasticsearch in your project? There is no clear answer to that. Elastic has written a FAQ that should help you. If you already had some form of a contract with Elastic, nothing will change. If you previously only used the binaries, not a lot will change either. Maybe you should check whether you comply with the license. When you build a service based on Elastic products, make extensions to this and offer them as a service in the cloud, you will have to talk to Elastic and negotiate with them. Or you must have faith in a fork and continue on that particular path. If you now use a service like AWS Elasticsearch, Bonsai, Logz.io, you will have to pay close attention to their strategy in the near future. Most companies will negotiate with Elastic or switch to a fork. Of course you can also switch to a cloud provider that has partnered with Elastic, or use Elastic\u2019s cloud service itself. These are all very good choices if you want to enjoy all the good things Elasticsearch has to offer.\nAs a software engineer I am a big fan of the open source model. But as a very experienced Elasticsearch consultant I also feel a lot of sympathy for the position Elastic is in. I furthermore share the opinion of many developers in the community that a company like Elastic should be able to earn enough money from their own cloud service and their support contracts. I will continue to monitor developments closely. For now, I will continue to build projects with Elasticsearch, but I will also not stop with the projects that use AWS Elasticsearch. We will continue to do projects with Apache Solr and look with great interest at tools such as Vespa and Weaviate to see if they offer better alternatives for customers.\nReferences in chronological order\nFebruary 27, 2018 \u2013 https://www.elastic.co/blog/doubling-down-on-open\nSeptember 4, 2019 \u2013 https://www.elastic.co/blog/dear-search-guard-users\nJanuary 14, 2021 \u2013 https://www.elastic.co/blog/licensing-change\nJanuary 19, 2021 \u2013 https://www.elastic.co/blog/why-license-change-AWS\nJanuary 19, 2021 \u2013 https://www.elastic.co/blog/license-change-clarification\nJanuari 19, 2021 \u2013 https://drewdevault.com/2021/01/19/Elasticsearch-does-not-belong-to-Elastic.html\nJanuary 20, 2021 \u2013 https://logz.io/blog/open-source-elasticsearch-doubling-down/\nJanuary 21, 2021 \u2013 https://aws.amazon.com/blogs/opensource/stepping-up-for-a-truly-open-source-elasticsearch/\nAlso interesting to read\nhttps://www.elastic.co/pricing/faq/licensing\nhttps://opendistro.github.io\nhttps://opensource.org/node/1099\n", "tags": [], "categories": ["Blog", "Search"]}
{"post_id": 25397, "title": "Cloud: The Game. How we used AWS Amplify to motivate our colleagues", "url": "https://www.luminis.eu/blog-en/cloud-en/cloud-the-game-how-we-used-aws-amplify-to-motivate-our-colleagues/", "updated_at": "2021-08-10T16:20:12", "body": "Why?\nAt Luminis we are focusing on becoming Cloud proficient and we are using any means necessary. We share our knowledge, follow courses and get certifications. However, we wanted to also take the road less traveled and think of other ways we might motivate this journey for our colleagues.\nOur Idea: We will use the Cloud to host a Cloud Game that will motivate our colleagues to Cloud! Yes, I used Cloud as a verb\u2026\nSince we are also becoming more proficient with AWS Amplify at Luminis, we decided to use it to quickly get started.\nGoals\nThe main goal is to motivate our colleagues to do more with the Cloud. We decided that they should be able to do the following:\n\nLog in using our existing Azure Active Directory Luminis accounts\nBe able to add \u201cachievements\u201d related to the Cloud, such as reading a blog, giving a presentation or get a certificate.\nThey will get points based on these achievements\nThey will get badges that based on points and combinations of things they achieve\n\nThese were the main requirements for the first version of the application. Furthermore it was important that we can go live quickly and that we can add new features later on. This is where Amplify comes in.\nUnder the hood\nOur entire setup looks like this:\n\nWe have created an AWS Cognito user pool, API Gateway and Lambda using the Amplify CLI. Our Angular frontend uses the AWS SDK to easily authenticate our users and to communicate with our REST endpoints hosted on API Gateway.\nSince all of our Luminis colleagues have an account on Azure Active Directory, we want them to be able to use these accounts to log in. We have created an Open ID Connect provider in the Cognito console and link this with our Azure Active Directory. If you want to know how to do this, see this link.\nAfter this is set, we can just call \u201cAuth.federatedSignin({provider:\u201dOffice\u201d})\u201d in our frontend where \u201coffice\u201d in this case is the name of our custom OIDC identity provider to sign in. We will automatically get redirected to the Microsoft office page where you can sign in to your account.\nAnother aspect you might notice is that we use RDS instead of DynamoDB, which comes out of the box with Amplify. Why did we do this?\n\nWe wanted to try out Aurora Serverless\nWe wanted to use SQL so that we can simply write our SQL queries per badge and have these queries run at certain times to check if players deserve new badges\nWhile we like DynamoDB as a database and for its pricing, we are not that good yet with the query language and we wanted to quickly be able to add queries for badges\n\nOn RDS and Aurora Serverless\nBecause RDS is not a standard part of Amplify we had some work to do in order to get this working. We first created an Aurora Serverless cluster in the RDS console. Then we gave Lambda RDS full access via policies in its role so that it can communicate with it. Finally in the Lambda, we used the RDSDataService available in the \u201caws-sdk\u201d package to be able to communicate with the database.\nWhile we are happy that we got this working, we are not quite happy with the results yet. This is because:\n\nIt turns out that Aurora Serverless on RDS is quite expensive to keep running. We were already paying a few euros on our first day without having any users. To give a comparison, another project I run with DynamoDB cost me less than 1 euro for an entire year when in \u201cOn demand\u201d pricing mode\nIf we turned the scaling of the RDS to 0, which is one of the benefits of Serverless, then we have a very long cold start.\n\nWe are still looking into how we can fix this problem, however since it is an internal application within Luminis it is not that bad if some requests fail once in a while.\nHosting and continuous deployment\nWe used GitLab to host our repository and we could easily use the Amplify console to build and deploy our application on every commit to a certain branch. We then registered the domain cloudthegame.com on Route53 and with a few clicks we were able to link our Amplify deployment to the domain and even HTTPS was set up for us!\nResults\nThe end result looks like this:\n\n\u00a0\nAs mentioned previously, we used Route53 to register a domain. You can visit the app on https://www.cloudthegame.com. You won\u2019t be able to log in, but you can at least see the design.\nWe used a retro gaming theme and we try to add the word \u201cCloud\u201d as much as possible. It is currently only available in Dutch, but the screenshot above shows that I have earned 2 badges so far! We already have around 90 users with almost all of them having at least 1 badge!\nWe have big plans for Cloud: The Game, however we are glad we were able to use Amplify to quickly get started and focus on what matters.\n", "tags": [], "categories": ["Blog", "Cloud"]}
{"post_id": 25150, "title": "InformationGrid release 21.1 now available", "url": "https://www.luminis.eu/blog-en/news-en/informationgrid-release-21-1-now-available/", "updated_at": "2021-08-10T16:20:41", "body": "A new version of InformationGrid, the model-driven data platform developed by Luminis, was released yesterday. InformationGrid users now have access to new options for using search engines. In addition, they will benefit from the higher performance of projections and numerous other improvements.\nNew feature: search projections\nFor the integration of InformationGrid and search engines such as Elastic Search it is useful if information from the journal can be stored in a search index. Projections are used for this. These were already available in the library version of InformationGrid, but are now also available in the PAAS version of InformationGrid.\nImprovements and bug fixes\nFurthermore, this release contains dozens of improvements and bug fixes, which contribute to performance and provide additional possibilities for developers. For a full overview of all changes, improvements and bug fixes you can view the technical release notes here.\nSupport\nQuestions and requests can always be emailed to support@informationgrid.com.\nWant to know more about InformationGrid?\nPlease visit the InformationGrid website.\n", "tags": [], "categories": ["Blog", "Data", "Development", "News", "Search"]}
{"post_id": 24966, "title": "Dataflow with Python", "url": "https://www.luminis.eu/blog-en/cloud-en/dataflow-with-python/", "updated_at": "2021-08-10T16:21:06", "body": "introduction\nWhen you want to start doing some data ingestion on the Google Cloud Platform, Dataflow is a logical choice.\nJava offers more possibilities (see built-in I/O Transform) but still there might be reasons why you need to stick to Python (for example, broader use and adoption in your organization).\nsetup\npython version\nThe versions that are currently supported: 3.6, 3.7 or 3.8. If you are free to choose, pick up the latest version.\nbeam version\nWhen you decided on the python version check the dependencies versions as well. All versions work with the latest Apache Beam version but some other dependencies might hold you back.\ndependency management\nA Dataflow job is like any other Python application, so you first need to settle on a way to manage the dependencies.\nIn this post, I will be using pipenv.\nWhen pipenv is installed, you can start installing dependencies right away. pipenv will create a virtual environment and start populating it with the dependencies you install.\napache-beam is the first dependency you should install:\npipenv --python 3.8 install apache-beam\nDepending on what you need to achieve, you can install extra dependencies (for example: bigquery or pubsub).\npipeline\nworker setup\nIn order to have a correct setup on all worker, Dataflow is running a python script that can be specified as a pipeline option.\nCreate a new setup.py file with the following content updating it where needed:\nfrom setuptools import setup, find_packages\nVERSION_NUMBER = '0.0.1'\nsetup(\n\u00a0 name='df-python-example-job',\n\u00a0 version=VERSION_NUMBER,\n\u00a0 description=\"\",\n\u00a0 long_description=open(\"README.md\").read(),\n\u00a0 classifiers=[\n\u00a0\u00a0\u00a0 \"Programming Language :: Python\"\n\u00a0 ],\n\u00a0 keywords='',\n\u00a0 url='',\n\u00a0 packages=find_packages(),\n\u00a0 include_package_data=True,\n\u00a0 zip_safe=False,\n\u00a0 install_requires=[],\n\u00a0 author='Me',\n\u00a0 author_email='me@a-domain.nl'\n)\nYou can get a dependency list via pipenv, paste the output of the following command in the install_requires array:\npipenv run pip freeze | sed 's/^/\"/;s/$/\",/;$ s/,$//g'\nprepare pipeline\narguments\nIf you need to pass some information to the job depending on the context, you can do that by:\n\npassing command line arguments\nread a configuration file.\n\ncommand line arguments\nYou can rely on argparse to do that.\nparser = argparse.ArgumentParser()\nparser.add_argument('--environment',\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 dest='environment',\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0                    choices=['local', 'cloud'],\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 required=True,\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 help='Environment the pipeline is running on: [local or cloud].')\nknown_args, pipeline_args = parser.parse_known_args(argv)\nOne important (and arguably unmissable) argument is the environment, it makes it possible to apply different settings for local or cloud execution.\nYou can also add all the other arguments your pipeline needs.\npipeline_args are all argument that are not specifically listed, they can be used to set pipeline options.\nconfiguration file\nInstead of passing lots of command line arguments, you can read a configuration file depending on the environment the pipeline is executed on.\nYou can first get the environment from the command line argument and then select the correct configuration before reading its content.\nconfig = configparser.ConfigParser()\nconfig.read('example-%s.ini' % known_args.environment)\nFor the cloud environment, you can generate the production configuration with ConfigMaps.\npipeline options\nDataflow has its own options, those option can be read from a configuration file or from the command line.\npipeline_options = PipelineOptions(pipeline_args)\npipeline_options.view_as(StandardOptions).runner = 'DirectRunner'\ngoogle_cloud_options = pipeline_options.view_as(GoogleCloudOptions)\ngoogle_cloud_options.project = 'luminis-df-python-example'\nrunner and project are mandatory.\n\nrunner sets the data processing system the pipeline will run on\nproject sets the Google Cloud Project the pipeline will be bind to\n\nWhen running in the cloud, a different runner needs to be selected. In our case that is the DataflowRunner.\nif known_args.environment == 'cloud':\n\u00a0\u00a0\u00a0 pipeline_options.view_as(StandardOptions).runner = 'DataflowRunner'\nYou can refer to the list of options to set other aspects of the pipeline.\nAs mentioned above, the setup script location should be specified as an option as well.\npipeline_options.view_as(SetupOptions).setup_file = './setup.py'\npipeline steps\nNext you can start setting up the steps of the pipeline.\npipeline = beam.Pipeline(options=pipeline_options)\nWhen you write your pipeline, you describe a chain of transform operation. For this Apache Bean uses / and >> (their basic function is overwritten)\n(pipeline | 'read lines' >> ReadFromText(csv_file) \n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 | 'map lines to object' >> beam.ParDo(MapCsvLineToObjectFn()) \n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 | 'write objects to file as JSON' >> WriteToText(output, file_name_suffix='json', coder=JsonCoder()))\nReadFromText and WriteToText are built-in PTransforms. beam.ParDo() applies a function. MapCsvLineToObjectFn extends beam.DoFn.\nTo avoid any surprises, declare type hints when writing your DoFns and PTransfoms.\nYour pipeline will run on a distributed environment so always pay attention to the following rules:\n\nserializability\nthread-compatibility\nidempotence\n\nmetrics\nWhen executing your pipeline, you can log metrics during execution this allows you to:\n\nsee how far the pipeline is\naccess historical data\ntrigger some follow-ups if the metrics diverge from what is expected\n\nInstanciate a counter:\ncounter = Metrics.counter(self.__class__, 'map_csv_to_object_counter')\nIncrement it when needed:\ncounter.inc()\nThere are other metrics types available.\nThe metrics are visible when and after the job has run.\npipeline state\nAt any time a pipeline is in a given state. The state is linked to the type of runner not all possible states a available to all runner (sometimes a state doesn\u2019t make sense for a runner).\nYou don\u2019t want to be fetching the state of a pipeline after running it. You can do that as part of a different flow. The code bellow starts the pipeline and wait until if finishes (successfully or with an error).\nresult = pipeline.run()\nresult.wait_until_finish()\nresult.state contains the final state of the pipeline. Note that any exception won\u2019t be caught and it is useless to check the state in that kind of situation, you need to throw and catch your own exceptions.\nHaving the state of the pipeline at the end makes it possible to execute post-pipeline steps.\ntest\nThere are different ways to test a pipeline, you can re-define you pipeline steps and run them with TestPipeline or you can pass static data to your script and check the output (end to end).\nContinuing on the example above, you want to check that the conversion from CVS lines to JSON works correctly. You first define inputs and expected output:\nCSV = [\n\u00a0\u00a0 \"index;message\",\n\u00a0\u00a0 \"1;test1\",\n\u00a0\u00a0 \"2;test2\",\n\u00a0\u00a0 \"3;test3;incorrect\",\n]\nEXPECTED_OBJECTS = [\n\u00a0 {\n\u00a0\u00a0\u00a0 \"index\": \"1\",\n\u00a0\u00a0\u00a0 \"message\": \"test1\"\n\u00a0 },\n\u00a0 {\n\u00a0\u00a0\u00a0 \"index\": \"2\",\n\u00a0\u00a0\u00a0 \"message\": \"test2\"\n\u00a0 },\n]\n\nThen you can create temporary file containing the items in the CSV array. You can use the tempfile module for that.\n@staticmethod\ndef create_temp_file(contents: [] = None):\n\u00a0\u00a0 c_dump_file = tempfile.NamedTemporaryFile(mode='wb', delete=False, suffix='.csv')\n\u00a0\u00a0 if contents is not None:\n\u00a0\u00a0\u00a0\u00a0 for line in contents:\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 c_dump_file.write(line.encode('utf-8'))\n\u00a0 \u00a0 \u00a0\u00a0            c_dump_file.write(\"\\n\".encode(\"utf-8\"))\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 c_dump_file.close()\n\u00a0\u00a0 return c_dump_file.name\nYou can then start your pipeline:\noutput_file = create_temp_file()\ncsv_file = create_temp_file(self.CSV)\nexample.run(\n\u00a0\u00a0 ['--environment=local',\n\u00a0\u00a0\u00a0 '--output=%s.result' % output_file,\n\u00a0\u00a0\u00a0 '--csv-file=%s' % csv_file])\n\nBeam does output sharding, that means that the result will be split into multiple files with standard suffix format convention. So this means you need to merge the results.\nresults = []\nwith open_shards(output_file + '.result-*-of-*') as result_file:\n\u00a0\u00a0 for line in result_file:\n\u00a0\u00a0\u00a0\u00a0 results.append(json.loads(line))\nYou can then check if the generated output is the same as what is expected:\nself.assertCountEqual(self.EXPECTED_OBJECTS, results, 'Result and expected object list are different')\nrun\nexecute locally\npipenv run python3 -m example --environment local --csv-file example.csv --output example\nexecute on Google Cloud\nExecuting on Google Cloud means executing using the Dataflow runner. In our example, reading local file doesn\u2019t make sense. So we need to read/write from/to a Google Bucket.\nReadFromText and WriteToText are able to transparently handle that so no code change is needed. Note that the csv-file and output parameters require the scheme for GCS.\n\ncreate input and ouput bucket: luminis-df-python-example-in and luminis-df-python-example-out\nupload the CSV file to the input bucket.\n\npipenv run python3 -m example --environment cloud \\\n--csv-file gs://luminis-df-python-example-in/example.csv \\\n--output gs://luminis-df-python-example-out/example\ndeploy\nIn most cases, you want your pipeline to be executed multiple time (scheduled).\nThe most straight forward way to deploy your pipeline is to:\n\nbuild a Docker image based on the code above\nupload it to your GCP registry\nexecute the command above in the way that fits you own organization\n\nbuild an image that ships cron as the main process and add the command as a cron entry\nuse Kubernetes CronJob\n\n\n\nimage with cron\nFile containing cron expression, example-cron:\n0 1 * * * . /env.sh; cd / && pipenv run python3 -m example --environment cloud \\\n--csv-file gs://luminis-df-python-example-in/example.csv \\\n--output gs://luminis-df-python-example-out/example > /proc/1/fd/1 2>/proc/1/fd/2\nIn order to have the correct environment variables when running python in cron context, first a script is created, it will later be used when starting the cron job.\nstart.sh:\n#!/bin/bash\nprintenv | sed 's/^\\(.*\\)$/export \\1/g' > /env.sh\nchmod +x /env.sh\ncron -f\nDockerfile:\nFROM python:3.8-buster\nRUN apt-get update && apt-get install -y -qq --no-install-recommends cron\nCOPY example-cron /etc/cron.d/example-cron\nRUN chmod 0644 /etc/cron.d/example-cron\nRUN crontab /etc/cron.d/example-cron\nCOPY *.py /\nCOPY setup.py /\nCOPY Pipfile /\nCOPY Pipfile.lock /\nCOPY start.sh /usr/bin\nRUN chmod +x /usr/bin/start.sh\nRUN pip install pipenv\nRUN pipenv install\nCMD [\"start.sh\"]\nBuild a docker image:\ndocker build -t df-python-example-job:0.0.1 .\nConclusion\nI hope this post has given you some inputs to start implementing your own pipeline.\n", "tags": [], "categories": ["Cloud", "Development"]}
{"post_id": 24836, "title": "Building resilient connections using a circuit breaker", "url": "https://www.luminis.eu/blog-en/development-en/building-resilient-connections-using-a-circuit-breaker/", "updated_at": "2021-01-26T09:12:18", "body": "Software systems are more and more integrated systems. They integrate with datastores, with log and monitoring systems, with services provided by other components (micro-services) and with SAAS from external parties. When using the cloud to run software components, they need to be as stateless as possible, easy to scale, and resilient to other components\u2019 failures. One way of creating more resilience is by using decoupling, for example, by using asynchronous actions or queues. In some situations having synchronous communication is required. In cases where the user needs direct feedback, and the component needs to integrate with another system, we can put a circuit breaker in our component between the connection component to the other system. This circuit breaker monitors the calls to the external system. It prevents us from continuously trying to connect or call the system even when it is down. But it also provides us with information about the current state and some basic stats.\nThis blog post summarises what we need from a circuit breaker. Through a demo using the excellent library Resilience4j, we show some aspects of the circuit breaker.\nImportant circuit breaker concepts\nThe most known circuit breaker is the one that everybody has in the house. According to Wikipedia, an electrical switch opens up in case of an overload or a short circuit. The switch prevents this overload or short circuit to damage appliances connected to the switch by opening up.\nIn this post, Martin Fowler explains the circuit breaker\u2019s concepts and why this is interesting for software components. A circuit breaker monitors for failures (short circuit) and slow responses (overload). If the rate of failures or slow responses becomes too high, the circuit breaker opens up, and no calls can get past it any longer. After a cool-down period, the circuit breaker lets a few calls pass to scan for improvements. If the calls go through, the circuit breaker\u2019s state goes back to closed, and all calls can go through again.\nWith a circuit breaker in place, we want insights in the state, and all passed calls. Information about the current failure rate, slow rate, successful calls, failed calls. Next to monitoring, we also want to configure thresholds, rates, recovery time to suit our needs.\nIntroduction of the Resilience4j library\nGoogling for java and circuit breaker gives a lot of posts. In the past, I used a project called Hystrix. Hystrix integrates well with Spring, but it also needed a lot of libraries. Another thing is that Hystrix is now in maintenance mode, so discouraged to use. Another library that is mentioned a lot is Resilience4j. Resilience4j is a library with additional components for a retry mechanism, a rate limiter, and a time limiter. For the monitoring part, it has integrations with Micrometer and Grafana. It also comes with integrations for Spring Boot and Spring Cloud.\nI am going to use Resilience4j to demo the concepts. You can find the sample code on Github. The demo contains a basic spring boot application to integrate with through REST. The REST application has endpoints to ask for an error, success, slow, timeout.\nThe source code here focusses on the Resilience4j code. You can find all the other code in Github. The project contains a readme file to get started.\nFirst, we configure the circuit breaker.\u00a0Some configuration options deal with thresholds, for example, the number of failures in a window. Other deal with the open state and how fast we can go back to the half-open state. You can also configure the exceptions that should trip the circuit breaker and which should not. As the test I am doing is fast, the values are low. We need these to make sure we see all the different conditions.\nCircuitBreakerConfig circuitBreakerConfig = CircuitBreakerConfig.custom()\u00a0 \u00a0 \r\n    .failureRateThreshold(10) // 10% of requests result in an error\r\n\u00a0 \u00a0 .slowCallRateThreshold(50) // 50% of calls are to slow\r\n\u00a0 \u00a0 .waitDurationInOpenState(Duration.ofMillis(10)) // Wait 10 milliseconds to go into half open state\r\n\u00a0 \u00a0 .slowCallDurationThreshold(Duration.ofMillis(50)) // After 50 milliseconds of response time a call is slow\r\n\u00a0 \u00a0 .permittedNumberOfCallsInHalfOpenState(5) // Do a maximum of 5 calls in half open state\r\n\u00a0 \u00a0 .minimumNumberOfCalls(10) // Have at least 10 calls in the window to calculate rates\r\n\u00a0 \u00a0 .slidingWindowType(CircuitBreakerConfig.SlidingWindowType.TIME_BASED) // Use time based, not number based\r\n\u00a0 \u00a0 .slidingWindowSize(5) // Record 5 seconds of requests for the window\r\n\u00a0 \u00a0 .recordExceptions(UniformInterfaceException.class) // Exception thrown by REST client, used as failure\r\n\u00a0 \u00a0 .ignoreExceptions(DummyException.class) // Business exception that is not a failure for circuit breaker\r\n\u00a0 \u00a0 .build();\r\nthis.circuitBreaker = CircuitBreaker.of(\"dummyBreaker\", circuitBreakerConfig);\nWith the configured circuit breaker we can start wrapping method calls. Resilience4j uses the java functionals API to wrap method calls.\npublic void call(String message, LocalDateTime timestamp) {\r\n\u00a0 \u00a0 Supplier<String> stringSupplier = circuitBreaker.decorateSupplier(\r\n\u00a0 \u00a0 \u00a0 \u00a0 () -> dummyEndpoint.executeCall(message, LocalDateTime.now())\r\n\u00a0 \u00a0 );\r\n\u00a0 \u00a0 try {\r\n\u00a0 \u00a0 \u00a0 \u00a0 String test = stringSupplier.get();\r\n\u00a0 \u00a0 \u00a0 \u00a0 LOGGER.info(\"{}: {}\", timestamp.toString(), test);\r\n\u00a0 \u00a0 } catch (UniformInterfaceException e) {\r\n\u00a0 \u00a0 \u00a0 \u00a0 LOGGER.info(\"We have found an exception with message: {}\", message);\r\n\u00a0 \u00a0 } catch (CallNotPermittedException e) {\r\n\u00a0 \u00a0 \u00a0 \u00a0 LOGGER.info(\"The circuitbreaker is now Open, so calls are not permitted\");\r\n\u00a0 \u00a0 }\r\n\u00a0 \u00a0 printMetrics();\r\n}\nOf course, you want to know what is going on in the circuit breaker. You can request metrics using the Metrics API, and there is a way to get callbacks about successes, failures, state changes, etc.\nprivate void printMetrics() {\r\n\u00a0 \u00a0 CircuitBreaker.Metrics metrics = this.circuitBreaker.getMetrics();\r\n\u00a0 \u00a0 String rates = String.format(\"Rate of failures: %.2f, slow calls: %.2f\",\r\n\u00a0 \u00a0 metrics.getFailureRate(),\r\n\u00a0 \u00a0 metrics.getSlowCallRate());\r\n\u00a0 \u00a0 String calls = String.format(\"Calls: success %d, failed %d, not permitted %d, buffered %d\",\r\n\u00a0 \u00a0 \u00a0 \u00a0 metrics.getNumberOfSuccessfulCalls(),\r\n\u00a0 \u00a0 \u00a0 \u00a0 metrics.getNumberOfFailedCalls(),\r\n\u00a0 \u00a0 \u00a0 \u00a0 metrics.getNumberOfNotPermittedCalls(),\r\n\u00a0 \u00a0 \u00a0 \u00a0 metrics.getNumberOfBufferedCalls()\r\n\u00a0 \u00a0 );\r\n\u00a0 \u00a0 String slow = String.format(\"Slow: total %d, success %d, failed %d\",\r\n\u00a0 \u00a0 \u00a0 \u00a0 metrics.getNumberOfSlowCalls(),\r\n\u00a0 \u00a0 \u00a0 \u00a0 metrics.getNumberOfSlowFailedCalls(),\r\n\u00a0 \u00a0 \u00a0 \u00a0 metrics.getNumberOfSlowSuccessfulCalls()\r\n\u00a0 \u00a0 );\r\n\u00a0 \u00a0 LOGGER.info(rates);\r\n\u00a0 \u00a0 LOGGER.info(calls);\r\n\u00a0 \u00a0 LOGGER.info(slow);\r\n}\r\n\nprivate void addLogging(CircuitBreaker circuitBreaker) {\r\n\u00a0 \u00a0 circuitBreaker.getEventPublisher()\r\n\u00a0 \u00a0 \u00a0 \u00a0 .onSuccess(event -> LOGGER.info(\"SUCCESS\"))\r\n\u00a0 \u00a0 \u00a0 \u00a0 .onError(event -> LOGGER.info(\"ERROR - {}\", event.getThrowable().getMessage()))\r\n\u00a0 \u00a0 \u00a0 \u00a0 .onIgnoredError(event -> LOGGER.info(\"IGNORED_ERROR - {}\", \u00a0 \u00a0 \u00a0event.getThrowable().getMessage()))\r\n\u00a0 \u00a0 \u00a0 \u00a0 .onReset(event -> LOGGER.info(\"RESET\"))\r\n\u00a0 \u00a0 \u00a0 \u00a0 .onStateTransition(event -> LOGGER.info(\"STATE_TRANSITION - {} > {}\",\r\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 event.getStateTransition().getFromState(), event.getStateTransition().getToState()));\r\n}\nThat\u2019s it. Now you know what you need to know to use the Resillience4j library and create a circuit breaker. I leave the other features that Resillience4j has for you to explore. The final bit is about running the sample and see that it works.\nRunning the sample\nYou can find the source code in the following GitHub project:\nhttps://github.com/jettro/resilience4j-demo\nThe sample does a loop over three different calls. One to the dummy endpoint, one to the error endpoint, and one to the slow endpoint. We configured a minimum of 10 requests in the window. In the beginning, the following is logged:\nRate of failures: -1,00, slow calls: -1,00\r\nCalls: success 5, failed 2, not permitted 0, buffered 7\r\nSlow: total 2, success 0, failed 2\nNo rates are recorded yet, but after the next error call it all changes. We see we have enough requests now, the failure rate is above the configured threshold of 10%. Therefore the circuit breaker moves into the OPEN state.\nSTATE_TRANSITION - CLOSED > OPEN\r\nRate of failures: 30,00, slow calls: 30,00\r\nCalls: success 7, failed 3, not permitted 0, buffered 10\r\nSlow: total 3, success 0, failed 3\nAfter 10 milliseconds the circuit breaker changes state into half-open\nSTATE_TRANSITION - OPEN > HALF_OPEN\nBut as we keep getting errors, it goes back into OPEN after a few calls\nReferences\nhttps://en.wikipedia.org/wiki/Circuit_breaker\nhttps://martinfowler.com/bliki/CircuitBreaker.html\nhttps://resilience4j.readme.io/docs/circuitbreaker\nhttps://github.com/Netflix/Hystrix/wiki/How-it-Works\n\u00a0\n", "tags": [], "categories": ["Blog", "Development"]}
{"post_id": 24724, "title": "The new normal demands resilience engineering", "url": "https://www.luminis.eu/blog-en/the-new-normal-demands-resilience-engineering/", "updated_at": "2023-04-20T13:39:46", "body": "\nUnderstanding resilience engineering is essential for organizations that want to survive as we accelerate into the online future. Let me tell you what resilience engineering is and how it can help you roll with the punches that come with success.\nThe only way to keep the Dutch at home is by grounding their offspring, as our government has discovered twice over. So, after spending two weeks dreading the end of the holiday season, last week marked the start of the second homeschooling season in a year.\nThanks mostly to my very structured wife and non-male children (no coincidence there), our household was prepared. We set up two desks in the living room, made schedules, and brought laptops, tablets, headphones, pencils, paper. But when it was time for the digital school gates to open, they\u2026 didn\u2019t.\nWatskeburt?\nIn our case, the school\u2019s digital learning and communication platforms choked on the peak load all the homeschoolers generated. Luckily, Netflix/YouTube/the bookcase/outside was still online, so we just postponed the inevitable for a bit. By lunchtime, everything was already forgotten, replaced by the regular remote schooling joys and frustrations.\nAfter dinner and coffee and ice cream, and some delicious white chocolate, I, ehm\u2026 Where was I? Oh yeah, I started to get curious about that morning\u2019s outage. And I found an interesting ongoing status report by the Parro team. Apparently, they already anticipated the surge and had taken what they believed would be enough measures to survive it:\n\u201cTo be fair, we expected that Parro, with all the measures taken, would be able to handle the new extreme peak traffic.\u201d\nAfter what I imagine must have been a stressful few hours, the Parro team fixed all issues and had a smoothly running system by the end of the day again. Kudos to them. That\u2019s what mattered most to their customers, the teachers and parents of our small kingdom.\nBut their work is not done, as the updates on the status page show. Where is the next bottleneck? When will it become a problem? How will they fix it? Will they ever be done?\nThey won\u2019t. This is the new normal. But it doesn\u2019t have to be a negative thing. There is a way to use these weird times to engineer ourselves into the future.\nLet\u2018s examine how.\n\nPeaks are the new normal\nNow that we are accelerating into the online future, unpredictable massive peak usage will become just another truth. Scary? Sure. Exciting? I think so. But a reality, nonetheless.\nThis will lead to two types of problems to deal with:\n\nKnown unknowns: a problem that hasn\u2019t happened yet, but is not surprising once it happens. Server failures, network troubles, that kind of stuff.\nUnknown unknowns: something that comes as a complete surprise, shocking systems into failure. Or, as Donald Rumsfeld called them: \u201cthe ones we don\u2019t know we don\u2019t know.\u201d\n\nThe first type is tough enough to handle as it is. But a well designed, robust system can withstand these known unknowns. Redundancy, retries, fallbacks, failovers: when designing highly available systems, these terms are familiar. We use them to deflect what we know can go wrong, like a server going down or a request to a service timing out.\nBut what about a global pandemic leading to sudden, massive demand on a system designed for a different reality? Surely, you can\u2019t handle all the ways a system might break down under these unpredictable conditions?\nBut you can. It\u2019s called resilience engineering.\nHandling the unknown unknowns\nResilience engineering is a field still very much inside the scientific domain. It is closely related to the areas of human error, cognitive systems engineering, and safety. You know, plane crashes, oil spills, and nuclear disasters, that type of stuff.\nWebsites and mobile apps going down rarely lead to environmental disaster or death. That doesn\u2019t mean we can\u2019t learn anything from the tons of research that went into making our world a little safer. Resilience engineering takes all the good stuff that prevents 747s and powerplants from going down and applies it to our young (software) engineering field.\n\nOld thinking, new thinking\nLike my colleague Hans Bossenbroek so eloquently put: as the need for agility and speed increases, we need to move towards event thinking. The solution: event-driven, modular, highly scalable systems. But also: super complex architectures resembling chaotic spider webs. And a gazillion events flowing through at breakneck speed.\nOf course, that\u2019s not something you design and deploy at the end of the first sprint. And it might be a long way off from your massive monolith. But since we live in web-scale-or-die-times, it\u2019s the logical solution to real-world problems. As the technology evolves, so should the way you manage it. You have to fight the urge to control the new system the way you did your old one.\nInstead of focusing on the things that might go wrong (known unknowns!), try instead to look at the things that go right and do those things more often. This will help you and your teams:\n\nUnderstand what leads to the right thing and do that more often.\nIncrease the chance of success instead of nervously awaiting the next breakdown.\nBe proactive instead of reactive.\nDrive towards continuous improvement.\n\nYou\u2019ve read that right; it\u2019s about humans as much as it is about hardware and software!\nSystems thinking\nYou see, systems encompass both people and technology. Think about it: when was the last time you heard about a developer accidentally deleting a production database? Or the other way around: some Sherlock Holmes type saving the day by tweaking a setting even the most experienced colleague was unaware of?\nAnd that is where resilience comes from: humans doing the right things. We do that because we just know what is right. We have a knack for it, some kind of intuition. It\u2019s what keeps complex systems from falling apart.\nAnd when it does fall apart, like last week, something must have tipped the scales. Despite all the hard work by the humans and the computers, systems go down from time to time. And when they do, they have to be brought back up as soon as possible.\nNow, stop and think: if production goes down, what do you do? Maybe something like:\n\nFind the root cause.\nFix it.\nMake sure that never ever happens again.\n\nIf you want to practice resilience engineering: wrong answer.\n\n\u00a0\n\u00a0\nThere is no root cause!\nStop again. Think about the complex system. We already saw that keeping a system running is a balancing act. Finding a root cause would imply that there is a single thing that caused the error. If good behavior is explained by complexity, how can a single cause explain faulty behavior?\nIt can\u2019t. It\u2018s a logical fallacy. There is no root cause.\nThis also means that a proper postmortem should not be about explaining the hunt for a single point of failure. An excellent postmortem is about showing that you understand that complexity sometimes results in unwanted behavior. And that you realize, in hindsight, that many things were working together until they didn\u2019t. But that you have all learned from it. That you now know how to do even more things right.\nIn other words: good postmortems are blameless, balancing team safety and accountability.\nGoing resiliently into the future\nSo, to err is\u2026 complex. Faults are a given. Resilience engineering is not about preventing them. It\u2018s about doing ever more things right so that you can increase complexity while maintaining control. So that when the next unknown unknown strikes, you will be prepared.\nSo get to it! Learn:\n\nWhat is resilience engineering? (You already know!)\nHow do I start practicing it?\nWhom should I involve?\nHow do we get better over time?\n\nNeed help? Keep an eye on our blog series on resilience to find out the answers to these questions!\nObservations on the resilience of bone\nIn the meantime, here is an excellent talk by Dr. Richard Cook about resilience engineering and bones. Bones? Yes, you know, you have them inside your body most of the time! And they are a great vehicle for explaining two types of resilience engineering. Watch and learn:\n\nRelated post:blogpost8 Dec 2020-Hans BossenbroekResilience engineering: an introductionOver the past years, people at Luminis have become involved in the design and development of new types of systems. Systems that implement requirements beyond stability and robustness. Systems that need to be resilient. Systems that can cope with unforeseen...\n", "tags": [], "categories": ["Blog", "Resilience"]}
{"post_id": 24307, "title": "Resilience: The rise of events, a Copernican view on data", "url": "https://www.luminis.eu/blog-en/resilience-en/resilience-the-rise-of-events-a-copernican-view-on-data/", "updated_at": "2021-08-10T16:21:41", "body": "There was an indisputable truth to every system built up to some years ago: a database with a single managed data model as a starting point. As the need for agility and speed increased, this truth has proven to be a significant stumbling block.\nData (not) at the center\nThis challenge has been technically addressed with the advent of new storage technologies like document, graph, and semantic databases. However, with the emergence of this new, polyglot reality, it has become clear that understanding and using these new possibilities have proven to be far from simple. It\u2019s almost similar to the experience Copernicus had after he conceived a model of the universe that placed the Sun rather than the Earth at the center of the universe. Game-changing stuff at the time.\nLike the errors that astronomers made before Copernicus, many architects and technologists are obsessed with the idea that data is at the center of the computing universe. This thinking often leads to static, stove-piped, monolithic systems with a single database at the center, hard to change or migrate over time because of its volume and complexity.\nThinking in events\nLet\u2019s take a look at a resilient example we all know: our own body. We constantly react to and act upon events that arrive via our senses. These events relay information that\u2019s stored in our memory, replayed in our minds, and may lead to an action that produces new events.\nThinking in an event-driven manner turns organizations into a sensory element in the universe of computing. New events sensed by web applications, mobile apps, IoT sensors, and legacy systems are forwarded to an infrastructure that can process and store events properly. Just like the way our sensory system sends events to our central nervous system for interpretation.\nIn this event-driven world, the central system should be able to:\n\nReceive and process different stimuli lightning fast.\nStore, retrieve, and combine\nAdapt to changes in events and event processing.\n\nThis resilient and highly scalable system is made possible by combining the advantages of an event-sourced architecture with a fit-for-purpose approach to storing event objects. By storing all state changes as a sequence of events, (re)building its state can be done by simply (re)applying all events.\nResilient, scalable, adaptable, secure: InformationGrid\nDesigned and developed by Luminis, InformationGrid exemplifies this approach and adds adaptability and security:\n\nIntegrated security in event objects. By separating events from event-processing, it is possible to implement fine-grained authorization mechanisms for information.\nData adaptability. By assuming that event objects can change over time and designing mechanisms that can handle these changes, it is possible to create resilient data strategies.\nHigh scalability. Using an event-sourced approach allows you to separate data storage, data processing, and data querying. This results in very high scalability.\n\nRelated post:blogpost8 Dec 2020-Hans BossenbroekResilience engineering: an introductionOver the past years, people at Luminis have become involved in the design and development of new types of systems. Systems that implement requirements beyond stability and robustness. Systems that need to be resilient. Systems that can cope with unforeseen...\nTime for event thinking\nSo, the central data model\u2019s problems have been solved, and the new model\u2019s complexity has been tamed. Time for all digital businesses to leverage event processing so that they can be always-sensing and always-ready:\n\u201cApplication leaders engaged in digital transformation initiatives must add \u2018event thinking\u2019 to their technical, organizational, and cultural strategies.\u201d\n\u00a0\n", "tags": [], "categories": ["Blog", "Resilience"]}
{"post_id": 24364, "title": "Why Kotlin? My experiences with this language in the past 1,5 years", "url": "https://www.luminis.eu/blog-en/development-en/why-kotlin/", "updated_at": "2021-08-10T16:21:47", "body": "For my current project at Bol.com, I worked on two business critical applications for the Platform Quality department. These applications apply business rules on the assortment of partner retailers to ensure the quality of the Bol.com platform. At peak hours, thousands of offers per second are being approved or rejected. The apps are being developed in Kotlin, and for me it was the first time I worked with this language in \u2018real\u2019 applications. In this article I\u2019ll explain what made me love Kotlin and what are the main benefits of this language in my opinion.\n\u00a0\nNull-safety\u00a0\nKotlin is null-safe by design. Of course you can choose to create nullable values, but it is always a conscious decision to do this. When you create nullable values it will lead to compiler errors when you do not handle them correctly. Also, the compiler knows when a value cannot be null and you don\u2019t need to care about it possibly being null.\n\u00a0\nExamples:\nTo create a value that can hold null, in Koltin you declare the variable with a ?, for example var a: String?. \u2018a\u2019 can be set to null if you want. To check a nullable property, there are some different options:\n\nFirst you can explicitly check if \u2018a\u2019 is null:\u00a0 if (a == null ) {\u2026}\nThe second option is the safe call operator ?.. For example println(a?.length)\u00a0will print a.length if a is not null and null otherwise. These safe call operators can also be chained.\nThird, you can use let\u00a0in combination with the safe call operator to perform an operation only on non-null values: a?.let { println(it) }\nThe Elvis Operator ?:\u00a0can be used when you want to use \u2018a\u2019 if it is not null and otherwise use some non-null value: val l = a?.length ?: -1\nThe !! Operator can be used when you want to convert a nullable value into a non-null type and throw an exception when the value is null: val l = a!!.length will cause an NPE when a is null.\n\nAt my current project, the null-safety of Kotlin really helped us out when it turned out that a property of an object that was requested from another API turned out to be nullable, in contrast to what we would expect it to be. After we added the ?\u00a0to indicate it was nullable, the compiler pointed out the functions we needed to adjust to handle this nullability.\n\u00a0\nExtension functions\nIn our microservice applications at Bol.com, we have a controller layer, service layer and data layer. We don\u2019t want the service layer to have any \u2018knowledge\u2019 about the controller layer but we do want some mapping functions to convert the domain objects to api objects. For this we used Extension functions. Extension functions are used to extend a class with new functionality, without having to inherit from the class.\nTo use extension functions, you prefix the function\u2019s name with the type being extended, the \u2018receiver type\u2019. Our object mapping functions look like:\nfun Comment.toApi() = CommentDTO(id, author, text)\nfun ProductPolicy.toApi() = ProductPolicyDTO(policy.toApi(), userName, comment?.toApi(), created)\nreturn policyService.findById(id)?.toApi() ?: throw ResourceNotFoundException(\"Policy with id $id not found.\")\nAlso note the safe operator in for example comment?.toApi(). The extension function is only being called if comment is not null. In the latest example, it is only being called if the response of policyService.findById(id) is not null. If it is not null, an exception is being thrown by using the Elvis Operator.\n\u00a0\nType inference\nKotlin has the concept of Type Inference, which means that you don\u2019t need to explicitly specify the type of a variable or return type of a function but it is inferred by the compiler. There are two types of type inference:\n\nLocal type inference for expressions locally in statement or expression scope. For example\nval policies = policyService.findAll()\nA pitfall is that this can hide the type of the variable which can lead to less readable code. The solution for this is to name your variables properly. A list of Policy objects should be named \u2018policies\u2019 and not \u2018list\u2019 for example. Also, of course your IDE can tell you the type when you want to know. When you really want it or when it is needed if the type cannot be inferred, there is also an option to specify the type:\nval policies: List<Policy> = policyService.findAll()\n\nFunction signature type inference, for inferring types of functions return values and/or parameters. For example:\nInstead of\nfun sum(a: Int, b: Int) : Int {return a + b}\nyou can write\nfun sum(a: Int, b: Int) = a + b\nThe advantage is that the code looks shorter and cleaner, but sometimes it can be unclear what the return type is. Also, there is a danger that your method \u2018inherits\u2019 a type change from a calling method. A solution for this can be use shorthand functions with an explicit return type:\nfun sum(a: Int, b: Int) : Int = a + b\n\n\n\u00a0\nImmutable\nA reference in Kotlin is either a Variable (var) or a Value (val). The difference between these two is that a var can be reassigned after its definition and a val not. The same as for nullable values, you as a developer make the (hopefully conscious) decision about the mutability of your objects.\nAt Bol.com, sometimes we needed to change for example a list that was a variable of an object. To do this with a val, you can use the \u2018copy\u2019 method:\nreturn policy.copy(\r\n    rules = rulesSorted,\r\n    sets = setsSorted\r\n)\n\u00a0\nData classes\nInstead of declaring a class with properties and getters and setters for classes that hold data, in Kotlin you use data classes. When you mark a class with \u2018data\u2019, you can access it\u2019s properties by just the name:\ndata class Policy(val id: String, val name: String)\r\nval policy = Policy(\u201c1\u201d, \u201cpolicy1\u201d)\nwhen you want to print the name, just write\nprintln(policy.name)\nWhen declaring a class as data class you get some other things for free as well:\n\nEquals()/hashCode()\ntoString()\ncopy() functions for immutable types\n\n\u00a0\nNo static keyword\nKotlin has no \u2018static\u2019 keyword. Why this is, I can\u2019t really tell. You can use Kotlin\u2019s Companion Objects when you need to declare an object inside a class:\nclass MyClass {\r\n    companion object {\r\n        const val CHUNK_SIZE = 50\u00a0\r\n        fun create(): MyClass = MyClass()\r\n    }\r\n}\nMembers of the companion object can be called by using the class name as qualifier: val instance = Myclass.create(). Also, the const CHUNK_SIZE is created in this companion object, which can be used the same way as a static final Integer in Java.\n\u00a0\nWhat makes life better\nAfter working with Kotlin for 1,5 years, I see a lot of advantages of this language in just the way the code it clean and readable and in some nice syntactic features. Some examples:\n\u00a0\nUtility methods on collections\nMy favorite features in Kotlin are all the utility methods on Collections that Kotlin provides. I\u2019ll not dive into every one of them, but will name some of the features that I used a lot in my current project.\n\nMap: the mapping transformation creates a collection from the results of a function on the elements of another collection.\n\nval data = dataRaw.map { it.trim().toLowerCase() }\n\nForEach: the forEach function iterates a collection and executes the given code for each element:\n\nshops.forEach { \r\n    shopRepository.create(it)\r\n}\n\nFilter, filterNot: In Kotlin, filtering conditions are defined by predicates:\n\nval allRules = policyRegistry.getRules()\r\nallRules.filter { ruleNames.contains(it.name) }\r\n    .map { it.errorCode.name }\nWhen you want to filter a collection by negative conditions, you can use filterNot:\nval invalidProducts = uploadedProducts.filterNot { isValid(it.globalId) } \n\nOrdering: ordering a list in Kotlin is very easy. An example with two properties of an object to use for ordering:\n\nval rulesSorted = policy.rules.sortedWith(\r\n    compareBy<PolicyRuleDTO> { it.state }.thenBy { it.name }\r\n) \n\nisEmpty(): a very basic operation, but what makes it nice is that there is also a isNotEmpty() which makes the code a lot more readable in my opinion then !isEmpty()\n\n\u00a0\nString templating\nKotlin has built in String templating:\nval name = \"Sofie\"\r\nval children = listOf(\"Maxim\", \"Otis\")\r\nprintln(\"$name has ${children.size} children\")\n\u00a0\nMultiline String constants\nWe use this for example in tests, when we want to post JSON to an endpoint:\nval json = \"\"\"\r\n    {\r\n\u00a0 \u00a0     \"name\": \"Sofie\",\r\n\u00a0 \u00a0 \u00a0 \u00a0 \"children\": [\"Maxim\", \"Otis\"]\r\n\u00a0 \u00a0 }\r\n\"\"\"\n\nIf/when expressions\nIn Kotlin if and when are expressions, which means they can be used inline to return a value. For example with if:\nval max = if (a > b) a else b \nWhen is used to replace the switch statement:\nfun validate(value: String, listData: ListData) : String {\r\n    return when(listData) {\r\n        ListData.BRICK -> validateBrick(value)\r\n        ListData.CHUNK -> validateChunk(value)\r\n        else -> value\r\n    }\r\n}\u00a0\nWhen can also be used without argument, as a replacement for an if-else if chain:\nprivate fun mapOnlyValid(product: Product?): Product? {\r\n    fun notANumber(input: String) = !input.matches(NUMBER_REGEX)\r\n    return when {\r\n        product == null -> null\r\n        notANumber(product.globalId) -> null\r\n        notANumber(product.ean) -> null\r\n        notANumber(product.brick) -> null\r\n        notANumber(product.chunk) -> null\r\n        else -> product\r\n    }\r\n}\u00a0\n\u00a0\nConclusion\nAfter working with Kotlin now for 1,5 years, I can say that I really like how it makes my code more compact but still readable. Every time I need to do a complex operation on a collection for example, and think that it must be possible in Kotlin, it turns out to be possible indeed. What I also like is the extensive documentation on kotlinlang.org with very helpful examples. With the help of this documentation and the features Intellij provides, it is not hard to learn when you are used to write Java. Just give it a try!\n\u00a0\n", "tags": [], "categories": ["Blog", "Development"]}
{"post_id": 24303, "title": "Resilience engineering: an introduction", "url": "https://www.luminis.eu/blog-en/resilience-en/resilience-engineering-an-introduction/", "updated_at": "2021-08-10T16:21:55", "body": "Over the past years, people at Luminis have become involved in the design and development of new types of systems. Systems that implement requirements beyond stability and robustness. Systems that need to be resilient. Systems that can cope with unforeseen disturbances.\nWhat is resilience?\nIf you look up the terms resilient, resilience, or resiliency, a whole new world of definitions and interpretations emerge before your eyes. Clearly, resilience goes back a long way. Starting in the world of materials and ecosystems before finding its way into psychology and even systems theory.\nSo yes, buzzwords. Who can live without them? This got me thinking about the term: what does resilience really mean, and is it used correctly?\nFor many developers, architects, and analysts, resilience appears as something new. It isn\u2019t. System safety and cybersecurity engineers have been leading the field of resiliency. Combined with the academic research on this topic, their work has proven to be foundational. In fact, if you\u2019re interested in this field or are planning to design and develop a resilient system, you might as well start at that part of the domain.\nThe standard definition of resilience is a systems\u2019 ability to recover quickly from disturbances. This definition works OK in general terms but falls short when describing the resilience of complex systems. Even more so in the case of systems of systems. And what if you were to consider the resiliency of an energy network or networked defense system? Which characteristics come to mind?\nI think that resilience does not just refer to how well a system can handle disturbances and variations that fall outside its designed adaptive mechanisms. It\u2019s also about the design of the mechanisms to recover from these situations. Strictly speaking: in a perfect world, truly resilient systems can handle both anticipated and unanticipated change.\nNow we have a better definition. Resilience is about:\n\nThe ability to recover from faults.\nThe persistence of service reliability when facing change.\n\nThis is the essence of adaptative behavior.\nWhat is resilience engineering?\nNow that we have a good definition of resilience let\u2019s dive into the specifics of designing and developing resilient systems. Enter resilience engineering.\nHanging out under the same umbrella as chaos engineering, resilience engineering is a way of building systems designed to withstand failure and change. Resilience engineering acknowledges this reality of constant and sometimes unexpected change and faults. Through engineering, it provides a way for systems to cope with this.\nGood resilience engineering produces systems that can activate the appropriate adaptive behavior. Resilience can be built into a system. It also offers perspectives on critical areas like cybersecurity, safety, and operations.\nHere are some example results of resilience engineering:\n\nA system that autonomously starts using the next best CPU after the cloud provider stops providing the previous one.\nA system that can scale up by acquiring servers from a different availability zone in the same region when all the servers in its zone fail.\nA system that adapts to hardware configuration changes by maintaining its features without human intervention.\n\nWhat is autonomous adaptive behavior?\nThe defining trait of resilience is autonomous adaptive behavior. If you provide a SaaS product and your systems go down, you no longer have a product.\nHumans have long been the primary agent in making systems adapt. Investigating failure, getting things up and running again, and thus making systems resilient to failure. It used to be the work of on-the-ready people.\nBut human labor is too costly, error-prone, and slow in the age of cloud systems.How Complex Systems Fail by Richard Cook presents an interesting overview of common ways that systems fail. 50% of the choices have to do with human error or the necessity of human intervention.\nFortunately, software comes to the rescue. Software has proven to be the goto-mechanism to implement adaptive behavior. With the emergence of cloud computing and infrastructure abstractions like Docker and Kubernetes, more and more software is taking over people\u2019s work.\nHow do you design resilience?\nWhen thinking about resilience, it is usually difficult to avoid terms like failure prevention and robustness. Stronger even, one of our best practices is to find weak spots in systems or designs and reinforce them. It\u2019s the way we arrive at mechanisms like redundancy, firewalls, and endless procedures.\nResilience takes another approach. Instead of reducing failure, it strengthens success. Furthermore, assuming it is possible to build supporting adaptive mechanisms, the system can display emergent, resilient behavior.\nHere are a few rules for building resilience into systems.\nAlways react to failures and (unforeseen) change\nWhen errors occur, teams need to understand what normal, desired behavior is and act accordingly to restore that. When a failure or unforeseen change occurs, and there is no correct response, you are not adapting.\n(Note: not responding to failures or disturbances is a key characteristic of the organizational death spiral.)\nAlways log correctly in a comprehensive logging infrastructure\nThe key to successfully treating failures and disturbances is identifying them as early as possible and finding their root cause. Having a comprehensive logging infrastructure in place that allows you to build good logging messages is essential. The underlying infrastructure should be able to:\n\nHelp identify errors quickly.\nSupport an initial root cause analysis.\nAllow staff to handle and treat the errors with ease.\n\nDesign a metrics system and stick to it\nOne of the biggest problems when encountering faults or disturbances is subjective interpretations of information. Therefore, it is important to base your designed resiliency on a limited set of important metrics. Mainstream metrics in the world of cloud computing are:\n\nService or system availability.\nMean time between failures (MTBF).\nMean down time (MDT).\n\nAnd let\u2019s not forget good old metrics like response time, latency, and bandwidth.\n(Note: mean time to failure (MTTF) and mean time to repair (MTTR) are metrics belonging to the domain of non-repairable systems.)\nHow do you test resilience?\nThe easiest way to test a resilient system is to wait for failure to happen and then hope for the best. A better way is to simulate failure before it happens in real life. In other words: simulating chaos.\nLike I mentioned before, resilience engineering and chaos engineering have a relationship. The latter is often used to test the resilience of a system:\nChaos engineering, a practice developed at Netflix, aims to help test a system\u2019s resiliency by proactively throwing common and unexpected failures at a system. The original idea was an experiment: how can engineers build the system to be more resilient before bad things happen, instead of waiting until after the event?\nThis led to the creation of Chaos Monkey, a tool that simulated common failures in the system\u2019s infrastructure. Like its namesake, the tool acts like a monkey rampaging through a data center, unplugging and cutting cords wherever it goes.\nThe next step is moving away from tests and ensuring resilience thinking is baked into the way everything is built. Ways to segue into that state of mind are creating a chaos engineering team, doing chaos architecture, and organizing game days where teams focus all their efforts on testing their systems\u2019 resiliency.\nConclusion\nResilience engineering is all about adapting to expected and unexpected changes. A resilient system expects failure and can find ways to keep on working reliably.\n", "tags": [], "categories": ["Blog", "Resilience"]}
{"post_id": 24287, "title": "Remote design sessions: five ways to design with your team", "url": "https://www.luminis.eu/blog-en/concepting-ux-en/remote-design-sessions-five-ways-to-design-with-your-team/", "updated_at": "2022-12-08T10:47:06", "body": "One of the most important parts of your work as a designer is developing empathy with a group of people. Of course, talking to people face-to-face works best. Whether you\u2019re interviewing people or observing how people use the product, you\u2019ll learn the most when seeing them in real life. However, it\u2019s also possible to develop that empathy from a distance, using online tools like teleconferencing.\nIn this article, we\u2019re going to explore how to conduct design sessions online to learn more about people in our target audience. With design sessions, you can both get to know the needs of people and involve them in the design process. This way, you\u2019ll be able to design new features for your product and test those new features quickly. There are several methods to do this, for instance, Lean UX (1), design sprints (2) or generative methods (3). An added advantage of designing together with your target audience is shared ownership: people appreciate it when they\u2019re allowed to participate in the design process (4).\nEvery design method works differently, however, there\u2019s always going to be an analysis phase, an ideation phase and a prototyping phase. For example, using Lean UX, you\u2019ll formulate assumptions in an early phase and choose the most critical one to test. We\u2019ll explore how to do those phases online and still experience the magic.\nAnalysis phase\nA successful virtual design session needs preparation. First of all, start with inviting a small design team. Invite diverse roles like a software architect, engineer, business rep and customer, but make sure to invite up to six, seven people. The larger the team, the more communication lines there\u2019ll be between every member of the team. Verbose people will dominate the group, and with a large group, it\u2019s difficult to put shy team members in the spotlight.\nAsk the design team to prepare the session. A business developer, for example, could gather market data; a designer would bring usability research findings. Before the session, compile this information and put it in one place. I found that the best way to do this is using an online tool like Miro, Coggle or LucidChart. These tools show a giant, virtual canvas that all team members can access and change.\nIt\u2019s important that all team members have a shared understanding of the design goal and context. During the session, let the customer in the team present their values and goals. This way, the team will hear people talk about their experiences with the product and will develop some empathy. When you\u2019ve formulated the design goal, put it on a prominent spot on the virtual canvas, so the design team can focus on it and build empathy with the benefit people look for in your product (5).\nIdeation: ideas are cheap and plentiful\nThis is where the magic happens. Time to sketch. Use techniques like design studio or design charrettes (4) to let the design team draw ideas. With the design goal and context in mind, team members will have some constraints to think about and will have to use their imagination.\nAs a designer, you\u2019re used to quickly sketch something, throw it away when needed and draw something else. Not everyone is comfortable doing this, however. Coach people and tell them everyone that can draw a box, circle and squiggly lines for text can contribute.\nCrazy Eight sketch technique\nLet the team members focus and draw ideas on their own; this works very well in an online design session. Just shut down the videoconferencing tools a few rounds. Give them the opportunity to call you directly when they\u2019re stuck. During one of our sessions, a team member wasn\u2019t too sure about himself, but in the end, his idea got a lot of traction. Letting people draw on their own and present their ideas instead of brainstorming has advantages. People can refer to each other\u2019s ideas and diverge into new ideas.\nLet people draw multiple ideas. This improves the quality of the ideas, because people often have a fixed, first idea they take with them the moment they enter the session. Making up some more ideas might change their thinking and lower their resistance to abandoning their first idea for an idea that proves to be better (6). Use methods like crazy eight or six-up (1) to force people to diverge. Each round, be sure to keep a tight time table and encourage them not to dwell on details too much.\nKeep it low tech\nAfter each round, let the team members join the teleconferencing session again and let them present their ideas. Keep it low tech, ask people to just show their drawings for the camera and present their ideas. After a few rounds, help the team converge on the best ideas, for example, by using dot voting.\nDot voting\nPrototyping phase \ud83d\udd28\nAs a designer, you\u2019re probably used to sharing designs and prototypes online. Expand on the team ideas from the design session and craft a design, set of wireframes or prototype. This is the time to shine as a designer. Using platforms like InDesign or Zeplin allow you to share your design and gather feedback.\nMore and more teams I encounter can prototype in code and use continuous integration to build new features quickly. These teams can roll out new features to a portion of their customers and roll them back if the design turns out to be a bad idea. I think this is Lean UX to its core: to be able to quickly test your assumptions with real code.\nConclusion: tips for online collaboration \u2601\ufe0f\nWith simple tools like teleconferencing and a virtual, shared canvas, it\u2019s possible to conduct a design session online. Keep it simple. Developing group rapport and understanding your customer is more important than using fancy design tools.\n\nOnline collaboration can be exhausting. Make sure to plan some breaks \u2615\ufe0f.\nPeople don\u2019t need to stare at a screen all day. Give people some time to sketch on paper with the camera off. Let them focus and gather their thoughts.\nKeep it low tech, using simple teleconferencing tools and sketching will feel like a slightly messy real-life design session. Using tools people already know will lower the threshold to join, as well.\nKeep it fast and loose. You\u2019re bound to have someone in your design team that lets their OCD flag fly and starts arranging things on the shared canvas. Mess it up a little; shared understanding is more important than having all details tidied up in place (7).\nMost important: keep on innovating , especially in times of crisis!\n\nSources\n\nGothelf J. 2013 \u2013 Lean UX\nKnap J., Zeratsky J., Kowitz B. 2016 \u2013 Sprint\nSanders E.B.N. 2018 \u2013 Convivial Toolbox\nhttps://www.nngroup.com/articles/design-charrettes\nChristensen C.M. 2016 \u2013 Competing against Luck\nDow S., Fortuna J., Schwartz D., Altringer B., Klemmer S. 2011 \u2013 Prototyping Dynamics: Sharing Multiple Designs Improves Exploration, Group Rapport, and Results\nBroadfoot O, Bennett R 2003 \u2013 Design Studio: Online?\n\n", "tags": ["Design"], "categories": ["Blog", "Concepting &amp; UX"]}
{"post_id": 24271, "title": "Useful tools for local development with AWS services", "url": "https://www.luminis.eu/blog-en/cloud-en/useful-tools-for-local-development-with-aws-services/", "updated_at": "2020-12-08T13:33:47", "body": "Over the last 2.5 years, I\u2019ve been working with AWS and a wide range of its services. During this time I noticed that for most projects it\u2019s useful to be able to test your application against AWS services without having to deploy or move your code into the cloud. There are several free solutions available for you to use depending on the services required by your project. In this post, I\u2019ll describe some of the tools that I use.\nDynamoDB local\nAt one of my previous projects, we made extensive use of the combination of DynamoDB and Elasticsearch for storing and querying data. The fact that DynamoDB is a managed database service with immense scale and performance benefits, makes DynamoDB a great fit for high traffic applications.\nAs a user, it\u2019s quite simple to use as it\u2019s a key-value store. Most of the other AWS databases are managed instances of existing services, however, DynamoDB is an AWS specific service which you can\u2019t really download and install locally. Luckily back in 2018 AWS\u00a0introduced\u00a0a simpler way to work with DynamoDB utilizing\u00a0DynamoDB local, a dockerized version of DynamoDB which you can simply run as a docker container to develop and test against.\nRunning DynamoDB local is as simple as executing:\n$ docker run -p 8000:8000 amazon/dynamodb-local\nOr if it\u2019s part of a bigger set of dependencies you could leverage docker-compose.\n\nWith that it\u2019s a matter of running:\n$ docker-compose up\nAnd you should see something like:\n\nWith the AWS CLI you can easily query for available tables:\n$ aws dynamodb list-tables --endpoint-url http://localhost:8000\nWhich should result in something like:\n\nAnd of course, you can use the AWS SDK with your preferred language as well.\nI hear you thinking: are there no limitations? Yes of course there are some limitations with using DynamoDB local compared to the managed service. For instance, parallel scans are not supported (they will happen sequentially). Most limitations are nicely outlined in the DynamoDB Developer guide.\nAmazon also provides docker images for other services like AWS Step functions Local and OpenDistro for Elasticsearch. Be sure to check out the Amazon repo on DockerHub for other usefull images.\nLocalStack\nNow when you\u2019re developing a simple service that only depends on DynamoDB, DynamoDB local is a good choice. However, once you start to leverage more and more services it might be worthwhile to look for other options as not all services are available as single docker images.\nWhen you\u2019re building services that are part of a microservices architecture, you\u2019re probably using other AWS services like SNS, SQS, and perhaps S3. This is where a tool like\u00a0LocalStack\u00a0can add a lot of value. So what is LocalStack?\nLocalStack is a project open-sourced by Atlassian that provides an easy way to develop AWS cloud applications directly from your localhost. It spins up a testing environment on your local machine that provides almost the same feature parity and APIs as the real AWS cloud environment, minus the scaling and robustness of course.\n\nLocalstack focuses primarily on providing a local AWS cloud environment that adheres to the AWS APIs and offers a free and pro version, which you can leverage depending on your requirements. In my experience, the free/community version offers a lot of value and supports a whole range of services.\nYou can install LocalStack via\u00a0pip\u00a0if you\u2019re familiar with python and its package system or you can use it via docker(compose). On my Mac, I found that installing LocalStack as a python package was a bit of a hassle, so I always prefer to use it via docker-compose.\nUsing LocalStack with docker-compose is as simple as creating a docker-compose.yml file with the content:\n\u00a0\n\nIf you\u2019re running on a Mac be sure to prepend\u00a0TMPDIR=/private$TMPDIR\u00a0before running.\n$ TMPDIR=/private$TMPDIR docker-compose up \nAfterwards, you should see something similar to the following output.\n\nAs you can see, it starts a whole bunch of services out of the box. If you don\u2019t use all those services you can also provide a list of services required when starting localstack by providing a SERVICES variable like:\n$ TMPDIR=/private$TMPDIR SERVICES=s3,sqs docker-compose up\nNow you should see in the startup output that it only started S3 and SQS.\n\nUpdate: I just learned that homebrew also supports installing LocalStack. I\u2019ve not used it, so can\u2019t say if it\u2019s any good, but it looks pretty simple \ud83d\ude42\n$ brew install localstack\nIf you don\u2019t want to manually start LocalStack via docker-compose, but want to start it for instance during your build/test phase, you can also leverage\u00a0testcontainers\u00a0and just add a localstack rule to your Unit test:\n\nAWSLocal\nIf you\u2019re already using LocalStack, it\u2019s worthwhile to also install\u00a0awslocal. It\u2019s a CLI that proxies the AWS CLI and adds the\u00a0--endpoint-url http://localhost:4566/ after every command, so you don\u2019t have to.\nYou can install it by running\n$ pip install awscli-local\nNow we can just run:\n$ awslocal dynamodb list-tables\nwhich just like with DynamoDB local would return:\n\nCommandeer\nLocalstack used to come with a Web UI, which is now marked as deprecated. As an alternative, I would recommend using\u00a0Commandeer. It\u2019s a very useful tool and also supports working with LocalStack (next to a whole bunch of other services). It can give a nice overview of the services started with localstack, but also offers dashboards and UIs for example DynamoDB in which you can explore data, create tables, etc.\n\nIf you know of other useful tools that help you in your day to day work working with and developing on the AWS platform, feel free to leave a comment or tweet at @jreijn or @luminis_eu!\n", "tags": [], "categories": ["Blog", "Cloud", "Development"]}
{"post_id": 24000, "title": "Keep innovating with cloud Services", "url": "https://www.luminis.eu/blog-en/cloud-en/keep-innovating-with-cloud-services/", "updated_at": "2020-12-01T11:10:07", "body": "If we have learned one thing from the COVID-19 pandemic, it is that the online world has taken a prominent place in our existence and can no longer be ignored. Its biggest driver? The cloud. Increasingly companies realise that their future will take place in the cloud and that they cannot miss out on this. Because what if your competitor has the power of cloud and knows how to use it?\nOnline is a must\nOf course online has been at the forefront of our minds much longer, but a lot of companies still do not focus massively on online strategies. In 2020 having an online strategy is no longer considered a luxury. Online used to be something companies did besides their usual business. The investments needed for implementing a solid online strategy are quite steep, which made choosing to focus on online a strategic choice.\nToday this is no longer the case. You should just do it. Whether you are a restaurateur in lockdown (again) who wants to set up a takeout and delivery service, are selling trinkets from China for next to nothing, or are a large supermarket chain. You want to be online, because your customer is online. In some cases online is the only place consumers go these days.\nQuick results at low costs\nThe average IT manager of a large(r) organisation often spends about 80-90% of his IT budget on maintaining the status quo. This leaves very limited resources to invest. Spending the remaining budget by taking risks might not sound very appealing. How nice would it be if you could conduct a number of experiments at a low cost?\nInnovation requires experimenting. With cloud technology you get the opportunity to conduct experiments at low entry costs. You are furthermore able to pull the plug on an experiment if it does not give you the desired results, without having to think about wasting significant investments.\nShorter time to value with managed services\nEvery day companies are realising that the power of cloud can be used to execute business ideas that were considered impossible until recently. Furthermore the cloud allows you to respond quickly to changing circumstances. Cloud technology is not necessarily new. It has been around for over 10 years and was used for the virtualisation of computer centres. However recently cloud has gone through a rebirth making it much more valuable than it was when it was just a data centre all the way across the internet. Modern cloud providers such as Amazon Web Services (AWS) and Microsoft Azure offer a wide range of reusable \u201cmanaged services\u201d in addition to the well-known virtualisation technology. These services can be used as a building block of semi-finished product for creating your own solutions: you only need to add your own business perspective. In the landscape of your cloud provider it is becoming easier to connect these managed services, without requiring much programming. The most important advantage of managed services is that the underlying infrastructure no longer requires attention. Which means there is no more need to spend a lot of time or money on it. Examples of managed services are databases, API gateways, and machine learning applications. Instead of losing a lot of time and money setting up, maintaining and adjusting these solutions, they are now turnkey services. You can immediately start work on applying the respective service. For example you will get the first insights based on machine leaning with Amazon Sagemaker by simply putting in some data. Because the underlying matters hardly need more attention, time, money and personnel can be used to achieve results faster, but above all to outperform the competition in places where it matters: the customer experience! You no longer have to spend your time competing for who can best maintain a database.\nCloud for everyone\nWith the modern cloud, a democratisation of the technology landscape has taken place. The same technology and innovative power has become accessible to everyone. Deep pockets, or shallower pocket: innovations that until recently were only available for big companies with ample R&D budgets are suddenly in reach for smaller players. A big difference between the cloud and having your own data centre is that there is no big up-front cost. In many cases, cloud offers a pay-as-you-go model where payment is made according to use. No fees have to be paid in advance, but with increasing use you will pay more. You can however count on a solid return on investment. That is why it pays to experiment with different applications of the cloud. Does something not work properly? Unplug it without having to say goodbye to major investments. Is something working well? Scale up easily to grow even more successful.\nFrom experiment to digital transformation\nThe combination of access to advanced technology and disruptive cost models does not only offer a wealth of possibilities, but comes with some challenges as well. Taking advantage of what the cloud has to offer is of course possible for a single solution, but you will not really benefit properly until you choose to place your entire application landscape in the cloud. In many cases this means you need to execute a cloud migration. With a cloud migration you not only transfer your applications to the cloud, but also adapt your processes and work methods in order to take full advantage of what the cloud has to offer. Ultimately this means lower operational costs and shorter turnaround. However, cloud is not some magical solution for cost reduction. In many cases, a migration to the Cloud will be quite expensive up front. This has to do with the fact that simply transferring the status quo to the cloud without making adjustments is not enough to make full use of the possibilities of the cloud. By gradually making adjustments to the application landscape and software production processes, these benefits can be achieved and will in many cases lead to significant cost reduction in the long term. Even if it only means you no longer have to invest time in manufacturing the underlying infrastructure and you can therefore use money and resources elsewhere: the cloud will help you compete at a different level.\nRelated post:cloudwhitepaperWhite paper Cloud migrationCloud migration: 5 effective steps towards a successful result. Download our free white paper. Do you want to migrate your organisation\u2019s systems to the cloud? A cloud migration provides speed and efficiency, among many other advantages. This white paper is...\nThe digital future of companies takes place in the cloud. More and more companies are realising that this is something they cannot miss out on. Because what if your competitor knows how to use the power of cloud? Can you still close the gap or are they out of reach?\n", "tags": [], "categories": ["Blog", "Cloud"]}
{"post_id": 23942, "title": "Did you already fix your water leak?", "url": "https://www.luminis.eu/blog-en/development-en/did-you-already-fix-your-water-leak/", "updated_at": "2020-12-22T15:20:13", "body": "Do you ever have the idea that your mailbox is only filling up more and more? Are you the person that still has hundreds if not thousands of unread mails in his inbox? Then it is time to start fixing your water leak.\nThis post is a cross-post from Patrick\u2019s article on LinkedIn.\nSome time ago I saw a colleague that had his mail completely organized, having folders for everything and an inbox that only contained mails that were important at that moment.\u00a0This was the moment I became jealous and said to myself: \u201cI need this too\u201d.\nIt was time for me to start organizing my mail. But where to start? Do you start making folders and moving mails in them? Then when would I be done, since the content of my inbox was close from reaching infinity\u2026\nThen it struck me that I have seen a resembling issue before in my professional experience as a software engineer; fixing bugs. My experience improving code quality in large software projects is that you don\u2019t know where to start, you (me in particular at that time) don\u2019t know when to stop and by the time you think you\u2019ve come a long way a large batch of fresh bugs is standing in line for processing.\nIn software development the solution came with the introduction of a recent update of SonarQube introducing the water leak principle. The idea is the following:\nImagine you come home one day to find a puddle of water on the kitchen floor. As you watch, the puddle slowly gets larger.\nDo you reach for the mop? Or do you try to find the source and fix it? The choice is obvious, right? You find the source of the leak!\nIn SonarQube this is achieved by separating existing/old code and newly created code. This allows for imposing high standards on newly created and modified content while permitting code that may have been written decades ago some slack by limiting work to enforcing fixing of the most hazardous issues. So how would this work on your mailbox? In my case I started fixing the water leak by introducing a set of folders that cover most (if not all) of my work activities. Separating projects, corporate communication and automatic notifications by development tooling brought a quick start. Secondly I defined several mail rules to automatically sort (incoming) mails among these folders. Running these rules on my inbox already addressed a lot of low-hanging fruits. The next step would then depend on the amount of mails in your inbox. For my professional mail I went through all remaining mail in a single (time consuming) session. In case of my private mail, which for some reason contains a bit more history, I introduced an \u201cUnsorted\u201d folder. This folder can be cleaned up bit by bit whenever time suits it, no rush. At the moment you reach the mailbox \u201cZero\u201d moment, this gives a huge feeling of satisfaction. A feeling that returns every time when for example at the end of the day you finish handling the newly received mails of that day. My mailbox currently only contains email that I still have to reply to that day, have to perform some sort of action or have not been sorted yet. It really gives me that well deserved rest at the end of the day, when all mails are gone so I have no more open tasks or actions to think of during the night. I hope this will help you in the way that it did for me.\n\n", "tags": [], "categories": ["Blog", "Development"]}
{"post_id": 23902, "title": "Machine learning: analyzing images with Amazon Rekognition Custom Labels", "url": "https://www.luminis.eu/blog-en/machine-learning-ai-en/machine-learning-analyzing-images-with-amazon-rekognition-custom-labels/", "updated_at": "2023-01-25T13:24:46", "body": "Artificial Intelligence (AI), machine learning and big data are concepts that sound a bit alien but are around us for some time now. Time for me to dive into it!\nIn this blog I\u2019ll explain my first steps in the machine learning world, more specific the Amazon Rekognition Custom Labels environment. Why this one? Amazon Rekognition offers some easy to enter examples and models to get you started without diving deep into machine learning. At the moment Amazon has showcases for object and scene detection, image moderation, facial analysis, text in image and face comparison including celebrity recognition.\n\u00a0\nCustom Labels is the service I used because I work with images unique to my (demo) business. The idea is to use image recognition to load a specific part of a website, prefilled with data based on an image. Machine learning is used to train a model to receive correct information from the image.\n-Luminis\nThe case\nCompany Van Dam\u00a0checks and maintains industrial doors on platforms and vessels. Every check involves a long and difficult form that needs to be filled in, depending on the status of a door, a lock or a tag plate. Pictures are taken to record the situation. The circumstances at those locations make it hard to fill in those forms easily and the error rate is quite high.\nA solution\u00a0could be to let the Van Dam\u2019s employee take a picture of the object at location. In the background the picture gets uploaded to the cloud, a machine learning model fires to classify the object. Data is sent to the employee\u2019s maintenance app and the correct form is opened and prefilled based on the received information.\nI will explain the steps I took to create a dataset and get a trained model \u2013 the other parts of the case are not considered in this blog.\n\nNote 1: in this demo I used my AWS admin account. When using AWS in case of a project you need to adjust the security settings conform the specifications of that project.\nNote 2: company Van Dam has a database full of images (of mixed quality but that makes this experience realistic and interesting). A a lot of images are needed to get a properly trained model.\n\nSet up\nTo be able to use the services from Amazon you need to have an AWS account. I won\u2019t go into detail how to set up accounts and permissions \u2013 Amazon has an excellent online manual to help you out.\nServices that I used:\n\nAWS IAM\nAmazon S3\nAWS CLI\nAmazon Rekognition Custom Labels\n\nAWS IAM\u00a0Via the AWS Management Console you find the IAM service in section Security, Identity, & Compliance. You need to create a group and a user in that group with sufficient rights.\nI created a group \u2018ml-administrators\u2019 with \u2018AdministratorAccess\u2019 permissions and added a user called \u2018ml-administrator\u2019. The user has \u2018IAMUserChangePassword\u2019 permissions.\nAmazon S3\u00a0Amazon Rekognition creates a S3 bucket to hold training images and test images. During my demo project I used another S3 bucket from which I imported images. Permissions need to be set correctly.\nAWS CLI\u00a0Via the AWS Documentation you can read the instructions to install the AWS CLI. I used version 2. Follow the steps as described and verify your installation with \u2018$ aws \u2013version\u2019 in the shell.\nPreparing the model\nStep one was to start a project, and I named it \u2018VanDamDoors\u2019. Next I created a dataset for training with the simple name \u2018Doors\u2019. To get images in the dataset I chose for \u2018upload images from your computer\u2019 and after submitting them Amazon directly warned with \u2018your dataset must at least have 2 labels\u2019.\nLabeling\nHow to do that was explained on that page too: every image needs a label and a bounding box to highlight exactly where that label shows up in the image.\nOk. Before I uploaded the images, I had made a decision about the labels I wanted to use. The employee has to get information about the front side of a door, the back side of a door, a lock or a tag plate. This is how the pictures in the Van Dam\u2019s database are structured and I uploaded 5 images of each category. So then I came up with these 4 labels: \u2018front, \u2018back, \u2018lock\u2019, \u2018tagPlate\u2019. I created the labels in my dataset in Rekognition and assigned them to the correct image.\nAfter that I opened every image to draw a bounding box in it, covering the spot where the door, lock or tag plate was located, and saved the changes.\nTraining & Testing\nReady to train the model! When you don\u2019t see a big orange button with \u2018train model\u2019 on it, you need to exit the labelling modus or refresh the page. For creating a test dataset I choose the option to split the training dataset. And then \u2026 I had to wait. For quite a long time. But then there were the results:\nI uploaded 20 images in total, from which Rekognition used 16 images for my training set, and 4 images for the test set. In the test set only one image was tested \u2018true positive\u2019, two others were \u2018false negative\u2019 and the other one was \u2018false positive\u2019. Disappointing.\nOf course the amount of images was too small for a reliable test, but it made me wonder if the bounding box method is the right was to do this. And it takes some time to do this manually for a lot of images.\nMeanwhile Rekognition has uploaded all data to the S3 bucket it created on my behalf. A bucket with a long name, in my case \u2018custom-labels-console-eu-west-1-f79af322a7\u2019. It holds the training images and testing images per dataset, to be found in the folder \u2018assets\u2019. Folder \u2018datasets\u2019 contains the metrics per dataset in subfolder \u2018custom-labels\u2019.\nUsing Amazon Cloud Storage S3\nI was convinced that there was another way to do this and I also wanted to use more than 20 images to train my model. I decided to upload all images in the S3 bucket that Rekognition uses. On the same level as the \u2018datasets\u2019 folder, directly in the root folder, I created a folder called \u2018input\u2019. Feel free to use another name \u2013 this is just for demo purposes.\nIn the \u2018input\u2019 folder I created 4 subfolders representing the labels of my new dataset. I used the same names as before (\u2018front, \u2018back, \u2018lock\u2019, \u2018tagPlate\u2019) to be able to compare the results. Every subfolder contained the images that belong to that label, in my case around 326 images per label.\nRe-training & Testing\nIn Amazon Rekognition I created a new dataset named \u2018Doors2\u2019 and I imported the images from the S3 bucket folder I just created.\nNote: the backslash at the end of the URL (s3://custom-labels-console-eu-west-1-f79af322a7/input/) is very important \u2013 don\u2019t forget this otherwise Rekognition keeps complaining about an \u2018invalid folder location\u2019.\nTo get the test images I let Rekognition split the dataset like I did before.\nThen I trained this model and in less than a second it was ready! A significant difference with my previous experience, and this time I had my labels in automatically (taken from the folder name).\nResults\nIn total I uploaded 1306 images. I used 1,043 images for the training set and 263 test images from which 258 images resulted in a \u2018true positive\u2019, 5 in a \u2018false positive\u2019 and also 5 in a \u2018false negative\u2019. Quite good result since a lot of images are not very clear. And again all locks and tag plates turned out to be marked as positive.\nQuick peek in the S3 bucket: when opening the \u2018evaluation\u2019 folder (root) and making your way through the subfolders, you will find some json files with a reference to Amazon SageMaker Ground Truth. That\u2019s the fully managed Amazon data labeling service that makes it easy to build highly accurate training datasets for machine learning.\nConclusion\nAutomatic labelling via uploading a folder structure is much more efficient with a lot of images. A lot of images are needed to properly train a model. I don\u2019t know what a bounding box adds to this process. User experience in Rekognition can definitely be improved!\nUsing the model\nNow we have the model set up, we can start to use it. To see if the model recognizes a random picture of an industrial door, I searched for an image on the internet. I put this image in a separate S3 bucket called \u2018custom-labels-input-images\u2019.\nTo get the model running I used the AWS CLI. In Rekognition the link to the latest model (for me \u2018VanDamDoors.2020-05-14T10.30.41\u2019) in the section \u2018Projects\u2019 opens a page with the API info. Scroll to the end of the page and click on the arrow to view the code.\nIn my Terminal (AWS CLI command) I started my latest model with:\naws rekognition start-project-version \\\r\n\u2013project-version-arn \u201carn:aws:rekognition:eu-west-1:938506314627:project/VanDamDoors/version/VanDamDoors.2020-05-14T10.30.41/1589445041847\u201d \\\r\n\u2013min-inference-units 1 \\\r\n\u2013region eu-west-1 \u2013profile Lydia\nI had to add \u2018\u2013profile Lydia\u2019 for AWS to use the correct account. Error \u2018The security token included in the request is invalid.\u2019 will show up if AWS doesn\u2019t recognize the profile.\nThe model starts when you get this feedback:\n{ \u201cStatus\u201d: \u201cSTARTING\u201d }\nTime to get the test image involved, and see how accurate the model is:\naws rekognition detect-custom-labels \\\r\n   \u2013project-version-arn \u201carn:aws:rekognition:eu-west-1:938506314627:project/VanDamDoors/version/VanDamDoors.2020-05-14T10.30.41/1589445041847\u201d \\\r\n   \u2013image \u2018{\u201cS3Object\u201d: {\u201cBucket\u201d: \u201ccustom-labels-input-images\u201d,\u201dName\u201d: \u201cDizRvKxWsAALTiY.jpeg\u201d}}\u2019 \\\r\n   \u2013region eu-west-1 \u2013profile Lydia\nNotice the reference to the bucket (\u2018custom-labels-input-images\u2018) and the image file \u2018DizRvKxWsAALTiY.jpeg\u2019.\nFeedback I received:\n{ \u201cCustomLabels\u201d: [\r\n     {\"Name\": \"front\",\r\n     \"Confidence\": 87.34600067138672}\r\n]}\nNice! It recognized the image as a front door view with 87 percent confidence.\nNote: don\u2019t forget to stop the model otherwise the costs will pile up. Use the API code to stop the model and check the feedback in the command line:\n    { \"Status\": \"STOPPING\" }\nDouble check the Rekognition \u2018Projects\u2019 overview section that the model indeed has stopped.\nConclusion\nUsing the model is easy as 1-2-3. It is very clear about the accuracy and it is easy to compare it with the other test results.\nNext steps\nNext steps in this demo case will be to use the data to open the correct form and prefill it. For example, when the input image (taken by the employee) is recognized as a lock with a certainty of at least 85% the lock information page should be opened.\nIn short\nTake picture > upload picture to S3 > start model > analyze image > receive label and confidence > stop model > open correct page > prefill form.\nAt the moment Amazon Rekognition only gives the option to use the model via the API code in AWS CLI command. Python is coming soon, it says.\nSome other features that can complete the app and what can be done with Amazon Rekognition are:\n\nEscape route if the picture is not a lock\nFacial recognition of the employee to prefill personal data\nGet date time when picture is taken\nGet text from image when it is a tag plate\n\nConclusion\nAmazon Rekognition is a service that offers handy features that can be easily used. Custom Labels targets specific objects and scenes used by your business. User experience can be improved to speed up the process, in my opinion. Using the API via Python is not an option at the moment \u2013 hopefully this will come soon. Since everything is cloud based you need to keep an eye on the costs when starting the model and using a lot of pictures. Don\u2019t forget to stop the model when ready.\n", "tags": [], "categories": ["Blog", "Machine learning &amp; AI"]}
{"post_id": 23892, "title": "Big Data: Where to start?", "url": "https://www.luminis.eu/blog-en/data-en/big-data-where-to-start/", "updated_at": "2020-11-30T15:37:13", "body": "The big data world is so big that it is humongous. Big data Engineer, Big data analyst, Big data scientist \u2013 Are these different names for the same role? It is all overwhelming to figure out which strand to take hold of. And how to climb that big mountain! To add on top of that \u2013 which algorithm to use, which tooling to use, which language to use.\n\nDo I learn Hadoop, Kafka, AWS \u2013 What in AWS stack?\n\nLet us start with understanding the role and responsibilities of these job titles. It can also serve us as a reference of skillset we need, if we want to do all of it by ourselves. Later, we shall dive deeper into what stack is usually recommended.\u00a0Spoiler alert \u2013 There is no one recipe.\n\u00a0\nWeather stations continuously use big data to predict the future\nData Analyst\nThe process of the extraction of information from a given pool of data is called data analytics.\u00a0A data analystextracts the information through several methodologies like data cleaning, data conversion, and data modeling. There are several industries where data analytics is used, such as \u2013 technology, medicine, social science, business etc. Industries can now make careful data-driven decisions because they are able to analyze trends in the market, requirements of their clients and overview their performances with data analysis.\nA Data Analyst is also well versed with several visualization techniques and tools. It is utmost necessary for the data analyst to have presentation skills. This allows them to communicate the results with the team and help them to reach proper solutions.\nData Analytics allows the industries to process fast queries to produce actionable results that are needed in a short duration of time. This restricts data analytics to a more short term growth of the industry where quick action is required.\nData Engineer\nA Data Engineer is a person who specializes in preparing data for analytical usage. S/He develops the foundation for various data operations. A Data Engineer is responsible for designing the format for data scientists and analysts to work on.\nThey need to work with both structured and unstructured data. Data Engineers allow data scientists to carry out their data operations. They have to deal with Big Data where they engage in numerous operations like data cleaning, management, transformation, data deduplication etc.\nA Data Engineer is more experienced with core programming concepts and algorithms. Therole of a data engineer also follows closely to that of a software engineer. This is because a data engineer is assigned to develop platforms and architecture that utilize guidelines of software development. For example, developing a cloud infrastructure to facilitate real-time analysis of data requires various development principles. Therefore, building an interface API is one of the responsibilities of a data engineer.\nFurthermore, a data engineer has a good knowledge of engineering and testing tools. It is up to a data engineer to handle the entire pipelined architecture to handle log errors, agile testing, building fault-tolerant pipelines, administering databases and ensuring a stable pipeline.\nData Scientist\nNowadays, every company is looking for data scientists to increase their performance and optimize their production.\nThere is a massive explosion in data. This explosion is contributed by the advancements in computational technologies like High-Performance Computing. This has given industries a massive opportunity to unearth meaningful information from the data.\nCompanies extract data to analyze and gain insights about various trends and practices. In order to do so, they employ specialized data scientists who possess knowledge of statistical tools and programming skills. Moreover, a data scientist possesses knowledge of machine learning algorithms. These algorithms are responsible for predicting future events. Therefore, data science can be thought of as an ocean that includes all the data operations like data extraction, data processing, data analysis and data prediction to gain necessary insights.\nHowever, Data Science is not a singular field. It is a quantitative field that shares its background with math, statistics and computer programming. With the help of data science, industries are qualified to make careful data-driven decisions.\nThe skills mentioned above can be summarized in the table below:\n\n*** = Very important\n** = Important\n* = Trivial\n\n\n\n\n\u00a0\nData Analyst\nData Engineer\nData Scientist\n\n\n\n\nCalculus and Linear Algebra\n*\n*\n***\n\n\nData Intuition\n**\n**\n***\n\n\nData Visualization and Communication\n***\n**\n***\n\n\nData Wrangling\n*\n***\n***\n\n\nMachine Learning\n*\n*\n***\n\n\nProgramming Tools\n***\n***\n*\n\n\nSoftware engineering\n*\n***\n**\n\n\nStatistics\n**\n**\n***\n\n\n\n\u00a0\nBig Data programming language comparison\nThere is a plethora of programming languages today used for a variety of purposes.\nWe have compared a few in different aspects to make the decision-making process easier:\n\n\n\n\u00a0\nScala\nPython\nR\nJava\nGO\nJulia\n\n\n\n\nSpeed\n\u2713\n\u00a0\n\u00a0\n\u2713\n\u2713\n\u2713\n\n\nEase of use\n\u00a0\n\u2713\n\u2713\n\u2713\n\u2713\n\u2713\n\n\nQuick Learning curve\n\u00a0\n\u2713\n\u00a0\n\u2713\n\u2713\n\u2713\n\n\nData Analysis capability\n\u2713\n\u2713\n\u2713\n\u2713\n\u00a0\n\u00a0\n\n\nGeneral-purpose\n\u2713\n\u2713\n\u00a0\n\u2713\n\u2713\n\u00a0\n\n\nBig Data support\n\u2713\n\u2713\n\u2713\n\u2713\n\u2713\n\u00a0\n\n\nInterfacing with other languages\n\u00a0\n\u2713\n\u2713\n\u00a0\n\u2713\n\u00a0\n\n\nProduction-ready\n\u2713\n\u00a0\n\u00a0\n\u2713\n\u2713\n\u00a0\n\n\n\nA much more detailed list of pros and cons can be found below\nPython\nAdvantages\n\nAI\nMachine Learning\nPredictive analysis\nCan be used with fast big data engines like Apache Spark via the available Python API\nLoads of libraries\nLow learning curve\nPopular libraries to clean and manipulate data \u2013 pandas, NumPy, SciPy are python based\nTensorFlow written in Python\nInteractive computing through Jupyter notebooks\n\nDisadvantages\n\nCommunity data for exploration and learning is not as extensive as that for R\n\nR / Programming with Big Data in R (pbdR)\nAdvantages\n\nMachine Learning\nData Science\nProvides Statiscal models\nGraphical capabilities- useful in visualization patterns and associations\nPackages like GGPLOT2 can further enhance R\u2019s data visualization capabilities and generate high quality graphs\nComes in handy for data visualization and modeling rather than analysis\nSupport for Jupyter notebooks\n\nDisadvantages\n\nSteep learning curve\nSpeed and effeciency issues\nCode written in R is not production-deployable and generally has to be translated to some other programming language such as Python or Java\n\nJava\nAdvantages\n\nHadoop HDFS \u2013 processing and storing big data applications\nETL applications \u2013 apache Camel, Apatar, Apache Kafka\nApache Hadoop based on Java\nLarge ecosystem of tried and tested tools and libraries\n\nSQL\nRetrieving data\nJulia\nAdvantages\n\nHigh performance numerical analysis\nNew \u2013 Capable of general purpose programming\nFaster execution \u2013 complex projects\n30 times quicker than Python; somewhat quicker than C\nBest performancce parallel computing language focussed on numerical computing\n\nDisadvantages\n\nNewer than Python and C\n\nScala\nAdvantages\n\nRuns on JVM\nHigh volume data sets\nFull support for functional programming\nCluster computing framework Apache Spark is written in Scala. Idle for juggling data in a thousand processor cluster and pile of Java legacy code\nLesser lines of code compared to Java\nApache Kafka is written in Scala\nFast and robust\n\nDisadvantages\n\nFewer libraries\n\nMATLAB\nAdvantages\n\nQuick, stable and ensures solid algo foe numerical computing lang\nfourier transforms, signal proccessingm image processing and matrix algebra\nused in statical analysis\n\nTensorFlow\nAdvantages\n\nSoftware lib for numerical computation\nmachine learning framework suitable for large scale data\nGraph can be broken into many chunks that can keep running parallel over various GPUs or CPUs.\nSupports distributed computing\nSecond generation for Google brain\n\nGo\nAdvantages\n\nKubernetes and Docker written in GO\nFast, easy to learn\nFairly easy to write applications in and deploy\nGo-based systems are being used to integrate machine learning and parallel processing of data\nEfficient distributed computing\n\nAWS for a big data project\nBefore analysis the data and making it useful, we need to set up the infrastructure. Setting up and managing data lakes involves a lot of manual and time-consuming tasks such as loading, transforming, securing, and auditing access to data. AWS Lake Formation automates many of those manual steps and reduces the time required to build a successful data lake from months to days.\nSome of the available AWS Services are:\n\nUse cases for AWS services in Big Data\nData Warehousing\nRun SQL and complex, analytic queries against structured and unstructured data in your data warehouse and data lake, without the need for unnecessary data movement.\n\nAmazon RedShift\n\nBig data processing\nQuickly and easily process vast amounts of data in your data lake or on-premises for data engineering, data science development, and collaboration.\n\nAmazon EMR\n\nReal time analytics\nCollect, process, and analyze streaming data, and load data streams directly into your data lakes, data stores, and analytics services so you can respond in real time.\n\nAmazon MSK\nAmazon Kinesis\n\nOperational analytics\nSearch, explore, filter, aggregate, and visualize your data in near real time for application monitoring, log analytics, and clickstream analytics.\n\nAmazon Elasticsearch Services\n\nApart from AWS services, whether it\u2019s a trendy syntax language like Python or more conventional languages like Java and R, choosing the right programming language for big data really comes down to you and your business\u2019 preference.\nWhen starting out, it can be to take advantage of books and other free resources. Doing so can allow beginners to become more familiar with the terminology and build a strong foundation for future development. Those who are looking to make a more streamline move into the field, however, should look for opportunities to gain and practice the skills needed to become an expert data analyst.\nOne of the most efficient ways to do this is through numerous online short and long term courses.\n", "tags": [], "categories": ["Blog", "Data"]}
{"post_id": 23509, "title": "Migrating to the cloud: 6 strategies for future success", "url": "https://www.luminis.eu/blog-en/cloud-en/migrating-to-the-cloud-6-strategies-that-will-help-you-prepare-for-the-future/", "updated_at": "2023-04-20T13:38:30", "body": "Let\u2019s say you\u2019re convinced (and rightly so!) that the future of your organization is in the cloud. How will you get there? Many roads lead to Rome, some more complex and promising than others. In this blog, I will share six strategies for getting into the cloud successfully. Which one fits your organization best?\nChoosing a migration strategy strongly depends on the goals you wish to achieve. Some approaches offer you many strategic opportunities, but they are often a bit more complex to implement. Some options are relatively straightforward but come with higher costs in the long run. Therefore, be aware of the\u00a0why\u00a0of your migration and choose the strategy that best suits your needs.\n\nStrategy 1: Lift & Shift\nAlso called\u00a0rehosting, the idea is simple: you move your systems to the cloud platform with as little change as possible. You basically move your whole data center to the cloud.\nRehosting may be an attractive choice: you\u2019re done relatively quickly, and little risk is involved. However, this strategy has a significant disadvantage: operational costs will be relatively high, especially in the long run. Furthermore, it will take some time to set up your new environment and connect your existing CI/CD solutions.\nDo you need to move out of your data center sooner rather than later? This strategy might be a good fit. But beware:\u00a0the clock is ticking. Once the migration is done, you need to start changing your applications and architecture for a better cloud fit. Monitor your costs meticulously, so you know which applications are good candidates for the next round of refactoring.\nAnother option is to do targeted lifts and shifts. There\u2019s a good chance that a handful of older systems has to move to the cloud as well. You can move them\u00a0at the end of the migration, then phase them out or replace them at a later stage.\nIn short: this can be a useful strategy, but use it wisely and sparingly. Lifting and shifting is a way to quickly get into the cloud, but it comes at the cost of having to do more work afterward.\nStrategy 2: Replatforming\nA.k.a.\u00a0lift-tinker-and-shift. You\u2019re still not changing your systems\u2019 functionalities, but the underlying platform gets an upgrade.\nDo you have a chunky database server with an expensive license running somewhere? Then this is a great option. A\u00a0database-as-a-service\u00a0is a considerable improvement in several ways: no more managing systems and paying for idle time.\nAnother example: a colossal application server running lots of deployments on expensive hardware. Setting up and maintaining clusters of those is very time consuming and complex.\u00a0Moving these applications to Docker containers\u00a0is certainly an attractive option in that perspective.\nBeware the disadvantages, though, which mostly lie in the less visible parts. The details of the cloud\u2019s underlying platform differ subtly from your on-prem\u2019s. Keep the\u00a0fallacies of distributed computing\u00a0in mind and practice\u00a0resiliency.\nNot only does this strategy provide you with the chance to\u00a0shift your IT spending to a radically different model, but it can also greatly improve your organization\u2019s agility. So: lots of\u00a0cloud gold, without touching your architecture. But there\u2019s more to gain when you move towards cloud-native.\nStrategy 3: Refactoring / rearchitecting\nRefactoring (also called: rearchitecting) is the most far-reaching strategy in terms of architectural and applicative change, but it\u2019s\u00a0bursting with potential. By most effectively using what the cloud offers, things that were impossible on-prem are within reach when taking the rearchitecting route. Think unprecedented scalability for minimal costs and using\u00a0services that would conventionally mean huge upfront material and human capital investments. Not to mention time-to-value improvements.\nThe impact of this strategy is mostly dependent on the current state of your system landscape. How tightly coupled are your applications? How modular is your architecture? If you\u2019re already reasonably service-oriented, you\u2019re halfway there. A step towards\u00a0microservices\u00a0\u2014 even better:\u00a0serverless\u00a0\u2014 is not a giant leap from there. It\u2018ll put you right at the cutting edge.\nFortunately, again, this isn\u2019t an all-or-nothing strategy. It\u2019s an excellent fit for migrating applications that exhibit cloud-native characteristics. But it is equally interesting to view rearchitecting from a business value perspective.\nSuppose you could do magic and instantly accelerate your idea-to-production time. Deliver multiple times a day. With fewer bugs. Without having to compromise on security and stability. Which applications would yield the best results then? That part of your portfolio probably benefits the most from the refactoring / rearchitecting strategy.\nStrategy 4: Keep\nRest assured, migrations are not all about doing buzzword bingo heavy work. Not all systems have to be migrated. Some are doing just fine in the data center. You might migrate them later on, or maybe you can switch them off entirely in the future.\nDuring a cloud migration, you will acquire tremendous amounts of technical know-how. But a cloud migration is not a strictly technical affair. Slowly but surely, your organization will start shifting towards a new way of thinking and working (together). Sometimes, systems turn out to be not such a good fit anymore. Sure, they can last for a little while longer, but they need to go sooner or later. Bringing them to the cloud might not be worth the hassle.\nMaybe you\u2019ve just gone through a big systems upgrade project. Or have other reasons just to leave some parts be. It might be a valid option.\nStrategy 5: Phase out\nIt\u2019s not always clear from the get-go, or maybe you just didn\u2019t expect it. But during a migration, you always bump into stuff that can be retired.\u00a0\nFor example, when we helped\u00a0OHRA\u00a0plan and execute their migration from their data center to the AWS cloud, they eventually switched off about 20% of their applications. That saved them a lot of migration work.\nStrategy 6: Replace (with SaaS)\nThis one is aimed at systems that cost a lot but yield little. Replacing them with SaaS solutions is a viable strategy here. A cluster full of mail servers and file servers filled with spreadsheets might be a candidate for replacement with Office 365 or Google Workspace. Salesforce can replace a CRM system, and an on-prem content management system has plenty of SaaS alternatives.\nDeveloping software is costly enough as it is. Save time, energy, and money to build something that sets you apart from the competition.\nWhich cloud migration strategy is right for your organization?\nThere is no such thing as a one-size-fits-all cloud migration strategy. Every organization has a different set of goals and ambitions. Pick a strategy that best fits yours. But keep this in mind: you will reap the most cloud benefits if you try to be as cloud-native as possible. The more you stick to the old data center centric model, the more you will see this reflected in your operational costs.\nWith the right combination of knowledge, experience, and crew, you can make your cloud migration a success story. We\u2019d be happy to help!\nWant to know more? Download our cloud migration white paper\nMy colleague\u00a0Bert Ertman\u00a0and I have written a\u00a0white paper about cloud migrations. In it, we answer the following questions:\n\nWhat\u00a0promises\u00a0does the cloud hold? What are its\u00a0pitfalls?\nWhich migrationstrategies can you use? (You already know)\nWhich people do you need and when?\nHow do you plan a cloud migration?\nWhen can you start with the first migrations?\nHow do you gain migration velocity?\nWhen and how can you complete a migration successfully?\nWhat\u2019s next?\n\nRelated post:cloudwhitepaperWhite paper Cloud migrationCloud migration: 5 effective steps towards a successful result. Download our free white paper. Do you want to migrate your organisation\u2019s systems to the cloud? A cloud migration provides speed and efficiency, among many other advantages. This white paper is...\n", "tags": [], "categories": ["Blog", "Cloud"]}
{"post_id": 23251, "title": "MediGrid improves the speed and quality of medical research", "url": "https://www.luminis.eu/blog-en/news-en/medigrid-improves-the-speed-and-quality-of-medical-research/", "updated_at": "2020-11-25T15:57:15", "body": "During the corona crisis, Luminis Technologies in Apeldoorn worked hard on a solution that improves data quality and research options at various English hospitals, universities and pharmaceutical companies. There is now international interest in this innovation. Luminis Technologies has decided to make this solution more widely available under the name MediGrid.\nRaymond ter Riet, director of Luminis Technologies, says: \u201cAt the end of 2019, Luminis was approached by a large pharmaceutical company because of our innovations in the field of data in the cloud. They worked on a project together with the University of Liverpool and a number of English hospitals. There was a significant challenge there to merge different types of data from all kinds of sources, to improve them in order to subsequently be able to perform real-time analyzes. The next step is to exchange this data in a secure manner, whereby all regulations in the field of privacy and security must be properly followed. \u201d\nIn the months that followed, this suddenly turned out to be a global challenge. Poor data quality and data exchange also proved to cause a lot of delays in the search for COVID-19 treatment methods and vaccines during the corona crisis. Research shows that 50% of hospitals and research institutions still exchange information via paper. E-mail, Excel files and even the fax are still widely used. These inefficient and unsafe methods are an obstacle to a much greater need: international exchange of medical data, so that research into medicines, treatments and side effects can be carried out more efficiently and faster.\nStart with a data strategy\nMartin van Mierloo, product manager of MediGrid, says: \u201cMany parties in the healthcare sector are still fully engaged in digitizing processes. This is step one and a precondition for taking bigger steps. Such as organizing care around the patient and better cooperation between institutions. But digitization alone is not enough. If you don\u2019t think about a data strategy from day one and put data quality at the center of the process, significant challenges in the area of approval and complianc\n\ne arise later on. Research projects where the data is 100% in order ultimately lead to better health and a longer life expectancy. And I like to work for that every day. \u201c\nMediGrid initially focuses on data research from clinical and preclinical trials and research into, among other things, side effects of medicines. The solution is distinguished by an effective way of data management, a fully managed environment and a user-friendly interface.\nRaymond ter Riet: \u201cWe believe in solutions that not only digitize the process surrounding medical data, but also help organizations to take a big step in the field of data processing and collaboration. The corona crisis has clearly shown that there are major problems with medical data processing. We are convinced that our solution helps to greatly improve the speed and quality of investigations. \u201d\nFor more information about MediGrid see the website: https://medigrid.io\nOr you can contact Martin van Mierloo (martin.vanmierloo@luminis.eu, tel +31 6 14 84 15 19)\n", "tags": [], "categories": ["Blog", "News"]}
{"post_id": 3556, "title": "Continuous Delivery with AWS CDK Pipelines", "url": "https://www.luminis.eu/blog-en/cloud-en/continuous-delivery-with-aws-cdk-pipelines/", "updated_at": "2022-06-24T09:56:25", "body": "In this blog post I\u2019m giving an example of how you can automate your deployment pipeline with some bleeding edge AWS technologies. Step by step we will create CI/CD pipeline using AWS CDK, CodeCommit and CodePipeline which is fully defined in TypeScript. At the end we have a simple project which uses so called Pipeline as Code.\nBefore diving into specifics, let me first unravel the title of this blog post: \u201cContinuous Delivery with AWS CDK Pipelines\u201d.\nContinuous Delivery\nI assume you already have an idea what Continuous Delivery means, but for this post I define it as: the discipline of being able to release your software to production at any time. Or like Josh Long says often says: \u201cProduction is my favorite place on the internet. I LOVE production. YOU should love production. You should go as early, and often, as possible. Bring the kids, the whole family. The weather is amazing! It\u2019s the happiest place on earth!\u201d.\nAWS (Amazon Web Services)\nThe first and still the largest public cloud provider.\nCDK (Cloud Development Kit)\nAWS CDK is a software development framework for defining cloud infrastructure in code and provisioning it through AWS CloudFormation. Since about 2011 CloudFormation allowed us to codify the details of an infrastructure into a configuration file. It\u2019s nice to have Infrastructure as Code, but for serious applications it becomes a hell to maintain these humongous YAML files. In 2019 AWS introduced an abstraction on top of Cloud Formation and called it the Cloud Development Kit (CDK) which makes it possible to describe your Cloud Application in a familiar programming language. Currently the AWS CDK supports TypeScript, JavaScript, Python, Java, and C#/.Net. It also introduced the possibility to compose and share reusable custom components, but that\u2019s beyond the scope of this post.\nCDK Pipelines\nA year later, July 2020, AWS introduced CDK Pipelines which makes it easy to setup continuous delivery pipelines with AWS CodePipeline. With this new CDK construct, it becomes easy to define and share \u201cpipelines-as-code\u201d for your application which automatically build, test, and deploy your new version.\u00a0CDK Pipelines are self-updating: if you add new application stages or new stacks, the pipeline automatically reconfigures itself to deploy those new stages and/or stacks.\nTo position CDK in the AWS landscape:  On the lowest level are the AWS Resources like for example lambda, S3 and IAM. These can be provisioned using CloudFormation Stacks. CDK provides a higher level of abstraction to define the resources. These higher level components also provide proven defaults so that you don\u2019t need to be an expert.\nGetting started\nThe focus of this post will be on creating pipelines-as-code, so for demonstration purposes we will create and deploy a simple application. Essentially we are going to create two CDK Applications in one Git repository, 1 CDK application for the Pipeline, and 1 CDK application for software itself. You can continue reading this post in two ways; 1) you continue reading, look at screenshots and believe me this is how it works or 2) you follow along by creating a project yourself and copy the commands and code snippets and feel how easy it is to get a fully automated continuous delivery pipeline. Either way, the end result of this post is available in a public Github repo as well.\nPreparation\nWhen you have not used CDK before, you need to install the command line interface. This can be done using NPM by running: npm install -g aws-cdk\nOtherwise refer to the getting started page for AWS CDK. If you use CDK for the first time in your AWS account, CDK needs to be bootstrapped. This creates a CloudFormation Stack called \u201cCDKToolkit\u201d with all the required resources and permissions for execution CDK commands. So when you haven\u2019t done this, run in a Terminal:\nexport CDK_NEW_BOOTSTRAP=1\ncdk bootstrap\nNow create a new Git repository in CodeCommit for the app. I named my project: \u201caws.blog.cdk-pipelines\u201d. After the repository is created, run in a Terminal on your machine:\ngit clone CODECOMMIT-REPO-URL aws.blog.cdk-pipelines\ncd aws.blog.cdk-pipelines\nMore information about creating repositories in CodeCommit and how to clone them can be found in the AWS documentation. Initialise a new project with the following commands:\ncdk init --language typescript\r\nnpm install --save-dev \\\r\n\taws-cdk@1.66.0 \\\r\n\t@aws-cdk/aws-apigateway@1.66.0 \\\r\n\t@aws-cdk/aws-codebuild@1.66.0 \\\r\n\t@aws-cdk/aws-codecommit@1.66.0 \\\r\n\t@aws-cdk/aws-codedeploy@1.66.0 \\\r\n\t@aws-cdk/aws-codepipeline@1.66.0 \\\r\n\t@aws-cdk/aws-codepipeline-actions@1.66.0 \\\r\n\t@aws-cdk/aws-lambda@1.66.0 \\\r\n\t@aws-cdk/core@1.66.0 \\\r\n\t@aws-cdk/aws-s3@1.66.0 \\\r\n\t@aws-cdk/pipelines@1.66.0\nSince CDK pipelines is still in developer preview I chose to specifically install version 1.66.0, the newest version at the time of writing. This generates a folder structure as follows:  The folder bin contains the code for defining the Application. It has already generated the CDK Application for the pipeline. The folder lib contains the actual Stacks, for now it\u2019s an empty Stack for the pipeline which we will fill in later The folder test contains tests for the Pipeline Application. We have infra as code, and a big advantage of this is that we can define testable infra as code! You can run `npm run test` if you want. In this post we skip testing our pipeline-as-code, so replace the contents of `test/cdk-test.test.ts` with:\ntest('Placeholder test which never fails', () => {\n});\nOtherwise the test will fail after the changes we are about to make. Finally, add the @aws-cdk/core:newStyleStackSynthesis\u00a0feature flag to the project\u2019s cdk.json file. The file will already contain some context values; add this new one inside the context object.\n{\r\n  ...\r\n  \"context\": {\r\n    ...\r\n    \"@aws-cdk/core:newStyleStackSynthesis\": \"true\"\r\n  }\r\n}\nIn a future release of the AWS CDK, \u201cnew style\u201d stack synthesis will become the default, but for now we need to opt in using the feature flag.\nCreate the pipeline\nReplace the contents of `lib/aws.blog.cdk-pipelines-stack.ts` with:\nimport {Repository} from \"@aws-cdk/aws-codecommit\";\r\nimport {Artifact} from \"@aws-cdk/aws-codepipeline\";\r\nimport {CdkPipeline, SimpleSynthAction} from \"@aws-cdk/pipelines\";\r\nimport {CodeCommitSourceAction} from \"@aws-cdk/aws-codepipeline-actions\";\r\nimport {Construct, Stack, StackProps} from \"@aws-cdk/core\";\r\n\r\nexport class AwsBlogCdkPipelinesStack extends Stack {\r\n  constructor(scope: Construct, id: string, props?: StackProps) {\r\n    super(scope, id, props);\r\n\r\n    const repoName = \"aws.blog.cdk-pipelines\";\t// Change this to the name of your repo\r\n    const repo = Repository.fromRepositoryName(this, 'ImportedRepo', repoName);\r\n\r\n    const sourceArtifact = new Artifact();\r\n    const cloudAssemblyArtifact = new Artifact();\r\n\r\n    const pipeline = new CdkPipeline(this, 'Pipeline', {\r\n      pipelineName: 'MyAppPipeline',\r\n      cloudAssemblyArtifact,\r\n\r\n      // Here we use CodeCommit instead of Github\r\n      sourceAction: new CodeCommitSourceAction({\r\n        actionName: 'CodeCommit_Source',\r\n        repository: repo,\r\n        output: sourceArtifact\r\n      }),\r\n\r\n      synthAction: SimpleSynthAction.standardNpmSynth({\r\n        sourceArtifact,\r\n        cloudAssemblyArtifact,\r\n        // Use this if you need a build step (if you're not using ts-node\r\n        // or if you have TypeScript Lambdas that need to be compiled).\r\n        buildCommand: 'npm run build && npm run test',\r\n      }),\r\n    });\r\n    \r\n\r\n    // Here we will add the stages for the Application code later\r\n\r\n\r\n  }\r\n}\nCommit all changes and push it.\ngit add --all\ngit commit -m \"initial commit\"\ngit push\nNow run `cdk deploy`, this will show an overview of all the resources that will be created. When asked \u201cDo you wish to deploy these changes?\u201d enter y. This can take while for the first time so be patient. In AWS Console you can go to CloudFormation to see that a Stack named \u2018AwsBlogCdkPipelinesStack\u2019 is created and how it looks like. In the case you face weird inexplainable issues while deploying the Stack, try removing the folder `cdk.out` and try it again. Now open CodePipeline in the AWS Console and when the Cloudformation Stack is created you should see a pipeline named \u201cAwsBlogCdkPipeline\u201d with 3 stages:  1. Source stage is checking out the Git repository 2. Build stage performs the Synth action, in our case that is building all the code in this project, which until now is only the pipeline code 3. The UpdatePipeline stage performs the SelfMutate action which changes this current pipeline Thanks to the last stage this was the last time we needed the CDK command, the following changes can all be done via Git commits.\nCreate the application\nLet\u2019s start with creating code for the lambda itself. Create `src/greeting.ts` and copy the following contents:\nconst DEPLOY_TIME = process.env.DEPLOY_TIME!\r\nconsole.info(\"I was deployed at: %s\", DEPLOY_TIME);\r\n\r\nexport async function handler(event: any) {\r\n    console.debug(\"Received event: \", event);\r\n    return {\r\n        statusCode: 200,\r\n        body: \"Hello from AWS Lambda, DEPLOY_TIME: \" + DEPLOY_TIME\r\n    };\r\n}\nAnd because we\u2019re well behaving developers, we also create a test for our application code. Create `src/greeting.test.ts` and copy:\nimport {handler} from './greeting'\r\n\r\ndescribe('Test calculationHandler', function () {\r\n    it('Happy flow', async () => {\r\n        let emptyBody = {};\r\n        let event = {body: emptyBody };\r\n\r\n        const result = await handler(event);\r\n        expect(result.statusCode).toEqual(200);\r\n    });\r\n});\nNow open `jest.config.js` in the root of the project folder and remove \u201c/test\u201d from line 2 so that it results in:\nroots: ['<rootDir>'],\nNow also our application code is tested during the build stage of our pipeline (triggered by `SimpleSynthAction.standardNpmSynth()` in `aws.blog.cdk-pipelines-stack.ts`). The test can be executed by running `npm run test` or by pushing some button in your favorite IDE.\nNow that we have a very basic application, let\u2019s create the infra for it. Next to the pipeline stack, we create a second stack for our application. Create the file `lib/aws.blog-lambda-stack.ts` and copy the contents:\nimport {AssetCode, Function, Runtime} from \"@aws-cdk/aws-lambda\"\r\nimport {CfnOutput, Duration, Stack, StackProps} from '@aws-cdk/core';\r\nimport {Construct} from \"@aws-cdk/core/lib/construct-compat\";\r\nimport {LambdaIntegration, RestApi} from \"@aws-cdk/aws-apigateway\"\r\n\r\nexport class AwsBlogLambdaStack extends Stack {\r\n  public readonly urlOutput: CfnOutput;\r\n\r\n  constructor(app: Construct, id: string, props?: StackProps) {\r\n    super(app, id, props);\r\n\r\n    // Configure the lambda\r\n    const lambdaFunc = new Function(this, 'BlogLambda', {\r\n      code: new AssetCode(`./src`),\r\n      handler: 'greeting.handler',\r\n      runtime: Runtime.NODEJS_12_X,\r\n      memorySize: 256,\r\n      timeout: Duration.seconds(10),\r\n      environment: {\r\n        DEPLOY_TIME: new Date().toISOString()   // Example of how we can pass variables to the deployed lambda\r\n      },\r\n    });\r\n\r\n    // Configure API in API Gateway\r\n    const api = new RestApi(this, 'blog-greetingsApi', {\r\n      restApiName: 'Greeting Service'\r\n    });\r\n    // Integration with the lambda on GET method\r\n    api.root.addMethod('GET', new LambdaIntegration(lambdaFunc));\r\n\r\n    // Make the URL part of the outputs of CloudFormation (see the Outputs tab of this stack in the AWS Console)\r\n    this.urlOutput = new CfnOutput(this, 'Url', { value: api.url, });\r\n  }\r\n}\nThis is the definition of the Stack of our Application, which should be added to the pipeline. For this we have to open up the file `lib/aws.blog.cdk-pipelines-stack.ts`. We start with creating a custom Stage for our application, so copy the contents below just after all the imports (above `export class AwsBlogCdkPipelinesStack`):\nexport class AwsBlogApplicationStage extends Stage {\r\n    public readonly urlOutput: CfnOutput;\r\n\r\n    constructor(scope: Construct, id: string, props?: StageProps) {\r\n        super(scope, id);\r\n        const lambdaStack = new AwsBlogLambdaStack(this, 'AwsBlogLambdaStack');\r\n        this.urlOutput = lambdaStack.urlOutput;\r\n    }\r\n}\nLater on it becomes clear why created a custom Stage here. Now scroll down to the bottom. Below the comment:\n// Here we will add the stages for the Application code later\nand paste:\n\tlet testEnv = new AwsBlogApplicationStage(this, 'Test-env');\r\n\tconst testEnvStage = pipeline.addApplicationStage(testEnv);\r\n\nWhen you using a proper IDE it can automatically import the newly used classes for you. Otherwise you can replace all imports with:\nimport {Repository} from \"@aws-cdk/aws-codecommit\";\r\nimport {Artifact} from \"@aws-cdk/aws-codepipeline\";\r\nimport {CdkPipeline, SimpleSynthAction} from \"@aws-cdk/pipelines\";\r\nimport {CodeCommitSourceAction} from \"@aws-cdk/aws-codepipeline-actions\";\r\nimport {CfnOutput, Construct, Stack, StackProps, Stage, StageProps} from \"@aws-cdk/core\";\r\nimport {AwsBlogLambdaStack} from \"./aws.blog-lambda-stack\";\nNow we have everything needed for the pipeline to deploy the stack for the application code. Commit and push this, and follow the progress in Pipeline of the AWS Console. Please be patient, because it can take a while, especially the first time. After the stage UpdatePipeline is finished the Pipeline the new stage for \u2018Test-env\u2019 should be visible.  When you want to access the deployed API, scroll down in the pipeline to the last action \u201cAwsBlogLambdaStack.Deploy\u201d and click on \u201cDetails\u201d to open the Stack in CloudFormation, then go to the tab \u201cOutputs\u201d. The Key \u201cUrl\u201d shows the URL of your freshly deployed API. Just click on it to see that it works. Another trick to retrieve the URL of this API is with the command:\naws cloudformation describe-stacks --stack-name Test-env-AwsBlogLambdaStack \\\n--query \"Stacks[0].Outputs[?OutputKey=='Url'].OutputValue\" \\\n--output text\n\nNow we have a simple, but fundamental base on which we can build upon. Before we wrap up I want to demonstrate how easy it can be to make this pipeline more comprehensive. As you may have noticed, I named last stage \u201cTest-env\u201d. It would be nice to create a production environment as well. But then again, when should we deploy to Prod? Most of the time we want some sort of verification before the code is rolled out to Production.\nCreate the Production environment\nWe will add 1 automated verification step and 1 manual verification step. When both pass, our new commit is allowed to be deployed to Production. Open `lib/aws.blog.cdk-pipelines-stack.js` and scroll down to line 55, below the line we\u2019ve defined `testEnvStage`. Then copy the following contents:\n        testEnvStage.addActions(\r\n            // Add automated verification step in our pipeline \r\n            new ShellScriptAction({\r\n                actionName: 'SmokeTest',\r\n                useOutputs: {\r\n                    ENDPOINT_URL: pipeline.stackOutput(testEnv.urlOutput),\r\n                },\r\n                commands: ['curl -Ssf $ENDPOINT_URL'],\r\n                runOrder: testEnvStage.nextSequentialRunOrder(),\r\n            }),\r\n            // Add manual verification step in our pipeline \r\n            new ManualApprovalAction({\r\n                actionName: 'ManualApproval',\r\n          \t\texternalEntityLink: \"https://hardcoded-url.execute-api.eu-west-1.amazonaws.com/prod/\",\r\n                runOrder: testEnvStage.nextSequentialRunOrder(),\r\n            })\r\n        );\r\n\r\n        // Deploy to the Production environment\r\n        let prodEnv = new MyApplication(this, 'Prod-env');\r\n        const prodStage = pipeline.addApplicationStage(prodEnv);\r\n        // Extra check to be sure that the deployment to Prod was successful\r\n        prodStage.addActions(new ShellScriptAction({\r\n            actionName: 'SmokeTest',\r\n            useOutputs: {\r\n                ENDPOINT_URL: pipeline.stackOutput(prodEnv.urlOutput),\r\n            },\r\n            commands: ['curl -Ssf $ENDPOINT_URL'],\r\n        }));\nAgain, I hope you\u2019re using an IDE which can auto-import the new classes for you. Otherwise replace all the imports on top of the file with:\nimport {Repository} from \"@aws-cdk/aws-codecommit\";\r\nimport {Artifact} from \"@aws-cdk/aws-codepipeline\";\r\nimport {CdkPipeline, ShellScriptAction, SimpleSynthAction} from \"@aws-cdk/pipelines\";\r\nimport {CodeCommitSourceAction, ManualApprovalAction} from \"@aws-cdk/aws-codepipeline-actions\";\r\nimport {CfnOutput, Construct, Stack, StackProps, Stage, StageProps} from \"@aws-cdk/core\";\r\nimport {AwsBlogLambdaStack} from \"./aws.blog-lambda-stack\";\nNow commit and push and open the pipeline in the AWS Console to follow the progress.  Now we added 2 actions to the stage for the test environment. The first is ShellScriptAction which is run directly after the deployment is done. In this example we simply use a cURL command as automated action to verify that our service is up and running. This action will fail when the call does not return a HTTP 200. This action could be changed to invoking a shell script from this repository or even to run an \u201cintegration test suite\u201d. The second step we added is manual verification. At this action the pipeline will hold until someone manually approves or rejects this change. In the screenshot above you may notice a \u201cReview\u201d button, when this is clicked a popup is opened when you can approve or reject the change.  It even has an option to show the URL for review, but I have no idea how we can set it to the URL of our generated environment. In ShellScriptAction we could use the Stack outputs, but that is not (yet?) supported by ManualApprovalAction. As an example I set \u201cexternalEntityLink\u201d to a hardcoded URL, but that is not recommended. Also, when multiple pushes happen simultaneously, it is unclear which change your are reviewing or approving. In such scenario\u2019s you probably want an environment per change. But that\u2019s food for another blog post.\nConclusion\nI demonstrated how you can create your own pipeline in just a few steps while it\u2019s fully defined in version control. The end result of this post is shared in this Github repo. \u00a0See for yourself how little files were needed to get this up-and-running. In my experience you can create a pipeline quite fast yourself using the new CDK pipelines construct.\nIt is still in developer preview and there\u2019s much to add. But this is how it mostly goes with new services in AWS. First get it out to the users, listen to their feedback, and polish it. So I have high hopes that the current shortcomings will be improved. When your projects are already running in existing infrastructure I certainly would not move it directly. But when starting a new AWS project I would definitely use CDK for provisioning the resources and I would strongly consider using the CDK CodePipeline as well. In this example I used the inferior AWS CodeCommit, but you can easily integrate with other systems like Github, Gitlab or Bitbucket.\nCleaning up\nTo avoid unexpected AWS charges, destroy your AWS CDK stacks after you\u2019re done. To be sure, open CloudFormation in the AWS console and manually Delete the stacks \u201cTest-env-AwsBlogLambdaStack\u201d and \u201d Prod-env-AwsBlogLambdaStack\u201d. Then destroy the CDK application with the command:\ncdk destroy AwsBlogCdkPipelineStack\nThis deletes the CloudFormation Stack and all its related resources. The used s3 buckets are emptied, but not automatically deleted. So these should be deleted manually. For me there were several buckets which name started with awsblogcdkpipelinesstack-.\nFinally, delete the AWS CodeCommit repository from the AWS Console.\nReferences\nThis post is based on\u00a0creating a CDK app and supplemented with CDK pipelines HOWTO and finally sprinkled with some research and knowledge of my own. Please refer to these pages when you want more background information. Some more related links:\n\nhttps://cdkworkshop.com/\nhttps://github.com/aws-samples/aws-cdk-examples\nOther ways of deploying your code to AWS:\n\nhttps://sharing.luminis.eu/blog/the-aws-serverless-application-model-a-one-stop-shop-for-your-serverless-apps/\nhttps://sharing.luminis.eu/blog/amplify-your-deployment/\n\n\n\nLearn from our experts:10 Mar 2021-Training: Continuous Integration / Continuous DeliveryIn order to deploy features quickly and reliably, a good continuous integration (CI) and continuous delivery (CD) setup is a prerequisite. A proper CI/CD pipeline automates error prone steps and gives both developers and management confidence in deployment often and...\n", "tags": [], "categories": ["Blog", "Cloud"]}
{"post_id": 23105, "title": "New InformationGrid cloud native infrastructure released", "url": "https://www.luminis.eu/blog-en/news-en/new-informationgrid-cloud-native-infrastructure-released/", "updated_at": "2020-11-25T16:02:51", "body": "On 4 September 2020 a major update of \u00a0InformationGrid was released. Besides almost 30 improvements and bug fixes this release contains a completely new cloud infrastructure.\nOver the last few years InformationGrid used Luminis CloudRTI (Real Time Infrastructure) as the cloud infrastructure. CloudRTI allowed our customers to setup scalable, secure, resilient and managed infrastructures on AWS and Azure with nice features such as container orchestration, zero downtime deployments, ingress management and centralized logging and monitoring. Over the years we\u2019ve noticed that Amazon and Microsoft have added so many new services and tools that is was time to rethink the InformationGrid cloud infrastructure.\nIn this 20.4 release we have rebuilt the InformationGrid cloud infrastructure from the ground up. Important requirements were security-by-design, less required infrastructure knowledge and full lifecycle support (CI/CD, DevOps). We used the available Amazon and Azure services and components where possible.\nThe result is a new cloud native infrastructure based on:\n\nInfrastructure as code (using AWS SDK). Enables a declarative approach and easier, repeatable and better manageable configurations.\nManaged Kubernetes (using AWS EKS/Azure AKS)\nNative and integrated monitoring (Azure monitor)\nEasier and improved deployments and DevOps support (using GitOps)\n\nThis release contains full support for the new native cloud infrastructure on AWS and some building blocks for an Azure cloud native infrastructure, which we hope to fully support in one of the next releases.\nRelease notes\n\u200bThe full release notes can be found here.\nSupport \nIf you have any questions then please send them to support@informationgrid.com or add a ticket (registered users only).\nWant to know more about InformationGrid? \nPlease check out the InformationGrid website.\n", "tags": [], "categories": ["Blog", "News"]}
{"post_id": 3469, "title": "Flexible immutability with Jackson and Lombok", "url": "https://www.luminis.eu/blog-en/development-en/flexible-immutability-with-jackson-and-lombok/", "updated_at": "2020-11-25T16:25:23", "body": "This blog describes how to model JSON serializable data classes in a clean way, allowing for immutability at compile time and flexibility at runtime.\nWhen modelling my data classes, I often find myself combining Lombok and Jackson. However, these frameworks may bite when combining annotations. For example:\n\nSome validations go on the constructor, some don\u2019t.\nSome annotations require a no-args constructor, but this breaks your immutability.\nSome annotations do not combine well with super classes\n\n\nThe following setup demonstrates the usage of both of these frameworks for a data model that is immutable, but remains flexible at runtime. My requirements for this data model are: \n1. Must be immutable \n2. Must be able to distinguish between required and optional fields, regardless of whether we instantiate the object from code or from JSON \n3. Creation of the object must fail with an exception when required fields are missing \nFurthermore, I want to allow for fields to be added at runtime, because sometimes I don\u2019t know exactly what fields may be part of my data classes, outside of the ones I\u2019ve explicitly modelled. This is often the case when working with Elasticsearch, which allows for either a strict or dynamic mapping of fields. Therefore I\u2019m going to add one additional requirement: \n4. Data model allows for flexible addition of properties at runtime \nThis last requirement sounds like it\u2019s conflicting with the earlier requirement that the data model must be immutable, but I\u2019ll show how to achieve both in the next sections.\nImmutability\nWe will first implement an immutable data model with field validation. This validation will work when creating the object, but also when (de-)serializing it later on.\nWe start with the following example data class:\n@Getter\r\n@ToString\r\n@SuperBuilder(toBuilder = true)\r\n@JsonInclude(Include.NON_EMPTY)\r\npublic class PersonMessage extends FlexibleContentMessage {\r\n\r\n    public enum Gender {\r\n        MALE, FEMALE\r\n    }\r\n\r\n    @NonNull\r\n    private final Instant dateOfBirth;\r\n\r\n    @NonNull\r\n    private final Gender gender;\r\n\r\n    @NonNull\r\n    private final String name;\r\n\r\n    private List children = new ArrayList();\r\n\r\n    @JsonCreator\r\n    public PersonMessage(\r\n        @JsonProperty(required = true) Instant dateOfBirth,\r\n        @JsonProperty(required = true) Gender gender,\r\n        @JsonProperty(required = true) String name) {\r\n        this.dateOfBirth = dateOfBirth;\r\n        this.gender = gender;\r\n        this.name = name;\r\n    }\r\n\r\n    @JsonIgnore\r\n    public boolean isAdult() {\r\n        return dateOfBirth.isBefore(Instant.now().minus(Period.ofYears(18)));\r\n    }\r\n}\nTake note of the following things:\n\nI\u2019m using @NonNull on fields to mark properties required for Lombok. These cannot go on the constructor.\nI\u2019m using @JsonProperty on constructor parameters to mark properties required for Jackson. These cannot go on the fields.\nI\u2019m using @JsonCreator on the constructor to indicate that this particular constructor needs to be used for deserialization.\n@JsonIgnore is required in order to avoid serializing the isAdult() method.\nI\u2019m using @Getter only and do not need @Setter, @NoArgsConstructor or @AllArgsConstructor, which would take away from an immutable data model.\nI\u2019m using @SuperBuilder to let Lombok generate a builder which will be the only way of instantiating my data class.\nI can use myInstance.toBuilder() if I want to create a copy of my immutable object because of the usage of toBuilder = true on my @SuperBuilder annotation.\nI\u2019m using @JsonInclude to exclude empty fields when serializing (e.g. null fields, empty lists or strings and such).\n\nThis setup allows us to construct instances of our class with Lombok\u2019s builder pattern with validation that automatically fires for required fields:\nPersonMessage.builder()\r\n    .dateOfBirth(Instant.now())\r\n    .gender(Gender.MALE)\r\n    .name(\"John Doe\")\r\n    .build();\nAlso see this unittest for usage examples.\nFlexibility\nThis satisfies requirements 1 through 3. For requirement 4, I\u2019m creating a super class for the above class to extend:\n\u00a0\n@Getter\r\n@SuperBuilder(toBuilder = true)\r\n@JsonInclude(Include.NON_EMPTY)\r\npublic abstract class FlexibleContentMessage {\r\n\r\n    private final Map otherFields;\r\n\r\n    public FlexibleContentMessage() {\r\n        otherFields = new HashMap();\r\n    }\r\n\r\n    @JsonAnySetter\r\n    private void setOtherFields(String key, Object value) {\r\n        otherFields.put(key, value);\r\n    }\r\n\r\n    @JsonAnyGetter\r\n    public Map getOtherFields() {\r\n        return otherFields;\r\n    }\r\n\r\n    @JsonIgnore\r\n    public Map toMap() {\r\n        return ObjectMapperFactory.getInstance().convertValue(this, ObjectMapperFactory.MAP);\r\n    }\r\n}\nNote the usage of @JsonAnyGetter and @JsonAnySetter. Furthermore, note that the @JsonAnySetter is private. This allows Jackson to set all unmapped fields when deserializing but doesn\u2019t expose the setter to any users of our data model. I\u2019ve also added a toMap() method for ease of use. When doing this, make sure that you reuse your ObjectMapper.\nDependencies\nThe Maven dependencies involved for this setup are as follows:\n\u00a0\n        \r\n            com.fasterxml.jackson.core\r\n            jackson-annotations\r\n            2.11.1\r\n        \r\n        \r\n            com.fasterxml.jackson.core\r\n            jackson-databind\r\n            2.11.1\r\n        \r\n        \r\n            com.fasterxml.jackson.datatype\r\n            jackson-datatype-jsr310\r\n            2.11.1\r\n        \r\n        \r\n            com.fasterxml.jackson.module\r\n            jackson-module-paranamer\r\n            2.11.1\r\n        \r\n\r\n        \r\n            org.projectlombok\r\n            lombok\r\n            1.18.12\r\n        \nThe first two dependencies pull in the Jackson annotations and ObjectMapper functionality, respectively. We use jackson-datatype-jsr310 for proper serialization of the Instant class and use jackson-module-paranamer to help Jackson deserialize without us having to define an empty constructor (and thus taking away from our data model\u2019s immutability). The implementation of all these examples and code snippets can be found on my Github repository here.\n", "tags": [], "categories": ["Blog", "Development"]}
{"post_id": 3427, "title": "How to blend into an OSX/Linux based team with Windows and WSL", "url": "https://www.luminis.eu/blog-en/development-en/how-to-blend-into-an-osx-linux-based-team-with-windows-and-wsl/", "updated_at": "2020-11-25T16:27:20", "body": " What if you get rolled into a development team where everyone uses Mac, but you use Windows and perhaps never even touched a Mac? I\u2019ve been there, and I\u2019m here to share some knowledge on how to blend in and contribute to the team just as well.\nDISCLAIMER: This post is NOT meant to discuss the differences or indifferences in one environment or another. There are plenty of other places to do that.\nSuppose it\u2019s your first workday in a new team. You set up your machine, clone your project\u2019s Git repository on your drive, etc. But there are some things you\u2019ll notice that day:\n\nGuidelines for development processes within the team are written with Linux/OSX in mind;\n(Probably) no one in the team can help with issues you encounter on your machine;\nThere are some wonderful .sh helper scripts for the project that you cannot use.\n\nOf course there\u2019s Docker which also resolves many (if not all) of your problems, but I\u2019d like to talk about something that\u2019s also really cool..\nMeet WSL\nMicrosoft has introduced WSL, or Windows Subsystem for Linux. The first release of WSL would act as a translation layer so that Linux commands (or scripts) could be used on a windows machine. With WSL 2, however, a full native Linux kernel is included. With the translation layer no longer needed, the experience would become much faster.\nSet up WSL\nVersion 2 of WSL is available with version 2004 of Windows 10.\nFirst you will need to enable two components. Go to the start menu and type \u2018windows features\u2019. This will show \u2018Turn Windows features on or off\u2019, go there.\nFrom here, enable \u2018Virtual Machine Platform\u2019 and \u2018Windows Subsystem for Linux\u2019 as shown below.\n\nYou will still need to set the default version of WSL to 2. To do that, open PowerShell and run the following command:\nwsl --set-default-version 2\nIt may return a message about updating the kernel component. Follow the link (this one) to download the installer. Run the command again after installing.\nChoose your Linux distribution\nYou can now install your favorite linux distribution from the Windows Store. In this example I am going to install Ubuntu 20.04 LTS.\nAfter installing the distribution, open it up and set up your username and password.\n\nThat\u2019s it, you\u2019re done! You can already run some .sh scripts if you navigate to a folder and hold [Shift] while rightclicking inside the folder. This will show the \u2018Open Linux Shell here\u2019 option!\n\nDocker Desktop and WSL 2\nFor those who are already using Docker Desktop on Windows, you may or may not have noticed that it now also supports WSL 2!\n\nEnabling WSL 2 in Docker Desktop will greatly improve the resource consumption as it will use the dynamic memory allocation feature. See the Docker docs for more information.\nAlso, if you decide to run Node.js inside docker, IDE software such as IntelliJ support WSL as well. See here.\nWSL is open source\nWhat\u2019s also great about WSL is that it\u2019s open source and maintained on GitHub. There is also a dedicated repo where users can report issues and bugs found while using WSL.\nBut you simply cannot replace a Mac machine with a Windows machine!\nIt goes without saying that one still needs a Mac for native iOS development or building on iOS devices. So remember to discuss this with your future colleagues should you ever run into this situation.\n", "tags": [], "categories": ["Blog", "Development"]}
{"post_id": 3393, "title": "Azure Functions for your long-lasting logic", "url": "https://www.luminis.eu/blog-en/cloud-en/azure-functions-for-your-long-lasting-logic/", "updated_at": "2020-11-09T15:28:53", "body": "Recently I started to study for the renewed Azure AZ-204 exam. My biggest pitfall while studying is not having a clear goal. Therefore, I eventually stop studying. (Please tell me that I\u2019m not the only one\u2026)\nThis blog is a try-out for my new learning routine. By writing blogs, I hope to cover all the topics of the exam and pass it. If this comes to a series of blogs, I probably won\u2019t cover all topics of the exam. I\u2019ll just write about the interesting topics I come across while studying.\nFor those interested in the exam visit the exam page. I\u2019m currently learning by using the free learning paths from Microsoft. Additionally, I\u2019ve watched an Udemy course for the AZ-203 exam (the predecessor of the AZ-204 exam). Finally, I was lucky enough to learn about a big chunk of the subjects in a real project at my employer.\nAzure Durable Functions\nLet\u2019s start with the first subject: Azure Durable Functions. Most of you probably already know about Azure Functions, the serverless implementation of Microsoft. You may know other implementations of serverless by names as: Google Cloud Functions and AWS Lambda.\nAn Azure Function (and serverless in general) is stateless by design. So, when your function execution ends, your state is gone. In most cases this is not a problem at all. Nevertheless, there are scenarios where you need to maintain some state. Let\u2019s say I\u2019m making an application for generating thumbnails. I upload a dozen of images that need to be processed. After that, the thumbnails are generated and I need to upload them to some storage (hint: blob storage, part of the AZ-204 exam \ud83d\ude09).\nYou could make this in a single Azure Function, but you\u2019re limited to the 5-minute maximum duration of the runtime. Sure, this is extendable up to 10 minutes but still not a desirable solution. You should create a function that receives a single picture and transforms it into a thumbnail. Another (parent / orchestrating) function should distribute the work, in case you want to create multiple thumbnails. This is where Azure Durable Functions comes into play.\nA Durable function consists of three separate functions:\n\nA client function, which is responsible for receiving your initial \u2018start\u2019 event. For example, the start event may be a HTTP request. It also starts the orchestrator function.\nAn orchestrator function, describes your actual long-lasting logic. It delegates work to more specific activity functions.\nAn activity function, which executes the actual work; making a thumbnail.\n\nWe\u2019ll walk through some code examples to illustrate how these functions work together.\nThe thumbnail generator\nWe\u2019re going to create a durable function that takes a batch of images and creates thumbnails of these images.\nThe client function will be a function that can be triggered with a HTTP-call. The user must provide an URL or connection string, to a blob storage container, along with this HTTP-call. I\u2019m hosting the pictures in a blob container for this example. You\u2019re free to use any other service to host your pictures, for example an FTP server.\nThe orchestrator function takes the blob storage URL, and starts a separate activity to retrieve URL\u2019s for all images in the blob container. Once the first activity is finished, a new thumbnail activity for each image will be started. They run parallel of each other.\nThe thumbnail activity receives an URL to one specific picture. I\u2019ll use Azure Cogntive Services for generating thumbnails. Azure Cognitive Services is an API service, offered by Microsoft, with different artificial intelligence functionalities. One of the APIs they offer can be used to generate thumbnails in a smart way. The API determines the point of interest in the picture using artificial intelligence, and resizes the image to a specified width and height.\nThe client function\nFor the first function, the client function, we\u2019ll navigate to Visual Studio. Create a new Function App Project like you would normally do when making Azure functions. We\u2019ll make our four functions within this function app. For now, choose to create an empty project.\nNote: for the exam you should know your way around the Azure Portal. You could make these functions via the Azure Portal. Initially this was my plan for this blog. I really do not like development in Azure Portal though. It takes too much time due to the absence of intellisense.\n\nOnce created, I\u2019ll add a function to the function app. I\u2019ll do this using the context menu of the project file in my solution explorer. When you click on this, you\u2019ll be prompted with a pop up. Yet again for choosing a template for your function. But now you\u2019re able to choose for a \u2018Durable Function Orchestration\u2019. This template will create the base of our project. The created file contains three functions:\n\nA HTTP Starter function\nAn Orchestrator function\nAn Activity function\n\n\u00a0\n\nLet\u2019s get started creating the HTTP Starter function. My final implementation of the function looks something this:\n[FunctionName(\"Function2_HttpStart\")]\r\npublic static async Task HttpStart(\r\n    [HttpTrigger(AuthorizationLevel.Anonymous, \"post\")] HttpRequestMessage req,\r\n    [DurableClient] IDurableOrchestrationClient starter,\r\n    ILogger log)\r\n{\r\n    const string orchestratorFunctionName = \"DurableFunctionsOrchestrator\";\r\n\r\n    // Function input comes from the request content.\r\n    var requestBody = await req.Content.ReadAsAsync();\r\n    var instanceId = Guid.NewGuid()\r\n    \r\n    await starter.StartNewAsync(orchestratorFunctionName, instanceId.ToString(), requestBody.BlobConnectionString)\r\n    \r\n    log.LogInformation($\"Received blob url: '{requestBody.BlobConnectionString}'.\");\r\n    log.LogInformation($\"Started orchestration with ID = '{instanceId}'.\")\r\n    \r\n    return starter.CreateCheckStatusResponse(req, instanceId.ToString());\r\n}\nThe orchestrator function\nThe orchestrator function is going to call two different activity functions. But first, I retrieve the connection string for the Azure Blob Storage from the request body. You can start new activities via the injected IDurableOrchestrationContext.\n\n[FunctionName(\"DurableFunctionsOrchestrator\")]\r\npublic static async Task RunOrchestrator(\r\n    [OrchestrationTrigger] IDurableOrchestrationContext context,\r\n    ILogger log)\r\n{\r\n    string connectionString = context.GetInput();\r\n\r\n    log.LogInformation($\"Start durable orchestrator.\");\r\n    var blobUrls = await context.CallActivityAsync(\"GetAllBlobUrlsActivity\", connectionString);\r\n\r\n    var thumbnailActivities = new List();\r\n\r\n    foreach (var url in blobUrls)\r\n    {\r\n        var activity = context.CallActivityAsync(\"GenerateThumbnailActivity\", (url, connectionString));\r\n        thumbnailActivities.Add(activity);\r\n    }\r\n\r\n    var resultUrls = (await Task.WhenAll(thumbnailActivities)).ToList();\r\n\r\n    log.LogInformation($\"Finish durable orchestrator.\");\r\n\r\n    return resultUrls;\r\n}\nThe image URL retrieval activitiy\nThe first activity is just a normal Azure Function, triggered by an ActivityTrigger. This activity reads the original images from the blob storage. For each image an absolute URI is added to a list, which is eventually returned to the orchestrator.\n\n[FunctionName(\"GetAllBlobUrlsActivity\")]\r\npublic static async Task GetAllBlobUrlsActivityAsync([ActivityTrigger] string blobConnectionString, ILogger log)\r\n{\r\n    log.LogInformation($\"Start retrieving images from container\");\r\n    \r\n    var blobContainer = GetImageBlobContainer(blobConnectionString);\r\n    var blobs = await blobContainer.ListBlobsSegmentedAsync(\"original\", true, BlobListingDetails.All, null, null, null, null);\r\n    \r\n    log.LogInformation($\"Received {blobs.Results.Count()} images\");\r\n    \r\n    return blobs.Results\r\n        .Select(blob => blob as CloudBlockBlob)\r\n        .Where(blob => blob != null)\r\n        .Select(blob => blob.Uri.AbsoluteUri);\r\n}\r\n\r\nprivate static CloudBlobContainer GetImageBlobContainer(string blobConnectionString)\r\n{\r\n    var storageAccount = CloudStorageAccount.Parse(blobConnectionString);\r\n    var client = storageAccount.CreateCloudBlobClient();\r\n    return client.GetContainerReference(\"pictures\");\r\n}\nThe thumbnail generating activity\nWe\u2019re going to create a second activity within our file. This is also a pretty standard Azure Function. The Azure Cognitive Service API accepts a URL for generating a thumbnail. So, no need to download the images first. The API needs some information along with the request:\n\nThe thumbnail dimensions\nAPI key\nSmart Cropping attribute\n\n\n[FunctionName(\"GenerateThumbnailActivity\")]\r\npublic static async Task GenerateThumbnailActivityAsync([ActivityTrigger] IDurableActivityContext context, ILogger log)\r\n{\r\n    var imageWidth = 400;\r\n    var imageHeight = 200;\r\n\r\n    var apiUrl = $\"https://[your-service-name].cognitiveservices.azure.com/vision/v1.0/generateThumbnail?width={imageWidth}&height={imageHeight}&smartCropping=true\";\r\n    var apiKey = \"Fill-In-Your-API-Key\";\r\n\r\n    var (imageUrl, blobConnectionString) = context.GetInput();\r\n    var requestBody = JsonConvert.SerializeObject(new { url = imageUrl });\r\n\r\n    using (var client = new HttpClient())\r\n    {\r\n        client.DefaultRequestHeaders.Add(\"Ocp-Apim-Subscription-Key\", apiKey);\r\n        var thumbnailResponse = await client.PostAsync(apiUrl, new StringContent(requestBody, Encoding.UTF8, \"application/json\"));\r\n        var imageName = imageUrl.Split('/').Last();\r\n        \r\n        if (thumbnailResponse.IsSuccessStatusCode)\r\n        {\r\n            var imageStream = await thumbnailResponse.Content.ReadAsStreamAsync();\r\n            var blobContainer = GetImageBlobContainer(blobConnectionString);\r\n            var blob = blobContainer.GetBlockBlobReference($\"thumnail/thumbnail-{imageName}\");\r\n            blob.Properties.ContentType = \"image/jpeg\";\r\n            await blob.UploadFromStreamAsync(imageStream);\r\n\r\n            return blob.Uri.AbsoluteUri;\r\n        }\r\n        else\r\n        {\r\n            return $\"{imageName} Failed\";\r\n        }\r\n    }\r\n}\r\n\r\nprivate static CloudBlobContainer GetImageBlobContainer(string blobConnectionString)\r\n{\r\n    var storageAccount = CloudStorageAccount.Parse(blobConnectionString);\r\n    var client = storageAccount.CreateCloudBlobClient();\r\n    return client.GetContainerReference(\"pictures\");\r\n}\nTakeaways\nThat is how you create your first durable function. When creating the thumbnail generator, I first tried to retrieve the images via a blob input binding. This didn\u2019t work because you can\u2019t do any I/O from the orchestrator function. Therefore, I made a separate activity to retrieve all images. Creating a separate activity for this was a bit more cumbersome, but it offered a cleaner software architecture. Orchestration functions are after all only for orchestrating tasks, and not for retrieving input by themselves.\nThis blog provides a basic understanding of Azure Durable Functions. If you do want to study for one of the Azure exams make sure you read the full documentation.\n", "tags": [], "categories": ["Blog", "Cloud"]}
{"post_id": 22188, "title": "Luminis supports the largest esports event in the world", "url": "https://www.luminis.eu/blog-en/luminis-supports-the-largest-esports-event-in-the-world/", "updated_at": "2020-11-24T21:25:52", "body": "Luminis subsidiary Studio 397 has contributed to a unique esports event in recent days: the virtual edition of the 24 hours of Le Mans. This virtual race was broadcast worldwide by channels such as ESPN, Sky Sports, Eurosport and online channels such as YouTube and Twitch. In total, the reach of this event was more than 63 million people. Because most professional racing classes \u2013 such as Formula 1 \u2013 have not yet started, an unprecedented high level of participants was at the start. A total of 200 drivers, including Formula 1 drivers and professional sim racers, took part in the event. \u00a0\n\nNormally, the 24-hour race is held at the Circuit de la Sarthe in the French city of Le Mans. Due to the corona measures, the race has been postponed to September. The organization then sought cooperation with various esports parties and publishers. They have decided to make Le Mans a virtual event.\nSince the start of the corona crisis, there has been a strong growth in the number of esports events and an increasing interest from publishers and broadcasters. Because many regular matches have been dropped, they find a nice replacement in esports. Studio 397 and Luminis have therefore been involved in all kinds of international esports events in recent months.\nA technical and organizational challenge\nThe virtual 24 hours of Le Mans is organized in a very short time. Studio 397 has been asked to provide the simulation and platform for this virtual edition. Together with partners such as Motorsport, ACO and FIA WEC, we have worked hard in recent weeks to prepare everything. The Ferrari 488 GTE was added to the existing cars so that the Ferrari factory drivers could also participate. The track has been modified with new sponsors and all 50 teams have provided their cars with unique color schemes for this race.\nHans Bossenbroek, CEO of Luminis says: \u201cFrom the first moment that this virtual Le Mans was official, we knew that we had to do everything we could to make this a great success. Not only colleagues from Studio 397, but also several Luminis colleagues have worked hard to prepare everything as well as possible. The entire infrastructure for such an event is very complex. Many parties are also involved, so organizing and communicating quickly and effectively is important. These kinds of processes demand the utmost from us, which is extremely satisfying. \u201d\nNot all went well\nViewers of the race saw that drivers were ran into several technical issues. During the 24 hours there has been a red flag period twice to restart the server, after which the race resumed behind the safety car. It is of course frustrating that this happens, especially for the drivers. As with the real endurance competitions, the technology is put to the extreme.\nHans also saw speed and resilience: \u201cUnfortunately, disruptions cannot always be prevented, certainly not at such a complex event. The point is to fix them quickly and learn from them. I know that the team is already doing an in-depth analysis, so that we can work on improvements for the next race.\u201d\nPositive reactions\nMedia, teams and manufacturers look back positively on the event.\nPierre Fillon, President of the Automobile Club de l\u2019Ouest (ACO): \u201cThis first edition of the 24 Hours of Le Mans Virtual was indeed worthy of the Le Mans name, and it perfectly captured all the excitement, tension and magic that is seen in real life at the Circuit de la Sarthe every June. Our congratulations to all the competitors and all the teams who made this incredible event possible.\u201d\nG\u00e9rard Neveu, CEO of the FIA World Endurance Championship and Le Mans Esports Series: \u201cSuch an event would not have been possible without the enthusiasm and belief of all our partners, starting with the ACO and Motorsport Games and our competitors plus our commercial partners who allowed us to capitalise on this event and share it on a global basis. A huge thank you to everyone involved, especially the fans and the organisation team.\u201d\nDmitry Kozko, CEO of Motorsport Games: \u201cThe 24 Hours of Le Mans Virtual illustrated just what is possible when the worlds of motorsport and esports meet in perfect synergy. The co-operation, dedication and partnership of everyone involved in the project enabled us to take racing esports to the next level. The biggest teams and drivers in racing and in esports were quick to see the potential of the event and that is reflected in these stunning results.\u201d\nMichelin: \u201cNot only did the realism of the rFactor 2 software employed for the inaugural Le Mans 24 Hours Virtual provide endurance racing fans with a welcome chance to enjoy their favourite discipline once more after the recent halt to real-world motorsport, but it also gave video game fans an opportunity to discover motor racing from a fresh perspective that introduced them to the important role that tyres play in motorsport.\u201d\nJoshua Rogers, pole-sitter and GTE winner at the 24 Hours of Le Mans Virtual with Porsche Esports Team: \u201cThe result shows how much we\u2019ve put into this. The competition was insanely strong. I think it\u2019s without a doubt one of the strongest grids in a sim race ever.\u201d\nFrits van Eldik, autosports fotographer: \u201cIt is really impressive how good the virtual 24 Hours of Le Mans looks like. Initially, the only drawback I could find was the light at the beginning of the race. But as the day came to an end, that light became incredibly realistic.\u201d\nHighlights\nWatch a video with the highlights of the race here:\n\nFor questions or more information please contact Martin van Mierloo, martin.vanmierloo@luminis.eu, tel. +31 6 14841519.\n", "tags": [], "categories": ["Blog", "News"]}
{"post_id": 21899, "title": "Luminis develops booking platform for the first Official F1\u00ae Racing Centre", "url": "https://www.luminis.eu/blog-en/luminis-ontwikkelt-reserveringsplatform-voor-allereerste-the-official-f1-racing-centre/", "updated_at": "2023-03-28T17:49:39", "body": "On June 1, 2020, the very first The Official F1\u00ae Racing Center opens its doors in Utrecht, the Netherlands. At this new location in The Wall along the A2 near Utrecht, visitors can race against each other individually or in groups on professional hardware. Luminis developed the reservation platform based on InformationGrid.\nLuminis was asked to provide a booking platform that made reservations and payment quick and easy. InformationGrid was chosen as the development and data platform because the reservation platform had to be delivered in a short time. In addition, the race center wants to keep a good overview of the customer and reservation dates. InformationGrid enables The Official F1\u00ae Racing Center to easily add new functionality and easily expand to multiple locations in the coming years.\n\nLuminis business unit \u201cStudio 397\u201d focuses on professional race simulator software and competition infrastructure. During the development of the esports concept, the organization came into contact with Studio 397. From there, discussions arose about the infrastructure for the Official F1\u00ae Racing Center.\n\u201cGiven the ever-growing popularity of Formula 1 in the Netherlands, we are delighted to open the first Official F1\u00ae Racing Center together with F1 in Utrecht,\u201d said director Niels Roodenburg. \u201cVisitors of all ages and abilities are welcome here to experience the joy of driving a virtual F1 car. Formula 1 is here for everyone! \u201d\nFor news and more information see the website of The Official F1\u00ae Racing Center: https://www.racesquare.com\n", "tags": [], "categories": ["Blog"]}
{"post_id": 3310, "title": "Staying connected while distancing from your lovely colleagues", "url": "https://www.luminis.eu/blog-en/development-en/staying-connected-while-distancing-from-your-lovely-colleagues/", "updated_at": "2020-11-25T16:28:06", "body": "Seems that the effects of COVID-19 are here to stay. Although the effect is heavy on all of us, one doesn\u2019t have to take it all negatively. Apart from people getting to spending more time with their families(fortunately, or unfortunately\ud83d\ude09), it\u2019s time to get creative. Activate that right side of the brain. It\u2019s time to think out of the box.\nWhat we need to do is virtual team building and also, looking out for our colleagues who are a bit more affected by the situation than others. I had the opportunity to work with 2 teams in this time. Also, had a chance to witness how my spouse, who is working with multiple teams, is continuously striving to improve the team spirit.\nThe age range of all colleagues I came across with is 24-50ish. And the living situations differ from living alone in an apartment in Amsterdam to living with a family of 7 and a newborn. Some people were doing just fine, while others were struggling to find motivation to do their daily tasks.\nDuring these times, I felt blessed to see how humane all the colleagues around me are. Their actions brought the remote teams together and helped build a trustful, meaningful connection through the team. And I can ensure you that the whole team felt valued and seen.\nFew issues people are facing is the feeling of being disconnected, unmotivated and unsupported. Virtual team building can be used as an alternative to the \u201cwatercooler\u201d to reduce these effects. It can provide an intentional space where remote co-workers get to catch up and socialize and connect in a way that they might not have considered otherwise.\nThe requirements for our story: \u2013\nA leader \u2013 Probably the team manager/scrum master or anyone who keep the ball rolling in usual times.\nTime \u2013 Set aside some dedicated time for the team building. Team members must give undivided attention during team building. Otherwise, members will fall off uninterested or uninvolved.\nScreen-sharing tools or some collaboration platform\n\u00a0\nNow, here are a few things I saw work, and a few more that I thought of \u2013\n\nReally KNOW your colleague \u2013 Some questions to start the meetings with. Get everyone\u2019s attention and get the brain going.\n\nName one random thing on your desk that doesn\u2019t belong. And share its back story.\nwhat is the food that you could eat every week for the rest of your life?\nWhat is the best trip you have ever taken?\nWho is wearing pants \ud83d\ude1b (Use with caution)\nTell us 3 things, two of which are false. Let the others guess or let them keep guessing which one is true.\n\u2026\n\n\nShare your baby photo. Who was cutest in good old days?\nShare a goal\n\nLet\u2019s do the standup in tree pose and build up strength. Will keep stand ups short for sure. \ud83d\ude09\nWho can do max push-ups? Let\u2019s work on it and compete weekly.\n\u2026\n\n\nHobby talk. Everyone gets x minutes to tell about theirs.\nCooking (themed)competition. Points for presentation. Spouse rates the dish and the kitchen mess \ud83d\ude09\nVirtual happy hour. Get those bottoms up at the end of the week, or day!!!\nVirtual walk \u2013 take your phone and your colleague on facetime outside for a walk.\nMarco Polo Check-in \u2013 Video Walkie talkie. Got your attention. Didn\u2019t I? Get your team to download the app and every morning, record a message with the day\u2019s check-in/ prompt. Then invite everyone to respond with a short video.\n\nOne word to describe your morning\nBird making music outside your house\nWhat are you having for breakfast?\n\n\nSURPRISE in May/Jun/\u2026 With the Easter that went by, my husband\u2019s company sent the employees an Easter breakfast basket. It was awesome. My company personally delivered pizza ingredients for a virtual pizza session one afternoon. It was both a surprise and a particular valued feeling we got from those gestures. It doesn\u2019t have to be the management to do it. You can do it for your small teams too.\nNever Have I ever \u2013 Play a PG-version of it with your team, no matter what field you are in.\n\u2026\n\nThe times have changed. Although sudden, we can still make the best out of it. We can aim to help each other try. It\u2019s time for collaborating and compassion. Stay healthy, stay safe. \ud83d\ude4f\n", "tags": [], "categories": ["Blog", "Development"]}
{"post_id": 3230, "title": "The AWS Serverless Application Model: A One-Stop-Shop for your serverless apps", "url": "https://www.luminis.eu/blog-en/cloud-en/the-aws-serverless-application-model-a-one-stop-shop-for-your-serverless-apps/", "updated_at": "2020-11-17T15:02:37", "body": "It won\u2019t surprise you when I say that the cloud train has left the station a while ago and is gaining more and more speed every day.\nBecause this trend doesn\u2019t seem to stop it is important to be able to act fast on your changing environment to meet your customers demand with the least effort and time involved. The most obvious way to achieve this is to be as cloud native as possible. This means that you have to aim to use the building blocks that your cloud provider provides. One of these building blocks the Amazon AWS cloud offers, is the Serverless Application Model.\nIn this blogpost I will explain and demonstrate the basics this framework has to offer.\n\nWhat is the Serverless Application Model?\n\u00a0\n\nThe AWS Serverless application model is a framework that can be used to build serverless applications. This framework consist of 2 components:\n\nThe AWS SAM template specification\n\nThis specification is used to define your serverless application. It provides a clean and simple syntax to describe all necessary components to make your serverless application. The AWS SAM template specification is related and shares syntax with AWS Cloudformation. \n\nThe AWS SAM command line interface\n\nThe AWS SAM CLI can be used to build serverless applications that you define using the template specification. It also enables you to verify the templates written and invoke lambda functions locally. The CLI can also be used to package and deploy serverless applications to the AWS cloud.\nWhy should you use AWS SAM?\nUsing AWS SAM provides a lot of benefits regarding the development of serverless applications on the AWS cloud:\n\nSingle-deployment configuration. All required components are defined in one template.\nExtension of AWS Cloudformation. AWS SAM is an extension of AWS Cloudformation which brings us these reliable deployment capabilities for free.\nBuilt-in best practices. AWS SAM can be used to deploy your required infrastructure components as configuration. This enables you to check-in your template definition into your VCS and apply software development best practises like code reviews and such.\nLocal debugging and testing: The AWS SAM makes it possible to locally build , test and debug your serverless applications your have defined in your SAM template. With the help of Docker it is also possible to locally test your application against other infrastructure components like DynamoDB.\nDeep integration with development tools. The AWS SAM makes it fairly simple to setup a Code deploy pipeline to build and deploy your serverless application on every vcs change. \n\n\nA Simple yet complete REST-API\nTo demonstrate the capabilities of the SAM, we are going to create a REST-API that exposes CRUD actions on a DynamoDB. This app will cover most of the basic functionalities the Serverless Application Model provides\nThe high-level services that are part of AWS SAM are:\n\nLambdas\nAPI Gateway\nDynamoDB Tables\n\nFor more servies you can add other resources as specified in the Cloudformation specification.\nLambdas\n\nLambdas are the one of the most pure serverless component: It empowers a developer to be able to focus on writing the business logic only without the need to worry about boilerplate and infrastructure. Just upload your code/ binary to s3 and configure the lambda with the required instructions regarding environment variables and/ or credentials and you are good to go. The best part is that you only have to pay for the time your lambda is actually doing work! Our app will consist of several lambdas which will ensure the correct data is stored and retrieved from storage.\n\nAPI Gateway\n\nIn order to access the business logic that we put in our lambdas that cover the app functionality, we need to have a way to interact with this functionality from the internet. The most obvious way to do that is by deploying an API Gateway. An API Gateway can be configured to invoke various services available within the AWS landscape, like for example Lambdas. It supports various authentication mechanisms, out-of-the-box request logging and more advanced features like request throttling. The API Gateway can do some transformations before forwarding the request to the service of destination too. This feature can be very powerful. For example: when your lambdas evolve faster than your api users can keep up with, you can use transformations to convert the incoming request to the datamodel the lambda expects. Or vice versa: transform the response of your lambda to match the model of the defined api contract.\n\nPersistence\n\nThe AWS SAM framework has support for DynamoDB which is profiled as a serverless document database solution \u2018for any scale\u2019TM. Since our requirements are not that fancy, DynamoDB will fit our requirements just fine: we only have to store our entries somehow and we need to be able to fetch them by specific criteria and be able to update an entry.\n\nPutting it all together\nWe will create/ deploy the following resources using the Serverless Application Model:\n\nA few Lambda Functions\nAn API Gateway\nA DynamoDB table\n\n\u00a0\nI\u2019ve already created an application to illustrate the capabilities of the Serverless Application Model framework and it is available on GitHub. This application consists of 3 Lambdas that use DynamoDB to retrieve, store and update entries.\nThe Template\nNow the Serverless application model magic happens. The template is the key component in this framework and will make sure AWS will create the desired resources in the cloud. Lets highlight some parts of the template than can be found in the before mentioned GitHub repository\n\u00a0\nAn excerpt of the template.yml file\nAWSTemplateFormatVersion: '2010-09-09'\r\nTransform: AWS::Serverless-2016-10-31\r\nDescription: >\r\n  This template describes the resources to be deployed for the Todo app\r\n  \r\nGlobals:\r\n  Function:\r\n    Timeout: 3\r\n\r\nResources:\r\n  ListTodosFunction:\r\n    Type: AWS::Serverless::Function\r\n    Properties:\r\n      CodeUri: functions/list-todos\r\n      Handler: list-todos.lambdaHandler\r\n      Runtime: nodejs12.x\r\n      Policies:\r\n        - DynamoDBCrudPolicy:\r\n            TableName: !Select [1, !Split ['/', !GetAtt DynamoTodosTable.Arn]]\r\n      Environment:\r\n        Variables:\r\n          TABLE_NAME: !Select [1, !Split ['/', !GetAtt DynamoTodosTable.Arn]]\r\n      Events:\r\n        ListTodos:\r\n          Type: Api\r\n          Properties:\r\n            Path: /list\r\n            Method: get\nMost of this template.yml looks quite similar to a Cloudformation template, and in a SAM-template the Resources section is the most important/ interesting part too.\nLambdas\nWe see that there is a function definition of type AWS::Serverless::Function that points to function code. Besides the type this is similar to a Lambda definition in a Cloudformation template. However, this function definition also defines an event of type Api which is an additional property the SAM framework provides.\nWhen an event is defined of type Api, the Serverless Application Model will deploy an API gateway for you! As can be seen in the template.yml, there is also a Path defined. The framework will automatically configure the API Gateway to route all HTTP GET requests for /list to the Lambda function this event is defined for!\nTo find out what all available options for functions are, please refer to the resource specification.\nDynamoDB\nAs mentioned earlier, the Lambda functions require a DynamoDB table to store, update and retrieve entries. As you can see the template also contains a resource definition with name DynamoTodosTable and type\u00a0AWS::Serverless::SimpleTable. This is the most simple way to define a DynamoDB table requiring minimal configuration:\n\u00a0\n\n  DynamoTodosTable:\r\n    Type: AWS::Serverless::SimpleTable # if you want to define a more complex table, use AWS::DynamoDB::Table\r\n    TableName: !Ref TableName\r\n    PrimaryKey:\r\n      Name: todo-id\r\n      Type: String\r\n    BillingMode: PROVISIONED\r\n    ProvisionedThroughput:\r\n      ReadCapacityUnit: 1\r\n      WriteCapacityUnits: 1\r\n    Tags:\r\n      AppType: Serverless\nThe DynamoDB simple table definition can be configured via various parameters, like providing a meaningful table name, setting the number of provisioned read and write units and what the primary key look like.\nDeployment\n\u00a0\nTo deploy the application, we have to first build the application. Assuming you have installed the SAM CLI (if not, follow the instructions here), execute the sam build command:\n\u00a0\n\n$ sam build\r\nBuilding resource 'ListTodosFunction'\r\nRunning NodejsNpmBuilder:NpmPack\r\nRunning NodejsNpmBuilder:CopyNpmrc\r\nRunning NodejsNpmBuilder:CopySource\r\nRunning NodejsNpmBuilder:NpmInstall\r\nRunning NodejsNpmBuilder:CleanUpNpmrc\r\nBuilding resource 'UpdateTodoFunction'\r\nRunning NodejsNpmBuilder:NpmPack\r\nRunning NodejsNpmBuilder:CopyNpmrc\r\nRunning NodejsNpmBuilder:CopySource\r\nRunning NodejsNpmBuilder:NpmInstall\r\nRunning NodejsNpmBuilder:CleanUpNpmrc\r\nBuilding resource 'CreateTodoFunction'\r\nRunning NodejsNpmBuilder:NpmPack\r\nRunning NodejsNpmBuilder:CopyNpmrc\r\nRunning NodejsNpmBuilder:CopySource\r\nRunning NodejsNpmBuilder:NpmInstall\r\nRunning NodejsNpmBuilder:CleanUpNpmrc\r\n\r\nBuild Succeeded\r\n\r\nBuilt Artifacts  : .aws-sam/build\r\nBuilt Template   : .aws-sam/build/template.yaml\r\n\r\nCommands you can use next\r\n=========================\r\n[*] Invoke Function: sam local invoke\r\n[*] Deploy: sam deploy --guided\r\n$\nThis command creates an application that is ready to be deployed by Cloudformation. You can review the result of this action in the folder .aws-sam/build.\nNow it is time to deploy the application to the AWS Cloud. To do so, execute the sam deploy --guided command in a terminal window. The --guided flag will cause the SAM CLI to prompt with some questions. Luckily, the answers can be saved for future use in a samconfig.toml file and will be picked up by the SAM CLI automatically for future executions.\n\u00a0\n\n% sam deploy --guided\r\n\r\nConfiguring SAM deploy\r\n======================\r\n\r\n        Looking for samconfig.toml :  Not Found\r\n        Reading default arguments  :  Success\r\n\r\n        Setting default arguments for 'sam deploy'\r\n        =========================================\r\n        Stack Name [todos-app]: todos-app\r\n        AWS Region [eu-west-1]: eu-west-1\r\n        Parameter TableName [my-todos]: my-todos\r\n        #Shows you resources changes to be deployed and require a 'Y' to initiate deploy\r\n        Confirm changes before deploy [y/N]: y\r\n        #SAM needs permission to be able to create roles to connect to the resources in your template\r\n        Allow SAM CLI IAM role creation [Y/n]: Y\r\n        Save arguments to samconfig.toml [Y/n]: Y\r\n\r\n        Looking for resources needed for deployment: Found!\r\n\r\n\r\nInitiating deployment\r\n=====================\r\n\r\nWaiting for changeset to be created..\r\n\r\nCloudFormation stack changeset\r\n---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\r\nOperation                                                                               LogicalResourceId                                                                       ResourceType                                                                          \r\n---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\r\n* Modify                                                                                CreateTodoFunction                                                                      AWS::Lambda::Function                                                                 \r\n* Modify                                                                                ListTodosFunction                                                                       AWS::Lambda::Function                                                                 \r\n* Modify                                                                                ServerlessRestApi                                                                       AWS::ApiGateway::RestApi                                                              \r\n* Modify                                                                                UpdateTodoFunction                                                                      AWS::Lambda::Function                                                                 \r\n---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\r\n\r\nChangeset created successfully. arn:aws:cloudformation:eu-west-1:*********:changeSet/samcli-deploy15******/338abafc-adb7-489f-b677-1f87c4d6ab31\r\n\r\n\r\nPreviewing CloudFormation changeset before deployment\r\n======================================================\r\nDeploy this changeset? [y/N]:\r\n\nThe procedure will pause when it is ready for deployment, and wants you to confirm the deployment. It is possible to enable auto confirmation in the samconfig.toml file.\n\nDeploy this changeset? [y/N]: y\r\n\r\nCloudFormation events from changeset\r\n---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\r\nResourceStatus                                                    ResourceType                                                      LogicalResourceId                                                 ResourceStatusReason                                            \r\n---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\r\nUPDATE_IN_PROGRESS                                                AWS::Lambda::Function                                             ListTodosFunction                                                 -                                                               \r\nUPDATE_IN_PROGRESS                                                AWS::Lambda::Function                                             UpdateTodoFunction                                                -                                                               \r\nUPDATE_IN_PROGRESS                                                AWS::Lambda::Function                                             CreateTodoFunction                                                -                                                               \r\nUPDATE_COMPLETE                                                   AWS::Lambda::Function                                             UpdateTodoFunction                                                -                                                               \r\nUPDATE_COMPLETE                                                   AWS::Lambda::Function                                             CreateTodoFunction                                                -                                                               \r\nUPDATE_COMPLETE                                                   AWS::Lambda::Function                                             ListTodosFunction                                                 -                                                               \r\nUPDATE_COMPLETE_CLEANUP_IN_PROGRESS                               AWS::CloudFormation::Stack                                        todos-app                                                         -                                                               \r\nUPDATE_COMPLETE                                                   AWS::CloudFormation::Stack                                        todos-app                                                         -                                                               \r\n---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\r\n\r\nCloudFormation outputs from deployed stack\r\n----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\r\nOutputs                                                                                                                                                                                                                                                              \r\n----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\r\nKey                 TodoApi                                                                                                                                                                                                                                          \r\nDescription         API Gateway endpoint URL for Prod stage for the todos functions                                                                                                                                                                                  \r\nValue               https://8wthljigzd.execute-api.eu-west-1.amazonaws.com/Prod/                                                                                                                                                                                     \r\n----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\r\n\r\nSuccessfully created/updated stack - todos-app in eu-west-1\r\n\r\n\r\n\nAs you can see the url where we can find our deployed endpoints is printed in the console.\nValidating our deployment\nLet\u2019s store a todo by executing a HTTP POST request using curl:\n\n$ curl -i --location --request POST 'https://8wthljigzd.execute-api.eu-west-1.amazonaws.com/Prod/' \\ --header 'Content-Type: application/json' \\ --data-raw '{ \"activity\": \"Write a Blog about AWS SAM\", \"done\": false }' \r\nHTTP/2 201 \r\nx-created-item-id: fZbbSVdm-\nLets use curl again to issue an update on the created todo, where we use the identifier as path variable:\n$ curl -i --location --request PUT 'https://8wthljigzd.execute-api.eu-west-1.amazonaws.com/Prod/update/fZbbSVdm-' \\ --header 'Content-Type: application/json' \\ --data-raw '{\"activity\": \"Write a Blog about AWS SAM\",\"done\": true}' \r\nHTTP/2 204\nNow, to check if the todo is updated, let\u2019s list all the todo\u2019s we have in our database:\n$ curl -i https://8wthljigzd.execute-api.eu-west-1.amazonaws.com/Prod/list\r\nHTTP/2 200 \r\n\r\n[{\"id\":\"fZbbSVdm-\",\"activity\":\"Write a Blog about AWS SAM\",\"done\":true}]\nAs we can see the list endpoint returns exactly what we expected to see: An updated todo item!\n\u00a0\n", "tags": [], "categories": ["Blog", "Cloud"]}
{"post_id": 3257, "title": "Amplify your deployment!", "url": "https://www.luminis.eu/blog-en/cloud-en/amplify-your-deployment/", "updated_at": "2020-11-18T14:46:49", "body": "In my previous blog post I gave an introduction to the AWS Amplify framework and how you can use it to generate an AWS backend and use it in your application. \nIn this blog post we are going to take things one step further and use the Amplify Console to set up a deployment pipeline which will be triggered whenever we make a commit to Git. Amplify will then build, test and deploy our application for us. \nGoal of the article\nThe Goal of this article is to have a setup so that every time you make a commit to Git, your frontend application is built, tested and deployed to S3. Your AWS cloud resources also all get updated. You will be able to get an overview of all deployments in the Amplify console and also see screenshots that are made of the home page in order to show that the app has been deployed correctly.\nConnect to Git repository\nThe first step is to connect with your Git repository. This article assumes that you already have an Amplify project set up. If not, follow the instructions in my previous article. The first thing you need to do is to log in to the AWS console and search for Amplify. There you will see the option to \u201cconnect app\u201d. From this point you have the option to connect to repositories on GitHub, BitBucket, GitLab, AWS CodeCommit and \u201cDeploy without Git provider\u201d. I am not sure how the last one works and I will omit this option for the remainder of the article.\n\nFor this article I am going to use GitLab, but feel free to use any option. Once you select one, you will need to log in. After this, you need to select a repository and a branch. For this example I will use my \u201camplify-deployment\u201d dummy project and use the \u201cmaster\u201d branch. This project has a simple React page and also Amplify API and Function that are empty.\nOnce you are authenticated and have selected your repository and branch, you can configure your build settings. The following build settings are the default:\n\nThis is the YAML that represents the entire build and deployment for your setup. It is divided into backend and frontend. In the backend we have only one command, that is the \u201camplifyPush \u2013simple\u201d, which is a command similar to running \u201camplify push -y\u201d in the CLI on your own. It will push all changes to your AWS backend configuration that you have checked in to your repository to AWS.\nThe frontend part is more complicated. It first defines two phases. The first is the pre-build. This is where you would install your dependencies and setup anything you need to do before actually building your application. My advice would be to add a \u201cprebuild.sh\u201d script with everything in it and call it here. Then we have the build phase. It is now using Yarn, but you can also change it to NPM if you want.\nFor the frontend you also need to define where the artifacts will be. These are the files that need to get pushed to S3, where your frontend is hosted normally when you use Amplify. In this case we are telling it that it needs to take ALL files in the build directory and push them to S3. Finally, we also tell the build settings to cache the NPM modules so that we don\u2019t have to install them from scratch every time.\nYou can edit this configuration here or you can download it as a YAML file and check it in to your repository. I prefer the latter, but feel free to choose which one you like best. If you do choose to download it, make sure to give it the name \u201camplify.yml\u201d and add it to the root of your project.\n\nFinally you will get a summary of your settings and you are ready to go.\nBuild in action\nOnce you click on \u201cSave and deploy\u201d it will immediately try to run the build and deploy it. You will get to see the following:\n\nWhich shows you the progress of your build. Clicking on \u201cProvision\u201d will you show you more detailed information of what is going on during this step:\n\nWe can see that in this case it is provisioning an Amazon Linux 2 container. You can scroll through this output to see what kind of dependencies are available to you immediately. If you need to install anything else, you will need to update your amplify.yml file to download this.\nWhen clicking on the \u201cbuild\u201d tab, you will see the progress of the Git repo checkout, the frontend and the backend parts of your build:\n\nAnd there is also a verify step that checks the landing page of your application on different size screens:\n\nLastly, this build gets a special domain that is shown here:\n\nNotice that the name of the branch is incorporated into the domain. This means you can have a different domain per branch. Clicking on this will open your hosted application, which has been deployed by your new Amplify deployment pipeline.\nConclusion\nI hope this has given you an idea of what is possible with the Amplify console and how easy it is to set it up if you have an Amplify project. For more information on what you can do, including adding a testing phase, check the official documentation. For more information about the pricing, visit this site.\n", "tags": [], "categories": ["Blog", "Cloud"]}
{"post_id": 21667, "title": "Luminis ISO 270001 certified despite the corona crisis", "url": "https://www.luminis.eu/blog-en/luminis-ondanks-de-coronacrisis-iso-270001-gecertificeerd/", "updated_at": "2020-06-09T11:16:55", "body": "Luminis passed the ISO 27001 audit without any remarks this week. This shows that Luminis meets the requirements regarding information security and is well structured in terms of processes. For customers, this means that services and product development proceed according to predictable steps.\nArjan Schaaf, senior cloud architect, is responsible for the implementation of ISO 27001 within Luminis. Arjan is proud that the certification was successful: \u201cAfter the last test, we still had a number of points for attention. We solved these immediately. We also had confidence in a good outcome. \u201d Certainly in the last few weeks, the process went quite differently: \u201cDue to the corona crisis, the audit was carried out remotely. This was not a problem for the document assessment and the interviews. With the physical inspection, I walked through our office with a camera while the auditor watched live and asked questions. It was also a special situation for the auditor!\u201d.\nArjan Schaaf \u2013 projectleider ISO 27001 certificering\nRaymond ter Riet, director of Luminis Technologies, is also pleased that the ISO certification has been completed: \u201cWe consider information security essential for all the products and services we provide to our customers. Due to the rapid growth of Luminis, we have started to serve increasingly larger customers. Also in sectors where information security must meet the highest standards, for example in the health and education sector. More and more customers are using our data-as-a-service platform InformationGrid and use this platform to capture large amounts of confidential information. Of course we have always paid a lot of attention to securing the data of our customers. With this certification, we now also formally demonstrate that we have properly organized our organization, solutions and processes for this.\u201d\nClearly defined processes\nWas it an easy process? Arjan explains: \u201cA large group of colleagues has worked hard on this. As a technology company, we have excellent security and technical matters. We have a cloud infrastructure that allows 100% remote working, so we were able to switch immediately when we were no longer allowed to come to the office. We have spent even more time on documenting our processes. This directly helps us in our further growth, because we have clearer processes and new employees can always check our processes and way of working.\u201d\nNew certifications\nAnd are there any new plans for certification Arjan? \u201cWe are Amazon Web Services consulting partner and Microsoft Gold partner. This means that many colleagues are engaged in certifications in the field of the latest cloud technology several times a year. We have already helped many organizations in the cloud field, and have seen further acceleration in recent months due to the corona crisis. That is why we will be drawing even more attention to AWS and Microsoft Azure certifications in the near future. \u201d\nMore information\nIf you have any questions you can contact Luminis.\n", "tags": [], "categories": ["Blog"]}
{"post_id": 3233, "title": "Create a Robust Logstash configuration", "url": "https://www.luminis.eu/blog-en/search-en/robust-logstash-configuration/", "updated_at": "2020-11-30T15:39:54", "body": "The past week I was going over an ELK configuration for log processing at a customer. Like always, there were some things to improve. Before diving into the configuration, I wanted to have a look at a few options that Logstash provides these days. I started a small experiment to investigate using pipelines, and persistent queues. To make it run on my laptop I decided to have a look at a docker-compose configuration. In this blog post, you will learn about some of my findings to make this experiment work.\nContext of the experiment\nThe main goal for the experiment is to make a more robust Logstash configuration for a project with a lot of steps in the filter part and some performance issues on the UDP input. Using the most recent version of Logstash (7.6 at the time of writing), I want to implement an example using multiple pipelines, one for the input of UDP messages, one for the output to elasticsearch using multiple indexes, and more than one for the filter part of different paths. The sub-goal for the experiment is gaining experience with Elastic X-Pack features that are available through Kibana concerning pipeline monitoring.\nUsed websites and libraries\nThe most interesting resource for this experiment was this GitHub repository: \u201chttps://github.com/deviantony/docker-elk\u201c. This gave me a jump start to configure the complete ELK stack. Of course, it was not completely what I needed, but with some small changes, I could easily work with it. Other resources are the documentation pages of Elastic:\n\nhttps://www.elastic.co/guide/en/logstash/current/pipeline-to-pipeline.html\nhttps://www.elastic.co/guide/en/logstash/current/persistent-queues.html\nhttps://www.elastic.co/guide/en/logstash/current/tuning-logstash.html\n\nChanges to the default docker-compose\nOne of the things with Docker and especially with docker-compose scripts is that every script you open seems to be different. I tried to stick as much as possible to the notations of the original author. One of the things I like is adding a volume to a local folder for the Elasticsearch data folder. This enables you to keep the data after a restart. Therefore I added this to the volumes part of elasticsearch config.\ndocker-compose.yml\n- type: bind\r\n   source: ./elasticsearch/data\r\n   target: /usr/share/elasticsearch/data\r\n\nThe Logstash part contains more changes, therefore I post it complete. I added the pipelines.yml, the pipelines folder and the data folder for the persistent queue data. I also added the port for http input connector and made the reloading of config work by adding the command part.\ndocker-compose.yml\n  logstash:\r\n    build:\r\n      context: logstash/\r\n      args:\r\n        ELK_VERSION: $ELK_VERSION\r\n    volumes:\r\n      - type: bind\r\n        source: ./logstash/config/logstash.yml\r\n        target: /usr/share/logstash/config/logstash.yml\r\n        read_only: true\r\n      - type: bind\r\n        source: ./logstash/config/pipelines.yml\r\n        target: /usr/share/logstash/config/pipelines.yml\r\n        read_only: true\r\n      - type: bind\r\n        source: ./logstash/pipeline\r\n        target: /usr/share/logstash/pipeline\r\n        read_only: true\r\n      - type: bind\r\n        source: ./logstash/data\r\n        target: /var/lib/logstash/data\r\n    ports:\r\n      - \"9101:5000/tcp\"\r\n      - \"9101:5000/udp\"\r\n      - \"8003:8003\"\r\n      - \"9600:9600\"\r\n    command: --config.reload.automatic \r\n    environment:\r\n      LS_JAVA_OPTS: \"-Xmx1g -Xms1g\"\r\n    networks:\r\n      - elk\r\n    depends_on:\r\n      - elasticsearch\r\n\nUsing Persistent queues\nAn interesting option when the number of records coming in fluctuates a lot is using a persistent queue between your input and the filter part. Especially useful when dealing with a UDP input that just tosses the overflow messages, having a queue that writes messages to a file that cannot be handled immediately. Configuring the queue type is as easy as adding the queue.type and path.queue options through the logstash.yml file. In our case, we configure the queue.type per pipeline. But the path is configured in the logstash.yml file to be: \u201c/var/lib/logstash/data/queue\u201d. Yes the path as configured in the docker-compose.yml file. In the next part, we are going to configure the pipelines.\nConfiguring the pipelines\nBefore diving into the code, there are a few things to take note of. First, be sure to configure the pipelines.yml file in your docker-compose. If you forget this. all pipeline files in the pipeline folder are automatically loaded and everything becomes a mess. In the pipelines.yml file we configure the different pipelines. The main pipeline contains the input through UDP and http. The first pipeline splits the name by a space character. If the last name is Coenradie, the message is sent to the coenradie pipeline. In all other cases the message is sent to the other pipeline. Both these pipelines set the field index_group. Which is used in the final pipeline to be sent to elasticsearch. The name of the index is taken from the field index_group. The code blocks below show the different pipelines. \u00a0 \u00a0\npipelines.yml\n- pipeline.id: main_pipeline\r\n  path.config: \"/usr/share/logstash/pipeline/main.pipeline\"\r\n  queue.type: persisted\r\n- pipeline.id: elasticsearch_pipeline\r\n  path.config: \"/usr/share/logstash/pipeline/elasticoutput.pipeline\"\r\n- pipeline.id: coenradie_pipeline\r\n  path.config: \"/usr/share/logstash/pipeline/coenradie.pipeline\"\r\n- pipeline.id: other_pipeline\r\n  path.config: \"/usr/share/logstash/pipeline/other.pipeline\"\nmain.pipeline\ninput { \r\n\thttp {\r\n\t\tid => \"receiveMessagesHttp\"\r\n\t\tport => 8003\r\n\t\tcodec => \"json\"\r\n\t}\r\n\r\n\ttcp {\r\n\t\tid => \"receiveMessagesTcp\"\r\n\t\tport => 5000\r\n\t\tcodec => \"json\"\r\n\t}\r\n\r\n}\r\nfilter {\r\n\tmutate {\r\n\t\tlowercase => [ \"name\" ]\t\r\n\t}\r\n\tgrok {\r\n\t\tid => \"splitName\"\r\n\t\tmatch => { \"name\" => \"%{WORD:firstname} %{WORD:lastname}\" }\r\n\t}\r\n}\r\noutput {\r\n  \tif [lastname] == \"coenradie\" {\r\n\t\tpipeline { \r\n\t\t\tid => \"sendToCoenradie\"\r\n\t\t\tsend_to => coenradie_pipeline\r\n\t\t}\r\n\t} else {\r\n\t\tpipeline {\r\n\t\t\tid => \"sendToOther\"\r\n\t\t\tsend_to => other_pipeline\r\n\t\t}\r\n\t}\r\n}\r\n\ncoenradie.pipeline\ninput { \r\n\tpipeline {\r\n\t\tid => \"readCoenradie\"\r\n\t\taddress => coenradie_pipeline\r\n\t}\r\n\r\n}\r\nfilter {\r\n\tmutate {\r\n\t\tid => \"addIndexGroupCoenradie\"\r\n\t\tadd_field => {\"index_group\" => \"coenradiegroup\"}\r\n\t}\r\n\r\n}\r\noutput {\r\n\tpipeline {\r\n\t\tid => \"fromCoenradieToElastic\"\r\n\t\tsend_to => elasticsearch_pipeline\r\n\t}\r\n}\r\n\nother.pipeline\ninput { \r\n\tpipeline {\r\n\t\tid => \"readOther\"\r\n\t\taddress => other_pipeline\r\n\t}\r\n}\r\nfilter {\r\n\tmutate {\r\n\t\tid => \"addIndexGroupOther\"\r\n\t\tadd_field => { \"index_group\" => \"othergroup\" }\r\n\t}\r\n}\r\noutput {\r\n\tpipeline {\r\n\t\tid => \"fromOtherToElastic\"\r\n\t\tsend_to => elasticsearch_pipeline\r\n\t}\r\n}\r\n\nelasticoutput.pipeline\ninput { \r\n\tpipeline { \r\n\t\taddress => elasticsearch_pipeline\r\n\t}\r\n}\r\n\r\noutput {\r\n\telasticsearch {\r\n\t\thosts => \"elasticsearch:9200\"\r\n\t\tindex => \"%{index_group}-%{+YYYY.MM.dd}\"\r\n\t\tuser => \"elastic\"\r\n\t\tpassword => \"changeme\"\r\n\t}\r\n}\nThat\u2019s it, now you can run the sample. Send a few messages and go to the special monitoring board in Kibana to see the messages coming in. Open de url http://localhost:5601, login with elastic \u2013 changeme. Open de monitoring app (almost at the bottom). Of course you can also use the Kibana console to inspect the indexes. If you are not sure how to send messages using curl or nc on the mac. Be sure to check the last code block. The code:\u00a0https://github.com/jettro/docker-elk-blog\n\u00a0\n$ echo \"{\\\"name\\\":\\\"Jettro Coenradie\\\"}\" | nc 127.0.0.1 9101\r\n$ curl -XPOST \"http://localhost:8003\" -H 'Content-Type: application/json' -d'{  \"name\": \"Byron Voorbach\"}'\n\n", "tags": [], "categories": ["Blog", "Search"]}
{"post_id": 3224, "title": "Testing your secured Azure Function with Postman", "url": "https://www.luminis.eu/blog-en/development-en/testing-your-secured-azure-function-with-postman/", "updated_at": "2022-06-24T09:13:25", "body": "In my previous blogpost I described how to secure your Azure Function using EasyAuth. In this blogpost I will describe how you can test your secured Azure Function using Postman.\nIf you didn\u2019t read the previous blog post, you can find it here.\nAt the end of this blog post, you will find a link to download the Postman collection with the different request.\nUpdate your Application Registration in Azure Active Directory\nTo be able to test the authentication, you need to add http://localhost:4200 to the redirect urls of the application registation in Azure Active Directory. You could use the built-in OAuth-flow of Postman (see links in the end of this blog for more details) to get a token, but we had some bad experience with the audience-id received in the jwt-token. So we will mimic the OAuth authorization-code flow as used by the Authentication libraries and as described here: https://docs.microsoft.com/en-us/azure/active-directory/develop/v1-protocols-oauth-code.\nRetrieve Authentication Code\nThe first step you need to make is retrieve the Authentication Code. This can be done by constructing the login-URL based on the following template:\n https://login.microsoftonline.com/<TenantId>/oauth2/authorize?client_id=<ClientId>&response_type=code&response_mode=query&prompt=admin_consent&resource_id=<ClientId>&redirect_uri=http://localhost:4200.\nThe variables TenantId and ClientId should sound familiar by now. Copy the URL to your browser and execute the request. You will be prompted to login, after logging in, the consent screen is shown. Click \u2018Accept\u2019 and you will be redirected to localhost:4200. No problem if there is nothing running. You need the code from the URL to be able to continue.\nUsing the authentication token to get a bearer token\nThis step will be done in Postman. Use the authentication code just received and make the following request.\n\nAfter pressing \u2018Send\u2019, you will get the token details as response. Validate the \u2018scope\u2019 you received in the response. It should be the same as the scope created earlier in Azure AD. If it looks like \u201cprofile openid email 00000003-0000-0000-c000-000000000000/User.Read\u201d, you probably forgot to register or pass the scope. Grab the access_token and use it as Bearer token for your request to your Azure Function.\nUseful links\n\nhttps://www.bruttin.com/2017/11/21/azure-api-postman.html: this gave me the right instructions to get started with the authentication flow in postman.\nhttps://medium.com/agilix/getting-started-with-windows-azure-ad-authentication-using-postman-e37a185b642d: another resource for getting the authentication code flow working in Postman.\n\nLearn from our experts:25 Jan 2021-Mark van KesselTraining: Azure Application InsightsWith applications increasingly running in the Cloud, the way we monitor and maintain them is changing. Where we used to log in to a VM to gain access to all logging and tooling services of our monolith, the information is...\n", "tags": [], "categories": ["Blog", "Development"]}
{"post_id": 3221, "title": "Securing your Azure Functions with Microsoft Azure AD (EasyAuth)", "url": "https://www.luminis.eu/blog-en/cloud-en/securing-your-azure-functions-with-microsoft-azure-ad-easyauth/", "updated_at": "2021-09-09T14:10:25", "body": "For one of our projects we had to secure the Azure Functions (AF) with Microsoft Azure Active Directory (AAD). Searching the web revealed a lot of partial solutions, but no complete overview. In this post, I will try to give that overview.\nThe easiest way to get the job done\nIn the Azure portal, go to the Function App you want to secure, select the tab \u2018Platform features\u2019 and choose \u2018Authentication/ Authorization\u2019 under Networking. This will take you to a screen where you can turn App Service Authentication on. So far, so good. You get the question what should happen when the user is not authenticated. Do you have functions in the Function App everyone should be able to reach? In that case select \u2018Allowed Anonymous requests (no action)\u2019. This implies that you should handle the authentication validation yourself in the functions. Otherwise select \u2018Log in with Azure Active Directory\u2019. This will block users. Click on the line \u2018Azure Active Directory \u2013 Not Configured\u2019 to configure the provider. Select the Management mode \u2018Express\u2019 and \u2018Create New AD App\u2019. This will do all configuration in the Azure Function and AAD for you.\nBut what did just happen here?\nExpress Mode is nice, but all control is gone. When saving the Function App settings, a new Application is registered in the Azure AD, a client secret is generated and together with the Client Id and Tenant Id configured in the Function App. Authentication is enabled with a redirect to \u2018https://yoursite.azurewebsites.net/.auth/login/aad/callback, allowing an Implicit Grant with ID tokens. The API permission User.Read is granted to the application. And a scope is added for user_impersonation.\nDoing it the harder way\nIf you want to have more control, go to the Azure Active Directory and register a new Application. Give it a name and determine where the accounts are coming from. After pressing register, you will see the overview screen. Here are details you need later (client-id and tenant-id).\nAuthentication\nThe next step is defining the redirect-uri for the Azure Function. This is where the function will retrieve the authentication details. The uri is the url of the function app + /.auth/login/aad/callback. Under implicit grant, tick ID tokens.\nCertificates and secrets\nYou will now need to generate a client-secret. It is optional when configuring the Azure Function, but crucial if you want to test the function using postman. Do not forget to copy the secret before leaving this page.\nExpose an api\nThe next step is registering a scope. When you don\u2019t, you could end up with an invalid audience in the jwt-token. Click Add a scope. This will prompt you for an Application ID URI. The default should be just fine, but you can change it if you like. Now you can add a scope. Give it a name and description, allow users and admins and fill in the rest of the details.\nConfigure using ARM templates\nThis is something you want to automate in your deployment pipelines. ARM templates come in handy there. The templates generated from Azure are of no help at all. For more details about these settings in the ARM template, refer to https://docs.microsoft.com/en-us/azure/templates/microsoft.web/2018-11-01/sites/config-authsettings.\n\nHow to get the user details\nYou are probably not just securing your function; you also want to make use of the credentials in your function. This part is easy, all the heavy lifting is done by Azure AD. Just add the ClaimsPrincipal-parameter to the function entrypoint and you will be able to use the credential details of the user.\nTake care\nWhen you selected \u2018Allow anonymous requests\u2019 before, you should validate the credentials you receive in your function. Except of course when the function should be public. When you configured the function with \u2018admin\u2019 or \u2018function\u2019 as authentication level, the claim will contain at least two keys for the authentication level. Just checking if you receive claims is not enough.\nUseful links\n\nhttps://jwt.io: here you can evaluate the received jwt token.\nhttps://jwt.ms: the Microsoft version of jwt.io. It also shows details of the claim.\nhttps://docs.microsoft.com/en-us/azure/active-directory/develop/v1-protocols-oauth-code: a good resource for understanding the authentication code flow used when signing in to Azure AD.\nhttps://docs.microsoft.com/en-us/azure/templates/microsoft.web/2018-11-01/sites/config-authsettings: the location to get more details on ARM-templates.\nhttps://docs.microsoft.com/en-us/azure/app-service/configure-authentication-provider-aad: more elaboration on the steps to configure Azure AD. Also describes the need for Application ID URI.\n\n", "tags": [], "categories": ["Blog", "Cloud", "Development"]}
{"post_id": 2172, "title": "Elastic Workplace Search: search through your enterprise (cloud) applications", "url": "https://www.luminis.eu/blog-en/search-en/elastic-workplace-search-blog/", "updated_at": "2020-11-30T15:40:58", "body": "One of our clients asked us to research if it\u2019s possible to include their SharePoint and Confluence content in the enterprise search application we build for them, which was very difficult. Since December, with the 7.5 release, Elastic has included Elastic Enterprise Search in the Elastic Stack. This triggered me to dive a bit deeper into this solution.\nNote: This blog covers my view on version 7.6.0 and at this version Elastic Workplace Search is still in beta, which means it is not considered production ready yet.\nLet me start by saying that the products in the Elastic Enterprise Search suite are part of the Elastic Platinum license. This means they\u2019re not free to use. However, it\u2019s possible to start free trials for each product which will last a month. In this blog I will walk through the Elastic Workplace Search product. First I\u2019ll explain what it is about. Then I\u2019ll cover the administrative side with integration of sources, user management, permissions & access control and settings for search/relevance. After that I will walk you through my view on the client side. I will show how the web client looks like and talk about other clients. Then I will finish with my honest opinion about the product.\nWhat\u2019s Workplace Search about?\nBy acquiring Swiftype in 2017, Elastic got ahold of their search solutions: Enterprise Search, Site Search and App Search. Recently Elastic has renamed Enterprise Search to Workplace Search and all three products are now part of the broader Elastic Enterprise Search suite. This blog is about the Workplace Search product. As said above, one of our clients asked us to investigate the possibility of including SharePoint and Confluence content in their search solution. According to Elastic, Workplace Search provides easy to use, powerful search across all the tools used throughout your organisation. It is an all-in-one solution to search through your company\u2019s enterprise (cloud) tools in a single place. It allows you to quickly connect to multiple sources and automatically index all content into Elasticsearch to make it searchable. Searching can be done through the web application which comes with the suite, but other clients are also in the making. More on that later.\nThe Administrative Side\nSource Integration\nWorkplace Search can integrate with all kinds of (mostly cloud based) sources. It has built-in connectors which can synchronise the content periodically. From what I\u2019ve seen it does this every two hours and it is not (yet) possible to change this. All currently available content sources require you to create an OAuth App within that content source. For this OAuth App you need to create API credentials which you then need to provide to Workplace Search and after that the content will automatically be indexed in Elasticsearch. All of this is well documented in the Workplace Search Guide. Note: this means you need administrator privileges within the source you want to configure. The applications shown in the following image are currently supported:  There are currently more connectors in the making for more sources. These are a few sources which we can expect to connect with in the future:\n\nOther Office365 apps (Yammer etc.)\nRails Docs\nBox\nEvernote\n\n\u00a0 Beside all connectors for these applications it\u2019s also possible to connect your own source and index content from a custom application by configuring a custom API. Custom APIs are push only, so synchronisation of documents should be done from your own application. Each custom API source can have a custom schema. The below image shows how the management for a custom source looks like:  When adding a new custom API, Workplace Search provides you with an Access Token and a Key. These credentials are needed to ingest data into Elasticsearch through Workplace Search. See\u00a0the documentation on how to ingest data. Within the management section of your custom API you can create or alter field names and types for your schema. Ingesting data that matches the fields defined in your schema helps Workplace Search interpret and style the information. Ingesting documents which contains fields that don\u2019t exist will create default fields of type text. It\u2019s also possible to alter the display settings of your custom API data. With this you can change how the search results from your source will be presented. User management For the management of user there currently are three options.\n\n\u201cStandard Mode\u201d. With this mode all users are managed within Workplace Search. Users have to be invited from an administrator within Workplace Search (requires a SMTP server for emails).\n\u201cElasticsearch Native Realm\u201d. With this option the users are managed within the Elasticsearch layer. Users and their role mappings can be created/altered through the Elasticsearch API\u2019s or using Kibana.\n\u201cElasticsearch SAML\u201d. New since version 7.6.0 is the option to configure Elasticsearch SAML, which delegates user management to third-party authentication providers and identity management platforms like Auth0 or Okta. You must use Workplace Search\u2019s Role Mapping to coordinate user attributes with Elasticsearch.\n\n\u00a0 The following image shows how the mapping of roles looks like in Workplace Search:  As you might see, user attributes can be used to map users to Workplace Search roles and groups. Sources can be shared with groups to give users in these groups access to a specific set of sources. For example, someone from HR or marketing would not benefit from getting search results from GitHub. If the GitHub source is not shared with these groups, the search results from users within these groups will not include GitHub items. The following image shows the group management page:  Permissions & Access Control One of the problems we ran into ourselves when researching Sharepoint/Confluence connectivity for our client is that those application have a complex permission/access structure. There is no way you can retrieve this and therefore cannot map this to your users within your search engine. You have to do all the mapping yourself which could be a lot of work. There could be sensitive content in those applications, which should not be visible to all users, but still you want the users working with that sensitive content be able to find that as well. Workplace Search doesn\u2019t seem to have solution for this problem either, except to let every user connect their private accounts (private sources), but if you have a lot of users that will be very bulky. How this works in Workplace Search is that an administrator can set up sources for the whole organisation to use. These are called organisation sources. As said in the previous section sources can be shared with groups. If a user is not in a group where the source is with, the user will not be able to search in this source. Note that the content that is synchronised in organisation sources is everything that is accessible through the account with which it is connected. Connecting it with a master/administrator account might make private documents visible for the whole organisation within Workplace Search, because permission/privilege/role settings are not synchronised with the content. Document or object level permissions are not available (yet?) for sources with predefined connectors. Therefore make sure to connect the source with a special search user which only has access to documents publicly available for the organisation. Every user can connect their own sources that only they can use. These are called private sources. Only sources for which the connector has been configured in Workplace Search can be added as a private source, since you have to give access to the OAuth App created for the connector. Users might add their own private GitHub source or private Google Drive, for example. Its content will only be available for that user. This could be useful when you have a lot of private documents which you want to include in Workplace Search. For custom API\u2019s it is possible to configure document level permissions. These are applied at index time. For each document you are able to define allow and deny permissions. Deny permissions take precedence here. A big side note here is this is only possible when users are managed within Workplace Search or through the Elasticsearch Native Realm. Managing permissions is done via the Workplace Search Permissions API. This can not be managed in the UI. Read the documentation about this topic if you wish to know more. Search Settings As for options to tune relevance, there currently are not many. Right now the only way to influence ranking is to prioritise sources within groups. You can make certain sources more important than others for a group. You can see how that looks like in the below image:  Since the product is still in beta and more relevance tuning options, such as synonyms and boosts, are present in the other applications in the Enterprise Search Suite (App Search and Site Search) I expect those will be available later in Workplace Search as well.\nThe Client Side\n(All images in this section are taken from Elastic) Web client For the client side of Workplace Search, they\u2019ve built a web client which comes with the suite. They wrote a\u00a0searcher\u2019s guide which you should show the users before they start using Workplace Search. This is to make the users aware of all the possible features and how to use them. Let me first start with the good things about the client. It includes facets for filtering, time filters (past week etc.), option to sort on relevance or recently updated and has highlighting in the search results. When typing your search query it shows you your search history, which is synced in your account. You can click the \u201cview in \u2026\u201d icon next to a search result when you want to go directly to the external source. When clicking on the search result itself it shows a preview in a sidebar with an excerpt and other metadata. In that way you sometimes don\u2019t even have to leave Workplace Search to get the information you need. See the below image of how the search result page looks like:  In my opinion the most notable feature is the natural language syntax. This syntax will let you search as if you were talking to a person, such as \u201cissues assigned to Dani\u00ebl in Jira with status in progress\u201d. It recognises parameters in the query (they turn blue) and toggles and filters your results automatically. See the earlier mentioned searcher\u2019s guide if you want to know more about this. A little impression on how this looks like in the search bar:  Now about the bad things. First it lacks proper autocompletion. It only gives you search history or suggestions for sources after you type the word \u201cin\u201d or \u201con\u201d (natural language syntax) and then the first letter of a source. It seems to have some form of toleration for typos, but I get no feedback about that. Maybe a feature like \u201cdid you mean\u201d would be nice. The next thing is there is no option for UI customisation whatsoever. Other clients Next to that Elastic is working on more types of clients, such as native mobile apps, Slack integration and a browser plugin. This browser plugin will be useful while browsing through one of your connected cloud applications. It will provide you with relevant information from other connected sources on the fly. What this means is that, for example if you are browsing in Salesforce and you\u2019re on an account page, you get information from other sources that could be relevant to this client. See the below image:  In case you want to build your own client, this is not possible right now. They haven\u2019t made the search API public yet. Elastic is planning to release this with GA, so you have to wait until then.\nMy Conclusion\nIn its current state I would not recommend Workplace Search. I believe it\u2019s missing some essential features for a good search experience. No relevance tuning options, not any type of suggestions (except source autocomplete), no customisation to the client UI (except for the layout of custom source results) and no search API for a custom client either. There\u2019s also the fact that a platinum license is needed. Unless you already have a platinum license for other Elastic solutions, I would not consider to purchase a platinum license just for this product. What I was really hoping for in this product is that they found some solution for the permission/access mapping. Unfortunately Workplace Search runs into the same problems we did. Still, Workplace Search has a lot of potential and I\u2019m curious on how this product develops. Will definitely keep an eye on it.\nTry it yourself\n\nThe download page\nThe documentation\n\n\u00a0\n", "tags": [], "categories": ["Blog", "Search"]}
{"post_id": 3172, "title": "Designing for good behaviour", "url": "https://www.luminis.eu/blog-en/concepting-ux-en/nudging-for-good-desiging-for-persuasive-technology/", "updated_at": "2020-11-16T16:26:34", "body": "It\u2019s Monday morning. You\u2019re driving to work where an awesome project is waiting for you. Only minutes later, your excitement is crushed because you\u2019re trapped in a traffic jam. And why? Because somebody was speeding and caused an accident.\nNow, let\u2019s say there are two ways to decrease speeding behaviour. One is to motivate good behaviour by providing rewards combined with information, another is to enforce it through punishment. While both are essentially means to an end, the first is considered persuasion and the second is considered coercion. Persuasion is fundamentally different from coercion because it relies on a voluntary change, rather than power. So how can persuasion and technology be combined to change behaviour for the better?\nPersuasive technology\n\nIt might seem a bit unnerving to learn about persuasion. Not only are we easily influenced, but we can also easily influence (or even manipulate) others. We do that\u00a0every day\u00a0on a small scale\u00a0by the clothes we\u2019re wearing and how we behave. However, persuasive technology isn\u2019t about manipulation or playing with people\u2019s feelings. People view and respond to computing technologies in various ways, and persuasive technology uses motivation in combination with user interfaces to support users in achieving goals. For example, games apply both interactivity and narratives to create persuasive experiences.\u00a0\nThere are endless choices when it comes to\u00a0choosing\u00a0a goal. From\u00a0changing eating behaviour,\u00a0to\u00a0promoting\u00a0home energy efficiency.\u00a0The most\u00a0successful\u00a0examples come from nudging people to do something they wanted to do anyway:\u00a0\n\nHow can we help people remember to take their medicine on time?\u00a0GlowCaps\u00a0is a special cap that fits on top of a standard pill\u00a0bottle, which lights up when the patient needs to take their medicine. It even sends reports about how well the patient is sticking to the schedule.\u00a0The cap is 86 percent effective in helping people to take their doses.\u00a0\u00a0\nHow can we help people to exercise?\u00a0Imoveyou.com allows people to challenge\u00a0each other\u00a0in quick\u00a0\u2018if/then\u2019\u00a0challenges. \u201cIf I walk my dog for 20 minutes, you will ride your bicycle around the block\u201d.\u00a0It\u2019s persuasive because people can act quickly, but also provides challenges nearby. \u00a0\n\n\u00a0\nDesigning a persuasive technology\u00a0\n\nPersuasive design is\u00a0a\u00a0practice that\u00a0support designers\u00a0to make\u00a0decisions based on\u00a0insights from psychology and social studies. These insights can be applied to interface design. It is important to know that many\u00a0previous\u00a0attempts to create a persuasive technology\u00a0have failed, simply because the\u00a0goal\u00a0was\u00a0too ambitious.\u00a0A good example is helping people to stop smoking, which is a\u00a0tough\u00a0long-time\u00a0habit to break.\u00a0Persuasive technology guru B.J.\u00a0Fogg\u00a0created\u00a0eight steps to increase the probability of success:\u00a0\u00a0\n1. Choose a simple behaviour to target\nFind an appropriate behaviour to target for change.\u00a0It\u2019s okay to choose an ambitious goal, but\u00a0you\u2019ll want it to be feasible as well. A solution is to break it down into a small goal that either ultimately achieves a larger goal, or is an approximation of that larger goal.\nFor instance, I want to ensure that elderly people (i.e. age 70 and up) are not infected with\u00a0a new virus until a vaccine is available.\u00a0Since this is quite\u00a0an\u00a0ambitious goal, it can be broken down\u00a0into\u00a0people who\u00a0keep regularly visiting their elderly parents (regardless of the possible consequences). \u00a0\n2. Choose a receptive audience\nPick an audience that is the most receptive.\u00a0It\u2019s tempting\u00a0to choose\u00a0the toughest audience or\u00a0\u2018all users\u2019, but\u00a0unfortunately\u00a0neither\u00a0seemed to\u00a0have\u00a0worked\u00a0well in the past. At the last step there\u2019s the opportunity to increase the audience, but for now we begin small.\nAn example of an audience can be people that need\u00a0more exercise.\u00a0Since this is quite a big group,\u00a0I want to break this down as\u00a0well. Mothers with young children seem a good audience, as they often can\u2019t find the time to exercise and/or don\u2019t know how.\u00a0\u00a0\n3. Find what prevents the target behaviour\u00a0\nFind the problem that prevents the audience from performing their target behaviour.\u00a0You need motivation, ability, and/or a well-timed trigger to succeed.\u00a0However, if both motivation and ability are lacking,\u00a0it\u2019s\u00a0probably time to reconsider the project.\u00a0\nAn example can be young male drivers who\u00a0lack the motivation to stop\u00a0speeding.\u00a0It\u2019s also likely that a well-timed trigger is missing during driving, so\u00a0I want to\u00a0combine both in the solution.\u00a0\u00a0\n4. Choose a familiar technology channel\u00a0\nThe challenge here is to choose between available channels and which one matches the target behaviour best.\u00a0There\u00a0are many channels to choose from such as social platforms, smartphones, and games. Since people can only change one behaviour at the time, it would be best to choose a familiar channel.\nTo continue\u00a0on the example of people with elderly parents,\u00a0a familiar channel can be\u00a0smartphone or the television.\u00a0I think a\u00a0smartphone could\u00a0be the best choice since it can provide\u00a0various triggers throughout\u00a0the day, whereas a television can only provide triggers when it\u2019s on.\u00a0\u00a0\nUp until this point you\u00a0probably\u00a0noticed that\u00a0we\u2018ve followed the steps in sequence. However, if\u00a0you work for a specific company (e.g. health insurance company) you might not have a choice in audience and/or channel.\u00a0In that case you\u00a0can\u00a0still execute all four steps,\u00a0albeit in a different\u00a0order.\u00a0\u00a0\n5. Find relevant examples & 6. Imitate successful examples\u00a0\nUnlike 10 years ago, there are many\u00a0examples of\u00a0successful\u00a0persuasive\u00a0technologies relevant to a chosen\u00a0intervention.\u00a0Even though there rarely is an example that perfectly fits all categories (audience, behaviour, channel), it suffices to find examples of each category individually that matches the project. Afterwards\u00a0you can\u00a0combine them into a solution.\nContinuing on the example of the\u00a0mothers, I can find examples of mothers with young children (audience), how they currently (try to) exercise (behaviour), and how a social platform can help them (channel).\u00a0From combining\u00a0these\u00a0findings\u00a0I\u00a0could\u00a0conclude that a social platform\u00a0provides\u00a0solutions on how\u00a0the mothers can exercise, even with young children around.\u00a0Mothers can exchange tips, but the social platform itself can\u00a0also\u00a0provide\u00a0tips and\u00a0triggers to exercise.\u00a0\u00a0\n7. Test & iterate quickly\u00a0\nDesigning for persuasion is harder than designing for usability, as many attempts to change people\u2019s behaviours fail. That is why\u00a0rapid testing with many trials is a good way\u00a0to find out if your idea succeeded in the audience adopting a simple target behaviour.\u00a0\nContinuing on the example of the young male drivers,\u00a0a\u00a0proposed solution\u00a0could\u00a0be\u00a0an in-car app that provides well-timed audio triggers and one visual trigger (e.g. a car that\u00a0gets increasingly damaged according to the car speed).\u00a0I could do a\u00a0rapid test\u00a0by accompanying young males in their rides, while imitating audio triggers and using paper prototypes for visual triggers.\u00a0The results can be used for improvements, until a behavioural change is observed.\u00a0\u00a0\n8. Expand on success\u00a0\nCongratulations, we\u2019ve created a persuasive technology! At this point the behaviour or audience can be scaled up. For instance, creating a more difficult target behaviour to reach, or\u00a0selecting a wider audience.\u00a0\u00a0\nNo matter how small or simple, remember that creating a persuasive technology is always a milestone. Small behavioural changes will eventually lead to projects that are going to be successful in changing long-term habits. Additionally, the persuasive design approach can be used in any design project that entails some sort of behavioural change.\u00a0\nYou\u00a0might\u00a0wonder\u00a0how ethical persuasive technology is.\u00a0Well\u00a0that\u2019s a whole new topic,\u00a0but\u00a0just remember that there\u2019s nothing inherently wrong about trying to persuade someone.\u00a0A\u00a0combination of persuasion\u00a0and technology can lead to great behavioural changes that are better for everyone. \u00a0\n", "tags": [], "categories": ["Blog", "Concepting &amp; UX"]}
{"post_id": 3045, "title": "Backup Azure Storage using Azure Functions in 3 easy steps", "url": "https://www.luminis.eu/blog-en/cloud-en/backup-azure-storage-using-azure-functions-in-3-easy-steps/", "updated_at": "2020-11-24T15:04:28", "body": "At Luminis, we develop many Cloud Native Solutions in Azure, but there is no easy way to make backups for our Azure Storage Accounts. So we\u2019ve developed a NuGet package that does exactly that, and you can use it too! Here are 3 easy steps how to implement this using an Azure Function.\n1. Create timer trigger\nAdd a new timer trigger to your existing project. If you are using Visual Studio, you can right click on your existing Function App project to add a new Azure Function.\nFunctionName(\"Function1\")]\r\npublic static void Run([TimerTrigger(\"0 */5 * * * *\")]TimerInfo myTimer, ILogger log)\r\n{\r\n     log.LogInformation($\"C# Timer trigger function executed at: {DateTime.Now}\");\r\n}\nIf you don\u2019t have a Function app, yet this blog should get you started.\n2. Install the NuGet package\nInstall the NuGet package in your project, through the NuGet window or the Visual Studio package manager:\nInstall-Package Luminis.AzureStorageBackup -Version 1.6.0\r\n\r\n\n3. Configure the backup\nNow the only thing left to do, is providing the correct configuration, so the library knows what to backup, and where to.\n[FunctionName(\"Function1\")]\r\npublic static async Task Run([TimerTrigger(\"0 */5 * * * *\")]TimerInfo myTimer, ILogger log, ExecutionContext context)\r\n{\r\n    var sourceAccountName = Environment.GetEnvironmentVariable(\"BackupSourceAccountName\");\r\n    var sourceKey = Environment.GetEnvironmentVariable(\"BackupSourceAccountKey\");\r\n\r\n    var backupAzureStorage = new Luminis.AzureStorageBackup.BackupAzureStorage(sourceAccountName, sourceKey, log, context.FunctionAppDirectory);\r\n\r\n    var destinationAccountName = Environment.GetEnvironmentVariable(\"BackupDestinationAccountName\");\r\n    var destinationKey = Environment.GetEnvironmentVariable(\"BackupDestinationAccountKey\");\r\n    var destinationContainerName = Environment.GetEnvironmentVariable(\"BackupDestinationContainer\");\r\n\r\n    // Backup Tables\r\n    await backupAzureStorage.BackupAzureTablesToBlobStorage(\"table1,table2\", destinationAccountName, destinationKey, destinationContainerName, \"tables\");\r\n\r\n    // Backup Blobs\r\n    await backupAzureStorage.BackupBlobStorage(\"container1,container2\", destinationAccountName, destinationKey, destinationContainerName, \"blobs\");\r\n}\nFor more information checkout the GitHub page. Interested, and you want want to learn more about Azure Functions checkout our traning about Microservice using Azure Functions from the Luminis Academy\n", "tags": [], "categories": ["Blog", "Cloud"]}
{"post_id": 2897, "title": "Solr managed resources filters", "url": "https://www.luminis.eu/blog-en/development-en/solr-managed-resources-filters/", "updated_at": "2020-11-17T10:10:41", "body": " Solr has a nice functionality called managed resources. Solr provides a couple of them but makes it quite easy to add your own. In this blog post, I\u2019d like to take a concrete example and go through through the steps of writing the code for a custom managed resource and making it available to Solr, in order to be used during analysis.\nIt is commonly known that spell checking are not perfect. Even when using a dictionary based stem factory like HunspellStemFilterFactory, you will still have words incorrectly stemmed. You can dig into the hunspell affix file and add rules to fix the issues, but it is sometimes easier to just ignore the word.\nSolr provides a filter factory that does just that. KeywordMarkerFilterFactory uses a plain text file containing the words to be ignored by stemming filters. Maintaining such a list is not very easy, you need to upload it with the config set and any change requires updating the config set. That is not very manageable, it would be better to leverage the power of managed resources in order to edit the list. You could even build a UI so other users can maintain this list, after all this can be considered content as well.\nBaseManagedTokenFilterFactory is an abstract class that does all the heavy leverage so to write a managed filter factory, we need to extend on it. Normally you would wrap the \u2018standard\u2019 factory while writing a managed resource. In our case KeywordMarkerFilterFactory is pretty simple.\nThe interesting part is:\n  public TokenStream create(TokenStream input) {\r\n    if (pattern != null) {\r\n      input = new PatternKeywordMarkerFilter(input, pattern);\r\n    }\r\n    if (protectedWords != null) {\r\n      input = new SetKeywordMarkerFilter(input, protectedWords);\r\n    }\r\n    return input;\r\n  }\nThis factory can produce two type of objects: SetKeywordMarkerFilter or PatternKeywordMarkerFilter. We are interested in SetKeywordMarkerFilter.\nThis is how the managed resource factory looks like:\npackage my.package;\r\n\r\nimport org.apache.lucene.analysis.CharArraySet;\r\nimport org.apache.lucene.analysis.TokenStream;\r\nimport org.apache.lucene.analysis.miscellaneous.SetKeywordMarkerFilter;\r\nimport org.apache.solr.common.SolrException;\r\nimport org.apache.solr.common.util.NamedList;\r\nimport org.apache.solr.rest.ManagedResource;\r\nimport org.apache.solr.rest.schema.analysis.BaseManagedTokenFilterFactory;\r\nimport org.apache.solr.rest.schema.analysis.ManagedWordSetResource;\r\n\r\nimport java.util.Map;\r\nimport java.util.Set;\r\n\r\n/**\r\n * TokenFilterFactory that uses the ManagedWordSetResource implementation\r\n * for managing stop words using the REST API.\r\n */\r\npublic class ManagedKeywordMarkerFilterFactory extends BaseManagedTokenFilterFactory {\r\n\r\n    private CharArraySet protectedWords = null;\r\n\r\n    public ManagedKeywordMarkerFilterFactory(final Map args) {\r\n        super(args);\r\n    }\r\n\r\n    @Override\r\n    public String getResourceId() {\r\n        return \"/schema/analysis/protwords/\" + handle;\r\n    }\r\n\r\n    protected Class getManagedResourceImplClass() {\r\n        return ManagedWordSetResource.class;\r\n    }\r\n\r\n    @Override\r\n    public void onManagedResourceInitialized(final NamedList args, final ManagedResource res) throws SolrException {\r\n        final Set managedWords = ((ManagedWordSetResource)res).getWordSet();\r\n        boolean ignoreCase = args.getBooleanArg(\"ignoreCase\");\r\n        protectedWords = new CharArraySet(managedWords.size(), ignoreCase);\r\n        protectedWords.addAll(managedWords);\r\n    }\r\n\r\n\r\n    @Override\r\n    public TokenStream create(final TokenStream input) {\r\n        if (protectedWords == null) {\r\n            throw new IllegalStateException(\"Managed protwords not initialized correctly!\");\r\n        }\r\n        return new SetKeywordMarkerFilter(input, protectedWords);\r\n    }\r\n}\r\n\ngetResourceId(\u2026) returns the location that should be used to hit the managed resource (the handle is an extra path parameter that is added when instantiating the factory see manage bellow)\nonManagedResourceInitialized(\u2026) initializes the list (it goes without saying that we are not expecting a huge list!)\ncreate(\u2026) instantiates a filter that will load the list of word that should be \u201cprotected\u201d against stemmers.\nThe next step is to package this class into a JAR file using the way you are used to. Then this JAR file can be made available to Solr by placing it into the dist sub-folder of the Solr installation.\nThe JAR file can then be loaded by adding a lib definition in the solrconfig.xml file of the collection that will make use of it.\n\n\r\n    8.2.0\r\n    .\r\n    .\r\n    .\r\n    \r\n    .\r\n    .\r\n    .\r\n\nAfter restarting Solr, you can use the factory in your schema.\n\n\r\n    .\r\n    .\r\n    .\r\n    \r\n    .\r\n    .\r\n    .\r\n\nThis is the handle mentioned in the factory implementation.\nThe new managed resource is accessible as it is for any other managed resource.\nFor example: http://localhost:8981/solr/<my-collection>/schema/analysis/protwords/en\nYou can apply this to all kind of use cases, the bottom line, if you need to maintain a dynamic list, check out managed resources. Creating your own is as easy as following the steps above. For a more complex case, check out ManagedSynonymFilterFactory.\n", "tags": [], "categories": ["Blog", "Development"]}
{"post_id": 2759, "title": "Integration testing Pub/Sub interactions in your application", "url": "https://www.luminis.eu/blog-en/development-en/integration-testing-pub-sub-interactions-in-your-application/", "updated_at": "2021-02-01T16:26:12", "body": "How do you make sure all goes well when working with big data? At Bol.com we frequently make use of Pub/Sub queues to decouple data streams between applications. This blog explains how we created integration tests that allow you to test these queues and the interactions that they have with the application.\nAs part of the Google Cloud Platform (GCP), Google provides Pub/Sub as its queuing mechanism. After creating a Pub/Sub topic in GCP, you can create publishers to send messages to a topic and subscribers to receive messages from a topic. In order to send or receive Pub/Sub messages from GCP, you can choose to authenticate with GCP through user account or service account credentials. This has one disadvantage; when running your build either locally or on a build server such as Gitlab, your application will attempt to communicate with GCP for its Pub/Sub interactions. Your build will either fail because there are no credentials available (usually the case on build servers), or it will use you personal user account credentials and publish messages to topics that may also be in use on other (test) environments. Both situations are not desirable. In this article, I will first describe how to Dockerize Pub/Sub, followed by the changes required in your application. If you want to dive right into the code, go here. You can also find a prebuilt Docker image here.\nDockerizing the Pub/Sub server\nIn order to still allow for a build that includes tests with Pub/Sub interactions, we are going to Dockerize the Pub/Sub server. With this in place, we don\u2019t need to authenticate with GCP anymore and can actually verify the messages that go through our topics and subscriptions without interfering on other environments. Google already provides us with an emulator that allows us to emulate a Pub/Sub server locally. Our Dockerfile is as follows:\nFROM google/cloud-sdk:272.0.0\r\n\r\nRUN pip install google-cloud-pubsub==1.6.1\r\nRUN mkdir -p /root/bin\r\n\r\nCOPY start-pubsub.sh pubsub-client.py /root/bin/\r\n\r\nENV PUBSUB_EMULATOR_HOST=localhost:8432\r\nEXPOSE 8432\r\n\r\nCMD [\"./root/bin/start-pubsub.sh\"]\r\n\nAs per the emulator\u2019s installation instructions, we clone the Google repository and install the Pub/Sub requirements. Afterwards, we execute start-pubsub.sh:\n#!/bin/bash\r\n\r\nif [[ -z \"${PUBSUB_PROJECT_ID}\" ]]; then\r\n  echo \"No PUBSUB_PROJECT_ID supplied, setting default of docker-gcp-project\"\r\n  export PUBSUB_PROJECT_ID=docker-gcp-project\r\nfi\r\n\r\n# Start the emulator in the background so that we can continue the script to create topics and subscriptions.\r\ngcloud beta emulators pubsub start --host-port=0.0.0.0:8432 &\r\nPUBSUB_PID=$!\r\n\r\nif [[ -z \"${PUBSUB_CONFIG}\" ]]; then\r\n  echo \"No PUBSUB_CONFIG supplied, no additional topics or subscriptions will be created\"\r\nelse\r\n  echo \"Creating topics and subscriptions\"\r\n  python /root/bin/pubsub-client.py create ${PUBSUB_PROJECT_ID} \"${PUBSUB_CONFIG}\"\r\n  if [ $? -eq 1 ]; then\r\n    exit 1\r\n  fi\r\nfi\r\n\r\n# After these actions we bring the process back to the foreground again and wait for it to complete.\r\n# This restores Docker's expected behaviour of coupling the lifecycle of the Docker container to the primary process.\r\necho \"[pubsub] Ready\"\r\nwait ${PUBSUB_PID}\r\n\nThis script indicates that we can provide the Docker container with two optional environment variables:\n\nPUBSUB_PROJECT_ID: GCP project ID\nPUBSUB_CONFIG: a JSON array that describes the topics and associated subscriptions you want to create.\n\nWith this script, we start the Pub/Sub server at http://localhost:8432 and execute a simple Python script (pubsub-configuration-parser.py) that interprets the JSON object.\nBuilding and running the Dockerfile can be done as follows:\ncd src/test/resources/docker\r\n\r\ndocker build . -t pubsub\r\n\r\ndocker run  --name pubsub \\\r\n            -p 8432:8432 \\\r\n            -e PUBSUB_PROJECT_ID=my-gcp-project \\\r\n            -e PUBSUB_CONFIG='[{\"name\": \"my-topic\", \"subscriptions\": [\"my-subscription\"]}]' \\\r\n            -d pubsub\r\n\r\ndocker logs -f pubsub\r\n\nOnce you have the Pub/Sub server running you can publish and receive messages for debugging purposes with the following commands:\ndocker exec -it pubsub /bin/bash\r\n# The following executes within the Docker container\r\ncd /root/bin\r\npython pubsub-client.py publish $PUBSUB_PROJECT_ID my-topic my-message-content\r\npython pubsub-client.py receive $PUBSUB_PROJECT_ID my-subscription\r\n\nMaking the application configurable\nWhat remains is configuring the publisher and subscriber Java classes to connect with the locally running Pub/Sub server. Note that the code examples below use the configuration object PubSubConfig and that the examples are not 100% complete. Please refer to the code repository for the complete listing. Given a topic with the name my-topic and a subscription with the name my-subscription, the minimal setup of a publisher and subscriber is as follows:\nPublisher publisher = Publisher\r\n    .newBuilder(projectTopicName)\r\n    .build();\nSubscriber subscriber = Subscriber\r\n    .newBuilder(subscriptionName, messageReceiver)\r\n    .build();\nBoth builder objects can take a CredentialsProvider instance that determines how we authenticate with GCP. I\u2019ve created my own CredentialsProviderFactory that returns either no credentials, user credentials or service account credentials based on the Spring property gcloud.authentication.method. If you are using service account credentials, you will also have to set the property gcloud.serviceaccount.credentials.file, which is a reference to the JSON file that contains the actual credentials. This JSON file can be retrieved from GCP in the IAM section.\npublic CredentialsProvider create() {\r\n    switch (pubSubConfig.getAuthenticationMethod()) {\r\n        case NONE:\r\n            return getNoCredentialsProvider();\r\n        case USER_ACCOUNT:\r\n            return getUserCredentialsProvider();\r\n        case SERVICE_ACCOUNT:\r\n            return getServiceAccountCredentials();\r\n        default:\r\n            throw new IllegalArgumentException(\"Unexpected authentication method \" + pubSubConfig.getAuthenticationMethod());\r\n    }\r\n}\r\n\r\nprivate CredentialsProvider getNoCredentialsProvider() {\r\n    return NoCredentialsProvider.create();\r\n}\r\n\r\nprivate CredentialsProvider getUserCredentialsProvider() {\r\n    try {\r\n        return FixedCredentialsProvider.create(GoogleCredentials.getApplicationDefault());\r\n    } catch (IOException e) {\r\n        throw new UncheckedIOException(e);\r\n    }\r\n}\r\n\r\nprivate CredentialsProvider getServiceAccountCredentials() {\r\n    try (InputStream stream = Files.newInputStream(Paths.get(pubSubConfig.getServiceAccountCredentialsFile()))) {\r\n        ServiceAccountCredentials credentials = ServiceAccountCredentials.fromStream(stream);\r\n        return FixedCredentialsProvider.create(credentials);\r\n    } catch (IOException e) {\r\n        throw new UncheckedIOException(e);\r\n    }\r\n}\nLastly, you will have to provide both the publisher and subscriber builder objects with a TransportChanneProvider. This provider will allow us to instruct Pub/Sub to use a plain text connection when we are running our application against the Dockerized Pub/Sub server. Furthermore, it also allows us to set the URL of the Pub/Sub server. This has been made configurable through the gcloud.pubsub.url property, which is set to localhost:8432 by default, but should be set to pubsub.googleapis.com when running in GCP.\npublic TransportChannelProvider create() {\r\n    ManagedChannelBuilder<?> channelBuilder = ManagedChannelBuilder.forTarget(pubSubConfig.getPubSubUrl());\r\n    if (AuthenticationMethod.NONE.equals(pubSubConfig.getAuthenticationMethod())) {\r\n    channelBuilder.usePlaintext();\r\n    }\r\n    ManagedChannel channel = channelBuilder.build();\r\n    return FixedTransportChannelProvider.create(GrpcTransportChannel.create(channel));\r\n    }\nDifferent scenarios, different settings\nThe table below shows the different scenarios that you can have when working with Pub/Sub and the appropriate property settings that go with each scenario:  You can also run the Application.java and publish a message with the use of the Swagger UI or checkout this Pub/Sub integration test.\n", "tags": [], "categories": ["Blog", "Development"]}
{"post_id": 2869, "title": "Stop making search optimizations based on your gut\u00a0feeling", "url": "https://www.luminis.eu/blog-en/search-en/stop-making-search-optimizations-based-on-your-gut-feeling/", "updated_at": "2020-12-01T09:27:39", "body": "You are running a website with lots of users. These users want to buy something, find the latest news, find a nice restaurant. You have learned from the internet that users that use search are more likely to stick around or buy something, at least if the search provides the right experience. You are not sure if that is the case for your website. Does your site offer the right user experience?\nYou want to validate this assumption on your website, but how? What reports do you need, what KPI\u2019s should you consider? This article discusses the different KPI\u2019s you need and how to collect them.\nIntroduction\nEvery website with a large amount of content like products or articles offers search functionality to its users. Overall, users have other options to buy products. These options can be using the search engine underneath, or direct links from a search engine like Google. However, often, users that do use the search box on your site tend to stay longer and buy more. Now is the moment you should ask me these questions: \u201cSure, but does this work for my site as well?\u201d, \u201cWhat should I do when it does not work for me?\u201d and \u201cHow do I track users on my site?\u201d. It is not uncommon to see a 5 to 10 times higher conversion rate for people using the onsite search engine in contrast with people that do not.\nYou can track how well your onsite search solution is performing using Key Performance Indicators (KPIs). Using KPIs like conversion, you can determine the impact of search on your users. You can also use KPIs to learn how your onsite search solution is used. Think about KPIs for the number of queries without a result and query results without interaction. In this article, we discuss the top KPI\u2019s that you should monitor for an onsite search solution. We group the KPI\u2019s in three different categories: Used search terms, found results, and interaction with the results. There are a lot of different tools that can help you to track your users. Apparent choices are Google analytics or Adobe analytics. These tools have search metrics available; however, too little out of the box. Google can use tools like Google Tag Manager. Another option is to create your data pipeline that is capable of receiving and handling all website events. There are a lot of cloud-based platforms that facilitate this architecture.\nKPIs for used search terms\nUsers can type all sorts of different queries or search terms. The intent for each search term is not always clear. It could be different from user to user and even from situation to situation. Metrics do not always clarify the intent of the user. Still, it remains essential to look at the search term metrics to get an idea about what users are looking for. In the case of an online book store, they could be searching for book titles, authors, genres. Another area is the diversity of queries. Does everybody use the same terms, or do they look for very different things? Head versus Tail\u200a\u2014\u200aWhen analyzing the used queries, we separate the head from the tail. The head is the part with the terms that your users use the most. Usually, the head is around the top 20% of the terms that generate around 50% of your income. Optimizing the head is often easier than optimizing the tail. They both represent half your income, but having a good tail behavior makes the difference with your competitors.\nKPIs for found results\nNext to analyzing the search terms, you learn a lot from analyzing the results. By analyzing the search results, you get an idea about items that are seen regularly. You can learn about the used search terms to find the same result. The metrics also provide insight into the different categories of items that are found. In the case of an eCommerce website with very different groups of products, the metrics tell you if some categories are found more often than others. In the case of different suppliers, metrics can indicate those suppliers that enter metadata wrong to get better search positions. The zero results metric is another essential metric. This metric deals with search terms that do not return any results. Additional metrics are required to understand why these search terms do not return any results. Think about typos, using too many words, searching for things that are just not available. Helping users find what they are looking for\u200a\u2014\u200aIn an ideal world, the content is written in a language that your users understand. In a perfect world, your users do not make mistakes, and you understand completely what your users are looking for. However, we are not living in an ideal world. Search engines provide technologies to overcome these mistakes like stemming and fuzzy search. To better understand your users, you want to get an idea about the effectiveness of your users as well. Using Machine Learning, you can analyze your user\u2019s sessions.\nKPIs for interaction with results\nThe real quality of your search engines appears from the way actual users make use of it. Without tracking how your users are interacting with the results, you will never know how your search engine is performing. An Essential KPI is the Click Trough Diagram that shows which items in the list are clicked. Theoretically, the clicks from position one in the list to position ten should decrease exponentially. Another metric is the search terms with results that are never clicked, indicating no conversion, or for which users always go to the second page of results.\n\nComparing sessions with and without using search\nBesides these search specific queries, you should also monitor the differences in standard website metrics between sessions with and without using search. You should monitor metrics like bounce percentage, conversion, time on site, average session value.\nThe first step\nAfter reading this article, you could be in doubt about the current status of your search engine. You wonder what the performance of your search engine is and how your users are interacting with it. Maybe you have some metrics through familiar tools like Google Analytics or a custom system. Do you know how to use it? Do you have all the essential metrics available? Feel free to contact me in case of questions or if you are in search of help.\n", "tags": [], "categories": ["Blog", "Search"]}
{"post_id": 2784, "title": "Integrate your Spring Boot application with AWS", "url": "https://www.luminis.eu/blog-en/cloud-en/integrate-your-spring-boot-application-with-aws/", "updated_at": "2020-11-11T17:00:35", "body": "So you\u2019ve heard to cloud is hot and your team has decided your Spring Boot application must integrate with it. But what does integration with the cloud exactly mean? In this blogpost I\u2019ll walk you through ways to integrate your Spring Boot application with AWS.\nSpring Cloud makes interacting with AWS services super easy and you can stay safely in you Spring Boot annotation comfort zone. This post will cover four subjects; managing configuration, messaging, caching and infrastructure as a code.\nPreparation: defining SDK credentials\nTo be able to interact with AWS services, SDK credentials need to be configured. These credentials can be created in the IAM service of AWS. There are multiple options for setting the configuration in your application but the most easy and flexible way is to use Spring properties to define credentials. (cloud.aws.credentials.accessKey and cloud.aws.credentials.secretKey). To make this work the spring-cloud-aws-autoconfigure module needs to be added to your maven/gradle project.\nManaging configuration\nKeeping track and managing of all your environment and application specific properties can be a nightmare. Luckily Spring Cloud integrates with the AWS Systems Manager Parameter Store. The parameter store provides a secure and centralised solution for managing configuration. The parameter store can be found below the AWS Systems Manager service.\nWhen the spring-cloud-starter-aws-parameter-store-config module is added to your project, the service will automatically use properties from the parameter store. The convention for properties of the application with name \u2018demo-app\u2019 is as follows:\nSpring property:\u00a0spring.datasource.url\nParameter store:\u00a0/config/demo-app/spring/datasource/url\nWhen a Spring profile is activated, it will search for parameters specific to this profile. Below an example where the \u2018production\u2019 profile is activated.\nSpring property:\u00a0spring.datasource.url\nParameter store:\u00a0/config/demo-app_production/spring/datasource/url\nMessaging\nThe main AWS messaging services are SNS and SQS. The spring-cloud-aws-messaging module makes interacting \u2013 sending and receiving messages / notification \u2013 super easy. For sending messages the QueueMessagingTemplate can be used:\n\nimport com.amazonaws.services.sqs.AmazonSQS;\r\nimport org.springframework.beans.factory.annotation.Autowired;\r\nimport org.springframework.cloud.aws.messaging.core.QueueMessagingTemplate;\r\nimport org.springframework.messaging.support.MessageBuilder;\r\n\r\npublic class SqsQueueSender {\r\n\r\n    private final QueueMessagingTemplate queueMessagingTemplate;\r\n\r\n    @Autowired\r\n    public SqsQueueSender(AmazonSQS amazonSqs) {\r\n        this.queueMessagingTemplate = new QueueMessagingTemplate(amazonSqs);\r\n    }\r\n\r\n    public void send(String message) {\r\n        this.queueMessagingTemplate.send(\"physicalQueueName\", MessageBuilder.withPayload(message).build());\r\n    }\r\n}\nReceiving message is as easy as adding an annotation to a method with a specific queue name\n@SqsListener(\"queueName\")\r\npublic void queueListener(Person person) {\r\n    // ...\r\n}\nCaching\nSince Spring v3.1 a unified cache abstraction is added to the framework. This abstraction is based on caching on method level, each time a\u00a0targeted method is invoked, the abstraction will apply a caching behavior checking whether the method has been already executed for the given arguments. If it has, then the cached result is returned without having to execute the actual method.\u00a0 Spring Cloud AWS now integrates the AWS ElastiCache into this unified cache abstraction. ElastiCache is a Fully managed in-memory data store, compatible with Redis or Memcached. In Both Memcached and Redis is integrated in Spring cloud AWS, for Memcached an custom implementation is created and for Redis the existing Spring data redis module is used.\nConfiguring the caching can be done using an annotation on any class with the @Configuration annotation. The expiration can be specified for a specific cluster and is a value in seconds. Setting caches can be done using the @Cacheable annotation with the cache cluster as value.\n@EnableElastiCache({@CacheClusterConfig(name = \"firstCache\", expiration = 23), @CacheClusterConfig(name = \"secondCache\")})\r\n@Configuration\r\npublic class ApplicationConfiguration {\r\n}\r\n\r\n@Service\r\npublic class ExpensiveService {\r\n\r\n    @Cacheable(\"CacheCluster\")\r\n    public String calculateExpensiveValue(String key) {\r\n        ...\r\n    }\r\n}\nCloudformation\nCloud environments like AWS can become very complex pretty easily. Setting up an environment requires much more than just deploying an application, you have to think about things like VPC\u2019s, Gateways, Databases and Queue\u2019s. Also, for all the different components you need to specify what type of instance (micro, medium, large, etc.) you want to spin up. AWS has build CloudFormation for this particular use case and \u2013 not surprisingly \u2013 Spring Cloud supports this! CloudFormation is \u2013 what they call \u2013 infrastructure as Code and uses a JSON file that specifies what components you want to spin up. Below an example on how the integration works between Spring Cloud and CloudFormation:\nBelow an fragment from a cloudformation file where cache is being specified, please note that the logic name (MyAppCacheCluster) can be specified by the developer.\n\"MyAppCacheCluster\": {\r\n    \"Type\": \"AWS::ElastiCache::CacheCluster\",\r\n    \"Properties\": {\r\n        \"AutoMinorVersionUpgrade\": \"true\",\r\n        \"Engine\": \"memcached\",\r\n        \"CacheNodeType\": \"cache.t2.micro\",\r\n        \"CacheSubnetGroupName\" : \"sample\",\r\n        \"NumCacheNodes\": \"1\",\r\n        \"VpcSecurityGroupIds\": [\"sample1\"]\r\n    }\r\n}\nIf you want to use the created \u201cMyAppCacheCluster\u201d in your app, you can just specify this name as your cluster name in your cache configuration. The only requirement is that, within the CloudFormation file, you also specify to deploy your application using this cache. \n@EnableElastiCache({@CacheClusterConfig(name = \"MyAppCacheCluster\", expiration = 23)})\r\n@Configuration\r\npublic class ApplicationConfiguration {\r\n}\n", "tags": [], "categories": ["Blog", "Cloud"]}
{"post_id": 2712, "title": "Migrating to Azure Function v3 in 5 minutes", "url": "https://www.luminis.eu/blog-en/development-en/migrating-to-azure-function-v3-in-5-minutes/", "updated_at": "2020-11-16T16:15:27", "body": " Azure Function v3 is now officially ready for production! And as a .NET developer I\u2019m happy that I finally can use this new version. It provides netcore 3.x support. You can start using the latest improvements from Netcore and start using C# 8 syntax! It is time to upgrade from version 2 azure functions to this new version. I\u2019ve also added an explanation how to upgrade you azure devops build pipeline to be able to build netcore 3. It is easy and only takes 5 minutes. \nVisual Studio 2019 Upgrade\nAzure functions version 3 requires some upgrades being done to your visual studio. It is required to upgrade to version 16.4 or higher to be able to work with azure functions v3. This version check can be done in visual studio by right-clicking on the red icon on the bottom right.\n\nAmongst other things the upgrade of Visual Studio installs the Azure Function local runtime for version 3.\nUpgrading your Azure functions csproj file\nWhat also need to be done is to upgrade the application to netcore 3.1. It also requires the Azure Function Runtime Version 3.0. This can be done by editing the csproj file.\n\nAfter the .csproj file is edited the project is build as netcore 3.1. However, because the application was upgraded from a netcore 2.1 version, there are some compiler errors.\n\nThis can easily be resolved. Upgrade the Microsoft.NET.Sdk.Function nuget package to version 3.x.\n\nVisual Studio should now be able to run your azure function again when executing locally.\n\nUpgrading the Azure ARM template\nNow the Azure Function app is working with the v3 function runtime and netcore 3.x. What still needs to be done is to modify the ARM template so it will deploy the azure function correctly. This can be done by modifying the FUNCTION_EXTENSIONS_VERSION setting in the ARM template.\n\nUpgrading the Azure Devops pipeline to build correctly\nOnce there is a correct ARM template we need to deploy it using Azure devops. Your Azure Devops pipeline should be be able to build and deploy correctly. But if any build problem occur it could be that the following task prevents using the latest dotnet version. This task can be removed.\n\nIf the dotnet-ef command was used to generate sql upgrade scripts it will now fail because as of netcore 3.1. The dotnet-ef tool is not packaged with dotnet anymore by default.\n\nIf the error shown above occurs there is a dotnet-ef issue. Add the following task prior to executing the dotnet-ef command.\n\n\u00a0\u2013\u00a0script:\u00a0dotnet\u00a0tool\u00a0install\u00a0\u2013global\u00a0dotnet-ef\nThe build should now succeed when the devops pipeline is run. If it also automated the azure function deployment into azure the following warning will be shown in Azure.\n\nIt can be ignored unless problems occur. Microsoft gives this warning as possible csproj settings could be broken due to the upgrade. Their advice is to create a new function app project. Move the sources into that project. This way it is ensured the csproj file is correct.\nSummary\nBy following this guide you have now upgraded to the latest version of azure function. You are now ready to release your Azure function V3 to production!\n", "tags": [], "categories": ["Blog", "Development"]}
{"post_id": 2449, "title": "Migrating a Spring Boot service from Java 11 to Kotlin", "url": "https://www.luminis.eu/blog-en/development-en/migrating-a-spring-boot-service-from-java-11-to-kotlin/", "updated_at": "2020-11-16T08:26:11", "body": "At my current project we\u2019ve just finished migrating a Spring Boot 2.1.x service from Java 11 to Kotlin.\nWhile doing so we\u2019ve learned quite a few things along the way and I created some notes that I wanted to share in case somebody else runs into the same issues. It was our first Kotlin migration and getting to know the Kotlin language better was/is a lot of fun, but also confusing at times.\nThe lack of static properties in Kotlin\nIn Java, having a static property for something like a Logger is a very common use case. It\u2019s pretty straight forward, but in Kotlin there are a few different ways to solve the problem of defining a logger. Kotlin does not know the static keyword, so for instance one option is to use a companion object\n\nclass SomeService {\r\n    companion object {\r\n        private val LOGGER = LoggerFactory.getLogger(SomeService::class.java)\r\n    }\r\n}\nThe whole concept of a companion object was something I really needed to get used to. The above example was what we went with in our code, but there are several different ways do define a logger with their own pros and cons. Before I repeat a detailed explanation I would like to point you to\u00a0this insightful article on Baeldung about Kotlin and Loggers.\nData classes\nIn our Java based version of the service we were using Lombok to avoid most of the Java boilerplate code. We leveraged Lombok mostly for our Value or Data classes. It\u2019s also very easy to add a builder for for instance a DTO class.\nLombok does a lot for you in the background and you really should know the effect of adding a certain annotation, because it will generate quite some code. Getting rid of this \u2018magic\u2019 was one of the reasons we started looking at migrating some parts of our code base to Kotlin and leverage Kotlin Data classes with named parameters. Now let\u2019s take an example Person class which we will convert from Java + Lombok to a Kotlin data class.\n\nimport java.time.LocalDate;\r\nimport lombok.Value;\r\n\r\n@Value\r\n@Builder\r\npublic class Person {\r\n    private String name;\r\n    private String country;\r\n}\nThat\u2019s a pretty straightforward class right? Now in Kotlin you can create a data class by adding the data classifier before the class name.\n\ndata class Person(val name: String, val country: String)\nUsing named parameters in Kotlin allows you to use a similar construction as to using a Builder, but without having to generate a lot of boilerplate code like in plain Java.\n\nval person = Person(name=\"Jeroen\", country = \"The Netherlands\")\nBe careful with manual conversion\nAfter converting our main Spring Boot Application.java class and some modifications to the code, we tried to run our Spring Boot application and ended up with the following strange message:\nExecution failed for task ':demoservice:bootJar'.> Main class name has not been configured and it could not be resolved\nSpring Boot has been supporting Kotlin for a while now, so that could\u2019t be it. Generating a new Spring Boot project from https://start.spring.io. with Kotlin as the default language also did not immediately show an obvious answer, but the answer was staring us right in the face. Let\u2019s take a look at a basic Java version of an Application class.\n\n@SpringBootApplication\r\npublic class MainApplication {\r\n\r\n    private static final Logger LOGGER = LoggerFactory.getLogger(MainApplication.class);\r\n\r\n    public static void main(String[] args) {\r\n        SpringApplication.run(MainApplication.class, args);\r\n    }\r\n}\nIf you\u2019re new to Kotlin and would manually convert the Java class to Kotlin you might end up with something like this:\n\nclass MainApplication {\r\n    private val LOGGER = LoggerFactory.getLogger(MainApplication::class.java)\r\n    \r\n    fun main(args: Array) {\r\n        SpringApplication.run(MainApplication::class.java, *args)\r\n    }\r\n}\nComing from a Java background this still looks fine, but there is a slight difference if we compare that to the Spring Boot initializer generated Application class as seen below.\n\n@SpringBootApplication\r\nclass MainApplication\r\n\r\nfun main(args: Array) {\r\n\trunApplication(*args)\r\n}\nThose of you that read both code snippets carefully will notice there are no curly braces after the MainApplication class definition in the second example compared to the first code snippet. So the above example has the main function as a package level function. You might also notice that there is also no static keyword. Kotlin represents package-level functions as static methods. Kotlin can also generate static methods for functions defined in named objects or companion objects if you annotate those functions as @JvmStatic. If you use the @JvmStatic annotation, the compiler will generate both a static method in the enclosing class of the object and an instance method in the object itself. It turned out Spring really needed that static main method and after we moved the function out of the class the Spring Boot Gradle plugin was able to start our application just fine.\nSpring boot 2.1.x and Kotlin 1.3\nWe also ran into a problem when introducing detekt, a static code analyzer for Kotlin, into our build cycle. After adding the Detekt Gradle plugin (version 1.1.1) we ran into a strange exception during the Kotlin compile phase:\n\n> Task :springcommon:compileKotlin FAILED\r\ne: java.lang.NoClassDefFoundError: kotlin/coroutines/jvm/internal/RestrictedSuspendLambda\r\n       at java.base/java.lang.ClassLoader.defineClass1(Native Method)\r\n       at java.base/java.lang.ClassLoader.defineClass(ClassLoader.java:1016)\r\n       at java.base/java.security.SecureClassLoader.defineClass(SecureClassLoader.java:174)\r\n       at java.base/java.net.URLClassLoader.defineClass(URLClassLoader.java:550)\r\n       at java.base/java.net.URLClassLoader$1.run(URLClassLoader.java:458)\r\n       at java.base/java.net.URLClassLoader$1.run(URLClassLoader.java:452)\r\n       at java.base/java.security.AccessController.doPrivileged(Native Method)\r\n       at java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:451)\r\n       at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:588)\r\n       at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:521)\r\n       at org.jetbrains.kotlin.scripting.definitions.ScriptiDefinitionsFromClasspathDiscoverySource\nThis had to be some sort of classpath or dependency issue so it required some more debugging and looking at dependency trees to figure out what was going on. Apparently Spring boot (2.1.x) manages the Kotlin version for several of its modules, which for Spring Boot 2.1 is version 1.2.x of Kotlin. That specific version was conflicting with the version of our project, which was 1.3.x, and also with the version of Kotlin used by the detekt plugin. Luckily the fix was pretty simple. You can explicitly set the Kotlin version in your build.gradle so it will be used for all plugins within your project.\next['kotlin.version'] = '1.3.10'\nLuckily we were not the first to encounter this issue and there was already a thread on the specific problem. See https://github.com/spring-gradle-plugins/dependency-management-plugin/issues/235 for more background information.\n", "tags": [], "categories": ["Blog", "Development"]}
{"post_id": 19698, "title": "Another FD Gazellen award for Luminis", "url": "https://www.luminis.eu/blog-en/another-fd-gazellen-award-for-luminis/", "updated_at": "2020-02-24T14:31:53", "body": "In a spectacular award show the new FD Gazelle awards were presented in the Orpheus Theatre in Apeldoorn, The Netherlands on 12 November 2019. The FD Gazelle awards are a recognition for fast growing companies with a year on year growth of at least 20% over the last three years. Luminis falls in the \u2018large companies\u2019 category with a \u20ac10 million+ revenue.\u00a0\nLaurens Miedema, CFO at Luminis, is happy with the award: \u201cOver the last few years Luminis has grown at a steady pace. With our seven locations in the Netherlands and in the UK we can work closely with our customers. Our customers benefit from the knowledge and capacity of 200 Luminis employees. Having this size makes Luminis interesting for larger companies. Besides our consultancy our cloud, search and data intelligence solutions are taking off as well. All these factors lay the foundation for solid growth. \u201c\n\nFor questions about Luminis you can contact +31 (0)88-58 64 600 or send an email to info@luminis.eu\n", "tags": [], "categories": ["Blog"]}
{"post_id": 2399, "title": "Multiword synonyms in search using Querqy", "url": "https://www.luminis.eu/blog-en/search-en/multiword-synonyms-in-search-using-querqy/", "updated_at": "2020-11-30T15:41:41", "body": "There is one topic that gives even the toughest search engineers a headache, multi-word synonyms. There are some ways to sort of get a solution, one of them is a nice tool called Querqy. Querqy is created by Ren\u00e9 Kriegler.\nThe goal for this blog post is to introduce Querqy for Solr, give you an easy way to start experimenting using Docker. As a bonus, I\u2019ll show you how to use another tool called SMUI, which is a graphical user interface to help you write the rules.\nThe past week I talked with a few people that were responsible for adding synonyms to their e-commerce search solution. It is an integration of an e-commerce platform and Solr 4.8.1. There are limitations due to the Solr version. Next to that, there are lots of constraints due to the integration with the e-commerce platform. I had to explain the multi-word synonyms problem to them. If you have not heard from this before and like to learn more about the technical side, check the references. The editors finally found a tool that they can use to influence results returned to users. But now I was telling them they could not use it, as they fixed one product search, but they broke numerous others. When thinking about a solution, I remembered the talk from Rene Kriegler about Querqy. Querqy is a query re-write tool that accepts lots of rules to specify what to re-write. I could not find a version that I could use for Solr 4.8.1. Still, I wanted to have a good look at Querqy.\nSetting up the test environment using Docker\nIf you want to try this out yourself, checkout my github repo: https://github.com/jettro/querqy-tryout I don\u2019t want this to be an extensive Docker tutorial. I use Docker to make some things more manageable. Querqy comes as a plugin to Solr, which is not hard to accomplish using Docker. You can download the jar by following the link on the querqy website. For convenience, I have included the specific version I use in the Github repository. First, we need to create our own Docker contain with the jar file for Querqy included. Second, we create a docker-compose with three containers. For SMUI, we need a MySQL database. We need Solr, and we need to SMUI container. Below you can see the Docker container configuration for the Querqy container.\n\u00a0\nFROM solr:slim\r\nCOPY ./lib/*.jar /opt/solr/dist\r\nEXPOSE 8983\r\nCMD [\"solr-precreate\", \"gettingstarted\"]\r\nCOPY ./lib/ /var/solr/data/gettingstarted/conf/solrconfig.xml\r\n\nNext, you can see the docker-compose.yml containing the three mentioned containers.\nversion: \"3\"\r\n\r\nservices:\r\n  solr:\r\n    container_name: my_solr\r\n    build:\r\n      context: .\r\n      dockerfile: ./Dockerfile\r\n    image: my_solr\r\n    restart: \"no\"\r\n    volumes:\r\n      - \"./solrdata/:/var/solr/\"\r\n    ports:\r\n      - \"8983:8983\"\r\n\r\n  db:\r\n    image: mysql:5.7\r\n    restart: \"no\"\r\n    environment:\r\n      MYSQL_DATABASE: 'smui'\r\n      # So you don't have to use root, but you can if you like\r\n      MYSQL_USER: 'smui'\r\n      # You can use whatever password you like\r\n      MYSQL_PASSWORD: 'smui'\r\n      # Password for root access\r\n      MYSQL_ROOT_PASSWORD: 'password'\r\n    ports:\r\n      #  : \r\n      - '3306:3306'\r\n    expose:\r\n      # Opens port 3306 on the container\r\n      - '3306'\r\n      # Where our data will be persisted\r\n    volumes:\r\n      - ./mysql-db:/var/lib/mysql\r\n\r\n  smui:\r\n    image: pbartusch/smui\r\n    restart: \"no\"\r\n    environment:\r\n      - \"SMUI_DB_URL=jdbc:mysql://db:3306/smui?autoReconnect=true&useSSL=false\"\r\n      - \"SMUI_2SOLR_SOLR_HOST=solr:8983\"\r\n      - \"SMUI_2SOLR_SRC_TMP_FILE=/smui/temp/rules.txt\"\r\n    ports:\r\n      - \"9000:9000\"\r\n    expose:\r\n      - '9000'\r\n    volumes:\r\n      - ./smui_path:/smui/temp\nI still have some issues with the auto replacement of the rules.txt using the provided SMUI script. Therefore I copy the rules file as generated by SMUI by hand. There is also some default setting up commands that you need to run only once. You can find them below:\n\u00a0\n# Initialize SMUI\r\ncurl -X PUT -H \"Content-Type: application/json\" -d '{\"name\":\"gettingstarted\", \"description\":\"Gettingstarted\"}' http://localhost:9000/api/v1/solr-index\r\n\r\n# Copy the solr config\r\ncp ./lib/solrconfig.xml ./solrdata/data/gettingstarted/conf/solrconfig.xml\r\n\r\n# Copy the rules.txt as generated by SMUI\r\ncp ./smui_path/rules.txt ./solrdata/data/gettingstarted/rules.txt\r\n\nSetting up Solr\nNow we are ready to configure the Solr core. First, we need to add two fields; then, we need to add some data; next, commit the data; finally, reload the core. If you use PAW, all commands can be executed using that, otherwise you have to execute the curl commands.\u00a0\n\u00a0\n## Add field title\r\ncurl -X \"POST\" \"http://localhost:8983/solr/gettingstarted/schema\" \\\r\n     -H 'Content-Type: application/json' \\\r\n     -H 'commitWithin: 1000' \\\r\n     -H 'overwrite: true' \\\r\n     -d $'{\r\n  \"add-field\": {\r\n    \"multiValued\": false,\r\n    \"name\": \"title\",\r\n    \"type\": \"text_general\",\r\n    \"stored\": true\r\n  }\r\n}'\r\n\r\n## Add field category\r\ncurl -X \"POST\" \"http://localhost:8983/solr/gettingstarted/schema\" \\\r\n     -H 'Content-Type: application/json' \\\r\n     -H 'commitWithin: 1000' \\\r\n     -H 'overwrite: true' \\\r\n     -d $'{\r\n  \"add-field\": {\r\n    \"multiValued\": false,\r\n    \"name\": \"category\",\r\n    \"type\": \"string\",\r\n    \"stored\": true\r\n  }\r\n}'\r\n\r\n## ADD all docs\r\ncurl -X \"POST\" \"http://localhost:8983/solr/gettingstarted/update\" \\\r\n     -H 'Content-Type: application/json' \\\r\n     -H 'commitWithin: 1000' \\\r\n     -H 'overwrite: true' \\\r\n     -d $'[\r\n  {\r\n    \"title\": \"My trip to San Francisco\",\r\n    \"category\": \"travel\"\r\n  },\r\n  {\r\n    \"title\": \"My hobby is horseback riding\",\r\n    \"category\": \"sports\"\r\n  },\r\n  {\r\n    \"title\": \"IT\",\r\n    \"category\": \"movie\"\r\n  },\r\n  {\r\n    \"title\": \"IT chapter two\",\r\n    \"category\": \"movie\"\r\n  },\r\n  {\r\n    \"title\": \"IT chapter one\",\r\n    \"category\": \"movie\"\r\n  },\r\n  {\r\n    \"title\": \"Clean it\",\r\n    \"category\": \"housing\"\r\n  }\r\n]'\r\n\nOne step I did not mention yet is changing the Solr configuration to enable the Querqy query re-writing step. This step is well documented in the Querqy documentation. Look at the solrconfig.xml file in the git repository; the relevant part is the addition of a query parser with the name querqy. This part configures the\u00a0re-write chain\u00a0as well as the logging. Copy the solrconfig.xml from the lib to the Solr core configuration, again reload the core, and you are ready to start playing around.\nAdding the first rule\nI am not going to re-write the documentation entirely here. I do want to give you an idea about the available options. There are two parts for each rule: Input matching and output rules.\u00a0 Input matching \u2013\u00a0Select one or multiple words, you can specify to select them only if they are the complete query, or if the query starts with them, or if the query ends with them. Another impressive trick is you can select the starting part of a word and use the remainder in the synonym. See some of the examples below: \u201cIT\u201d -> Select only those queries that completely IT, no other terms \u201chouse -> Select only of the query starts with house ebook\u201d -> Select only if the query ends with ebook sofa* -> Match query like\u00a0sofabed\u00a0and search for\u00a0sofa bed.\nOutput rules\u00a0\u2013 There are several different rules for you to choose and use. One that we started this blog post for, is using synonyms with multiple words. The rule I like is adding boosting to a specific category.\n\u00a0\ncheap notebook =>\r\n\tUP(10): * price:[350 TO 450]\r\n\tDOWN(20): * category:accessories\nBesides boosting, you can add filters in almost the same way.\nnotebook =>\r\n\tFILTER: * -category:accessories\nThere is also an option to remove terms, and the final rule I\u2019d like to mention is called Decorate rules. With these rules, you can return a response that would indicate a redirect, for example.\nfaq =>\r\n\tDECORATE: redirect, /service/faq\nThere are some more advanced options, look at the Querqy documentation for more information about those options.\nWhat Querqy does\nAlthough it is nice that Querqy does what it does, and it helps us with multi-word synonyms, you might be wondering how it works. A good trick is to use the debug option in Solr, that way you can check the query after Querqy rewrote it.\nThe first example is the query: trip SF The matched rule is:\nSF =>\r\n    SYNONYM: San Francisco\r\n(title:trip (title:sf | (+title:san +title:francisco)^0.5))~2\r\n\nNotice the or\u00a0\u2018|\u2019\u00a0between the original and the synonym clause of the query, and the negative boost for the synonym it both would match. Another interesting example is the movie IT. This is a very common word. With the next version of the movie out now, the title has become IT Chapter one and IT Chapter Two. We can create a rule to math the word IT. Problem is that the document IT chapter two has two words. However, the document Clean IT contains only two words. Based on normal scoring this would score higher than the two new movies. Therefore we add a rule to boost category movies when someone searched for IT. Below the rule in SMUI, a graphical user interface to create Querqy rules. And below that the result of sending the query IT.\n\n\nI hope you now have an idea about the power of Querqy. Check the documentation for even more options.\nReferences\nhttps://github.com/renekrie/querqy https://github.com/pbartusch/smui https://lucidworks.com/post/multi-word-synonyms-solr-adds-query-time-support/ https://opensourceconnections.com/blog/2013/10/27/why-is-multi-term-synonyms-so-hard-in-solr/ https://github.com/jettro/querqy-tryout\n", "tags": [], "categories": ["Blog", "Search"]}
{"post_id": 19670, "title": "Luminis Devcon 2020", "url": "https://www.luminis.eu/no-category/devcon-2020/", "updated_at": "2020-02-24T14:31:54", "body": "And\u2026the date has been set! Please mark May 14th 2020 in your agenda as this is the date for the 6th edition of Luminis Devcon.\u00a0\nMore information will follow soon.\nMeanwhile you can enjoy the 2019 aftermovie:\n\n", "tags": [], "categories": ["No category"]}
{"post_id": 2336, "title": "Where to put WebSockets?", "url": "https://www.luminis.eu/blog-en/development-en/where-to-put-websockets/", "updated_at": "2020-11-18T11:11:15", "body": " In a typical 3-tier architecture of your application, you have three tiers or layers called presentation, application, and data. In a Spring Boot application, these can be implemented as controllers, services, and repositories. In this architecture, the controllers might form an API of your application to which the front-end communicates. Now where to put the WebSockets? And more specifically from where to call them?\nIt depends of course what the required functionality is. In our case, we wanted other user\u2019s updates (can also be creations or deletions) to the database to be pushed to the rest of the users. What would be a \u2018logical\u2019 place for this?\nWe put the WebSockets themselves into the presentation layer. This means that there is a (Spring) component that sends the actual messages via the opened WebSockets to the front-end (UI). This is a logical place for sending and receiving messages.\nAn intermediate code sample in a Spring context:\n@Controller\r\n@Slf4j\r\npublic class WebSocketController {\r\n\r\n    private final SimpMessagingTemplate messagingTemplate;\r\n...\r\n    public void sendCreated(DomainFullDto domainObject) {\r\n        log.debug(\"Sending created domain object via web socket: {}\", domainObject);\r\n        messagingTemplate.convertAndSend(\"/topic/domainobjects\", new WebSocketMessage(\"DOMAIN_OBJECT_CREATED\", domainObject));\r\n    }\r\n}\nBut then the more tricky part. Where to call them from? What is the trigger?\nAt first, we put calling the WebSockets in the controller. If a user sends an update of a domain object, the receiving endpoint not only sends the updated object back but sends this object also to the other/all users through a WebSocket. This seemed logical because all code for the communication to the front-end remained in the presentation layer.\nLater on, we build other functionality which also updates domain objects triggered by scheduled jobs. Because no user interaction triggered these updates, we could not place any hook in the existing code of the controllers. We did want the updated domain objects to be immediately available in the front-end. So the triggering of the WebSockets needed to be placed elsewhere.\nI didn\u2019t want the triggering to happen from all over the code, so I proposed a new centralised place. I used aspect-oriented programming (AOP) to build this. This is a cross-cutting concern to place hooks to trigger some other code. in our case the trigger should be \u2018successful modifications to the database\u2019. So I put aspects on the successful returns of methods of the repository for database manipulations.\n\nThe aspect is responsible for catching the updated domain object(s), transforming it to data transfer object(s) (DTO(s)) for the front-end and sending it through the WebSockets.\nA code sample in a Spring context:\n@AfterReturning(pointcut = \"execution(* eu.luminis.application.datastore.DomainObjectRepository.save(*))\", \r\n            returning = \"domainObject\")\r\n    public void afterSaveDomainObject(DomainObject domainObject) {\r\n        log.debug(\"Caught saved domain object to send via websocket: {}\", domainObject);\r\n        var savedDomainObjectDto = domainObjectMapper.domainObjectToDto(domainObject);\r\n        webSocketController.onSaved(savedDomainObjectDto);\r\n    }\nNow if future development of the application also requires manipulation of the database, automatically the functionality is used to send the updates to the front-end. No need for developers to think about it anymore. It is future proof!\nOther things to consider\n\nWhat are the disadvantages? A disadvantage of this setup is that all successful updates of the domain objects are always sent via the WebSockets. If you do not want certain updates to be pushed, you have to create alternative flows for them or make the setup less generic. This could make things more confusing.\nDo you send the updated domain object DTO double to the user who did the update? That is as a response to a REST update request and through the WebSocket? Due to for example a firewall some users might not have the ability to receive WebSocket messages. For them, it is important that they still receive the REST responses.\nHow to handle the updated domain objects in the front-end? Especially if you receive them double? To prevent sending them double you could do a check in the back-end before sending a message through the WebSocket. For this, you need to know who triggered the update. It might be easier to just send messages to all users. In the front-end, you just process updates received from WebSockets differently than from REST responses.\n\nFeedback\nIf you have any questions or remarks, please let me know!\n", "tags": [], "categories": ["Blog", "Development"]}
{"post_id": 2331, "title": "Release features and test on a separate environment", "url": "https://www.luminis.eu/blog-en/development-en/release-features-and-test-on-a-separate-environment/", "updated_at": "2020-11-16T17:05:34", "body": "In this blog I will talk about how we used the CI/CD environment in Azure DevOps to deploy a branch /feature to its own Azure environment.\nWhy\nIn my ideal scrum team, the Definition of Done says that the product owner or tester needs to approve/test the feature. The only way for her to do this properly is to see and test the feature on an environment as close as possible to production. You could merge and release the feature to the develop branch en deploy it but now other features might already have been started with the previous changes and it makes it hard revert the changes or release the other feature until the first one has been fixed.\nLet me give an example, you have building a feature that connects to an external API. You\u2019ve tested everything it seems to work, now you merge it into develop and the tester or product owner finds an issue that needs to be resolved in the external API, and it will take 1 month. What do you do, other features have already been branched of develop and maybe even be completed. Do you have feature toggles build in for every feature, do you manually remove the changes from develop or do you postpone the release for one month?\nWould it not be great if we could fully test the feature on Azure, before anything gets merged to develop or master.\nInfrastructure as code\nFor all this to work it\u2019s that your whole environment is scripted so it can be deployed automatically when needed. Infrastructure as code makes deploying your environment so much easier and predictable. In Azure you can use the Azure Resource Manager (ARM) and specifically ARM Templates, these JSON templates can be checked in into source control and used in your Build and Release pipeline to create our environment.\n\n{\r\n  \"$schema\": \"https://schema.management.azure.com/schemas/2015-01-01/deploymentTemplate.json#\",\r\n  \"contentVersion\": \"\",\r\n  \"apiProfile\": \"\",\r\n  \"parameters\": {  },\r\n  \"variables\": {  },\r\n  \"functions\": [  ],\r\n  \"resources\": [  ],\r\n  \"outputs\": {  }\r\n}\r\n\nPull-request\nWe need to decide we want to release to the new feature environment, we have chosen to do this when a pull-request is made. This has several advantages; you want to test before you complete the pull-request so it makes sense to release when a pull-request it is created. Besides that, a pull-request build, is merged with master and the build is invalidated if master changes. Because of this you are a 100% sure that if you complete the pull-request this is exactly the same code that will end up on your target branch (probably develop or master) as been released to the feature environment.\nRelease pipeline\nOnce you made sure you have a Build definition that has all the artifacts needed you can get started on the release. I\u2019m using the preview yaml multi-stage pipelines.\nWe\u2019ve created a stage trigger that builds every time the source branch is not equal the master branch, in our case these are the pull-request builds (because they are merged with master, they are not equal to master).\n\n- stage: Deployment_To_Feature\r\n    condition: and(succeeded(), ne(variables['Build.SourceBranch'], 'refs/heads/master'))\r\n    dependsOn:\r\n      - Build\r\n    displayName: Deploying feature\r\n\nNow that we have a stage that represents the feature release triggered by a pull-request. There is one more challenge, we need to make sure that every feature has its own unique environment in Azure so that they don\u2019t interfere with each other. To accomplish this, we use our branching name convention to determine the id of the environment we need to roll out. For example when we have a feature with id 2223 the branch that contains the work would be called feature/2223_{name}, we want to create an environment 2223 in azure when this branch is build.\n\n$variant = $env:variant\r\nif (!$variant) {\r\n    Write-Host \"variant is null\" \r\n    $sourcebranchname = $env:SYSTEM_PULLREQUEST_SOURCEBRANCH\r\n    if (!$sourcebranchname) {\r\n        Write-Host \"Getting name from source branch\"\r\n        $sourcebranchname = $env:BUILD_SOURCEBRANCHNAME\r\n        $sourcebranchname -match \"([^\\/]\\d+)\"\r\n        Write-Host $sourcebranchname\r\n        $variant = $Matches[0]\r\n    }\r\n    else {\r\n        Write-Host \"Getting name from pull-request source branch\"\r\n        $sourcebranchname -match \"([^\\/]\\d+)\"\r\n        Write-Host $sourcebranchname\r\n        $variant = $Matches[0]\r\n    }\r\n}\r\nWrite-Host \"output variant is: $variant\"\r\nWrite-Host \"##vso[task.setvariable variable=variant;]$variant\"\r\n\nThis script will determine this id that we call the variant. If you don\u2019t provide one it gets it from either the pull-request source branch or from the actual source branch in the case of a non-pull-request build.\nNow we have is id we can postfix it to our environment and deploy our environment and code. In the example below you see the 3 tasks that get the environment ready to deploy.\n\u2022 pwsh runs the powershell script to determine the variant.\n\u2022 AzureCLI tasks creates a resource group to deploy the infrastructure to.\n\u2022 AzureResourceGroupDeployment task takes care of deploying the infrastructure as described in the ARM templates.\n\n- pwsh: $(Agent.BuildDirectory)/${{parameters.artifactName}}/ps/determineVariant.ps1\r\nname: Set_variables\r\ndisplayName: Set variables\r\nenv:\r\n    variant: ${{ parameters.variant }}\r\n\r\n- task: AzureCLI@1\r\ndisplayName: Create Resource Group\r\ninputs:\r\n    azureSubscription: ${{parameters.azureSubscription}}\r\n    scriptLocation: \"inlineScript\"\r\n    inlineScript: az group create -n res$(variant) -l ${{parameters.resourceGroupLocation}}\r\n\r\n- task: AzureResourceGroupDeployment@2\r\ndisplayName: deploy ARM template\r\ninputs:\r\n    azureSubscription: \"${{parameters.azureSubscription}}\"\r\n    action: \"Create Or Update Resource Group\"\r\n    resourceGroupName: \"res$(variant)\"\r\n    location: \"West Europe\"\r\n    csmFile: \"$(Agent.BuildDirectory)/${{parameters.artifactName}}/ARM/azuredeploy.json\"\r\n    deploymentMode: \"incremental\"\r\n    overrideParameters: -variant $(variant)\r\n\nThe only thing now is to make sure your tester or product owner gets the URL of the environment that contains the id, and make sure he approved the pull-request and (automatically) also completes the feature.\nDo you want to know everything about Azure Devops checkout Azure Devops workshop from the Luminis Academy.\n", "tags": [], "categories": ["Blog", "Development"]}
{"post_id": 2210, "title": "Neo4j For Python Users and Broken Pipe Error", "url": "https://www.luminis.eu/blog-en/data-en/neo4j-for-python-users-and-broken-pipe-error/", "updated_at": "2020-11-30T15:37:57", "body": "Neo4j is probably the most favorite graph database for many developers by providing:\n\nBiggest and Most Active Graph Community\nHighly Performant Read and Write Scalability\nHigh Performance\nEase of use\nFree licenses\n\nAs a data scientist, I enjoy every moment of working with Neo4j because of the above reason as well as its application within data scientists, such as:\n\nRecommendation systems\nFraud detection\nSupply chain transparency and optimization\n\nAlthough the great promises Neo4j can create for data-scientists, it can still be unknown for many data scientists at this moment. Nevertheless, Python is a well-known\u00a0tool in the data-science toolkit and that is the reason I would like to address Neo4j within Python for data science purposes.\nThe aim of this article is twofold:\n\nA short introduction on how to use Neo4j within Python\nAddressing the \u2018Broken Pipe\u2019 error which can happen when you use Neo4j with Python.\n\nThis article is written in a simple language however to follow this tutorial, you need to have a basic knowledge and understanding of Python and Cypher Queries (the query language of Neo4j).\nNeo4j package\nNeo4j released its own package to help python users communicate with Neo4j through Python language. It is of great help to have the power of Python next to Neo4j. You can download this package from its official website:\u00a0https://pypi.org/project/neo4j/\nAlthough there are other third-party Python packages for Neo4j, I highly recommend you to use the official package, as it is maintained better than the rest and is more popular among programmers.\nCypher Query Language\nJust like many other databases, Neo4j has its own query language which in my opinion is very intuitive and easy to learn. For example, creating a node in Cypher language is:\nCREATE (w:MyNodes {Name : 'John',  Title : 'President',  Age : 22}) RETURN id(w);\nI do not aim at this article to cover Cypher queries. You are encouraged to check other tutorials for this purpose.\nRunning Cypher Queries in Python\nThere are many scenarios where you would like to run Cypher queries within Python. Think of parsing an XML file where the data should be extracted from a complex XML file and translated to Neo4j. Python and its packages are ideal to parse XML files, so you can combine the power of Python with Neo4j. Additionally, as a data scientist, Python offers many packages for machine learning and data analytics.\nHere is an example of running Cypher Query in Python:\nfrom neo4j import GraphDatabase\r\nuri = \"bolt://localhost:7687\"\r\nuser=\"xxxx\"\r\npassword=\"xxxx\"\r\n\r\ndriver = GraphDatabase.driver(uri, auth=(user, password))\r\nwith driver.session() as session:\r\n    session.run(\"CREATE (w:MyNode {Name : 'John',  Title : 'President',  Age : 22}) RETURN id(w)\")\r\n    session.close()\nThe above example is the simplest way of running a Cypher query within Python. By running the above query, a new node is created in your Neo4j database where you can check it in your Neo4j instance.\nWhen things get complex and \u201cIOError: [Errno 32] Broken pipe\u201d error\nMy personal experience is when you try to write huge files (more than 4000 nodes + 4000 relations), the performance of the communication between Python and Neo4j degrades. You probably get the Broken Pipe error which does not let you upload more nodes and relations into your Neo4j database.\nI tried many things to solve this issue including setting up the \u2018driver\u2019 parameters and trying to let Python ignore this error and etc.\nhowever, none of them worked for me. The only out of the box solution that I came up with is the following:\n\nInstead of running queries from Python, saving them to a text file (called Cypher query file). It can have any extension such as .txt but the official extension is .cypher or .cql.\n\nIn our example here the file called example.txt and its content is the below two lines (two Cypher queries):\nCREATE (w:MyNodes {Name : 'John',  Title : 'President',  Age : 22}) RETURN id(w);\r\nCREATE (w:MyNodes {Name : 'Alan',  Title : 'Manager',  Age : 33}) RETURN id(w);\nPython programmers are aware to make such a file is not difficult. In our example this part is fixed:\nCREATE (w:MyNodes {Name : 'xxx',  Title : 'xxx',  Age : xxx}) RETURN id(w);\nAnd the variable parts (marked as xxx) are filled during parsing our XML file (in our hypothetical scenario).\nPlease note that semicolons at the end of each line are compulsory in our example.\u00a0\n\nGo to your terminal and run the following command:\n\nCat example.txt | cypher-shell -u USERNAME -p PASWWORD -a ADDRESS_OF_NEO4J\n[for many users ADDRESS_OF_NEO4J is \u201cbolt://localhost:7687\u201d )\nOnce you run the above command your database will be updated with new data. In my last use-case, I have almost 200k queries (nodes + relationships). I remember when it reached almost 8k (nodes + relationships) I constantly got a \u201cBroken Pipe\u201d error and this is what I have done to solve the issue.\nConclusion\nIn this article I tried to make you familiar with how to use Neo4j within Python and to the best of my knowledge is the only resource on the internet (at the time of writing this tutorial) that addresses the \u201cBroken Pipe\u201d error and its solution. I hope you can make the best use of these two powerful tools and enjoy working Neo4j within Python.\nReferences\n\nhttps://neo4j.com/top-ten-reasons/\nhttps://neo4j.com/blog/5-noteworthy-use-cases-graph-technology-analytics/\n\n", "tags": [], "categories": ["Blog", "Data"]}
{"post_id": 2220, "title": "3 ways Cloud is disrupting design", "url": "https://www.luminis.eu/blog-en/cloud-en/3-ways-cloud-is-disrupting-design/", "updated_at": "2020-11-17T15:38:40", "body": "Okay. So Cloud is disrupting our world. It is vastly changing the way we develop, deploy and maintain our digital products. Although indirectly, it is also impacting the field of UX and UI design. Some of these changes might be less visible or obvious to us, but they are real. I believe designers and their work would benefit from taking these implications into account. And even if you\u2019re not a designer, it wouldn\u2019t hurt to continue reading.\n1: The way designers work\nDesign collaboration will keep moving towards Cloud services and tools for sharing, reviewing, testing and ultimately also creating designs. Adobe is a good example of a major software company transitioning towards hybrid products that combine the richness of desktop software and the ubiquitousness of data- and asset sharing, as well as integration with other Cloud services. But also the new(ish) kids on the block, like Sketch or InVision, each is rooted (with at least one foot) in the Cloud.\n2: The work designers deliver\nA Cloud product is usually more than what you perceive on a single device; it is an ever-present, ever-live service that can be interacted with in many ways. By many devices, in many modalities and by many actors; some human, some machine. Consistent experiences across contexts will be expected, while simultaneously taking advantage of device- and context-specific qualities. For example: hierarchical information in a user interface is often represented in a tree view. Although a powerful visualisation, a multilevel tree is not viewed or navigated well on small screens. Drag and drop is another pattern that works well on a desktop with a large screen and a mouse, but much less on a touchscreen. When designing for Cloud products we need to prevent device-specific patterns to keep UX consistent and predictable across different touch points.\nComplex technologies like object recognition, voice interfaces and deep learning have become available for smaller budgets than we\u2019d ever imagined possible a few years back. It is now relatively cheap and easy to disclose (parts of) your Cloud product for voice assistants, which increases the product\u2019s reach and in a lot of cases could offer a great advantage for the customer. A few years ago applying this kind of UX would be unthinkable in most cases.\nCloud concepts will ultimately change our perception of what a computing device is and what we expect from it; we need to change our UI paradigm accordingly. For example: the save button will become obsolete now every mutation becomes stored in the Cloud automatically. This does introduce some interesting new challenges around concurrency, data synchronisation and locking. These are exactly the kind of things designers should be focussing on right now.\n3: The way design integrates into the development process\nA Cloud product is never done. Cloud will further boost the concept of early MVP shipping and continuous monitoring and improving of the product. Faster development and easier deployment enables designers to test their ideas in production. Like the way a company like Deliveroo is developing their services: instead of creating a testable prototype for a new feature, a quick-and-dirty implementation of the feature is done in the live software. The new feature is then rolled out for a limited amount of users, evaluated, rejected or iteratively improved upon, while the sandbox is gradually increased until the feature is exposed to all users. A completely new approach to research and design, and a trick that could have hardly been pulled off without the Cloud.\nSome things will never change\nYes, Cloud will impact nearly every aspect of our work. The essence of the designer\u2019s role will remain the same though: translating business and customer goals into compelling products. But businesses, customers, their goals and needs, and what they will perceive as compelling will all change. Which in turn means the experiences we design and the way we design them also should. At least, if we want to keep ahead in the Cloud.\n", "tags": [], "categories": ["Blog", "Cloud"]}
{"post_id": 2127, "title": "Generate a Cloud Backend using AWS Amplify", "url": "https://www.luminis.eu/blog-en/cloud-en/generate-a-cloud-backend-using-aws-amplify/", "updated_at": "2022-06-24T09:14:21", "body": "Ever had a great idea for a web or mobile app, however the thought of creating an entire backend consisting of user management, API\u2019s, storage and hosting kept you from creating it? With AWS Amplify, you can generate a backend by answering some questions in a command line tool and use the SDK to quickly use this new backend. This blog will show you how to get started\nWhat is AWS Amplify?\nAWS Amplify is a set of tools to help you connect your web or mobile app to AWS Cloud resources.\u00a0 These include:\n\nAWS Command line interface (CLI): Allows you to create AWS resources from the command line and set up your project to make use of them\nLibraries/SDK: JS, Android and iOS libraries to help you easily access these cloud resources from your application. Available for many frameworks.\nUI components: UI components for common use cases related to the AWS resources you can create with the CLI.\nAmplify console: The console shows you all your Amplify projects for a region in an account and allows you to set up a CI/CD pipeline and manage a lot of things related to your project.\n\nThe UI components part is optional, however to get the most out of Amplify you will need to use the CLI\u2019s and libraries.\u00a0With Amplify you can generate and use AWS resources by simply answering some questions in the CLI and using the libraries provided by Amplify.\u00a0There are Amplify libraries for React and React-Native, Angular and Ionic, Vue.js, Native Android and iOS, Flutter and NextJS.\u00a0Amplify consists of categories of services which can be configured:\n\nAnalytics: Collect analytics for your app. Makes use of Amazon Pinpoint and Amazon Kinesis.\nAPI: HTTP REST or GraphQL API. Uses API Gateway(REST) or AWS AppSync (GraphQL)\u00a0\nAuthentication: Create user authentication. Makes use of Amazon Cognito.\nFunction: Create serverless functions. Makes use of AWS Lambda.\nHosting: Hosts your web app in the Cloud. Makes use of S3 and Amazon CloudFront.\nInteractions: Add Chatbot functionality to your app. Makes use of Amazon Lex.\nPubSub: Create a publish-subscribe message-oriented middleware. Makes use of AWS IoT and MQTT over WebSockets.\nPush Notifications: Set up push notification functionality for mobile apps. (only available for React Native)\nStorage: Add Content or a NoSQL database. Makes use of S3 and DynamoDB\nXR: Add Augmented reality to your app. Makes use of Amazon Sumerian.\n\nIn this blog we will cover Hosting, Authentication, API, Function and Storage.\nRequirements\nIn order to get started, we need to have the following:\n\nAn Amazon AWS account. In order to create one, you will need a credit card.\nInstall AWS Amplify CLI.\n\nYou can find instructions here: https://aws-amplify.github.io/docs/ You also need:\n\nA code editor. (I use Visual Studio Code)\nSome knowledge of either React, Angular or Vue.js\n\n\nAdvice: I advise you to set a budget alarm for your AWS account just to be safe. Go to the \u201cBilling\u201d service in the AWS console and set a budget alarm for 10-20 dollars. You will get an email if your costs ever reach or pass this budget. I have not experienced this yet, as AWS free tier was enough for me so far.\nConfiguring Amplify CLI\nOnce you have the CLI installed and an active AWS account, run the first command (current directory does not matter yet), which is: \u201camplify configure\u201d which will open the AWS console login page. Log in with your account and follow the steps.\u00a0The steps will create an IAM account, which will have permissions to create AWS Cloud resources from the command line. You will be asked to enter an access key ID and a Secret Access key, which belong to the IAM user you create in the configure step. You only do this once.\n\nFrom this point on this IAM account is added as a local profile to your CLI and will be used to create Cloud resources. Note that this step does not have anything to do with your project specifically yet. This is only the configuration of your Amplify CLI. In case you already have other AWS profiles locally, check this page out related to multiple profiles: https://docs.aws.amazon.com/cli/latest/userguide/cli-configure-profiles.html. If you want to read more about IAM, see this page: https://aws.amazon.com/iam/\nInitializing Amplify Project\nNow that our Amplify CLI is configured, we can use it to \u201camplify\u201d our projects.\u00a0I have created a project, which is available on GitLab where you can check out all of the steps I took and compare it to your own project.\u00a0Use a terminal and navigate to the root of your new project. Run the following command: \u201camplify init\u201d. You will get a series of questions that you must answer in order for Amplify to initialise. I will share what I entered for my demo project:\n\n\n\nQuestion\nAnswer\nExplanation\n\n\nEnter a name for the project\namplify-blog-article\nThis name will be used to identify the project and will be used as a suggestion when adding functionality.\n\n\nEnter a name for the environment\ndev\nYou can add multiple environments, which will be discussed later. The name entered here will be used as a postfix for the name of certain resources created in later steps.\n\n\nChoose your default editor\nVisual Studio Code\nNot sure what this does, but just choose your default editor from the list\n\n\nChoose the type of app you\u2019re building\njavascript\nAmplify works differently depending on which you choose. This blog will focus on a javascript web app.\n\n\nWhich javascript framework are you using\nreact\nI create a react app with \u201ccreate-react-app\u201d. Feel free to choose which one you prefer.\n\n\nSource directory path\nsrc\nWhere your source code will be. Amplify will add some files there.\n\n\nDistribution Directory Path\nbuild\nWhere the distribution of your app will end up once it is built. Amplify will use it when you want to publish your app for Hosting on S3.\n\n\nBuild command\nnpm run-script build\nThe command that is used to build the distribution of your app.\n\n\nStart command\nnpm run-script start\nThe command that is used to start/run your app.\n\n\nDo you want to use an AWS profile?\nYes\nI really don\u2019t know what happens if you choose no here\u2026\n\n\nPlease choose the profile you want to use\namplify\nThis should be the name of the profile you used in the \u201cconfigure\u201d step. If it doesn\u2019t show up in the list of options, you need to go back and try configuring it again.\n\n\n\n\u00a0\nAfter you answer all of these questions, Amplify will create an S3 bucket and store the project configurations. It will also make the following changes to your project:\n\nYour .gitignore fill will get updated with paths to files that are generated by Amplify that should not be checked in.\nThese files are added (paths are relative to root directory of project):\n\namplify/.config/local-aws-info.json (gitignored): Tells Amplify which AWS profile to use.\namplify/.config/local-env-info.json (gitignored): Contains user and system preferences, such as default code editor\namplify/.config/project-config.json: Project specific information, such as where the src directory is located.\namplify/#current-cloud-backend/amplify-meta.json: Contains metadata of all resources pushed to the Cloud via Amplify. More on this later.\namplify/backend/amplify-meta.json: The same as the previous file, except this reflects the local state of your amplify resources. More on this later.\namplify/backend/backend-config.json: Contains a description of all the resources you have created with Amplify. It will be an empty object when starting off.\namplify/team-provider-info.json: This file has information about the environments tied to this project and should be checked into git so that other team members can also make use of these configured environments. More on this later.\nsrc/aws-exports: A file used to configure the Amplify frontend libraries, which we will be using later on in this article.\n\n\n\nAmplify works similar to Git with regards to file changes. You will first make changes locally, which will edit files in your amplify/backend directory. Once you push your changes, your amplify/#current-cloud-backend directory will be updated.\u00a0 You can run the following command to check the current status of your local changes: \u201camplify status\u201d which should show the following at this point:\nCurrent Environment: dev\n| Category | Resource name | Operation | Provider plugin |\n| -------- | ------------- | --------- | --------------- |\nIt is similar to the \u201cgit status\u201d command.\u00a0In the next section we will be making a change by adding Amplify Hosting.\nHosting\nNow we have our CLI configured and our project initialised, however we have not added any Amplify functionality yet. In this section, we will be adding the Amplify Hosting functionality, which will deploy our application to the Cloud and make it publicly available. To get started, we will run this command: \u201camplify add hosting\u201d This will trigger a list of questions again, which I answered in the following way:\n\n\n\nQuestion\nAnswer\nExplanation\n\n\nSelect the environment setup\nPROD (S3 with CloudFront using HTTPS)\nEven though I called the environment \u201cdev\u201d, I want to use https, so I chose PROD\n\n\nhosting bucket name\namplify-blog-article-20190630152229-hostingbucket\nIt was the default recommended name. You can name it whatever you want.\n\n\nindex doc for the website\nindex.html\nIndicates which file in the distribution directory is the index doc\n\n\nerror doc for the website\nindex.html\nSince I have a Single Page Application, errors will be handled by the framework and not by separate .html files. Feel free to change this if you need to.\n\n\n\nThat\u2019s it! We should now have the following local changes in our project:\n\namplify/backend/backend-config.json: An entry has been added that represents the hosting functionality.\namplify/backend/hosting/S3AndCloudFront/parameters.json: This file is created and contains the name of the S3 bucket that Amplify will publish the app to.\namplify/backend/hosting/S3AndCloudFront/template.json: Not sure what this does, looks complicated to me.\n\nIf you run \u201camplify status\u201d, you should see the following:\nCurrent Environment: dev\n| Category | Resource name   | Operation | Provider plugin   |\n| -------- | --------------- | --------- | ----------------- |\n| Hosting  | S3AndCloudFront | Create    | awscloudformation |\nWe can see that Amplify now has local changes. Note that nothing has been pushed to the Cloud yet. In order to push our changes to the Cloud and actually make the S3 bucket, we need to run: \u201camplify push\u201d. For Hosting I have noticed that this step could take a while, approximately 10 minutes. After that is done, an S3 bucket will have been created. You can log in to the AWS console in your browser and go to the S3 service. Here\u2019s mine:\n\nWARNING: Do not make any changes in the console here, because they will not get synced back to your Amplify project. More on that later. If you check your /amplify/#current-cloud-backend directory, it should now also contain a \u201chosting\u201d folder. Also, running \u201camplify status\u201d should now show this:\nCurrent Environment: dev\n| Category | Resource name   | Operation | Provider plugin   |\n| -------- | --------------- | --------- | ----------------- |\n| Hosting  | S3AndCloudFront | No Change | awscloudformation |\nHosting endpoint: https://d12609snxlw6tg.cloudfront.net\nNote that your app has not been published yet, only the S3 bucket has been created. To build your app and publish it so that it is available publicly, run: \u201camplify publish\u201d If you click on the link shown, the browser should open and you should see your app. (Blog update: Note that after publishing your application in this way for the first time it could take up to 30 minutes to actually see your app live. Before that you will see something in the range of \u201cAccess denied\u201d. The newer version of Amplify allows you to deploy via the Amplify Console which will show you immediate results). \u00a0For me, that is the default create-react-app starter page:\n There you have it, just like that your app is published and available online, with https setup. In the next section we will add the \u201cAuth\u201d functionality.\nAmplify push vs publish\nThe difference between \u201camplify push\u201d and \u201camplify publish\u201d is that the first one pushes your local Amplify changes to the Cloud, meaning that AWS resources will be created, updated or removed. The publish command\u00a0 performs an \u201camplify push\u201d and builds your app and pushes it to S3. So anytime you make changes in your app (not amplify), you can run publish to see it live.\nAuthentication\nIn this step we are going to be adding the Amplify Authentication functionality to our app. When we are done, users will be prompted to log in our register before being able to use our app. We will not only be making use of the Amplify CLI, but also the Amplify frontend libraries and UI components. We will start off by running this command: \u201camplify add auth\u201d. I answered the questions in the following way:\n\n\n\nQuestion\nAnswer\nExplanation\n\n\nDo you want to use the default authentication and security configuration?\nDefault Configuration\nChoose \u201cI want to learn more\u201d for an explanation\n\n\nHow do you want users to be able to sign in when using your Cognito User Pool?\nEmail\nUsers will register and sign in using an email as username.\n\n\nWhat attributes are required for signing up?\nEmail\nYou can add more if you want to enforce this, however for this project email is fine.\n\n\n\nThe following changes will have been made to your repository:\n\namplify/team-provider-info.json modified: A cognito pool is added to this configuration. This is part of the environment \u201cdev\u201d we created.\namplify/backend-config.json modified: \u201cauth\u201d has been added next to our \u201chosting\u201d configuration\namplify/backend/auth/<<cognitoPoolId>>/<<templatename.yml created: A YAML file describing the Cognito Pool, which will be used by Amplify to actually create the Cognito Identity Pool on AWS\namplify/backend/auth/<<cognitoPoolId>>/parameters.json created: Also used for the creation of the Identity Pool on AWS.\n\nRunning \u201camplify status\u201d now shows the following:\nCurrent Environment: dev\n| Category | Resource name   | Operation | Provider plugin   |\n| -------- | --------------- | --------- | ----------------- |\n| Auth \u00a0| cognito58be9282 | Create | awscloudformation |\n| Hosting  | S3AndCloudFront | No Change | awscloudformation |\nHosting endpoint: https://d12609snxlw6tg.cloudfront.net\nWe are now ready to push our changes to AWS by running \u201camplify push\u201d. This will again take several minutes to create the AWS resources. After this is done you can head on over to the AWS console in your browser and navigate to the Cognito service. Click on manage user pool and you should see one entry there. Click on it to see its configuration. Mine looks like this:\n\n\nYour amplify/#current-cloud-backend directory should have also been updated now to reflect the changes you pushed. Now it\u2019s time to make use of this Cognito Pool in our app. To do that, we first need to use npm to get two dependencies: \u201cnpm install aws-amplify aws-amplify-react\u201d. The second one can be the Angular or Vue.js dependency if you want to use another language. For this example I will be using React. From the aws-amplify-react I am going to be using the \u201cWithAuthenticator\u201d React component, which is going to wrap my App component and show a Sign in/Log in UI if the user is not yet logged in. To do that, I first need to import Amplify and configure it in the file I want to use it in. In the App.js file I am going to add the following:\nimport Amplify from 'aws-amplify';\nimport awsconfig from './aws-exports';\nimport { withAuthenticator } from 'aws-amplify-react'; // or 'aws-amplify-react-native';\nAmplify.configure(awsconfig);\nThe first line imports the Amplify component, which will be used by many other components in order to know which AWS resources should be used. The second line imports the configuration that Amplify generated for you when you ran \u201camplify init\u201d. This will be used to configure the Amplify component with the correct information. The third import is the withAuthenticator component, which I already mentioned. And at the bottom of the App.js file, I replaced \u201cexport default App;\u201d with \u201cexport default withAuthenticator(App, {usernameAttributes:\u2019email\u2019});\u201d That\u2019s it! Now if we run our app using \u201cnpm run start\u201d, we will see the Amplify Auth UI component asking us to log in or register:\n\nClick on Create account and fill in the form. Once that is done you will receive a confirmation email with a code:\n\nEnter this code and you will can now log in. After logging in you will see the home page of the app again. From this point on the user is logged in and you can make use of the \u201cAuth\u201d component from the \u201caws-amplify\u201d library to do things like get the currently logged in user. See this page for all the info. It would be nice to also give the user an option to sign out. You can import the \u201cAuth\u201d component and add a button somewhere that calls \u201cAuth.signOut()\u201d onClick. This will show the user Sign in/Register page again. See the repository for the complete example. In the next section we will be adding a functioning REST API that will allow us to CRUD a resouce in a database. It will also make use of the Cognito user pool we created in this step to authorize the REST calls.\nREST API, Lambda and Database\nNow that we have Hosting and users need to register and log in to view our site, it is time to add some functionality. We are going to use Amplify API, Function and Storage in order to create a functioning backend where we can CRUD a resource. In this case we will be able to create, get, update and delete a list of beer. To get started, we will run this command: \u201camplify add api\u201d and answer the questions as follows:\n\n\n\nQuestion\nAnswer\nExplanation\n\n\nPlease select from one of the below mentioned services\nREST\nREST or GraphQL\n\n\nProvide a friendly name\u2026..\napi83194554\nI just used the recommended, you can name it whatever you want.\n\n\nProvide a path\n/beer\nAdd the name of the resource you want to expose via your API\n\n\nChoose a Lambda source\nCreate new Lamda function\nWe have not created a Lambda function in Amplify yet\n\n\n\nFrom this point the questions will be about the new Lambda function which will be hooked up to our /beer endpoint in our API.\n\n\n\nQuestion\nAnswer\nExplanation\n\n\nProvide a friendly name\u2026\namplifyblogarticle703db77c\nI went with the recommended default again\n\n\nProvide the AWS lambda function name\nbeerLambda\nI gave it a name for the endpoint it will be hooked up to\n\n\nChoose the function template you want to use\nCRUD function for Amazon DynamoDB table (Integration with Amazon API Gateway and Amazon DynamoDB)\nThis will hook up DynamoDB for us in our Lambda function.\n\n\nChoose a DynamoDB data source option\nCreate a new DynamoDB table\nWe don\u2019t have a DynamoDB table yet.\n\n\n\nAnd now the questions will be about the DynamoDB database:\n\n\n\nQuestion\nAnswer\nExplanation\n\n\nProvide a friendly name\u2026category in the project\ndynamo5ed5aae4\nThe default again.\n\n\nPlease provide table name\nbeer\nName of the resource we want to CRUD\n\n\nAdding columns\nid: string\nWe only need an ID. If you want to be able to query based on other properties of your records in the DB, you can add these columns later. For this example that is not necessary\n\n\nDo you want to add a sort key to your table?\nNo\nNot necessary for this demo, however if you want to sort by any of the columns it will be a good idea to add this.\n\n\nDo you want to add global secondary indexes to your table?\nNo\nIf you are going to query the table based on something other than the ID, then it is a good idea to add this.\n\n\n\nA question about the Lambda function:\n\n\n\nQuestion\nAnswer\nExplanation\n\n\nDo you want to edit the local lambda function now?\nNo\nWe can look at the lambda in our code editor after this is done. By default there will be functions to CRUD the resource.\n\n\n\nAnd back to the API:\n\n\n\nQuestion\nAnswer\nExplanation\n\n\nRestrict API access\nYes\nWe want only users logged in to be able to make calls to our API\n\n\nWho should have access?\nAuthenticated users only\nSince registration happens outside of our API, everything else can be restricted\n\n\nWhat kind of access do you want for Authenticated users?\ncreate, read, update, delete\nWe want to be able to call all http methods. Make sure to use the space button to select all 4!\n\n\nDo you want to add another path?\nNo\nWe can always add more later.\n\n\n\nThere are a lot of changes in our repository after this action. I will only mention the most important ones:\n\namplify/backend/backend-config.json: Api,Function and Storage have been added.\namplify/backend/function/<<lamdaidentifier>>/src/app.js: This file contains the lambda function, which is an Node express server with 4 functions for the GET, GET ALL, POST, PUT and DELETE. Edit this file to change how your API calls get handled.\u00a0\n\nRun \u201camplify push\u201d to push all your changes to AWS and create the resources. Once this is done, we can log in to the AWS console in your browser and navigate to API Gateway to see that the API has been created:\n\nAnd clicking on it shows that the Beer path has been created as well:\n\n\u00a0\nNotice how we have a /beer resource but the methods we support are \u201cOPTIONS\u201d and \u201cANY\u201d. As we will soon see, this resource is mapped to a Lambda function which will handle any calls going to this API that starts with /beer. This includes the GET, PUT, POST and DELETE. Next, we can navigate to Lambda and see that our function has also been created:\n\nAnd finally, our table in DynamoDB has also been created: \n\u00a0\nJust like that we now have a REST API, a Serverless Function and a Database running and ready to use. We are going to be making use of another of Amplify\u2019s React libraries for API. At this point I added a React component that has a Table for showing all beer in the database and a form for adding new beer. Check the git Repo for the code, however here is a screenshot: \n\u00a0\nDisclaimer: I am no good with styling and also did not spend a lot of time on this. Feel free to create any kind of UI you want.\u00a0 The idea is that the user can enter a name, brand and type for the beer and click on Save. This should call our POST /beer endpoint which will save it in the database. When this is done, we will call a GET /beer to get all of the beer. this will then show in the table. There will also be an \u201cx\u201d button, which will call the DELETE /beer for that item.\nCreating new beer items /POST\nI created a BeerApi.js class which does the interaction with the Amplify API library. You can view the entire code in the Git repo, however this is the example code that does the POST: \u00a0 \nThis function gets the \u201cbeer\u201d object from the form in my UI and uses the API library to make a post call. Since Amplify is configured, it only needs the API name and Path to know which url it should use. Note that no \u201cid\u201d is sent in this body. Instead, I modified my lambda to inject a uuid whenever a post is done.\u00a0\nList all items /GET\nThe lambda function generated for the \u201clist\u201d items does not let you get all items in the /beer resource. I made some changes in the lambda function for the GET LIST:\n\nRemoved the \u201c+ hashKeyPath\u201d from the path\nRemoved KeyConditions from the query parameters\nused \u201cscan\u201d instead of query\n\nAnd then in the BeerApi.js, you can call:\n\nDelete item /DELETE This one did not require any changes to the lambda. Simply call this function with the id of the item you want to delete:\n Customize your business logic It is up to you to customize the Lambda functions to suite your needs. You have access to the request and to the Dynamo database at this point.\u00a0\nShortcomings\nWith a few quick commands we were able to publish our app which has an authentication page and can make calls to an API that stores data in a database. However, Amplify is very new and there are some drawbacks.\nAmplify push takes a long time Every Amplify push command takes at least 5 minutes. This can be frustrating, especially when editting Lambda\u2019s. There is a way to test your functions/lambda\u2019s locally, however I could not get it running with DynamoDB.\nBlack magic It can sometimes feel like you are not in control since so much is done for you. If something goes wrong it can take a while to figure out where to problem lies. However, there is a lot of documentation and most things get logged to the CloudWatch service. Knowledge of the AWS services behind the Amplify functionality helps here. Like most things, the more you use it the more you start to understand and get better at it.\nConclusion\nAWS Amplify is a useful tool to quickly get your (Web) app up and running. The reason it is so powerful is the combination of the CLI and libraries, which automatically hook up the different services together. In its current state I advise you to use this for small beginner projects with small teams, since it can get tricky when working with multiple people. Create different environments if that is the case. There is still a lot more you can do with Amplify as mentioned in the intro. Also: You do not need to use all of the functionality of Amplify. If you only want to use the AUTH part for example, that is also possible. The documentation gives clear examples of how you could do this. It is up to you to decide what you want to use Amplify for and what you want to use something else for. I hope this article has given you an idea of what is possible with AWS Amplify. If you have any questions please let me know in the comments of this article. Good luck and enjoy.\nLearn from our experts:30 Jan 2021-Evertson CroesTraining: AWS AmplifyHave you always wanted to create a Cloud native app but you don\u2019t know where to begin? The Cloud can be an intimidating concept. In this training you will get a good introduction to the Cloud and you are going...\n", "tags": [], "categories": ["Blog", "Cloud"]}
{"post_id": 18587, "title": "Luminis is Microsoft Gold partner for DevOps services", "url": "https://www.luminis.eu/blog-en/luminis-is-microsoft-gold-partner-for-devops-services/", "updated_at": "2020-02-24T14:31:55", "body": "Amersfoort \u2013 Software and technology company Luminis recently had the honour of becoming a Microsoft Gold Partner in the DevOps competency. Every year Microsoft only rewards a few companies with Gold-status recognition in particular competencies. Accreditation is awarded to companies that are able to constantly deliver high-quality services and developments. Earlier this year, Luminis was awarded with the Gold-status in the Application Development competency.\nThe partnership with Microsoft is in line with the international strategy of Luminis which is mainly focused on being a high standard technology and innovation partner for its customers. As a Microsoft Certified Partner Luminis and Microsoft collaborate intensively with one another. In order to do so Luminis employs certified experts who have a broad and profound knowledge of Microsoft Technology. In addition to the Gold Partner status for DevOps and Application Development, Luminis is also a Cloud Solution Provider (CSP) which enables Luminis to offer Microsoft\u2019s Azure Cloud software directly to its customers.\n\n", "tags": [], "categories": ["Blog"]}
{"post_id": 2104, "title": "Search Maturity Index", "url": "https://www.luminis.eu/blog-en/search-en/search-maturity-index/", "updated_at": "2020-12-01T15:26:21", "body": "I have been helping customers with their search challenges for more than 10 years now. In these years, I recognized patterns in how companies deal with their search engine. Some time ago, we got the idea to create a maturity index around search. We can plot customers on this matrix, giving them an idea of how mature they are and what should be their next steps to improve their search solution. In this blog post, I write about the journey I took and give you first insights into the maturity index.\n\nOver 10 years ago, I was working for a company that did a lot of search consultancy. With search consultancy I mean, we created search solutions for customers using the Solr search engine. Being the Chief Architect for that company, I wanted to learn about search as well. I started learning about Solr, and soon, I got the hang of writing queries and mappings. I also found an excellent way of using Highcharts library together with Solr facets to do analysis and reporting on events. I\u2019ll get back to why this is interesting later on.\nIt was around 2010. I lost many colleagues that were into search; they founded the company Elastic(search). That was the moment I started doing search for real, at least I thought. I got into everything you can do with Elasticsearch. I helped customers start with Elasticsearch, became an Elasticsearch trainer. One of the business cases that are very important to elasticsearch is log analysis. Yes, this was what I did with my Solr/Highcharts solution as well. Ok, with the coming of Kibana they did it a lot better, but the concepts were the same. Use a search engine to analyze (log) events.\nWhen helping customers resolving their Elasticsearch challenges, I got to learn more and more about search in general. I learned about other than technical challenges. I learned about the importance of the visibility of search in the whole company. Especially for companies that earn a big part of there money through search, management should understand and support the search team. People writing the content should understand the importance to write it in such a way that the users can find it. Another important part is that people delivering the search solution must understand how actual users are using their search solution. You have to create analytics and learn to use them.\nLooking back at my learning path, I see many similarities with companies that set out on creating a better search experience. Within a company, it is often a developer or someone from operations that install elasticsearch to help with basic log analysis. With Elasticsearch being present in the company, the step to using it for a next search challenge is small.\nAlternatively, a developer that is appointed with the task to create a search solution often takes one of the more significant open source projects being Elasticsearch or Solr. After some time, the project delivers a search engine. It does not give the right results to the users of the search solution. The developer makes changes, but they do not help in getting better results. That is often the time we get a call for help. Other customers ask us for help before embarking on the search journey.\nAfter helping many customers with their search solutions and reading blogs, articles, and books about search, we started seeing why projects were successful. Successful search solutions are created when people of the different disciplines work together supported by the entire company, including management. We developed a Search Maturity Index to make it visible where companies are, and what the best next steps need to be.\nSearch Maturity Index\nThe maturity index is a matrix of 5 levels of maturity over 5 categories. The best experience is achieved when all categories are in balance. We often see customers with a higher technical level than, for instance, analytics. Without the right analytics in place, you can have a solution that is to complex.\n\nThe five pillars of our maturity index are:\nOrganization \u2013 Someone in the organizations needs to decide upon the KPI\u2019s and needs to take ownership of the search solution.\nContent \u2013 A crucial aspect of a good search experience is good content. With lousy content, you can never create a good search experience.\nAnalytics \u2013 With the right analytics, you can monitor your users and your own goals and KPI\u2019s for the search solution.\nTechnical \u2013 You need a well-optimized search engine to deliver the right results as fast as possible.\nUser Experience \u2013 Your users want a good user experience. They want a fast, intuitive experience with relevant results.\nTo get an idea about what fits in where in the maturity level, below is an example roadmap of features for the 5 different levels.\n\nThe first thing you have to do as a company is to decide about the maturity level you want to reach. Not every company needs to be level 5. Another thing you need to take into account, reaching the next level takes time. Of course, you can learn a lot from others. Still, you have to get your own experience, run into your challenges, and fix your problems. If you already have a search solution, or you are not sure about the level of maturity to reach, we can do a search maturity scan. After the scan, we present the maturity level and of course, our findings that led us to that maturity level. Together with you, we determine the first steps to take and how we can help you. If you want to get an idea about the scan, try our Quick Scan. After just 10 questions, you get a very high level of the maturity of your company.\nQuick Scan (beta, not everything is functional yet)\n", "tags": [], "categories": ["Blog", "Search"]}
{"post_id": 18578, "title": "Raymond ter Riet appointed the director of Luminis Technology in Business", "url": "https://www.luminis.eu/blog-en/raymond-ter-riet-appointed-the-director-of-luminis-technology-in-business/", "updated_at": "2020-02-24T14:31:56", "body": "Amersfoort \u2013 Raymond ter Riet has been appointed the director of Luminis Technology in Business (TIB). The appointment of Raymond ter Riet marks another important step in Luminis\u2019 product development and the further fulfilment of its international growth ambitions. Raymond is the successor of Marcel Offermans, who will focus on Studio 397: Luminis\u2019 gaming studio responsible for rFactor 2: the world\u2019s most realistic racing simulator to date.\nRaymond ter Riet has a background in Computer Science and AI. He has extensive experience as an entrepreneur and has worked in various commercial and management positions for large and small technology companies. He has worked for companies such as Exact Software, Compuware, Be Informed and TMG Media Groep. At Luminis, he is responsible for marketing Cloud technology and data products.\u00a0\nRaymond ter Riet: \u201cI am very enthusiastic about Luminis products and the growing number of clients whose businesses are benefiting from them. Our strength lies in both the innovative concepts that underlie the products, and outstanding quality of our people and software. For that reason, I feel more people and organizations should become acquainted with our products and services. I will be focusing much of my attention on expanding the go-to-market organization to improve our reach and ability to provide service to existing and new customers and users.\u201d\nLuminis products\nThe Luminis product portfolio consists of the InformationGrid, a low-code Cloud platform for data-intensive applications; Summar.io, a data-driven, personalized learning platform and rFactor 2, one of the world\u2019s most realistic racing simulators. Luminis Technologies in Business is an organization with a team of 45 international employees who are based in the recently opened office Apeldoorn, located at Regentesselaan 11, Apeldoorn. Apeldoorn is also home to one of the Luminis consultancy companies, located at John F. Kennedylaan.\nLuminis is a software and technology company with offices in Amsterdam, Apeldoorn, Arnhem, Eindhoven and Rotterdam. This network organization focuses on clients for whom information technology and the Internet have strategic value. That is why Luminis specializes in the application of new technology and develops its own products in the field of Cloud technology and data management. These form the foundation for innovative solutions created for clients in both the realm of business and government.\n\u00a0\n\u00a0\n", "tags": [], "categories": ["Blog"]}
{"post_id": 2080, "title": "Kids, a game and some CSS animations", "url": "https://www.luminis.eu/blog-en/development-en/kids-a-game-and-some-css-animations/", "updated_at": "2020-11-13T09:17:59", "body": "Once in a while I need a project to experiment with some new technology. This time I thought it would be cool to learn some CSS animations and to find out how to communicate with my Philips Hue lights. I also had the idea for a while now to create a game for my kids. Crack the code! Or maybe also known as Mastermind. \nSo in this post I will tell you how this all came together in a now addicted and fun game for my kids.\nThe game \u2013 the rules\nThe idea was to create a game where a random code of three numbers was generated and the kids can guess the code by entering numbers from one to nine.\u00a0 If a number exists in de code and is in the correct position, it will be coloured green. If code contains the number but is in the wrong position, it will be coloured yellow. And if the code does not contain the number it will be coloured red.\u00a0 To make it a bit harder, they have a limited number of attempts.\nPhilips Hue lights\nEach time that a code was entered I wanted to change my lights corresponding with the color of the attempt. So if the code was incorrect, the lights should turn red. If the code was correct, the lights would turn green.\nWhile I was expecting a lot of time to configure my lights, it turned out to be really easy. You need an API key and call some REST endpoints. To change a color for a specific room, all I had to do was the following call:\n\n  this.http.put(`http://${this.yourHueBridgeIpAddress}/api/${this.yourHueApiKey}/groups/${this.room}/action`, {\r\n            on: true,\r\n            hue: this.color\r\n        }).subscribe();\nWhile I was kind of disappointed that this took me only 15 minutes, I created an Ionic 4 app for the first version of the app. After a while I had something working which looked like this:\n\nCorrect or wrong?!\nBecause I wanted to use the Hue lights and make the app a bit more appealing for the kids, I implemented that after each correct of incorrect attempt to crack the code the lights responded.\u00a0 If the code was (in)correct a different image appears to make it more fun. Also the correct code will be displayed in case of the maximum attempts has been reached. The cool thing for me was that all these features were feature requests by my children. Who said that user feedback is not important?! \ud83d\ude42 This resulted in the following screens:\n \nAnimations\nNow that the game is functionality working, it is time for some animations. The animation should not distract too much so I decided to add a simple one. If you have ever placed a Super Mario game, you will recognise the moving bullet.\n\nIn the html I have added the image with a class bullet-container.\n  \r\n\nIn the CSS you see the class .bullet-container which contains the animation. Lets see how the animation is build up:\n\nslide-right \u2013\u00a0the name of the animation\n5s \u2013\u00a0the duration of the animation\nease-out \u2013\u00a0the timing function which specifies the speed curve of an animation\ninfinite \u2013\u00a0the iteration count\nboth \u2013\u00a0the fill mode which\u00a0specifies a style for the element when the animation is not playing (before it starts, after it ends, or both).\n\nThe .bullet-container also contains a delay to make sure the animation is not started directly.\nMoving the dragon in the minecraft level looks really fancy, but that is just a gif image moving from right to left with the same animation properties as the bullet \ud83d\ude09\n\n.bullet-container {\r\n  animation: slide-right 5s ease-out infinite both;\r\n  animation-delay: 5s;\r\n}\r\n\r\n@keyframes slide-right {\r\n  0% {\r\n    transform: translateX(-100);\r\n    margin-left: -200px\r\n  }\r\n  100% {\r\n    transform: translateX(500px);\r\n  }\r\n}\nAgain\u2026 too easy\nA simple animation is not that hard. A single game level is not that much\u2026 So I decided that the game needed more levels and that you need to have a different game mode so you can go to the next level after you have completed one.\nHere is a short screen capture of a few levels with some animations between the levels and after a level is completed.\n\nhttps://www.luminis.eu/wp-content/uploads/2019/06/levels.mp4\nThis is the code for animating the Level text, which comes from the top of the screen and has some bouncing effect. The percentages in the keyframes are points on the animation timeline.\nSo at each point the text will move downwards and at 95% it will move a bit up again to create the bouncing effect.\n.bounce-in-top {\r\n  animation: bounce-in-top 1.1s both;\r\n}\r\n\r\n@keyframes bounce-in-top {\r\n  0% {\r\n    transform: translateY(-500px);\r\n    animation-timing-function: ease-in;\r\n    opacity: 0;\r\n  }\r\n  38% {\r\n    transform: translateY(0);\r\n    animation-timing-function: ease-out;\r\n    opacity: 1;\r\n  }\r\n  55% {\r\n    transform: translateY(-65px);\r\n    animation-timing-function: ease-in;\r\n  }\r\n  72% {\r\n    transform: translateY(0);\r\n    animation-timing-function: ease-out;\r\n  }\r\n  81% {\r\n    transform: translateY(-28px);\r\n    animation-timing-function: ease-in;\r\n  }\r\n  90% {\r\n    transform: translateY(0);\r\n    animation-timing-function: ease-out;\r\n  }\r\n  95% {\r\n    transform: translateY(-8px);\r\n    animation-timing-function: ease-in;\r\n  }\r\n  100% {\r\n    transform: translateY(0);\r\n    animation-timing-function: ease-out;\r\n  }\r\n}\nFun is also important\nIt turns out that this project is much more fun (and more work) than I expected. I learned a bit of CSS animations and can probably improve a lot more on that part, but it\u2019s a good start. Combined with the Hue lights in each of my children\u2019s bedroom the app has become an extra thing (next to reading a bedtime story) to do before going to bed.\nI guess that the feedback of my children is the most fun part of this project. They have a lot of ideas for animations and features. So on the backlog we have collecting stars on level completion, a bonus level, an end boss level and creating some augmented reality portals \ud83d\ude42\nI hope you liked my post and if you have any questions feel free to ask them.\n", "tags": [], "categories": ["Blog", "Development"]}
{"post_id": 1981, "title": "The new Elasticsearch 7: Why I think you should upgrade", "url": "https://www.luminis.eu/blog-en/search-en/the-new-elasticsearch-7-why-i-think-you-should-upgrade/", "updated_at": "2020-11-11T16:52:11", "body": "Elasticsearch 7 has been released for almost two months now and Elastic claims it\u2019s the fastest, safest, most resilient, easiest to use version of Elasticsearch ever. After attending the Elastic Stack 7 Highlight meetup from Elastic at their HQ three weeks ago, I got excited about the improvements in this version and wanted to dive a bit deeper into them. This blog post is about my opinion on why you should upgrade to version 7.\nElasticsearch 7.0 Release Notes \u2013 Elasticsearch 7.1 Release Notes\nSome security features moved to basic license in 7.1\nWhile writing this blog Elastic has released version 7.1. In this version they moved some security features to the basic license. TLS (Transport Layer Security), which encrypts communication between Elasticsearch nodes, and RBAC (Role Based Access Control) to configure users, groups, roles and permissions (at index-level), are now free to use.\nScalability and resiliency\nFirst I\u2019ll start with the scalability and resiliency related improvements in this version for which I think the upgrade is worth it.\nNew cluster coordination system\nThe most important improvement (as said by the Elastic crew) is the new cluster coordination system. In previous versions Elasticsearch relied on Zen Discovery for scaling and resiliency to failures. The minimum_master_nodes setting was an important piece to prevent split brains and losing data. The best practice for this setting is to set it to N/2+1, were N is the amount of master-eligible nodes. Unfortunately this setting is often misconfigured and maintaining this setting across large and dynamically resizing clusters is also difficult. In Elasticsearch 7, they have completely rewritten the system, now called Zen2. The minimum_master_nodes setting is now removed and Elasticsearch now chooses which nodes can form a quorum by itself. With Zen2, master elections will now take milliseconds compared to the old Zen, where it could take several seconds. Growing and shrinking cluster now becomes safer and easier with much less room to misconfigure the system. Elastic has written a nice blog about this new system if you want to read more about it.\nPrimary shard default\nPreviously the default value of primary shards for indices was set to five. This caused performance problems to a lot of users due to oversharding. Because shards are still performant to a size of up to 30-40 GB, the five shard default is an overkill in a lot of use-cases. In version 7 the default is set to one primary shard per index. The shard count can of course still be altered via the index settings.\nLess heap\nIn the newest version some improvements were made for memory usage and protection against out-of-memory errors. The first thing is the new circuit breaker that keeps track of the memory used by the JVM. It uses a functionality in the JVM to measure current memory usage instead of only accounting for the currently tracked memory. A node will now reject requests if they exceed a threshold of (default) 95% of the heap allocated to the process. This will prevent OOM errors. The node will respond with a HTTP 429 error with information about the current memory usage and the amount of memory needed by the request. The .NET, Ruby, Python and Java clients already implement retry policies to deal with this response. Another change made is the default maximum buckets to return as part of an aggregation which was unbounded by default in previous versions. It is now set to 10,000. This will prevent Elasticsearch to calculate a large number of buckets which could cost a lot of memory. Another feature to reduce memory usage is making an index \u201cfrozen\u201d. Frozen indices were already implemented in 6.6, but was also explained in detail at the Elastic Stack 7 Highlights meetup. In short: open indices always keep some data in memory to efficiently search and index to them. If you have indices which you access very rarely, but don\u2019t want to close them, it might be handy to freeze them. This will free up the resources used to keep them open. This data has to be rebuilt each time you want to search these indices, which makes searches on them slower.\nOptimisations for performance\nNext up some important optimisations implemented to improve search performance.\nFaster retrieval of top hits\nA breaking change has been made to the way Elasticsearch calculates the total hits. The default number of top results that are scored is now set to 10.000. What it means is that Elasticsearch will now, by default, only calculates scores for the top 10.000 hits. In case of more than 10.000 hits it says the total hits amount is greater than 10.000. This prevents Elasticsearch from going through all hits to calculate an exact amount, which makes the retrieval of top hits faster. It\u2019s possible to change the default value to a different amount or still calculate an exact amount with the \u201ctrack_total_hits\u201d property in a search request. Read more about it in the documentation.\nEasier relevance tuning with rank feature\nTo make relevance tuning easier, Elastic has implemented the rank_feature and rank_features datatypes and the rank_feature query. The rank_feature datatype holds a numeric value which can be used in a rank_feature query to boost documents. This is useful when you want to, for example, boost documents on popularity or importance. The rank_features field holds numeric feature vectors which works better for a set of weighted tags or categories. These fields can only be used by the rank_feature query and this type of query only works on these fields. The query is typically put in a should clause of a bool query so its score is added to the score of the main query. The benefit of this query compared to the function_score query is that is compatible with the new way of retrieving top hits which makes it faster.\nAdaptive replica selection\nAdaptive replica selection was already introduced in version 6.1 as an experimental feature. With this feature, each node tracks and compares how long search requests to other nodes take, and uses this information to adjust how frequently to send requests to shards on particular nodes. This leads to nodes sending requests to the least busiest nodes and avoid the slowest nodes. In previous Elasticsearch versions this was done in round robin fashion, instead of using such information. Now in version 7.0 this feature is enabled by default.\nOther improvements\nScript Score Query\nDesigned to replace the Function Score Query, the Script Score Query is now introduced. This new type of query allows you to write scripts to compute a new score for each document returned by the query. This is useful when a score function is computationally expensive and you only want to compute the score on a filtered set of documents. Note that this query is still marked as experimental.\nInterval queries\nIn some use-cases you would like to find records in which words or phrases are within a certain distance from each other. The only way to do this was to use span queries. This could be quite difficult, since span queries don\u2019t use the analyzer. Hence the new intervals query. With the intervals query you get more control over the order and proximity of matching terms.\nFeature-complete Java HLRC\nThe high-level REST client for Java is now marked as complete and now covers all APIs. Since I\u2019m using this client a lot myself I\u2019m glad to see this.\n", "tags": [], "categories": ["Blog", "Search"]}
{"post_id": 2035, "title": "Setting up two-way TLS", "url": "https://www.luminis.eu/blog-en/development-en/setting-up-two-way-tls/", "updated_at": "2022-05-10T15:55:45", "body": "This tutorial will walk you through the process of protecting your application with mutual TLS, which enables secured connection (HTTPS) and enables you to choose who is allowed to access your application.\nThis sample project demonstrates a basic setup of a server and a client. The communication between the server and client happens through HTTP, so there is no encryption at all. The goal is to ensure that all communication happens in a secure way.\nThese are the following steps:\n\nStarting the server\nSaying hello to the server (without encryption)\nEnabling HTTPS on the server (one-way TLS)\nRequire the client to identify itself (two way TLS)\nTwo way TLS based on trusting the Certificate Authority\n\nStarting the server\nRequired to have:\n\nAt least Java 8, Recommended: Java 11\nMaven 3.5.0\nEclipse, Intellij IDEA (or any other text editor like VIM)\nA terminal\nOpenssl (Windows users can get it from here: https://git-scm.com/)\nClone the project from: https://github.com/Hakky54/mutual-tls\n\nStart the server by running the main method of the app class in the server project.\nSaying hello to the server (without encryption)\nCurrently, the server is running on the default port of 8080 without encryption. You can call the hello endpoint with the following curl command in the terminal:\ncurl -i -XGET http://localhost:8080/api/hello\nIt should give you the following response:\nHTTP/1.1 200\r\nContent-Type: text/plain;charset=UTF-8\r\nContent-Length: 5\r\nDate: Sun, 11 Nov 2018 14:21:50 GMT\r\n\r\nHello\nYou can also call the server with the provided client in the client directory. The client is an integration test based on Cucumber, and you can start it by running the ClientRunnerIT class. There is a Hello.feature file that describes the steps for the integration test. You can find it in the test resources of the client project.\nThere is another way to run the server and client and that is with the following command: mvn clean install\nEnabling HTTPS on the server (one-way TLS)\nNow, you will learn how to secure your server by enabling TLS. You can do that by adding the required properties to the application properties file named: application.yml\nAdd the following property:\nserver:\r\n  port: 8443\r\n  ssl:\r\n    enabled: true\nYou will probably ask yourself why the port is set to 8443. The port convention for a tomcat server with https is 8443, and for http, it is 8080. So, we could use port 8080 for https connections, but it is a bad practice. See Wikipedia for more information about port conventions.\nRestart the server so that it can apply the changes you made. You will probably get the following exception: IllegalArgumentException: Resource location must not be null.\nYou are getting this message because the server requires a keystore with the certificate of the server to ensure that there is a secure connection with the outside world. The server can provide you more information if you provide the following VM argument: -Djavax.net.debug=SSL,keymanager,trustmanager,ssl:handshake\nTo solve this issue, you are going to create a keystore with a public and private key for the server. The public key will be shared with users so that they can encrypt the communication. The communication between the user and server can be decrypted with the private key of the server. Please never share the private key of the server, because others could intercept the communication and will be able to see the content of the encrypted communication.\nTo create a keystore with a public and private key, execute the following command in your terminal:\nkeytool -genkeypair -keyalg RSA -keysize 2048 -alias hakan -dname \"CN=Hakan,OU=Altindag,O=Luminis,C=NL\" -ext \"SAN:c=DNS:localhost,IP:127.0.0.1\" -validity 3650 -keystore server/src/main/resources/identity.jks -storepass secret -keypass secret -deststoretype pkcs12\nNow, you need to tell your server where the location of the keystore is and provide the passwords. Paste the following in your application.yml file:\nserver:\r\n  port: 8443\r\n  ssl:\r\n    enabled: true\r\n    key-store: classpath:identity.jks\r\n    key-password: secret\r\n    key-store-password: secret\nCongratulations! You enabled a TLS-encrypted connection between the server and the client! Now, you can try to call the server with the following curl command: curl -i --insecure -v -XGET https://localhost:8443/api/hello\nLet\u2019s also run the client in the ClientRunnerIT class.\nYou will see the following error message: java.net.ConnectException: Connection refused (Connection refused). It looks like the client is trying to say hello to the server but the server is not there. The problem is that the client it trying to say hello to the server on port 8080 while it is active on the port 8443. Apply the following changes to the HelloStepDefs class:\nFrom:\nprivate static final String SERVER_URL = \"http://localhost:8080\";\nTo:\nprivate static final String SERVER_URL = \"https://localhost:8443\";\nRequire the client to identify itself (two way TLS)\nThe next step is to require the authentication of the client. This will force the client to identify itself, and in that way, the server can also validate the identity of the client and whether or not it is a trusted one. You can enable this by telling the server that you also want to validate the client with the property client-auth. Put the following properties in the application.yml of the server:\nserver:\r\n  port: 8443\r\n  ssl:\r\n    enabled: true\r\n    key-store: classpath:identity.jks\r\n    key-password: secret\r\n    key-store-password: secret\r\n    client-auth: need\nIf you run the client, it will fail with the following error message: javax.net.ssl.SSLHandshakeException: Received fatal alert: bad_certificate. This indicates that the certificate of the client is not valid because there is no certificate at all. So, let\u2019s create one with the following command:\nkeytool -genkeypair -keyalg RSA -keysize 2048 -alias suleyman -dname \"CN=Suleyman,OU=Altindag,O=Altindag,C=NL\" -ext \"SAN:c=DNS:localhost,IP:127.0.0.1\" -validity 3650 -keystore client/src/test/resources/identity.jks -storepass secret -keypass secret -deststoretype pkcs12\nYou also need to create a truststore. A truststore is a suitcase containing trusted certificates. The client or server will compare the certificate, which it will receive during the SSL Handshake process with the content of its truststore. If there is a match, then the SSL Handshake process will continue. Before creating the truststores, you need to have the certificates of the client and server. You can get it with the following command:\nExport certificate of the client\nkeytool -exportcert -keystore client/src/test/resources/identity.jks -storepass secret -alias suleyman -rfc -file client/src/test/resources/client.cer\nExport certificate of the server\nkeytool -exportcert -keystore server/src/main/resources/identity.jks -storepass secret -alias hakan -rfc -file server/src/main/resources/server.cer\nNow, you can create the truststore for the client and import the certificate of the server with the following command:\nkeytool -keystore client/src/test/resources/truststore.jks -importcert -file server/src/main/resources/server.cer -alias hakan -storepass secret\nThe next step is to do the same for the truststore of the server:\nkeytool -keystore server/src/main/resources/truststore.jks -importcert -file client/src/test/resources/client.cer -alias suleyman -storepass secret\nYou created the two keystores for the client. Unfortunately, the client is not aware of this. Now, you need to tell that it needs to use the keystores with the correct location and password. You also need to tell the client that ssl is enabled. Provide the following property in the application.yml file of the client:\nclient:\r\n  ssl:\r\n    enabled: true\r\n    key-store: identity.jks\r\n    key-password: secret\r\n    key-store-password: secret\r\n    trust-store: truststore.jks\r\n    trust-store-password: secret\nThe server is also not aware of the newly created truststore. Therefore replace the current properties with the following properties:\nserver:\r\n  port: 8443\r\n  ssl:\r\n    enabled: true\r\n    key-store: classpath:identity.jks\r\n    key-password: secret\r\n    key-store-password: secret\r\n    trust-store: classpath:truststore.jks\r\n    trust-store-password: secret\r\n    client-auth: need\nIf you run the client again, you will see that the test passed and that the client received the hello message from the server in a secured way. Congratulations! You finished installing two-way TLS!\nTwo way TLS based on trusting the Certificate Authority\nThere is another way to have mutual authentication and that is based on trusting the Certificate Authority. It has pros and cons.\nPros\n\nClient\u2019s do not need to add the certificate of the server\nServer does not need to add all the certificates of the clients\nMaintenance will be less because only the Certificate Authority\u2019s certificate validity can expire\n\nCons\n\nYou don\u2019t have control anymore for which applications are allowed to call your application. You give permission to any application who has a signed certificate by the Certificate Authority.\n\nThese are the following steps:\n\nCreating a Certificate Authority\nCreating a Certificate Signing Request\nSigning the certificate with the Certificate Signing Request\nReplace unsigned certificate with a signed one\nTrusting the Certificate Authority only\n\n\nCreating a Certificate Authority\nNormally there is already a Certificate Authority and you need to provide your certificate to have it signed. Here you will create your own Certificate Authority and sign the Client and Server certificate with it. To create one you can execute the following command:\nkeytool -genkeypair -keyalg RSA -keysize 2048 -alias root-ca -dname \"CN=Root-CA,OU=Certificate Authority,O=Luminis,C=NL\" -validity 3650 -keystore root-ca/identity.jks -storepass secret -keypass secret -deststoretype pkcs12\nOr you can use the one which is already provided in the repository, see identity.jks in the root-ca directory.\nCreating a Certificate Signing Request\nTo get your certificate signed you need to provide a Certificate Signing Request (.csr) file. This can be created with the following command:\nCertificate Signing Request for the server\nkeytool -certreq -keystore server/src/main/resources/identity.jks -alias hakan -keypass secret -storepass secret -keyalg rsa -file server/src/main/resources/server.csr\nCertificate Signing Request for the client\nkeytool -certreq -keystore client/src/test/resources/identity.jks -alias suleyman -keypass secret -storepass secret -keyalg rsa -file client/src/test/resources/client.csr\n", "tags": [], "categories": ["Blog", "Development", "Security"]}
{"post_id": 18490, "title": "New release of the InformationGrid available", "url": "https://www.luminis.eu/blog-en/new-release-of-the-informationgrid-available/", "updated_at": "2020-02-24T14:31:57", "body": "Apeldoorn \u2013 On Tuesday May 21st a new version of the InformationGrid was released. The InformationGrid is Luminis\u2019 low code platform for data-intensive cloud applications. In this release a number of new features was introduced to further maximize the productivity of analysts and developers.\nThe Encrenaz release, contains the following improvements:\n\u2022 Support for end-to-end development of userinterfaces using DSL-based components\n\u2022 New advanced features in the DSL editors for processing financial data\n\u2022 Improved multitenancy by supporting physical and logical separation of tenants\n\u2022 Improved scale-out support for massive systems\n\u2022 Lots of small fixes and updates\n\nDe Encrenaz release of the InformationGrid has been extensively tested during internal alfa-tests and beta-tests in production environments.\n\nInterested to know more? For more information please contact Raymond ter Riet (raymond.terriet@luminis.eu)\n\n", "tags": [], "categories": ["Blog"]}
{"post_id": 1910, "title": "Attending the Haystack conference (day 2)", "url": "https://www.luminis.eu/no-category/attending-the-haystack-conference-day-2/", "updated_at": "2020-11-24T14:46:06", "body": "In my previous blog I have described the first day of the Haystack conference. In this blog as you might have guessed I am going to tell about the talks I visited in day 2. After a nice dinner, some beers and a nice game of Bocce it was time to go to bed.\nDay one of the conference \u2013 The game of Bocce\nAddressing variance in AB tests: Interleaved evaluation of rankers\nThe second day started off with a very interesting presentation by\u00a0Erik Bernhardson from\u00a0Wikimedia. The goal for AB tests is to find the better version of two ranking algorithms or queries. One way is to deploy two versions or use a switch and send some percentage of users to one version (A) and the other percentage to the second version (B). This does come with some challenges like deployment issues and previous results have shown you need a lot of data to determine the better version. Erik discussed another method called interleaving. If you\u2019d like more detailed information about interleaving please read the following paper:\u00a0Large Scale Validation and Analysis of Interleaved Search Evaluation. Erik describes two forms of interleaving:\n\nBalanced Interleaving -> with the result sets you to choose the first item of list A for the first item than take the first item from the second list if it is not the same. Move on to the next item till you have a complete list. Now clicks are recorded for the original list the item was taken from. This method could have a disproportionate number of clicks for one of the lists due to the way of choosing items.\nTeam draft Interleaving -> Difference with the balanced interleaving is that we choose for each round which list can pick first. A nice blog post that Erik gave us is from Netflix: Interleaving in online experiments at Netflix.\n\nAt Wikimedia they are using the team draft interleaving. They use the _msearch API from Elasticsearch to send both queries at the same time. To collect the clicks the right way, for each hit the originating list is provided by the backend. This is most likely one of the first talks I am going to use to do my own experiments. In short, a really nice talk, if you just have time for one talk, this is the one I would watch.\nSolving for satisfaction: Introduction to click models\nThis talk by Elizabeth Haubert turned out to be a big challenge. Not due to her level of knowledge, not due to her presentation skills, but due to technical difficulties. It turned out that the projector\u00a0did no longer work nicely with laptops using the long cable. Therefore she had to do without a screen in front of her. Still, she managed to deliver a good presentation about using clicks to learn about your users. As I already read a lot of articles about this, I just listened to her and forgot to take notes. Therefore not a lot of notes. You can always wait for her presentation or read her excellent blog post. What is learning to rank \u2013 blog by Elizabeth Haubert Learning to rank search results \u2013 video by Jettro Coenradie and Byron Voorbach\nArchitectural considerations on search relevancy in the context of e-commerce\nInteresting presentation by Johannes Peter from the MediaMarkt / Saturn concern about their journey from a commercial search solution to a new solution based on Elasticsearch. Their complete solution is running on Docker / Kubernetes. It makes heavy use of a project called Apache NiFi to connect all sources with Elasticsearch. What I like about their approach is the way they handle the user query. They take four steps to parse the user query into an optimized Elasticsearch query. In the first step, they take away stop words, do things like stemming and lemmatization. The second step is about finding redirects if the user is searching for a category. After the redirect, they check for campaign rules in the third step. A campaign rule can be in place for Black Friday or Easter. In the final query parsing step they can add contextual information based on chosen categories. When the query is executed they do re-ranking based on clicks or popularity and after that using stock information or other information that is added later on. In this step, they also can change the order of the facets based on the chosen or available categories.\nImproving Search Relevance With Numeric Features in Elasticsearch\nFirst presentation about new features of Elasticsearch 7 I attended. The presentation was given by\u00a0Mayya Sharipova\u00a0from Elasticsearch. She told us all about three new features already available in 7.0 and coming available in 7.1. Rank feature(s) \u2013 This is a mapping type as well as a query type. The goal is to replace some of the ways the function score query was used. But then in an optimized way. It is created to implement popularity fields. The query supports a number of functions that you can use to calculate a boost from the numeric field. More information in this blog: Easier relevance tuning in elasticsearch 7. Distance Feature Query \u2013 A better way to query with distances in mind. Again a more optimized query when working with distances on date fields and geo points. More information in the Elasticsearch documentation. Vector Fields \u2013 Can deal with word embeddings, dense_vectors, and sparse_vectors from within your document store or inverted index. Especially in the field of Learning To Rank, the use of vectors is very interesting. Cannot wait to give it a spin. More information can be found here. If your looking for samples, check this Github repo:\u00a0https://github.com/jtibshirani/text-embeddings\nNatural Language Search with Knowledge Graphs\nFor the past few months, I have been boosting my NLP knowledge. Therefore I was very interested in this talk by Trey Grainger from Lucidworks. Trey is a very energetic presenter, hard to keep up with notes. He started out explaining the difference between ontology and knowledge graph. If I understood well if you would compare an ontology with a class than the knowledge graph would be the object instantiation of the class. I also liked the way he looks at the unstructured\u00a0text, which has a lot of structure. It can have different kind of words, words that strengthen the meaning of other words. It can contain references to other articles or entities. If you can construct a semantic knowledge graph from your text, you can understand your text a lot better. He mentioned some ways of doing this with Solr components. Need more time to dive into this subject.\nSearch with Vectors\nThis was my final presentation of the conference. This one was presented by Simon Hughes from dice.com. He started out with very familiar topics. Using clicks to impact search results. He continued with extracting concepts from strings by comparing string vectors. This feels like embeddings right? He continued with distributional approaches to word meanings with a reference to the following article. A review of the recent history of natural language processing. Suddenly I realized that I lost it somewhere, tough theory for the last talk, at least for me. Still a very nice presentation. Will look at this one again when the video becomes available.\nWhat a great conference\nThat is it, what a great conference it has been. I really liked meeting a lot of people I only knew via the relevance slack channel. And of course, the quality of all the talks was really great. I hope to be here again next year.\n", "tags": [], "categories": ["No category"]}
{"post_id": 1904, "title": "Attending the Haystack conference", "url": "https://www.luminis.eu/no-category/attending-the-haystack-conference/", "updated_at": "2020-11-24T14:44:46", "body": "The past two days I attended the Haystack Conference in Charlottesville. Two days with a lot of talks about search relevance. Always nice to spend some days with like-minded people. Besides the talks, there was plenty of room to network. As a committer to the Learning to Rank plugin, it was nice to finally meet some of the other committers to the project. In this blog post, I am giving a summary of the sessions I attended.\nThe Haystack conference is all about relevant search. The conference is organized by Opensource Connections. There were around 150 people that wanted to learn and talk about search relevance. All sessions are recorded and will be made available online. So if I write about a session that is of interest to you, check the haystack website for the talk.\nKeynote by Max Irwin\nAs with every conference, it all starts with a keynote. Max Irwin is a managing consultant at open source connections. What I took away from his talk has to do with judgments list and experts. He talked about differences in judgments between different experts. Judgments lists are lists where the most relevant documents are appointed to specific queries by experts. Differences\u00a0in these expert judgments do not always mean that one of them is right and the other is wrong, it could mean a difference in context. When having multiple judgments you can calculate a disagreement factor. This factor was calculated using an Elo rating. Using this rating you can find the best experts for your specific situation. You might give more weight to their opinion when creating the judgment list. Having a good judgment list enables you to start tuning your search relevance.\n\u00a0\nOntology and Oncology: NLP for Precision Medicine\nThis talk was presented by\u00a0Sean Mullane from the University of Virginia. An interesting talk about using Natural Language Processing techniques to search through all the available papers about cancer treatments to find the right treatment for a specific patient. A lot of research is done for cancer treatment. A lot of different forms of cancer are available and treatment of one form is completely different from the other. With a lot of information available Sean did research to help find the best information to create the best plan for treating a specific form of cancer.\nIn his research, Sean worked on mapping terms and phrases to specific concepts. With these concepts available during indexing, it is easier to find them using different names or identifiers of the same concept. He showed a form of query expansion using graphs. Using the graphs he can find relatedness of one concept to the other. It was interesting to see how he found the most important relation by not looking at relations that are related to a lot of other concepts but find those relations that have an almost unique\u00a0relation with the concept you are searching for. Using this technique each path in the graph gets a relatedness factor. Using this factor the query is expanded.\nSome references:\n\nhttps://github.com/seanmullane/TREC_2018_UVA\nhttps://www.researchgate.net/publication/300337713_Path-Based_Semantic_Relatedness_on_Linked_Data_and_Its_Use_to_Word_and_Entity_Disambiguation\n\n\u00a0\nAutocomplete as Relevancy\nThis talk was presented by three different speakers from Lexis Nexis: Rimple Shah, Revant Malay and David Rhodes. The intention of their project was to help the user write better queries, so improvement before the query gets to the search engine. The way to do it is through optimizing the auto suggestions while typing. They started out with the suggestions as provided by Solr. In the end, they created their own custom endpoint. More flexibility in sorting the suggestions and adding more properties for scoring and matching. I liked this approach as it is sort of the same I use for one of my customers. Some of the take aways from this talk:\n\nWith longer queries, the focus of the suggestions is only the last few words.\nTo make a boost for the starting word using edismax queries, they added a place holder Term at the beginning of the sentence and query. That way a match with one word is already a match with two words. Interesting way to use pf2/ps2 with only one term.\n\nAfter the talk, I had a good chat with the speakers about the topic. If you by any chance read this post, thanks for that.\nQuery relaxation \u2013 a rewriting technique between search and recommendations\nFor me, Rene Kriegler presented one of the most interesting talks of the conference. The goal for the talk was to present results to the user when the actual query did not return any results. For some of my own\u00a0customers, the first step is to switch from the AND operator to the OR operator. We have even implemented a way to show the missing terms in case of an OR query. Rene took another approach. He used techniques to determine the best word to skip to give the most relevant results. He presented multiple approaches and made it really interesting by presenting a technique to determine which approach returned the best results.\nHe used word vectors as the input for a neural network to train with the goal to determine the best word to remove from the query. When working with word2vec input vectors you can also add other features. He showed some examples with the length of terms, and frequency of terms.\nAn interesting presentation that deserves more research.\nEvolution of Yelp search to a generalized ranking platform\nUmesh Dungat\u00a0is working for Yelp. If you do not know Yelp, they provide a way to search for local businesses. Umesh talked about the way they moved from a custom Lucene based solution to an Elasticsearch based solution. At Yelp they have multiple custom plugins to support their business. They are also a big user of the Learning To Rank plugin. The\u00a0most important reason for them to start working with the LTR plugin was the option to dynamically change the model, without real downtime. Check the item on their technical blog: Moving Yelp\u2019s Core Business Search to Elasticsearch\u00a0and\u00a0Fast Order Search Using Yelp\u2019s Data Pipeline and Elasticsearch.\nLightning Talks\nThe first day ended with lightning talks. As they are very short by nature, the intention is to trigger viewers to start some research or experiment. Some things on my list to have a look at the coming period.\n\nQuaerite \u2013 Search relevance evaluation toolkit\nSmui \u2013 Search Management UI\nQuerqy \u2013 Is a framework for query preprocessing in Java-based search engines\nQuepid \u2013\u00a0Makes improving your app\u2019s search results a repeatable, reliable engineering process that the whole team can understand\n\n\u00a0\nThat was the end of day 1. After the presentations, there was beer and food and games outside. A well spend evening with some nice chats, mostly about search :-).\nContinue reading about the second day\n", "tags": [], "categories": ["No category"]}
{"post_id": 24127, "title": "Introduction to Cloud Native Computing  [Luminis DevCon 2019]", "url": "https://www.luminis.eu/blog-en/cloud-en/introduction-to-cloud-native-computing/", "updated_at": "2020-11-25T11:04:58", "body": "As we gained experience with PaaS, VMs, containers, DevOps, continuous integration and microservices architectures, a new IT environment arose from this melting pot. By building greenfield applications with the intent of using agile application development and architecting specifically for the cloud, a new model evolved: Cloud Native Computing.\nThe overall objective of this new approach is to improve speed, scalability and ultimately profit margin for the enterprise. This is not just for internet-scale giants like Twitter, Netflix or Facebook, but also for typical workloads of most enterprises.  But how does that work? And what is the impact of such architecture on how we design and develop software? This session will explain the concepts of Cloud Native Architecture and the impact on traditional system design.\n\n", "tags": [], "categories": ["Blog", "Cloud"]}
{"post_id": 24120, "title": "Will Neural Networks kill the Inverted Index?", "url": "https://www.luminis.eu/blog-en/search-en/will-neural-networks-kill-the-inverted-index/", "updated_at": "2020-11-24T21:23:47", "body": "Will Neural Networks kill the Inverted Index? With search engines like Solr and Elasticsearch becoming mainstream, limitations become apparent. In this talk, you\u2019ll learn about well-known limitations for the inverted index. Limitations like language detection, searching images, finding answers instead of documents.\nWhat fun is there in discussing limitations if you\u2019re not discussing improvements. For each limitation, you\u2019ll learn about a possible solution based on Neural Networks. Meet Neural search. In the end, we\u2019ll see if Neural Search and Inverted indexes are at war, who\u2019s winning, or if they can live together.\n\n", "tags": [], "categories": ["Blog", "Search"]}
{"post_id": 18450, "title": "Luminis organizes the fifth edition of DevCon", "url": "https://www.luminis.eu/blog-en/luminis-organizes-the-fifth-edition-of-devcon/", "updated_at": "2020-02-24T14:31:58", "body": "Utrecht \u2013 On Thursday 11 April 2019, Luminis organized the fifth edition of its annual developer community event, DevCon. The conference not only focused on developments in the field of Cloud computing, Artificial Intelligence and the latest trends in software technology, but also delved into the ethical and cultural considerations involved in the Internet and the application of technology. \nThe conference was opened by Hans Bossenbroek (CEO of Luminis) reflecting on the developments in the past 5 years in the realm of social coding, the applicability of technology and professionalism. Marleen Stikker (director of Waag Society) followed up by delving into the ethics of Artificial Intelligence and big data and used examples to explain how open strategies impact municipalities and business models.\nThis year, Luminis organized its fifth edition of DevCon in the Path\u00e9 Leidsche Rijn in Utrecht. The extensive anniversary program, provided by Luminis employees for the 500 attending software developers, consisted of a wide range of subjects. With topics including Voice User Interfaces, the human brain, Cloud Native computing and game development, the event offered compelling insights for everyone.\nBesides serious technical subject matter, the attendees were invited to think outside the box and laugh with the well-known stand-up comedian and mathematician Matt Parks, who entertained DevCon\u2019s audience with humorous examples of maths mistakes. The recently graduated Sam van Geijn\u2019s hobby project, The Dutch Rap Generator, was highly rated in the conference App.\nLuminis organizes the DevCon with no sponsors, no targeted recruitment and no direct commercial objectives. By following these leading principles, Luminis is able to organize a conference \u201cfor developers by developers\u201d; with knowledge sharing and human interaction at its core. All Luminis DevCon presentations were filmed and will be published online.\n\n\n\n\n\n\n\n\n\n", "tags": [], "categories": ["Blog"]}
{"post_id": 18425, "title": "Roparun and Luminis announce partnership", "url": "https://www.luminis.eu/blog-en/roparun-and-luminis-announce-partnership/", "updated_at": "2020-02-24T14:32:00", "body": "Rotterdam \u2013 On March 1st, 2019, Roparun and Luminis announced that they are starting a partnership with the intent to transform Roparun\u2019s IT. This partnership aims to help Roparun to benefit from the advantages of Cloud computing and data intelligence. The long term objective is to optimize the funding efficiency of the organization\u2019s charitable work.\nPhoto: Wiljan Vloet and Alex Hoeksma\nRoparun, ready for the future\nSince 1992, Roparun has been committed to supportive care for people with cancer. The annual Roparun event, that takes place during Whitsun weekend, is thus aptly named \u2018the run for life\u2019. Year after year, thousands of participants are involved with Roparun, which is made possible by the efforts of hundreds of volunteers and the support of the organization\u2019s staff. The event has grown considerably over the past years. The first steps are now being taken to ensure improved support for employees, volunteers and participants in the future.\nStrategic partnership\nRoparun has entered into a partnership with Luminis to implement these plans. Roparun director Wiljan Vloet: \u201cWe have found a strategic partner in Luminis and we foresee a long-term partnership. It is good to know that they believe in Roparun\u2019s objective and will be actively contributing to finding the best solutions.\u201d The first system to be developed will form the starting point within Roparun and will be extended further in the future. Luminis will be contributing part of its efforts and services itself. Luminis director Alex Hoeksma: \u201cRoparun is an outstanding Rotterdam-based initiative, and we are proud to be able to support them. Luminis aims to have an impact with software, and we are keen to help raise more money for palliative care.\u201d\nThe run for life\nRoparun is a relay race starting from both Paris and Hamburg to Rotterdam, where participants in teams raise money for the palliative care for people with cancer. The teams participating in 2018 raised over 5 million euros. Roparun offers financial support to dozens of projects annually. The 28th edition of Roparun will take place from 8 to 10 June 2019.\nInnovative entrepreneurship with heart\nLuminis is focused on developing innovative solutions for clients for whom IT and the Internet have strategic value. The company\u2019s 200 employees work in small business units throughout the Netherlands and the UK. That way, Luminis stays close to its customers. With the combination of consultancy services and (largely Cloud-based) products, Luminis offers solutions that make optimal use of the Cloud, the Internet of Things and big data. Where possible, Luminis applies Open Source and Open Standards.\n\n", "tags": [], "categories": ["Blog"]}
{"post_id": 2456, "title": "Filter tools at the National Government", "url": "https://www.luminis.eu/blog-en/development-en/filter-tools-at-the-national-government/", "updated_at": "2020-11-11T16:06:38", "body": "Since spring 2017, three colleagues of Luminis Amsterdam are working in a Scrum Team for the National Governement. These are Jettro Coenradie, Jeroen Reijn and me (Sander Meinema). There are just as many self-employed people on the team too, namely Tom Jonkman, Roos Labberton and Farley Samson. The Product Owner Gerrit Berkouwer and his substitute Harold Steenwinkel are of the Ministry of General Affairs. In July 2017 our team got the opportunity to build the very first Content Tool: Filter Tools.\n\nCitizens can filter information with this system, so they only get relevant information tailored for their personal situation. There was a strict deadline though because the current system was going to be switched off. Before the end of the year, the solution needed to be in production and in use. This meant for us that we had to develop the entire solution within four months. We accepted this challenge and together we pulled it off!\nThe Filter Tools system consists of roughly four components. First of all, there is a Content Management System (CMS) in which editors can write information in so-called content blocks. In addition, they can also formulate questions with answer options. Then they can link the answer options to content blocks. This allows citizens to fill out a questionnaire and in that way only receive the relevant content for her or his personal situation. In the first instance, this system is used to provide citizens with the necessary information for life events. For example, what do you have to do when you start living on yourself or if you\u2019re getting married?\n\nThe second component is an API. If the editor has finished entering all the content, questions and links of a filtering tool, she or he can publish it. This makes the data from the filter tool publicly available through an open REST API. Through the API the data can be queried with \u201ccurl\u201d, Postman or any another HTTP client. The data is returned as JSON.\nThe third component is a client written in vanilla JavaScript with some basic styling. A website of a ministry or a public agency can easily access the content and questionnaire through this client. It only needs to embed the client in its own website. There are various configuration options for customizing and the styling can be extended or overwritten so that it fits into the look of the rest of the website.\n\nFor developers that would like to use the API or the JavaScript client on their website we also created a documentation website. We described the technical use of the API and how to implement the client on your own website. This is the fourth component of the product. The documentation website also contains a demo of the JavaScript client with which the published filter tools can be consulted. Take a look at\u00a0https://www.contenttoolsrijksoverheid.nl/\nBecause we had to build the entire system from the ground up in a limited amount of time, we opted for common frameworks with which we could quickly take big steps. \u2018Common\u2019 such that the components remain maintainable by ops. \u2018Quickly big steps\u2019 because some frameworks take a lot of work off our hands. For example, we have a lot of experience with the Spring Framework, enabling us to quickly put modules from it to good use. Furthermore, for the CMS we used React in combination with redux-form, react-bootstrap, and flexbox, which offered us the opportunity to quickly create something beautiful.\nThe API is a Spring Boot application which exposes the data via Spring Data REST. The data is stored in a MongoDB. We used Spring REST Docs to document the API. For this, we only needed to write JUnit tests. Thus, we have at the same time unit test coverage and automatically generated technical documentation. We only needed to place these on our website.\n\nTo simplify the integration of the filter tool data on for instance a website, we\u2019ve also build a JavaScript client which connects to the API. A consumer of the data from the API can also develop its own client, for example, a chatbot, in another language than JavaScript, or with different functionality. Our challenge was to make it as widely applicable as possible. Our aim was to support all commonly used (versions of) browsers as possible. That\u2019s why we chose to write the client in vanilla JavaScript and don\u2019t use any dependencies on libraries. We actually only built the functionality and delivered a basic styling. We used many \u2018classes\u2019, so that the appearance and a bit of the interaction can be adjusted as desired. We used a Model-View-Controller pattern for this. If the model is modified, the view will be rendered again. We have taken this idea from React. We came up with this because we applied React in the CMS.\n\nA lot of data is entered into the CMS. When changes are made, you want those to be immediately visible. For example, if links are made or if the order changes this must be made clear. We did this by storing the data in a Redux store. I have written a blog about this before. The front-end consists of React components that render the data from it. The backend also consists of a Spring Boot application. We also used Spring Security to block various attack vectors. We have thought carefully about blocking code injection, XSS, CSRF, and Man-in-the-Middle.\nYou can see the result in action on\u00a0the website of the national government. That\u2019s how we did it. Do you have more questions, you know how to reach us. \ud83d\ude09\n", "tags": [], "categories": ["Blog", "Development"]}
{"post_id": 18417, "title": "Bert Ertman appointed VP Technology at Luminis", "url": "https://www.luminis.eu/blog-en/bert-ertman-appointed-vp-technology-at-luminis/", "updated_at": "2020-02-24T14:32:00", "body": "Amersfoort \u2013 Software technology company Luminis wants to strengthen its leading position in the field of Cloud computing by appointing Bert Ertman VP Technology. Bert is actively involved in Luminis\u2019 Cloud strategy and will be responsible for the partnerships with AWS and Microsoft: our strategic Cloud partners.\n\nBy combining products and services, Luminis can optimally support its customers with the implementation of their Cloud strategy. Bert is not only a sought-after speaker at international conferences such as QCon, Oracle Code and Devoxx, but also a driving force in Luminis\u2019 own Cloud strategy. Bert has worked with Luminis since 2011 and has over 20 years of experience within the software industry. In his role as consultant and advisor, Bert has worked in various market segments including e-commerce, FinTech and industry.\n\n\u201cCloud computing is a field that is developing at the speed of light and has a major impact on organizations. In my role as VP Technology, I can put my broad knowledge and experience to use in order to make Luminis the best technology partner for our clients.\u201d \u2013 Bert Ertman.\n\nInternet and software technology are no longer topics restricted to a specific department of a company. They are a relevant part of the essential digital strategy that drives companies. A competitive digital strategy, that acknowledges the role of Cloud technology, provides companies with the resilience and ability required to respond to rapidly changing markets and client needs. Combining a suitable digital strategy with Cloud computing presents many organizations with complex challenges. The key to success increasingly lies in the ability to build the right bridge between business and clients on the one side and Cloud computing on the other. Cloud computing minimizes the time-to-market and can increase the agility of companies.\nAbout Luminis\nLuminis is a networked organization that focuses on clients for whom information technology and Internet have strategic value. That is why Luminis specializes in the application of new technology and the development of new products in the field of Cloud technology and data management. They constitute the foundation for the innovative solutions we deliver to our clients in both business as well as government.\n", "tags": [], "categories": ["Blog"]}
{"post_id": 19665, "title": "Modern Development with Java", "url": "https://www.luminis.eu/no-category/modern-development-with-java/", "updated_at": "2020-02-24T14:32:01", "body": "A three day training course bringing students up to date with the latest in Java 8 and beyond. This course covers the key features around functional programming with Java 8 over the first two days. You then build upon this theory by refactoring a project using different modern concurrency techniques including promises using Java 8\u2019s CompletableFuture, actors using Akka and reactive streams using RxJava. You\u2019ll learn the good, the bad and the ugly between these approaches in terms of compositionality, testability and simplicity. Hands on exercises will also be provided in order to help students fully solidify their knowledge.\n", "tags": [], "categories": ["No category"]}
{"post_id": 1846, "title": "Cloudy design opportunities", "url": "https://www.luminis.eu/blog-en/cloud-en/cloudy-design-opportunities/", "updated_at": "2020-11-24T15:08:54", "body": "These days cloud technology is rapidly growing and is being adopted and implemented. It requires us to rethink the design and development process. For example with design systems. More on this later on. Especially for designers like me there seems to be less room and time available in cloud related projects. But is this really a problem, or an opportunity? In this article I\u2019d like to point out some tips that might help you. \nTake a point of view and guide clients\nAs most of us know, cloud technology is focused on important business goals, such as reducing costs and improving the scalability. It demolishes classic services (e.g. setting up on premise environments). To keep ahead of innovative products and services of other companies it is our task to advise the client. To take a point of view. Especially with design questions.\nAs said, cloud technology can reduce costs. In the end we might even kill some of our own \u2018old\u2019 services, but it\u2019s at the same time an enabler for (design) experiments. It\u2019s up to us to pro-actively advise the client to invest in what truly matters: the user experience of the product or service. In the end development exists because it serves the purpose of solving a user\u2019s problem, which in return brings the company profit.\n\n\u00a0\nEmbrace the process\nLet\u2019s admit it: we don\u2019t like to fail. We have an aversion to it. When I design a solution for a user problem I try to eliminate as many project risks as possible. For example, create multiple visual designs and discuss this with other designers within Luminis. Of course, sprints and other methodologies keep us moving on in a project, but I really think that cloud related projects will boost the speed even more. Therefore, it\u2019s safe to say that we need to prepare ourselves for more mistakes. It asks designers to set our perfectionist frame of mind aside and embrace the process even more. Fail faster!?\nMindset: tools are just tools\nWithin the world of User Experience you often hear the term \u2018unicorn\u2019. The skills you need to master to cover every aspect of User Experience Design are insane. The landscape is still expanding every day. Cloud technology adds another skill to the immense skillset. Will there soon be UX designers specialized in the cloud territory? Maybe\u2026 In my opinion every designer should at least have a decent grasp of this technology. But, as with everything you design, it\u2019s more about the mindset than about the tools. Tools are just tools. Cloud technology is no exception. To make a metaphor: a pile of the most expensive bricks (the tools) in the world isn\u2019t a house. It is the architect that creates the experience, aesthetic and functional design. To reflect it back to an actual project: the architect is the designer and the engineer combined. It\u2019s a co-creation.\nNothing is impossible\nCloud technology opens doors to possibilities that were just not there, or very hard to reach, a few years ago. To name two: better detection possibilities or improved prediction capabilities. Spotify is a good example. They already know which music you like, before you know you would like it. As a designer I adopted the \u2018everything is possible\u2019 mind set. Especially in the beginning of a project it\u2019s important to not limit ideas. Challenge yourself, developers and stakeholders to think the impossible possible. Cloud technology motivates people to experiment with it and the solution might be somewhere in the ever growing exploding library of cloud services.\n\nDesign systems\nWith the increased speed that the cloud brings we can\u2019t afford to design everything from scratch every single time. Therefore systems, patterns and guidelines are getting more important than ever before. The system forms the base with all kinds of components like buttons and modals. Design systems that are spread throughout the organization can help everyone in the company to build quicker, in a more consistent way, and with better integrated philosophy. It\u2019s up to us, designers, to promote this and show the long-term benefits.\nEpilogue\nTechnology will always keep pushing us to respond to it. It\u2019s up to us to take a point of view. First think of the solution, then what technology might help you in solving the problem. Whether if it\u2019s cloudy or not.\n", "tags": [], "categories": ["Blog", "Cloud"]}
{"post_id": 1794, "title": "Cloud native: this ain\u2019t your father\u2019s cloud", "url": "https://www.luminis.eu/blog-en/cloud-en/cloud-native-this-aint-your-fathers-cloud/", "updated_at": "2020-11-24T20:13:41", "body": "The adage \u201cthe cloud is just someone else\u2019s computer\u201d is true: that someone else is just insanely good at actually running that computer. With the advent of high-value services, run in a way that\u2019s beyond what any company can manage on its down, this image is rapidly changing.\nIn the end, everything runs on a computer that has storage and networking. On top of this, however, we see several types of high-value services being built.\n\nWe start out with those services that your IT department already considers as separate: storage, databases, load balancing, API gateways, the like. These are the things that show up in your network or infrastructure diagram. Then we have application-level services, such as machine learning, queueing services, identity management, or stream processing. These things usually show up in software architecture diagrams, and are typically embedded in some software stack you may have adopted. Using these services, the cloud providers have built offerings for business horizontals: mobile application support, internet of things, gaming backends. These serve as blueprints for well-thought-out solutions, working as building blocks that benefit from scale and experience. In traditional development, these would be represented by architecture patterns for typical solutions, or make-or-buy decisions for horizontal-specific components. Perpendicular to these, we have maturity services for monitoring and maintaining your applications.\n\nDo what you do best\u2026 and gain a few skills\nEach of these services is, in its own right, something that requires significant investment and skill to operate at the price point and quality that they\u2019re offered. Also, they take some element of application construction, neatly define its boundaries (way neater than your own in-house architecture team would ever have the chance to), and build and run a service better, cheaper, and more reliable than you ever could\u2013that is, if you\u2019re willing to play by the rules. First rule is on functionality: you may have to re-architect your application to work with one of these offerings, and there may be key parts of functionality that are simply missing. Also, by picking these high value services, you are likely no longer \u201ccloud agnostic\u201d: the architecture of your solution will be highly influenced by the available services. Thirdly, consider separation of concerns: the cloud provider provides incredibly powerful building blocks, and it\u2019s your job to compose them into something that\u2019s valuable for your customer\u2013if you\u2019re just reselling the cloud provider\u2019s service, there\u2019s no use for you to be in the value chain. It\u2019s now up to you to determine whether the opportunity cost of cloud lock-in outweighs the additional value provided by going all in with one provider.\nRelated post:cloudwhitepaperWhite paper Cloud migrationCloud migration: 5 effective steps towards a successful result. Download our free white paper. Do you want to migrate your organisation\u2019s systems to the cloud? A cloud migration provides speed and efficiency, among many other advantages. This white paper is...\nCloud native ain\u2019t your father\u2019s cloud\nThis new cloud status quo is no longer a good match for the existing IT skills that come from traditional deployment. As IT departments, we need to start thinking in heterogeneous vertical building blocks in stead of horizontal, well-known services, and get used to more complicated price models. On the development side, we\u2019ll need to suppress the urge to start building software right away, and update our architecture process find a good balance between high value, off-the-shelf services and custom development. Finally, as consultancies, we must develop a stronger sense of how to compose solutions using these tools, and take a more central role in the solution design process, not only in the development or operations of solutions. Cloud native ain\u2019t your father\u2019s cloud. Act accordingly.\n", "tags": [], "categories": ["Blog", "Cloud"]}
{"post_id": 18207, "title": "New release of the InformationGrid available", "url": "https://www.luminis.eu/blog-en/informationgrid-dochamps-release/", "updated_at": "2019-02-22T14:56:39", "body": "Apeldoorn \u2013 On Monday February 4th a new version of the InformationGrid was released.\u00a0The InformationGrid\u00a0is Luminis\u2019 low code aPaaS for the development of data-intensive cloud-native applications. In this release a number of new\u00a0features were introduced to further\u00a0maximize the productivity of analysts and developers.\u00a0\nThe new release (named Dochamps.1 ), contains the following improvements:\n\u2022 Integration with Microsoft PowerBI\u00a0to\u00a0support ad hoc data analysis\n\u2022 Uses the latest Amdatu Blueprint\u00a0to feature improved security and infrastructure\n\u2022 More advanced features in the DSL editors\n\u2022 Support for pipe-and-filter architectures for advanced\u00a0event processing\n\u2022 Lots of small fixes and updates\nDe Dochamps.1 release of the InformationGrid has been extensively tested during\u00a0internal alfa-tests and beta-tests in production environments.\nInterested to know more? For more information please contact Raymond ter Riet (raymond.terriet@luminis.eu)\n\n", "tags": [], "categories": ["Blog"]}
{"post_id": 1662, "title": "Java 11: HttpClient API", "url": "https://www.luminis.eu/blog-en/development-en/java-11-httpclient-api/", "updated_at": "2020-11-13T09:03:59", "body": "For years, performing HTTP calls from Java meant you would use an external dependency. That has changed with Java 11, where an all-new HttpClient API is introduced to the platform.\nWhen doing HTTP calls from Java code, you typically use an HTTP client library. Examples are the Apache HttpClient from the HttpComponents project, Square\u2019s OkHttp or even more high-level libraries like the JAX-RS Client API. Even though these libraries are mostly fine, it does mean taking yet another dependency in your application. Can\u2019t Java offer a basic functionality like making an HTTP request with its standard library?\nIt turns out you can do so using the HttpURLConnection class, which is right there in the java.net package. However, this API (which was introduced in JDK 1.1 around 1997) is no joy to use, to say the least. Many have lamented its deficiencies. Remember, the API was created before generics, enums, lambdas, and other modern features were in the Java language. I think you can see the problem.\nFortunately a new HttpClient API has been introduced in Java 11 (released September 2018). It\u2019s much more modern, and also supports HTTP/2 and WebSocket communication. So what does it look like? Here\u2019s the simplest possible scenario, of performing a simple GET request:\n\nHttpClient client = HttpClient.newHttpClient();\r\n\r\nHttpRequest request = \r\n  HttpRequest.newBuilder(URI.create(\"https://luminis.eu\"))\r\n             .build();\r\n           \r\nHttpResponse response = \r\n  client.send(request, HttpResponse.BodyHandlers.ofString());\nAfter creating an HttpClient instance, we create an HttpRequest (using the builder API) and pass it to the the client\u2019s send method. Couldn\u2019t be simpler! In addition to the request, we must also tell the send method how to interpret the response body sent by the server. For this, several pre-defined BodyHandlers are available. Here we simple turn the server\u2019s response into a String.\nThe above example shows an HttpClient and request configured with their defaults. On both the client and the request builder APIs there are many possible configuration options, for example for timeouts and HTTP redirect policies. Once a client or request is built, it is immutable. These objects are thread-safe and can be safely shared throughout an application.\nIn this example, a synchronous (and blocking) call is performed to the HTTP server. Once the response has been completed, it\u2019s turned into a String and wrapped in an HttpResponse object. On this response we can check things like the HTTP status code, and of course get retrieve the body with the body() method.\nYou can also perform asynchronous, non-blocking calls with the new HttpClient API:\nHttpClient client = HttpClient.newHttpClient();\r\n\r\nHttpRequest request = \r\n  HttpRequest.newBuilder(URI.create(\"https://luminis.eu\"))\r\n             .build();\r\n           \r\nCompletableFuture> response = \r\n  client.sendAsync(request, HttpResponse.BodyHandlers.ofString());\r\n\r\nresponse.thenApply(r -> System.out.println(r.body()));\nThe sendAsync method returns a CompletableFuture immediately. This CompletableFuture represents the response that will be returned at a later time by the server. The rest of the program keeps running, and the HTTP request is performed on a separate thread. Once the server returns a response, the CompletableFuture is completed. You typically use the result through methods like thenApply and other combinators that let you asynchronously work with the result.\nHttpClient is not just an updated version of the old HttpURLConnection API. It also supports newer standards, like HTTP/2 and WebSockets. It\u2019s a much need upgrade to the HTTP functionalities that ship with the JDK. Although this post only scratches the surface, it should give you enough to get started and try it out yourself!\n", "tags": [], "categories": ["Blog", "Development"]}
{"post_id": 1666, "title": "Bookaroo note #1: Designing a Voice User Interface", "url": "https://www.luminis.eu/blog-en/concepting-ux-en/bookaroo-note-1-designing-a-voice-user-interface/", "updated_at": "2020-11-24T15:06:57", "body": "I\u2019m a designer of digital products. The past 20 years my work was primarily focussed on GUI \u2013 Graphical User Interfaces. Until now. With the rise of voice-controlled digital assistents from Amazon, Google and the others, a new way of communicating with digital products has become available: the VUI \u2013 the Voice User Interface.\nWe all know how to create conversations\nAnd now I\u2019m confused. I still like to think that designers like me should be the ones that decide how a VUI should behave; what it should say, and when. But are we even equipped for designing for voice? As it doesn\u2019t involve any visual elements, it seems to boil down to designing pure conversations. And why would you need a designer for that? We all know how to create conversations, everybody has them all the time. So, what does it take to design a good VUI? Looking for anwers, I first stumbled on some dear memories from my own youth. \nThe internet\u2019s predecessor\nMany years ago, yet not so very long ago, people used telephones solely for talking to each other over distance. No apps, no displays, just that single simple purpose. You dialed a number and got connected to the person on the other side. And so you could reach virtually any person, anytime, from anywhere. To chat, share experiences, to place an order or to get information; The telephone was in many ways the internet\u2019s predecessor. My grandmother worked at 008, which was not, as you might guess from the name, in any way related to the British Secret Intelligence Service. No, 008 once was the number you dialed in the Netherlands when you needed \u201cinformation\u201d. By information the Dutch telecom provider basically meant telephone numbers; you called this service when you needed to contact someone and didn\u2019t have the number or address. There were other services too: you dialed 002 for the time, 003 for the weather forecast and so you dialed 008 to get someones phone number. These phone services were one of the very first to introduce Voice User Interfaces, or \u2018talking computers\u2019 as people usually called them those days. Before that, 008 employed human operators for the task of looking up the numbers. My grandmother was one of them. In her first years as an operator she needed to look up the requested numbers in phone books; the frequently requested numbers she knew by heart. In 1979 computers were introduced at 008 and from that time on the human operator used computers to retrieve the requested information.\n\n\nA human operator as a proxy for the computer system. My grandmother used to tell me stories about people, mostly men, calling for other purposes than getting a telephone number. I remember one story about a man who had called and asked her for the time. My grandmother kindly told the man he had probably dialed the wrong number, that 002 was the number to get the time. The man had replied he knew what time it was, but that he wanted to ask at what time my grandmother would be off from work, and if she would care to join him for diner. My grandmother proudly told us that these male callers always kept inquiring about her age, because over the phone she apparently sounded like a twenty year old girl. While in fact she was in her late fifties, she had always kept them in the dark and played hard to get, just for the fun of sharing her adventures with her colleagues over lunch. And, years later, with her own grandchildren. The 008 ladies finally got replaced by voice-controlled computers in the late nineties. Computers that we could call and talk to, that got us information faster and cheaper than my grandmother could. As long as the caller kept to the script and didn\u2019t ask for anything out of the ordinary, it was great technology. But I\u2019ll tell you these computers were a lousy flirt compared to my grandmother.\nDisfunctional relationships\nIt\u2019s been about 20 years since computers took over from the 008 ladies and you would think voice recognition technology would have improved dramatically by now\u2026\u00a0Let\u2019s look at this typical conversation between me and my car:  Does any of this sound familiar to you?\u00a0Or maybe I should just get a new car.\u00a0But no, I\u2019ve got a similar disfunctional relationship with Siri, Apple\u2019s smart voice assistant. But I don\u2019t blame Siri. It is still a kid. I don\u2019t blame her just the way I don\u2019t blame my own kids if they misinterpret my words or behave in a way that is not quite socially accepted. Siri, just like my kids, needs to learn by making mistakes. Human conversation appears to be so simple, but if you take a moment to think about it you\u2019ll see that it is absurdly complex.\nSiri is still a kid\nI do blame my car by the way. Because my car is basically not so very different from the voice-controlled computer of 008 from the late nineties. It only knows a very limited set of commands and asks for very specific input at specific points during the interaction. That is no conversation at all. Siri on the other hand listens to everything we say and tries its best to interpret our intention. That, I think, is a remarkable step forward in techology. Even if it still often fails to grasp our true intention, it is unbelievable what it is already capable off. Human communication is complex because our language is so very ambiguous. Words have different meanings in different contexts, what we say literally is often not the same as what we mean. Humans are complex beings. We have tempers, become emotional, are always seeking self-confirmation. Even with the advanced state of technology of today, we are still a long way from real human-like conversation with computers. Watch this fragment from the movie HER.\u00a0\nhttps://m.youtube.com/watch?feature=youtu.be&v=u9gaM1GBud0\nNow that is a real conversation! It is interesting to see two types of VUI in this fragment: the first is the voice that guides Theodore through the setup of the OS, which is not so very different from the way we are used to interact with computers today. After initialisation this other VUI turns up: the very human-like and instantly intriguing Samantha. Until we have reached that level of conversational and emotional skill in voice user interfaces, let us restrict from using the term \u2018conversational interface\u2019. Today we are still in the infancy of the VUI, this is still the era of voice commands. Still, these VUIs need to be designed well in order for us humans to be able to interact with them confidently.\nThis is the era of voice commands\nSo I\u2019m back to my original quest of seeking what it takes to design a good VUI. I have described what I think we can expect from a VUI today, how that is very different from 20 years ago, and in what direction we might expect this technology to evolve. But that doesn\u2019t answer my question. So, in order to find out what skills you need to create a good VUI today, I will just need to create one myself. I have found a use case that seems perfectly fit for this: booking a meeting room at our office. I will call it Bookaroo (just add an m and you\u2019ll see why). I will start with a rudimentary prototype that I\u2019m planning to install in one of our meeting rooms to test and evaluate. In the next couple of months I will keep you updated on Bookaroo\u2019s progress and my learnings about designing a VUI by posting these blogs. Hope you will like it!\n", "tags": [], "categories": ["Blog", "Concepting &amp; UX"]}
{"post_id": 1647, "title": "Use SSL in a local network", "url": "https://www.luminis.eu/blog-en/development-en/use-ssl-in-a-local-network/", "updated_at": "2020-11-13T11:01:37", "body": "I recently built a Cordova application with Angular. The application periodically calls a server to get information. One of the requirements was that the client server communication had to be over SSL. I didn\u2019t know much about the SSL protocol but I thought it should be a piece of cake. With most hosting providers it is done in just a few clicks.\nBut this case is slightly different. The application must function without internet connection. the app communicates with an on site server in a local network by IP address.\nIn this post I will cover what the main reasons are to use SSL, what problems arise when you want to use SSL in a local network and how I fixed it for a Cordova application. Everything that is described in this post will also work (even better) for native applications.\nWhy we want to use SSL\nThe two reasons why we want the app to communicate with the server over SSL are Encryption and Identification.\nEncryption\nEncryption is basically hiding what is sent from one computer to another. If a you don\u2019t use SSL, the data will be transmitted as is and any computer on any of the networks between the client and the server can intercept and read the data.\n\nThe SSL protocol puts a barrier around the communication. All the data that is sent from the client to the server and vice versa will first be changed before it gets transmitted. The data can still be intercepted by any computer on the network but they can\u2019t read the data. The data is encrypted.\n\n\u00a0\nIdentification\nThere can be a secure connection, but that does not necessarily mean that the client can trust the computer that is on the other end. To validate the identity of a server the SSL Protocol uses a certificate. In the certificate are details of the receiver/owner. To make the certificate reliable, there is a whole process behind obtaining a certificate. The organization has to communicate with another organization called a\u00a0Certificate Authority (CA). The organization asks a certificate for a specific domain, the CA will check domain ownership and look up the details of the organization.\u00a0For some versions the CA will check more information of the business/organization but domain ownership is always mandatory.\nIf the CA approves the request, they create a certificate for the organization and sign it with their private key. The certificate can than be installed on the server to which the domain points to.\nIf a client connects through HTTPS to the server, the server sends its public key with its SSL certificate to the client. Once the client gets the certificate it will check the signature to make sure it\u2019s valid. This is done by e.g. checking if it is signed by a trusted CA and the current hostname matches the domain in the certificate.\nIf all checks succeed, a secure connection will be established. If not, (depending on the client) the request will fail.\nProblems\nTo get a valid certificate we need to pass a CA. A certificate belongs to a domain and the CA checks the domain ownership. In a local network we don\u2019t have a domain and we communicate to the server by IP address. Because of this, the domain ownership check cannot take place. A CA will never issue a certificate for a local IP address.\nBut can\u2019t we just use the certificate of the domain for our organization. We have a valid certificate. We can just installed that one on the server\u2026 Fortunately it is not that easy. When setting up the SSL connection, the client also check if the domain matches the domain on the certificate this is called \u2018Hostname verification\u2019. We communicate by an IP address, this does not match the domain and the connection fails.\nOur solution\nIn order for the hostname verification to succeed, our certificate must contain the IP address. We can\u2019t count on a CA so we have to make a certificate ourselves. This is possible and is called a \u2018Self signed certificate\u2019. This is a certificate that is not validated and signed by a CA, but by the organization itself. The IP address can be added by adding it as a \u2018subjectAltName\u2019 to the certificate. That way the hostname verification will succeed\nEarlier we said that the validation of a CA is used in the SSL checks. If the certificate is not signed by a CA that is known by the client, the check will fail. To make sure the client trusts our self signed certificate we need to pin the certificate in the application.\nIn Cordova certificate pinning is not possible. The server connections are handled by the webview. Therefore it is not possible to intercept SSL connections to perform the check of the server\u2019s certificate. True certificate pinning is only possible in native code. That means that every request must happens through native code instead of traditional XHR/AJAX requests.\nTo make all this work, we used a cordova plugin to pin the certificate and made sure the application makes all requests using this plugin. For convenience we created an Angular httpClient interceptor that stops the normal requests and delegates it to the plugin.\n", "tags": [], "categories": ["Blog", "Development"]}
{"post_id": 1566, "title": "Using Compound words in Elasticsearch", "url": "https://www.luminis.eu/blog-en/search-en/using-compound-words-in-elasticsearch/", "updated_at": "2020-11-18T08:39:58", "body": "From one of our customers, we got a question about using compound words in queries. Compound words are essential in languages like Dutch and German.\nSome examples in Dutch are zomervakantie, kookboek and koffiekopje. When a user enters koffiekopje, we want to find documents containing koffie kop as well as koffiekop and of course koffiekopje. In Elasticsearch, and other Lucene based search engines, you can use the\u00a0Compound Word Token Filter\u00a0to accomplish this.\nThere are two different versions, the Hyphenation decompounder, and the Dictionary decompounder. Reading the documentation, you\u2019ll learn that you should always use the Hyphenation one. The hyphenation breaks up the terms in their hyphens. By combining adjacent hyphens, we create terms and check them with a provided dictionary. With a match, the term gets added to the query, just like a synonym.\nAn example of hyphenation is:\nKoffiekopje -> kof \u2013 fie \u2013 kop \u2013 je\nThese hyphens would potentially result in the terms:\nkoffie, koffiekop, kop, kopje, koffiekopje.\nAs with other things with the Dutch language, I was a bit skeptic about the results of the filter. Therefore I decided to have a look at the implementation and try it out before I started using it for real. I had to create a class and do some sub-classing to get access to protected parameters and methods. But I was able to distill the mechanism as used by the filter and the appropriate Lucene classes.\nThe first step is the hyphenation part. For this part, you use the Lucene class\u00a0HyphenationTree. The following piece of code shows the construction of the hyphenation tree using the\u00a0mentioned XML filefrom the Objects For Formatting Objects project.\n\npublic TryHyphenation(String hyphenRulesPath) {\r\n    HyphenationTree tree = new HyphenationTree();\r\n    try (InputStream hyphenRules = new FileSystemResource(hyphenRulesPath).getInputStream()) {\r\n        InputSource source = new InputSource(hyphenRules);\r\n        tree.loadPatterns(source);\r\n    } catch (IOException e) {\r\n        LOGGER.error(\"Problem while loading the hyphen file\", e);\r\n    }\r\n    this.tree = tree;\r\n}\nThe constructor gives us access to the HyphenationTree containing the rules. Now we can ask for the hyphens of a string we can choose our selves. The result is an array with numbers. Each number represents the start of a new hyphen. The following code block finds a list of strings containing the found hyphens. Printing the hyphens is just a matter of joining the strings with a separator.\npublic List hyphenate(String sourceString) {\r\n    Hyphenation hyphenator = this.tree.hyphenate(sourceString, 1, 1);\r\n    int[] hyphenationPoints = hyphenator.getHyphenationPoints();\r\n    List parts = new ArrayList<>();\r\n    for (int i = 1; i < hyphenationPoints.length; i++) {\r\n        parts.add(sourceString.substring(hyphenationPoints[i-1], hyphenationPoints[i]));\r\n    }\r\n    return parts;\r\n}\r\n\r\nTryHyphenation hyphenation = new TryHyphenation(HYPHEN_CONFIG);\r\nString sourceString = \"Koffiekopje\";\r\nSystem.out.println(\"*** Find Hyphens:\");\r\nList hyphens = hyphenation.hyphenate(sourceString);\r\nString joinedHyphens = StringUtils.arrayToDelimitedString(\r\n        hyphens.toArray(), \" - \");\r\nSystem.out.println(joinedHyphens);\nRunning the code results in the following hyphens (or output).\n*** Find Hyphens:\r\nKof - fie - kop - je\nNext step is finding the terms we want to search for based on the provided compound word. The elasticsearch analyzer uses the Lucene class HyphenationCompoundWordTokenFilter to find terms out of compound words. We can use this class in our sample code as well; we have to extend it to get access to the protected tokens variable. Therefore we created this following sub-class.\nprivate class AccessibleHyphenationCompoundWordTokenFilter extends HyphenationCompoundWordTokenFilter {\r\n    public AccessibleHyphenationCompoundWordTokenFilter(TokenStream input, \r\n                                                        HyphenationTree hyphenator, \r\n                                                        CharArraySet dictionary) {\r\n        super(input, hyphenator, dictionary);\r\n    }\r\n\r\n    public List getTokens() {\r\n        return tokens.stream().map(compoundToken -> compoundToken.txt.toString())\r\n                .collect(Collectors.toList());\r\n    }\r\n}\nWith the following code, we can find the tokens that are available in our dictionary that are equal to found hyphens or combinations of hyphens. This class is not meant for our way of using it. Therefore the code looks a bit weird. But it does help us to understand what happens. We need a tokenizer; we use the Standard tokenizer from Lucene. We also need a reader with access to the string that needs to be tokenized. Next, we create the CharSetArray containing our dictionary of terms to find. With the HyphenationTree, the tokenizer and the dictionary we create the AccessibleHyphenationCompoundWordTokenFilter. After calling the internal methods of the filter, we can call our method with access to the internal variable tokens.\npublic static final List DICTIONARY = Arrays.asList(\"koffie\",\"kop\", \"kopje\");\r\npublic List findTokens(String sourceString) {\r\n    StandardTokenizer tokenizer = new StandardTokenizer();\r\n    tokenizer.setReader(new StringReader(sourceString));\r\n\r\n    CharArraySet charArraySet = new CharArraySet(DICTIONARY, true);\r\n    AccessibleHyphenationCompoundWordTokenFilter filter = \r\n            new AccessibleHyphenationCompoundWordTokenFilter(tokenizer, tree, charArraySet);\r\n    try {\r\n        filter.reset();\r\n        filter.incrementToken();\r\n        filter.close();\r\n    } catch (IOException e) {\r\n        LOGGER.error(\"Could not tokenize\", e);\r\n    }\r\n    return filter.getTokens();\r\n}\nNow we have the terms from the compound word that is also in our dictionary.\nSystem.out.println(\"\\n*** Find Tokens:\");\r\nList tokens = hyphenation.findTokens(sourceString);\r\nString joinedTokens = StringUtils.arrayToDelimitedString(tokens.toArray(), \", \");\r\nSystem.out.println(joinedTokens);\r\n\r\n*** Find Tokens:\r\nKoffie, kop, kopje\nUsing this test class is nice, but now we want to use it within elasticsearch. The following link is a reference to a gist containing the commands to try it out in Kibana Console. Using this sample, you can play around and investigate the effect of the HyphenationCompoundWordTokenFilter. Don\u2019t forget to install the Dutch language file in the config folder of elasticsearch.\u00a0Compound Word Token Filter Instalation\nGist containing java class and Kibana Console example\n", "tags": [], "categories": ["Blog", "Search"]}
{"post_id": 1703, "title": "Head first Serverless with AWS Lambda in Kotlin", "url": "https://www.luminis.eu/blog-en/cloud-en/head-first-serverless-with-aws-lambda-in-kotlin/", "updated_at": "2020-12-10T09:05:17", "body": "In this post I\u2019ll take you with me on my journey into Serverless with AWS. Head first, because I won\u2019t dive into details like\u00a0what is Serverless, why\u00a0should you or should you not\u00a0want Serverless,\u00a0what those\u00a0Amazon Web Services\u00a0(AWS) are, nor into\u00a0Kotlin. I will just work out a concrete case.\nAt our office we regularly hold knowledge sessions. Mostly around\u00a0lunch time a colleague will tell and show something about a topic. In the morning of an upcoming knowledge session someone sends out an announcement via email to all our colleagues.\u00a0Since most of us also are on Slack, we could announce it there. And this is something we could easily automate.\u00a0In this post I am going to build a Slack bot that announces knowledge sessions in the morning. What do we need?\n\nA schedule of planned sessions\nPiece of code that sends a message to a Slack channel\n\nFor the sake of brevity I will focus on the absolute minimum of these requirements.\nAmazon Web Services\nThe following AWS services will be used:\n\nS3 to store the scheduled sessions.\nLambda to retrieve data from store and send a message to Slack when needed.\nCloudWatch for logging/monitoring and triggering a scheduled event every morning.\n\n\u00a0\nIAM (Identity and Access Management)\nOne service that\u2019s missing in the list above list is IAM. This service is an inherent part of AWS. Before starting with the other services first we\u2019ll create a role for our App. This role should have permissions to:\n\nAmazonS3FullAccess\nAWSLambdaExecute\nCloudWatchLogsFullAccess\n\nThis single role will be used for all services I\u2019ll be creating next.\nS3 (Simple Storage Service)\nFirst step is to create a\u00a0S3 bucket. S3 is an object storage that can be used to store and retrieve files. Open S3 from the AWS console and create a bucket with the name \u2018upcoming-sessions\u2019 and use the default settings (click next, next, next). In this bucket upload a file \u2018next-session.json\u2019 with the following contents:\n\u00a0\n{\r\n\"date\": \"2019-01-20\",\r\n\"presenter\": \"Rachid\",\r\n\"topic\": \"Creating a Slack announcer using AWS\",\r\n\"type\": \"Chalk & Talk\"\r\n}\nLambda\nNext step is to create a lambda which will read this file and send a message to Slack when needed. In the AWS management console go to: Lambda -> functions -> Create function. As name choose: \u201csession-announcer\u201d.\u00a0 Since I want to write the lambda in Kotlin for runtime I choose Java 8. For Role select \u201cchoose an existing role\u201d and select the Role we just created. Please note that everything can be done via scripts using the\u00a0AWS CLI\u00a0as well. Creating a new lambda function would look like:\n\u00a0\naws lambda create-function \\\r\n--function-name session-announcer \\\r\n--runtime java8 \\\r\n--role arn:aws:iam::123456789:role/sessionAnnouncerRole \\\r\n--handler eu.luminis.chefke.event.ScheduledEventAnnouncer::handleRequest \\\r\n--zip-file announcer.jar\nSince we don\u2019t have any code or jar file yet I won\u2019t use this now.\n\u00a0\n\nCoding in Kotlin\nNow it\u2019s time for the fun part; coding the logic to retrieve the sessions from S3 and send a message to Slack when needed. The\u00a0handleRequest\u00a0function is the entrypoint of our code. This function should be invoked to run our code.\n\u00a0\nimport com.amazonaws.services.lambda.runtime.Context\r\nimport com.amazonaws.services.lambda.runtime.RequestHandler\r\nimport com.amazonaws.services.lambda.runtime.events.APIGatewayProxyRequestEvent\r\nimport com.amazonaws.services.lambda.runtime.events.APIGatewayProxyResponseEvent\r\nimport com.amazonaws.services.s3.AmazonS3ClientBuilder\r\nimport com.beust.klaxon.Klaxon\r\nimport java.io.InputStream\r\nimport java.time.LocalDate\r\nimport java.time.format.DateTimeFormatter\r\n \r\ndata class SlackResponseMessage(val text: String)\r\n \r\ndata class LuminisEvent(val topic: String, val type: String, val presenter: String, val date: String)\r\n \r\nconst val filename = \"next-session.json\"\r\n \r\nclass SessionAnnouncer : RequestHandler<APIGatewayProxyRequestEvent, APIGatewayProxyResponseEvent> {\r\n \r\n    val s3client = AmazonS3ClientBuilder.defaultClient()\r\n    val restClient = RestClient()\r\n \r\n    override fun handleRequest(event: APIGatewayProxyRequestEvent, context: Context): APIGatewayProxyResponseEvent {\r\n        val today = LocalDate.now()\r\n        getEvent(today)?.let { e ->\r\n            val message = \"Today we have a ${e.type} session about '${e.topic}' presented by ${e.presenter}.\"\r\n            println(\"Sending the event to Slack: $message\")\r\n            val slackJsonMessage = Klaxon().toJsonString(SlackResponseMessage(message))\r\n            // POST the announcement as a JSON payload to the Slack webhook URL.\r\n            restClient.post(Config.slackWebhook, slackJsonMessage)\r\n            return APIGatewayProxyResponseEvent().apply {\r\n                statusCode = 200\r\n                headers = mapOf(\"Content-type\" to \"text/plain\")\r\n                body = \"Message sent to Slack: $message\"\r\n            }\r\n        }\r\n \r\n        val notFoundMessage = \"No event found for ${today} to post to Slack\"\r\n        println(notFoundMessage)\r\n        return APIGatewayProxyResponseEvent().apply {\r\n            statusCode = 404\r\n            headers = mapOf(\"Content-type\" to \"text/plain\")\r\n            body = notFoundMessage\r\n        }\r\n    }\r\n \r\n    /**\r\n     * Retrieves JSON file from S3, parse it and return the Object when it's today.\r\n     */\r\n    private fun getEvent(day: LocalDate): LuminisEvent? {\r\n        getEventFromBucket()?.let {\r\n            val event = Klaxon().parse(it)\r\n            event?.let { nextEvent ->\r\n                val eventDay = LocalDate.parse(nextEvent.date, DateTimeFormatter.ISO_DATE)\r\n                if(eventDay.equals(day)) {\r\n                    return nextEvent\r\n                }\r\n            }\r\n        }\r\n        return null\r\n    }\r\n \r\n    fun getEventFromBucket(): InputStream? {\r\n        val s3Bucket = Config.s3Bucket\r\n        if(s3client.doesObjectExist(s3Bucket, filename)) {\r\n            return s3client.getObject(s3Bucket, filename).objectContent\r\n        }\r\n        println(\"'$filename' does not exists in the bucket: $s3Bucket\")\r\n        return null\r\n    }\r\n}\nThere are\u00a0several ways\u00a0to write this code and publish it to AWS. For any serious coding I personally prefer to use my favorite IDE on my local machine. To keep build/test/package related logic in a central place I use Gradle. Luckily there\u2019s a\u00a0Gradle plugin\u00a0for uploading the code to an AWS lambda. My\u00a0build.gradle\u00a0file is as follows:\n\u00a0\nimport com.amazonaws.services.lambda.model.InvocationType\r\nimport jp.classmethod.aws.gradle.lambda.AWSLambdaInvokeTask\r\nimport jp.classmethod.aws.gradle.lambda.AWSLambdaMigrateFunctionTask\r\n \r\nbuildscript {\r\n    ext.kotlin_version = '1.2.31'\r\n \r\n    repositories {\r\n        mavenCentral()\r\n        maven {\r\n            url \"https://plugins.gradle.org/m2/\"\r\n        }\r\n    }\r\n    dependencies {\r\n        classpath 'com.github.jengelman.gradle.plugins:shadow:2.0.2'\r\n        classpath \"org.jetbrains.kotlin:kotlin-gradle-plugin:$kotlin_version\"\r\n        classpath \"jp.classmethod.aws:gradle-aws-plugin:0.30\"\r\n    }\r\n}\r\n \r\nversion '1.0-SNAPSHOT'\r\n \r\napply plugin: 'com.github.johnrengelman.shadow' // To create a fatjar which can be uploaded to AWS\r\napply plugin: 'jp.classmethod.aws'              // Gradle tasks for AWS stuff\r\napply plugin: 'jp.classmethod.aws.lambda'       // Gradle tasks for deploying and running lambda's\r\napply plugin: 'kotlin'\r\n \r\nrepositories {\r\n    jcenter()\r\n    mavenCentral()\r\n}\r\n \r\ndependencies {\r\n    implementation \"org.jetbrains.kotlin:kotlin-stdlib-jdk8:$kotlin_version\"\r\n    // AWS API\r\n    implementation 'com.amazonaws:aws-lambda-java-core:1.2.0'\r\n    implementation 'com.amazonaws:aws-lambda-java-events:2.1.0'\r\n    implementation 'com.amazonaws:aws-java-sdk-s3:1.11.308'\r\n    // JSON parser for Kotlin\r\n    implementation 'com.beust:klaxon:3.0.1'\r\n}\r\n \r\ncompileKotlin {\r\n    kotlinOptions.jvmTarget = \"1.8\"\r\n}\r\n \r\nlambda {\r\n    region = \"eu-west-1\"\r\n}\r\n \r\n// Task to deploy the code to AWS\r\ntask deployFunction(type: AWSLambdaMigrateFunctionTask, dependsOn: [shadowJar, test]) {\r\n    functionName = \"session-announcer\"\r\n    runtime = com.amazonaws.services.lambda.model.Runtime.Java8\r\n    role = \"arn:aws:iam::${aws.accountId}:role/scheduleCntRole\"\r\n    zipFile = shadowJar.archivePath\r\n    handler = \"eu.luminis.blog.SessionAnnouncer::handleRequest\"\r\n    memorySize = 512\r\n    timeout = 20\r\n}\r\n \r\n// Task to directly invoke the lambda in AWS\r\ntask invokeFunction(type: AWSLambdaInvokeTask) {\r\n    functionName = \"session-announcer\"\r\n    invocationType = InvocationType.RequestResponse\r\n    payload = \"\"\r\n    doLast {\r\n        println \"Lambda function result: \" + new String(invokeResult.payload.array())\r\n    }\r\n}\nOn top the plugins are applied and the dependencies are declared, at the bottom there are two custom tasks;\u00a0deployFunction\u00a0and\u00a0invokeFunction. The first should be executed to upload the code to AWS, and the latter can be used for directly invoking the code running at AWS. Note that in\u00a0deployFunction\u00a0we\u2019ve specified the\u00a0handler function of our lambda. Now let\u2019s upload the code by executing the task\u00a0deployFunction. In IntelliJ this can be done by expanding the tasks in the Gradle view and double click\u00a0deployFunction. Or just open a Terminal in the root of the project and run\u00a0./gradlew session-announcer:deployFunction. NB: When using this Gradle plugin or AWS CLI for the first time you may need to authenticate first. When the function is deployed successfully we can try to invoke it with:\u00a0./gradlew session-announcer:invokeFunction Oh noes, I got an error: {\"errorMessage\":\"Missing env var 'S3_BUCKET'!\",\"errorType\":\"java.lang.IllegalStateException\"} Of course, the code uses\u00a0Config.s3Bucket\u00a0which reads the bucket name from an environment variable because we don\u2019t want to have any configuration in code.\n\u00a0\nobject Config {\r\n    val s3Bucket by lazy { getRequiredEnv(\"S3_BUCKET\") }\r\n    val slackWebhook by lazy { getRequiredEnv(\"SLACK_WEBHOOK_URL\") }\r\n \r\n    private fun getRequiredEnv(name: String): String {\r\n        println(\"Retrieving environment variable: $name\")\r\n        return System.getenv(name) ?: throw IllegalStateException(\"Missing env var '$name'!\")\r\n    }\r\n}\nWe should add two required environment variables. Open the AWS console and go to lambda -> session-announcer and scroll down to the section \u201cEnvironment variables\u201d. Here we can add the key/value pairs. So add the key S3_BUCKET and for value the name of the bucket we created, e.g. \u2018upcoming-sessions\u2019. Before we can add the second environment variable, first a bit more about Slack.\nSlack integration\nIn order to send messages to a Slack Channel you need to create a so called \u2018Slack App\u2018 and install it in the Slack workspace. After you\u2019ve added the app to a channel, a webhook URL is generated and can be retrieved from the Slack console via:\u00a0https://api.slack.com.\u00a0\u00a0This URL is the second environment variable we need to configure for the AWS lambda. So go back to the AWS console and add to the section \u201cEnvironment variables\u201d a new key: SLACK_WEBHOOK_URL with the slack webhook URL as value. Now try to invoke the lambda again with:\u00a0./gradlew session-announcer:invokeFunction Hopefully you will either see something like: {\"statusCode\":200,\"headers\":{\"Content-type\":\"text/plain\"},\"body\":\"Message sent to Slack: Today we have a Chalk & Talk session about 'Creating a Slack announcer using AWS' presented by Rachid.\"} Or: {\"statusCode\":404,\"headers\":{\"Content-type\":\"text/plain\"},\"body\":\"No event found for 2019-01-20 to post to Slack\"} To get more insights, the logs can be viewed in CloudWatch. In the AWS console go to CloudWatch -> Logs. There should be a log with the name\u00a0/aws/lambda/session-announcer\u00a0containing all the logs of our lambda.\nScheduling\nNow we have a piece of code running as a lambda in AWS that can announce the sessions of the current day. Next step is to trigger this lambda every morning. This can be done with \u2018Events\u2019 in CloudWatch. In the AWS console go to CloudWatch -> Events -> Rules and click \u201ccreate rule\u201d. Then select Schedule (instead of Event pattern). To let it run every morning at 7:30 AM you can enter the following CRON expression:\u00a030 7 * * ? *. At the right-hand side click \u2018Add target\u2019 and for function select the name of our lambda:\u00a0session-announcer. Click configure details, choose a name and Save the rule. The next morning at 7:30 AM local time of the AWS region the lambda will be triggered. For testing purposes you could trigger it every minute with this CRON expression:\u00a0*/1 * * * ? *. Quite quickly we have automated a simple task. When you want to play around with this yourself, all the code from this post can be found in\u00a0this Git repository.\nBecoming an AWS developer\nIn the beginning playing around with AWS can be quite overwhelming. When you want to get some serious knowledge I recommend the Udemy course\u00a0AWS Certified Solutions Architect \u2013 Associate\u00a0 (most of the time priced around $10,-). A lot of information is scattered on the internet via blog posts and in the Amazon documentation. But what I like about this course is that it is kept up-to-date and gives you a single cohorent story which is easy to follow.\nWind up\nIn this post I focused on the bare minimum, but\u00a0actually I also built:\n\nREST interface to add sessions to the store; using\u00a0API gateway\u00a0and another lambda\nREST interface to get session from store; called by a custom Slack command and backed by another lambda\n\nNow we have a small working app, some improvements and new features I have in mind:\n\nStore the session in a data store like\u00a0Dynamo DB\nMake it possible to schedule multiple sessions\nAutomatically provisioning of the services with\u00a0CloudFormation\n\n", "tags": [], "categories": ["Blog", "Cloud"]}
{"post_id": 18190, "title": "Luminis is Microsoft Gold Partner", "url": "https://www.luminis.eu/blog-en/luminis-is-microsoft-gold-partner/", "updated_at": "2019-02-12T15:35:25", "body": "Amersfoort \u2013 Every year Microsoft only rewards a few companies with Gold-status recognition in particular competencies. \u00a0Accreditation is awarded to companies that are able to constantly deliver high-quality services and developments. \u00a0Software and technology company Luminis recently had the honor of becoming a Microsoft Gold Partner in the Application Development competency. \u00a0This partnership is in line with the international strategy of Luminis which is mainly focused on being a high standard technology and innovation partner for its customers.\n\u00a0As a Microsoft Certified Partner Luminis and Microsoft will collaborate intensively with one another. In order to do so Luminis employs certified experts who have a broad and profound knowledge of Microsoft Technology. In addition to the Gold Partner status for Application Development, Luminis is also a Cloud Solution Provider (CSP) which enables Luminis to offer Microsoft\u2019s Azure Cloud software directly to its customers.\n\u00a0\n\u00a0\n", "tags": [], "categories": ["Blog"]}
{"post_id": 1559, "title": "Axon Metrics with Spring Boot 2 + Prometheus + Grafana", "url": "https://www.luminis.eu/blog-en/development-en/axon-metrics-with-spring-boot-2-prometheus-grafana/", "updated_at": "2020-11-24T14:55:53", "body": "Included in the Axon framework is a module that will collect metrics on the different Axon message types (commands, events and queries). In this blog, I will explain how to use the Axon metrics module in combination with Prometheus and visualize the metrics in Grafana.\u00a0\nFull disclaimer, I\u2019m contributing to the Axon Metrics module myself, so I\u2019m not completely impartial \ud83d\ude09\nIntro Axon\nAxon is a Java framework for writing applications using the CQRS paradigm. One of the things CQRS focuses on more than the traditional CRUD/Active Record style of writing applications is business interactions. In a CRUD based system the focus lies more on the Creating/Read/Update/Deletion (hence CRUD) of records while CQRS focuses more the business interactions by modeling the explicit actions (Commands).\nFor example, instead of an update on the customer information record when the customer moves to a different address, CQRS also models the intent of the change. There will be an explicit MoveCustomer command. When using CQRS with event sourcing this command will also result in an explicit event that indicates that the customer has moved. Probably something like a CustomerMoved event. This explicit modeling of business interactions and state changes has great benefits in applications with complex business processes.\nIn a traditional CRUD application this information is usually lost because it\u2019s often impossible to deduce why, in this example, the customer\u2019s address was updated (was it a misspelling or did the customer actually move to a new address). When correctly applying event sourcing we can record and react to this information.\nAnother advantage of having the business interactions modeled in explicit commands and events is that with Axon metrics we can measure the actual business itself. Visualizing metrics collected on these messages holds great value for businesses. Instead of measuring that certain (obscure) technical calls and updates are made to a system. Why not measure the amount of OrderPlaced events and put a thermometer in the core business process itself.\nWhen the number of OrderPlaced events suddenly go down there is a problem that affects the bottom line of the business, not just a technical problem of some obscure REST API call or internal service call that doesn\u2019t work anymore where it\u2019s unclear what the impact is on the business.\nExample application\nI\u2019ve created an example application to demonstrate the capabilities of using Axon with the metrics module. The code can be found here:\u00a0https://github.com/luminis-ams/axon-metrics-example\u00a0The example application contains a flight ticket booking domain. There is a component that simulates user interaction by generating commands that automatically create flights and books seats on flights.\nI\u2019ve instrumented the message processors in such a way that is possible to monitor individual message payload types (like BookSeatCommand and SeatBookedEvent for example) to individual message handlers (like the SlowEventListener component for example). I\u2019ve tried to put in some interesting problems in the application so that I can demonstrate how you can detect problems in your Axon application by monitoring it.\nThe Axon Metric configuration code can be found\u00a0here.\nSmall side note, the current metrics module uses Dropwizard metrics. In order to expose the metrics using Prometheus I\u2019ve configured an exporter for the Dropwizard metrics to Prometheus with this line:\ncollectorRegistry.register(new DropwizardExports(metricRegistry));\r\n\n(see\u00a0MetricConfig line 33)\nMicrometer Metrics in Axon\nI\u2019ve sent in a\u00a0pull request\u00a0to Axon to add support for Micrometer metrics. Micrometer is the new default metrics library which is used in Spring Boot 2.0. If you check out the axon-micrometer_beta branch on the GitHub repository you can see how to use the new module (for now you only can use it if you maven install my pull request branch)\nIn the configuration class on the Micrometer branch, you can see it\u2019s not necessary to export the Dropwizard metrics to Prometheus anymore. Micrometer provides an abstraction of Prometheus which we now use directly. Micrometer can be used with a lot of different metric implementations. For an overview see\u00a0https://micrometer.io/docs\nDashboard with Grafana\nNow that the metrics are exposed via Prometheus we can use Grafana to visualize them in a dashboard. (For more information about Prometheus and Grafana see the excellent blog of my colleague Jeroen Reijn about \u201cMonitoring Spring Boot applications with Prometheus and Grafana\u201d)\nHere are some screenshots from Grafana.\nCapacityMonitor on events\nThis one displays the capacity of the event listeners\n\nAs you can see in the above image there is 1 event listener that is close to full capacity. If you have a single thread processor this means that almost 100% of the time the event listener is busy. Probably events are queuing up for this event processor. The capacity is not always 1 because the capacity doesn\u2019t measure the overhead of the Axon Framework itself, Axon has to retrieve event messages from a database and store them once their processed.\nMessageTimerMonitor on events\nThis one displays the timings of the different event processors.\n\nYou can see that two of the four event processors show a latency of sometimes more than 4 seconds for half of the requests. This is a problem and should be looked at.\nMessageCountingMonitor on BookSeatCommand\nThis graph displays the success, failure and ingested counter of the BookSeatCommand.\n\nAs you can see sometimes the command handler of the BookSeatCommand throws an Exception which will be counted as a failure in the graph.\nRun the example application\nI\u2019ve created a docker setup if you want to run the full stack yourself. You can check out the code of example application here:\nhttps://github.com/luminis-ams/axon-metrics-example\nAfter that first do a build of the app project to create the application docker image. From the project root run:\n./mvnw clean build \r\n\nAfter that run:\ndocker-compose -f docker/docker-compose.yml up\r\n\nAs told the metrics are exposed using Prometheus. On the Prometheus endpoint\u00a0http://localhost:8080/actuator/prometheus\u00a0you can see the metrics in raw form.\nIn Prometheus, you can see the metrics under\u00a0http://localhost:9090\nYou can import the example metrics dashboard in Grafana in the following way:\n\nLogin\u00a0http://localhost:3000\u00a0(login admin:password)\nOn the left side of the screen click on the + icon and select import\nCopy the contents of grafana/dashboard.json into the bottom text field\nSelect the Prometheus data source in the dropdown\n\nConclusion\nBecause of Axon\u2019s strong focus on modeling business interactions, metrics on those business interactions contain a lot more useful information. Useful not only for troubleshooting but also for measuring the performance of the business itself.\n", "tags": [], "categories": ["Blog", "Development"]}
{"post_id": 1333, "title": "Migrating from GSA to Elasticsearch with ANTLR", "url": "https://www.luminis.eu/blog-en/search-en/migrating-from-gsa-to-elasticsearch-with-antlr/", "updated_at": "2020-11-16T16:11:40", "body": "Starting January 1st of 2019, the Google Search Appliance (GSA) is set to be EOL. For one of my clients, we have chosen Elasticsearch as our alternative from 2019 onwards.\nHowever, there was one problem; the current GSA solution is in use by several API\u2019s that can\u2019t be changed for various reasons. Therefore, it was up to us to replicate the GSA\u2019s behaviour with the new Elasticsearch implementation and come migration time, to swap out the GSA for Elasticsearch with as little functional changes as possible.\nThe GSA provides\u00a0a range of functionality, some of which is easily implemented with other technologies. In our case, this included of course search functionality, but also other things such as website crawling. The part that I found most interesting however, was the GSA\u2019s ability to enable users to form queries based on two small domain specific languages (DSL).\nQueries in these two DSL\u2019s reach the GSA as query parameters on the GSA URL. The first DSL, specified with query parameter\u00a0q, has three \u201cmodes\u201d of functionality:\n\nFree text search, by simply putting some space separated terms\nallintext\u00a0search, a search query much like the free text search, but excluding fields such as metadata, anchors and URL\u2019s from the search\ninmeta\u00a0search, which can potentially\u00a0do a lot, but in our case was merely restricted to searches on metadata of the form\u00a0key=value.\n\nThe second DSL, specified with query parameter\u00a0partialFields\u00a0also provides searching on metadata. In this case, searches are of the form\u00a0(key:value)\u00a0and may be combined with three boolean operators:\n\n.: AND\n|: OR\n-: NOT\n\nAn example query could then be\u00a0(key1:value1)|(key2.value2).\nIn this blog, I will explain how to implement these two DSL\u2019s using ANTLR and I will show you how ANTLR enables us to separate the parsing of the DSL from our other application logic.\nIf this is your first time working with ANTLR, you may want to read two posts ([1],\u00a0[2]) that have been posted on our blog earlier.\nIf you are looking for the complete implementation, then please refer to the\u00a0Github repository.\nParsing the GSA DSL\nLet us start with parsing the\u00a0q\u00a0DSL. I have split the ANTLR grammar into a separate parser and lexer file for readability.\nThe parser is as follows:\nparser grammar GsaQueryParser;\r\n    \r\noptions { tokenVocab=GsaQueryLexer; }\r\n    \r\nquery   : pair (OR? pair)*  #pairQuery\r\n        | TEXT+             #freeTextQuery;\r\n    \r\npair    : IN_META TEXT+                 #inmetaPair\r\n        | ALL_IN_TEXT TEXT (OR? TEXT)*  #allintextPair;\nAnd the lexer is defined below:\nlexer grammar GsaQueryLexer;\r\n\r\nALL_IN_TEXT : 'allintext:';\r\nIN_META     : 'inmeta:';\r\n\r\nOR          : 'OR';\r\n\r\nTEXT        : ~(' '|'='|':'|'|'|'('|')')+;\r\n\r\nWHITESPACE  : [ \\t\\r\\n]+ -> skip;\r\nIGNORED     : [=:|()]+ -> skip;\nNote that the parser grammar reflects the two different ways that the q DSL can be used; by specifying pairs or by simply putting a free text query. The pairs can be separated by an OR operator. Furthermore, the allintext keyword may separate terms with OR as well.\nThe definition of the partialFields DSL is somewhat different because it allows for query nesting and more boolean operators. Both the parser and the lexer are shown below, again in two separate files.\nParser:\nparser grammar GsaPartialFieldsParser;\r\n\r\noptions { tokenVocab=GsaPartialFieldsLexer; }\r\n\r\nquery       : pair\r\n            | subQuery;\r\n\r\nsubQuery    : LEFTBRACKET subQuery RIGHTBRACKET\r\n            | pair (AND pair)+\r\n            | pair (OR pair)+\r\n            | subQuery (AND subQuery)+\r\n            | subQuery (OR subQuery)+\r\n            | subQuery AND pair\r\n            | subQuery OR pair\r\n            | pair AND subQuery\r\n            | pair OR subQuery;\r\n\r\npair        : LEFTBRACKET KEYWORD VALUE RIGHTBRACKET        #inclusionPair\r\n            | LEFTBRACKET NOT KEYWORD VALUE RIGHTBRACKET    #exclusionPair\r\n            | LEFTBRACKET pair RIGHTBRACKET                 #nestedPair;\nLexer:\nlexer grammar GsaPartialFieldsLexer;\r\n\r\nAND         : '.';\r\nOR          : '|';\r\nNOT         : '-';\r\n\r\nKEYWORD     : [A-z0-9]([A-z0-9]|'-'|'.')*;\r\nVALUE       : SEPARATOR~(')')+;\r\n\r\nSEPARATOR   : [:];\r\nLEFTBRACKET : [(];\r\nRIGHTBRACKET: [)];\r\nWHITESPACE  : [\\t\\r\\n]+ -> skip;\nCreating the Elasticsearch query\nNow that we have our grammars ready, it\u2019s time to use the parse tree generated by ANTLR to construct corresponding Elasticsearch queries. I will post some source code snippets, but make sure to refer to the\u00a0complete implementation\u00a0for all details.\nFor both DSL\u2019s, I have chosen to walk the tree using the visitor pattern. We will start be reviewing the\u00a0qDSL.\nCreating queries from the q DSL\nThe visitor of the\u00a0q\u00a0DSL extends a\u00a0BaseVisitor\u00a0generated by ANTLR and will eventually return a\u00a0QueryBuilder, as indicated by the generic type:\n\npublic class QueryVisitor extends GsaQueryParserBaseVisitor\nThere are three cases that we can distinguish for this DSL: a free text query, an allintext query or an inmeta query. Implementing the free text and allintext query means extracting the TEXT token from the tree and then constructing a MultiMatchQueryBuilder, e.g.:\n@Override\r\npublic QueryBuilder visitFreeTextQuery(GsaQueryParser.FreeTextQueryContext ctx) {\r\n    String text = concatenateValues(ctx.TEXT());\r\n    return new MultiMatchQueryBuilder(text, \"album\", \"artist\", \"id\", \"information\", \"label\", \"year\");\r\n}\r\n\r\nprivate String concatenateValues(List textNodes) {\r\n    return textNodes.stream().map(ParseTree::getText).collect(joining(\" \"));\r\n}\nThe fields that you use in this match query depend on the data that is in Elasticsearch \u2013 in my case some documents describing music albums.\nAn inmeta query requires us to extract both the field and the value, which we then use to construct a MatchQueryBuilder, e.g.:\n@Override\r\npublic QueryBuilder visitInmetaPair(GsaQueryParser.InmetaPairContext ctx) {\r\n    List textNodes = ctx.TEXT();\r\n\r\n    String key = textNodes.get(0).getText().toLowerCase();\r\n    textNodes.remove(0);\r\n    String value = concatenateValues(textNodes);\r\n\r\n    return new MatchQueryBuilder(key, value);\nWe can then combine multiple pairs by implementing the visitPairQuery method:\n@Override\r\npublic QueryBuilder visitPairQuery(GsaQueryParser.PairQueryContext ctx) {\r\n    BoolQueryBuilder result = new BoolQueryBuilder();\r\n    ctx.pair().forEach(pair -> {\r\n        QueryBuilder builder = visit(pair);\r\n        if (hasOrClause(ctx, pair)) {\r\n            result.should(builder);\r\n            result.minimumShouldMatch(1);\r\n        } else {\r\n            result.must(builder);\r\n        }\r\n    });\r\n    return result;\r\n}\nCreating queries from the partialFields DSL\nThe visitor of the partialFields DSL also extends a BaseVisitor generated by ANTLR and also returns a QueryBuilder:\npublic class PartialFieldsVisitor extends GsaPartialFieldsParserBaseVisitor\nThere are three kinds of pairs that we can specify with this DSL (inclusion, exclusion or nested pair) and we can override a separate method for each option, because we labelled these alternatives in our grammar. A nested pair is simply unwrapped and then passed back to ANTLR for further processing:\n@Override\r\npublic QueryBuilder visitNestedPair(GsaPartialFieldsParser.NestedPairContext ctx) {\r\n    return visit(ctx.pair());\r\n} \nThe inclusion and exclusion query implementations are quite similar to each other:\n@Override\r\npublic QueryBuilder visitInclusionPair(GsaPartialFieldsParser.InclusionPairContext ctx) {\r\n    return createQuery(ctx.KEYWORD().getText(), ctx.VALUE().getText(), false);\r\n}\r\n\r\n@Override\r\npublic QueryBuilder visitExclusionPair(GsaPartialFieldsParser.ExclusionPairContext ctx) {\r\n    return createQuery(ctx.KEYWORD().getText(), ctx.VALUE().getText(), true);\r\n}\r\n\r\nprivate QueryBuilder createQuery(String key, String value, boolean isExcluded) {       \r\n    value = value.substring(1);\r\n\r\n    if (isExcluded) {\r\n        return new BoolQueryBuilder().mustNot(new MatchQueryBuilder(key, value));\r\n    } else {\r\n        return new MatchQueryBuilder(key, value).operator(Operator.AND);\r\n    }\r\n}\nRemember that we included the\u00a0:\u00a0to help our token recognition? The code above is where we need to handle this by taking the substring of the value. What remains is to implement a way to handle the combinations of pairs and boolean operators. This is done by implementing the\u00a0visitSubQuerymethod and you can view the implementation\u00a0here. Based on the presence of an AND or OR operator, we apply must or should clauses, respectively.\nExamples\nIn my repository, I\u2019ve included a REST controller that can be used to execute queries using the two DSL\u2019s. Execute the following steps to follow along with the examples below:\n\nStart an Elasticsearch instance at\u00a0http://localhost:9200\u00a0(the application assumes v6.4.2)\nClone the repository:\u00a0git clone https://github.com/markkrijgsman/migrate-gsa-to-elasticsearch.git && cd migrate-gsa-to-elasticsearch\nCompile the repository:\u00a0mvn clean install\nRun the application:\u00a0cd target && java -jar search-api.jar\nFill the Elasticsearch instance with some documents:\u00a0http://localhost:8080/load\nStart searching:\u00a0http://localhost:8080/search\n\nYou can also use the Swagger UI to execute some requests:\u00a0http://localhost:8080/swagger-ui.html. For each example I will list the URL for the request and the resulting Elasticsearch query that is constructed by the application.\nGet all albums mentioning Elton John\nhttp://localhost:8080/search?q=Elton\n\nGET rolling500/_search\r\n{\r\n  \"query\": {\r\n    \"bool\": {\r\n      \"must\": [\r\n        {\r\n          \"multi_match\": {\r\n            \"query\": \"Elton\",\r\n            \"fields\": [\r\n              \"album\",\r\n              \"artist\",\r\n              \"id\",\r\n              \"information\",\r\n              \"label\",\r\n              \"year\"\r\n            ],\r\n            \"type\": \"best_fields\",\r\n            \"operator\": \"AND\",\r\n            \"lenient\": true,\r\n          }\r\n        }\r\n      ]\r\n    }\r\n  }\r\n}\nGet all albums where Elton John or Frank Sinatra are mentioned\nhttp://localhost:8080/search?q=allintext:Elton%20OR%20Sinatra\nGET rolling500/_search\r\n{\r\n  \"query\": {\r\n    \"bool\": {\r\n      \"must\": [\r\n        {\r\n          \"bool\": {\r\n            \"must\": [\r\n              {\r\n                \"multi_match\": {\r\n                  \"query\": \"Elton Sinatra\",\r\n                  \"fields\": [\r\n                    \"album\",\r\n                    \"artist\",\r\n                    \"id\",\r\n                    \"information\",\r\n                    \"label\",\r\n                    \"year\"\r\n                  ],\r\n                  \"type\": \"best_fields\",\r\n                  \"operator\": \"OR\",\r\n                  \"lenient\": true\r\n                }\r\n              }\r\n            ]\r\n          }\r\n        }\r\n      ]\r\n    }\r\n  }\r\n}\r\n\nGet all albums where the artist is Elton John\nhttp://localhost:8080/search?partialFields=(artist:Elton)\nGET rolling500/_search\r\n{\r\n  \"query\": {\r\n    \"bool\": {\r\n      \"must\": [\r\n        {\r\n          \"match\": {\r\n            \"artist\": {\r\n              \"query\": \"Elton\",\r\n              \"operator\": \"AND\"\r\n            }\r\n          }\r\n        }\r\n      ]\r\n    }\r\n  }\r\n}\nGet all albums where Elton John is mentioned, but is not the artist\nhttp://localhost:8080/search?partialFields=(-artist:Elton)&q=Elton\nGET rolling500/_search\r\n{\r\n  \"query\": {\r\n    \"bool\": {\r\n      \"must\": [\r\n        {\r\n          \"bool\": {\r\n            \"must_not\": [\r\n              {\r\n                \"match\": {\r\n                  \"artist\": {\r\n                    \"query\": \"Elton\",\r\n                    \"operator\": \"OR\"\r\n                  }\r\n                }\r\n              }\r\n            ]\r\n          }\r\n        },\r\n        {\r\n          \"multi_match\": {\r\n            \"query\": \"Elton\",\r\n            \"fields\": [\r\n              \"album\",\r\n              \"artist\",\r\n              \"id\",\r\n              \"information\",\r\n              \"label\",\r\n              \"year\"\r\n            ],\r\n            \"type\": \"best_fields\",\r\n            \"operator\": \"AND\",\r\n            \"lenient\": true\r\n          }\r\n        }\r\n      ]\r\n    }\r\n  }\r\n}\nGet all albums created by Elton John between 1972 and 1974 for the label MCA\nhttp://localhost:8080/search?partialFields=(artist:Elton).(label:MCA)&q=inmeta:year:1972..1974\nGET rolling500/_search\r\n{\r\n  \"query\": {\r\n    \"bool\": {\r\n      \"must\": [\r\n        {\r\n          \"bool\": {\r\n            \"must\": [\r\n              {\r\n                \"match\": {\r\n                  \"artist\": {\r\n                    \"query\": \"Elton\",\r\n                    \"operator\": \"AND\"\r\n                  }\r\n                }\r\n              },\r\n              {\r\n                \"match\": {\r\n                  \"label\": {\r\n                    \"query\": \"MCA\",\r\n                    \"operator\": \"AND\"\r\n                  }\r\n                }\r\n              }\r\n            ]\r\n          }\r\n        },\r\n        {\r\n          \"bool\": {\r\n            \"must\": [\r\n              {\r\n                \"range\": {\r\n                  \"year\": {\r\n                    \"from\": \"1972\",\r\n                    \"to\": \"1974\",\r\n                    \"include_lower\": true,\r\n                    \"include_upper\": true\r\n                  }\r\n                }\r\n              }\r\n            ]\r\n          }\r\n        }\r\n      ]\r\n    }\r\n  }\r\n}\nConclusions\nAs you can see, the usage of ANTLR allows us to specify fairly complex DSL\u2019s without compromising readability. We\u2019ve cleanly separated the parsing of a user query from the actual construction of the resulting Elasticsearch query. All code is easily testable and makes little to no use of hard to understand regular expressions.\nA good addition would be to add some integration tests to your implementation, which you can learn more about\u00a0here. If you have any questions or comments, let me know!\n", "tags": [], "categories": ["Blog", "Search"]}
{"post_id": 1585, "title": "Release your hidden classes", "url": "https://www.luminis.eu/blog-en/development-en/release-your-hidden-classes/", "updated_at": "2020-11-16T17:07:11", "body": "In a lot of software that I\u2019ve seen, the class model could be better. One of the problems I often see is that a class contains other \u201chidden\u201d classes: a set of fields that really should be its own class.\nI will discuss where this originates from, and why it may cause problems.\nFirst, I will provide two common examples. The first is the address, a set of at least street, house number, and city, but often also containing ZIP code, state and country. In a lot of cases these will just be fields in another class such as customer.\nThe second example is a date range; a period of time between a start date and an end date, for instance to indicate when a given rule is valid. Often, a date range is a start and end date in the class that it applies to.\nWhat is the problem anyway?\nSo, why is it a problem if these are not represented as separate classes?\nFirst, it may lead to duplication of code. If you have several date ranges in your code, there will be several places where you are going to check if a given date is in that date range, or if another date range overlaps. Similarly, there may be validation and rendering methods for an address. All this code must also be tested, sou you get a lot of duplication in tests as well.\nSecond, it makes the code less readable in many ways. If-statements comparing four dates from two date ranges is hard to understand, and tricky to debug. Quick, what does this do?\n\nJava\r\n\r\nLocalDate dateRangeStart = ...;\r\nLocalDate dateRangeEnd = ...;\r\nLocalDate otherDateRangeStart = ...;\r\nLocalDate otherDateRangeStart = ...;\r\n\r\nif (dateRangeEnd.isAfter(otherDateRangeStart) && dateRangeStart.isBefore(otherDateRangeEnd)) {\r\n  ...\r\n}\nIf a class contains multiple addresses, your field names will become less readable:\nJava\r\n\r\nclass Customer {\r\n  private String firstName;\r\n  private String surname;\r\n  private String residenceAddressStreet;\r\n  private String residenceAddressHouseNumber;\r\n  private String residenceAddressCity;\r\n  private String postalAddressStreet;\r\n  private String postalAddressHouseNumber;\r\n  private String postalAddressCity;\r\n  ...\r\n}\r\n1\r\n2\r\n3\r\n4\r\n5\r\n6\r\n7\r\n8\r\n9\r\n10\r\n11\r\nclass Customer {\r\n  private String firstName;\r\n  private String surname;\r\n  private String residenceAddressStreet;\r\n  private String residenceAddressHouseNumber;\r\n  private String residenceAddressCity;\r\n  private String postalAddressStreet;\r\n  private String postalAddressHouseNumber;\r\n  private String postalAddressCity;\r\n  ...\r\n}\nThis may be the right moment to consider why classes are modelled this way. There may be a lot of different reasons, but one of the most important ones I see is that an application blindly copies an external data model. If a database is provided, and the database has a table \u201ccustomer\u201d with street, house number and city fields, there will be a class Customer with those same fields. Similarly, if the interface to a front end specifies the address as fields in a larger object instead of a separate object, that model may easily become the back end model as well.\nAnother reason may be that no suitable class is available. Java has great support for dates and durations, but not for a date range. So the easy way is to just add a start and end date to your class.\nType safety\nNow, for a last warning: upon discovering the code duplication and readability problems, one might be tempted to create static helper methods in utility classes, because this has less impact on the code as a whole. Apart from all the reasons why\u00a0utility classes should be avoided, I want to add one more specific to this case: since many fields of an address are Strings, any helper method will have a signature that does not check whether you pass the right fields:\n\nclass AddressHelper() {\r\n  public static boolean isAddressValid(String street, String houseNumber, String city) {\r\n    ...\r\n  }\r\n}\r\n\r\n// No compile error\r\nif (AddressHelper.isAddressValid(houseNumber, street, city) {\r\n  ...\r\n}\nEspecially error-prone because different countries have a different order in which they present the address fields: compare the Dutch street \u2013 house number \u2013 ZIP code \u2013 city to the US house number \u2013 street \u2013 city \u2013 ZIP code.\nUsing separate classes\nIf you represent an address as a separate class, it is far easier to provide it to an external service in a type-safe manner. It also makes your code more readable.\n\nclass Address {\r\n  private String street;\r\n  private String houseNumber;\r\n  private String city;\r\n}\r\n\r\nclass Customer {\r\n  private String firstName;\r\n  private String surname;\r\n  private Address residenceAddress;\r\n  private Address postalAddress;\r\n}\r\n\r\nclass AddressService() {\r\n  public boolean isAddressValid(Address address) {\r\n    ...\r\n  }\r\n}\r\n\r\nif (addressService.isAddressValid(customer.getResidenceAddress())) {\r\n  ...\r\n}\n\nclass DateRange {\r\n  private LocalDate start;\r\n  private LocalDate end;\r\n\r\n  ...\r\n\r\n  public boolean overlaps(DateRange other) {\r\n    return end.isAfter(other.getStart()) && start.isBefore(other.getEnd());\r\n  }\r\n}\r\n\r\nif (dateRange.overlaps(otherDateRange)) {\r\n  ...\r\n}\nIn closing\nRefactoring may incur significant cost to the development project, and the later you start the more it will cost. However, operational cost is significant over the lifetime of a software product, and solid, readable code will drive that cost down. Experience will make it easier to spot these problems earlier, but even if you run into the problems late in a project, it is worth considering refactoring your class model.\n", "tags": [], "categories": ["Blog", "Development"]}
{"post_id": 1219, "title": "Improve the performance of your AngularJS application", "url": "https://www.luminis.eu/blog-en/development-en/improve-the-performance-of-your-angularjs-application/", "updated_at": "2020-11-12T17:01:42", "body": "AngularJS is a huge framework and already has many performance enhancements built in. Despite that, it\u2019s very easy to write code in AngularJS that will slow down your application. To avoid this, it is important to understand what causes an AngularJS application to slow down and to be aware of trade-offs that are made in the development process. \nIn this post I describe things that I have learned from developing AngularJS applications that will hopefully enable you to build faster applications.\nFirst of all, a quick win to boost your applications performance. By default AngularJS attaches information about binding and scopes to DOM nodes and adds CSS classes to data-bound elements. The application doesn\u2019t need this information but it is added for debugging tools like Batarang to work properly. You can disable this by one simple rule in your application config.\n\napp.config(['$compileProvider', function ($compileProvider) { \r\n    $compileProvider.debugInfoEnabled(false); \r\n}]);\nIf you want to temporarily enable the debug information just open the debug console and call this method directly in the console:\n\nangular.reloadWithDebugInfo();\nWatchers in AngularJS\n\u201cWith great power comes great responsibility\u201d\nAngularJS provides the $watch API to observe changes in the application. To keep track of changes you need to set a watcher. Watchers are created on:\n\n$scope.$watch.\n{{ \u00a0}} type bindings.\nDirectives like ngShow, ngClass, ngBindHtml, etc.\nIsolated scope variables: scope: { foo: \u2019=\u2019 }.\nfilters.\n\nBasically: everything in AngularJS uses watchers.\nAngularJS uses dirty checking,\u00a0this means it goes through every watcher to check if they need to be updated. This is called the digest loop. If a watcher relies on another watcher, the digest loop is called again, to make sure that all of the changes have propagated. It will continue to do so, until all of the watchers have been updated.\nThe digest loop runs on:\n\nUser actions (ngClick, ngModel, ngChange).\n$http responses.\npromises are resolved.\nusing $timeout/$interval.\ncalling $scope.$apply() of $scope.$digest().\n\nBasically: the digest loop is called a lot.\nThe \u201cMagic\u201d of AngularJS works great but it is fairly easy to add so many watchers, that your app will slow down. Especially when watchers doing too much work. With this in mind we will go on and talk about some easy changes to make our applications faster!\nAvoid a deep watch\nBy default, the $watch() function only checks object reference equality. This means that within each $digest, AngularJS will check to see if the new and old values pointing to the same object. A normal watch is really fast and preferred from a performance point of view.\nWhat if you want to perform some action when a modification happened to an object or array? You can switch to a deep watch. This means that within each $digest, AngularJS will check the whole tree to see if the structure or values are changed. This could be very expensive.\nA collection watch could be a better solution. It works like a deep watch, except it only checks the first level of object\u2019s properties and checks for modifications to an array like adding, removing, replacing, and reordering items.\u00a0 Collection watches are used internally by Angular in the\u00a0ngRepeat\u00a0directive\nAnother effective alternative that I recommend is using one-way dataflow. This can be accomplished by making a copy of the watched object, modify the copy and then change the variable to point to the copy. Than a normal (shallow) watch will do the job.\nUse track by in ng-repeat\nngRepeat uses $watchCollection to detect changes in collections. When a change happens, ngRepeat makes the corresponding changes to the DOM.\nLet\u2019s say you want to refresh a flight list with new flight data. The obvious implementation for this refresh would be something like this:\n\u00a0\n\n$scope.flights = serverData.flights;\nThis would cause ngRepeat to remove all li elements of existing tasks and create them again, which might be expensive if we have a lot of flights or a complex li template. This happens because AngularJS adds a $$hashkey property to every flight object. When you replace the flights with the exact same flights from the server the $$hashkey is missing and ngRepeat won\u2019t know they represent the same elements.\nThe solution\nUse ngRepeat\u2019s track by clause. It allows you to specify your own key for ngRepeat to identify objects, instead of just generating a unique hashkey.\nThe solution wil look like:\n\u00a0\n\n\nNow ngRepeat reuse DOM elements for objects with the same id.\n\u00a0\nBind once\nUse bind once where possible. If bind once is used, Angular will wait for a value to stabilize after it\u2019s first series of digest cycles, and will use that value to render the DOM element. After that, Angular will remove the watcher and forget about that binding. This will minimize the watchers and thereby lightens the $digest loop.\nFrom AngularJS 1.3 there is the ::\u00a0notation to allow one time binding.\n\u00a0\n\u00a0\n\n{{::name}}\r\n\r\n\r\n\u00a0\u00a0\u00a0 \r\n\nUse ng-if instead of ng-show/ng-hide\nMaybe this is obvious but i think it\u2019s worth mentioning it. Use ng-if where possible. The ng-show/ng-hide directives just hide the element by setting it\u2019s style property \u201cdisplay\u201d to none. The element still exists in the DOM including every watcher inside it. Ng-if removes the element and watchers completely and generates them again when needed.\n", "tags": [], "categories": ["Blog", "Development"]}
{"post_id": 1217, "title": "Learning to rank search results [DevCon]", "url": "https://www.luminis.eu/blog-en/search-en/learning-to-rank-search-results/", "updated_at": "2020-11-13T09:40:25", "body": "\nWith advanced tools available for search like Solr and Elasticsearch, companies are embedding search in almost all their products and websites. Search is becoming mainstream. Therefore we can focus on teaching the search engine tricks to return more relevant results. One new trick is called \u201clearning to rank\u201d. Learning to rank uses a trained model to come up with a better ranking of the search results. During the presentation you\u2019ll learn what Learning To Rank is. To be able to understand the machine learning part, you get information about machine learning models, feature extraction and the training of models. You will also learn about when to apply learning to rank and of course you\u2019ll get an example to show how it works using elasticsearch and a learning to rank plugin. After this presentation you have learned how and why to combine Machine Learning and Search.\n", "tags": [], "categories": ["Blog", "Search"]}
{"post_id": 1213, "title": "Digital security for engineers [DevCon]", "url": "https://www.luminis.eu/no-category/digital-security-for-engineers/", "updated_at": "2020-11-10T10:47:36", "body": "As an engineer, you will probably have wondered \u201cwhy didn\u2019t they secure this just a little better\u201d after you\u2019re notified of yet another breach of your information. As professionals, we understand how security should work in our corner of the organization, but what about the rest? Outside IT, security and risk are often handled in a very different way. This talk will show you a peek of how the \u201creal world\u201d thinks about security. We\u2019ll touch upon the models that are used, how risk is quantified, and why smart companies sometimes take dumb risks. We\u2019ll show you the breadth of security, ranging from encryption, IDSs and firewalls to personnel security training, risks vs measures, legislation (like Europe\u2019s GDPR and Dutch law) and terms like \u201cresidual risk.\u201d To top it off, we will go into some of the main risks threatening organizations today. This talk will bring you a more complete picture of what \u201csecurity\u201d actually is, and helps you understand\u2014and provide arguments against\u2014decisions that don\u2019t make sense from your point of view.\n", "tags": [], "categories": ["No category"]}
{"post_id": 1418, "title": "Tracing API\u2019s: Combining Spring\u2019s Sleuth, Zipkin & ELK", "url": "https://www.luminis.eu/blog-en/development-en/tracing-apis-combining-springs-sleuth-zipkin-elk/", "updated_at": "2020-11-17T17:15:07", "body": " Tracing bugs, errors and the cause of lagging performance can be cumbersome, especially when functionality is distributed over multiple microservices.\nIn order to keep track of these issues, the usage of an ELK stack (or any similar sytem) is already a big step forward in creating a clear overview of all the processes of a service and finding these issues. Often bugs can be traced by using ELK far more easily than just using a plain log file \u2013 if even available. Optimization in this approach can be preferred, as for example you may want to see the trace logging for one specific event only.\nHere Sleuth comes in handy.\nDepicted from systems as Dapper, Zipkin (we will come to that one later) and HTrace, Sleuth can ensure that every request event that is initiated in a Spring application, will start a new trace. Every internal event (that will possibly trigger an event to another application) will initiate a new span and closes after the event is finished. As the request event is finished, the trace will be closed and a new one will start eventually when there is a new request event initiated.\nGetting Sleuth to work in your Spring-Boot application, is as easy as putting the dependency in your Maven\u2019s pom.xml, as described at https://cloud.spring.io/spring-cloud-sleuth/:\n\n\r\n    \r\n        \r\n            org.springframework.cloud\r\n            spring-cloud-sleuth\r\n            2.0.0.M5\r\n            pom\r\n            import\r\n        \r\n    \r\n\r\n\r\n    \r\n        org.springframework.cloud\r\n        spring-cloud-starter-sleuth\r\n    \r\n\r\n    \r\n        spring-milestones\r\n        Spring Milestones\r\n        https://repo.spring.io/libs-milestone\r\n        \r\n            false\r\n        \r\n    \r\n\nAs you integrate Sleuth in all the (micro-)services, it will automatically pass the trace ids to the next to be called service (if you are using RestTemplate; for other rest service implementations, you may need to add your own configuration), so that it inherits the value. This ensures that all logs behind one starting event can be grouped under one trace. By itself this already comes in handy, as you can filter in your logging by traceid and thus easily oversee the escalating flow of one event over the services.\n\n\nAll four services can be filtered by the identical traceid, but the services will have different spanids. (Only service A is set to debug level, other services to info.)\n\nWhen coming to investigating performance issues, we can already analyse the logs \u2013 with the help of the traceids in place and take a look at the timing of the services. But here Zipkin can come in handy. Zipkin is \u2013 as the authors say \u2013 a distributed tracing system. It runs as Spring Boot application and its default storage is in-memory, but you may as well choose one of the database options available to store your data, with help of their spark job. You can start using Zipkin locally by using the docker image available:\u00a0https://hub.docker.com/r/openzipkin/zipkin/.\u00a0The server is default available on http://localhost:9411/zipkin.\nImplementing Zipkin in your (micro-)services is also adding a dependency to your Maven structure:\n\n\r\n    org.springframework.cloud\r\n    spring-cloud-starter-zipkin\r\n\r\n\nOf course, you may want to change configuration such as the base-url and those properties are available by spring.zipkin.* .\nAs Sleuth already opens and closes spans by events that are opened and closed (namely, requests done with RestTemplate), Zipkin can be integrated to visualize those spans correctly.\nEach trace can be found separately and you can also search per available service.\nSo, let\u2019s look at an example call done to service a:\n\n\nSearching in service a to find our trace.\n\nWhen you have found the trace of interest, you can click on it and an overview of the underlying spans is shown:\n\n\nView of same call as previous.\n\nAs seen in this example, we see the trace with spans from service a, to b to d & a to c. Also, we can see the time it took to perform each call (or span).\nWhen a trace has an error, you may view the trace and find out it is red instead of blue.\n\n\nA trace that has gone wrong.\u00a0Inside this trace, you can still find the called services with their spans and find out were it could have went wrong.\n\n\n\nSpans of the errored trace.\nYou may click on a span and a popup with extra information will open. When you click again on \u201cmore info\u201d, you will find the traceid & spanid. This traceid can be used to look into the logs in ELK again to find the exception.\n\n\n\nExtra information of a span.\n\n\n\nFinding the error in the logs. Expanding the message will give the information needed to trace the exception that is thrown.\n\nOf course, finding bugs or exceptions can be perfectly optimized without the use of Zipkin and as suggested by\u00a0https://peter.bourgon.org/blog/2016/02/07/logging-v-instrumentation.html, it is important to consider the difference between logging and instrumentation tooling.\nBut, when it comes to creating an overview of multiple called microservices over time and tracing any timing issues, Zipkin can really increase the ability to gain insight of performance issues.\u00a0So let\u2019s look into an example:\n\n\nA call to service a is taking more than 30 seconds.\n\nThere are complaints that a call to some endpoint of service a is taking around 30 seconds \u2013 which is unexpected.\nAs seen in the logs, we can already confirm the call is taking from 18:16:05 to 18:16:35.\nThe cause cannot be fully confirmed by looking at the logs, although an estimated guess can be made.\nIn Zipkin, all we need to do is select service a (the root service), filter the traces by datetime and endpoint call and the culprit can be viewed in an eyeblink:\n\n\nThe culprit.\u00a0Here we can see which value Zipkin in combination with Sleuth may add to the use of microservices.\n\nOne last thing to add, is something we need to customize: the possibility to group by conversation, with several follow-up calls that need to be grouped, as they form a cascade of events. When a user wants to start such a conversation and wants to trace that whole conversation, it will be convenient if the user can use a level higher id than traceid \u2013 which will only return one call of the conversation.\n\nWith Sleuth we can add baggage items & tags:\nBaggage items are items you can add to a trace and will be propagated over the trace context; thus also passed over the services. But, they are not passed to Zipkin.\nTags, on the contrary, are normally used for spans to tag them and are passed to Zipkin. By adding a webfilter (or any other interceptor you prefer), we can create our own id system, which can provide a grouping over multiple calls \u2013 a conversation:\n\n@Configuration\r\npublic class SleuthConversationConfiguration {\r\n \r\n  public static final String TAG_NAME = \"X-B3-CONVID\";\r\n \r\n  @Bean\r\n  @Order(TraceFilter.ORDER + 5)\r\n  public GenericFilterBean customTraceFilter(final Tracer tracer) {\r\n    return new GenericFilterBean() {\r\n \r\n      @Override\r\n      public void doFilter(ServletRequest servletRequest, ServletResponse servletResponse,\r\n          FilterChain filterChain) throws IOException, ServletException {\r\n        if (servletRequest instanceof RequestFacade) {\r\n          //Get header conversation id IF available -> overrules any other one\r\n          String existingConversationId = ((RequestFacade) servletRequest)\r\n              .getHeader(TAG_NAME);\r\n          Span span = tracer.getCurrentSpan();\r\n          if (existingConversationId != null && span.getBaggage().get(TAG_NAME) == null) {\r\n            span.setBaggageItem(TAG_NAME, existingConversationId);\r\n          //IF the conversation id is not available in header of request, we try to get it from baggage item\r\n          } else if (existingConversationId == null) {\r\n            existingConversationId = span.getBaggage().get(TAG_NAME);\r\n            //If not available, generate one for us\r\n            if (existingConversationId == null) {\r\n              existingConversationId = UUID.randomUUID().toString();\r\n              span.setBaggageItem(TAG_NAME, existingConversationId);\r\n            }\r\n          }\r\n          //Add conversation id as tag in order to make it available in Zipkin\r\n          tracer.addTag(TAG_NAME, existingConversationId);\r\n          //Add to MDC in order to make it visible in logging\r\n          MDC.put(TAG_NAME, existingConversationId);\r\n          //Will continue span\r\n          tracer.continueSpan(span);\r\n          //Will ensure the conversation id is also available in the response\r\n          HttpServletResponse response = (HttpServletResponse) servletResponse;\r\n          if (response != null) {\r\n            response.addHeader(TAG_NAME, existingConversationId);\r\n            response.addHeader(\"Access-Control-Expose-Headers\", TAG_NAME);\r\n          }\r\n        }\r\n        filterChain.doFilter(servletRequest, servletResponse);\r\n      }\r\n    };\r\n  }\r\n}\r\n\nThe part with the headers is optional; for service a it is convenient, as the user can pass its own id to the service, in order to ensure that the conversation id will take its place in all the calls. But, for service b, c and d, Sleuth will already ensure that the id is passed as baggage item. Therefore, the part in which we try to get it from the header is unnecessary, and the code could be as follows:\n@Configuration\r\npublic class SleuthConversationConfiguration {\r\n  public static final String TAG_NAME = \"X-B3-CONVID\";\r\n \r\n  @Bean\r\n  @Order(TraceFilter.ORDER + 5)\r\n  public GenericFilterBean customTraceFilter(final Tracer tracer) {\r\n    return new GenericFilterBean() {\r\n \r\n      @Override\r\n      public void doFilter(ServletRequest servletRequest, ServletResponse servletResponse,\r\n          FilterChain filterChain) throws IOException, ServletException {\r\n        Span span = tracer.getCurrentSpan();\r\n        String existingConversationId = span.getBaggage().get(TAG_NAME.toLowerCase());\r\n        if(existingConversationId == null){\r\n          existingConversationId = UUID.randomUUID().toString();\r\n          span.setBaggageItem(TAG_NAME, existingConversationId);\r\n        }\r\n        tracer.addTag(TAG_NAME, existingConversationId);\r\n        MDC.put(TAG_NAME, existingConversationId);\r\n        tracer.continueSpan(span);\r\n        filterChain.doFilter(servletRequest, servletResponse);\r\n      }\r\n    };\r\n  }\r\n}\nFurther, you need to ensure that you add this extra field in your logback configuration (or any log configuration you may use) of all the services that use it by using %X{X-B3-CONVID:-} as property value; else you will not see it in your logs!\nIf you configured all correctly and do some calls with a header \u201cX-B3-CONVID: OUR-UNIQUE-IDENTIFIER\u201d, you may see the following in the logs:\n\n\nUsing our own unique identifier will provide a way to get all logging of the conversation \u2013 thus, of multiple calls that are given in a cascade of calls.\n\nIn Zipkin, we can provide the name of the tag we created to filter all the traces.\n\nThis way, all traces will be shown and we can look into them if we want to gather more info of their performance and/or error handling.\nHence, Sleuth & Zipkin are helpful tools to create a more concrete overview of the distribution of calls over (micro-)services and can enhance the tracing of bugs or performance issues. With some additional tweaking, we can even create overcoupling ids to trace conversations. It will not replace ELK or similar services, but will be a helpful tool for monitoring your applications.\nUseful resources:\n\nhttps://hub.docker.com/r/openzipkin/zipkin/\nhttps://peter.bourgon.org/blog/2016/02/07/logging-v-instrumentation.html\nhttps://cloud.spring.io/spring-cloud-sleuth/\nhttps://spring.io/blog/2016/02/15/distributed-tracing-with-spring-cloud-sleuth-and-spring-cloud-zipkin\n\n", "tags": [], "categories": ["Blog", "Development"]}
{"post_id": 1176, "title": "Java Modules", "url": "https://www.luminis.eu/blog-en/development-en/java-modules/", "updated_at": "2020-11-13T09:09:15", "body": "After reading the Java 9 modularity book by Sander Mak and Paul Bakker (excellent book btw), I was wondering how many Java libraries already had been migrated to Java 9 modules. I couldn\u2019t find much about it, except for Stephen Colebourne\u2019s Module names for Java SE 9.\nEven though it\u2019s more focussed on module naming, it contains a markdown table that mentions a number of migrated libraries (40, with 30 from one project). But it didn\u2019t completely answer my question, so I wondered if I shouldn\u2019t do some research myself and (\u201chow hard can it be?\u201d) started writing a script that gets jars from maven central and checks their Java 9 module status. Because I didn\u2019t fancy the idea of entering lots of maven coordinates of libraries I knew, I started with a few well-known ones and extracted the dependencies from the pom file to discover more. It took some coding effort (which made me realise again that when something isn\u2019t hard, that doesn\u2019t necessarily mean it\u2019s not a lot of work ;-)), but it worked out pretty well. Even though I only started with a list of 15 to 20 libraries, the script resulted in more than 4000 (!) libraries being checked.\n59 Java Modules\nThe results were somewhat disappointing: i found only 59 Java modules. On a total of 4527, that\u2019s only 1,3%! It seems we must conclude that adoption of the Java 9 Module system is still very limited.\nAutomatic modules\nBut maybe that is a premature or unfair conclusion. First of all, in this list there are 534 that have explicitly set the \u201cAutomatic Module Name\u201d in their manifest. So you might argue that at least the library maintainers put some effort in making their library ready for use in a modular Java application. Taking these into account leads to an adaption figure of 13% \u2013 10 times better then my first conclusion.\nDead libraries\nNext, even though the script always obtains the latest version of a library, there is of course a number of libraries that is too old, that is not maintained anymore, for example because they are superseded by something new (jboss -> wildfly) or just got new maven coordinates (tomcat -> org.apache.tomcat). It wouldn\u2019t be fair to count them as \u201cstill not migrated\u201d, as they probably never will. I should only check libraries that are \u201cnew enough\u201d; using the date that Java 9 was released seems the logical thing to do.\nFiltering my list of libraries on being updated after Sept 22, 2017, the figures are:\n\u2013 modules: 53\n\u2013 defined automatic modulde name: 530\n\u2013 total number of libraries: 2756\nSo 1,9% is a real Java module, and 21% is \u201cmodule ready\u201d. The latter is much better than the 13% we got before, but the 1,9% for real modules is still not very impressive.\n(B.t.w., did you notice the number of modules is less then before? Apparently, some libraries where already migrated before Java 9 was released; I guess there are people who are enthusiastic about the Java 9 module system ;-))\nLibraries or artifacts\nThe last thing to consider is that while I was talking about libraries, I should have used the term \u201cartifacts\u201d. I don\u2019t think there is a formal definition of \u201clibrary\u201d, but I think most people would agree with me that a library can (and often will) consist of several artifacts. So an interesting question is what the figures will be when we take this into account.\nUnfortunately, there is no easy way to regcognize which artifacts belong to which library. But what we can do, is measure against maven group id\u2019s; it\u2019s likely that artifacts within one group somehow form a project and are update / migrated together. Counting maven group-id\u2019s, the figures are:\n\u2013 modules: 22\n\u2013 defined automatic modulde name: 130\n\u2013 total number of libraries: 684\nresulting in: 3,2% real modules, 22% \u201cmodule ready\u201d.\nEven though the percentage of real modules has increased with 50%, it\u2019s still not much, is it?\nModularity is key\nMay be I should have known better after I\u2019d discovered\u00a0Stephen Colebourne\u2019s list, but I think the results are disappointing. And it makes me a bit sad, because I think that creating modular software is important in order for it to be and stay maintainable. At Luminis we have always believed (and still do) that OSGi is an excellent framework for writing modular code and I think that Java 9 modules is a nice addition in the field of modularity that fits nicely for systems that don\u2019t require the dynamic nature that OSGi supports so nicely. I wonder whether the low adoption rate indicates indifference, or that the library maintainers are just to busy with other priorities.\nBut let\u2019s end positive: there are quite some library maintainers that did put effort in support Java 9 modules, and we should appreciate that. Let\u2019s send them some kudos: pick some from the list and tweet that you\u2019re glad they made it a module!\n", "tags": [], "categories": ["Blog", "Development"]}
{"post_id": 1156, "title": "The Scrum Pendulum", "url": "https://www.luminis.eu/blog-en/development-en/the-scrum-pendulum/", "updated_at": "2020-11-17T17:10:03", "body": " In product development advanced tooling and so-called \u2018proven\u2019 development methods can sometimes cause more problems than they solve. For instance when they distract the development team from what they should really be focussing on: the end result, the product itself.\nAlthough Scrum provides a healthy framework that minimises risk and maximises agility of the development process, I\u2019ve seen Scrum teams completely go astray at the expense of the product. In this blog I will outline a recurring pattern I have noticed in Scrum development teams and I will share some of my ideas on how teams can keep the right focus within Scrum.\nWhen I think about product development and Scrum, I see that the product and the development team often live in separate worlds. The world of the product is ruled by business and user goals, while the world of the development team is ruled by sprint goals. These are not the same kind of goals. For instance, a user goal could be to easily order a coffee-to-go, while a sprint goal could be to implement single sign-on. Like I said: different worlds.\nSo the development process goes something like this: During a sprint new functionality is added to the product and gets released to the market after it\u2019s considered done. This new functionality triggers some kind of response of the business and/or the users. This response (or the lack of it) might then be translated back into new product requirements that are fed back to the development team. During this whole process the development team is often \u2018trapped\u2019 inside their own sprint bubble. They are lulled to sleep by what I call \u2018the Scrum Pendulum\u2019; the constant rhythm of the sprints and the hyper-focus on the sprint goals. Most of the time they are unaware of the effect of their work on the actual product, unaware also of the effect the product has on the business or the users. The Scrum Pendulum creates a \u2018detached\u2019 development team, that might very well be meeting its sprint goals, but has no idea whether it is moving the product in the right direction.\n\nThis means you could also regard the PO as a SPOF: a Single Point Of Failure.\nMost of the time the only link between the development team and the product is the product owner (PO). The PO has a pretty crucial role in the development process; she (or he) is responsible for translating business and user goals to product requirements and also for deciding whether new functionality that has been developed is fit te be released to the market. So you can imagine that if the PO fails, the whole process will fail. This means you could also regard the PO as a SPOF: a Single Point Of Failure. And that is something I think you need to avoid in product development.\nAs a team you should define what \u2018success\u2019 means.\nSo what can a development team do to prevent the Scum Pendulum from happening? I think it is key to take a shared responsibility for the end result, the product. Even if this was not explicitly asked by your PO, manager or client. As a professional engineer, designer or tester I believe you have a responsibility that goes beyond the development team. As a team you should define what \u2018success\u2019 means: when do you think your team has delivered a good job? You might not all agree on this, which is even more reason to discuss this with your team members and create a common understanding of \u2018success\u2019. For example, for a team \u2018success\u2019 might mean a minimum of 4 stars in the App Store rating (if it is an app you are developing). It could also be something like acceptable response times during heavy traffic or a good overall product usability. It doesn\u2019t matter what you as a team define as \u2018success\u2019, the point is that you start thinking about product quality beyond the sprint boundaries.\nDefining success is a useful exercise in itself, but it becomes even more valuable when the development team measures it\u2019s success using their own defined standard. This doesn\u2019t need to take a lot of time or be very complex. Checking the app store rating and comments will often give a lot of insight. Also, a quick-and-dirty user test is really simple to conduct and doesn\u2019t need a lot of preparation. The results might not be 100% representative, they do give some level of insight. Which is a lot more to go on then when the development team would forever remain inside their sprint bubble. An important last detail is of course to use the insights to learn and improve the work inside the sprints, and everybody will benefit.\nDon\u2019t let Scrum (or any other development method or tool) replace common sense.\nThis team\u2019s definition and constant evaluation of it\u2019s own success creates an \u2018outer loop\u2019 around the Scrum process that provides a backup for failing (or even non-existing) product owners. But more importantly, it prevents the team from getting too detached from the product and the world of the business and the users. Don\u2019t let Scrum (or any other method or tool) replace common sense. And don\u2019t hide behind your sprint commitment; product quality and success are a shared responsibility.\nWant to learn more about how to avoid the pitfalls of tooling and methods in product development? I\u2019ll be sharing my thoughts on this at the DevCon 2018 (April 12, Ede) and CodeMotion Amsterdam 2018 (May 8, Amsterdam) conferences. Or sign up for the training Product Thinking at the Luminis Academy.\n", "tags": [], "categories": ["Blog", "Development"]}
{"post_id": 1367, "title": "Being relevant with Solr", "url": "https://www.luminis.eu/blog-en/search-en/being-relevant-with-solr/", "updated_at": "2020-11-13T11:43:41", "body": "Ok, I have to admit, I am not (yet) a Solr expert. However, I have been working now with elasticsearch for years and the fundamentals of obtaining relevant results for Solr and Elasticsearch are still the same. For a customer, I am working on fine-tuning their search results using an outdated Solr version. They are using Solr 4.8.1, yes an upgrade is planned in the future. Still, they want to try to improve their search results. Using my search knowledge I started getting into Solr and, I liked what I saw. I saw a query that had matching algorithms, filters to limit documents that need to be considered and on top of that lots of boosting options. So many boosting options that I had to experiment a lot to get to the right results.\nIn this blog post, I am going to explain what I did with Solr coming from an elasticsearch background. I do not intend to create a complete guideline on how to use Solr. I\u2019ll focus on the bits that surprised me and the stuff that deals with tuning the\u00a0edismax\u00a0type of query.\nA little bit of context\nImagine you a running a successful eCommerce website. Or even better, you have created a superb online shopping experience. With an excellent marketing strategy, you are generating lots of traffic to your website. But, yes there is a but, sales are not as expected. Maybe it is even a bit disappointing. You start evaluating the visits to your site using analytics. When going through your search analytics you notice that most of the executed searches do not result in a click and therefore not into sales. It looks like the results shown to your visitors for their searches are far from optimal. So we need better search results.\nGetting to know your data\nBefore we start writing queries, we need to have an idea about our data. We need to create inverted indexes per field, or for combinations of fields, using different analyzers. We need to be able to search for parts of sentences, but also be able to boost matching sentences. We want to be able to find matches for combinations of fields. Imagine you sell books, you want people to be able to look for the latest book by Dan Brown called Origin. Users might enter a search query like\u00a0Dan Brown Origin. If might become a challenge if you have structured data like: https://amsterdam.luminis.eu/2018/12/01/being-relevant-with-solr/\n\u00a0\n{\r\n    \"author\": \"Dan Brown\",\r\n    \"title\": \"Origin\"\r\n}\nHow would you do it if people want to have the latest Dan Brown? What if you want to help people choose by using the popularity of books using ratings or sales. Or how to act if people want to look at all the books in the Harry Potter series. Of course, we need to have the right data to be able to facilitate our visitors with these new requirements. We also need a media_type field later on. With the media type, we can filter on all eBooks for example. So the data becomes something like the following block.\n{\r\n    \"id\": \"9781400079162\",\r\n    \"author\": \"Dan Brown\",\r\n    \"title\": \"Origin\",\r\n    \"release_date\": \"2017-10-08T00:00:00Z\",\r\n    \"rating\": 4.5,\r\n    \"sales_pastyear\": 239,\r\n    \"media_type\": \"ebook\"\r\n}\nRanking requirements\nBased on analysis and domain knowledge we have the following thoughts translated into requirements for the ranking of search results:\n\nRecent books are more important than older books\nBooks with a higher rating are more important than lower rated books\nUnrated books are more important than low rated books\nBooks that are sold more often in the past year are more important than unsold books\nNormal text matching rules should be applied\n\nMapping data to our index\nIn Solr, you create a schema.xml to map the expected data to specific types. You can also use the copy_to functionality to create new fields that are analyzed differently or are a combination of the provided fields. An example could be to add a field that contains all searchable other fields. In our case, we could create a field containing the author as well as the title. This field is analyzed in the most optimal way to do matching. We add a tokenizer, but also filters for lowercasing, stop words, diacritics, and compound words. We also have fields that are more for boosting using phrases and numbers or dates. We want fields like title and author to support phrases but also full matches. With this, we got a few extra search requirements\n\nDocuments of which the exact author or title matches the query should be more important\nDocuments of which the title contains the words in the query in the same order are more important\n\nWith these rules, we can start to create a query and apply our matching and boosting requirements.\nThe Query\nCreating the query was my biggest surprise when going for Solr. Another configuration mechanism is the Solrconfig.xml. This file configures the Solr node. It gives you the option to create your own endpoint for a query that comes with lots of defaults. One thing we can do for instance is to create an endpoint that automatically filters for only ebooks. We can call this endpoint to search for ebooks only. Below you\u2019ll find a sample of the config that does just this.\n\r\n       json \r\n       media_type:ebook \r\n       combined_author_title \nFor our own query, we\u2019ll need other options that Solr provides. This is called the edismax query. This comes by default with options to boost your results using phrases, but also for boosting using ratings, release dates, etc. Below an image giving you an idea of what the query should do.  Next, I\u2019ll show you how this translates into the Solr configuration\n\u00a0\n\r\n       json\r\n       2\r\n       3\r\n       author^4 title^2\r\n       author^4 title^2\r\n       author^4 title^2\r\n       author_full^5 title_full^5\r\n       product(div(def(rating,4),4),recip(ms(NOW/DAY,releasedate),3.16e-11,1,1),log(product(5,sales_pastyear)))\r\n       combined_author_title\r\n       edismax\r\n       false\r\n\nI am not going over all the different parameters. For multi-term queries we use phrases. These are configured with pf, pf2, and pf3. Also, mm is used for multi-term queries. This has to do with the number of terms that have to match. So if you use three terms, they all have to match. The edismax query also supports using AND/OR when you need more control over what terms to match. With lowercaseOperators we prevent that and/or in lowercase are also used to create your own boolean query. With respect to boosting there is the bq, these numbers are added to the score. With the field boost, we do a multiplication. Look also at the diagram. Also notice the bq has text related scores, while the boost has numeric scores. That is about it for now. I think it is good to look at the differences between Solr en Elasticsearch. I like the idea of creating a query with Solr. Of course, you can do the same with Elasticsearch. The json API for creating a query is really flexible, but you have to create the constructs used in Solr yourself.\n", "tags": [], "categories": ["Blog", "Search"]}
{"post_id": 1341, "title": "How to Install PySpark and Apache Spark on MacOS", "url": "https://www.luminis.eu/blog-en/development-en/how-to-install-pyspark-and-apache-spark-on-macos/", "updated_at": "2020-11-11T15:08:53", "body": "Here is an easy Step by Step guide to installing PySpark and Apache Spark on MacOS.\nStep 1: Get Homebrew\nHomebrew makes installing applications and languages on a Mac OS a lot easier. You can get Homebrew by following the instructions on its website. In short you can install Homebrew in the terminal using this command:\n\u00a0\n/usr/bin/ruby -e \"$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/master/install)\"\nStep 2: Installing xcode-select\nXcode is a large suite of software development tools and libraries from Apple. In order to install Java, and Spark through the command line we will probably need to install xcode-select. Use the blow command in your terminal to install Xcode-select: xcode-select \u2013install You usually get a prompt that looks something like this to go further with installation:  You need to click \u201cinstall\u201d to go further with the installation.\nStep 3: DO NOT use Homebrew to install Java!\nThe latest version of Java (at time of writing this article), is Java 10. And Apache spark has not officially supported Java 10! Homebrew will install the latest version of Java and that imposes many issues!\nTo install Java 8, please go to the official website: https://www.oracle.com/technetwork/java/javase/downloads/jdk8-downloads-2133151.html Then From \u201cJava SE Development Kit 8u191\u201d Choose:\nMac OS X x64 245.92 MB jdk-8u191-macosx-x64.dmg\nTo download Java. Once Java is downloaded please go ahead and install it locally.\nStep 3: Use Homebrew to install Apache Spark\nTo do so, please go to your terminal and type: brew install apache-spark Homebrew will now download and install Apache Spark, it may take some time depending on your internet connection. You can check the version of spark using the below command in your terminal: pyspark \u2013version You should then see some stuff like below: \nStep 4: Install PySpark and FindSpark in Python\nTo be able to use PyPark locally on your machine you need to install findspark and pyspark If you use anaconda use the below commands: \u00a0\n\u00a0\n#Find Spark Option 1: \r\n     conda install -c conda-forge findspark \r\n#Find Spark Option 2: \r\n     conda install -c conda-forge/label/gcc7 findspark \r\n#PySpark: \r\n     conda install -c conda-forge pyspark\r\n\r\nIf you use regular python use pip install as: \r\n     pip install findspark \r\n     pip install pyspark\nStep 5: Your first code in Python\nAfter the installation is completed you can write your first helloworld script:\n\u00a0\n    import findspark from pyspark \r\n    import SparkContext from pyspark.sql \r\n    import SparkSession \r\n    findspark.init() \r\n    sc = SparkContext(appName=\"MyFirstApp\") \r\n    spark = SparkSession(sc) \r\n    print(\"Hello World!\") \r\n    sc.close() #closing the spark session\n", "tags": [], "categories": ["Blog", "Development"]}
{"post_id": 1338, "title": "How to bring your python machine learning model in production using RESTful APIs", "url": "https://www.luminis.eu/blog-en/data-en/how-to-bring-your-python-machine-learning-model-in-production-using-restful-apis/", "updated_at": "2020-11-12T10:45:32", "body": "In recent years there is much attention to bringing machine learning models in production whereas up to a few years ago the results of machine learning came into slides or some dashboards. Bringing machine learning in production is important to integrate the outputs of machine learning with other systems.\nWhat does \u201cbring in production\u201d mean?\nTo bring a machine learning in production means to run the machine learning model more often and integrate and use the model\u2019s output in other systems.\nThere are some ways that you can bring your machine learning models in production, such as:\n\nTo build a web-service around it and use it in real time (API calls, microservice architecture)\nSchedule your code to be run regularly (such as Oozie and Airflow)\nStream analytics (such as Spark Streams) for lambda/kappa architect\n\nThe focus of this article is to discuss the first way of going on production. This method is usually used in web-based environments and microservices architect.\nPython and modeling\nIn this article, we build up a sample machine learning model for Online Shoppers\u2019 Purchasing Intention Dataset Data Set, available and discussed in https://bit.ly/2UnSeRX.\nIn the below you can find the code for the data preparation and modeling:\n\nimport pandas as pd # to use dataframes and etc. \r\nimport numpy as np #for arithmatic operationms\r\nfrom time import strptime #to convert month abbrivations to numeric values\r\nfrom sklearn.model_selection import train_test_split #to split up the samples\r\nfrom sklearn.tree import DecisionTreeRegressor #ression tree model\r\nfrom sklearn.metrics import confusion_matrix # to check the confusion matrix and evaluate the accuracy\r\n\r\n#reading the data\r\ndataset=pd.read_csv(\".../online_shoppers_intention.csv\")\r\n\r\n#preparing for split\r\ndf=dataset.drop(\"Revenue\",axis=1)\r\ny=dataset[\"Revenue\"].map(lambda x: int(x))\r\nX_train, X_test, y_train, y_test = train_test_split(df, y, test_size=0.2)\r\n\r\n#making a copy (to be able to change the values)\r\nX_train=X_train.copy()\r\nX_test=X_test.copy()\r\n\r\n#data prep phase\r\ndef dataPrep(localData):\r\n    #The problem is \"June\"is the full month name and the rest abbriviation --> we turn all in abbr.\r\n    localData[\"Month\"]= localData[\"Month\"].map(lambda x: str.replace(x,\"June\",\"Jun\"))\r\n    # Our model doesn't ingest text, we transform it to int\r\n    localData[\"Month\"]= localData[\"Month\"].map(lambda x: strptime(x,\"%b\").tm_mon)\r\n    # The weeknd should also be turned into int\r\n    localData[\"Weekend\"]= localData[\"Weekend\"].map(lambda x: int(x))\r\n    # turning string to int\r\n    localData[\"VisitorType\"] = localData[\"VisitorType\"].astype('category').cat.codes\r\n    return localData\r\n\r\n#Sending the data through data prep phase\r\nX_train=dataPrep(X_train)\r\nX_test=dataPrep(X_test)\r\n#define the regression tree\r\nregr_tree = DecisionTreeRegressor(max_depth=200)\r\n#fiting the tree with the training data\r\nregr_tree.fit(X_train, y_train)\r\n# running the predictions\r\npred=regr_tree.predict(X_test)\r\n# looking at the confusion matrix\r\nconfusion_matrix(y_test,pred)\nSave the model\nThe next step which does not appear in data scientists workflow regularly is to save the model on hard-drive. This step is necessary if you bring your python code in production.\nIn below you can see how \u201cjoblib\u201d can be of assistant to do this:\n\nfrom sklearn.externals import joblib\r\njoblib.dump(regr_tree, '.../model3.pkl')\nBuild your Flask back-end\nIf you are not familiar with building back-end programs and RESTful APIs, I highly recommend reading https://bit.ly/2AVTwxW and other related materials. But in short Web Services, and RESTful APIs are servers provide functions (on the server). The application can remotely call those function and get the outputs back. In our example, we call our machine learning model from anywhere through internet and TCPIP protocol. Once the model is called with the data, the result of classification is back to the client or the computer which has already called the machine learning model.\nDiscussing details about web-services and web APIs are beyond the scope of this article but you can find many interesting articles on this by some internet search.\nIn below we use Flask to build the webservice around the machine learning model.\n\nfrom flask import Flask, request\r\nfrom flask_restful import Resource, Api\r\nfrom sklearn.externals import joblib\r\nimport pandas as pd\r\n\r\n\r\napp = Flask(__name__)\r\napi = Api(app)\r\n\r\n\r\n\r\nclass Classify(Resource):\r\n    def get(self): # get is the right http verb because it caches the outputs and is faster in general\r\n        data = request.get_json() # greading the data\r\n        data1 = pd.DataFrame.from_dict(data,orient='index') # converting data into  DataFrame (as our technique does not ingest json)\r\n        data1=data1.transpose() # once Json is converted to DataFrame is not columnar, we need to convert it to columnar\r\n        model = joblib.load('../model3.pkl') # loading the model from disk\r\n        result = list(model.predict(data1)) # conversion to list because numpy.ndarray cannot be jsonified\r\n        return result # returning the result of classification \r\n\r\n\r\n\r\napi.add_resource(Classify, '/classify')\r\n\r\napp.run(port=5001)\r\n\nTest your API\nYou can use various techniques to test if the back-end works. I use Postman software to test if the API is working.\nYou need to consider that we made a GET request in our Flask application. The motivation behind choosing GET request is the ability of the webservers to cash the results helping with the speed of the webservice.\nAnother consideration is we send the data in JSON format (in the format after data preparation phase) for the call and the results are back also in JSON.\n\n{\r\n    \"Administrative\":0,\r\n    \"Administrative_Duration\": 0.0,\r\n    \"Informational\": 0,\r\n    \"Informational_Duration\": 0.0,\r\n    \"ProductRelated\": 1,\r\n    \"ProductRelated_Duration\": 0.0,\r\n    \"BounceRates\": 0.2,\r\n    \"ExitRates\": 0.2,\r\n    \"PageValues\": 0.0,\r\n    \"SpecialDay\": 0.0,\r\n    \"Month\": 5,\r\n    \"OperatingSystems\": 2,\r\n    \"Browser\": 10,\r\n    \"Region\": 5,\r\n    \"TrafficType\": 1,\r\n    \"VisitorType\": 87093223,\r\n    \"Weekend\":0\r\n}\nMicroservices architect:\nI personally like to bring machine learning in production using RESTful APIs and the motivation behind it is because of microservices architect. The microservices architect lets developers to build up loosely coupled services and enables continuous delivery and deployment.\nScaling up\nTo scale up your webservice there are many choices of which I would recommend load balancing using Kubernetes\n", "tags": [], "categories": ["Blog", "Data"]}
{"post_id": 1366, "title": "Monitoring Spring Boot applications with Prometheus and Grafana", "url": "https://www.luminis.eu/blog-en/development-en/monitoring-spring-boot-applications-with-prometheus-and-grafana/", "updated_at": "2020-11-11T17:06:06", "body": "At my current project, we\u2019ve been building three different applications. All three applications are based on Spring Boot, but have very different workloads. They\u2019ve all reached their way to the production environment and have been running steadily for quite some time now. We do regular (weekly basis) deployments of our applications to production with bug fixes, new features, and technical improvements. The organisation has a traditional infrastructure workflow in the sense that deployments to the VM instances on acceptance and production happen via the (remote) hosting provider.\nThe hosting provider is responsible for the uptime of the applications and therefore they keep an eye on system metrics through the usage of their own monitoring system. As a team, we are able to look in the system, but it doesn\u2019t say much about the internals of our application. In the past, we\u2019ve asked to add some additional metrics to their system, but the system isn\u2019t that easy to configure with additional metrics. To us as a team runtime statistics about our applications and the impact our changes have on the overall health are crucial to understanding the impact of our work. The rest of this post will give a short description of our journey and the reasons why we chose the resulting setup.\nSpring Boot Actuator and Micrometer\nIf you\u2019ve used Spring Boot before you\u2019ve probably heard of\u00a0Spring Boot Actuator. Actuator is a set of features that help you monitor and manage your application when it moves away from your local development environment and onto a test, staging or production environment. It helps expose operational information about the running application \u2013 health, metrics, audit entries, scheduled task, env settings, etc. You can query the information via either several HTTP endpoints or JMX beans. Being able to view the information is useful, but it\u2019s hard to spot trends or see the behaviour over a period of time.\nWhen we recently upgraded our projects to Spring Boot 2 my team was pretty excited that we were able to start using\u00a0micrometer\u00a0a (new) instrumentation library powering the delivery of application metrics. Micrometer is now the default metrics library in Spring Boot 2 and it doesn\u2019t just give you metrics from your Spring application, but can also deliver JVM metrics (garbage collection and memory pools, etc) and also metrics from the application container. Micrometer has several different libraries that can be included to ship metrics to different backends and has support for Prometheus, Netflix Atlas, CloudWatch, Datadog, Graphite, Ganglia, JMX, Influx/Telegraf, New Relic, StatsD, SignalFx, and Wavefront.\nBecause we didn\u2019t have a lot of control over the way our applications were deployed we looked at the several different backends supported by micrometer. Most of the above backends work by pushing data out to a remote (cloud) service. Since the organisation we work for doesn\u2019t allow us to push this \u2018sensitive\u2019 data to a remote party we looked at self-hosted solutions. We did a quick scan and started with looking into Prometheus (and Grafana) and soon learned that it was really easy to get a monitoring system up and we had a running system within an hour.\nTo be able to use Spring Boot Actuator and Prometheus together you need to add two dependencies to your project:\n\r\n    org.springframework.boot\r\n    spring-boot-starter-actuator\r\n\r\n\r\n\r\n    io.micrometer\r\n    micrometer-registry-prometheus\r\n\nActuator has an endpoint available for prometheus to scrape but it\u2019s not exposed by default, so you will need to enable the endpoint by means of configuration. In this case, I\u2019ll do so via the application.properties.\nmanagement.endpoint.prometheus.enabled=true\r\nmanagement.endpoints.web.exposure.include=prometheus,info,health\nNow if you browse to http(s)://host(:8080)/actuator/prometheus you will see the output that prometheus will scrape to get the information from your application. A small snippet of the information provided by the endpoint is shown below, but there is a lot more information that the prometheus endpoint will expose.\n# HELP tomcat_global_sent_bytes_total  \r\n# TYPE tomcat_global_sent_bytes_total counter\r\ntomcat_global_sent_bytes_total{name=\"http-nio-8080\",} 75776.0\r\ntomcat_global_sent_bytes_total{name=\"http-nio-8443\",} 1.0182049E8\r\n# HELP tomcat_servlet_request_max_seconds  \r\n# TYPE tomcat_servlet_request_max_seconds gauge\r\ntomcat_servlet_request_max_seconds{name=\"default\",} 0.0\r\ntomcat_servlet_request_max_seconds{name=\"jsp\",} 0.0\r\n# HELP process_files_open The open file descriptor count\r\n# TYPE process_files_open gauge\r\nprocess_files_open 91.0\r\n# HELP system_cpu_usage The \"recent cpu usage\" for the whole system\r\n# TYPE system_cpu_usage gauge\r\nsystem_cpu_usage 0.00427715996578272\r\n# HELP jvm_memory_max_bytes The maximum amount of memory in bytes that can be used for memory management\r\n# TYPE jvm_memory_max_bytes gauge\r\njvm_memory_max_bytes{area=\"nonheap\",id=\"Code Cache\",} 2.5165824E8\r\njvm_memory_max_bytes{area=\"nonheap\",id=\"Metaspace\",} -1.0\r\njvm_memory_max_bytes{area=\"nonheap\",id=\"Compressed Class Space\",} 1.073741824E9\r\njvm_memory_max_bytes{area=\"heap\",id=\"PS Eden Space\",} 1.77733632E8\r\njvm_memory_max_bytes{area=\"heap\",id=\"PS Survivor Space\",} 524288.0\r\njvm_memory_max_bytes{area=\"heap\",id=\"PS Old Gen\",} 3.58088704E8\nNow that everything is configured from the application perspective, let\u2019s move on to Prometheus itself.\nPrometheus\nPrometheus\u00a0is an open-source systems monitoring and alerting toolkit originally built at\u00a0SoundCloudand now part of the\u00a0Cloud Native Computing Foundation. To get a better understanding of what prometheus really is let us take a look at an architectural diagram.\n(Source: https://prometheus.io/docs/introduction/overview/)\nThe prometheus server contains of a set of 3 features:\n\nA time series database\nA retrieval component which scrapes its targets for information\nAn HTTP server which you can use to query information stored inside the time series database\n\nTo make it even more powerful there are some additional components which you can use if you want:\n\nAn\u00a0alert manager, which you can use to send alerts via Pagerduty, Slack, etc.\nA\u00a0push gateway\u00a0in case you need to push information to prometheus instead of using the default pull mechanism\nGrafana\u00a0for visualizing data and creating dashboards\n\nWhen looking at Prometheus the most appealing features for us were:\n\nno reliance on distributed storage; single server nodes are autonomous\ntime series collection happens via a pull model over HTTP\ntargets are discovered via service discovery or static configuration\nmultiple modes of graphing and dashboarding support\n\nTo get up and running quickly you can configure prometheus to scrape some (existing) Spring Boot applications. For scraping targets, you will need to specify them within the prometheus configuration. Prometheus uses a file called\u00a0prometheus.yml\u00a0as its main configuration file. Within the configuration file, you can specify where it can find the targets it needs to monitor, specify recording rules and alerting rules.\nThe following example shows a configuration with a set of static targets for both prometheus itself and our spring boot application.\n\nglobal:\r\n  scrape_interval:   15s # By default, scrape targets every 15 seconds.\r\n\r\n  # Attach these labels to any time series or alerts when communicating with\r\n  # external systems (federation, remote storage, Alertmanager).\r\n  external_labels:\r\n    monitor: 'bootifull-monitoring'\r\n\r\nscrape_configs:\r\n- job_name:       'monitoring-demo'\r\n\r\n  # Override the global default and scrape targets from this job every 10 seconds.\r\n  scrape_interval: 10s\r\n  metrics_path: '/actuator/prometheus'\r\n\r\n  static_configs:\r\n  - targets: ['monitoring-demo:8080']\r\n    labels:\r\n      application: 'monitoring-demo'\r\n\r\n- job_name: 'prometheus'\r\n\r\n  scrape_interval: 5s\r\n\r\n  static_configs:\r\n  - targets: ['localhost:9090']\nAs you can see the configuration is pretty simple. You can add specific labels to the targets which can, later on, be used for querying, filtering and creating a dashboard based upon the information stored within prometheus.\nIf you want to get started quickly with Prometheus and have docker on your environment you can use the official docker prometheus image by running the following command and provide a custom configuration from your host machine by running:\ndocker run -p 9090:9090 -v /tmp/prometheus.yml:/etc/prometheus/prometheus.yml \\\r\n       prom/prometheus:v2.4.3\nIn the above example we bind-mount the main prometheus configuration file from the host system, so you can, for instance, use the above configuration. Prometheus itself has some basic graphing capabilities (as you can see in the following image), but they are more meant to be used when doing some ad-hoc queries.\n\nFor creating an application monitoring dashboard Grafana is much more suited.\nGrafana\nSo what is\u00a0Grafana\u00a0and what role does it play in our monitoring stack?\nGrafana allows you to query, visualize, alert on and understand your metrics no matter where they are stored. Create, explore, and share dashboards with your team and foster a data driven culture.\nThe cool thing about Grafana is (next to the beautiful UI) that it\u2019s not tied to Prometheus as its single data source like for instance Kibana is tied to Elasticsearch. Grafana can have many different data sources like AWS Cloudwatch, Elasticsearch, InfluxDB, Prometheus, etc. This makes it a very good option for creating a monitoring dashboard. Grafana talks to prometheus by using the PromQL query language.\nFor Grafana there is also an official Docker image available for you to use. You can get Grafana up and running with a simple command.\n\ndocker run -p 3000:3000 grafana/grafana:5.2.4\nNow if we connect Grafana with Prometheus as the datasource and install\u00a0this excellent JVM Micrometer dashboard\u00a0into Grafana we can instantly start monitoring our Spring Boot application. You will end up with a pretty mature dashboard that lets you switch between different instances of your application.\n\nIf you want to start everything all at once you can easily use docker-compose.\n\nversion: \"3\"\r\nservices:\r\n  app:\r\n    image: monitoring-demo:latest\r\n    container_name: 'monitoring-demo'\r\n    build:\r\n      context: ./\r\n      dockerfile: Dockerfile\r\n    ports:\r\n    - '8080:8080'\r\n  prometheus:\r\n    image: prom/prometheus:v2.4.3\r\n    container_name: 'prometheus'\r\n    volumes:\r\n    - ./monitoring/prometheus/:/etc/prometheus/\r\n    ports:\r\n    - '9090:9090'\r\n  grafana:\r\n    image: grafana/grafana:5.2.4\r\n    container_name: 'grafana'\r\n    ports:\r\n    - '3000:3000'\r\n    volumes:\r\n    - ./monitoring/grafana/provisioning/:/etc/grafana/provisioning/\r\n    env_file:\r\n    - ./monitoring/grafana/config.monitoring\r\n    depends_on:\r\n    - prometheus\nI\u2019ve put together a small demo project, containing a simple Spring Boot application and the above prometheus configuration, in a\u00a0github repository\u00a0for demo and experimentation purposes. Now if you want to generate some statistics run a small load test with JMeter or Apache Bench. Feel free to use/fork it!\n", "tags": [], "categories": ["Blog", "Development"]}
{"post_id": 1407, "title": "Serverless, the Future of the Cloud?! [Devoxx 2018]", "url": "https://www.luminis.eu/blog-en/development-en/serverless-the-future-of-the-cloud-devoxx-2018/", "updated_at": "2020-11-17T10:02:36", "body": "\nAre you still using Docker in production? Get over it! Serverless is the NEW future of the Cloud. But since the Cloud is still someone else\u2019s computer, that needs to be managed too. And if it is sitting idle, you probably have to pay for it whether you like it or not. No server can be more easily managed than no server. Therefore: meet Serverless, a new paradigm that truly approaches the Pay-as-You-Go philosophy once promised by the Cloud. This talk explores Serverless, its impact on existing architectures, and assesses it\u2019s usability for Mobile Back-ends as a Service (MBaaS), Functions-as-a-Service (FaaS) and also for Microservices based architectures hosted in the cloud. Internet connectivity permitting, there will be demos too.\n", "tags": [], "categories": ["Blog", "Development"]}
{"post_id": 18091, "title": "Luminis releases a new version of the InformationGrid", "url": "https://www.luminis.eu/blog-en/luminis-releases-a-new-version-of-the-informationgrid/", "updated_at": "2018-11-06T11:11:11", "body": "Apeldoorn \u2013 On Monday November 5th a new version of the InformationGrid was released.\u00a0The InformationGrid\u00a0is a PaaS with high productivity characteristics for the development of data-intensive cloud-native applications. By using the combination of domain-specific\u00a0languages (DSL) and declarative models, the InformationGrid aims to minimize hand-crafted code. By combining this with a multi-cloud infrastructure, a next generation application environment has become available. In this release a number of new\u00a0features was introduced to further\u00a0optimise the productivity of analysts and developers.\u00a0\nThe Cauberg.1 release, which will be available as a PaaS, contains the following improvements:\n\u2022 A new DSL and trace facility for the description of projections of document collections\n\u2022 A new DSL-basede form component for the description of user interfaces\n\u2022 A migration tool for the GDPR-compliant \u2018right to be forgotten\u2019 allowing for the encryption of events from the past\n\u2022 Improved developer productivity by enriching the existing toolset\n\u2022 Improved native support for decimals, dates, timestamps and money types \u00a0in schemas\nDe Cauberg.1 release of the InformationGrid has been extensively tested during\u00a0internal alfa-tests and beta-tests in production environments.\nInterested to know more? For more information please contact Raymond ter Riet (raymond.terriet@luminis.eu)\n\u00a0\n", "tags": [], "categories": ["Blog"]}
{"post_id": 17952, "title": "Luminis intensiveert relatie met Microsoft", "url": "https://www.luminis.eu/blog-en/luminis-intensiveert-relatie-met-microsoft/", "updated_at": "2018-10-24T14:37:22", "body": "Amersfoort \u2013 Luminis is Cloud Solution Provider (CSP) voor Microsoft geworden. Hierdoor kan Luminis haar portfolio verder uitbreiden door ook de software van Microsoft rechtstreeks te kunnen leveren aan haar klanten. Deze intensivering van de relatie met Microsoft op het gebied van Cloud-technologie past in de internationale strategie van Luminis die erop gericht is om technologie- en innovatie partner te zijn voor haar klanten.\n\u00a0\nLuminis biedt een compleet portfolio van diensten en oplossingen aan op basis van Cloud-technologie. Door deze diensten uit te breiden met het portfolio van de Cloud-diensten van Microsoft, worden de relaties met klanten niet alleen eenvoudiger en wendbaarder, maar ontstaan er ook nieuwe mogelijkheden voor Microsoft en Luminis om elkaar te versterken.\nHet CSP-programma is onderdeel van het One Commercial Partner (OCP) programma van Microsoft. Met het OCP wil Microsoft de adoptie van Cloud-computing versnellen door actief haar eigen commerci\u00eble strategie te combineren met de commerci\u00eble strategie van haar partners. Microsoft is al in 2014 gestart met het CSP-programma. Toegang tot het CSP-programma maakt het voor CSP-partners mogelijk om klanten beter te ondersteunen door het rechtstreeks aanbieden van Azure Cloud Services als onderdeel van een klantspecifieke oplossingen.\n\n", "tags": [], "categories": ["Blog"]}
{"post_id": 17941, "title": "Luminis intensifies relationship with Microsoft", "url": "https://www.luminis.eu/blog-en/luminis-intensifies-relationship-with-microsoft/", "updated_at": "2018-10-24T15:15:50", "body": "Amersfoort \u2013 Software technology company Luminis is awarded Microsoft Cloud Solution Provider (CSP) status.\u00a0Luminis, one of the leading IT companies in the Netherlands is a long-standing Microsoft partner that has achieved the Cloud\u00a0Solution Provider (CSP) status.\u00a0CSP is a Microsoft offering that enables authorized partners the ability to bundle Microsoft Cloud along with their own services. Microsoft\u00a0partners, such as Luminis, can now support the complete customer lifecycle through direct billing, provisioning, management and support.\nBeing a CSP partner gives Luminis the opportunity to provide the complete Microsoft Cloud portfolio to businesses and organizations along with their own services\u00a0meaning clients can benefit from direct billing and combined offerings.\u00a0Becoming a CSP partner\u00a0strengthens Luminis\u2019 international strategy which is aimed at being a technology and innovation partner for businesses and organizations.\nThe CSP program is part of Microsoft\u2019s One Commercial Partner (OCP) program. With the OCP, Microsoft wants to accelerate the adoption of Cloud computing by actively combining its own commercial strategy with the commercial strategy of its partners. Microsoft has already started with the CSP program in 2014. Access to the CSP Program enables CSP partners to better support customers by directly offering Azure Cloud Services as part of customer-specific solutions.\n\n", "tags": [], "categories": ["Blog"]}
{"post_id": 17934, "title": "Winner of the Dutch FD Gazellen Award", "url": "https://www.luminis.eu/blog-en/winner-of-the-dutch-fd-gazellen-award/", "updated_at": "2018-10-25T09:45:04", "body": "Luminis won the Dutch entrepreneurial award \u201cFD Gazellen\u201d. We are very proud to be awarded as being one of the fastest growing companies in the Netherlands.\u00a0\n\u00a0\nThe FD Gazellen Award is an initiative of a Dutch financial magazine, called\u00a0Financieele Dagblad, and focuses on successful and fast growing companies.\u00a0Luminis has shown above-average growth over the past three years and besides this innovative entrepreneurship.\n\u00a0\n\nThis is an Award that all our colleagues can be proud of. We have been helping our customers and partners to innovate using software technology for years. I am very glad that there is recognition\u00a0for what we do from a business point of view now too. In the future it will be proven even more that innovation together with customers will lead to both substantive and financial positive results.\u201d,\u00a0says Laurens Miedema, CFO of Luminis.\n\nFD Gazellen Award 2018\nFD Gazellen 2018\nThe\u00a0FD Gazellen\u00a0Awards are presented this year for the fifteenth time to Dutch companies that have grown at least 20% in turnover during the previous three years and which have closed profitable in the past year.\u00a0Starting in this year, not only the revenue growth is counted, but also the growth of employees and the profitability of the fast-growing companies.\u00a0With these three indicators, the Award will be more balanced.\u00a0On November 13th 2018, the FD Gazellen award will be handed out by Financieele Dagblad.\nInnovative Entrepreneurship\nLuminis creates innovative IT solutions for business and government.\u00a0Our 180 employees are spread over various small offices the Netherlands, so we keep open collaboration with our customers. With the combination of consultancy services and Cloud-based products, we offer solutions that use the Cloud, IoT and big data.\n", "tags": [], "categories": ["Blog"]}
{"post_id": 1426, "title": "A single data store called Redux", "url": "https://www.luminis.eu/blog-en/data-en/a-single-data-store-called-redux/", "updated_at": "2020-11-30T15:38:31", "body": "Redux\u00a0is a data store container for JavaScript apps. I\u2019m currently on a team, that\u2019s building an application for the government in three months. We use React and Redux in the front-end. This combination works well for us. In this blog I\u2019ll explain a little bit about what we have done so far and how we implemented Redux.\nWe\u2019re building a Content Management System (CMS) for editors (users) to enter information and later disclose it through an API. In order to show and edit the data in a clear way for the users in the front-end, we want to have the display of the data (rendered UI components) linked to the data itself. The displayed data should be re-rendered when the data changes. Redux controlled React components come in handy for this! We store the data retrieved from the server through asynchronous calls in the data store container of Redux. When the user uses the user interface by going to different pages, tabs or saving forms, a call is triggered to fetch the latest data from the server. This data (partially) overwrites the old data in the store and brings the store to a new state. When data is changed in the store the parts (components) of the user interface that display the data are re-rendered. The actions of the user trigger (actually dispatch) Redux\u00a0Actions. Responses from the server also dispatch Redux Actions. These Actions contain a\u00a0type\u00a0and a\u00a0payload. The type specifies which kind of action is triggered. The payload contains the new data, which can be the response (body) of the server. This data needs to be put in the data store. This is done by a\u00a0Reducer. The Reducer is responsible of changing the state of the store. Actually it takes (part of) the previous state and switches over the type of Action, manipulates the data from it accordingly and returns the new state. It uses the payload of the Action to create a new state. It can do anything more you want too, like sorting arrays or execute other functions on the state. This way you have a time line of the state and its changes due to Actions.\n\u00a0\n// actions.js export const createResource = formData => ({ type: 'CREATE_RESOURCE_REQUEST_ACTION', payload: formData });\nexport const createResourceSuccess = response => ({ type: \u2018CREATE_RESOURCE_RESPONSE_SUCCESS_ACTION\u2019, payload: response });\n\nexport const createResourceFailed = err => ({ type: \u2018CREATE_RESOURCE_RESPONSE_FAILED_ACTION\u2019, payload: err });\n\n\u00a0\n// reducer.js const initialState = { isFetching: false, createdResources: [], };\nexport const reducer = (state = initialState, action) => { switch (action.type) { case \u2018CREATE_RESOURCE_REQUEST_ACTION\u2019: return { \u2026state, // copies the previous state isFetching: true // overwrites this field in the copied state }; case \u2018CREATE_RESOURCE_RESPONSE_SUCCESS_ACTION\u2019: return { isFetching: false, createdResources: [\u2026state.createdResources, action.payload] }; default: return state; } };\n\nIn a complex, big application all these can be split, for example by a feature. So you can make Actions, a Reducer and React components for a particular feature. This way you can easily order and maintain your application. There are also\u00a0Redux DevTools\u00a0and\u00a0extensions for multiple browsers\u00a0to look into the data store and debug while you code. In our application we use\u00a0Saga\u2019s. When a user triggers an Action that needs to make a request to the server this is handled by a worker Saga. The worker Saga sends an asynchronous request to the server. Depending on the response of the server, i.e. a success or a failure, the Saga dispatches a new Action specifically for a success or failure response of the specific request. Then the Reducer catches this to handle the data from the response.\n\u00a0\n// saga.js import { call, put, takeLatest } from 'redux-saga/effects'\nexport function* createResource(action) { const { formData } = action.payload; try { const response = yield call(axios.post, https://amsterdam.luminis.eu/api/resources, formData); yield put(actions.createResourceSuccess(response.data)); } catch (e) { yield put(actions.createresourceFailed(e)); } }\n\nexport function* saga() { yield takeLatest(\u2018CREATE_RESOURCE_REQUEST_ACTION\u2019, createResource); }\n\n", "tags": [], "categories": ["Blog", "Data"]}
{"post_id": 17645, "title": "New head-office for Luminis Nederland in Amersfoort", "url": "https://www.luminis.eu/blog-en/luminis-nederland-moved-amersfoort/", "updated_at": "2018-10-24T14:36:19", "body": "Luminis Nederland opens a new head-office in Amersfoort. Luminis is pleased to announce that the head-office of the dutch organisation, Luminis Nederland, has moved from Apeldoorn to Amersfoort. \u00a0Effective from the 1st of October, the new head-office will prove to be a perfect hub for all operations in The Netherlands.\nThe new head-office of Luminis Nederland in Amersfoort will feature an inspiring mix of a modern office space and an inviting home where people can meet and work. The new office also features a creative work environment and more and improved space for employees, customers and partners.\nIn the Netherlands, Luminis has offices in Arnhem, Apeldoorn, Rotterdam, Amsterdam and Eindhoven as part of her consulting organisation. The move is also intended to support the further growth of the Luminis organisation as a starting point for it\u2019s international strategy. The combination of consulting services and a cloud-based technology stack enable Luminis to offer services in areas of Cloud computing, IoT and Data-management.\nThe new address of Luminis Nederland is Stadsring 107 in Amersfoort. You are always welcome to drop by for a cup of coffee!\n\u00a0\n\n", "tags": [], "categories": ["Blog"]}
{"post_id": 1642, "title": "Handle asynchronous processes more elegantly with RxJS", "url": "https://www.luminis.eu/blog-en/development-en/handle-asynchronous-processes-more-elegantly-with-rxjs/", "updated_at": "2020-11-12T10:06:49", "body": "Any webdeveloper who has worked with Angular as their front-end framework lately has probably encountered the RxJS (reactive extensions for javascript) library. This is because the angular http module is dependent on RxJS and this modules\u2019 various methods for http put, post and patch requests return a RxJS \u2018Observable\u2019, which is the key component and concept of RxJS.\nI was, until recently, used to working with promises to handle asynchronous Javascript operations. Although I was familiar with the observer design pattern in general, the way RxJS works and how to work with it confused me at first. I decided to write down my recent experience with RxJS in the form of this blog. This is partly to benefit my own learning process and partly hoping that it may aid that of the reader. Alternatively, I hope it will peak the readers\u2019 interest in making use of RxJS when writing code.\nRxJS turns out to be an elegant and useful tool when structuring javascript applications that need to handle asynchronous processes. As such, I feel it is useful for any javascript developer to take note of.\nSo what is a RxJS observable?\nLet\u2019s start off with the most basic example of handling a http get request may look like in an Angular app, taken straight from the angular.io documentation:\n\ngetConfigResponse(): Observable> {\r\n  return this.http.get(\r\n    this.configUrl, { observe: 'response' });\r\n}\nLet\u2019s try to put into words what this code is for:\nThis is a piece of typescript that shows a method getConfigResponse(), that returns an observable of a HttpResponse , that maps to a Config object\u2019. \u00a0In this piece of code, this.http is the angular http client. If we look at the get() methods of this \u2018class\u2019 we can see that they return an observable object from the rxjs library. \u00a0And, courtesy of typescript, we can check out the typescript definitions of Observable ourselves. At the very top of the code in the Observable.d.ts interface it says the following about the Observable<T> class:\n\n/**\r\n\u00a0* A representation of any set of values over any amount of time. This is the most basic building block\r\n\u00a0* of RxJS.\r\n\u00a0* @class Observable\r\n*/\nThe official RxJS documentation describes an observable like this: \u201cAn observable represents the idea of an invokable collection of future values or events\u201d. So we can infer that the Observable class is at the heart of RxJS and that this building block always functions as a representation, or wrapper, of something else. Moreover, it represents the things it wraps as values or events, that are bound to become available at some unknown time in the future.\nLet\u2019s explore some examples of RxJS observables used in code from an actual application!\nIn the interface definition of Observable we see the definitions of some methods: \u00a0subscribe()\u00a0and\u00a0\u00a0pipe()\u00a0are the most interesting ones and will be discussed shortly. Apparently, a RxJS observable can be interacted with via a limited set of methods defined on the Observable class.\nAnd how should I use this thing?\n\nimport { Observable } from 'rxjs/Rx';   \r\n\r\n@Injectable()\r\nexport class AuthService {\r\n\r\n...//other AuthService code\r\n\r\nprivate userSettings: UserSettings;\r\n\r\n...//other AuthService code\r\n\r\npublic getProfile(): Observable {\r\n        if (!this.userSettings) {\r\n            return this.http.get(this.config.userServiceUrl + '/users/usersettings', { withCredentials: true }).do(userSettings=> {\r\n                this.userSettings= userSettings;\r\n            });\r\n        }\r\n        return Observable.create(this.userSettings);\r\n    }\r\n}\nThis code fragment is already illustrative of the powerful things we can do with RxJS observables. Here, we return an observable of some user\u2019s userprofile settings (note that userId is not handled via url in this specific case).\u00a0 If there are no user settings in memory yet, we assign the object returned from the http response to the userSettings variable of the class, then return an observable of the user settings for further interaction. If instead there are user settings in memory already, we simply instantiate a new observable of the existing userSettings with Observable.create().\nThe latter is a very handy aspect of RxJS observables: You can wrap anything in the observable and subsequently program against that observable in a reactive manner. Thus easily mixing synchronous with asynchronous code.\nThe .do method is the other new concept in the above piece of code. This method is not present on Observable (that returns from this.http.get<UserSettings>()) itself. Rather this \u2018operator\u2019 gets imported from\u00a0\u2018rxjs/operators\u2019. The official RxJS documentation describes pretty clearly what this specific operator is for: \u201cPerform a side effect for every emission on the source Observable, but return an Observable that is identical to the source\u201d.\nRxJS has a large number of operators similar to .do, that can filter, buffer, delay, map (etc., etc.) your observables and always create a new observable as output. Observable.pipe() that I mentioned before is present on the Observable object itself, but otherwise functions more or less the same as an operator in that it returns a new observable based on the input of an existing one. The new Observable in the case of .pipe () is projected to from the old one, by passing through the functions defined in the pipe. I\u2019ll refer to this external article for readers who want more info on piping observables.\nNow let\u2019s cover how to actually observe a RxJs Observable. This is where, gasp, observable.subscribe() comes into play. I\u2019ll give a hypothetical example based on the code above for getProfile():\n\n@Component\r\nexport class SomeViewComponent {\r\n\r\nconstructor (private authService: AuthService){\r\n}\r\n\r\nprivate userSettingsOnScreen(): void {\r\n   this.authService.getProfile().subscribe(userSettings => {\r\n   this.backGroundColor = userSettings.preferences.backGroundColor;\r\n });\r\n}\nThe subscribe method in the above example takes a function as method parameter. This function kind of acts like an observer in a classic observer design pattern and in fact the RxJS documentation actually calls this function the \u2018observer\u2019 as well. This makes a lot of sense given that:\n\nYou can subscribe multiple of these functions to the same Observable\nYou can unsubscribe these methods from the Observable\n\nThe \u2018observer\u2019 function(s) can listen to only 3 kind of \u2018events\u2019 emitted by the Observable and those are:\n\u2018onnext\u2019: The Observable delivering it\u2019s wrapped value (as in the above example).\n\u2018onerror\u2019: The Observable delivering an error result due to an unhandled exception.\n\u2018oncompleted\u2019: All onnext calls have been completed and the observer function has finished.\nThe onnext callback confused me for a while, but it makes more sense knowing you can do this (example taken from here):\n\npublic Listener(name: string): void {\r\n  return {\r\n    onNext: function (v) {\r\n      console.log(name + \".onNext:\", v)\r\n    },\r\n    onError: function (err) {\r\n      console.log(name + \".onError:\", err.message)\r\n    },\r\n    onCompleted: function () {\r\n      console.log(name + \".onCompleted!\")\r\n    },\r\n  }\r\n\r\nlet obs = Observable.of(1, 2)\r\nobs.subscribe(Listener(\"obs\"));\r\n\r\n//output\r\nobs.onNext: 1\r\nobs.onNext: 2\r\nobs.onCompleted!\nNot only can we subscribe multiple times to a single Observable but, as the example above should clarify, we can also observe multiple things in sequence with the same Observable. Pretty nice!\nI\u2019ll round up this write-up with showing error handling in RxJS:\n// Try three times with the retry operator, to get the data and then give up\r\nvar observableOfResponse = this.http.get('someurl').retry(2);\r\n\r\nvar subscription = observableOfResponse.subscribe(\r\n  data => console.log(data),\r\n  err => console.log(err)\r\n);\r\n\r\n\r\n// catch a potential error after 2 retries with the catch operator\r\nvar observableOfResponse = get('someurl').retry(2).catch(trySomethingElse());\r\n\r\nvar subscription = observableOfResponse.subscribe(\r\n  data => {\r\n    // Displays the data from the URL or cached data\r\n    console.log(data);\r\n  }\r\n);\nHere we see that we can either let the error\u00a0(maybe a code 500?) happen like in the top example, or catch and manage it like in the bottom one. It\u2019s pretty intuitive, once you know what operators are available. Note that in the first scenario an exception still gets thrown.\nRounding up\nRxJS is an extensive subject and deserves more attention than I\u2019m able to give in this little blog. I have tried in this article to condense a vast amount of information into a short overview that aimed to cover the core ideas of the library but at the same time be very practical:\u00a0 The information above is enough to start working with it immediately. So rounding up, I\u2019ll say good luck and have fun to anyone using RxJS in their front-end. I know I will.\n", "tags": [], "categories": ["Blog", "Development"]}
{"post_id": 17477, "title": "Devoxx4Kids Zwolle", "url": "https://www.luminis.eu/no-category/devoxx4kids-zwolle/", "updated_at": "2019-09-26T12:09:11", "body": "Programming is cool!\nAre you between 10 and 14 years old and have you always wanted to build your own robot, develop a game or be the boss of a Minecraft-world?\nThen, join us in Zwolle on Saturday 9 november 2019. Because at that moment, the best (and coolest) computer programmers of Luminis will teach\u00a0you exactly how to do this. How? By teaching\u00a0you how to program on real computers! You will not learn from books but you can do it yourself directly in our workshops!\nWhy learn how to program?\nBecause it is very cool to build your own robot and develop your own game of course! You will learn a lot on this day. You will learn to think creatively, fix problems and work together with other kids to get the best results. The best thing is, you can save everything and finish it at home if you want.\nThe workshops\nThere will be 4 different rooms, with different cool themes. For example, you can learn how to build your own robot and how to control it with Lego Mindstorms. Also, you can build your own game by programming in Python, and you can make a game extension in Minecraft! How cool is that? You will take part in 3 of 4 different workshops. Every workshop lasts 1,5 hours.\u00a0\nHave you ever been before at one of our Devoxx4Kids events? Please, do come back, because all of our workshops have been renewed! Above all they are easy to follow for all levels. For the the beginners there is an extra explanation and for the advanced kids there are challenging bonus assignments.\nThe agenda\nWe will start at 9.00 am with a short introduction about the schedule of the day. At 10.00 am your parents will go home, and you can start the coolest workshops. During the day there will be all kinds of food and drinks. After the workshops are over, at 4.30 pm\u00a0everyone can show\u00a0their parents what they\u00a0have learned and achieved this day.\nThis edition of Luminis Devoxx4Kids has been made possible by\u00a0Hogeschool VIAA.\u00a0Luminis and VIAA are sharing the same interest to teach\u00a0kids how to program.\n", "tags": [], "categories": ["No category"]}
{"post_id": 1693, "title": "Hybrid app development \u2013 The Good, the Bad and the Ugly", "url": "https://www.luminis.eu/blog-en/development-en/hybrid-app-development-the-good-the-bad-and-the-ugly/", "updated_at": "2020-11-12T10:57:51", "body": "Software is eating your smartphone.\u00a0Or something along those lines.\u00a0So it is not surprising there is a lot to be said about mobile app development and especially about all the non-native frameworks that are around these days. We know it\u2019s hard to find your way in all these voices so we have made a neat overview of our experience and findings for your convenience. But before we dive into it, let\u2019s start with some clich\u00e9s, \nClich\u00e9 #1: The world of mobile app development is ever changing.\nFace ID, ARCore, Jetpack, Swift 4: There is no shortage of \u2018new\u2019 things for mobile app users or developers and keeping track of everything can easily become a day-job. Thankfully there is a large community of writers (like me) that will help you navigate the landscape. But your hands are on the wheel and it\u2019s your car when you crash it into a tree. In other words, you have to make the tough choices. The best thing we can do is guide you along the way.\nClich\u00e9 #2: Software development is about balancing trade-offs.\nA lot of frameworks and technologies for software development are selling their product or service like it\u2019s the best thing since sliced bread. Especially the last couple of years I see more and more of their websites turned into a polished and well-tuned commercial website. With marketing slogans, customer blurbs, and video commercials.\nBut every practiced software developer knows there is no such thing as a holy grail. Context is everything and when searching for the best tool for the job, it\u2019s always good to look at both. Otherwise, you will be lured into a solution that has too many trade-offs because of clever, superficial marketing campaigns.\nClich\u00e9 #3: Android is better/worse than iOS.\nThe usefulness of this discussion is zero. When you\u2019re building a serious mobile app there is no \u201cor\u201d between Android and iOS. Sure, they have different market shares but you can\u2019t neglect one over the other because of personal preference. Any serious app developer should target both.\nThe hybrid: Toaster or skinjob?\nI think the Battlestar Galactica analogy is somewhat apt. Android as the sturdy and rough metal robots. iOS as the most human-like but still artificial machine. Whatever your point of view, supporting competing platforms for your app can be a bit frustrating. Writing and maintaining the same set of features multiple times has a real impact on development costs and time to market. It feels like waste so it\u2019s not surprising a lot of hybrid app frameworks are available to mitigate these deficiencies. When you\u2019re willing to leave the native ecosystem behind, another set of choices become available accompanied with their own pros and cons.\n\nDownload the overview here\nAt Luminis, we have developed a lot of mobile apps with native and various hybrid frameworks. We understand the difficulties, especially when you want to put things into context. To help you out we have summarized our insights in this high-level rundown. It\u2019s especially useful when you have some functional requirements available and a rough idea of the user experience you seek. At the very least you will get some foreknowledge of the impact on user experience and development costs of these frameworks.\n", "tags": [], "categories": ["Blog", "Development"]}
{"post_id": 1348, "title": "Elasticsearch instances for integration testing", "url": "https://www.luminis.eu/blog-en/search-en/elasticsearch-instances-for-integration-testing/", "updated_at": "2021-02-01T15:37:56", "body": "In my latest project I have implemented all communication with my Elasticsearch cluster using the high level REST client. My next step was to setup and teardown an Elasticsearch instance automatically in order to facilitate proper integration testing. This article describes three different ways of doing so and discusses some of the pros and cons. Please refer to this repository for implementations of all three methods.\ndocker-maven-plugin\nThis generic Docker plugin allows you to bind the starting and stopping of Docker containers to Maven lifecycles. You specify two blocks within the plugin; configuration and executions. In the configuration block, you choose the image that you want to run (Elasticsearch 6.5.3 in this case), the ports that you want to expose, a health check and any environment variables. See the snippet below for a complete example:\n<plugin>\r\n    <groupId>io.fabric8</groupId>\r\n    <artifactId>docker-maven-plugin</artifactId>\r\n    <version>0.32</version>\r\n    <configuration>\r\n        <imagePullPolicy>always</imagePullPolicy>\r\n        <images>\r\n            <image>\r\n                <alias>docker-elasticsearch-integration-test</alias>\r\n                <name>elasticsearch/elasticsearch:6.5.3</name>\r\n                <run>\r\n                    <namingStrategy>alias</namingStrategy>\r\n                    <ports>\r\n                        <port>9299:9200</port>\r\n                        <port>9399:9300</port>\r\n                    </ports>\r\n                    <env>\r\n                        <cluster.name>integration-test-cluster</cluster.name>\r\n                    </env>\r\n                    <wait>\r\n                        <http>\r\n                            <url>http://localhost:9299</url>\r\n                            <method>GET</method>\r\n                            <status>200</status>\r\n                        </http>\r\n                        <time>60000</time>\r\n                    </wait>\r\n                </run>\r\n            </image>\r\n        </images>\r\n    </configuration>\r\n    <executions>\r\n        <execution>\r\n            <id>docker:start</id>\r\n            <phase>pre-integration-test</phase>\r\n            <goals>\r\n                <goal>start</goal>\r\n            </goals>\r\n        </execution>\r\n        <execution>\r\n            <id>docker:stop</id>\r\n            <phase>post-integration-test</phase>\r\n            <goals>\r\n                <goal>stop</goal>\r\n            </goals>\r\n        </execution>\r\n    </executions>\r\n</plugin>\r\n\nYou can see that I\u2019ve bound the plugin to the pre- and post-integration-test lifecycle phases. By doing so, the Elasticsearch container will be started just before any integration tests are ran and will be stopped after the integration tests have finished. I\u2019ve used the maven-failsafe-plugin in order to trigger the execution of tests ending with *IT.java in the integration-test lifecycle phase. Since this is a generic Docker plugin, there is no special functionality to easily install Elasticsearch plugins that may be needed during your integration tests. You could however create your own image with the required plugins and pull that image during your integration tests. The integration with IntelliJ is also not optimal. When running an *IT.java class, IntelliJ will not trigger the correct lifecycle phases and will attempt to run your integration test without creating the required Docker container. Before running an integration test from IntelliJ, you need to manually start the container from the \u201cMaven projects\u201d view by running the docker:run commando:\n\nAfter running, you will also need to run the docker:stop commando to kill the container that is still running. If you forget to kill the running container and want to run a mvn clean install later on it will fail, since the build will attempt to create a container on the same port \u2013 as far as I know, the plugin does not allow for random ports to be chosen.\nPros: \n\nLittle setup, only requires configuration of one Maven plugin \n\nCons: \n\nNo out of the box functionality to start the Elasticsearch instance on a random port\nNo out of the box functionality to install extra Elasticsearch plugins\nExtra dependency in your build pipeline (Docker)\nIntelliJ does not trigger the correct lifecycle phases\n\nelasticsearch-maven-plugin\nThis second plugin does not require Docker and only needs some Maven configuration to get started. See the snippet below for a complete example:\n<plugin>\r\n    <groupId>com.github.alexcojocaru</groupId>\r\n    <artifactId>elasticsearch-maven-plugin</artifactId>\r\n    <version>6.16</version>\r\n    <configuration>\r\n        <version>6.5.3</version>\r\n        <clusterName>integration-test-cluster</clusterName>\r\n        <transportPort>9399</transportPort>\r\n        <httpPort>9299</httpPort>\r\n    </configuration>\r\n    <executions>\r\n        <execution>\r\n            <id>start-elasticsearch</id>\r\n            <phase>pre-integration-test</phase>\r\n            <goals>\r\n                <goal>runforked</goal>\r\n            </goals>\r\n        </execution>\r\n        <execution>\r\n            <id>stop-elasticsearch</id>\r\n            <phase>post-integration-test</phase>\r\n            <goals>\r\n                <goal>stop</goal>\r\n            </goals>\r\n        </execution>\r\n    </executions>\r\n</plugin>\r\n\n\nAgain, I\u2019ve bound the plugin to the pre- and post-integration-test lifecycle phases in combination with the maven-failsafe-plugin. This plugin provides a way of starting the Elasticsearch instance from IntelliJ in much the same way as the docker-maven-plugin. You can run the elasticsearch:runforked commando from the \u201cMaven projects\u201d view. However in my case, this started the container and then immediately exited. There is also no out of the box possibility of setting a random port for your instance. However, there are solutions to this at the expense of having a somewhat more complex Maven configuration. Overall, this is a plugin that seems to provide almost everything we need with a lot of configuration options. You can automatically install Elasticsearch plugins or even bootstrap your instance with data. In practice I did have some problems using the plugin in my build pipeline. Upon downloading the Elasticsearch ZIP the build would sometimes fail, or in other cases when attempting to download a plugin. Your mileage may vary, but this was reason for me to keep looking for another solution. Which brings me to plugin number three.\nPros:\n\nLittle setup, only requires configuration of one Maven plugin\nNo extra external dependencies\nHigh amount of configuration possible\n\nCons:\n\nNo out of the box functionality to start the Elasticsearch instance on a random port\nPoor integration with IntelliJ\nSeems unstable\n\ntestcontainers-elasticsearch\nThis third plugin is different from the other two. It uses a Java testcontainer that you can configure through Java code. This gives you a lot of flexibility and requires no Maven configuration. Since there is no Maven configuration, it does require some work to make sure the Elasticsearch container is started and stopped at the correct moments. In order to realize this, I have extended the standard SpringJUnit4ClassRunner class with my own ElasticsearchSpringRunner. In this runner, I have added a new JUnit RunListener named JUnitExecutionListener. This listener defines two methods testRunStarted and testRunFinished that enable me to start and stop the Elasticsearch container at the same points in time that the pre- and post-integration-test Maven lifecycle phases would. See the snippet below for the implementation of the listener:\npackage nl.luminis.articles.elasticsearch.integration.testcontainers;\r\n\r\nimport org.junit.runner.Description;\r\nimport org.junit.runner.Result;\r\nimport org.junit.runner.notification.RunListener;\r\nimport org.junit.runner.notification.RunNotifier;\r\n\r\nimport io.restassured.RestAssured;\r\nimport lombok.extern.slf4j.Slf4j;\r\nimport org.testcontainers.elasticsearch.ElasticsearchContainer;\r\n\r\n@Slf4j\r\n\r\npublic class JUnitExecutionListener extends RunListener {\r\n\r\n    private static final String ELASTICSEARCH_IMAGE = \"docker.elastic.co/elasticsearch/elasticsearch\";\r\n    private static final String ELASTICSEARCH_VERSION = \"6.5.3\";\r\n    private static final String ELASTICSEARCH_HOST_PROPERTY = \"spring.elasticsearch.rest.uris\";\r\n    private static final int ELASTICSEARCH_PORT = 9200;\r\n\r\n    private ElasticsearchContainer container;\r\n    private RunNotifier notifier;\r\n\r\n    public JUnitExecutionListener(RunNotifier notifier) {\r\n        this.notifier = notifier;\r\n    }\r\n\r\n    @Override\r\n    public void testRunStarted(Description description) {\r\n        try {\r\n            if (System.getProperty(ELASTICSEARCH_HOST_PROPERTY) == null) {\r\n                log.debug(\"Create Elasticsearch container\");\r\n                int mappedPort = createContainer();\r\n                System.setProperty(ELASTICSEARCH_HOST_PROPERTY, \"localhost:\" + mappedPort);\r\n                String host = System.getProperty(ELASTICSEARCH_HOST_PROPERTY);\r\n                RestAssured.basePath = \"\";\r\n                RestAssured.baseURI = \"http://\" + host.split(\":\")[0];\r\n                RestAssured.port = Integer.parseInt(host.split(\":\")[1]);\r\n                log.debug(\"Created Elasticsearch container at {}\", host);\r\n            }\r\n        } catch (Exception e) {\r\n            notifier.pleaseStop();\r\n            throw e;\r\n        }\r\n    }\r\n\r\n    @Override\r\n    public void testRunFinished(Result result) {\r\n        if (container != null) {\r\n            String host = System.getProperty(ELASTICSEARCH_HOST_PROPERTY);\r\n            log.debug(\"Removing Elasticsearch container at {}\", host);\r\n            container.stop();\r\n        }\r\n    }\r\n\r\n    private int createContainer() {\r\n        container = new ElasticsearchContainer();\r\n        container.withBaseUrl(ELASTICSEARCH_IMAGE);\r\n        container.withVersion(ELASTICSEARCH_VERSION);\r\n        container.withEnv(\"cluster.name\", \"integration-test-cluster\");\r\n        container.start();\r\n        return container.getMappedPort(ELASTICSEARCH_PORT);\r\n    }\r\n}\nIt will create an Elasticsearch Docker container on a random port for use by the integration tests. The best thing about having this runner is that it works perfectly fine in IntelliJ. Simply right-click and run your *IT.java classes annotated with @RunWith(ElasticsearchSpringRunner.class) and IntelliJ will use the listener to setup the Elasticsearch container. This allows you to automate your build pipeline while still keeping developers happy.\nPros:\n\nNeat integration with both Java and therefore your IDE\nSufficient configuration options out of the box\n\nCons:\n\nMore complex initial setup\nExtra dependency in your build pipeline (Docker)\n\nIn summary, all three of the above plugins are able to realize the goal of starting an Elasticsearch instance for your integration testing. For me personally, I will be using the testcontainers-elasticsearch plugin going forward. The extra Docker dependency is not a problem since I use Docker in most of my build pipelines anyway. Furthermore, the integration with Java allows me to configure things in such a way that it works perfectly fine from both the command line and the IDE. Feel free to checkout the code behind this article, play around with the integration tests that I\u2019ve setup there and decide for yourself which plugin suits your needs best.\n", "tags": [], "categories": ["Blog", "Search"]}
{"post_id": 1342, "title": "The Forgotten Step in CRISP-DM and ASUM-DM Methodologies", "url": "https://www.luminis.eu/blog-en/development-en/the-forgotten-step-in-crisp-dm-and-asum-dm-methodologies/", "updated_at": "2020-11-16T16:50:53", "body": "CRISP-DM stands for the cross-industry standard process for data mining which an open standard for data mining existing since 1999 and proposed by IBM. CRISP-DM suggests a set of steps to perform data mining projects to maximize the success of the project and minimize the common faults happening in any data-oriented projects. Later in 2015, an extended version of CRISP-DM is proposed by IBM so-called ASUM-DM (the Analytics Solutions Unified Method). ASUM-DM is an extension of CRISP-DM having the same steps in data mining (development) plus an operational / deployment part.\nI personally pretty much a fan of CRISP-DM and ASUM-DM. In my daily consultancy life, I stick to the steps provided because it minimizes the risk of project failures. I believe following CRISP-DM and ASUM-DM methodologies properly distinguishes a senior data scientist from junior ones. Many data-scientists/data-miners have the tendency to quickly model the data to reach the insights ignoring proper understanding of the problem and the right data preparation. That is the reason CRISP-DM comes with clear steps that taking them minimizes the common failure in any data science/data mining projects. Being a data miner and later a data scientist for over 12 years, I believe CRISP-DM misses one crucial step. By writing this article I intend to add a new step in CRISP-DM/ASUM-DM which comes from some years of experience in data science.\nCRISP-DM Methodology\nCRISP-DM suggests these steps for data-mining/data-science: (1) Business understanding: which means the data scientist should properly understand the business of his/her client. Why is analytics important to them? How analytics can be of a great value for the business and so on. (2) Data understanding: which means the data scientist should go through all the fields within the data to understand the data like a domain expert. With a poor understanding of the data, a data scientist can barely provide high-quality data science solutions. (3) Data preparation: which is the most time-consuming step in any data science project being data preparation in the way that a model can ingest and understand it. (4) Modeling: the magical phase turning the raw-data to (actionable) insights. With recent advances in data science and the toolings such as AutoML and deep learning, modeling is less complicated as before. (4) Evaluation: checking the accuracy of the model which metrics such as a confusion matrix, RMSE, MAPE, and MdAPE. (5) Deployment: which means making the use of the model with the new data. As you can see in the picture, CRISP-Dm is an iterative approach, matches quite well with agile methodology. The steps can be taken in parallel and they are flexible enough to be redone quickly once there is a modification in any previous steps. \nASUM-DM methodology:\nASUM-DM adds a new deployment/operation wing to CRISP-DM. The development phase stays the same as CRISP-DM however in deployment new facets are added such as collaboration, version control, security, and compliance. \nThe forgotten step in CRISP-DM and ASUM-DM:\nCRISP-DM repeats itself in ASUM-DM as the development part however it misses an important step being data validation. My CRISP-DM version looks like this. \nWhy data validation?\nData validation happens immediately after data preparation/wrangling and before modeling. it is because during data preparation there is a high possibility of things going wrong especially in complex scenarios. Data validation ensures that modeling happens on the right data. faulty data as input to the model would generate faulty insight!\nHow is data validation done?\nData validation should be done by involving minimum one external person who has a proper understanding of the data and business. In my situation is usually my clients who technically good enough to check my data. Once I go through data preparation and just before data modeling, I usually make data visualization and give my newly prepared data to the client. The clients with the help of SQL queries or any other tools try to validate if my output contains no error. Combing CRISP-DM/ASUM-DM with the agile methodology, steps can be taken in parallel meaning you do not have to wait for the green light for data validation to do the modeling. But once you get feedback from the domain expert that there are faults in the data, you need to correct the data by re-doing the data-preparation and re-model the data.\nWhat are the common causes leading to a faulty output from data preparation?\nCommon causes are: 1. Lack of proper understanding of the data, therefore, the logic of the data preparation is not correct. 2. Common bugs in programming/data preparation pipeline that lead to a faulty output. 3. Data formats that make some troubles within the data-preparation step and generating faulty outputs with no error trace to be caught by the data scientist/engineer during the data-preparation.\nConclusion:\nIn this article, I would like to extend the CRISP-DM/ASUM-DM by adding a new step. The whole idea of these methodologies is to formalize the steps helping the data-scientists/data-miners to improve the success of the projects and reduce failures. In my CRISP-DM version, \u201cdata validation\u201d step is added which ensures even more success of the project and prevents, even more, the failures and faults of any data-science/data-mining projects.\n", "tags": [], "categories": ["Blog", "Development"]}
{"post_id": 1365, "title": "Documenting Hypermedia REST APIs with Spring REST Docs", "url": "https://www.luminis.eu/blog-en/data-en/documenting-hypermedia-rest-apis-with-spring-rest-docs/", "updated_at": "2020-11-11T16:49:01", "body": "Last year, at the end of summer, the project I was working on required a public REST API. During the requirements gathering phase we discussed the \u2018level\u2019 of our future REST API. In case you\u2019re unfamiliar with Leonard Richardson\u2019s REST maturity model I would highly recommend reading\u00a0this article written by Martin Fowler\u00a0about the model.\nIn my opinion a public API requires really good documentation. The documentation helps to explain how to use the API, what the resource represents (explain your domain model) and can help to increase adoption of the API. If I have to consume an API myself I\u2019m always relieved if there is some well written API documentation available.\nAfter the design phase we chose to build a Level 3 REST API. Documenting a level 3 REST api is not that easy. We looked at Swagger / OpenAPI, but in the 2.0 version of the spec, which was available at the time, it was not possible to design and or document link relations, which are part of the third level. After some research we learned there was a Spring project called\u00a0Spring REST Docs, which allowed you to document any type of API. It works by writing tests for your API endpoints and acts as a proxy which captures the requests and responses and turns them into documentation. It does not only look at the request and response cycle, but actually inspects and validates if you\u2019ve documented certain request or response fields. If you haven\u2019t specified and documented them, your actual test will fail. This is really neat feature! It makes sure that your documentation is always in sync with your API.\nUsing Spring REST Docs is pretty straight-forward. You can start by just adding a dependency to your Maven or Gradle based project.\n\r\n  org.springframework.restdocs\r\n  spring-restdocs-mockmvc\r\n  ${spring.restdoc.version}\r\n  test\r\n\nNow when you use for instance Spring MockMVC you can test an API resource by having the following code:\n@Test \r\npublic void testGetAllPlanets() throws Exception { \r\n    mockMvc.perform(get(\"/planets\").accept(MediaType.APPLICATION_JSON)) \r\n    .andExpect(status().isOk())\r\n    .andExpect(jsonPath(\"$.length()\",is(2))); \r\n} \nAll the test does is performing a GET request on the /planets resource. Now to document this API resource all you need to do is add the document() call with an identifier, which will result in documentation for the /planets resource.\n@Test\r\npublic void testGetAllPlanets() throws Exception {\r\n    mockMvc.perform(get(\"/planets\").accept(MediaType.APPLICATION_JSON))\r\n        .andExpect(status().isOk())\r\n        .andExpect(jsonPath(\"$.length()\",is(2)))\r\n        .andDo(document(\"planet-list\"));\r\n}\nNow when you run this test, Spring REST Docs will generate several AsciiDoc snippets for this API resource.\n\nLet\u2019s inspect one of these asciidoc snippets.\n\n[source,bash]\r\n----\r\n$ curl 'https://api.mydomain.com/v1/planets' -i -X GET \\\r\n    -H 'Accept: application/hal+json'\r\n----\nLooks pretty neat right? It generates a nice example of how to perform a request against the API by using\u00a0curl. It will show what headers are required or in case you want to send a payload how to pass the payload along with the request.\nDocumenting how to perform an API call is nice, but it gets even better when we start documenting fields. By documenting fields in the request or response we will immediately start validating the documentation for missing fields or parameters. For documenting fields in the JSON response body we can use the\u00a0responseFields\u00a0snippet instruction.\n\n@Test\r\npublic void testGetPerson() throws Exception {\r\n  mockMvc.perform(get(\"/people/{id}\", personFixture.getId())\r\n         .accept(MediaTypes.HAL_JSON_VALUE))\r\n         .andExpect(status().isOk())\r\n         .andDo(document(\"people-get-example\",\r\n                pathParameters(\r\n                    parameterWithName(\"id\").description(\"Person's id\")\r\n                ),\r\n                links(halLinks(),\r\n                      linkWithRel(\"self\").ignored()\r\n                ),\r\n                responseFields(\r\n                        fieldWithPath(\"id\").description(\"Person's id\"),\r\n                        fieldWithPath(\"name\").description(\"Person's name\"),\r\n                        subsectionWithPath(\"_links\").ignored()\r\n                 ))\r\n          );\r\n    }\nIn the above example we have documented 2 fields:\u00a0id\u00a0and\u00a0name. We can add a description, but also a type, specify if they are optional or we can even ignore specific sections like I did in the above example. Ignoring a section is possible in case you want to document them once since they will be available across multiple resources. Now if you are very strict with writing JavaDoc you might also want to consider using\u00a0Spring Auto REST Docs. Spring Auto REST Docs uses introspection of you Java classes and POJOs to generate the field descriptions for you. It\u2019s pretty neat, but I found some corner cases for when you use a hypermedia API. You can\u2019t really create specific documentation for Link objects. The documentation comes from the Spring Javadocs itself, so we chose to leave auto rest docs out.\nHaving a bunch of asciidoc snippets is nice, but it\u2019s better to have some human readable format like HTML. This is where the maven asciidoctor plugin comes in. It has the ability to process the asciidoc files and turn it into a publishable format like HTML or PDF. To get the HTML output (also known as backend) all you need to do is add the maven plugin with the correct configuration.\n\n\r\n  \r\n    ....\r\n     \r\n      org.asciidoctor\r\n      asciidoctor-maven-plugin\r\n      1.5.3\r\n      \r\n        \r\n          generate-docs\r\n          prepare-package \r\n          \r\n            process-asciidoc\r\n          \r\n          \r\n            html\r\n            book\r\n          \r\n        \r\n      \r\n      \r\n         \r\n          org.springframework.restdocs\r\n          spring-restdocs-asciidoctor\r\n          2.0.1.RELEASE\r\n        \r\n      \r\n    \r\n  \nNow to turn all the different asciidoc snippets into once single documentation page you can create an index.adoc file that aggregates the generated AsciiDoc snippets into a single file. Let\u2019s take a look at an example:\n= DevCon REST TDD Demo\r\nJeroen Reijn;\r\n:doctype: book\r\n:icons: font\r\n:source-highlighter: highlightjs\r\n:toc: left\r\n:toclevels: 4\r\n:sectlinks:\r\n:operation-curl-request-title: Example request\r\n:operation-http-response-title: Example response\r\n\r\n[[resources-planets]]\r\n== Planets\r\n\r\nThe Planets resources is used to create and list planets\r\n\r\n[[resources-planets-list]]\r\n=== Listing planets\r\n\r\nA `GET` request will list all of the service's planets.\r\n\r\noperation::planets-list-example[snippets='response-fields,curl-request,http-response']\r\n\r\n[[resources-planets-create]]\r\n=== Creating a planet\r\n\r\nA `POST` request is used to create a planet.\r\n\r\noperation::planets-create-example[snippets='request-fields,curl-request,http-response']\nThe above asciidoc snippet shows you how to write documentation in asciidoc and how to include certain operations and even how you can selectively pick certain snippets which you want to include. You can see the result in\u00a0the Github pages version.\nThe advantage of splitting the generation from the actual HTML production has several benefits. One that I found appealing myself is that by documenting the API in two steps (code and documentation) you can have multiple people working on writing the documentation. At my previous company we had a dedicated technical writer that wrote the documentation for our product. An API is also a product so you can have engineers create the API, tests the API and document the resources by generate the documentation snippets and the technical writer can then do their own tick when it comes to writing good readable/consumable content. Writing documentation is a trade by itself and I have always liked\u00a0the mailchimp content style guide\u00a0for some clear guidelines on writing technical documentation.\nNow if we take a look at the overall process we will see it integrates nicely into our CI / CD pipeline. All documentation is version control managed and part of the same release cycle of the API itself.\n\nIf you want to take look at a working example you can check out my\u00a0DevCon REST TDD demo repositoryon github or see me use Spring Rest Docs to\u00a0live code\u00a0and document an API during my talk at DevCon.\n", "tags": [], "categories": ["Blog", "Data"]}
{"post_id": 17326, "title": "Luminis\u2019 Cloud platform now on UK Government G-Cloud 10", "url": "https://www.luminis.eu/blog-en/luminis-cloud-platform-now-uk-government-g-cloud-10/", "updated_at": "2018-07-19T11:10:38", "body": "Software technology company Luminis registered their seminal Cloud platform on the UK Government\u2019s G-Cloud Digital Marketplace. Government Cloud (G-Cloud 10) is a UK government initiative to ease procurement of cloud services by government departments and to promote government-wide adoption of cloud computing. \nLuminis UK proactively encourages departments and authorities across the United Kingdom to use more Open Source technology in the Cloud. Luminis\u2019 Cloud platform, The Cloud RTI, consists of a Cloud platform with a host of supporting tools that have been designed to help organizations in making the transition to Cloud computing, while benefiting from the advantages and insights of the Open Source community. Cloud RTI is infrastructure-agnostic and allows software to run on infrastructure from multiple providers including both Microsoft Azure and Amazon AWS.\nThe\u00a0Luminis Cloud RTI\u00a0incorporates the latest practices and insights in Cloud computing from the Open Source community; it is designed to help organizations to make the transition to Cloud computing whilst allowing them the maximum freedom of technology choices. As an example, Cloud RTI supports the ability to implement hybrid Cloud strategies, ranging from multiple public Cloud providers up to and including on-premise and private cloud (if required).\nIn January 2016, the Dutch software technology company Luminis selected the United Kingdom to establish their first operation outside the Netherlands as part of their international ambitions. \n\u00a0\n\n", "tags": [], "categories": ["Blog"]}
{"post_id": 1424, "title": "Scrum: Magic Estimation", "url": "https://www.luminis.eu/blog-en/development-en/scrum-magic-estimation/", "updated_at": "2020-11-17T09:59:03", "body": " Sometimes your Product Owner or the business wants to know how much work there is on the Product Backlog. They want to get an idea for the Roadmap ahead. Or perhaps to acquire some (internal) funds. Or as a Scrum Team, you want to know what is coming. So there could be several reasons why you want to get an estimation of unestimated Product Backlog Items (PBI).\nFor this, there is a method called \u2018magic estimation\u2019. In this blog post, I will describe\u00a0a\u00a0method and my experience with it as a Scrum Master. I have done it with my Scrum Team for several Product Backlogs. We were already developing the products, so we had a baseline and feeling with the products and our estimations.\nFirst things first\nPreparing this session properly makes the time the whole team sits together more efficient. A boundary condition is to have the titles of the Product Backlog Items\u00a0clearly\u00a0formulated. There must be as little room for interpretation as possible. Print for every PBI the title on paper. Then cut this so you have a small strip for every PBI. Make sure your team has a baseline. Otherwise, determine one in the session together.\nSession\n\nLay down one set of Fibonacci numbers of the planning poker cards in order on a table including the question mark. Leave enough space between the cards.\nGive every team member a set of about the same amount of strips of unique PBIs.\nExplain the process and rules to the team. It is not allowed to talk or practice non-verbal communication unless told so.\nIf the product is not known to the team, let the Product Owner share her/his vision of the product.\nLet every team member, in turn, put every PBI of her/his stock at the number at which (s)he estimates it. You can let the team member read the title of the PBI first aloud. In my experience, this costs extra time, but the upside is that every team member has heard of every item on the Product Backlog. This way the team gets more feeling with what is out there. Everybody else is not allowed to give any kind of reaction. Write the number on the strip of paper.\n\nIf someone does not know what is meant by a PBI, it can be placed at the question mark. This is treated later in the next step.\nAlso write the question mark on the strip of the PBI. Perhaps the Product Owner wants to reformulate the title afterward to make it more clear.\n\n\nThe PBIs placed at the question mark are now explained by the Product Owner. If the intention is clear, they can be divided amongst the group to be placed at a number. You can do this immediately or in the second round (see below).\nNow starts the second round. This is done in total silence and without non-verbal communication. Every team member can simultaneously pick up a PBI and replace it. He can confirm the magic estimation by placing it at the same number or give it a different magic estimation by placing it at a different number. He also writes down the (new) number after the previous magic estimation. This way it is clear which PBIs have already passed round two. This is done until all PBIs have an extra number. Everybody can do it at her/his own speed, so some will replace more PBIs than others. But the session can continue faster this way.\nNow do another round. You can do more rounds if you want, but for us, three works well to get enough magic estimations per PBI to make a conclusion and for the session not to be too long.\nRoundup and draw conclusions in two steps:\n\nFor PBIs that have changed slightly, for example, within a range of three following Fibonacci numbers, calculate the mean and write that down as the final magic estimation. This range can be [1,2,3] or [3,5,8]. If you think the distance between 3 and 8 is too big, you can also make ranges of a maximum of 3 story points, e.g. [2-5].\nPBIs that have changed more than in the previous step need to be discussed centrally. Let every team member who has written a number on it explain the reason behind it in one sentence. Then try to discuss a common magic estimation. Keep this discussion compact, because it is not a regular refinement. If it is not possible to reach an agreement take the mean of the magic estimations that remain standing.\n\n\n\nOutcome\nAll PBIs have a magic estimation. Everybody has an idea of the amount of work on the Product Backlog. But what is still unknown is the hidden work that surfaces when the team actually develops the PBIs. Keep that in mind, because it is a rougher estimation than the regular estimation.\nMy sources of inspiration\n\nYour experiences with magic estimation. Please tell me yours in the comments.\nhttps://www.scrum.nl/blog/magic-estimation/\nhttps://www.agilecockpit.com/magic-estimation/\nhttps://campey.blogspot.com/2010/09/magic-estimation.html\nhttps://blog.mikepearce.net/2011/05/02/how-do-i-estimate-how-long-an-agile-project-will-take-before-we-start/\n\n", "tags": [], "categories": ["Blog", "Development"]}
{"post_id": 1221, "title": "JMeter your testing application using Services Web Amazon, Docker", "url": "https://www.luminis.eu/blog-en/development-en/jmeter-your-testing-application-using-services-web-amazon-docker/", "updated_at": "2020-11-17T10:18:17", "body": " Oops, I guess I should have tested that title better. I wasn\u2019t expecting that many viewers to concurrently view this blog. If only I had performed load testing beforehand! Hopefully it\u2019s not too late for you and your application. Learn from my mitsake and test how your application will react when many users access it. When building your application, you probably test your application in a lot of ways, such as unit testing or simply just running the application and checking if it does remotely what you expect it to do. If this succeeds you are happy. Hooray for you, it works for one person! The title of this blog also looked good when only one person was viewing it!\nOf course, you\u2019re not in the business of making web applications that will only be used by just one person. Your app is going to be a success with millions of users. Can your application handle it? I don\u2019t know, you don\u2019t know\u2026nobody knows! Read the rest of this blog post to see how we tested an application we are working on at Luminis and found out for ourselves.\nWhy and how?\nWhy? Here\u2019s why! I work on a project team at Luminis and our job is to make sure that users of a certain type of connected device can always communicate with said device via their smart phones. There are thousands of these devices online at the moment and that number is just going to keep increasing. So instead of just hoping for the best, we decided to take matters into our own hands and see for ourselves just how robust and scalable our web applications really are.\nHow? Here\u2019s How:\n\nApache JMeter\nJMeter WebSocket Plugin by my colleague Peter Doornbosch\nDocker\nAmazon Web Services (Elastic Container service, Fargate, S3 and CloudWatch)\nA lot of swearing when things don\u2019t go the way you expect\n\n\u00a0\nApache JMeter\nI\u2019ll let the JMeter doc speak for itself:\n\u201cThe\u00a0Apache JMeter\u2122\u00a0application is open source software, a 100% pure Java application designed to load test functional behavior and measure performance. It was originally designed for testing Web Applications but has since expanded to other test functions.\u201d\nI\u2019m sure that description got you in the mood to start using JMeter immediately (/endSarcasm). The best way to look at JMeter for now is to think of it as a REST client that you can set up to do multiple requests at the same time instead of just performing one. You can create multiple threads that will run concurrently, configure the requests in many ways and view the results afterwards in many types of views.\nDownload and install it from here: https://jmeter.apache.org/\nWhen you startup JMeter, it looks like this:\n\nI know what you\u2019re thinking. \u201cMine doesn\u2019t/won\u2019t look like that, I have a Windows PC\u201d. Don\u2019t worry, it works there too (provided you have Java installed). It\u2019s just Java!\nThere are a lot of things you can do in JMeter, however I\u2019m going to focus on the essentials you need to perform a load test on your REST endpoints. In the next section I will also show how to perform load tests if you use WebSockets.\nLet\u2019s just ignore all of the buttons for a while and just focus on the item called \u201cTest Plan\u201d, which is highlighted in the first image. You can leave the name as it is, I personally never change it. I just change the name of the .jmx file, which is the extension for JMeter scripts.\nThread Group\nRight clicking on the Test plan will bring up the context menu. The first thing you want to do is create a thread group, which will be used to create multiple threads making it possible to make requests in parallel:\n\n\nThe most important settings here are:\n\nNumber of threads (users): Number of threads that will be started when this test is started. What exactly these threads will do will be defined later on.\nRamp-Up Period (in seconds): The number of seconds before all threads are started. JMeter will try to spread the start of the thread evenly within the ramp-up period. So if Ramp-Up is set to 1 second and Number of threads is 5, JMeter will start a thread every 0.2 seconds.\nLoop count:How many times you want the threads to repeat whatever it is you want it to do. I usually leave this set to 1 and just put loops later on in the chain.\n\nBy default, the thread group is set with 1 thread, 1 second ramp up and 1 loop count. Let\u2019s leave that like that for now.\nHTTP Request Sampler\nNow right click on the thread group and create a HTTP Request sampler:\n\n\nThis should (maybe) look familiar! It kind of looks like every REST client ever, such as Postman.\nThere\u2019s a button labeled \u201cadvanced\u201d. I never clicked it, neither should you (yet).\nFill in the following:\n\nProtocol [http]:http \u2014- Can also be https if you have a certificate installed.\nServer name or IP: localhost \u2014- It can also be a remote IP or your server name if you have one registered.\nPort number: 8151 \u2014- Or any port your application is listening to.\nMethod: GET \u2014- This can also be any other HTTP method. If you select POST, you can also add the POST body.\nPath: <<path to an endpoint you want to test>> \u2014- Example: /alexa/ temperature\nParameters: Here you can add query parameters for your request, which would usually look like this in your URL /alexa/temperature?parameterName1=paramterValue1\u00b6meterName2=paramterValue2\nBody data: Where you can add the body data in whatever format you need to send your POST body data. Since we are going to test a GET endpoint, leave this empty\u00a0for now.\n\nWhen you\u2019re done, it should look like this:\n\nYou could just press the play button now and it will perform exactly one GET request to the endpoint specified. However, you won\u2019t get much feedback in JMeter at the moment. Let\u2019s add a few more items to our test plan.\nHTTP Header Manager\nYou might want to add some headers to your HTTP request. To do so, right click on the HTTP Request sampler and select HTTP Header Manager:\n\nHere you can add your HTTP headers. Some examples are \u201cAuthorization\u201d where you might send an authentication token, or the \u201cContent-Type\u201d header when sending POST body data:\n\nView results\nWe\u2019re almost there! When running your JMeter script, you probably want to see the result of each action request. Right click on the Test Plan > Add > \u00a0Listeners and add the following two items:\n\nView results in Tree\nView results in Table\n\nAfter running the script by pressing on the green play button, this is how these two views will display the results:\n\nHere we can immediately see that the request was successful due to the green icon next to it in the tree. The sampler results give us a lot of extra info, including the status code. We can also check the request we sent and the response data we get back by clicking on those tabs. Here we can see that the response data is 20.0, which is what I programmed my mock object to return:\n\nThe table view looks like this:\n\nAnd if I run a request I know will fail, by sending an invalid token for example, then the table looks like this:\n\nIf you want to clear all your results, click on the button at the top with the cog and two brooms (clear all).\nUser defined variables\nWe\u2019ve just added a lot of configuration in JMeter, however it is also possible to add a list of variables that you define yourself and use them throughout JMeter. This is needed for later on when we want to run JMeter as a script and not from the GUI. To do this, right click on the Test plan > Add > Config Element and select User defined variables.\nLet\u2019s say I add the following variables that we previously entered into JMeter:\n\nNow that I have these four variables set up, I can refer to them in the following way:\n$(parameterName)\nThis means that if I want to reference the \u201cnumberOfThreads\u201d variable, I will add $(numberOfThreads) and it will use \u201c1\u201d in this case. With these variables, our Thread Group configuration looks like this:\n\nWe\u2019ll get back to these variables in a bit.\nCSV Data Set Config\nUp to this point, we have been running this request with a hardcoded authentication token. However, once we want to start running multiple requests and perform our load test, we might want to send a different authentication token per connection. This is possible by having a .csv file with these tokens and reading them in using the \u201cCSV Data Set Config\u201d.\nTo add this, right click on the Test plan > Add > Config element and select \u201cCSV Data Set Config\u201d.\nLet\u2019s say we have a .csv file that looks like this:\n1, <<token1>>\n2, <<token2>>\n3, <<token3>>\n\u2026etc\nIf we set the CSV Data Set Config up in the following way, we can use the tokens per connection:\n\nFilename: Relative path to the .csv file (from the .jmx script)\nFile-encoding: UTF-8 \u2014- I\u2019m not going to explain file encoding.\nVariable Names (comma-delimited): id, token\nRecycle on EOF?: False \u2014- If set to true, if your numberOfThreads > numberOfTokensInCSVFile, it will startover from the top of the file when it runs out of tokens to use.\nStop Thread on EOF?: True \u2014 This is because we don\u2019t want to have more threads than the number of tokens we have in our .csv file\n\nWhat we\u2019ve accomplished with this is that we\u2019ve created two new variables, namely \u201cid\u201d and \u201ctoken\u201d which is available to use through JMeter. This is similar to the User Defined Variables and can be accessed in the same way ($(token) for example). Our HTTP Header Manager config now looks like this:\n\nNow you should be ready to run multiple connections. Edit the numberOfThreads and rampUpTime in the User Defined Variables and see what happens! It probably won\u2019t work in the first try and it will probably be your fault. Look at the errors in the view results listeners to see what the cause of the problem is.\nRun JMeter in Non-GUI mode\nWe\u2019ve seen how to setup a JMeter test and run it in the JMeter GUI. However, when you want to perform this at a large scale, you will probably want to run it on a server somewhere in a non-GUI mode as a script.\nIn order to do this, you need to open your script one more time in JMeter and make a small adjustment. In the User Defined Variables, change the \u201cnumberOfThreads\u201d value to:\n${__P(numberOfThreads)}\nThis means that the value for this variable will be passed on to the script when it is called. You can do this with all your variables, but for now let\u2019s change this one.\nNavigate to the .jmx file location in the command line and run the following:\n./apache-jmeter-3.3/bin/jmeter -n \\\r\n    -t ./my_script.jmx \\\r\n    -j ./ my_script.log \\\r\n    -l ./my_script.xml \\\r\n    -Jjmeter.save.saveservice.output_format=xml \\\r\n    -Jjmeter.save.saveservice.response_data=true \\\r\n    -Jjmeter.save.saveservice.samplerData=true \\\r\n    -JnumberOfThreads=1 && \\\r\n    echo -e \"\\n\\n===== TEST LOGS =====\\n\\n\" && \\\r\n    cat my_script.log && \\\r\n    echo -e \"\\n\\n===== TEST RESULTS =====\\n\\n\" && \\\r\n    my_script.xml\nNo, I don\u2019t know what the Windows equivalent is of this. An explanation of what just happened:\n\n./apache-jmeter-3.3/bin/jmeter: This is the location to my JMeter when running this script. You should set the path to your JMeter.\n-t: The location of the .jmx file.\n-j: The location the log file should be saved\n-l: The location the results .xml file should be saved\nsave.saveservice.output_format: The format to save the results in. XML is a good choice because you can load it into the JMeter GUI results to view them.\nJnumberOfThreads: This is the value will be passing which will be mapped to the \u201cnumberOfThreads\u201d variables in our script. If you want to pass more values, be sure to add a CAPITAL J before the variable name.\n\nThe rest of the command is to just show output immediately to the command line. If you run this in the background, you can always follow the progress in the .log file.\nWebSockets\nI\u2019ve started this story by explaining why our team looked into load testing. The connected devices use a WebSocket connection to connect to our backend application. In order to test this, we couldn\u2019t use the HTTP Request Sampler. We needed a WebSocket Sampler. JMeter doesn\u2019t come with such a sampler by default, so our colleague Peter Doornbosch decided to make his own JMeter Sampler.\nYou can find instructions on how to install this sampler into JMeter on the GitHub page for his sampler: https://github.com/ptrd/jmeter-websocket-samplers. Be sure to give it a star!\nYou can add the samplers in the same way you would add the HTTP Request Sampler. There are multiple WebSocket samplers you can use, however the one we use the most are the \u201cWebSocket Open Connection\u201d and the \u201cWebSocket request-response Sampler\u201d.\nThe first allows us to open a WebSocket connection with a host. It is very straight-forward, similar to the HTTP request sampler. The \u201cWebSocket request-response Sampler\u201d allows us to send a message via the WebSocket connection created. You can send text or binary. Again, this is very straight-forward.\nFor any more explanation on this Sampler and how to do more complicated things like Secury WebSockets, see the documentation on GitHub.\nThere are a lot more things you can do with JMeter, however those will remain out of scope of this blog because it\u2019s my blog and I said so.\nDocker\nI\u2019m not going to explain what Docker is in this blog. If you don\u2019t know or want a refresher, view this page: https://www.docker.com/what-docker\nOur Dockerfile looks like this:\n\n\n# Use a minimal base image with OpenJDK installed\r\nFROM openjdk:8-jre-alpine3.7\r\n\r\n# Install packages\r\nRUN apk update && \\\r\n    apk add ca-certificates wget python python-dev py-pip && \\\r\n    update-ca-certificates && \\\r\n    pip install --upgrade --user awscli\r\n\r\n# Set variables\r\nENV JMETER_HOME=/usr/share/apache-jmeter \\\r\n    JMETER_VERSION=3.3 \\\r\n    WEB_SOCKET_SAMPLER_VERSION=1.2 \\\r\n    TEST_SCRIPT_FILE=/var/jmeter/test.jmx \\\r\n    TEST_LOG_FILE=/var/jmeter/test.log \\\r\n    TEST_RESULTS_FILE=/var/jmeter/test-result.xml \\\r\n    USE_CACHED_SSL_CONTEXT=false \\\r\n    NUMBER_OF_THREADS=1000 \\\r\n    RAMP_UP_TIME=25 \\\r\n    CERTIFICATES_FILE=/var/jmeter/certificates.csv \\\r\n    KEYSTORE_FILE=/var/jmeter/keystore.jks \\\r\n    KEYSTORE_PASSWORD=secret \\\r\n    HOST=your.host.com \\\r\n    PORT=443 \\\r\n    OPEN_CONNECTION_WAIT_TIME=5000 \\\r\n    OPEN_CONNECTION_TIMEOUT=20000 \\\r\n    OPEN_CONNECTION_READ_TIMEOUT=6000 \\\r\n    NUMBER_OF_MESSAGES=8 \\\r\n    DATA_TO_SEND=cafebabecafebabe \\\r\n    BEFORE_SEND_DATA_WAIT_TIME=5000 \\\r\n    SEND_DATA_WAIT_TIME=1000 \\\r\n    SEND_DATA_READ_TIMEOUT=6000 \\\r\n    CLOSE_CONNECTION_WAIT_TIME=5000 \\\r\n    CLOSE_CONNECTION_READ_TIMEOUT=6000 \\\r\n    AWS_ACCESS_KEY_ID=EXAMPLE \\\r\n    AWS_SECRET_ACCESS_KEY=EXAMPLEKEY \\\r\n    AWS_DEFAULT_REGION=eu-central-1 \\\r\n    PATH=\"~/.local/bin:$PATH\" \\\r\n    JVM_ARGS=\"-Xms2048m -Xmx4096m -XX:NewSize=1024m -XX:MaxNewSize=2048m -Duser.timezone=UTC\"\r\n\r\n# Install Apache JMeter\r\nRUN wget http://archive.apache.org/dist/jmeter/binaries/apache-jmeter-${JMETER_VERSION}.tgz && \\\r\n    tar zxvf apache-jmeter-${JMETER_VERSION}.tgz && \\\r\n    rm -f apache-jmeter-${JMETER_VERSION}.tgz && \\\r\n    mv apache-jmeter-${JMETER_VERSION} ${JMETER_HOME}\r\n\r\n# Install WebSocket samplers\r\nRUN wget https://bitbucket.org/pjtr/jmeter-websocket-samplers/downloads/JMeterWebSocketSamplers-${WEB_SOCKET_SAMPLER_VERSION}.jar && \\\r\n    mv JMeterWebSocketSamplers-${WEB_SOCKET_SAMPLER_VERSION}.jar ${JMETER_HOME}/lib/ext\r\n\r\n# Copy test plan\r\nCOPY NonGUITests.jmx ${TEST_SCRIPT_FILE}\r\n\r\n# Copy keystore and table\r\nCOPY certs.jks ${KEYSTORE_FILE}\r\nCOPY certs.csv ${CERTIFICATES_FILE}\r\n\r\n# Expose port\r\nEXPOSE 443\r\n\r\n# The main command, where several things happen:\r\n# - Empty the log and result files\r\n# - Start the JMeter script\r\n# - Echo the log and result files' contents\r\nCMD echo -n > $TEST_LOG_FILE && \\\r\n    echo -n > $TEST_RESULTS_FILE && \\\r\n    export PATH=~/.local/bin:$PATH && \\\r\n    $JMETER_HOME/bin/jmeter -n \\\r\n    -t=$TEST_SCRIPT_FILE \\\r\n    -j=$TEST_LOG_FILE \\\r\n    -l=$TEST_RESULTS_FILE \\\r\n    -Djavax.net.ssl.keyStore=$KEYSTORE_FILE \\\r\n    -Djavax.net.ssl.keyStorePassword=$KEYSTORE_PASSWORD \\\r\n    -Jhttps.use.cached.ssl.context=$USE_CACHED_SSL_CONTEXT \\\r\n    -Jjmeter.save.saveservice.output_format=xml \\\r\n    -Jjmeter.save.saveservice.response_data=true \\\r\n    -Jjmeter.save.saveservice.samplerData=true \\\r\n    -JnumberOfThreads=$NUMBER_OF_THREADS \\\r\n    -JrampUpTime=$RAMP_UP_TIME \\\r\n    -JcertFile=$CERTIFICATES_FILE \\\r\n    -Jhost=$HOST \\\r\n    -Jport=$PORT \\\r\n    -JopenConnectionWaitTime=$OPEN_CONNECTION_WAIT_TIME \\\r\n    -JopenConnectionConnectTimeout=$OPEN_CONNECTION_TIMEOUT \\\r\n    -JopenConnectionReadTimeout=$OPEN_CONNECTION_READ_TIMEOUT \\\r\n    -JnumberOfMessages=$NUMBER_OF_MESSAGES \\\r\n    -JdataToSend=$DATA_TO_SEND \\\r\n    -JbeforeSendDataWaitTime=$BEFORE_SEND_DATA_WAIT_TIME \\\r\n    -JsendDataWaitTime=$SEND_DATA_WAIT_TIME \\\r\n    -JsendDataReadTimeout=$SEND_DATA_READ_TIMEOUT \\\r\n    -JcloseConnectionWaitTime=$CLOSE_CONNECTION_WAIT_TIME \\\r\n    -JcloseConnectionReadTimeout=$CLOSE_CONNECTION_READ_TIMEOUT && \\\r\n    aws s3 cp $TEST_LOG_FILE s3://performance-test-logging/uploads/ && \\\r\n    aws s3 cp $TEST_RESULTS_FILE s3://performance-test-logging/uploads/ && \\\r\n    echo -e \"\\n\\n===== TEST LOGS =====\\n\\n\" && \\\r\n    cat $TEST_LOG_FILE && \\\r\n    echo -e \"\\n\\n===== TEST RESULTS =====\\n\\n\" && \\\r\n    cat $TEST_RESULTS_FILE\nThis file can be divided into 9 sections:\n\nSelect the base Docker image. In this case it was a minimal base image with OpenJDK installed\nSome bootstrap things to be able to install everything we need later.\nSet the environment variables. These will be referenced later in the Docker file. Worth noting:You should add your Amazon keys in this section, so that the result and log file can be copied to Amazon S3. These keys need to be changed:\n\nAWS_ACCESS_KEY_ID=<>\r\nAWS_SECRET_ACCESS_KEY=<>\r\nAWS_DEFAULT_REGION=<>\nThese should be available in account settings.\n\nInstall Apache JMeter.\nInstall the WebSocket Samplers made by Peter Doornbosch\nCopy the test plan (.jmx) to the location indicated in the environment variable. In this case when we build the Docker image, it is in the same directory as the Docker file.\nCopy some keystore information needed for WSS\nExpose port 443. This statement doesn\u2019t actually do anything. It is just for documentation. (see: https://docs.docker.com/engine/reference/builder/#expose)\nThe main command where we run JMeter with all the configurations and values we want to pass. This is similar to the command we used earlier to run JMeter as a script (non-GUI). What we also do here is use the Amazon CLI to copy our JMeter log and result files to Amazon s3 (storage). This will be explained in the next section.\n\nThis is it for the Docker part of things. Hopefully there is enough information in this section for you to setup your own Dockerfile. In the next section, we will see how to build the Docker image and upload it to Amazon and run it there.\nAmazon Web Services\n\nAlright, so now you know how to use JMeter to design your test script and how to create a Docker image that sets up an environment needed to run your script. The reason you would want to run these kinds of tests in a Cloud service such as Amazon Web Services (AWS) in the first place is because your Personal Computer has its limits. For a MacBook Pro for example, we could only simulate around 2100 WebSocket connections before we started getting errors stating that \u201cno more native threads could be created\u201d.\nAWS gives us the ability to run Docker containers on Amazon EC2 clusters which will run the tests for us. In order to do this, you will first need to sign up with AWS: https://portal.aws.amazon.com/billing/signup#/start\nOnce you have your account ready, log in to aws.amazon.com and you will hopefully see something like this:\n\nIn the search bar, type \u201cElastic Container Service\u201d and select it. This will be the service we will use to run our Docker container.\nElastic Container Service\nThe Elastic Container service is an AWS service that allows us to run our Docker containers in the Cloud. There are three things we are going to discuss regarding the Elastic Container Service:\n\nRepository: Where we save our Docker images\nTask Definitions: Where we define how to run our Docker containers\nClusters: Where we start a cluster of VM\u2019s, which will run the Tasks containing the Docker container which contains our JMeter script.\n\nThis is what it looks like:\n\nRepositories\nSelect \u201cRepositories\u201d to get started. As mentioned, this is where we will create a repository to push our Docker image to. Whenever we make a change to our Docker image, we can push to the same repository. Select \u201cCreate repository\u201d.\nFirst thing you have to do is think of a good name for your repository:\n\nNow this next page I personally really like. It is a page which all the Docker commands needed to get your Docker image pushed to this newly created repository. Follow the instructions on this page:\n\nIt\u2019s always nice not having to think too much\u2026\nIf everything on this page goes well, you should see the following:\n\nWith the table at the bottom showing all the versions of the image that have been pushed. If you want to push to this repository again, just click on the \u201cView Push Commands\u201d button to see the Docker commands again. I never really used any of the other tabs or buttons on this page, so let\u2019s ignore those.\nTask Definitions\nGo to Task Definitions and select \u201cCreate a new Task Definition\u201d. When prompted to select a compatibility type, choose \u201cFargate\u201d. On the next page, simply enter a valid name for the task and scroll to the bottom where the button \u201cadd container\u201d is situated. Ignore all the other settings on this page.\nClick on that button and a modal will show up. Fill in the following:\n\nContainer name: You\u2019re good at this by now\nImage: This can be found by going to repositories, clicking on your repository and copying the Repository URI (See last image)\nPort mappings: 443 (if you are using secure, otherwise 80)\nEnvironment Variables: Here you can overwrite any variables that are set in the Docker file. Scroll back up to the Docker section and notice that the Docker file was configured for 1000 threads. If I add an environment variables here with name \u201cNUMBER_OF_THREADS\u201d and set the value to 1500, the test will run with 1500 instead of the default set in the Docker file.\nLog configuration: Make sure Auto-configure CloudWatch logs is on.\n\nLeave everything else open/unchanged.\nOnce you are done with this, click on \u201cAdd\u201d to close the container settings modal and then click on \u201cCreate\u201d to create the task definition. The new task definition should now show up in the list of task definitions. Now we\u2019re ready to run our test!\n\nClusters\nNow we\u2019re going to start a cluster of EC2 instances, which will run our task we just defined. Navigate to Clusters and select \u201cCreate cluster\u201d. You will be prompted with three options. I usually go with EC2 Linux + Networking.\nGive the cluster a name and leave everything else the way it is and select \u201cCreate\u201d. Note:If you are going to be running a lot of requests and notice later on that the EC2 instance type does not have enough memory or CPU, you can select a larger instance type.See this page for more info: https://aws.amazon.com/ec2/instance-types/\nIf everything works out you should see a lot of green:\n\nSelect \u201cView Cluster\u201d to go to the Cluster management page:\n\nThe last thing to do is to open the \u201cTask\u201d tab, select run new task and fill in the following:\n\nLaunch type: FARGATE\nTask definition: The task we defined in the previous sub-section\nCluster: The cluster we just created\nNumber of tasks: 1 \u2014- You could add more, however this would mean that you would have the n-times EC2 instances performing the same operations.\nCluster VPC:Select the first one\nSubnets: Select the first one\n\nThe rest can be left unedited. Note: You can still overwrite environment variables for this run specifically by clicking on \u201cAdvanced Options\u201d and clicking on \u201cContainer Overrides\u201d. You can change any of the values there. Select \u201cRun Task\u201d to finally run your test. Viola! Your test is now running:\n\nNOTE:The JMeter Log file and .xml file will be copied to S3 \u201cbucket\u201d specified in the Docker file:\naws s3 cp $TEST_LOG_FILE s3://performance-test-logging/uploads/ && \\\r\naws s3 cp $TEST_RESULTS_FILE s3://performance-test-logging/uploads/ && \\\nS3 is another AWS service used for storage. You can find it under the list of services, in the same way that you found the Elastic Container service.\nCloudWatch\nRemember when I told you to make sure that auto config was on for Cloud Watch? This is where it comes in handy. CloudWatch is a service that offers many monitoring capabilities, including log streaming and storing. In this case, we are interested in following our tests as they are being run.\nClick on the Amazon logo to go back to the screen with all the services, type in CloudWatch and select it. It should look something like this:\n\nOn the left navigation bar, select \u201cLogs\u201d. You should see a table with log groups. One of the log groups should have the name of your Task Definition that you defined earlier. Click on it to see a list of logs for test runs that you have performed. When clicking on one, you will be able to see the logging of that test. It is also possible to the see the live logging as the test is being performed:\n\nAs the test is running, the log file becomes pretty big and hard to follow. Here are some keywords you can type into the \u201cfilter events\u201d text field in order to get some useful info of the test that is currently running:\n\n\u201csummary\u201d: This will show you the JMeter summar of the test, including the number of error percentages.\n\u201cjmeter\u201d: To see a list of all the parameters used for this test\n\u201cerror\u201d: To see a list of all the errors. You can click on an error log to see more info.\n\nSome advice based on experience\nHere is a list of some things we figured out while running these tests on our application:\n\nFeedback is important. Make sure you have enough feedback to be sure that your tests are being performed the way you expect. Some of the things you can do we already covered in this blog, such as setting up CloudWatch and copying the .log and .xml files to s3 to be viewed later on. Other valuable feedback is logging in your application. Set it to Debug mode if possible during these tests. If you have any type of monitoring that can check the server and how many open TCP connections it has would also be great for proving that your tests are successful if you are using WebSockets.\nYou might get errors because of your client rather than your application on the server. If you try to do too many requests or create too many websocket connections from one AWS EC2 instance, you might start to run into client side errors. For us, we could only run around 5000 WebSocket connections per instance. What we ended up doing was creating separate containers and task definitions per 5000 connections and running those tasks simultaneously.\nDon\u2019t only test your application for number of connections/request, but also test the connection/request rate it can handle. This means decreasing the ramp up time in order to increase the number of connections/requests per second. This is important to know, as your applications might be challenged with this connection rate in the future. For us it was important, since restarting the application meant that all connected devices would start connecting within a certain time period.\nA single TCP port can handle 64k open connections. If you want to perform a test where you will need more than this, you will have to use another port and have your application listen to both ports. You will also need to have a load balancer of some kind which can distribute the load between the ports.\n\nConclusion\nI hope this blog has given you an idea of what is possible when combining these three technologies. I know in a lot of the sections I simply told you to do something without giving much of an explanation as to what everything does. There are a lot of things you can do with JMeter, Docker and AWS and I encourage you to look them up and find what works for you. This setup worked for our case, and we are planning on running a test with 100k connections in the near future using this stack. Thanks for reading!\n\u00a0\n\u00a0\n", "tags": [], "categories": ["Blog", "Development"]}
{"post_id": 1425, "title": "Scrum: Daily Stand-Up", "url": "https://www.luminis.eu/blog-en/development-en/scrum-daily-stand-up/", "updated_at": "2020-11-17T09:54:47", "body": " Has your\u00a0Daily Scrum\u00a0become a bit boring? Is everybody just\u00a0sending, but not\u00a0listening? Has the\u00a0interaction\u00a0been lost? Then it\u2019s time to spice up your stand-up!\nIt wasn\u2019t this awful in my team, but during one of our\u00a0Sprint Retrospective\u00a0we discussed our Daily Scrums. The interaction was a little bit lost. One of the Agile principles is:\u00a0Individuals and interactions over processes and tools. The pitfall of the Daily is that it can feel like a mandatory part of the process instead of an opportunity to exchange information and help each other. A part of that mandatory feeling is answering the three questions: What did I do since the last Daily? What am I going to do until the next Daily? Do I encounter any\u00a0impediments?\nIn a previous team it even felt like reporting that you actually worked. Off course we kept the Daily and the three questions. But we changed the format to make it more dynamic. Every team member used to answer the questions for him or her in turn. Now we put more\u00a0focus\u00a0on the Sprint progress and the progress of completing Story\u2019s and deploying releases. We did this by addressing each lane on the Scrum Board separately. On each lane there is either a Story, the current release, or non-blocking bugs on the bugs lane and some independent tasks on the support lane. We created the bugs and support lanes to keep things transparent, even if team members were working outside the Story\u2019s we\u2019ve got in a Sprint. Also the deployments of the release to Acceptance and Production needs some work of the team. Therefore we created a separate lane for it as well. By addressing each lane separately we keep focus on progress on the completion of the work of every lane. Now every team member answers the three questions in regard to the lane. For example: What did I do since the last Daily for this Story? What am I going to do until the next Daily for this Story? Do I encounter any impediments while working on this Story? The answers also include: When do I expect to be done with a particular work item? Such that another team member can review it. Or it can be tested. Or somebody else can use it for another work item. This works especially well if back-end and front-end work is done by specialized developers. This improved interaction between team members (individuals) to tune their work to each other. At our next Sprint Retrospective we evaluated this change. Everybody was really positive about the extra interaction it brought. Besides that the focus on completion led to a more steady burn-down. We kept it ever since.\n", "tags": [], "categories": ["Blog", "Development"]}
{"post_id": 1345, "title": "Setting up data analytics pipeline: the best practices", "url": "https://www.luminis.eu/blog-en/data-en/setting-up-data-analytics-pipeline-the-best-practices/", "updated_at": "2020-11-11T14:50:52", "body": "In a data science analogy with the automotive industry, the data plays the role of the raw-oil which is not yet ready for combustion. The data modeling phase is comparable with combustion in the engines and data preparation is the refinery process turning raw-oil to the fuel i.e., ready for combustion. In this analogy data analytics pipeline includes all the steps from extracting the oil up to combustion, driving and reaching to the destination (analogous to reach the business goals).\nAs you can imagine, the data (or oil in this analogy) goes through a various transformation and goes from one stage of the process to another. But the question is what is the best practice in terms of data format and tooling? Although there are many tools that make the best practice sometimes very use-case specific but generally JSON is the best practice for the data-format of communication or the lingua franca and Python is the best practice for orchestration, data preparation, analytics and live production\n.\n\n\u00a01\u00a0Datapipeline Architect Example\nWhat is the common inefficiency and why it happens?\nThe current inefficiency is overusing of tabular (csv-like) data-formats for communication or lingua franca. I believe data scientists still overuse the structured data types for communication within data analytics pipeline because of standard data-frame-like data formats offered by major analytic tools such as Python and R. Data scientists start getting used to data-frame mentality forgetting the fact that tabular storage of the data is a low scale solution, not optimized for communication and when it comes to bigger sets of data or flexibility to add new fields to the data, data-frames and their tabular form are non-efficient.\nDataOps Pipeline and Data Analytics\nA very important aspect for analytics being ignored in some circumstances is going live and getting integrated with other systems. DataOps is about setting up a set of tools from capturing data, storing them up to analytics and integration, falling into an interdisciplinary realm of the DevOps, Data Engineering, Analytics and Software Engineering (Hereinafter I use data analytics pipeline and DataOps pipeline interchangeably.) The modeling part and probably some parts in data prep phases need a data-frame like data format but the rest of the pipeline is more efficient and robust if is JSON native. It allows adding/removing features easier and is a compact form for communication between modules. \u00a02\nThe role of Python\nPython is a great programming language used not only by the scientific community but also the application developers. It is ready to be used as back-end and by combining it with Django you can build up full-stack web applications. Python has almost everything you need to set up a DataOps pipeline and is ready for integration and live production.\nPython Example: transforming CSV to JSON and storing it in MongoDB\nTo show some capabilities of Python in combination with JSON, I have brought a simple example. In this example, a dataframe is converted to JSON (Python dictionaries) and is stored in MongoDB. MongoDB is an important database in today\u2019s data storage as it is JSON native storing data in a document format bringing high flexibility .\n\u00a0\n\r\n### Loading packages\r\n\r\nfrom pymongo import MongoClient import pandas as pd\r\n\r\n# Connecting to the database\r\n\r\nclient = MongoClient('localhost', 27017)\r\n\r\n# Creating database and schema\r\n\r\ndb = client.pymongo_test posts = db.posts\r\n\r\n# Defining a dummy dataframe\r\n\r\ndf = pd.DataFrame({'col1': [1, 2], 'col2': [0.5, 0.75]}, index=['a', 'b'])\r\n\r\n# Transforming dataframe to a dictionary (JSON)\r\n\r\ndic=df.to_dict()\r\n\r\n# Writing to the database\r\n\r\nresult = posts.insert_one(dic) print('One post: {0}'.format(result.inserted_id))\nThe above example shows the ability of python in data transformation from dataframe to JSON and its ability to connect to various tooling (MongoDB in this example) in DataOps pipeline.\nRecap\nThis article is an extension to my previous article on future of data science (https://bit.ly/2sz8EdM). In my earlier article, I have sketched the future of data science and have recommended data scientists to go towards full-stack. Once you have a full stack and various layers for DataOps / data analytics JSON is the lingua franca between modules bringing robustness and flexibility for this communication and Python is the orchestrator of various tools and techniques in this pipeline.\n\n1: The picture is courtesy of https://cdn-images-1.medium.com/max/1600/1*8-NNHZhRVb5EPHK5iin92Q.png\u00a02: The picture is courtesy of https://zalando-jobsite.cdn.prismic.io/zalando-jobsite/2ed778169b702ca83c2505ceb65424d748351109_image_5-0d8e25c02668e476dd491d457f605d89.jpg\n", "tags": [], "categories": ["Blog", "Data"]}
{"post_id": 17223, "title": "New release for Luminis\u2019 Open Source project Amdatu", "url": "https://www.luminis.eu/blog-en/new-release-luminis-open-source-project-amdatu/", "updated_at": "2018-06-20T13:22:53", "body": "\n\n\n\n\nLuminis releases Blueprint r3\u00a0of open source project Amdatu.\u00a0With Amdatu blueprint we provide a standardized workspace that provides templates to get quickly you up and running with building modular applications using Amdatu components.\u00a0Luminis is one of the founding members of the\u00a0Amdatu Foundation\u00a0and its Open Source project.\n\u00a0\nIn this release Luminis added support for creating Docker images for your application and the security feature has been updated to use the latest Amdatu Security version. Check out the blueprint documentation to get started quickly.\n\u00a0\nAmdatu subscribes to an Open philosophy aimed at open collaborations. It is geared towards enabling Open Innovation projects to deliver evolvable software systems quickly and reliably. To that end, the Amdatu project offers a large collection of Open Source components and development tools. The Amdatu project is designed around an opinionated architecture which applies the modular development paradigm.\n\n\n\n\n\n\n", "tags": [], "categories": ["Blog"]}
{"post_id": 1358, "title": "Looking back on AWS Summit Benelux 2018", "url": "https://www.luminis.eu/blog-en/development-en/looking-back-on-aws-summit-benelux-2018/", "updated_at": "2020-11-13T10:50:20", "body": "Last week I visited\u00a0AWS Summit Benelux\u00a0together with\u00a0Sander.\nAWS Summit is all about cloud computing and the topics that surround cloud computing. This being my first AWS conference I can say it was a really nice experience. Sure there was room for improvement (no coffee or tea after the opening keynote being one), but other than that it was a very good experience. Getting inside was a breeze with all the different check-in points and after you entered you were directly on the exhibitor floor where a lot of Amazon partners showed their products.\nOpening keynote\nThe day started with an introduction by Kamini Aisola, Head of Amazon Benelux. With this being my first AWS summit it was great to see Kamini showing some numbers about the conference: 2000 attendees and 28 technical sessions. She also showed us the growth pattern of AWS with an increasing growth of 49% compared to last year. That\u2019s really impressive!\nWho are builders?\nShortly after, Amazon.com CTO\u00a0Werner Vogels\u00a0started with his opening keynote. Werner showed how AWS evolved from being \u2018just\u2019 an IaaS company to now offering more than 125 different services. More than 90% of the developed services were based on customer feedback from the last couple of years. That\u2019s probably one of the reasons why AWS is growing so rapidly and customers are adopting the AWS platform.\nWhat I noticed throughout the entire keynote is that AWS is constantly thinking about what builders want to build (in the cloud) and what kind of tools those builders need to have to be successful. These tools come in different forms and sizes, but I noticed there is a certain pattern in how services evolve or are grown at AWS. The overall trend I noticed during the talks is that engineers or builders should have to spend less time focussing on lower level infrastructure and can start to really focus on delivering business value by leveraging the services that AWS has to offer.\nDuring the keynote Werner ran through a couple of different focus areas for which he showed what AWS is currently offering. In this post I won\u2019t go through all of them, because I expect you can probably watch a recording of the keynote on youtube soon, but I\u2019ll highlight a few.\nLet\u2019s first start with the state of Machine Learning and analytics. Werner looked back at how machine learning evolved at Amazon.com and how services were developed to make machine learning more accessible for teams within the organisation. Out of this came a really nice mission statement:\nAWS want\u2019s to put machine learning in the hands of\u00a0every\u00a0developer and data scientist.\nTo achieve this mission AWS is currently offering a layered ML stack to engineers looking into to using ML on the AWS platform.\n\nThe layers go from low-level libraries to pre-build functionalities based on these lower level layers. I really liked that fact that these services are built in such a way that engineers can decide at which level of complexity they want to start using the ML services offered by AWS. Most of the time data engineers and data scientist will start from either\u00a0SageMaker\u00a0or even lower, but most application developers might just want to use a pre-built functionality like image recognition, text processing or speech recognition. See for instance this\u00a0really awesome post\u00a0on using Facial recognition by my colleague\u00a0Roberto.\nAnother example of this layered approach was with regards to container support on AWS. A few years back Amazon added container support to their offering with Amazon Elastic Container Service (Amazon ECS). This allowed Amazon ECS helped customers run containers on AWS without having to manage all servers and manager their own container orchestration software. ECS delivered all of this. Now fast forwarding a few years Amazon is now offering\u00a0Amazon EKS\u00a0(managed Kubernetes on Amazon) after they noticed that about 63% of managed Kubernetes clusters ran on AWS. Kubernetes has become the current industry standard when it comes to container orchestration, so this makes a lot of sense. In addition, Amazon now also offers\u00a0Amazon Fargate. With Fargate they take the next step which means that Fargate allows you as the developer to focus on running containers \u2018without having to think about managing servers or clusters\u2019.\n\nDuring his keynote, Werner also mentioned the\u00a0Well-Architected framework. The Well-Architect framework has been developed to help cloud architects run their applications in the cloud based on AWS best practices. When implemented correctly it allows you to fully focus on your functional requirements to deliver business value to your customers. The framework is based on the following five pillars:\n\nOperational Excellence\nSecurity\nReliability\nPerformance Efficiency\nCost Optimization\n\nI had not heard about the framework before, so during the weekend I read through some of its documentation. Some of the items are pretty straightforward, but others might give you some insights in what it means to run applications in the cloud. One aspect of the Well-Architected framework, Security, had been recurring throughout the entire keynote.\nWerner emphasised a very important point during his presentation:\nSecurity is\u00a0EVERYONE\u2019s\u00a0job\nWith all the data breaches happening lately I think this is a really good point to make. Security should be everybody\u2019s\u00a0number one priority\u00a0these days.\nDuring the keynote, there were a couple of customers that showed how AWS had helped them achieve a certain goal. Bastiaan Terhorst, CPO at\u00a0WeTransfer\u00a0explained that being a cloud-scale company comes with certain problems. He explained how they moved from a brittle situation towards a more scalable solution. They could not modify the schema of their DB anymore without breaking the application, which is horrible if you reach a certain scale and customer base. They had to rearchitect the way they worked with incoming data and using historic data for reporting. I really liked the fact that he shared some hard-learned lessons about database scalability issues that can occur when you reach a certain scale.\nTim Bogaert, CTO at\u00a0de Persgroep\u00a0also showed how they moved from being a silo-ed organization with own datacenters and waterfall long-running projects towards all-in AWS with an agile approach and teams following the \u201cYou Build It, You Run It\u201d mantra. It was an interesting story because I see a lot of larger enterprises still struggling with these transitions.\nAfter the morning keynote, the breakout sessions started. There were 7 parallel tracks and all with different topics, so plenty to choose from. During the day I attended only a few, so here goes.\nImprove Productivity with Continuous Integration & Delivery\nThis really nice talk by\u00a0Clara Ligouri\u00a0(software engineer for AWS Developer Tools) and\u00a0Jamie van Brunschot\u00a0(Cloud engineer at Coolblue) gave a good insight into all the different tools provided by AWS to support the full development and deployment lifecycle of an application.\n\nClara modified some code in\u00a0Cloud9\u00a0(the online IDE), debugged some code, ran CI jobs, tests and deployments all from within her browser and pushed a new change to production within only a matter of minutes. It shows how far the current state of being a cloud-native developer has really come. I looked at Cloud9 years ago. Way before they were acquired by Amazon. I\u2019ve always been a bit skeptical when it comes to using an online IDE. I remember having some good discussions with the CTO at my former company about if this would really be the next step for IDEs and software development in general. I\u2019m just so comfortable with IntelliJ for Java development and it always works (even if I do not have any internet ;-)). I do wonder if anybody reading this is already using Cloud9 (or any other Web IDE) and is doing his / her development fully in the cloud. If you do, please leave a comment, I would love to learn from your experiences. The other tools like CodePipeline and CodeDeploy definitely looked interesting, so I need to find some time to play around with them.\nGDPR\nNext up was a talk on GDPR. The room was quite packed. I didn\u2019t expect that though, because everybody should be GDPR compliant by now right? \ud83d\ude42 Well not really. Companies are still implementing changes to be compliant with GDPR. The talk by Christian Hesse looked at different aspects of GDPR like:\n\nThe right to data portability\nThe right to be forgotten\nPrivacy by design\nData breach notification\n\nHe also talked about the\u00a0shared responsibility model\u00a0when it comes to being GDPR compliant. AWS as the\u00a0processor\u00a0of personal data and the company using AWS being the\u00a0controller\u00a0are both responsible for making sure data stays safe. GDPR is a hot topic and I guess it will stay so for the rest of the year at least. It\u2019s something that we as engineers will always need to keep in the back of our minds while developing new applications or features.\nServerless\nIn the afternoon I also attended a talk on Serverless by Prakash Palanisamy (Solutions Architect, Amazon Web Services) and Joachim den Hertog (Solutions Architect, ReSnap / Albelli). This presentation gave a nice overview of Serverless and Step functions, but also showed new improvements like the Serverless Application Repository, save Serverless deployments and incremental deployments. Joachim gave some insights into how Albelli was using Serverless and Machine Learning on the AWS platform for their online photo book creator application called ReSnap.\n\nUnfortunately I had to leave early, so I missed the end of the Serverless talk and the last breakout session, but all in all AWS Summit Benelux was a very nice experience with some interesting customer cases and architectures. For a \u2018free\u2019 event it was amazingly organized, I learned some new things and had a chance to speak with some people about how they used AWS. It has triggered me to spend some more time with AWS and its services. Let\u2019s see what interesting things I can do on the next Luminis TechDay.\nBuild On!\n", "tags": [], "categories": ["Blog", "Development"]}
{"post_id": 23457, "title": "Tech day: Facial recognition on my magic mirror", "url": "https://www.luminis.eu/blog-en/development-en/tech-day-facial-recognition-on-my-magic-mirror/", "updated_at": "2020-11-17T14:58:35", "body": "Last month we had another tech day! On this day we try to do cool stuff. This time I had the idea to use facial recognition on my magic mirror (See my previous\u00a0blog).\nSo with a group of 5 people, we started thinking of the requirements. When someone is standing in front of the mirror, we want the mirror to detect that. This means we need motion detection! We also want to recognise the user and his emotion. If we can recognise the user, it would be cool to display a message on the mirror or even speak out a message. Well enough requirements for a day! We divided the group in two teams, one working on the frontend, the other on the backend and it was time to get to work!\nFrontend\nFor the frontend we used\u00a0tracking.js\u00a0to detect if someone is standing in front of the camera. Once the person is detected, we take a picture and send it to our backend. Here is some code of the tracking part. To prevent that every movement resulted in taking a picture, we also added a check that the next picture will be taken after 10 seconds.\n\n// Start tracking\r\ntracker.on('track', function (event) {\r\n    context.clearRect(0, 0, canvas.width, canvas.height);\r\n    // Track a person\r\n    event.data.forEach(function (rect) {\r\n        const timeAgo = new Date(new Date().getTime() - 10 * 1000);\r\n        const inLastTime = pictureTaken.getTime() > timeAgo;\r\n        // Take a new picture\r\n        if (rect.total >= 1 && !inLastTime) {\r\n            takeSnapshot();\r\n            pictureTaken = new Date();\r\n        }\r\n        // Draw rectangle\r\n        context.strokeStyle = '#a64ceb';\r\n        context.strokeRect(rect.x, rect.y, rect.width, rect.height);\r\n        context.font = '11px Helvetica';\r\n        context.fillStyle = \"#fff\";\r\n        context.fillText('x: ' + rect.x + 'px', rect.x + rect.width + 5, rect.y + 11);\r\n        context.fillText('y: ' + rect.y + 'px', rect.x + rect.width + 5, rect.y + 22);\r\n    });\r\n});\nBackend\nAt our DevCon conference my colleague Bert Ertman showed how he used\u00a0AWS Rekognition\u00a0to identify who was ringing his doorbell. This looked very promising so we decided to use this service as well for the face recognition part. We want the backend to receive a photo taken by the frontend, which than can be used to check with AWS Rekognition if the face is recognised. To make this process work we needed to configure a few things for AWS. Note that I\u2019m using the AWS command line interface to execute most of these steps, but this can also be done in code.\n\nAdd AWS credentials to your local machine. (link)\nCreate a bucket on S3\nUpload a file to the bucket\nCreate a collection\n\n\naws rekognition create-collection \\ \r\n--collection-id \"someCollectionId\" \\ \r\n--region eu-west-1 \\ \r\n--profile default \n5.Index each face that you want to be recognised\naws rekognition index-faces \\ \r\n--image '{\"S3Object\":{\"Bucket\":\"bucket-name\",\"Name\":\"file-name\"}}' \\ \r\n--collection-id \"collection-id\" \\ \r\n--detection-attributes \"ALL\" \\ \r\n--external-image-id \"example-image.jpg\" \\ \r\n--region eu-west-1 \\ \r\n--profile default \nNow that we have indexed a face, we can check if the person is recognised by AWS Rekognition. The following command will search for faces in the given image.\naws rekognition search-faces-by-image \\\r\n    --image '{\"S3Object\":{\"Bucket\":\"bucket-name\",\"Name\":\"Example.jpg\"}}' \\\r\n    --collection-id \"collection-id\"\nThe response will give us each face that has been matched and with which confidence it has matched. One other request that we can do is the detect faces request. This response contains information like emotions, gender but also if the person is wearing (sun)glasses or smiling.\naws rekognition detect-faces \\\r\n--image '{\"S3Object\":{\"Bucket\":\"bucket\",\"Name\":\"file\"}}' \\\r\n--attributes \"ALL\"\nEnd of tech day\nAt the end of this day we had a working demo app which tracked the user, took a picture and displayed some information about the user on our laptop.While this was already great, it would even be greater if this worked on my raspberry pi and magic mirror. So I decided to remove the trackingjs part and use a camera connected to my raspberry pi as input. On the backend I created an endpoint which can be called in the future by an IoT button, just to prevent that the camera is always on or is taking pictures every time a person walks by.\nAdding AWS Polly\nBecause I want to give the user a personal message when he is recognised, I have added\u00a0AWS Polly\u00a0to turn a message that I constructed with the information from AWS Rekognition into lifelike speech. All you need to do is sent a synthesize speech request to AWS and you will get a response which contains the audio stream.\n\n\n\n\n\n\n1\n2\n3\n4\n5\n\n\n\naws polly synthesize-speech \\\n\u00a0\u00a0\u00a0\u00a0--output-format mp3 \\\n\u00a0\u00a0\u00a0\u00a0--voice-id Joanna \\\n\u00a0\u00a0\u00a0\u00a0--text 'Hello Roberto. You look happy today' \\\n\u00a0\u00a0\u00a0\u00a0hello.mp3\n\n\n\n\n\n\n\nThis is now the flow on my raspberry pi. The backend is created with Spring boot and provides an endpoint to trigger the whole process. The backend will send a request to my mirror with the text which is shown by the\u00a0IFTTT module\u00a0of the\u00a0MagicMirror platform. The backend will also play the audio stream that we received from AWS Polly.\n\nResult\nI have made a video to show you the result. You will see that it recognised my happy face, greets me (in Dutch) and tells me that I look happy \ud83d\ude42\nI\u2019m going to continue to make the code more configurable, so it may be useful to others. Let me know if you have any questions or thoughts on what you think that need to be configurable.\n[1]: https://amsterdam.luminis.eu/2017/07/25/techday-smart-mirror/\n[2]: https://trackingjs.com/\n[3]: https://aws.amazon.com/rekognition/\n[4]: https://docs.aws.amazon.com/cli/latest/userguide/cli-config-files.html\n[5]: https://aws.amazon.com/polly/\n[6]: https://github.com/jc21/MMM-IFTTT\n[7]: https://magicmirror.builders/\n", "tags": [], "categories": ["Blog", "Development"]}
{"post_id": 1392, "title": "Improvements the Agile way: KATA board", "url": "https://www.luminis.eu/blog-en/strategy-innovation-en/improvements-the-agile-way-kata-board/", "updated_at": "2020-11-25T14:39:02", "body": "No, you don\u2019t have to throw away your Scrum and Kanban boards. Kata board (if you will) is a process that lives next to your scrum process. A kata typically refers to fundamental movements in Japanese martial arts but can refer to any basic form, routine, or pattern of behaviour.\nThe goal is to internalize the movements and techniques of a kata, so they can be executed and adapted under different circumstances, without thought or hesitation.\u00a0And hence the need of kata in Agile development.\nWe shall look into Kata process to check if the agile world sees it a fit solution for making improvements. It\u2019s an excellent tool for all forms of autonomous teams. Although agile is a very versatile broad and covers almost all aspects of project planning, there is still a gap in how to deal with improvements that span out over indefinite periods of time. There are some similarities in way of working in both the approaches though.\nThe traditional way of working\nLet\u2019s dive into the concept with an example. Here, the team very well understands that the business value is delivered by a usable mode of transport.\nThe eventual goal being a 4 wheeled car.\nThe team is expected to deliver something useful by each end of the sprint.\nSo, here they deliver a skateboard, followed by a kick-board scooter.\nThese are enhanced, and the stakeholders get a cycle, a motorcycle and eventually a car, in the consecutive sprints.\n\n\u00a0\nKata way of working\nNow the team identifies that they can achieve the goal above by practising agile. But they also know that the quality of their product can be improved by improving the quality of metal/alloy they use.\nSo, they follow kata to experiment with the quality, instead of first, taking a sprint or two to figure out the best possible alloy to use for achieving the best quality possible, before delivering anything useful.\n\nAs we can see, similar to the Agile way of working, Kata improves in small steps and doesn\u2019t plan the whole path to the desired improvement. The desired end state or \u2018definition of done\u2019 is known. But only the first achievable target condition is determined in advance. No further milestones.\nIn the example above, the team learns more about the alloy. They gain knowledge about their boundaries. They might also find out that they can do more/less than the expected.\nFor e.g.: \u2013 They might figure out that there might be a composition of the alloy that is strong enough but at the same time lighter, which gives better performance and is more efficient.\nOr they might figure out that it\u2019s only possible to have a durable metal if its heavy. This might also mean that they have to take a blow on the performance of the vehicle.\nAdditional to how Agile develops, Kata Improvement put even more emphasis on learning. An experiment may fail, as long as the team has learned from it. Agile does this to some extent, by working on minimum viable products that can be tested in practice. The experiments in the Improvement Kata are even more frequent. Many small experiments ensure continuous learning and continuous improvement. It doesn\u2019t tell you how to get to the next target condition, let alone how to get to your desired situation. It doesn\u2019t even tell you which steps to take to reach this year\u2019s target. The Kata lets you discover the route as you go.\nRecognizable patterns of behaviour and clear expectations make it easy to recognize abnormalities (problems) and also serve as a basis for improvement, setting and attaining higher standards.\nKata is a repeating four-step routine by which an organization improves and adapts. It makes continuous improvement through the scientific problem-solving method of plan, do, check, act (PDCA) a daily habit.\u00a0The four steps as shown in the improvement theme charter below are:\n\nDetermine a vision or direction \u2013 Definition of Awesome\nGrasp the current condition \u2013 Now/Problem\nDefine the next target condition \u2013 Next target condition\nMove toward the target (the plan or \u201cP\u201d defined by the first three steps)\u00a0through quick, iterative PDCA cycles to uncover and remove obstacles \u2013 First steps\n\n\nKata shows strong similarities to Agile and Scrum. This makes it the best improvement and problem-solving method for Agile teams, Squads, Tribes, Scrum Teams. It\u2019s the best way to get to a true learning organization and continuous improvement. This enables you to cope with the ever-changing demands of customers and regulators.\nI would strongly recommend teams to try Kata alongside Agile to improve the quality of their deliverables or to improve on any pain points that create annoyance regularly.\nGood luck and happy Improving!\n", "tags": [], "categories": ["Blog", "Strategy &amp; Innovation"]}
{"post_id": 1671, "title": "Back button flow", "url": "https://www.luminis.eu/blog-en/development-en/back-button-flow/", "updated_at": "2020-11-24T15:03:05", "body": "Android devices hebben een terug-knop. Dit heeft het voordeel voor de gebruiker om gemakkelijk terug te kunnen waar je vandaan kwam. Voor Android apps is het aan de developer op de juiste manier met de back button om te gaan. Bij het maken van native apps voor Android gaat dat min of meer vanzelf, omdat daar gewerkt wordt vanuit de gedachte van child views, waarbij je met de back button naar de parent navigeert. Bij het maken van apps met bijvoorbeeld Ionic en Cordova moet hier echter bewust over worden nagedacht en logica voor worden geschreven.\nDe terug-knop werkt niet altijd zoals verwacht\nOm het probleem duidelijk te maken ga ik uit van een eenvoudige app met een menu om naar de verschillende views binnen de app te kunnen navigeren. Het menu heeft een aantal items, bijvoorbeeld \u2018dashboard\u2019, \u2018aanbiedingen\u2019, \u2018instellingen\u2019 en \u2018profiel\u2019.\n\nDe gebruiker bevindt zich op het dashboard en klikt in het menu op \u2018instellingen\u2019\nDe app navigeert naar de instellingen view\nDe gebruiker klikt nu op de terug-knop en verwacht terug te komen op het dashboard\n\nEen Angular applicatie navigeert m.b.v. de url. Klikken op de back button brengt de gebruiker naar de vorige url. Maar wat gebeurt er als de gebruiker een paar keer tussen twee pagina\u2019s in het menu klikt;\n\nDe gebruiker bevindt zich op de instellingen view en klikt in het menu op \u2018profiel\u2019\nDe gebruiker vindt daar niet wat hij zoekt en klikt in het menu op \u2018instellingen\u2019\nOok bij \u2018instellingen\u2019 vindt hij niet wat hij zoekt en gaat via het menu toch weer terug naar \u2018profiel\u2019\nBij \u2018profiel\u2019 herinnert hij zich opeens dat hij iets zag bij \u2018instellingen\u2019 wat hij zoekt en gaat via het menu opnieuw naar \u2018instellingen\u2019.\nAls de gebruiker uiteindelijk heeft gevonden wat hij zoekt en klaar is, klikt ie op de terug-knop van het device en verwacht terug te komen op het dashboard\n\nOmdat een Angular applicatie navigeert m.b.v. de url zal de gebruiker bij het klikken op de back button niet naar het dashboard gaan, maar eerst een aantal keer heen en weer navigeren tussen \u2018instellingen\u2019 en \u2018profiel\u2019 voordat hij uiteindelijk op het dashboard belandt. Dat is ongewenst/onverwacht gedrag van de applicatie!\nHoe gaan we om met navigeren tussen views?\nIn een Angular Cordova applicatie kan een eventListener op de terug-knop worden gezet.\n\u00a0\ndocument.addEventListener(\"backbutton\", (ev) => {\r\n    ev.preventDefault();\r\n}, false);\nMet deze code snippet wordt voorkomen dat heen en weer genavigeerd wordt. Maar dit is niet genoeg, want nu doet de back button helemaal niks meer. Hiermee wordt een hardware knop van het device helemaal uitgeschakeld en dat is nogal een rigoreuze beslissing. Er zijn verschillende manier om dit aan te pakken; We zouden voor elke view kunnen defini\u00ebren wat de parent view is. Klikken op de terug-knop moet de gebruiker dan niet naar de vorige view brengen maar altijd naar de parent. Dit werkt niet als een pagina vanaf 2 pagina\u2019s bereikbaar is; wat is dan de parent? Zolang de applicatie een eenvoudige navigatie structuur heeft, waarbij er een dashboard bestaat, overzicht (level 1) pagina\u2019s en detail (level 2) pagina\u2019s, kunnen we voor elke view defini\u00ebren welk level het heeft. In dat geval zijn er 3 mogelijkheden;\n\nAls de huidige pagina een detail (level 2) pagina is, gebruik dan de history.back() om uit te zoeken waar de gebruiker vandaan kwam.\nAls de huidige pagina een level 1 pagina is, ga dan naar het dashboard.\nAls de huidige pagina het dashboard is, vraag de gebruiker dan of hij de applicatie wil afsluiten.\n\nHiermee kan dan de volgende route logica worden opgesteld;\n\u00a0\ndocument.addEventListener(\"backbutton\", (ev) => {\r\n  ev.preventDefault();\r\n  historyService.onBack();\r\n}, false);\n\u00a0\nexport class HistoryService {\r\n\r\n  onBack(): void {\r\n    // if page is level 2? then go back to previous page\r\n    // else if current page is dashboard? then open exit app confirm modal\r\n    // else go to dashboard\r\n    const currentRoute = getCurrentRoute()\r\n    if (currentRoute.level > 1) {// current view is a level 2 view\r\n      history.back();\r\n    } else if (currentRoute.default) {// current view is the dashboard\r\n      this.confirmExitApp();\r\n    } else {\r\n      this.navigateToDefaultView();\r\n    }\r\n  }\r\n\r\n  this.getCurrentRoute(): Route {\r\n    // this function should return the current route as een object with it's level\r\n    // this logic goes beyond the scope of this article\r\n  }\r\n\r\n  confirmExitApp(): void {\r\n    // this function should open a confirm modal with the question to exit the app\r\n    // on confirm close the app with\r\n    navigator.app.exitApp();\r\n    // on dismiss close the modal\r\n    // this logic goes beyond the scope of this article\r\n  }\r\n\r\n  navigateToDefaultView(): void {\r\n    // this function should navigate to the dashboard\r\n    // this logic goes beyond the scope of this article\r\n  }\r\n\r\n}\nDeze logica werkt alleen als\n\nhet menu alleen level 1 pagina\u2019s en eventueel het dashboard bevat\nlevel 2 pagina\u2019s alleen bereikbaar zijn vanaf level 1 pagina\u2019s\n\nEen level 2 pagina kan vanaf 2 verschillende level 1 pagina\u2019s bereikbaar zijn, zolang de level 2 pagina maar niet vanaf een andere level 2 pagina bereikbaar is. Deze eenvoudige navigatie structuur heeft ook als voordeel dat een gebruiker nog begrijpt waar hij zich bevindt. Als de applicatie level 2, level 3 en misschien zelfs level 4 pagina\u2019s zou hebben, of als een level 3 pagina bereikbaar is vanaf een level 2 pagina die bereikbaar is vanaf een level 4 pagina, dan wordt het moeilijker voor de gebruiker voor te stellen waar in de applicatie hij zich bevindt. Natuurlijk zullen er situaties zijn waar toch een level 3 pagina bereikbaar is. Al is het voor de gebruikersbeleving af te raden, zolang de level 2 pagina alleen vanaf de level 3 pagina bereikbaar is met de terug-knop of met een button die van history.back() gebruik maakt, blijft de route logica werken.\nState changes en navigeren binnen een modal\nVoorbeeld 1: De gebruiker gaat zijn wachtwoord aanpassen bij \u2018profiel\u2019;\n\nDe gebruiker klikt in het menu op \u2018profiel\u2019\nOp de profiel view klikt de gebruiker op een knop \u2018wachtwoord aanpassen\u2019\nDan opent een modal met 2 invoervelden om een nieuw wachtwoord in te voeren\nDe gebruiker vult een nieuw wachtwoord in en klikt op een knop \u2018verder gaan\u2019\nDe modal toont dan een scherm waarin hij zijn oude wachtwoord moet invullen ter bevestiging\nOp dat moment wil de gebruiker toch een ander nieuw wachtwoord kiezen en klikt op de back button van het device\nDe gebruiker verwacht dan dat de modal geopend blijft en de 2 invoervelden om een nieuw wachtwoord in te voeren worden getoond\n\nVoorbeeld 2: De gebruiker wil een lijst met aanbiedingen als bulk aanpassen;\n\nDe gebruiker klikt in het menu op \u2018aanbiedingen\u2019\nOp de aanbiedingen overzichtsview klikt de gebruiker op een knop \u2018aanbiedingen aanpassen\u2019\nDan verandert de state van de view in een \u2018edit\u2019 state, waarbij verwijder- en sorteerknopjes verschijnen\nOp dat moment bedenkt de gebruiker zich en klikt op de terug-knop\nDe gebruiker verwacht dan dat de \u2018edit\u2019 state verandert in de \u2018view\u2019 state waarmee hij op de pagina terecht kwam\n\nHelaas zullen de verwachtingen in beide voorbeelden niet uitkomen en zal de applicatie netjes, zoals bedacht naar het dashboard gaan. Om dit soort gevallen te kunnen afvangen willen we een lijst bijhouden waarin staat wat er op welk moment moet gebeuren als op de back button wordt geklikt. Soms zal genavigeerd moeten worden naar een andere view, in andere gevallen moet een bepaalde state veranderen naar de vorige state of moet een modal sluiten. En ook hier geldt weer dat als je een paar keer tussen 2 states wisselt, dat klikken op de terug-knop niet opnieuw in omgekeerde volgorde tussen de states gaat wisselen. En nog specifieker; als je op een \u2018bewerken\u2019-knop klikt en dan op \u2018opslaan\u2019 dan moet de terug-knop niet naar de \u2018bewerken\u2019 state gaan, maar de terug functionaliteit uitvoeren die uitgevoerd zou worden voordat je op de \u2018bewerken\u2019-knop klikte.\nEen \u2018history stack\u2019\nDoor een \u2018history stack\u2019 bij te houden kunnen we defini\u00ebren dat bijvoorbeeld bij het openen van een modal de stack wordt opgehoogd met een functionaliteit die de modal sluit. En bij het sluiten van de modal zouden we alles van de stack kunnen verwijderen tot en met de functionaliteit die de modal sluit. De logica voor de back button wordt uitgebreid met een check of er een history stack bestaat. Als die bestaat, dan wordt de laatste functionaliteit in de history stack uitgevoerd en dit item wordt vervolgens verwijderd van de stack.\n\u00a0\nexport class HistoryService {\r\n\r\n  private historyStack: HistoryStackItem[] = [];\r\n\r\n  onBack(): void {\r\n    // if history stack exist, run last callBack and remove this item from stack\r\n    // else if page is level 2? then go back to previous page\r\n    // else if current page is dashboard? then open exit app confirm modal\r\n    // else go to dashboard\r\n    const currentRoute = getCurrentRoute();\r\n    if (this.historyStack && this.historyStack.length) {\r\n      const lastItemOnStack = this.historyStack[this.historyStack.length - 1];\r\n      if (lastItemOnStack.popByBackButton) {\r\n        this.historyStack.pop();\r\n      }\r\n      lastItemOnStack.callback();\r\n    } else if (currentRoute.level > 1) {// current view is a level 2 view\r\n      history.back();\r\n    } else if (currentRoute.default) {// current view is the dashboard\r\n      this.confirmExitApp();\r\n    } else {\r\n      this.navigateToDefaultView();\r\n    }\r\n  }\r\n\r\n  setHistoryStack(callBack): void {\r\n    this.historyStack.push(this.createHistoryStackItem(callBack, true);\r\n  }\r\n\r\n  removeHistoryStack(): void {\r\n    this.historyStack = [];\r\n  }\r\n\r\n  private createHistoryStackItem(callBack, popByBackButton): HistoryStackItem {\r\n    return {\r\n      callBack: callBack,\r\n      popByBackButton: popByBackButton\r\n    };\r\n  }\r\n\r\n...\r\n\r\n}\nDe setHistoryStack kan worden aangeroepen bij het openen van een modal of het veranderen van een state. Ook kan de \u2018history stack\u2019 worden geleegd om ervoor te zorgen dat daarna weer genavigeerd wordt op basis van level n views.\nTerug-knop afhandelen is eenvoudig\nDe history service is redelijk eenvoudig en effectief! Hiermee vang je de meeste situaties af, waarin de verwachting bij het klikken op de terug-knop afwijkt van history.back();\n", "tags": [], "categories": ["Blog", "Development"]}
{"post_id": 1232, "title": "Contract-first development of RESTful services", "url": "https://www.luminis.eu/blog-en/development-en/contract-first-development-of-restful-services/", "updated_at": "2020-12-10T09:05:31", "body": "When I was a young backend developer SOA was hot. Every enterprise application consists of web services exchanging XML messages over SOAP.\u00a0The functionality offered by these web services were described in a WSDL file (Web Services Description Language). This WSDL is the contract both parties use to communicate with each other. It specifies how the service can be called, what parameters it expects, and what data structures it returns. Normally there were two ways we would work with WSDL\u2019s.\nContract last\nWe just started coding the functionality in Java. With the frameworks we used we just add some annotations or comply to a certain convention and the web service was published. From the code a WSDL could be generated and threw over the fence to the other party, good luck with it.\nContract first\nWhen working contract first the WSDL is created beforehand. And at the moment the WSDL got finished we started implementing, mostly by starting with generating Java code from the WSDL.\nI remember we could always have fiery discussions in the team whether to work contract first or contract last, but in the end we always had a contract. When we delivered the web services, the WSDL was indisputable the single source of truth, everyone was happy. In the past everything was better, right?\nREST\nFast-forward a few years and all new services should be RESTful with JSON. Instead of those verbose XML files we transfer lightweight JSON files. When developing my first REST endpoints I loved the conciseness and flexibility, but deep down I also missed the firmness I got used to when working with WSDL\u2019s.\nSome WSDL-like tooling for REST arose in the form of WADL (Web Application Description Language).\u00a0This describes the REST endpoints, parameters, data structures in XML quite similar to WSDL. After working with that flexible and concise JSON, putting a lot of effort to create a WADL in XML just didn\u2019t felt right. So I took a look at it, but it never occurred to me to really use this. Until today I never came across a WADL in the wild wild REST world.\nSwagger\nAround 2011 Swagger came around. Now we could add the Swagger library to our project that provides some annotations. Based on these annotations documentation about the REST interface could be generated like this:\n\nWith minimal extra effort (adding some annotations) we\u2019re almost at the point of contract last development of REST endpoints.\u00a0In addition Swagger gives us an easy-to-use tool to test the REST endpoints. Also useful for other parties who write software to integrate with this interface. But still, this is no contract. We have useful documentation of the REST interface for humans, but it still lacks a format that is\u00a0indisputable,\u00a0unambiguously, and preferably machine readable.\nOpenAPI\nSo this is where the OpenAPI Specification (OAS) kicks in. Originally developed by Swagger and meanwhile donated to the Open API Initiative. With the OpenAPI Specification we can create a single interface file as contract in JSON or YAML. From this interface file documentation of methods, parameters and models can be generated. Also REST clients and servers for most major languages/frameworks can be generated.\n\nGetting started\nThe easiest way to get in touch with OpenAPI is by opening the online editor at: editor.swagger.io. By default this loads the pet store API which is a comprehensible sample project. The specification (in YAML) can be edited in the left pane of the editor. The righthand side contains the documentation and is refreshed on the fly. Seeing your changes and their effects being applied in real time is a great way of developing your contract!\nWhen working on your private project you may not want to edit your specification in the public editor. This same editor can simply be installed locally via NPM. To get started I created\u00a0this Git repo\u00a0that provides a decent starting point. If you have any questions, file a new issue in Github or ask it here below in the comments.\nFinal thought\nI gave a brief starting point and now it\u2019s up to you to play around and use it for real.\u00a0Most of us programmers are a lazy bunch and we prefer to automate as much as possible. Today I will cheer on this habit and tell you to \u201cnever ever write your REST interface code by hand\u201d. Let it be generated based on an indisputable contract!\nReferences:\n\nswagger.io\napis.guru/awesome-openapi3 list of tools around the Open API Specification from code generators to\u00a0editors\nswagger.io/blog/difference-between-swagger-and-openapi\n\n", "tags": [], "categories": ["Blog", "Development"]}
{"post_id": 1556, "title": "Indoor positioning at our developer conference", "url": "https://www.luminis.eu/no-category/indoor-positioning-at-our-developer-conference/", "updated_at": "2020-11-12T17:03:42", "body": "A few months ago we had a meeting with a potential client. They told us that they were looking into indoor navigation/positioning so they could use that to provide visitors of their events a better experience. After some research and tests they said that IndoorAtlas gave them the best results. The client decided to give priority to other functionality, but we were enthusiastic.\nEach year we organise our own developers conference (Luminis DevCon). Last year we developed an app that contained information about the sessions, speakers, the location and the possibility to rate a session. While all this was great, we at Luminis value the conversation and so this year we wanted to create something new in the app. Something that was fun, cool and could help in having more conversations. So we thought what if we use indoor positioning to show the location of the speakers in real-time?! Then people can find a speaker and start asking questions or just take a selfie \ud83d\ude09\nIn this blog I will tell you a bit about the technology and I will show you the result of how we have used it at our conference.\nIndoorAtlas \u2013 How does it work\nIndoorAtlas, a hybrid indoor positioning platform, uses different data sources in a phone such as the build-in magnetic sensor and the barometric sensor to create a map of a building. Combined with other sources like Wi-Fi and beacons they can optimise the location of a user.\nThe system contains of a SDK that runs on the device, which sends compressed sensor data to IndoorAtlas Cloud. IndoorAtlas Cloud will match this data to so called \u201csignal maps\u201d. Signal maps are maps that are generated based on the data that is collected during the mapping the floor plan.\nPlatform\nBefore you can use the platform you first have to upload a floor plan to the developers application and place this on correct location. After that you can start mapping. Mapping, also known as fingerprinting, is done using the MapCreator2 for Android by physically walking through the venue. In the MapCreator app you will need to add waypoints. The waypoints act as confirmation points for your data collection paths.\nThe image below shows how many waypoints we have added before we started to map the floor plan.\n\nWhen you\u2019re done with adding all waypoints, you will need to walk different routes between the waypoints. When you arrive at a waypoint, you need to check-in. That way IndoorAtlas knows your position. The best way to get a good result is to walk many different routes.\nThe next image shows all the paths that we have walked in the venue. We have tried to walks as many paths as possible to get the best coverage. It is also possible to exclude incorrect paths so that those are not processed when generating a map.\n\nAfter uploading all the paths to IndoorAtlas, you can check if you have enough coverage. A well mapped floor plan should look something like this:\n\nSDK\nNow that we are done with mapping, it is time to develop our app! IndoorAtlas provides an API for native Android and iOS, cordova and Unity. They also have different example projects on\u00a0GitHub.\nThe API offers the possibility to retrieve a floor plan, of course some calls to do something with your position but also to see if you are located in a certain region. Regions are used to determine if a user is changing floors and therefore need to see a different floor plan. Other features are indoor-outdoor detection and wayfinding.\nResult\nWe have created our conference app with\u00a0Ionic\u00a0and have used the cordova API for IndoorAtlas. To increase the accuracy of the speakers location we have used 8 beacons during the conference. In the video below you can see the result. You will see that in the beginning the accuracy is not very high, but becomes better when we start moving around.\n\n", "tags": [], "categories": ["No category"]}
{"post_id": 1389, "title": "Devcon 2018", "url": "https://www.luminis.eu/no-category/devcon-2018/", "updated_at": "2020-11-24T15:17:32", "body": "Devcon, a developer conference is a platform where you and your colleagues can share knowledge with like-minded IT specialists. The focus is on trends and the latest software development technologies. Multiple technical are presented by renowned international and national speakers who also share their experience and knowledge through a series of parallel sessions.\nHave been to all Devcons held so far. This one being the fourth. Couldn\u2019t have been happier, more excited and motivated to be there year after year. I have seen Devcon evolve and improve and develop enthusiasm amongst the developer community. This time it was the best so far. And I am sure, I will be able to reuse this statement every year.\n\nOn 12th April 2018, Cinemec Ede, opened its doors for Devcon 2018. The event is by the developers and for the developers. Of course, we don\u2019t mind managers dropping in to catch up with the latest software development technologies. There were multiple technical sessions presented by renowned national and international speakers. They were also available during the day if you couldn\u2019t get enough and wanted to have more of an in-depth discussion. This reminds me that Luminis has enhanced their app with \u201cspeaker tracking\u201d. This allows you to track a speaker in the Cinemec during the event. No escaping! \nThe day started with opening keynote by Hans Bossenbroek (Founder and CEO Luminis) & Rudy van Buren(official F1 simulator driver for the McLaren F1 team). The host for the day was Niels Houtpen, who is an illusionist. When he introduced himself on the stage, people sat still in their chairs. I guess it was a mix of surprise and curiosity. He showed us some tricks, which I think worked very well for waking up the people and engage their brains in finding out the logic in everything he did. Majority of the audience was software developer after all. We understand logic. And anyways, we are in some way illusionists too. Making grand, working systems out of nothing. \nLater, Hans came up to the stage and awed us with a talk about future of software. He also talked about Dunning Kruger Effect. It was hilarious how everyone acknowledged that they too had been on mount stupid.  At this point of time, he called on to the stage his colleague, Ren\u00e9 van Hees, with whom he had climbed this particular mountain and more. Ren\u00e9 explained how the best radars were developed. He talked about architectural principals and also about the path for knowledge growth. \nNext was Sam Aaron on the stage, who played code. Yes!! He played music via code. He pressed on the fact that code is not limited to solve business problems or enhance business. Code is everywhere. It is an integral part of our life. And we can adapt it in any way we like. He put the music system of the Cinemec to good use and gave us a demo on how he plays code. And I just didn\u2019t want it to end. \nTo close the keynote, Rudy van Buren was invited to the stage. Niels talked to him about how his career and technology came together and limits for each field were put to test. He gave a lot of insight about his career and how high the competition is. He was very happy that he could be the chosen one and he hoped it goes on for a few years. And for the dreamers out there, who think he has a glamorous job \u2013 It is not easy at all. He shared info about his daily routine and his personal development. It was nice to hear about another career, who also is impacted by technology.\nAlert! This blog post will be long. But I am just getting started. There is so much to share. After catching up with colleagues and praising the engaging keynote, I moved on to the first technical talk for the day, which was \u201cThe internet of ships\u201d. This is developed by Pim Voeten from Luminis in collaboration with Florus Wilming from Onboard. I was amazed by the way they have built a wonderful product from scratch. Not only did they push their software skills, but also got their hands dirty with hardware. The product is complete with a good looking, robust server and top-quality UI design. The goal was to help captains by providing a serverless bridge and they nailed it. \nAfter congratulating Pim for a splendid presentation, I headed towards the next talk of my choice \u2013 \u201cStop w(REST)ling with your API\u2019s: an introduction to GraphQL\u201d.\u00a0Everton from Luminis Arnhem talked about the issues we face when we develop APIs with traditional databases. A few of the issues are: \u2013\n\ndo multiple joins before you can generate a view of your data\nexpose multiple endpoints for similar views\nbreak RESTful conventions\nor do multiple request for a single view\n\nTo add to the pain, if we update our schema, we have to update endpoints too. And we know we all have been there \u2013 breaking RESTful practices to solve our problems. Well, one of the ways you could solve the problem is by using GraphQL. With GraphQL it is possible to allow the frontend to decide what data is needed. Everton was brave to show us a live demo, where he experimented with the schemas, queries, mutations and subscriptions, all in line with GraphQL. I am quite intrigued and would like to replicate the project I am working on with GraphQL separately. \nAfter having a wonderful lunch with so much to choose from, we were greeted by Pep Rosenfeld from Boom Chicago. The title of the keynote session was \u201cThe Future is Here. And it\u2019s Kind of Annoying.\u201d There was no dull moment during his stand up. He was very well prepared for his technical audience. After shaking off our mid-day slump with Pep, we headed towards the parallel talks. This time my pick was \u201cMigrating to Java Modules\u201d by Sander Mark. Sander is a Fellow at Luminis in The Netherlands, where he crafts modular and scalable software, most often on the JVM, but with a touch of TypeScript when needed. He also is a Java Champion and author of the O\u2019Reilly book \u2018Java 9 Modularity\u2019. The module system delivered in Java 9/10 is a great advancement for the Java language, and we would like to migrate existing code to make use of the module system. Migrating an existing code base from the class path to any kind of module system can be a challenging task. The Java module system comes with a number of features to ease migration. This includes automatic modules and the unnamed module. While these features provide great value, they do require understanding of the module system to use them to their full potential. In this talk we looked at examples of migrating real code, based on a Spring/Hibernate application. We\u2019ll face common problems we run into during migration, which gives us practical tips to apply, but also a good understanding of the module framework itself and the various migration features it supports. This talk was an excellent preparation to start migrating existing code. Now, that\u2019s the easy part. The hard part is convincing business about the value it brings to the project. \nAfter an energy break, for the last bit, I picked \u201cTest Driven Documentation for your RESTful service\u201d by Jeroen Reijn. He is a software architect at Luminis and is very passionate about software engineering and engineering culture. He likes to share his broad knowledge and helps people to apply practices like Continuous Delivery to reach a new level of productivity. I think you can already deduce from my choices that I am working with a Java 7/8 RESTful project. Jeroen, in his talk, shared his thought about the struggles we face in keeping the documentation and code in sync. He named a few options we have like RAML and Swagger/Open API. Also, he highlighted the pitfalls they come with. He introduced AsciiDoc and Spring (Auto) REST Docs in the talk and showed us a demo with changing a few things and showing us how the docs adapt accordingly. \nPhew! It was a long post. But there is so much I am excited about. It was a jam-packed day with loads of knowledge.\u00a0But it was awesome. After the interesting talks, I had a chance to catch up with fellow developers. All were still in high energy and wished the day would never end. We talked about the sessions and were happy that the talks we had to miss will soon be on the\u00a0\u00a0Luminis channel. \nI hope I have generated some interest for you to know more about the splendid talks we had at Devcon. You can go, watch them all at the given link. Till next year! Who knows we might get a chance to visit Bueno Aires in a year. \ud83d\ude09\n", "tags": [], "categories": ["No category"]}
{"post_id": 1388, "title": "Captain Review: a Sprint review that everyone loves", "url": "https://www.luminis.eu/blog-en/development-en/captain-review-a-sprint-review-that-everyone-loves/", "updated_at": "2020-11-16T08:59:09", "body": "I got the opportunity to attend an offsite event organized by a client. We learned and shared our experiences, our problems, with the motive of finding a way to work more effectively together, be it technical or process related.There were posters of superhero movies with our faces morphed on it. There was so much energy from the start till the end of the event. We had meals together. We played chess, football, quizzes and what not. The talks were meticulously planned and picked for our group. Wonderful sessions! And I would like to share my experience for one of the agile sessions here.\nOne of the first tracks I attended was about Captain Review! The goal was to talk about how you can have an effective review and have fun doing it. Our scrum coach,\u00a0Jill Janssen\u00a0gave us the intro. We discussed the importance of review and what should you look for in a feedback. She talked briefly about the elements of a good review:-\n\nDone\u00a0\u2013 Overview what will and will not be demonstrated, taking note of those things that are complete but won\u2019t be demoed.\nNot done\u00a0\u2013 Explain the reasons for the incomplete items, but save the improvement discussion for the Sprint Retrospective.\nDemo\u00a0\u2013 Demo only new/changed functionality, beware of ad hoc demo requests.\nFeedback\u00a0\u2013 Obtain feedback, which will be discussed in detail.\n\nMention key occurrences, events, or problems from the sprint.\n\n\nRefinement\u00a0\u2013 Refine, adjust, and review product backlog based on feedback.\n\nIt was an eye opener to know about a few signs of an ineffective Sprint review (and their solutions)\n\nTeam does not request feedback from participants/stakeholders\n\n\n\n\nPrepare the sprint review to maximize feedback\nAsk engaging questions to the audience\nLeave enough time after each item to discuss it while the information is fresh.\n\n\n\n\nTeam reports to management to sign off on the increment\n\n\n\n\nThe key participants are the team, product owner and stakeholders, not management.\nThere is nothing wrong with management reports, but they are not the purpose of the Sprint Review.\nSignoff comes from the Product Owner.\n\n\n\nSince this session was focussed on getting a feedback, we played a little game where a volunteer took a seat in front of the other participants and was asked to throw a couple of balls, one by one in a box behind him. This box was not directly behind him and he didn\u2019t know how far or to which side of him, was it placed. Also, the audience was not allowed to give any hints. Obviously, without a feedback when blindly throwing balls into the box, there was no success. In the next round, Jill allowed the audience to give cues. The brave volunteer had more success in landing the balls in the box with a proper feedback. This demonstrated how important is feedback. And how can we achieve more when we keep the communication lines open and regular. Later, we played another game where we were offered a few products, for which we had to do a review with the group. The group was divided into 5 and were given a choice of products to do review session for. To choose from, there was a pack of pencils, sticky notes, socks with anti-slip patches, chocolates and more. I was happy that my group member got us a pair of anti-slip socks. My love for yoga sprung up as soon as I saw the product and a plan for the demo and feedback started coming to me. We brainstormed over a few ideas for 5-10 minutes and had a wonderful plan. To elaborate on the exercise \u2013 We had to sell an additional feature on the product. It was the usual drill \u2013 Intro, demo and a feedback. After this, we had to gather feedback for the whole group. A few of the suggestions to gather\u00a0feedback\u00a0were: \u2013\n\nPost-its:\u00a0Let stakeholders write down the answers to a question on post-it notes. Limit the answer to one per post-it note. Let all stakeholders write down their answers first without revealing them to others. Depending on the question asked, this can take 2 to 5 minutes.\nShout outs:\u00a0Ask stakeholders to shout out their answer, one after another. You can limit the number of shout-outs you\u2019ll collect. Take notes on a flip chart.\nMessage Me:\u00a0Let stakeholders write down their answers in silence. Depending on the time available, let them write it down in format of a letter (long), email (a couple of lines), social media update (rather short), or tweet (short; 140 character).\nHigh Five:\u00a0Let stakeholders find their answer in silence. Let them show that they\u2019ve found their answer by putting a fist on their chest. Then count down \u201c3, 2, 1, now!\u201d On \u201cnow!\u201d the stakeholders should hold up 0 to 5 fingers with one hand. For every stakeholder, shout out their answer. Note the result on a flip chart or whiteboard, e.g., 1 x 3 fingers, 4 x 4 fingers, and 2 x 5 fingers.\n\nFollow-up Investigation:\u00a0Ask each stakeholder why they gave their answer. Or ask only the stakeholders with the lowest and highest amount of presented fingers.\nFollow-up Debate:\u00a0Ask the stakeholders with low finger answers to pair with the ones with high-finger answers. Let the pairs debate their different views for a couple of minutes. Afterwards, let the pair shout out a summary of their debate.\n\n\nThumbs Up:\u00a0Let stakeholders give their answer by showing their thumbs in 3 directions: thumb up (positive), thumb to the side (neutral; undecided; meh.), thumb down (negative).\n\nFollow-up Investigation:\u00a0Ask each stakeholder or only the ones you are interested in (for example, only the negative or undecided ones) why they gave their answer.\n\n\n\nWe had some fun coming up with features. One was low fat chocolate. Of course, we were given a demo, or you can say bribed. And we got to eat the chocolates! Also, the feedback loop became very interesting with the happy group of people in the session. Everyone enjoyed giving feedback like \u2013 \u201cThe white color pencil does not work on the white sticky note. Maybe you guys should collaborate on making this work.\u201d We were having quite some fun, which helped me maintain focus on the conversation. I grabbed the opportunity to take my turn after the chocolates team were done with their review. My pitch was \u2013 after we were done eating those delicious chocolates, we still should make sure that we have burned whatever fat we consumed. In the Netherlands, anyways, fitness is serious business. You have been using our cotton socks happily so far. At this time, I had the sock on display with the anti-slip part showing. I flipped the sock, and announced, we have added a new feature to our existing successful product, for the yoga and Pilates enthusiasts with anti-slip patches. I have one on my foot, and I will show you how much I trust it. I donned a simple yoga pose and claimed that I could hold it until the end of the review. <Applause> I must shamelessly state that it was the best demo so far. Meanwhile, I said, let me run a fresh sock around. You can put your hand in and check the grip on the table. Its clean, never worn and just out of the package. While people were testing it out, I started taking questions. Remember, I was on my one foot. I needed to make it short \ud83d\ude09 The group asked me why it was so heavy. In their opinion it should have been lighter. After all, it is for exercise. At this point, I had to think on my feet errr\u2026 foot. But I understood that this will take longer. So, I slipped my foot back on the ground without anyone noticing the flaw in the demo(J) and I replied \u2013 This is for grounding you. Remember this is not for running, but for more grounding forms of exercises. So, it\u2019s not a bug, but a feature. <Laughter> Another query I got, was about the durability. Can I toss it in washing machine and up to how many times? To this, I came up with an explanation that our team does not discontinue with existing features or replaces features. We have continued the existing capability and you can still wash them every day for 100 times. Although I am not sure if you will be able to keep up with exercise every day for 3 continuous months. J <Laughter> When the demo product came back to our table, I asked the people to vote with thumbs up/down. I had a quick look around, and everyone had good feedback. There was only one person who had a horizontal thumb (neither satisfied nor dissatisfied). I claimed that it\u2019s a winner feature for all. And pointed out to that person \u2013 You sir! I think you just want to know about my yoga classes. Don\u2019t you? Concluded the best review I ever had with applause and laughter. It was really fun experience and I thank our coach and the host for the wonderful exercise. I am all invigorated to make the reviews with my team at work more fun and engaging. P.S.: \u2013 The socks are really nice. I am keeping them.\n", "tags": [], "categories": ["Blog", "Development"]}
{"post_id": 2474, "title": "What I\u2019ve Learned About Kubernetes", "url": "https://www.luminis.eu/blog-en/cloud-en/what-ive-learned-about-kubernetes/", "updated_at": "2020-11-18T11:06:21", "body": " So, I recently finished a course about Kubernetes (and Docker and Google Cloud):\u00a0Scalable Microservices with Kubernetes. Unfortunately, it\u2019s only been a couple of days since then, and I\u2019m already not exactly sure about what I learned.\nTo start with, here are some basics:\n\nDocker\u00a0manages containers\nKubernetes\u00a0manages containers like Docker and is\u00a0declarative\u00a0(more about that later)\nGoogle Cloud\u00a0has a similar set of services to AWS, but of course with different names\n\nLastly, I\u2019m more a fan of the command-line than of GUIs so there will be more of a focus on commands.\nDocker\n\nDocker manages containers. I originally thought that a container was simply a virtual instance of a computer (in other words a virtual machine or VM). However, that\u2019s incorrect with regards to how a container is actually used.\nIf you use containers as if they were VMs, then that makes it hard to elegantly and efficiently build systems with or from containers. A VM has all of the services and resources that a normal computer has: does a container have all of those resources? A VM is \u201cthe world an application lives in\u201d. Is a container a \u201cfull world\u201d?\nThere were 2 things that I read that were changed my view about containers:\n\nA (well-known) post on\u00a0Hacker News\u00a0about\u00a0how Docker contains an OS\nA paper by Burns and Oppenheimer (Google) about \u201cDesign patterns for container-based distributed systems\u201c\n\nI would recommend reading both, but for those of who you just want the raw info, here it is:\n\nContainers do not need to contain a full operating system (there goes my VM analogy!)\nDevelopment with containers has reached the point that containers are being used as units \u2014 like lego-blocks \u2014 in design patterns\n\nSo, what are containers?\n\u00a0\n\nWell, sure, what does Wikipedia say? Wiki says that containers are, in essence, \u201can operating system feature in which the kernel allows the existence of multiple isolated user-space instances\u201c. (If you have a background in operating systems or linux, then reading about\u00a0cgroups\u00a0is also interesting).\nOf course,\u00a0it\u2019s my blog post, so I\u2019ll write about it if I want to. What would\u00a0I\u00a0say that containers are? Hmm\u2026\nContainers are containers. Hahahaha.. No, really. Containers are a place for (a piece of) software. And, to paraphrase Larry Wall (of Perl fame), that piece of software in your container is probably\u00a0communicating with something else, so containers always have network interfaces or access to a shared/external filesystem.\nIn short, containers are powerful because you can use a container to\u00a0both\u00a0isolate\u00a0(or \u201cmake modular\u201d) a piece of software\u00a0and\u00a0run\u00a0a piece of software. In other words, this is the next level of modularization (and thus reuse) and execution. Other similar mechanisms to a container in the past are:\n\ncompilers (which created a binary that could be run)\nJava (isolation from the operating system)\nService Oriented Architecture (isolation/modularization per computer, group of services)\nMicroservices (isolation/modularization per function/domain)\n\nInherent in the modularization + execution model is increased scalability.\nOne of the key takeaways here is that you should not be putting multiple applications/pieces of software in 1 container. In fact, it seems like this might be an anti-pattern and \u201ctight-coupling\u201d in essence.\nActual Docker use\nIn terms of actual facts, commands and code that I learned:\n\nDocker images are described in\u00a0Dockerfiles.\n\nActually, a Dockerfile basically1\u00a0just contains\n\nthe names of what you\u2019re using,\nthe commands to set it up (RUN,\u00a0ENV,\u00a0EXPOSE, etc..) and\nthe commands to run the actual \u201ccontained\u201d application (CMD).\n\n\n\n\ndocker run\u00a0runs the container,\u00a0docker ps\u00a0shows the docker processes,\u00a0docker stop\u00a0stops the container, and\u00a0docker rm\u00a0removes the container from the system.\nThere are a couple of sites that host\u00a0Dockerfiles: you only store 1 Dockerfile per \u201crepository\u201d, although the versions may differ (but not the name).\n\nHere\u2019s an example Dockerfile:\n\nFROM java:8\r\nRUN javac MyJavaClass.java\r\nCMD [\"java\", \"MyJavaClass\"]\nKubernetes\nKubernetes makes me a little bit sad. That\u2019s how awesome it is. What makes especially me sad about Kubernetes is that it\u2019s declarative.\nIf you look back at the big picture of what we as coders (sorry, \u201csoftware engineers\u201d) have been working on for the last 30 years, we\u2019ve been 1. building systems for other people and 2. making it easier for us to do (1) faster.\nSo (2), \u201cmaking it easier for ourselves\u201d, is great for other people and it\u2019s great for building stuff quickly. However, in my opinion, most of the really hard (and interesting) problems are part of (2). And Kubernetes solves one more problem that future generations will never really have to solve again, which is a little sad (but a lot awesome).\nKubernetes solves scaling with containers2.\nBefore we go further, if you\u2019ve never looked at\u00a0D3.js, then go do that first. You\u2019ve already read this far and deserve a break. No, really, go away. Come back later if you\u2019re still interested. Yes, I mean it. BYE!\n[ 2 ] I\u2019m officially calling this the first occurance of Rietveld\u2019s theory: the less words it takes to explain something in non-technical language, the more complex and impressive that thing is.\nDeclarative programming\nYou\u2019re back! Where was I? Uhm.. Kubernetes! and D3.js! Wait, What?!?\nSo, if you\u2019re used to most (imperative) programming languages, whether it\u2019s python, bash scripts or java, then you\u2019re used to spelling out everything:\n\nint s = 0;\r\nfor( i = 0; i < 10; ++i ) {\r\n  s += i;\r\n}\nDeclarative thinking means that you don\u2019t say\u00a0what\u00a0you want to do, you just say\u00a0what\u00a0you want to do! (Hahahaha.. ). Okay, what I actually mean is that you specify the\u00a0requirements\u00a0of the task as opposed to\u00a0describing the steps\u00a0of the task. It\u2019s a paradigm shift.\nD3.js\u00a0was the first time I explicitly ran into this way of coding: you specify what you want from D3 instead of how D3 should do it. Then I also learned that SQL was declarative as well. Oh.. Duh.\nBack to Kubernetes: you tell Kubernetes what you\u00a0want,\u00a0not\u00a0how\u00a0K8s should do it. (Yeah, people use K8s as\u00a0an abbreviation for Kubernetes.) So, for example, that K8s should create a load balancer in front of 3 (identical) instances of a specific container. You put that in a Kubernetes config file (in YAML format), and it does that.\nWhat\u2019s impressive to me is the amount of network \u201cmagic\u201d that Kubernetes is doing under-the-hood. The problems Kubernetes solves are both hard and relatively new, which says something about how much research Google has been doing in the last 2 decades.\nPods and Services\u2026\nKubernetes\u00a0pods\u00a0are groupings of containers. In Burns\u2019s and Oppenheimer\u2019s paper (about container design patterns), they write that multiple-container patterns that take place within one \u201cnode\u201d are equivalent to a Kubernetes pod. An example would be the Sidecar pattern. The Sidecar pattern is 1 pod containing 2 containers: 1 container with a web server and 1 container that streams the web server\u2019s logs to somewhere else.\nKubernetes\u00a0services\u00a0are basically a way to communicate between groups of pods. (Sort of like how an ESB makes sure that different components/webservices can easily communicate with eachother. )\nMaybe a quick example of service configuration file will help:\n\nkind: Service\r\napiVersion: v1\r\nmetadata:\r\n  name: my-service\r\nspec:\r\n  selector:\r\n    myLabel: MyKey\r\n  ports:\r\n  - protocol: TCP\r\n    port: 80\r\n    targetPort: 9376\nOops, I forgot to mention that K8s also has this concept of\u00a0labels, which are key-value pairs you can attach to pods (among other things). So the\u00a0my-service\u00a0service will route communication (messages, packets, etc.. ) to all of the pods that have the\u00a0mylabel\u00a0label with a value of\u00a0MyKey. Of course, there are other\u00a0types of services\u00a0(and thus other ways to define them), but you can go read up on that yourself!\n\u2026and Deployments\n\n\nLastly, Kubernetes also has the concept of\u00a0deployments. Remember that I said that Kubernetes was\u00a0declarative? A deployment is a little bit like a firewall rule. Deployments are mostly used to manage the clustering/load-balancing of pods.\nThe documentation states: \u201cYou describe a desired state in a Deployment object, and the Deployment controller changes the actual state to the desired state at a controlled rate\u201c. It also helpfully lists some\u00a0use cases\u00a0for when you would use a deployment.\nThe reason that I compared a K8s deployment to a firewall rule is that the deployment is not only applied, Kubernetes remembers that you\u2019ve specified this. So if you then start to do things with Pods or Services that don\u2019t match up to what you specified in your Deployment, you\u2019ll run into problems.\nOne last tip: Kubernetes calls clusters of pods \u201creplica sets\u201c: you have multiple \u201creplicas\u201d or (identical) copies of a pod in a cluster, which makes up a.. replica set. You can thus describe what type of replica set you want in your K8s deployment.\nActual Kubernetes Use\nSo, how would you actually use all of this?\n\nkubectl\u00a0is our main command.\nkubectl apply -f my-deployment.yaml\u00a0applies a deployment configuration (described in the\u00a0my-deployment.yaml\u00a0file)\nIf you have a docker image and just want to run it via K8s, then you can do\u00a0kubectl run MyK8sDeplName --image=myDockerImageNameAndVersion. This command also specifies a deployment.\n\nkubectl run\u00a0takes all sorts of other options; for example, you can use\u00a0--replicas=<num-replicas>\u00a0to specify how many replicas you want in your deployment.\n\n\nIf you want more granular control over your K8s resources, you can also use\u00a0kubectl create\u00a0to create \u201cresources\u201d (pods, services, etc.).\n\nYou would usually do something like this:\u00a0kubectl create -f my-service-config.yaml.\nHere\u2019s\u00a0a slightly more complex tutorial that covers that.\n\n\nOf course, it\u2019s also helpful to be able to\u00a0get\u00a0information about the actual status of your K8s clusters. You can use the following commands to do this:\n\nYou can use\u00a0kubectl get\u00a0to\u00a0get specific information. For example,\u00a0kubectl get pods\u00a0or\u00a0kubectl get services\u00a0to list all pods or services.\nFor more\u00a0in-depth information, use the\u00a0kubectl describe\u00a0command. For example,\u00a0kubectl describe pods/nginx.\n\n\nLastly, there is of course\u00a0kubectl delete\u00a0(which\u00a0deletes\u00a0stuff..).\n\nThere are a bunch more commands you can read about\u00a0here.\nLastly, there\u2019s a funny but very interesting talk by Kelsey Hightower\u00a0here\u00a0that uses Tetris to explain Kubernetes.\n\nThe Google Cloud Platform\n\n\nTo start with, I haven\u2019t actually worked with any cloud providers yet, so this was my first hands-on experience with one.\nIt seems to me like most cloud providers are now grouping their services into 4 types:\n\n\u201cFunctions\u201d or \u201cLambdas\u201d which can be thought of as pieces of code.\n\nAWS uses \u201cLambda\u201d to describe their service.\nGoogle Cloud calls theirs \u201cFunctions\u201c.\n\n\n\u201cPlatforms\u201d which, for us Java developers, mean \u201capplication containers\u201d, roughly. For example, a tomcat or jetty instance.\n\nWith AWS, you would use\u00a0Elastic Beanstalk\u00a0to do this.\nWith Google Cloud, you would use the\u00a0App Engine.\n\n\n\u201cContainers as a Service\u201d, which is a cloud service to manage Docker and Kubernetes resources.\n\nAWS has\u00a0ECS\u00a0(Elastic Container Service) and\u00a0EKS\u00a0(Elastic Kubernetes Service).\nThe Google Cloud service is called the \u201cKubernetes Engine\u201c.\n\n\nAnd, of course, \u201cInfrastructure as a Service\u201d which is simply Virtual Machines in the cloud.\n\nAWS has\u00a0EC2\u00a0(Elastic Compute Cloud).\nThe Google Cloud service is called the \u201cCompute Engine\u201c.\n\n\n\nUsing Google Cloud\nRight.\nWhile there are a bunch of different menus available in the google cloud console, one of the primary ways to interact with your Google Cloud resources is via the \u201cGoogle Cloud Shell\u201d. In short, this is an in-browser shell that seems to take place on a (virtual) linux machine.\nIn the Google Cloud shell, you have your own home directory as well as a fairly standard linux path. Of course, there are some other commands available, such as the\u00a0gcloud,\u00a0kubectl\u00a0and\u00a0docker\u00a0commands. To tell the truth, it all seems to work magically. By that, what I really mean is that it\u2019s not clear to me why those commands \u201cjust work\u201d in the google cloud shell, but they do.\nThere were only really 4 (or 5) commands I used with Google Cloud in the shell:\n\ngcloud compute instances create <instance-name> [OPTIONS]\u00a0creates\u00a0a Google Compute Engine instance, which is a VM.\n\nAmong other options, you can specify what type of OS you want with various options.\n\n\ngcloud compute ssh <instance-name>\u00a0allows you to\u00a0ssh\u00a0to the Compute Engine instance.\nYou need\u00a0gcloud compute zones list\u00a0and\u00a0gcloud config set compute/zone <zone>\u00a0to set the timezone that an instance is in.\nLastly, for Kubernetes, I used\u00a0gcloud container clusters create <kubernetes-cluster-name>\u00a0to create a Google Kubernetes Engine instance (a.k.a. a Kubernetes cluster on the Google Cloud).\n\n\u00a0\nAnd folks, that\u2019s all I wrote!\nImage sources\n\nShip in bottle\nDeployment clones\nGoogle Cloud Ops Continuum\n\n", "tags": [], "categories": ["Blog", "Cloud"]}
{"post_id": 16906, "title": "The importance of collaboration emphasized at the Luminis annual software developers\u2019 conference", "url": "https://www.luminis.eu/blog-en/importance-collaboration-emphasized-luminis-annual-software-developers-conference/", "updated_at": "2018-04-17T11:47:08", "body": "The software technology company Luminis held its annual developer community event on Thursday 12th\u00a0of April 2018. The Luminis DevCon highlights the latest software trends and technologies. \u201cSoftware is changing the world\u201d, stated Luminis\u2019 CEO: Hans Bossenbroek, at the opening of Devcon; \u201cand these changes demand that we find new ways to collaborate and share knowledge\u201d. The 500+ conference participants were called on to participate in the development of Open Source software.\nChildren learn to program\nHans Bossenbroek, CEO van Luminis\nIn his opening keynote talk, the British researcher Sam Aaron explained how music is being used to stimulate children to learn computer programing. The Sonic Pi\u00a0developed by his team at Cambridge University provides and Open Source environment to children and schools that introduces them to the fundamentals of programing, while composing and performing music through play and having fun.\nOpen Source development\nDuring the morning keynote presentation, Ren\u00e9 van Hees (Chief Software Architect of Thales Nederland) was joined by\u00a0Hans Bossenbroek (CEO of Luminis) to explain the important role of collaboration in the Open Innovation project: INEATICS. INEATICS is a project in which both Thales and Luminis participate to develop a scalable, architectural foundation, suitable for complex data-intensive applications such as defense systems. The project addresses a range of requirements including robustness, high-availability as well as security and scalability.\nLuminis has been an active developer and contributor of Open Source software since its inception. Luminis founded the Amdatu Foundation\u00a0that develops modular components that may be used and reused across a wide range of modern software systems.\nLuminis DevCon\nThe Devcon held at the CineMec in Ede on 12th April 2018 was the fourth annual conference organized by Luminis. The conference focusses on knowledge and experience sharing between IT professionals and features multiple interactive sessions from renowned national and international speakers. Participants were also invited to\u00a0join\u00a0demonstration-sessions during the breaks; a notable feature of this 2018 event was the opportunity to drive advanced racing simulators, powered by rFactor2, while being coached by Rudy van Buren McLaren\u2019s Formula1 simulator driver-in-residence. rFactor2 is developed by Studio397\u00a0\u2013 part of the Luminis group of companies.\n\n\n\n\n\n\n\t\t\t\tLuminis DevCon 2018\n\t\t\t\t\n\n\n\n\n\n\n\n\t\t\t\tRafa\u00ebla Breed \u2013 Luminis DevCon 2018\n\t\t\t\t\n\n\n\n\n\n\n\n\t\t\t\tSam Aaron \u2013 Sonic Pi\n\t\t\t\t\n\n\n\n\n\t\t\t\tImpressie van Luminis DevCon 2018\n\t\t\t\t\n\n\u00a0\n", "tags": [], "categories": ["Blog"]}
{"post_id": 1406, "title": "Racing into the future! [DevCon 2018]", "url": "https://www.luminis.eu/blog-en/development-en/racing-into-the-future-devcon-2018/", "updated_at": "2020-11-16T17:04:41", "body": "rFactor 2 is a realistic, easily extendable racing simulation from Studio 397. It offers the latest in vehicle and race customization, great graphics, outstanding multiplayer and the height of racing realism.\nIt features mixed class road racing with ultra realistic dynamics, an immersive sound environment and stunning graphics, perfect for top-level esports and a rich single-player experience. In this talk we will look at the software architecture for real-time simulation and high quality 3D graphics. We will demonstrate how the original monolithic C++ codebase was extended into a platform that leverages cloud computing and builds on a Java back-end paired with an Angular based front-end. Last year we ran a major competition for McLaren\u2019s World\u2019s Fastest Gamer and saw the rFactor 2 champion win and get a contract at McLaren. This competition and others run on a scalable back-end that leverages Luminis\u2019 Cloud RTI, Information Grid and Amdatu modular software building blocks. It keeps track of events, scoring, results and rankings for the individual drivers and teams. You will learn more about how to evolve a codebase, how to embed Java into a real-time simulation and how to use modularization techniques to extend an existing codebase well beyond its original scope and features.\n", "tags": [], "categories": ["Blog", "Development"]}
{"post_id": 16818, "title": "Jeroen Bouvrie appointed as head of Luminis Netherlands", "url": "https://www.luminis.eu/blog-en/jeroen-bouvrie-appointed-head-luminis-netherlands/", "updated_at": "2018-04-04T14:33:05", "body": "Luminis has announced that Jeroen Bouvrie has been appointed as head of Luminis Netherlands with immediate effect. This appointment is one of a number of internal changes in response to Luminis\u2019 continued growth \u2013 both in the Netherlands and internationally. It coincides with the 16th anniversary of Luminis\u2019 foundation.\nJeroen Bouvrie (left) & Ren\u00e9 Janssen (right)\nJeroen Bouvrie is a founder member of the Luminis group of companies and has led its Arnhem-based business for several years. Next to maintaining the solid growth that Luminis has demonstrated over the past years, Jeroen will oversee the start of additional Luminis locations in the Netherlands, starting with Eindhoven\n\u201cAlthough we offer our services across the Netherlands, it is also important to offer our colleagues with a regional home-base and our customers with a local contact point\u201d \u2013 according to Bouvrie.\nRen\u00e9 Janssen has worked as a senior member of Luminis Arnhem\u2019s management team for several years, will succeed Jeroen Bouvrie and plans to \u201cdo even more for and with our customers, to help them grow and excel\u201d as stated by Ren\u00e9.\nJeroen will succeed Laurens Miedema, who has been appointed as CFO of Luminis\u2019 parent company, where he will play a key role in Luminis\u2019 further internationalization and the commercialization of its cloud technology products. \u201cthere is a huge potential market for our cloud and data platforms, but we will need to develop a sophisticated financing strategy to exploit this potential\u201d stated Miedema.\nAbout\u00a0Luminis\nLuminis is a technology group, focused on clients for whom information technology and the Internet have strategical value. Luminis\u2019 specializes in applying the latest technology and develops its own Cloud-technology and data-management products. Its combination of services and technology products are the foundation\u00a0of innovative solutions for clients in business, industry and government.\u00a0\n", "tags": [], "categories": ["Blog"]}
{"post_id": 16775, "title": "Brain Upgrade about Quantum Computing!", "url": "https://www.luminis.eu/no-category/brain-upgrade-quantum-computing/", "updated_at": "2018-03-22T10:20:26", "body": "On May 2nd we are hosting a Brain Upgrade session in Eindhoven. Angelo van der Sijpt, fellow of Luminis in Eindhoven, will give you all the ins and outs about Quantum Computing.\nCherry-red sports cars go up into space, the most traded currency has no physical equivalent, and quantum computing is just around the corner: the future is now. While we can\u2019t help you launch your vehicle into a trans-Mars injection orbit, we can help you understand what you need to know about quantum computing.\nCompanies like JP Morgan and Daimler are already investing in quantum computing, Microsoft has released Q# as an experimental language for programming quantum computers, and IBM is actually allowing us developers to play around with real quantum computers.\n\nThis is your chance to be ahead of the curve, and have a mental survival guide for the upcoming quantum nonsense storm!\n\n\nIn this session, we will spend only a little time on the theory, and spend most of it on actual working code that shows you what things QC is good at, and how it is different from so-called \u201cclassical\u201d computing. We will use Microsoft\u2019s Q#, which comes with a powerful simulator, so you can run all examples on your own computer, and explore this new space.\nWhile we can\u2019t promise to make you into a full-fledged quantum programmer, you will end up with a better grasp of what is, and what isn\u2019t actually quantum. At the very least, you\u2019ll get to brag that you were at the head of the quantum wave (!)\n", "tags": [], "categories": ["No category"]}
{"post_id": 1368, "title": "Creating an Elastic Canvas for Twitter while visiting Elasticon 2018", "url": "https://www.luminis.eu/blog-en/search-en/creating-an-elastic-canvas-for-twitter-while-visiting-elasticon-2018/", "updated_at": "2020-11-24T15:13:27", "body": "The past week we visited Elasticon 2018 in San Francisco. In our previous blog post we wrote about the Keynote and some of the more interesting new features of the elastic stack. In this blog post, we take one of the cool new products for a spin: Canvas. But what is Canvas?\nCanvas is a composable, extendable, creative space for live data With Canvas you can combine dynamic data, coming from a query against Elasticsearch for instance, with nice looking graphs. You can also use tables, images and combine them with the data visualizations to create stunning, dynamic infographics. In this blog post, we create a Canvas about the tweets with the tags Elasticon during the last day of the elastic conference last week.\nBelow is the canvas we are going to create. It contains a number of different elements. The top row contains a pie chart with the language of the tweets, a bar chart with the number of tweets per time unit, followed by the total tracked tweets during the second day of elasticon. The next two elements are using the sentiment of the tweets. This was obtained using IBM Watson. Byron wrote a basic integration with Watson, he will give more details in a next blog post. The pie chart shows the complete results, the green smiley on the right shows the percentage of positive tweets of the total number of tweets that we could analyze without an error or be neutral.\n\nWith the example in place, it is time to discuss how to create these canvasses yourself. First some information about the installation. A few of the general concepts and finally sample code for the used elements.\nInstalling canvas\nCanvas is part of elastic Kibana. You have to install canvas as a plugin into Kibana. You do need to install X-Pack in Elasticsearch as well as in Kibana. The steps are well described in the\u00a0installation page of Canvas. Beware though, installing the plugins in Kibana takes some time. They are working on improving this, but we have to deal with it at the moment.\nIf everything is installed, start\u00a0Kibana in your browser. At this moment you could start creating the canvas, however, you have no data. So you have to import some data first. We used Logstash with a twitter input and elastic output. Cannot go into to much detail or else this blog post will be way too long. Might do this is a next blog post. For now, it is enough to know we have an index called twitter that contains tweets.\nCreating the canvas with the first element\nWhen clicking on the tab Canvas we can create a new Workpad. A Workpad can contain one of the multiple pages and each page can contain multiple elements. A Workpad defines the size of the screen. Best is to create it for a specific screen size. At elasticon they had multiple monitors, some of them horizontal, others vertical. It is good to create the canvas for a specific size. You can also choose a background color. These options can be found on the right side of the screen in the Workpad settings bar.\nIt is good to know that you can create a backup of you Workpad from the Workpads screen, there is a small download button on the right side. Restoring a dashboard is done by dropping the exported JSON into the dialog.\n\nTime to add our first element to the page. Use the plus sign at the bottom of the screen to add an element. You can choose from a number of elements. The first one we\u2019ll try is the pie chart. When adding the pie chart, we see data in the chart. Hmm, how come, we did not select any data. Canvas comes with a default data source, this data source is used in all the elements. This way we immediately see what the element looks like. Ideal play around with all the options. Most options are available using the settings on the right. With the pie, you\u2019ll see options for the slice labels and the slice angles. You can also see the\u00a0Chart style\u00a0and\u00a0Element style. These configuration options show a plus signed button. With this button, you can add options like color pallet and text size and color. For the element, you can set a background color, border color, opacity and padding\n\nNext, we want to assign our own data source to the element. After adding our own data source we most likely have to change the parameters for the element as well. In this case, we have to change the Slice labels and angles. Changing the data source is done using the button at the bottom, click the\u00a0Change Datasource\u00a0button/link. At the moment there are 4 data sources: demo data, demo prices, Elasticsearch query and timeline query. I\u2019ll choose the Elasticsearch query, select the index, don\u2019t use a specific query and select the fields I need. Selecting the fields I need can speed up the element as we only parse the data that we actually need. In this example, we only use the sentiment label.\n\nThe last thing I want to mention here is the Code view. After pushing\u00a0the >_ Code\u00a0button you\u2019ll see a different view of your element. In this view, you\u2019ll get a code approach. This is more powerful than the settings window. But with great power comes great responsibility. It is easy to break stuff here. The code is organized in different steps. The output of each step is, of course, the input for the next step. In this specific example, there are five steps. First a filter step, next up the data source, then a point series that is required for a pie diagram. Finally the render step. If you change something using the settings the code tab gets updated immediately. If I add a background color to the container, the render step becomes:\nrender containerStyle={containerStyle backgroundColor=\"#86d2ed\"}\r\n\nIf you make changes in the code block, use the\u00a0Run\u00a0button to apply the changes. In the next sections, we will only work in this code tab, just because it is easier to show to you.\n\nAdding more elements\nThe basics of the available elements or function are\u00a0documented here. We won\u2019t go into details for all the different elements we have added. Some of them use the defaults and therefore you can add them yourselves easily. The first one I do want to explain is the Twitter logo with the number of tweets in there. This is actually two different elements. The logo is a static image. The number is more interesting. This makes use of the\u00a0escount\u00a0function and the markdown element. Below is the code.\n\nfilters\r\n | escount index=\"twitter\"\r\n | as num_tweets\r\n | markdown \"{{rows.0.num_tweets}}\" font={font family=\"'Open Sans', Helvetica, Arial, sans-serif\" size=60 align=\"left\" color=\"#ffffff\" weight=\"undefined\" underline=false italic=false}\nThe filters are used to facilitate filtering (usually by time) using the special filter element. The next item is escount which does what you expect. It counts the number of items in the provided index. You can also provide a query to limit the results, but we did not need it. The output for escount is a number. This is a problem when sending it to a markdown element. The markdown element only accepts a datatable. Therefore we have to use the function\u00a0as. This accepts a number and changes it into a datatable. The markdown element accepts a table and exposes it as rows. Therefore we use the rows to obtain the first row and of that row the column num_tweets. When playing with this element it is easy to remove the markdown line, Canvas will then render the table by default. Below the output for only the first two rows as well as the changes after adding the third line (as num_tweets)\n\n200\n\nnum_tweets #\r\n\r\n200\nNext up are the text and the photo belonging to the actual tweets. The photo is a bit different from the Twitter logo as it is a dynamic photo. In the code below you can see that the image element does have a data URL attribute. We can use this attribute to get one cell from the provided data table. The getCell function has attributes for the row number as well as the name of the column.\nesdocs index=\"twitter*\" sort=\"@timestamp, desc\" fields=\"media_url\" count=5 query=\"\"\r\n | image mode=\"contain\" dataurl={getCell c=\"media_url\" r=2}\r\n | render\nWith the text of the tweet, it is a bit different. Here we want to use the markdown widget, however, we do not have the data URL attribute. So we have to come up with a different strategy. If we want to obtain the third item, we select the top 3 and from the top 3, we take the last item.\nfilters \r\n| esdocs index=\"twitter*\" sort=\"@timestamp, desc\" fields=\"text, user.name, created_at\" query=\"\" \r\n| head 3 \r\n| tail 1 \r\n| mapColumn column=created_at_formatted fn=${getCell created_at | formatdate 'YYYY-MM-DD HH:mm:ss'} \r\n| markdown \"{{#each rows}}\r\n**{{'user.name'}}** \r\n\r\n(*{{created_at_formatted}}*)\r\n\r\n{{text}}\r\n{{/each}}\" font={font family=\"'American Typewriter', 'Courier New', Courier, Monaco, mono\" size=18 align=\"right\" color=\"#b83c6f\" weight=\"undefined\" underline=false italic=false}\nThe row that starts with mapColumn is a way to format the date. The mapColumn can add a new column with the name as provided by the column attribute and the value as the result of a function. The function can be a chain of functions. In this case, we obtain the column create_at of the datatable and pass it to the format function.\nCreating the partly green smiley\nThe most complicated feature was the smiley that turns green the more positive tweets we see. The positiveness of the tweets was determined using IBM Watson interface. In the end, it is the combination of twee images, one grey smiley, and one green smiley. The green smiley is only shown for a specific percentage. This is the\u00a0revealImage\u00a0function. First, we show the complete code.\n\nesdocs index=\"twitter*\" fields=\"sentiment_label\" count=10000 \r\n| ply by=\"sentiment_label\" fn=${rowCount | as \"row_count\"} \r\n| filterrows fn=${if {getCell \"sentiment_label\" | compare \"eq\" to=\"error\"} then=false else=true}\r\n| filterrows fn=${if {getCell \"sentiment_label\" | compare \"eq\" to=\"neutral\"} then=false else=true}\r\n| staticColumn column=\"total\" value={math \"sum(row_count)\"} \r\n| filterrows fn=${if {getCell \"sentiment_label\" | compare \"eq\" to=\"positive\"} then=true else=false}\r\n| staticColumn column=\"percent\" value={math \"divide(row_count, total)\"} \r\n| getCell \"percent\" \r\n| revealImage image={asset \"asset-488ae09a-d267-4f75-9f2f-e8f7d588fae1\"} emptyImage={asset \"asset-0570a559-618a-4e30-8d8e-64c90ed91e76\"}\nThe first line is like we have seen before, select all rows from the twitter index. The second row does kind of a grouping of the rows. It groups by the values of sentiment_label. The value is a row count that is specified by the function. If I remove all the other rows we can see the output of just the ply function.\nsentiment_label         row_count\r\nnegative                32\r\npositive                73\r\nneutral                 81\r\nerror                   14\nThe next steps filter out the rows for error and neutral, then we add a column for the total number of tweets with a positive or negative label. Now each row has this value. Check the following output.\nsentiment_label         row_count       total\r\nnegative                32              105\r\npositive                73              105\nThe next line removes the negative row, then we add a column with the percentage, obtain just one cell and call the revealImage function. This function has a number input and attributes for the image as well as the empty or background image.\nThat gives us all the different elements on the canvas.\nConcluding\nWe really like the options you have with Canvas. You can easily create good-looking dashboard that contains static resources, help texts, images combined with dynamic data coming from Elasticsearch and in the future most likely other resources.\nThere are some improvements possible of course. It would be nice if we could also select doc_value fields and using aggregations in a query would be nice as well. We will closely monitor the progression as well believe this is going to be a very interesting technology to keep using in the future.\n", "tags": [], "categories": ["Blog", "Search"]}
{"post_id": 1229, "title": "Creating a Bluetooth Low Energy Central Implementation on Android", "url": "https://www.luminis.eu/blog-en/creating-a-bluetooth-low-energy-central-implementation-on-android/", "updated_at": "2020-11-10T09:49:36", "body": "At Luminis Arnhem we are currently working together with Nedap on the MACE project, a prototype implementation of a site access control system. In the MACE scenario, the users can access a physical location/open a door using a digital identity card on their mobile device. These digital identities replace those cumbersome physical cards that either take up space in your wallet or always seem to disappear when you need them.\nThe MACE mobile application receives these identities from a server application, which are then sent to a MACE reader device via Bluetooth Low Energy (BLE), Near Field Communication (NFC) or can be read by the reader via a Quick Response (QR) code. The idea is that if the correct identity is read by the reader, i.e. an identity that has the authority to gain access to the location guarded by the reader, then the user gains access to a physical location.\nIn order to create an initial prototype we needed to delve into Android BLE. In this blog I will describe how BLE works in the MACE scenario as well as how the implementation looks like in Android.\n\nFor more information on the MACE project itself, visit the MACE homepage.\nOn BLE\u00a0\nThere are two roles in BLE communication. These are the central and peripheral roles. The central role is responsible for scanning for advertisements which are made by peripherals. Once the central and peripheral are in range of each other, the central will start to receive these advertisements and can choose to connect to a peripheral.\n\nThink of it as the central being a job seeker and a peripheral being one of those recruiters on LinkedIn. What I\u2019m trying to say about peripherals is that they continuously (in most cases) advertise without even knowing if the central has any interest or use for its services. Luckily, the central can use a filter to only see advertisements that it finds interesting. This is done based on a UUID filter, which on the application level only shows advertisements that contain a service with that UUID.\nEach advertisement contains (at least) the following information:\n\u2013Bluetooth Device Address: Similar to a MAC address used to identify a BLE device.\n\u2013Device name: A custom name for the peripheral device which can be configured.\n\u2013RSSI value: The Received Signal Strength Indicator in decibel per meter. This is a value that indicates the strength of the signal, ranging from -100 to 0. The closer the RSSI is to 0, the better the signal is.\n\u2013List of Services: A list of services that are provided by the peripheral. In the BLE scenario, a service is a collection of characteristics. A characteristic contains a single value and a number of descriptors. Descriptors describe the value contained in the characteristic, such as a max length or type.\n\nThe image above illustrates the hierarchy of these concepts. The line between Peripheral and Service depicts the \u201cPeripheral has one or many Services\u201d relationship.\nThe rest of this blog will describe the building blocks for creating an Android BLE central implementation, such as scanning for advertisements, processing an advertisement, connecting to a device (peripheral) and reading and writing, from and to a BLE peripheral. We will not discuss BLE in more depth, as that is not the goal of this blog.\nMain act: Android implementation\nAsk nicely\nThe first thing we need to do is to ask for permission to turn on Bluetooth on the mobile device and use it. For Android versions lower than 6.0, we only need to add the permissions to the Android Manifest:\n<uses-permission android:name=\u201candroid.permission.BLUETOOTH\"/> (ble_permission.png)\nFor BLE on Android 6.0 or higher, we noticed that our app was crashing when trying to use the Bluetooth functionality. It turns out that you need to add one of the following permissions to the Android Manifest:\n<uses-permission android:name=\"android.permission.ACCESS_COARSE_LOCATION\"/>\r\n<uses-permission android:name=\"android.permission.ACCESS_FINE_LOCATION\"/>\nThese permissions are required for determining the rough or precise location of the mobile device respectively. You might be wondering: Why do I need these permissions that are related to location to turn on BLE? Good question. I believe it has something to do with the fact that the signal strength of your mobile device to a BLE peripheral can be used in combination with other sources to determine your location. I have not yet looked further into this. However in the MACE application we do use the RSSI value to estimate how far you are from the MACE reader, so it is not implausible.\nAs of Android 6.0, users now get prompted on whether to give an app certain permissions while the app is running, instead of simply accepting all permissions when installing the app. We have to change our code to ask the user for this permission the first time they try to use the BLE functionality in the app:\nif (activity.checkSelfPermission(Manifest.permission.ACCESS_COARSE_LOCATION) != PackageManager.PERMISSION_GRANTED) {\r\nactivity.requestPermissions(new String[]{Manifest.permission.ACCESS_COARSE_LOCATION}, 1);\r\n}\nThis code checks if the permission is already granted. If that is not the case, the user will be prompted with the request to give the ACCESS_COARSE_LOCATION permission. To handle the result of this action, you need to override the onRequestPermissionResult method. See this page for more info.\nIn case Bluetooth is turned off on the mobile device, we also need to ask the user if we can turn it on. To do this, we need an instance of the Bluetooth adapter. We will discuss this is the next section.\nBluetooth turn-ons\nNow that we have politely asked for permission from the user to use Bluetooth, it is time to get to work. The first thing we should do is get the Bluetooth Adapter instance.\u00a0The Bluetooth Adapter instance can be retrieved using the BluetoothManager. If there is no BluetoothManager present, this means that the device does not support Bluetooth. An example:\nBluetoothManager bluetoothManager = (BluetoothManager) context.getSystemService(Context.BLUETOOTH_SERVICE);\r\nif (bluetoothManager == null){\r\n//Handle this issue. Report to the user that the device does not support BLE\r\n} else {\r\nBluetoothAdapter adapter = bluetoothManager.getAdapter();\r\n}\nOnce we have the adapter, we can get the party started.\u00a0As mentioned before, we first need to check if Bluetooth is turned on for the mobile device. If this is not the case, we will ask the user to turn this on. This can be done in the following way:\nif(adapter != null && !adapter.isEnabled()){\r\nIntent intent = new Intent(BluetoothAdapter.ACTION_REQUEST_ENABLE);\r\nactivity.startActivityForResult(intent, 1);\r\n}else{\r\nSystem.out.println(\"BLE on!\");\r\n//do BLE stuff\r\n}\nThe handling of this request is done in the same way as when we asked for the permissions in the previous section.\u00a0From this point on, we know that BLE is up and running on the mobile device.\nHello\u2026is it me you\u2019re looking for?\nIt\u2019s time to start scanning! When we start the scanner, the mobile device will start receiving advertisements of BLE peripherals around it (range varies per device).\nWith the BluetoothAdapter we retrieved in the previous section, you can get an instance of a BLE scanner. We need to supply this scanner with at least an implementation of a ScanCallback, which describes what we should do with the results of the scan. Optionally, we can also supply the scanner with filters and settings. The filters help us specify our search in order to find the devices we are looking for. The settings are used to determine how the scanning should be performed.\nThis is a simple example of how to do all of this and get the scanner running:\npublic void startScanning(){\r\n\r\nBluetoothLeScanner scanner = adapter.getBluetoothLeScanner();\r\nScanSettings scanSettings = new ScanSettings.Builder().setScanMode(ScanSettings.SCAN_MODE_LOW_LATENCY).build();\r\nList<ScanFilter> scanFilters = Arrays.asList(\r\nnew ScanFilter.Builder()\r\n.setServiceUuid(ParcelUuid.fromString(\"some uuid\"))\r\n.build());\r\n\r\nscanner.startScan(scanFilters, scanSettings, new MyScanCallback());\r\n}\r\n\r\npublic class MyScanCallback extends ScanCallback {\r\n\r\n@Override\r\npublic void onScanResult(int callbackType, final ScanResult result) {\r\n//Do something with results\r\n}\r\n\r\n@Override\r\npublic void onBatchScanResults(List<ScanResult> results) {\r\n//Do something with batch of results\r\n}\r\n\r\n@Override\r\npublic void onScanFailed(int errorCode) {\r\n//Handle error\r\n}\r\n}\nA few things to note:\n-There are a few scan modes available. SCAN_MODE_LOW_LATENCY has the highest frequency of scanning and will thus cause more battery drain. SCAN_MODE_LOW_POWER has the lowest frequency and is best for battery usage. This mode would mostly be used for background scanning or if a low frequency of results is acceptable for your application. SCAN_MODE_BALANCED is somewhere in between the previously mentioned modes. SCAN_MODE_OPPORTUNISTIC is a special scan mode, where the application itself does not scan but instead listens in on other applications scan results.\n-The scan filter in this case is used for only detecting advertisements that contain a certain uuid as a service. All other advertisements will be ignored.\n-The ScanCallback has a onBatchScanResults function, which is only called when a flush method is called on the scanner. The scanner has the ability to queue scan results before calling the callback method, however I have not looked further into this.\n-In a real application, you might want to keep a reference to the scanner so that you can stop it from scanning when this is necessary.\nSo many advertisements\u2026\n\nA ScanResult object is returned for each advertisement scanned. The frequency of advertisements received is dependent on the scan mode. The scan callback will get called for each advertisement. This can become overwhelming. In the MACE project, we stop scanning as soon as we find a device we want to connect with.\nThe ScanResult\u00a0object contains some information on the Device such as the BLE address, the service uuids and the name given to the device. The ScanResult object also contains the RSSI, which tells you roughly how close the mobile device is to the peripheral device the advertisement belongs to.\nWith the information in the ScanResult it is possible to do a second-level filtering, such as checking if the peripheral is close enough using RSSI or checking if the device name is what you expect it to be. If everything checks out, we can proceed to connecting to the peripheral.\nGive it everything you gatt!\nThe concept of two BLE devices communicating with each other via services and characteristics is called Generic Attribute Profile, GATT for short. In order to use this in the mobile application, we need a BluetoothGatt instance. This can be obtained in the following way:\n@Override\r\npublic void onScanResult(int callbackType, final ScanResult result) {\r\nBluetoothDevice device = adapter.getRemoteDevice(result.getDevice().getAddress());\r\nBluetoothGatt gatt = device.connectGatt(mContext, false, new myGattCallBack());\r\n}\nWe first get an instance of the device using the BLE address obtained in the scan result. Using this device object we then call the connectGatt function, which gives us a Bluetooth Gatt instance.\nThe first parameter in the connectGatt method is the Android App context. The second parameter is indicates whether or not we should automatically connect to the device once it appears. This concept is also known as having devices paired with each other. Since we have commitment issues, we set this to false.\nThe last parameter is the callback which is called by the BluetoothGatt instance when there is a response from the peripheral. The callback should extend the BluetoothGattCallback class.\nYou should create your own implementation and override (at least) the following methods:\n-onConnectionStateChange(BluetoothGatt gatt, int status, int newState)\n-onServiceDiscovered(BluetoothGatt gatt, int status)\n-onCharacteristicRead(BluetoothGatt gatt, BluetoothGattCharacteristic characteristic, int status)\nOnce you call the connectGATT method, the onConnectionStateChange method will be called. If the newState value is 2 (Connected), we can carry on. If this is not the case, then we cannot communicate with the peripheral and all other operations on the gatt instance will fail.\u00a0Assuming we have connected successfully with the peripheral device, we can now trigger the service discovery by calling:\ngatt.discoverServices();\nwhich asks the peripheral device for a list of all services. These services are then loaded into the gatt instance. \u00a0The onServiceDiscovered method will be called with a status of 0 if it was successful. If this is the case, we can call:\nList<BluetoothGattService> services = gatt.getServices();\nsince the gatt instance now has these services after the discovery. Note that these services also contain a list of characteristics.\u00a0From this point on it is a matter of finding the right characteristic you are looking for within the right service to read from or write to.\nThese are not the characteristics you are looking for\u2026\nIt is important to make an agreement as to which UUIDS will be used for which services and characteristics on the peripheral device. These UUIDS need to be known on the mobile application. Using these UUIDS, we can loop through the services in the following way:\nprivate String serviceUUID = \"xyz\";\r\nprivate String characteristicUUID = \"xyz\";\r\nprivate BluetoothGattCharacteristic characteristic = null;\r\n\r\nprivate void findCharacteristic(List<BluetoothGattService> services){\r\nfor(BluetoothGattService service: services){\r\nif(service.getUuid().toString().equalsIgnoreCase(serviceUUID){\r\nfor(BluetoothGattCharacteristic serviceCharacteristic : service.getCharacteristics()){\r\nif(serviceCharacteristic.getUuid().toString().equalsIgnoreCase(characteristicUUID)) {\r\ncharacteristic = serviceCharacteristic;\r\n}\r\n}\r\n}\r\n}\r\n}\nNow we actually have an object that represents the characteristic that we want to read from or write to.\nChange the world around you\nWe have reached the final step in this blog. What we want to do is finally read from and write to a peripheral device. We do this by using the characteristic object obtained in the previous section in combination with our GATT instance. To read a data in the characteristic, we simply call:\nboolean successfullyRead = gatt.readCharacteristic(characteristic);\r\n\r\n@Override\r\npublic void onCharacteristicRead(BluetoothGatt gatt, BluetoothGattCharacteristic characteristic, int status){\r\nbyte[] characteristicValue = characteristic.getValue();\r\n//Do something with the value\r\n}\nThe onCharacteristicRead method is called with the characteristic. You can get the value in the characteristic by calling characteristic.getValue(), which returns a byte array with the value that the characteristic contains.\u00a0To write to the characteristic, we first have to set the value we want to write into the characteristic object we obtained. Once this is set, we can call write characteristic using the gatt instance:\nbyte[] valueToWrite = new byte[8];\r\nArrays.fill(valueToWrite, (byte) 0x00);\r\ncharacteristic.setValue(valueToWrite);\r\nboolean successfullyWritten = gatt.writeCharacteristic(characteristic);\nThe boolean successfullyWritten is true if the write action was successful, otherwise it returns false.\nEpilogue\nYou now have the basic building blocks to build an Android BLE central implementation. What you do from here on out is up to your imagination. Of course we couldn\u2019t cover everything in this blog. Certain things such as threading, error handling and battery optimisation were omitted in the hope to keep this entry concise. These topics will need to be covered in another blog.\nIn any case, you now know the essentials. Have fun and remember, scan responsibly.\n\u00a0\n", "tags": [], "categories": ["Blog", "Development"]}
{"post_id": 1369, "title": "Elasticon 2018 Day 1", "url": "https://www.luminis.eu/blog-en/search-en/elasticon-2018-day-1/", "updated_at": "2020-11-11T08:22:02", "body": "The past few days have been fantastic. Together with Byron I am visiting San Francisco. We have seen amazing sights, but yesterday the reason why we came started. Day one of Elasticon starts with the keynote showing us cool new features to come and sometimes some interesting news. In this blog post, I want to give you a short recap of the keynote and tell you what I think was important.\nRollups\nWith more and more data finding its way to elasticsearch, some indexes become too large for their purpose. We do not need to keep all data of the past weeks and months. We just want to keep the data needed for aggregations we show on a dashboard. Think about an index containing logs of our web server. We have a chart with HTTP status codes, response times, browsers, etc. Now you can create a rollup configuration providing the aggregations we want to keep, containing a cron expression telling when to run and some additional information about how much data to keep. The result is a new index with a lot less data that you can keep for your dashboards.\nCanvas\nLast year at Elasticon Canvas was already shown. Elastic continued with the idea and it is starting to look amazing. With Canvas you can create beautiful looking dashboards that go a big step further than the standard dashboards in Kibana. You can customise almost everything you see. It comes with options to put an image on the background, a lot of color options, new sort of data visualisation integrated of course with elasticsearch. In a next blog post I\u2019ll come up with a demo, it is looking very promising. Want to learn more about it, check this\u00a0blog post.\nKubernetes/Docker logging\nOne of the areas I still need to learn is the Docker / Kubernetes ecosystem. But if you are looking for a solution to monitor you complete Kubernetes platform, have a look at all the options that elastic has these days. Elastic has impressive support to monitor all the running images. It comes with standard dashboards in Kibana. It now has a dedicated space in Kibana called the\u00a0Infra\u00a0tab. More information about the options and how to get started\u00a0can be found here.\nPresenting Geo/Gis data\nA very cool demo was given on how to present data on a Map. The demo showed where all Elasticon attendees were coming from. The visual component has an option of creating different layers. So you can add data to give the different countries a color based on the number of attendees. In a next layer show the bigger cities where people are coming from in small circles. Use a logo of the venue in another layer. Etc. Really interesting if you are into geodata. In all makes use of the Elastic Maps Service. If you want more information about this, you can\u00a0find it here.\nElastic Site Search\nUp till now there was news about new ways to handle your logs coming from application monitoring, infrastructure components, application logs. We did not hear about new things around search, until showing the new product called Elastic Site Search. This was previously known as Swiftype. With Google naming its product google search appliance end of life, this is a very interesting replacement. Working with relevance, synonyms, search analytics is becoming a lot easier with this new product. More information\u00a0can be found here.\nElastic cloud sizing\nIf you previously looked at the cloud options elastic offers, you might have noticed that choosing elastic nodes did not give you a lot of flexibility. When choosing the amount of required memory, you also got a fixed amount of disk space. With the upcoming release, you have a lot more flexibility when creating your cluster. You can configure different flavours of clusters. One of them being hot-warm cluster. With specific master nodes, hot nodes for recent indexes with more RAM and faster disks, warm nodes containing the older indices with bigger disks. This is a good improvement if you want to create a cluster in the cloud. More information\u00a0can be found here.\nOpening up X-Pack\nShay told a good story about creating a company that supports an open source product. Creating a company only on support is almost impossible in the long run. Therefore they started working on commercial additions now called the X-Pack. Problem with these products was that the code was not available. Therefore working with elastic to help them improve the product was not possible. Therefore they are now opening up their repositories. Beware, it is not becoming free software. You still need to pay, but now it becomes a lot easier to interact with elastic about how stuff works. Next to that, they are going to make it easier to work with the free stuff in X-Pack. Just ask for a license once instead of every year again. And if I understood correct, the download will contain the free stuff in easier installation packages. More information about the what and why in this\u00a0blog post from Shay Banon.\nConference Party\nNice party but I had to sign a contract to prohibit me from telling stories about the party. I do plan on writing more blog post the coming two days.\n", "tags": [], "categories": ["Blog", "Search"]}
{"post_id": 1726, "title": "A practical guide to the quirks of Kafka Streams", "url": "https://www.luminis.eu/blog-en/search-en/a-practical-guide-to-the-quirks-of-kafka-streams/", "updated_at": "2020-11-25T14:33:03", "body": "Kafka Streams is a lightweight library that reads from a Kafka topic, does some processing and writes back to a Kafka topic. It processes messages one by one but is also able to keep some state. Because it leverages the mechanics of Kafka consumer groups, scaling out is easy. Each instance of your Kafka Streams application will take responsibility for a subset of topic partitions and will process a subset of the keys that go through your stream. \nKafka Streams at first glance\nOne of the first things we need to keep in mind when we start developing with Kafka Streams is that there are 2 different API\u2019s that we can use. There is the high-level DSL (using the\u00a0KStreamsBuilder) and the low-level Processor API (using the\u00a0TopologyBuilder). The DSL is easier to use than the Processor API at the cost of flexibility and control over details.\nWe will begin our journey by learning more about the DSL, knowing that sooner or later we will have to rewrite our application using the Processor API. It doesn\u2019t take long before we stumble upon the word count example.\nval source: KStream[String, String] = KStreamBuilderS.stream[String, String](\"streams-file-input\")\r\nval counts: KTableS[String, Long] = source\r\n  .flatMapValues { value => value.toLowerCase(Locale.getDefault).split(\" \") }\r\n  .map { (_, value) => (value, value) }\r\n  .groupByKey\r\n  .count(\"Counts\")\nEasy! Let\u2019s just skip the documentation and finish our application!\nOur first Kafka Streams application\nUnfortunately, we soon return to the documentation because each aggregation on our\u00a0KStream\u00a0seems to return a\u00a0KTable\u00a0and we want to learn more about this\u00a0stream/table duality. Aggregations also allow for windowing, so we continue reading about\u00a0windowing. Now that we know something about the theoretical context, we return to our code. For our use case, we need 1 aggregate result for each window. However, we soon discover that each input message results in an output message on our output topic. This means that all the intermediate aggregates are spammed on the output topic. This is our first disappointment.\nA simple join\nThe DSL has all the usual suspects like\u00a0filter,\u00a0map, and\u00a0flatMap. But even though\u00a0join\u00a0is also one of those usual suspects that we have done a thousand times, it would be best to read the documentation on\u00a0join semantics\u00a0before trying out some code. For in Kafka Streams, there are a bit more choices involved in joining, due to the stream/table duality. But whatever our join looks like, we should know something about Kafka\u2019s partitioning. Joins can be done most efficiently when the value that you want to join on, is the key of both streams and both source topics have the same number of partitions. If this is the case, the streams are co-partitioned, which means that each task that is created by the application instances, can be assigned to one (co-)partition where it will find all the data it needs for doing the join in one place.\nState\nWherever there are aggregations, windows or joins, there is state involved. KS will create a new RocksBD\u00a0StateStore\u00a0for each window in order to store the messages that fall within that window. Unfortunately, we would like to have a lot of windows in our application, and since we also have about 500,000 different keys, we soon discover that this quickly grows out of hands. After having turned the documentation inside out, we learn that each one of those stores has a cache size of 100 MB by default. But even after we change this to 0, our KS application is too state-heavy.\nInteractive queries\nThe same\u00a0StateStores that allow us to do joining and aggregating also allows us to keep a materialized view of a Kafka topic. The data in the topic will be stored by the application. If the topic is not already compacted, the local key-value-store will compact your data locally. The data will constantly be updated from the topic it is built from.\nThe store can be scaled out by running an extra instance of our application. Partitions will be divided among tasks, and tasks will be divided among instances, which results in each instance holding a subset of the data. This can lead to the situation where an instance is queried for a key that is contained in another instance. The queried instance may not be able to provide the value corresponding to the key, it knows, however, which other instance does hold the key. So we can relay the query. The downside is that we have to implement this API by ourselves.\nIn order for this to work, we need an extra configuration element\n\n    val p = new Properties()\r\n    p.put(StreamsConfig.APPLICATION_SERVER_CONFIG, s\"${settings.Streams.host}:${settings.Streams.port}\")\nTesting our app\nA popular library for unit-testing Kafka Streams apps seems to be\u00a0MockedStreams. However, not all topologies can be successfully tested with MockedStreams. But we can skip unit testing; integration tests are more important, anyway. Should we try using some EmbeddedKafka or spin up docker containers with\u00a0docker-it? In the future, testing Kafka Streams apps will hopefully be easier (https://cwiki.apache.org/confluence/display/KAFKA/KIP-247%3A+Add+public+test+utils+for+Kafka+Streams).\nWhen testing our topology, we wonder why our stream is not behaving like we would expect. After a while, we start banging our heads against a wall. Then we turn the documentation inside out again and we find some settings that may be helpful.\n\n    p.put(StreamsConfig.COMMIT_INTERVAL_MS_CONFIG, \"100\")\r\n    p.put(ProducerConfig.BATCH_SIZE_CONFIG, \"1\")\nOur second Kafka Streams application\nThen the inevitable happens: we need to do something that is not covered by the DSL; we need to resort to the Processor API. This would look a bit like the code below.\n\n    val builder: TopologyBuilder = new KStreamBuilder()\r\n    builder\r\n      .addSource(\"in\", keySerde.deserializer(), valueSerde.deserializer(), settings.Streams.inputTopic)\r\n      .addProcessor(\"myProcessor\", new MyProcessorSupplier(), \"in\")\r\n      .addSink(\"out\", settings.Streams.outputTopic, keySerde.serializer(), valueSerde.serializer(), \"myProcessor\")\nThe\u00a0Processor\u00a0interface lets us implement a\u00a0process\u00a0method that is called for each message, as well as a\u00a0punctuate\u00a0method that can be scheduled to run periodically. Inside these methods, we can use\u00a0ProcessorContext.forward\u00a0to forward messsages down the topology graph.\nPeriodically, in Kafka 0.11, means stream-time which means\u00a0punctuate\u00a0will be triggered by the first message that comes along after the method is scheduled to run. In our case we want to use wallclock-time, so we use the\u00a0ScheduledThreadPoolExecutor\u00a0to do our own scheduling. But if we do this, the\u00a0ProcessorContext\u00a0might have moved to a different node in our topology and the\u00a0forward\u00a0method will have unexpected behavior. The workaround for this is to get hold of the current node object and pass it along to our scheduled code.\n\nval currentNode = context.asInstanceOf[ProcessorContextImpl].currentNode().asInstanceOf[ProcessorNode[MyKey, MyValue]]\nIn Kafka 1.0 a\u00a0PunctuationType\u00a0was introduced to make it possible to choose between wallclock-time and stream-time.\nConclusion\nBy the time we had finished our application, we had re-written it a few times, seen all parts of the documentation more often than we would like and searched through the KS source code. We were unfamiliar with all the settings and unaware of all the places where messages could be cached, delayed, postponed or dropped, and at certain moments we started doubting our notion of time.\nIn retrospect, we should have kept things simple. Don\u2019t use too much state. Don\u2019t think a clever\u00a0Processor\u00a0will bend KS\u2019s quirks in our favor. Don\u2019t waste too much code on workarounds, because before you know it there will be a new version released that will break the API or obsolete our workarounds.\nAnd for those of you wanting to write an article or blog-post on Kafka Streams, make sure to finish it before the new release gets out.\nResoures\n\nhttps://docs.confluent.io/current/streams/\nhttp://www.bigendiandata.com/2016-12-05-Data-Types-Compared/\nhttps://www.confluent.io/blog/avro-kafka-data/\nhttps://github.com/confluentinc/kafka-streams-examples\nhttps://cwiki.apache.org/confluence/display/KAFKA/Kafka+Streams+Join+Semantics\n\n", "tags": [], "categories": ["Blog", "Search"]}
{"post_id": 1750, "title": "A practical guide to the quirks of Kafka Streams", "url": "https://www.luminis.eu/blog-en/development-en/a-practical-guide-to-the-quirks-of-kafka-streams-2/", "updated_at": "2020-11-25T15:40:25", "body": "Kafka Streams is a lightweight library that reads from a Kafka topic, does some processing and writes back to a Kafka topic. It processes messages one by one but is also able to keep some state. Because it leverages the mechanics of Kafka consumer groups, scaling out is easy. Each instance of your Kafka Streams application will take responsibility for a subset of topic partitions and will process a subset of the keys that go through your stream.\nKafka Streams at first glance\nOne of the first things we need to keep in mind when we start developing with Kafka Streams is that there are 2 different API\u2019s that we can use. There is the high-level DSL (using the\u00a0KStreamsBuilder) and the low-level Processor API (using the\u00a0TopologyBuilder). The DSL is easier to use than the Processor API at the cost of flexibility and control over details.\nWe will begin our journey by learning more about the DSL, knowing that sooner or later we will have to rewrite our application using the Processor API. It doesn\u2019t take long before we stumble upon the word count example.\n\nval source: KStream[String, String] = KStreamBuilderS.stream[String, String](\"streams-file-input\")\r\nval counts: KTableS[String, Long] = source\r\n  .flatMapValues { value => value.toLowerCase(Locale.getDefault).split(\" \") }\r\n  .map { (_, value) => (value, value) }\r\n  .groupByKey\r\n  .count(\"Counts\")\nEasy! Let\u2019s just skip the documentation and finish our application!\nOur first Kafka Streams application\nUnfortunately, we soon return to the documentation because each aggregation on our\u00a0KStream\u00a0seems to return a\u00a0KTable\u00a0and we want to learn more about this\u00a0stream/table duality. Aggregations also allow for windowing, so we continue reading about\u00a0windowing. Now that we know something about the theoretical context, we return to our code. For our use case, we need 1 aggregate result for each window. However, we soon discover that each input message results in an output message on our output topic. This means that all the intermediate aggregates are spammed on the output topic. This is our first disappointment.\nA simple join\nThe DSL has all the usual suspects like\u00a0filter,\u00a0map, and\u00a0flatMap. But even though\u00a0join\u00a0is also one of those usual suspects that we have done a thousand times, it would be best to read the documentation on\u00a0join semantics\u00a0before trying out some code. For in Kafka Streams, there are a bit more choices involved in joining, due to the stream/table duality. But whatever our join looks like, we should know something about Kafka\u2019s partitioning. Joins can be done most efficiently when the value that you want to join on, is the key of both streams and both source topics have the same number of partitions. If this is the case, the streams are co-partitioned, which means that each task that is created by the application instances, can be assigned to one (co-)partition where it will find all the data it needs for doing the join in one place.\nState\nWherever there are aggregations, windows or joins, there is state involved. KS will create a new RocksBD\u00a0StateStore\u00a0for each window in order to store the messages that fall within that window. Unfortunately, we would like to have a lot of windows in our application, and since we also have about 500,000 different keys, we soon discover that this quickly grows out of hands. After having turned the documentation inside out, we learn that each one of those stores has a cache size of 100 MB by default. But even after we change this to 0, our KS application is too state-heavy.\nInteractive queries\nThe same\u00a0StateStores that allow us to do joining and aggregating also allows us to keep a materialized view of a Kafka topic. The data in the topic will be stored by the application. If the topic is not already compacted, the local key-value-store will compact your data locally. The data will constantly be updated from the topic it is built from.\nThe store can be scaled out by running an extra instance of our application. Partitions will be divided among tasks, and tasks will be divided among instances, which results in each instance holding a subset of the data. This can lead to the situation where an instance is queried for a key that is contained in another instance. The queried instance may not be able to provide the value corresponding to the key, it knows, however, which other instance does hold the key. So we can relay the query. The downside is that we have to implement this API by ourselves.\nIn order for this to work, we need an extra configuration element\n\n    val p = new Properties()\r\n    p.put(StreamsConfig.APPLICATION_SERVER_CONFIG, s\"${settings.Streams.host}:${settings.Streams.port}\")\nTesting our app\nA popular library for unit-testing Kafka Streams apps seems to be\u00a0MockedStreams. However, not all topologies can be successfully tested with MockedStreams. But we can skip unit testing; integration tests are more important, anyway. Should we try using some EmbeddedKafka or spin up docker containers with\u00a0docker-it? In the future, testing Kafka Streams apps will hopefully be easier (https://cwiki.apache.org/confluence/display/KAFKA/KIP-247%3A+Add+public+test+utils+for+Kafka+Streams).\nWhen testing our topology, we wonder why our stream is not behaving like we would expect. After a while, we start banging our heads against a wall. Then we turn the documentation inside out again and we find some settings that may be helpful.\n\n    p.put(StreamsConfig.COMMIT_INTERVAL_MS_CONFIG, \"100\")\r\n    p.put(ProducerConfig.BATCH_SIZE_CONFIG, \"1\")\nOur second Kafka Streams application\nThen the inevitable happens: we need to do something that is not covered by the DSL; we need to resort to the Processor API. This would look a bit like the code below.\n\n    val builder: TopologyBuilder = new KStreamBuilder()\r\n    builder\r\n      .addSource(\"in\", keySerde.deserializer(), valueSerde.deserializer(), settings.Streams.inputTopic)\r\n      .addProcessor(\"myProcessor\", new MyProcessorSupplier(), \"in\")\r\n      .addSink(\"out\", settings.Streams.outputTopic, keySerde.serializer(), valueSerde.serializer(), \"myProcessor\")\nThe\u00a0Processor\u00a0interface lets us implement a\u00a0process\u00a0method that is called for each message, as well as a\u00a0punctuate\u00a0method that can be scheduled to run periodically. Inside these methods, we can use\u00a0ProcessorContext.forward\u00a0to forward messsages down the topology graph.\nPeriodically, in Kafka 0.11, means stream-time which means\u00a0punctuate\u00a0will be triggered by the first message that comes along after the method is scheduled to run. In our case we want to use wallclock-time, so we use the\u00a0ScheduledThreadPoolExecutor\u00a0to do our own scheduling. But if we do this, the\u00a0ProcessorContext\u00a0might have moved to a different node in our topology and the\u00a0forward\u00a0method will have unexpected behavior. The workaround for this is to get hold of the current node object and pass it along to our scheduled code.\n\nval currentNode = context.asInstanceOf[ProcessorContextImpl].currentNode().asInstanceOf[ProcessorNode[MyKey, MyValue]]\nIn Kafka 1.0 a\u00a0PunctuationType\u00a0was introduced to make it possible to choose between wallclock-time and stream-time.\nConclusion\nBy the time we had finished our application, we had re-written it a few times, seen all parts of the documentation more often than we would like and searched through the KS source code. We were unfamiliar with all the settings and unaware of all the places where messages could be cached, delayed, postponed or dropped, and at certain moments we started doubting our notion of time.\nIn retrospect, we should have kept things simple. Don\u2019t use too much state. Don\u2019t think a clever\u00a0Processor\u00a0will bend KS\u2019s quirks in our favor. Don\u2019t waste too much code on workarounds, because before you know it there will be a new version released that will break the API or obsolete our workarounds.\nAnd for those of you wanting to write an article or blog-post on Kafka Streams, make sure to finish it before the new release gets out.\nResoures\n\nhttps://docs.confluent.io/current/streams/\nhttp://www.bigendiandata.com/2016-12-05-Data-Types-Compared/\nhttps://www.confluent.io/blog/avro-kafka-data/\nhttps://github.com/confluentinc/kafka-streams-examples\nhttps://cwiki.apache.org/confluence/display/KAFKA/Kafka+Streams+Join+Semantics\n\n\n", "tags": [], "categories": ["Blog", "Development"]}
{"post_id": 1598, "title": "Creating Digital Products: Love Solving Problems", "url": "https://www.luminis.eu/blog-en/strategy-innovation-en/creating-digital-products-love-solving-problems/", "updated_at": "2023-01-04T10:03:14", "body": "Non-fiction books can be insightful, but I have noticed that I\u2019d expect those insights to come back to me right when they would be helpful \u2013\u2013 like when there\u2019s an obstacle in a project, I will have an \u201ca-ha\u201d moment and remember that great idea I have read about years ago. In reality, not so much. So I\u2019m trying something new. After reading a book, I\u2019ll try to write down the most important stuff that could be useful some day in my digital notepad.\nI did this with the book Digital Transformation Book by David L. Rogers as well, a book I\u2019ve written about before (for an introduction of the book, start there). My notes quickly got annotated with my own experiences, and as I thought that would be valuable to share, I started to write this post.\nRogers discusses in his book the digital transformation that requires pre-internet companies to rethink their business and way of doing things. Two themes are repeated throughout, and are obviously connected: Be agile and adapt; and have a company culture that is acceptable of change. Although primarily aimed at (C-level) managers, there is definitely some good stuff in it for designers and developers regarding those themes.\nEverything Is Marketing\nIn this digital age the network of customers and stakeholders has grown and become more complex. Customers are no longer passive consumers that are just at the receiving end; they now interact with the brand, the market and each other. This also means that the number of touch points in the customer journey is growing, turning marketing into a company-wide activity.\nFor the customer there is only one experience with the company: Using the product, visiting the company\u2019s Twitter page or asking for support by email, it is all the same brand to the customer. All that should be in sync to such a degree that the customer will have consistent experience of the brand (and preferably a positive one). In traditional companies where different departments take care of specific parts of the brand, it will be hard to achieve this. Those walls needs to be taken down, the silos dismantled.\nWhat can you do regarding this one brand perspective? To be more sensitive to the outside world and more customer-centric? Quite easily actually: Work together within your team beyond your own expertise to create a great product or service. We call this in Dutch. On top of that, you can talk to people of other departments, outside of your team. Ask the people at customer service what kind of issues customers are experiencing with the product. Get to know what content is created for the website and social media accounts. There is no excuse here, you can literally walk over to the next room or floor to find these insights.\nDeveloping Great Products\nLet us zoom in on the development process. Concepts like working agile and having a culture that is acceptable of change should make sense to us as developers and designers. However, some companies or clients are less familiar with this mindset.\nThere are some ways to do your part to help your company or client improve on this, even within a project. Rogers describes in his book experimental processes and principles based on what he observed at successful companies, which I have remixed using my own experience into the following five principles that can guide you in that process.\n1. Question The Assumptions\nIn the beginning of a project, assumptions are made and new ideas are born founded on hunches, not data. That is a good thing, because you want that outside of the box creativity. But accepting these assumptions as the truth without verifying them is just gambling the whole project on a brainwave. These assumptions sometimes become a dogma within the company \u2013\u2013 never questioned but also never proven to be right.\nTest those assumptions and various ideas early in the project and learn from them. Even if ideas are proven worthless, it could still lead to other insights and new ideas. Innovation means you wonder into unknown territories, so all you know could be worth being challenged. And this is not only true for new products but works evenly well if you are working on a new feature or improving an existing product.\n2. Iterate From The Start, And Get Consumer Feedback Quickly\nIt happens quite often that weeks are spent on requirements and scoping of a project, resulting in a hefty backlog. Everything is pretty much set in stone, and it is hard to change direction if the first consumer response is timid.\nTherefore it is better to bring the concept as quickly as possible to real customers to get feedback and iterate on that. And work on the main concept here, don\u2019t linger on the details \u2013\u2013 if the concept doesn\u2019t stick, it probably won\u2019t be due to a detail!\nWhen doing this, speak to real or potential customers to get that feedback. Present your concept preferably in a form that is realistic enough for a consumer, even if it is still in the early stages. The person you show it to needs to get a pretty good sense of what the product or service is, how it could work and what the benefits for him or her would be. Not everyone can interpret abstract storyboards or wireframes the right way.\n3. Kick-off With An Intensive\nTo kickstart a project without loads of requirements, we at Luminis do, as we call it, an Intensive with our client. It is short: It all happens in one day. With around five people (some designers and engineers from us and a few experts from the client, and perhaps even a end-user) we have a small and productive team. In the morning we start with the domain and marketplace, and the client discusses the potential he or she sees. In the afternoon we challenge assumptions, generate ideas and finish with some concepts or a first prototype at the end of the day. The Intensive not only helps to get us quickly into the context, it helps the client to get an outside perspective and some solutions that might work. And all within a day, making it fast and cheap. The results then can be shown to customers to get the first feedback.\n4. Fall In Love With The Problem, And Not The Solution\nMaybe the biggest reason the Intensive works is because it helps us and the client to understand the problem better \u2013 that is also why you should start with the marketplace and customer (and not that new technology everyone\u2019s talking about). The result of the Intensive itself is used to learn more about the problem. If the project continues after this kick-off session, we \u201cthrow\u201d away the solution and continue with the problem.Falling in love with the problem helps you to have the right focus. It makes you consider what customers really want or are struggling with; that is where you should start. Do you already have feedback on existing products? Great, start from there. Can you observe the customer in the specific context you are interested in, even better. This helps you to see the bigger picture and it could even unexpectedly provide a solution.Falling in love with the solution is a trap. It limits the number of solutions you consider (because why look further if the team\u2019s really happy with the solution on the table?), so it risks leaving a potentially better solution to be undiscovered. Loving a solution prevents you also from letting go of it when the response on it is too moderate or a better solution comes along. If an idea get\u2019s the team really excited, it is worth putting it aside for a few days before jumping on it. Keep looking for other, maybe better, possibilities. The rush you get from that great idea could very well be worn off when you have another look at it two days later.\nSolving the right problem right is the goal to create truly great products.\n5. Fail Smart\nFinding the right solution and developing a great product won\u2019t happen all the time. Some things may not work, and you and your team fail. It is important for all to expect this as a possible outcome and it shouldn\u2019t be regarded as a negative event per se. When it happens, it does require a moment of reflection: Why did it go wrong, and did it happen as early and as cheaply as possible? And with answers to these questions, it is possible to improve the process. Keep in mind that you now only know what doesn\u2019t work. Reflection is good but don\u2019t let the failure limit the creativity or turn into a bureaucratic rule that becomes an obstacle in this or the next project. And reconsider sharing it with your colleagues and other teams. The next time the context could differ so much that it renders the learnings of the failure useless in that case. Process failures are more relevant to share than product failures.\nMake Great Products\nWhether if you work at a company that existed before the internet or work at a startup, the lessons described above could help you equally well. The concepts, processes and principles are valuable for all those involved in the development of digital products and services. A lot is about being open-minded, breaking through dogmas, and understanding the real issue the consumer has that requires a solution. Focussing on the problem, loving it, understanding it, and than solving it with a great product, that should keep you going and innovating.\n\u00a0\n", "tags": ["Design"], "categories": ["Blog", "Concepting &amp; UX", "Strategy &amp; Innovation"]}
{"post_id": 1627, "title": "Analyzing a performance issue", "url": "https://www.luminis.eu/blog-en/development-en/analyzing-a-performance-issue/", "updated_at": "2020-11-18T14:47:38", "body": "Recently, a customer reported that the performance of the web application we are building, an AngularJS front end using REST services from a\u00a0 Java back end, is unsatisfactory. I had a fun time finding out what the problem was, understanding what was causing it and thinking up solutions. I\u2019d like to share the experience with you.\nWhat is the problem?\n\u201cBad performance\u201d can mean many things. Actual response times are only part of the story; an application that shows feedback while gathering a response can make you wait longer before it is perceived as slow. When users know their bandwith is limited, for instance on a mobile device on a slower network, they anticipate a slower site. But of course, it is always possible that the services on which your site relies are just too slow. So, first order of business: ask the reporter what he means. What action did he perform, and how long did it take? How long did he\u00a0expect\u00a0it to take? It turned out there were several problem areas, not all of which were actually related to performance. I will describe two of these areas in more detail:\n\nloading of the AngularJS application\nloading of list views\n\nLoading of the AngularJS application\nOur AngularJS application has to perform several tasks before it can render the first view. This takes time, and during this time the end user is staring at a blank page.\nFinding out what happens\nTo figure out what tasks are executed and which of these can most easily be sped up I opened a fresh Chrome window, trashed all the caches, opened the network panel and loaded the main page. \nThis is interesting. There is a flurry of activity at the start, followed by 300 ms of silence, after which a lot more is loaded. What does it mean? Among the first items to be loaded is our JavaScript library, including AngularJS and other components. This is a large file, so it takes several hundreds of milliseconds to get loaded. After that, the AngularJS application is initializing (nothing happens) and after that, additional data is downloaded as the AngularJS application is building its first view. The largest of these is a translations file. The application won\u2019t show anything until the translations are loaded, so the limiting path is:\n\nLoading of the JavaScript libraries\nInitialization of the AngularJS application\nRetrieving the translations file\n\nImprovements\nThe initialization of the AngularJS application is not an easy one to speed up, so I\u2019ll concentrate on the other two parts. The time it takes to download a JavaScript library is largely determined by its size. The library was already minified during the build. However, when a project is running for some time, it is not uncommon to have solved the same problem in two different ways, using two different libraries, or to end up with unused libraries. As it turned out, we were using two different libraries for file uploads. Choosing a single library for the uploads and removing libraries that are no longer used reduced the size of the final, minified JavaScript file and improved load time. The contents of the translations file were all used. However, since this is a plain text file, it was a prime candidate for zipped transfer. AngularJS handles zipped files out of the box, so the only thing we needed to do was add an @org.apache.cxf.annotations.GZIP annotation to the Apache CXF resource in the back end, which reduced the download time from several hundred milliseconds to ~10 ms.\nList views\nList views are always tricky, especially if there is no limit to the number of elements that may show up in the list, as was the case in some of our services. However, that turned out to not even be the biggest problem.\nFeedback: always\nWhen opening a page with a list, the application would show a spinner to indicate that the contents of the list were still being retrieved. The spinner would be removed, and the list shown, when the variable containing the list data was present on the scope:\n\n\n\n\n\n\n\n\n\u00a0\nHowever, the controller that retrieves the projects had no error handling:\nprojectService.query()\r\n.then(function (projects) {\r\n  $scope.projects = projects;\r\n});\nSo, if the back end service returned any error, the projects variable would not be set and the spinner would keep spinning \u2013 which made the testers conclude that the application was very slow! You should always provide feedback on the outcome of a back end call. At the very least, a spinner suggesting that data is retrieved should always be removed when a response is received. Showing the spinner should depend on whether a call is being performed, not an the presence of the result:\n\n\u00a0\n$scope.loadingProjects = true;\r\nprojectService.query()\r\n.then(function(projects) {\r\n  $scope.projects = projects;\r\n})\r\n.finally(function() {\r\n  $scope.loadingProjects = false;\r\n});\nEven better would be to provide an error message if the back end returned an error:\n$scope.loadingProjects = true;\r\nprojectService.query()\r\n.then(function(projects) {\r\n  $scope.projects = projects;\r\n})\r\n.then(function(error) {\r\n  // Process this to show an appropriate error message in the view\r\n})\r\n.finally(function() {\r\n  $scope.loadingProjects = false;\r\n});\nSlow REST services\nOf course, there\u2019s still the problem of REST services simply taking a long time to return a response. To monitor the response times for these services, we added simple metrics to all the REST services using Java Simon. This is a simple framework to gather performance data. You request a Stopwatch and use it to measure the call durations on the resource; it will then provide you with minumum, maximum and mean call durations:\n\u00a0\n@GET\r\n@Produces(MediaType.APPLICATION_JSON)\r\npublic Response getProjects(\r\n  final Split split = SimonManager.getStopwatch(\"getProjects\").start();\r\n  final Response response;\r\n  try {\r\n    response = Response.ok(projectManagement.findAll()).build();\r\n  } finally {\r\n    split.stop();\r\n  }\r\n  return response;\r\n}\n\u00a0\nStopwatch stopwatch = SimonManager.getStopwatch(\"getProjects\");\r\nlong minimumDuration = stopwatch.min(); // minimum duration in nanoseconds\r\nlong maximumDuration = stopwatch.max(); // maximum duration in nanoseconds\r\nlong meanDuration = stopwatch.mean(); // mean duration in nanoseconds\r\n\nWe then used Prometheus to provide access to the measurements. There are of course many reasons for a service to be slow, so I won\u2019t go into details here, but this setup provided insight into which services are consistently slow so we could focus improvement efforts on those services.\n", "tags": [], "categories": ["Blog", "Development"]}
{"post_id": 1349, "title": "Providing developers and testers with a representative database using Docker", "url": "https://www.luminis.eu/blog-en/development-en/providing-developers-and-testers-with-a-representative-database-using-docker/", "updated_at": "2021-02-01T14:52:05", "body": "When developing or testing, having a database that comes pre-filled with relevant data can be a big help with implementation or a scenario walk-through. However, often there is only a structure dump of the production database available, or less. This article outlines the process of creating a Docker image that starts a database and automatically restores a dump containing a representative set of data. We use the PostgreSQL database in our examples, but the process outlined here can easily be applied to other databases such as MySQL or Oracle SQL.\nDumping the database\nI will assume that you have access to a database that contains a copy of production or an equivalent representative set of data. You can dump your database using Postgres\u2019 pg_dump utility:\npg_dump --dbname=postgresql://user@localhost:5432/mydatabase \\\r\n        --format=custom --file=database.dmp\r\n\nWe will be using the custom format option to create the dump file with. This gives us a file that can easily be restored with the pg_restore utility later on and ensures the file is compressed. In case of larger databases, you may also wish to exclude certain database elements from your dump. In order to do so you have the following options:\n\nThe --exclude-table option, which takes a string pattern and ensures any tables matching the pattern will not be included in the dump file.\nThe --schema option, which restricts our dump to particular schemas in the database. It may be a good idea to exclude pg_catalog \u2013 this schema contains among other things the table pg_largeobject, which contains all of your database\u2019s binaries.\n\nSee the Postgres documentation for more available options.\nDistributing the dump among users\nFor the distribution of the dump, we will be using Docker. Postgres, MySQL and even Oracle provide you with prebuilt Docker images of their databases.\nExample 1: A first attempt\nIn order to start an instance of a Postgres database, you can use the following docker run command to start a container based on Postgres:\ndocker run -p 5432:5432 --name database-dump-container \\ \r\n           -e POSTGRES_USER=user -e POSTGRES_PASSWORD=password \\ \r\n           -e POSTGRES_DB=mydatabase -d postgres:9.5.10-alpine\nThis starts a container named database-dump-container that can be reached at port 5432 with user:password as the login. Note the usage of the 9.5.10-alpine tag. This ensures that the Linux distribution that we use inside the Docker container is Alpine Linux, a distribution with a small footprint. The whole Docker image will take up about 14 MB, while the regular 9.5.10 tag would require 104 MB. We are pulling the image from Docker Hub, a public Docker registry where various open source projects host their Docker images. Having started our Docker container, we can now copy our dump file into it. We first use docker execto execute a command against the container we just made. In this case, we create a directory inside the Docker container:\ndocker exec -i database-dump-container mkdir -p /var/lib/postgresql/dumps/\nFollowing that, we use docker cp to copy the dump file from our host into the container:\ndocker cp database.dmp database-dump-container:/var/lib/postgresql/dumps/\nAfter this, we can restore our dump:\ndocker exec -i database-dump-container pg_restore \r\n               --verbose --exit-on-error --format=custom \r\n               --username=user --dbname=mydatabase \r\n               /var/lib/postgresql/dumps/database.dmp\nWe now have a Docker container with a running Postgres instance containing our data dump. In order to actually distribute this, you will need to get it into a Docker repository. If you register with Docker Hub you can create public repositories for free. After creating your account you can login to the registry that hosts your repositories with the following command:\ndocker login docker.io\nEnter the username and password for your Docker Hub account when prompted. Having done this, we are able to publish our data dump container as an image, using the following commands:\ndocker commit database-dump-container my-repository/database-dump-image\r\n\r\ndocker push my-repository/database-dump-image\r\n\nNote that you are able to push different versions of an image by using Docker image tags. The image is now available to other developers. It can be pulled and ran on another machine using the following commands:\ndocker pull my-repository/database-dump-image\r\n\r\ndocker run -p 5432:5432 --name database-dump-container\r\n           -e POSTGRES_USER=user-e POSTGRES_PASSWORD=password\r\n           -e POSTGRES_DB=mydatabase-d\r\n           my-repository/database-dump-image\r\n\nAll done! Or are we? After we run the container based on the image, we still have an empty database. How did this happen?\nExample 2: Creating your own Dockerfile\nIt turns out that the Postgres Docker image uses Docker volumes. This separates the actual image from data and ensures that the size of the image remains reasonable. We can view what volumes Docker has made for us by using docker volume ls. These volumes can be associated with more than one Docker container and will remain, even after you have removed the container that initially spawned the volume. If you would like to remove a Docker container, including its volumes, make sure to use the -v option:\ndocker rm -v database-dump-container\nGo ahead and execute the command, we will be recreating the container in the following steps. So how can we use this knowledge to distribute our database including our dump? Luckily, the Postgres image provides for exactly this situation. Any scripts that are present in the Docker container under the directory /docker-entrypoint-initdb.d/ will be executed automatically upon starting a new container. This allows us to add data to the Docker volume upon starting the container. In order to make use of this functionality, we are going to have to create our own image using a Dockerfile that extends the postgres:9.5.10-alpine image we used earlier:\nFROM postgres:9.5.10-alpine\r\n\r\nRUN mkdir -p /var/lib/postgresql/dumps/\r\nADD database.dmp /var/lib/postgresql/dumps/\r\nADD intialize.sh /docker-entrypoint-initdb.d/\nThe contents of initialize.sh are as follows:\npg_restore --username=user --verbose --exit-on-error\r\n           --format=custom\r\n           --dbname=mydatabase\r\n           /var/lib/postgresql/dumps/database.dmp\nWe can build and run this Dockerfile by navigating to its directory and then executing:\ndocker build --rm=true -t database-dump-image .\r\n\r\ndocker run -p 5432:5432 --name database-dump-container \\\r\n           -e POSTGRES_USER=user-e POSTGRES_PASSWORD=password\r\n           -e POSTGRES_DB=mydatabase-d database-dump-image\nAfter starting the container, inspect its progress using docker logs -f database-dump-container. You can see that upon starting the container, our database dump is being restored into the Postgres instance. We can now again publish the image using the earlier steps, and the image is available for usage.\nConclusions and further reading\nWhile working through this article, you have used a lot of important concepts within Docker. The first example demonstrated the usage of images and containers, combined with commands exec and cpthat are able to interact with running containers. We then demonstrated how you can publish a Docker image using Docker Hub, after which we\u2019ve shown you how to build and run your own custom made image. We have also touched upon some more complex topics such as Docker volumes. After this you may wish to consult the Docker documentation to further familiarize yourself with the other commands that Docker offers. This setup still leaves room for improvement \u2013 the current process involves quite a lot of handwork, and we\u2019ve coupled our container with one particular database dump. Please refer to the Github project for automated examples of this process.\n", "tags": [], "categories": ["Blog", "Development"]}
{"post_id": 1724, "title": "A look at the source code of gensim doc2vec", "url": "https://www.luminis.eu/blog-en/development-en/a-look-at-the-source-code-of-gensim-doc2vec/", "updated_at": "2020-11-25T14:33:17", "body": "Previously, we\u2019ve built a simple PV-DBOW-\u2018like\u2019 model. We\u2019ve made a couple of choices, e.g., about how to generate training batches, how to compute the loss function, etc. In this blog post, we\u2019ll take a look at the choices made in the popular gensim library. First, we\u2019ll convince ourselves that we implemented indeed more or less the same thing :-). Then, by looking at the differences, we\u2019ll get ideas to improve and extend our own implementation (of course, this could work both ways ;-)). The first extension we are interested in is to infer a document vector for a new document. We\u2019ll discuss how the gensim implementation achieves this.\nDisclaimer: You will notice that we\u2019ll write this blog post in a somehwat dry, bullet-point style. You may use it for reference if you ever want to work on doc2vec. We plan to, anyway. If you see mistakes in our eyeball-interpretation of what gensim does, feel free to (gently) correct us; please refer to the same git commit version of the code against which we wrote this blog post, and use line numbers to point to code.\nCode walk through of gensim\u2019s PV-DBOW\nWe\u2019ll start with the non-optimized Python module doc2vec (https://github.com/RaRe-Technologies/gensim/blob/8766edcd8e4baf3cfa08cdc22bb25cb9f2e0b55f/gensim/models/doc2vec.py). Note that the link is to the specific version against which this blog post was written.\u00a0To narrow it down, and to stay as close to our own PV-DBOW implementation, we\u2019ll first postulate some assumptions:\n\nwe\u2019ll initialize the Doc2Vec class as follows\u00a0d2v = Doc2Vec(dm=0, **kwargs). That is, we\u2019ll use the PV-DBOW flavour of doc2vec.\nWe\u2019ll use just one, unique, \u2018document tag\u2019 for each document.\nWe\u2019ll use negative sampling.\n\nThe first thing to note about the Doc2Vec class is that is subclasses the Word2Vec class, overriding some of its methods. By prefixing methods with the class, we\u2019ll denote which exact method is called. The super class object is then initialized as follows, in lines 640-643, by deduction:\n\nWord2Vec(sg=1, null_word=0, **kwargs)\n\nsg stands for Skip-Gram. Remember from elsewhere on the net that the Skip-Gram Word2Vec model is trained to predict surrounding words (for any word in a corpus).\nUpon initialisation,\u00a0Word2Vec.train()\u00a0is called: a model is trained. Here, some parallelisation is taken care of that I will not go into at this point. At some point however,\u00a0Doc2Vec._do_train_job()\u00a0is called: in a single job a number of documents is trained on. Since we have self.sg = 1,\u00a0Doc2Vec.train_document_dbow()\u00a0is called there, for each document in the job.\nIn this method, the model is trained to predict each word in the document. For this,\u00a0Word2Vec.train_sg_pair()\u00a0is used. Only, instead of two words, this method now receives the document tag and a word: the task is to correctly predict each word given the document tag. In this method, weights are updated. It seems, then, that at each stochastic gradient descent iteration, only one training example is used.\nComparison of ours and gensim\u2019s Doc2Vec implementation\nBy just eyeballing the code, at first sight, the following similarities and differences stand out:\nSimilarities:\n\nThe network architecture used seems the same in both implementations: the input layer has as many neurons as there are documents in the corpus, there is one hidden layer, and the output layer has the vocabulary size.\nNeither implementation uses regularisation.\n\nDifferences:\n\nOne training example (document, term) per SGD iteration is used by gensim, whereas we allow computing the loss function over multiple training examples.\nAll terms in a document are offered to SGD right after each other in gensim, whereas we generate batches consisting of several term windows from different documents.\nIn gensim, the order in which training documents are offered is the same order each epoch; we randomize the order of term windows again each epoch.\n\n\u00a0\nInferring document vectors\nGiven a new, unseen document, using\u00a0Doc2Vec.infer_vector(), a document vector can be estimated anyway. How? Well, the idea is that we keep the word classifier that operates on the hidden space fixed. In other words, we keep the weights between the hidden layer and the output layer fixed. Now, we train a new mini-network with just one input neuron\u2013the new document id\u2013. We optimize the network such that the document gets an optimal position in the hidden space. In other words, again, we train the weights that connect this new document id to the hidden layer. How does gensim initialize the weights for the new input neuron? Randomly, set to small weights, just like it was done for the initial documents that were trained on. The training procedure consists of a fixed number of steps (we can choose how many). At each step, all words in the document are offered as a training example, one after the other.\nWrap-up\nWe\u2019ve seen that gensim\u2019s implementation and ours do implement roughly the same thing, although there are a number of differences. This consolidates our position with our small \u2018proof-of-concept\u2019 implementation of doc2vec (https://amsterdam.luminis.eu/2017/02/21/coding-doc2vec/). We\u2019ve eyeballed how gensim\u2019s doc2vec implementation manages to infer a document vector for an unseen document; now we are in a position to extend our own implementation to do the same. Of course, you can do it yourself, too!\n", "tags": [], "categories": ["Blog", "Development"]}
{"post_id": 1373, "title": "Part 2 of Creating a search DSL", "url": "https://www.luminis.eu/blog-en/part-2-of-creating-a-search-dsl/", "updated_at": "2020-11-25T15:41:23", "body": "In my previous blog post I wrote about creating your own search DSL using Antlr. In that post I discussed the Antlr language for constructs like AND/OR, multiple words and combining words. In this blog post I am showing how to use the visitor mechanism to write actual elasticsearch queries.\nIf you did not read the first post yet, please do so. It will make it easier to follow along. If you want the code, please visit the github page.\nhttps://amsterdam.luminis.eu/2017/06/28/creating-search-dsl/\nGithub repository\nWhat queries to use\nIn the previous blog post we ended up with some of the queries we want to support\n\napple\napple OR juice\napple raspberry OR juice\napple AND raspberry AND juice OR Cola\n\u201capple juice\u201d OR applejuice\n\nBased on these queries we have some choices to make. The first query seems obvious, searching for one word would become a match query. However, in which field do you want to search? In Elasticsearch there is a special field called the\u00a0_all\u00a0field. In the example we are using the _all field, however it would be easy to create a query against a number of specific fields using a multi_match.\nIn the second example we have two words with\u00a0OR\u00a0in between. The most basic implementation would again be a match query, since the match query by default uses OR if you supply multiple words. However, the DSL uses OR to combine terms as well as\u00a0and queries. A term in itself can be a quoted term as well. Therefore, to translate the\u00a0apple OR juice\u00a0we need to create a boolean query. Now look at the last example, here we use quotes. One would expect quotes to keep words together. In elasticsearch we would use the\u00a0Phrase query\u00a0to accomplish this.\nAs the current DSL is fairly simple, creating the queries is not that hard. But a lot more extensions are possible that can make use of more advance query options. Using wildcards could result in fuzzy queries, using\u00a0title:apple\u00a0could look into one specific field and using single quotes could mean an exact match, so we would need to use the term query.\nNow you should have an idea of the queries we would need, let us have a look at the code and see Antlr DSL in action.\nGenerate json queries\nAs mentioned in the introduction we are going to use the visitor to parse the tree. Of course we need to create the tree first. Below the code to create the tree.\n\nstatic SearchdslParser.QueryContext createTreeFromString(String searchString) {\r\n    CharStream charStream = CharStreams.fromString(searchString);\r\n    SearchdslLexer lexer = new SearchdslLexer(charStream);\r\n    CommonTokenStream commonTokenStream = new CommonTokenStream(lexer);\r\n    SearchdslParser parser = new SearchdslParser(commonTokenStream);\r\n \r\n    return parser.query();\r\n}\nAS mentioned in the previous posts, the parser and the visitor classes get generated by Antlr. Methods are generated for visiting the different nodes of the tree. Check the class\n\u00a0\n\nSearchdslBaseVisitor\n\n\u00a0\nfor the methods you can override.\nTo understand what happens, it is best to have a look at the tree itself. Below the image of the tree that we are going to\u00a0visit.\n\nWe visit the tree from the top. The first method or Node that we visit is the top level Query. Below the code of the visit method.\n\n@Override\r\npublic String visitQuery(SearchdslParser.QueryContext ctx) {\r\n    String query = visitChildren(ctx);\r\n \r\n    return\r\n            \"{\" +\r\n                \"\\\"query\\\":\" + query +\r\n            \"}\";\r\n}\nEvery visitor generates a string, with the query we just visit all the possible children and create a json string with a query in there. In the image we see only a child orQuery, but it could also be a Term or andQuery. By calling the visitChildren method we continue to walk the tree. Next step is the visitOrQuery.\n@Override\r\npublic String visitOrQuery(SearchdslParser.OrQueryContext ctx) {\r\n    List shouldQueries = ctx.orExpr().stream().map(this::visit).collect(Collectors.toList());\r\n    String query = String.join(\",\", shouldQueries);\r\n \r\n    return\r\n            \"{\\\"bool\\\": {\" +\r\n                    \"\\\"should\\\": [\" +\r\n                        query +\r\n                    \"]\" +\r\n            \"}}\";\r\n}\nWhen creating an OR query we use the bool query with the should clause. Next we have to obtain the queries to include in the should clause. We obtain the orExpr items from the orQuery and for each orExpr we again call the visit method. This time we will visit the orExpr Node, this node does not contain important information for us, therefore we let the template method just call the visitChildren method. orExpr nodes can contain a term or an andQuery. Let us have a look at visiting the andQuery first.\n@Override\r\npublic String visitAndQuery(SearchdslParser.AndQueryContext ctx) {\r\n    List mustQueries = ctx.term().stream().map(this::visit).collect(Collectors.toList());\r\n    String query = String.join(\",\", mustQueries);\r\n     \r\n    return\r\n            \"{\" +\r\n                    \"\\\"bool\\\": {\" +\r\n                        \"\\\"must\\\": [\" +\r\n                            query +\r\n                        \"]\" +\r\n                    \"}\" +\r\n            \"}\";\r\n}\nNotice how closely this resembles the orQuery, big difference in the query is that we now use the bool query with a must part. We are almost there. The next step is the Term node. This node contains words to transform into a match query, or it contains a quotedTerm. The next code block shows the visit method of a Term.\n@Override\r\npublic String visitTerm(SearchdslParser.TermContext ctx) {\r\n    if (ctx.quotedTerm() != null) {\r\n        return visit(ctx.quotedTerm());\r\n    }\r\n    List words = ctx.WORD();\r\n    String termsAsText = obtainWords(words);\r\n \r\n    return\r\n            \"{\" +\r\n                    \"\\\"match\\\": {\" +\r\n                        \"\\\"_all\\\":\\\"\" + termsAsText + \"\\\"\" +\r\n                    \"}\" +\r\n            \"}\";\r\n}\r\n \r\nprivate String obtainWords(List words) {\r\n    if (words == null || words.isEmpty()) {\r\n        return \"\";\r\n    }\r\n    List foundWords = words.stream().map(TerminalNode::getText).collect(Collectors.toList());\r\n     \r\n    return String.join(\" \", foundWords);\r\n}\r\n\nNotice we first check if the term contain a quotedTerm. If it does not contain a quotedTerm we obtain the words and combine them into one string. The final step is to visit the quotedTerm node.\n@Override\r\npublic String visitQuotedTerm(SearchdslParser.QuotedTermContext ctx) {\r\n    List words = ctx.WORD();\r\n    String termsAsText = obtainWords(words);\r\n \r\n    return\r\n            \"{\" +\r\n                    \"\\\"match_phrase\\\": {\" +\r\n                        \"\\\"_all\\\":\\\"\" + termsAsText + \"\\\"\" +\r\n                    \"}\" +\r\n            \"}\";\r\n}\nNotice we parse this part into a match_phrase query, other than that it is almost the same as the term visitor. Finally we can generate the complete query.\nExample\n\u201cmulti search\u201d && find && doit OR succeed && nothing\n\n{\r\n  \"query\": {\r\n    \"bool\": {\r\n      \"should\": [\r\n        {\r\n          \"bool\": {\r\n            \"must\": [\r\n              {\r\n                \"match_phrase\": {\r\n                  \"_all\": \"multi search\"\r\n                }\r\n              },\r\n              {\r\n                \"match\": {\r\n                  \"_all\": \"find\"\r\n                }\r\n              },\r\n              {\r\n                \"match\": {\r\n                  \"_all\": \"doit\"\r\n                }\r\n              }\r\n            ]\r\n          }\r\n        },\r\n        {\r\n          \"bool\": {\r\n            \"must\": [\r\n              {\r\n                \"match\": {\r\n                  \"_all\": \"succeed\"\r\n                }\r\n              },\r\n              {\r\n                \"match\": {\r\n                  \"_all\": \"nothing\"\r\n                }\r\n              }\r\n            ]\r\n          }\r\n        }\r\n      ]\r\n    }\r\n  }\r\n}\r\n\n", "tags": [], "categories": ["Blog", "Search"]}
{"post_id": 1372, "title": "Creating a search DSL", "url": "https://www.luminis.eu/blog-en/search-en/creating-a-search-dsl/", "updated_at": "2020-11-16T16:42:44", "body": "As an (elastic)search expert, I regularly visit customers. For these customers I often do a short analysis of their search solution and I give advice about improvements they can make. It is always interesting to look at solutions customers come up with. At one of my most recent customers I noticed a search solution based on a very extensive search DSL (Domain Specific Language) created with Antlr. I knew about Antlr, but never thought about creating my own search DSL.\nTo\u00a0better understand the options of Antlr and to practice with creating my own DSL I started experimenting with it. In this blog post I\u2019ll take you on my learning journey. I am going to create my own very basic search DSL.\nSpecifying the DSL\nFirst we need to define the queries we would like our users to enter. Below are some examples:\n\ntree\u00a0\u2013 This is an easy one, just one word to search for\ntree apple\u00a0\u2013 Two words to look up\ntree apple AND sell\u00a0\u2013 Find matching content for\u00a0tree apple, but also containing\u00a0sell.\ntree AND apple OR juice\u00a0\u2013 Find matching content containing the terms\u00a0tree\u00a0and\u00a0apple\u00a0or containing the term\u00a0juice.\n\u201capple tree\u201d OR juice\u00a0\u2013 Find content having the terms apple and tree next to each other in the right order (Phrase query) or having the term juice.\n\nThese are the combinations we need to make. In the next sections we setup our environment and I explain the basics of Antlr that you need to understand to follow along.\nSetting up Antlr for your project\nThere are lots of resources about setting up your local Antlr environment. I personally learned most from\u00a0tomassetti. I prefer to use Maven to gather the required dependencies. I also use the Maven Antlr plugin to generate the Java classes based on the Lexar and Grammar rules.\nI also installed Antlr using Homebrew, but you do not really need this for this blog post.\nYou can find the project on Github:\u00a0https://github.com/jettro/search-dsl\nI generally just load the Maven project into IntelliJ and get everything running from there. If you don\u2019t want to use an IDE, you can also do this with pure Maven.\nproj_home #> mvn clean install\r\nproj_home #> mvn dependency:copy-dependencies\r\nproj_home #> java -classpath \"target/search-dsl-1.0-SNAPSHOT.jar:target/dependency/*\"  nl.gridshore.searchdsl.RunStep1\nOf course you can change the RunStep1 into one of the other three classes.\nAntlr introduction\nThis blog post does not have the intention to explain all ins and outs of Antlr. But there are a few things you need to know if you want to follow along with the code samples.\n\nLexer\u00a0\u2013 A program that takes a phrase and obtains tokens from the phrase. Examples of lexers are:\u00a0AND\u00a0consisting of the characters \u2018AND\u2019 but also the specials characters \u2018&&\u2019. Another example is a\u00a0WORD\u00a0consisting of upper or lowercase characters and numbers. Tokens coming out of a Lexer contain the type of the token as well as the matched characters by that token.\nGrammar\u00a0\u2013 Rules that make use of the Lexer to create the syntax of your DSL. The result is a parser that creates a ParseTree out of your phrase. For example, we have a grammar rule\u00a0querythat parses a phrase like\u00a0tree AND apple\u00a0into the following ParseTree. The Grammar rule is:\u00a0query : term (AND term)+ ;.\nParseTree\u00a0\u2013 Tree by Antlr using the grammar and lexer from the provided phrase. Antlr also comes with a tool to create a visual tree. See an example below. In this blog post we create our own parser of the tree, there are however two better alternatives. The first is using the classic Listener pattern. The other is the Visitor pattern.\n\nListener\u00a0\u2013 Antlr generates some parent classes to create your own listener. The idea behind a a listener is that you receive events when a new element is started and when the element is finished. This resembles how for instance the SAX parser works.\nVisitor\u00a0\u2013 Antlr generates some parent classes to create your own Visitors. With a visitor you start visiting your top level element, then you visit the children, that way you recursively go down the tree. In a next blog post we\u2019ll discuss the visitor pattern in depth.\n\nSearch DSL Basics\nIn this section we are going to create the DSL in four small steps. For each step we have a\u00a0StepXLexerRules.g4\u00a0and a\u00a0StepXSearchDsl.g4\u00a0file containing the Antlr lexer and grammar rules. Each step also contains a Java file with the name RunStepX.\nStep 1\nIn this step we want to write rules like:\n\napple\napple juice\napple1 juice\n\n\nlexer\r\nWORD        : ([A-z]|[0-9])+ ;\r\nWS          : [ \\t\\r\\n]+ -> skip ;\r\n \r\ngrammar\r\nquery       : WORD+ ;\nIn all the Java examples we\u2019ll start the same. I\u2019ll mention the rules here but will not go into depth in the other steps.\nLexer lexer = new Step1LexerRules(CharStreams.fromString(\"apple juice\"));\r\nCommonTokenStream commonTokenStream = new CommonTokenStream(lexer);\r\n \r\nStep1SearchDslParser parser = new Step1SearchDslParser(commonTokenStream);\r\nStep1SearchDslParser.QueryContext queryContext = parser.query();\nFirst we create the Lexer, the Lexer is generated by Antlr. The input is a stream of characters created using the class CharStreams. From the Lexer we obtain a stream of Tokens, which is the input for the parser. The parser is also generated by Antlr. Using the parser we can obtain the queryContext. Notice the method query. This is the same name as the first grammar rule.\nIn this basic example a query consists of at least one WORD and a WORD consists of upper and lower case characters and numbers. The output for the first step is:\nSource: apple\r\nWORDS (1): apple,\r\nSource: apple juice\r\nWORDS (2): apple,juice,\r\nSource: apple1 juice\r\nWORDS (2): apple1,juice,\nIn the next step we are extending the DSL with an option to keep words together.\nStep 2\nIn the previous step you got the option to search for one or multiple words. In this step we are adding the option to keep some words together by surrounding them with quotes. We add the following lines to the lexer and grammar.\n\n1\r\n2\r\n3\r\n4\r\n5\r\n6\r\n7\r\n8\r\nlexer\r\nQUOTE   : [\"];\r\n \r\ngrammar\r\nquery               : term ;\r\n \r\nterm                : WORD+|quotedTerm;\r\nquotedTerm          : QUOTE WORD+ QUOTE ;\nNow we can support queries like\napple\n\u201capple juice\u201d\nThe addition to the lexer is QUOTE, the grammar becomes slightly more complex. The query now is a term, a term can be multiple WORDs or a quoted term consisting of multiple WORDs surrounded by QUOTEs. In Java we have to check from the termContext that is obtained from the queryContext if the term contains WORDs or a quotedTerm. That is what is shown in the next code block.\nStep2SearchDslParser.TermContext termContext = queryContext.term();\r\nhandleTermOrQuotedTerm(termContext);\r\n \r\nprivate void handleTermOrQuotedTerm(Step2SearchDslParser.TermContext termContext) {\r\n    if (null != termContext.quotedTerm()) {\r\n        handleQuotedTerm(termContext.quotedTerm());\r\n    } else {\r\n        handleWordTokens(termContext.WORD());\r\n    }\r\n}\r\n \r\nprivate void handleQuotedTerm(Step2SearchDslParser.QuotedTermContext quotedTermContext) {\r\n    System.out.print(\"QUOTED \");\r\n    handleWordTokens(quotedTermContext.WORD());\r\n}\nNotice how we check if the termContext contains a quotedTerm, just by checking if it is null. The output then becomes\nSource: apple\r\nWORDS (1): apple,\r\nSource: \"apple juice\"\r\nQUOTED WORDS (2): apple,juice,\nTime to take the next step, this time we make it possible to specify to make it explicit to query for one term or the other.\nStep 3\nIn this step we make it possible to make it optional for a term to match as long as another term matches. Example queries are:\n\napple\napple OR juice\n\u201capple juice\u201d OR applejuice\n\nThe change to the Lexer is just one type\u00a0OR. The grammar has to change, now the query needs to support a term or an orQuery. The orQuery consists of a term extended with\u00a0OR\u00a0and a term, at least once.\n\n1\r\n2\r\n3\r\n4\r\n5\r\n6\r\nlexer\r\nOR      : 'OR' | '||' ;\r\n \r\ngrammar\r\nquery   : term | orQuery ;\r\norQuery : term (OR term)+ ;\nThe handling in Java is straightforward now, again some null checks and handle methods.\n1\r\n2\r\n3\r\n4\r\n5\r\nif (queryContext.orQuery() != null) {\r\n    handleOrContext(queryContext.orQuery());\r\n} else {\r\n    handleTermContext(queryContext.term());\r\n}\nThe output of the program then becomes:\nSource: apple\r\nWORDS (1): apple,\r\nSource: apple OR juice\r\nOr query: \r\nWORDS (1): apple,\r\nWORDS (1): juice,\r\nSource: \"apple juice\" OR applejuice\r\nOr query: \r\nQUOTED WORDS (2): apple,juice,\r\nWORDS (1): applejuice,\nIn the final step we want to make the OR complete by also adding an\u00a0AND.\nStep 4\nIn the final step for this blog we are going to introduce\u00a0AND. With the combination of AND we can make more complicated combinations. What would you make from\u00a0one AND two OR three OR four AND five. In my DSL I first do the AND, then the OR. So this would become\u00a0(one AND two) OR three OR (four AND five). So a document would match if it contains one and two, or four and five, or three. The Lexer does change a bit, again we just add a type for\u00a0AND. The grammar has to introduce some new terms. It is good to have an overview of the complete grammar.\n\nquery               : term | orQuery | andQuery ;\r\n \r\norQuery             : orExpr (OR orExpr)+ ;\r\norExpr              : term|andQuery;\r\n \r\nandQuery            : term (AND term)+ ;\r\nterm                : WORD+|quotedTerm;\r\nquotedTerm          : QUOTE WORD+ QUOTE ;\nAs you can see, we introduced an orExpr, being a term or an andQuery. We changed an orQuery to become an orExpr followed by at least one combination of OR and another orExpr. The query now is a term, an orQuery or an andQuery. Some examples below.\n\napple\napple OR juice\napple raspberry OR juice\napple AND raspberry AND juice OR Cola\n\u201capple juice\u201d OR applejuice\n\nThe java code becomes a bit boring by now, so let us move to the output of the program immediately.\n\nSource: apple\r\nWORDS (1): apple,\r\nSource: apple OR juice\r\nOr query: \r\nWORDS (1): apple,\r\nWORDS (1): juice,\r\nSource: apple raspberry OR juice\r\nOr query: \r\nWORDS (2): apple,raspberry,\r\nWORDS (1): juice,\r\nSource: apple AND raspberry AND juice OR Cola\r\nOr query: \r\nAnd Query: \r\nWORDS (1): apple,\r\nWORDS (1): raspberry,\r\nWORDS (1): juice,\r\nWORDS (1): Cola,\r\nSource: \"apple juice\" OR applejuice\r\nOr query: \r\nQUOTED WORDS (2): apple,juice,\r\nWORDS (1): applejuice,\nConcluding\nThat is it for now, of course this is not the most complicated search DSL. You can most likely come up with other interesting constructs. The goal for this blogpost was to get you underway. In the next blog post I intend to discuss and show how to create a visitor that makes a real elasticsearch query based on the DSL.\n", "tags": [], "categories": ["Blog", "Search"]}
{"post_id": 1164, "title": "6 Rules for writing UI Guidelines", "url": "https://www.luminis.eu/blog-en/concepting-ux-en/6-rules-for-writing-ui-guidelines/", "updated_at": "2020-11-17T15:39:14", "body": "User interface (or UI)\u00a0guidelines are intended for designers and developers to ensure a\u00a0suiting and consistent\u00a0user experience of a specific product, product range or brand. The guideline can be a Word document, but these days it is more common to find UI guidelines on a Wiki or in another online format. Guidelines are an important resource,\u00a0but there is a problem. Designers are great in designing, but usually lack the ability\u00a0to\u00a0document their thinking in a way others can continue their work along the lines of their original concept. I am a designer and\u00a0I believe I\u2019ve found a way.\nCreativity is my trade as a designer. It thrives in moderate chaos; being overly organised kills my ability to think outside boxes and shift angles to come up with original solutions. \u201cTo think the unthinkable one\u2019s mind needs to wander freely.\u201d I know how that sounds, but it\u2019s true nonetheless. I am blessed to work with people that make up for my lack of organization. Colleagues that have the patience to put up with my chaos and turn my designs into actual products. But in writing UI guidelines I\u2019m on my own. \u00a0 Although writing is in itself a creative process, you need to know what you want to say before you can start writing. Before I can write UI guidelines I need to analyse my own design process and decisions. It is sometimes a bit worrying that for some design decisions the only thing that seems to come to mind is \u201cthis solution just felt right to me.\u201d Of course I can\u2019t write that down. I have to backtrack my (mostly intuitive) design process to find out why I took those decisions. I have to turn intuition into reasoning. \u00a0 Having written and contributed to multiple UI guidelines I found that sticking to these following 6 rules enables me to write a decent guideline, bypassing my lack of organizational skills.\n\n\nDesign a guideline like you would design any other product. A UI guideline is a utility; it is intended for a specific task and for a specific type of people. In order to know what to write and how to publish your writing,\u00a0you first\u00a0need to know who you are writing for and how these people will use your guideline. In that respect a guideline is not different from any other product I design. It helps a lot to involve the intended audience into your writing process. Ask them what they expect and need. Try to find out what they already know.\u00a0Let them review and comment on your work.\nA guideline is\u00a0never complete, so\u00a0focus on doing enough. You can go on forever in describing every little UI\u00a0detail and the harmony of it all, but you have to stop at some point. It is a tricky balance: write too little and people will not have enough to hold on to, write too much and people will not be able to grasp the big picture or how to apply the separate parts. My advise is to start small, see how it is understood and applied and then add more if needed.\nA guideline is a living thing, facilitate the process. New things will get added in the future, things will get\u00a0changed. Common solutions today might be deprecated tomorrow. You need a solid process in writing and maintaining the guideline to keep ensuring it\u2019s quality in the future. Make sure users can comment on (parts of) the guideline and suggest changes or additions. Keep users posted on new versions of the guideline and what has changed.\nNot just write the\u00a0rules, but also the reasoning. You can\u2019t cover everything in a limited set of rules. You need to make your audience smart enough to come up with solutions for unforeseen situations. So back up every rule with design principles. These design principles are what connects your rules, patterns and elements with the product\u00a0vision. Also:\u00a0spending an hour or two face-to-face\u00a0with your intended\u00a0audience\u00a0to explain the basic UI\u00a0concept can be more effective than weeks of writing.\nWrite guidelines while you are\u00a0designing. Design principles and patterns emerge during the process of designing a user interface, so you best frame them when you\u00a0identify them. Framing design principles and patterns also helps you in designing a better, more coherent product, even if you are not intending to write actual\u00a0guidelines for the product.\nAdd examples. Lots of them. Because examples really show how the separate parts of your guideline come together and make a user interface. It makes your guideline more accessible and if done right really proves the value of your design efforts. You could even add your own design files so they can be used as design templates to get people up and running fast.\n\n\n", "tags": [], "categories": ["Blog", "Concepting &amp; UX"]}
{"post_id": 1412, "title": "Looking ahead: new field collapsing feature in Elasticsearch", "url": "https://www.luminis.eu/blog-en/search-en/looking-ahead-new-field-collapsing-feature-in-elasticsearch/", "updated_at": "2020-11-13T10:47:37", "body": "At Luminis Amsterdam, Search is one of our main focus points. Because of that, we closely keep an eye out for upcoming features.\nOnly a few weeks ago, I noticed that the following\u00a0pull request (\u201cAdd field collapsing for search request\u201d)\u00a0was merged into the Elasticsearch code base, tagged for the 5.3/6.x release. This feature allows you to group your search results based on a specific key. In the past, this was merely possible by using a combination of an \u2018aggregation\u2019 and \u2018top hits\u2019.\nNow a good question would be: \u2018why would I want this?\u2019 or \u2018what is this grouping you are talking about?\u2019. Imagine having a website where you sell Apple products. MacBook\u2019s, iPhones, iPad\u2019s etc\u2026 Let\u2019s say because of functional requirements, we have to create separate documents for each variant of each device. (eg. separate documents for iPad Air 2 32GB Silver, iPad Air 2 32GB Gold etc..) When a user searches for the word \u2018iPad\u2019, having no result grouping, will mean that your users will see search results for all the iPads you are selling. This could mean that your result list looks like the following:\n\niPad Air 2 32GB Pink\niPad Air 2 128GB Pink\niPad Air 2 32GB Space Grey\niPad Air 2 128GB Space Grey\n..\n..\n..\n..\n..\niPad Pro 12.9 Inch 32GB Space Grey\nIpad Case with happy colourful pictures on it.\n\nNow for the sake of this example, let\u2019s say we only show 10 products per page. If our user was really looking for an iPad case, he wouldn\u2019t see this product, but instead, would be shown a long list of \u2018the same\u2019 iPad. This is not really user-friendly. Now, a better approach would be to group all the Ipad Air 2 products in one, so that it would take only 1 spot in the search results list. You would have to think of a visual presentation in order to notify the user that there are more variants of that same product.\nAs mentioned before, grouping of results was already possible in older versions of Elasticseach, but the downside of this old approach was that it would use a lot of memory when computing this on big data sets, plus that paginating result was not (really) possible. An example:\n\nGET shop/_search\r\n{\r\n  \"size\": 0,\r\n  \"query\": {\r\n    \"match\": {\r\n      \"title\": \"iPad\"\r\n    }\r\n  },\r\n  \"aggs\": {\r\n    \"collapse_by_id\": {\r\n      \"terms\": {\r\n        \"field\": \"family_id\",\r\n        \"size\": 10,\r\n        \"order\": {\r\n          \"max_score\": \"desc\"\r\n        }\r\n      },\r\n      \"aggs\": {\r\n        \"max_score\": {\r\n          \"max\": {\r\n            \"script\": \"_score\"\r\n          }\r\n        },\r\n        \"top_hits_for_family\": {\r\n          \"top_hits\": {\r\n            \"size\": 3\r\n          }\r\n        }\r\n      }\r\n    }\r\n  }\r\n}\r\n\n\nWe perform a Terms aggregation on the family_id, which results in the grouping we want. Next, we can use top_hits to get the documents belonging to that family.\n\nAll seems well. Now let\u2019s say we have a website where users are viewing 10 products per page. In order for users to go to the next page, we would have to execute the same query, up the number of aggregations to 20 and remove the first 10 results. Aggregations use quite some processing power, so having to constantly aggregate over the complete set will not be really performant when having a big data set. Another way would be to eliminate the first page results by executing a query with for page 2 combined with a filter to eliminate the families already shown. All in all, this would be a lot of extra work in order to achieve a field collapsing feature.\nNow that Elasticsearch added the field collapsing feature, this becomes a lot easier. You can download my\u00a0gist( with some setup for if you want to play along with the example. The gist contains some settings/mappings, test data and the queries which I will be showing you in a minute.\nAlongside query, aggregations, suggestions, sorting/pagination options etc.. Elasticsearch has added a new \u2018collapse\u2019 feature:\n\nGET shop/_search\r\n{\r\n  \"query\": {\r\n    \"match\": {\r\n      \"title\": \"Ipad\"\r\n    }\r\n  },\r\n  \"collapse\": {\r\n    \"field\": \"family_id\"\r\n  }\r\n}\nThe simplest version of collapse only takes a field name on which to form the grouping. If we execute this query, it will generate the following result:\n\"hits\": {\r\n    \"total\": 6,\r\n    \"max_score\": null,\r\n    \"hits\": [\r\n      {\r\n        \"_index\": \"shop\",\r\n        \"_type\": \"product\",\r\n        \"_id\": \"5\",\r\n        \"_score\": 0.078307986,\r\n        \"_source\": {\r\n          \"title\": \"iPad Pro ipad\",\r\n          \"colour\": \"Space Grey\",\r\n          \"brand\": \"Apple\",\r\n          \"size\": \"128gb\",\r\n          \"price\": 899,\r\n          \"family_id\": \"apple-5678\"\r\n        },\r\n        \"fields\": {\r\n          \"family_id\": [\r\n            \"apple-5678\"\r\n          ]\r\n        }\r\n      },\r\n      {\r\n        \"_index\": \"shop\",\r\n        \"_type\": \"product\",\r\n        \"_id\": \"1\",\r\n        \"_score\": 0.05406233,\r\n        \"_source\": {\r\n          \"title\": \"iPad Air 2\",\r\n          \"colour\": \"Silver\",\r\n          \"brand\": \"Apple\",\r\n          \"size\": \"32gb\",\r\n          \"price\": 399,\r\n          \"family_id\": \"apple-1234\"\r\n        },\r\n        \"fields\": {\r\n          \"family_id\": [\r\n            \"apple-1234\"\r\n          ]\r\n        }\r\n      }\r\n    ]\r\n  }\nNotice the total amounts in the query response, showing the total amount of documents that were matched against the query. Our hits only return 2 hits, but if we look at the \u2018fields\u2019 section of the result, we can see our two unique family_id\u2019s. The best matching result for each family_id is returned in the search results.\nIt is also possible to retrieve the documents directly for each family_id by adding an inner_hits block inside collapse:\nGET shop/_search\r\n{\r\n  \"query\": {\r\n    \"match\": {\r\n      \"title\": \"iPad\"\r\n    }\r\n  },\r\n  \"collapse\": {\r\n    \"field\": \"family_id\",\r\n    \"inner_hits\": {\r\n      \"name\": \"collapsed_by_family_id\",\r\n      \"from\": 1,\r\n      \"size\": 2\r\n    }\r\n  }\r\n}\nYou can use \u2018from:1\u2019 to exclude the first hit in the family, since it\u2019s already the returned parent of the family\nWhich results in:\n\"hits\": {\r\n    \"total\": 6,\r\n    \"max_score\": null,\r\n    \"hits\": [\r\n      {\r\n        \"_index\": \"shop\",\r\n        \"_type\": \"product\",\r\n        \"_id\": \"5\",\r\n        \"_score\": 0.078307986,\r\n        \"_source\": {\r\n          \"title\": \"iPad Pro ipad\",\r\n          \"colour\": \"Space Grey\",\r\n          \"brand\": \"Apple\",\r\n          \"size\": \"128gb\",\r\n          \"price\": 899,\r\n          \"family_id\": \"apple-5678\"\r\n        },\r\n        \"fields\": {\r\n          \"family_id\": [\r\n            \"apple-5678\"\r\n          ]\r\n        },\r\n        \"inner_hits\": {\r\n          \"collapsed_family_id\": {\r\n            \"hits\": {\r\n              \"total\": 2,\r\n              \"max_score\": 0.078307986,\r\n              \"hits\": [\r\n                {\r\n                  \"_index\": \"shop\",\r\n                  \"_type\": \"product\",\r\n                  \"_id\": \"6\",\r\n                  \"_score\": 0.066075005,\r\n                  \"_source\": {\r\n                    \"title\": \"iPad Pro\",\r\n                    \"colour\": \"Space Grey\",\r\n                    \"brand\": \"Apple\",\r\n                    \"size\": \"256gb\",\r\n                    \"price\": 999,\r\n                    \"family_id\": \"apple-5678\"\r\n                  }\r\n                }\r\n              ]\r\n            }\r\n          }\r\n        }\r\n      },\r\n      {\r\n        \"_index\": \"shop\",\r\n        \"_type\": \"product\",\r\n        \"_id\": \"1\",\r\n        \"_score\": 0.05406233,\r\n        \"_source\": {\r\n          \"title\": \"iPad Air 2\",\r\n          \"colour\": \"Silver\",\r\n          \"brand\": \"Apple\",\r\n          \"size\": \"32gb\",\r\n          \"price\": 399,\r\n          \"family_id\": \"apple-1234\"\r\n        },\r\n        \"fields\": {\r\n          \"family_id\": [\r\n            \"apple-1234\"\r\n          ]\r\n        },\r\n        \"inner_hits\": {\r\n          \"collapsed_family_id\": {\r\n            \"hits\": {\r\n              \"total\": 4,\r\n              \"max_score\": 0.05406233,\r\n              \"hits\": [\r\n                {\r\n                  \"_index\": \"shop\",\r\n                  \"_type\": \"product\",\r\n                  \"_id\": \"2\",\r\n                  \"_score\": 0.05406233,\r\n                  \"_source\": {\r\n                    \"title\": \"iPad Air 2\",\r\n                    \"colour\": \"Gold\",\r\n                    \"brand\": \"Apple\",\r\n                    \"size\": \"32gb\",\r\n                    \"price\": 399,\r\n                    \"family_id\": \"apple-1234\"\r\n                  }\r\n                },\r\n                {\r\n                  \"_index\": \"shop\",\r\n                  \"_type\": \"product\",\r\n                  \"_id\": \"3\",\r\n                  \"_score\": 0.05406233,\r\n                  \"_source\": {\r\n                    \"title\": \"iPad Air 2\",\r\n                    \"colour\": \"Space Grey\",\r\n                    \"brand\": \"Apple\",\r\n                    \"size\": \"32gb\",\r\n                    \"price\": 399,\r\n                    \"family_id\": \"apple-1234\"\r\n                  }\r\n                }\r\n              ]\r\n            }\r\n          }\r\n        }\r\n      }\r\n    ]\r\n  }\nPaging was an issue with the old approach, but since documents are grouped inside the search results, paging works out of the box. Same way as it does for normal queries and with the same limitations.\nA lot of people in the community have been waiting for this feature and I\u2019m excited that it finally arrived. You can play around with the data set and try some more \u2018collapsing\u2019 (eg by color, brand, size etc..). I hope this gave you a small overview of what\u2019s to come in the upcoming 5.3/6.x release.\n", "tags": [], "categories": ["Blog", "Search"]}
{"post_id": 1589, "title": "Writing tests like a novelist", "url": "https://www.luminis.eu/blog-en/development-en/writing-tests-like-a-novelist/", "updated_at": "2020-12-10T09:52:54", "body": "Last week my colleague Piet claimed: \u201cYou shouldn\u2019t need several hours to understand what a method, class or package does\u201d. Since unit tests are written in classes and methods, the same holds here. Welcome to the next episode of \u201creducing mental effort for software developers\u201d.\nIn this post I will lay out how AssertJ can help to reduce the mental effort needed while reading and writing test code, and as a bonus how it reduces the effort needed for understanding results of failing tests. AssertJ is a library that provides fluent assertions for Java. Before I dive into the fluent part, let\u2019s start with some examples of assertions. Suppose you want to check that a String is of a certain value. In JUnit this will be done in the following way:\nassertEquals(\"expected\", \"result\");\nIn natural language this statement can be described as: \u201cassert that expected and result are equal\u201d. The same check with AssertJ can be done with:\nassertThat(\"result\").isEqualTo(\"expected\");\nComparing to JUnit, the two values are in a reversed order. With assertThat() you specify which value you want to check, followed by isEqualTo() you specify to which value it should comply. Now the statement is expressed in a way closely to that of natural language. If you would strip the punctuation marks and \u201cde-CamelCase\u201d it, you\u2019ll get the sentence: \u201cassert that result is equal to expected\u201d. My English may not be perfect, but this statements sounds a lot more like a sane and natural sentence. Because the Strings of these two examples are unequal, these tests will fail with the message:\norg.junit.ComparisonFailure:\r\nExpected :expected\r\nActual   :result\nSometimes I come across unit tests where expected and result are swapped like this:\nassertEquals(\"result\", \"expected\");\nThis is correct, but can be confusing when you\u2019ve broken some tests and reading the message:\norg.junit.ComparisonFailure:\r\nExpected :result\r\nActual   :expected\nIn this example it\u2019s quite obvious that something is wrong in the test, but imagine that in more obscure situations you\u2019ll need a lot more mental effort before you find out what\u2019s wrong and why the test is failing. AssertJ does not offer bullet proof protection against these kind of programming errors, but it will reduce the chance. A bell should ring when you read or write:\nassertThat(\"expected\").isEqualTo(\"result\");\nThese equals checks are simple examples to make a clear difference between plain JUnit and the fluent assertions of AssertJ. The real power of fluent kicks in when applying multiple assertions in one single statement. For example:\nassertThat(alphabet)\r\n       .isNotNull()\r\n       .containsIgnoringCase(\"A\")\r\n       .startsWith(\"abc\")\r\n       .endsWith(\"xyz\");\nAs we\u2019ve seen before, this statement reads like natural language. In JUnit on the other hand the equivalent test will read like:\nassertNotNull(alphabet);\r\nassertTrue(alphabet.toUpperCase().contains(\"A\"));\r\nassertTrue(alphabet.startsWith(\"abc\"));\r\nassertTrue(alphabet.endsWith(\"xyz\"));\nApart from needing four separate statements, we now discover that JUnit provides quite a limited API. Bluntly, JUnit can check that something is true/false or that something is null (or not). Using only JUnit we can\u2019t say: \u201ccheck that this String contains the character A\u201d. We have to use the\u00a0contains method of Java\u2019s String class, and then check that its result is true. Let\u2019s zoom in on the example of contains(). The JUnit the test:\nassertTrue(\"abc\".contains(\"A\"));\nwill fail with the message:\njava.lang.AssertionError\r\n    at org.junit.Assert.fail(Assert.java:86)\r\n    at org.junit.Assert.assertTrue(Assert.java:41)\r\n    at org.junit.Assert.assertTrue(Assert.java:52)\r\n    at assertj.Strings.contains_junit(StringsTest.java:34)\r\n...\nWith Fluent Assertions the same test would be written as:\nassertThat(\"abc\").contains(\"A\");\nBecause we exactly tell what we want to test (that \u201cabc\u201d contains the character A), AssertJ has enough information to tell us what went wrong. So this test fails with the message:\njava.lang.AssertionError:\r\nExpecting:\r\n <\"abc\">\r\nto contain:\r\n <\"A\">\nWe\u2019ve now seen how we can write better readable tests which give more information when a test fails. Until now I only gave examples with Strings, but AssertJ provides API\u2019s for more data types. All examples can be found on AssertJ\u2019s website, but let me highlight another commonly used data type.\nCollections\nSuppose we want to test this List of Strings:\nList numberList = Arrays.asList(\"One\", \"Two\");\nIn JUnit this will look like:\nassertEquals(Arrays.asList(\"Two\"), numberList);\nAnd this fails with the message:\nExpected :[Two]\r\nActual   :[One, Two]\nUsing AssertJ the same would look like:\nassertThat(numberList).containsExactly(\"Two\");\nand this fails with the message:\nActual and expected should have same size but actual size was:\r\n  <2>\r\nwhile expected size was:\r\n  <1>\r\nActual was:\r\n  <[\"One\", \"Two\"]>\r\nExpected was:\r\n  <[\"Two\"]>\nSo AssertJ tells us that the size is incorrect. Nice, we do not have the scan all the elements to find out what the difference is ourselves. Another example where the size is equal, but the ordering is different. JUnit\u2019s:\nassertEquals(Arrays.asList(\"Two\", \"One\"), numberList);\nwill fail with:\nExpected :[Two, One]\r\nActual   :[One, Two]\nWhile AssertJ\u2019s:\nassertThat(numberList).containsExactly(\"Two\", \"One\");\nwill fail with:\nActual and expected have the same elements but not in the same order, at index 0 actual element was:\r\n  <\"One\">\r\nwhereas expected element was:\r\n  <\"Two\">\nIn these examples the lists only contained two elements, but when the list is larger, it will get hard to find out which element is missing, or to see the difference. A last example where the difference in Collections is a bit more obscure. Suppose we want to check if the following List of numbers correctly counts up:\nList largeNumberList = Arrays.asList(1, 2, 2, 4, 5);\nJUnit\u2019s:\nassertEquals(Arrays.asList(1, 2, 3, 4, 5), largeNumberList);\nwill fail with:\nExpected :[1, 2, 3, 4, 5]\r\nActual   :[1, 2, 2, 4, 5]\nUnless you become happy from\u00a0playing a game of\u00a0spot the difference\u00a0this results in needless occupation of your mental capacity. And that while AssertJ\u2019s:\nassertThat(largeNumberList).containsExactly(1, 2, 3, 4, 5);\nfails with:\nExpecting:\r\n  <[1, 2, 2, 4, 5]>\r\nto contain exactly (and in same order):\r\n  <[1, 2, 3, 4, 5]>\r\nbut could not find the following elements:\r\n  <[3]>\nIn a glance we see what is wrong. Again, when Collections tends to be larger in size, these kind of failure messages are only getting more helpful.\nWhy not Hamcrest?\nWell fair point. Hamcrest core has been included in JUnit since version 4.4 and tests using the hamcrest API look a lot more like AssertJ than that they look like plain JUnit. Also the failure messages are better than in Plain JUnit. But in my opinion Hamcrest does both these jobs not as well as AssertJ. Let\u2019s compare the two.\nComparing Strings with Hamcrest:\nassertThat(\"abc\", containsString(\"A\"));\r\nfails with:\r\nExpected: a string containing \"A\"\r\nbut: was \"abc\"\nAt least we see the expected (containing \u201cA\u201d) and actual ( \u201cabc\u201d ) here, so that\u2019s better than JUnit. At this point Hamcrest still reads like natural language just like the Fluent Assertions. But let\u2019s get back on the example with multiple assertions on the letters of the alphabet String.\nWith Fluent Assertions we saw:\nassertThat(\"abc\")\r\n    .isNotNull()\r\n    .startsWith(\"abc\")\r\n    .endsWith(\"xyz\");\r\n\nwhich fails with:\nExpecting:\r\n    <\"abc\">\r\nto end with:\r\n    <\"xyz\">\nThe equivalent in Hamcrest will look like:\u00a0\nassertThat(\"abc\", allOf(\r\n    is(notNullValue()),\r\n    startsWith(\"abc\"),\r\n    endsWith(\"xyz\")));\r\n\nand fails with:\nExpected: (is not null and a string starting with \"abc\" and a string ending with \"xyz\")\r\n    but: a string ending with \"xyz\" was \"abc\"\r\n\nDecide for yourself which failure message requires less effort to understand what is tested and what went wrong. As we can see in the test itself, Hamcrest provides a prefix notation like API to perform multiple assertions. This requires the reader to create a mental model of a stack with the operators like allOf() and is() while understanding the different assertions. With the given example, this may sound exaggerated, but in more complex situations, this requires quite some mental effort.\nAs I said in the beginning, only Hamcrest-core is part of JUnit, which is quite limited. When you want to test collections, for example, you need to add hamcrest-all to your project. And when already adding an extra dependency to your project anyway, why not choose AssertJ. The last release of Hamcrest dates back to 2012, while AssertJ is more actively developed (May 2017) and supports Java 8 features.\nThe last reason why I think AssertJ is the best, the only, and nothing but the best is code completion is the additional advantage of its Fluent API so that we can simply use code completion to explore all the possibilities. Without the the need for memorizing the whole API or the need for cheat sheets.\nGetting Started\nThe AssertJ website is filled with examples and instructions on how to include AssertJ in your project. For an extensive set of examples, see the assertj-examples tests project on GitHub. When you\u2019re using Eclipse, use this tip to get code completion. You could do the same for Mockito. by the way.\nWhile the examples in this post were in Java with the AssertJ library, the same ideas apply for other languages. See, for example, fluentassertions.com for .NET.\nAfter reading this, I hope you\u2019re even more devoted to creating code that is simple and direct. Or as Grady Booch, author of Object Oriented Analysis and Design with Applications, said, \u201cclean code reads like well-written prose.\u201d\n", "tags": [], "categories": ["Blog", "Development"]}
{"post_id": 1707, "title": "Creating an elasticsearch plugin, the basics", "url": "https://www.luminis.eu/blog-en/search-en/creating-an-elasticsearch-plugin-the-basics/", "updated_at": "2020-11-24T15:14:11", "body": "Elasticsearch is a search solution based on Lucene. It comes with a lot of features to enrich the search experience. Some of these features have been recognised as very useful in the analytics scene as well. Interacting with elasticsearch mainly takes place using the REST endpoint. You can do everything using the different available endpoints. You can create new indexes, insert documents, search for documents and lots of other things.\nStill some of the things are not available out of the box. If you need an analyser that is not available by default, you can install it as a plugin. If you need security, you can install a plugin. If you need alerting, you can install it as a plugin. I guess you get the idea by now. The plugin extension option is nice, but might be a bit hard to begin with. Therefore in this blog post I am going to write a few plugins. I\u2019ll point you to some of the resources I used to get it running and I want to give you some inspiration for your own ideas for cool plugins that extend the elasticsearch functionality.\nBit of history\nIn the releases prior to version 5 there were two type of plugins,\u00a0site\u00a0and\u00a0java\u00a0plugins. Site plugins were used extensively. Some well known examples are: Head, HQ, Kopf. Also Kibana and Marvel started out as a site plugin. It was a nice feature, however not the core of elasticsearch. Therefore the elastic team deprecated site plugins in 2.3 and the support was removed in 5.\nHow does it work\nThe default elasticsearch installation already provides a script to install plugins. You can find it in the\u00a0binfolder. You can install plugins from repositories but also from a local path. A plugin comes in the form of a jar file.\nPlugins need to be installed on every node of the cluster. Installation is as simple as the following command.\nbin/elasticsearch-plugin install file:///path/to/elastic-basic-plugin-5.1.2-1-SNAPSHOT.zip\r\n\nIn this case we install the plugin from our own hard drive. The plugins have a dependency on the elastic core and therefore need to have the exact same version as the elastic version you are using. So for each elasticsearch release you have to create a new version of the plugin. In the example I have created the plugin for elasticsearch\u00a05.1.2.\nStart with our own plugin\nElastic uses gradle internally to build the project, I still prefer maven over gradle. Luckily David Pilato wrote\u00a0a good blog post about creating the maven project. I am not going to repeat all the steps of him. Feel free to take a peek at the\u00a0pom.xml\u00a0I used in my plugin.\nCreate BasicPlugin that does nothing\nThe first step in the plugin is to create a class that starts the plugin. Below is the class that has just one functionality, print a statement in the log that the plugin is installed.\npublic class BasicPlugin extends Plugin {\r\n    private final static Logger LOGGER = LogManager.getLogger(BasicPlugin.class);\r\n    public BasicPlugin() {\r\n        super();\r\n        LOGGER.warn(\"Create the Basic Plugin and installed it into elasticsearch\");\r\n    }\r\n}\nNext step is to configure the plugin as described by\u00a0David Pilato\u00a0in his blog I mentioned before. We need to add the maven assembly plugin using the file\u00a0src/main/assemblies/plugin.xml. In this file we refer to another very important file,\u00a0src/main/resources/plugin-descriptor.properties. With all this in place we can run maven to create the plugin in a jar.\nmvn clean package -DskipTests\r\n\nIn the folder target/releases you\u2019ll now find the file\u00a0elastic-basic-plugin-5.1.2-1-SNAPSHOT.zip. Which is a jar file in disguise, we could change the extension to jar, there is no difference. Now use the command from above to install. If you get a message that the plugin is already there, you need to remove it first\nbin/elasticsearch-plugin remove elastic-basic-plugin\r\n\nThen after installing the plugin you\u2019ll find the following line in the log of elasticsearch when starting\n[2017-01-31T13:42:01,629][WARN ][n.g.e.p.b.BasicPlugin    ] Create the Basic Plugin and installed it into elasticsearch\r\n\nThis is of course a bit silly, let us create a new rest endpoint that checks if the elasticsearch database contains an index called\u00a0Jettro.\nCreate a new REST endpoint\nThe inspiration for this endpoint came from another blog post by David Pilato:\u00a0Creating a new rest endpoint.\nWhen creating a new endpoint you have to extend the class\u00a0org.elasticsearch.rest.BaseRestHandler. But before we go there, we first initialise it in our plugin. To do that we implement the interface\u00a0org.elasticsearch.plugins.ActionPlugin\u00a0and implement the method\u00a0getRestHandlers.\n\npublic class BasicPlugin extends Plugin implements ActionPlugin {\r\n    private final static Logger LOGGER = LogManager.getLogger(BasicPlugin.class);\r\n    public BasicPlugin() {\r\n        super();\r\n        LOGGER.warn(\"Create the Basic Plugin and installed it into elasticsearch\");\r\n    }\r\n \r\n    @Override\r\n    public List> getRestHandlers() {\r\n        return Collections.singletonList(JettroRestAction.class);\r\n    }\r\n}\r\n\nNext is implementing the JettroRestAction class. Below the first part, the constructor and the method that handles the request. In the constructor we define the endpoint url patterns that this endpoint supports. The are clear from the code I think. Functionality wise, if you call without an action or with another action than exists, we return a message, if you ask for existence we return true or false. This handling is done in the prepareRequest method.\npublic class JettroRestAction extends BaseRestHandler {\r\n \r\n    @Inject\r\n    public JettroRestAction(Settings settings, RestController controller) {\r\n        super(settings);\r\n        controller.registerHandler(GET, \"_jettro/{action}\", this);\r\n        controller.registerHandler(GET, \"_jettro\", this);\r\n    }\r\n \r\n    @Override\r\n    protected RestChannelConsumer prepareRequest(RestRequest request, NodeClient client) throws IOException {\r\n        String action = request.param(\"action\");\r\n        if (action != null && \"exists\".equals(action)) {\r\n            return createExistsResponse(request, client);\r\n        } else {\r\n            return createMessageResponse(request);\r\n        }\r\n    }\r\n}\r\n\nWe have two utility classes that transform data into XContent: Message and Exists. The implementations of the two methods: createExistsResponse and createMessageResponse,\u00a0can be found here.\nTime to re-install the plugin, first build it with maven, remove the old one and install the new version. Now we can test it in a browser or with curl. I personally use httpie to do the following requests.\nThis way we can create our own custom endpoint. Next we dive a little bit deeper into the heart of elastic. We are going to create a custom filter that can be used in an analyser.\nCreate a custom Filter\nThe first part is registering the Filter in the BasePlugin class. We need to extend the interface\u00a0org.elasticsearch.plugins.AnalysisPlugin\u00a0and override the method\u00a0getTokenFilters. We register a factory class that instantiates the filter class. The registration is done using a name that can later on be used to use the filter. The method looks like this\n\n@Override\r\npublic Map> getTokenFilters() {\r\n    return Collections.singletonMap(\"jettro\", JettroTokenFilterFactory::new);\r\n}\nThe implementation of the factory is fairly basic\npublic class JettroTokenFilterFactory extends AbstractTokenFilterFactory {\r\n    public JettroTokenFilterFactory(IndexSettings indexSettings, \r\n                                    Environment environment, \r\n                                    String name, \r\n                                    Settings settings) {\r\n        super(indexSettings, name, settings);\r\n    }\r\n \r\n    @Override\r\n    public TokenStream create(TokenStream tokenStream) {\r\n        return new JettroOnlyTokenFilter(tokenStream);\r\n    }\r\n}\nThe filter we are going to create has a bit strange functionality. It only accepts tokens that are the same as jettro. All other tokens are removed.\npublic class JettroOnlyTokenFilter extends FilteringTokenFilter {\r\n    private final CharTermAttribute termAtt = addAttribute(CharTermAttribute.class);\r\n \r\n    public JettroOnlyTokenFilter(TokenStream in) {\r\n        super(in);\r\n    }\r\n \r\n    @Override\r\n    protected boolean accept() throws IOException {\r\n        return termAtt.toString().equals(\"jettro\");\r\n    }\r\n}\nTime to test my fresh created filter. We can do that using the analyse endpoint\ncurl -XGET 'localhost:9200/_analyze' -d '\r\n{\r\n  \"tokenizer\" : \"standard\",\r\n  \"filter\" : [\"jettro\"],\r\n  \"text\" : \"this is a test for jettro\"\r\n}'\r\n\nThe response now is\n{\"tokens\":[{\"token\":\"jettro\",\"start_offset\":19,\"end_offset\":25,\"type\":\"\",\"position\":5}]}\r\n\nConcluding\nThat is it, we have created the foundations to create a plugin, thanks to David Pilato, we have written our own\u00a0Jettro\u00a0endpoint and we have created a filter that only accepts one specific word,\u00a0Jettro. Ok, I agree the plugin in itself is not very useful, however the construction of the plugin is re-useable. Hope you like it and stay tuned for more elastic plugin blogs. We\u2019re working on an extension to the synonyms plugin and have some ideas for other plugins.\n", "tags": [], "categories": ["Blog", "Search"]}
{"post_id": 2481, "title": "Alternating Least Squares with implicit feedback \u2013 The search for alpha", "url": "https://www.luminis.eu/blog-en/search-en/alternating-least-squares-with-implicit-feedback-the-search-for-alpha/", "updated_at": "2020-11-18T14:46:07", "body": "So you want to build a recommendation engine with ALS\u2026 You search the internet for some example code in your language of choice\u2026 You copy paste the code and start tweaking\u2026 But then you realize that your data is different from all the examples you found online. You don\u2019t have explicit ratings in some range from 1 to 10; instead, you have click events where 1 means \u2018clicked\u2019. Will you still be able to use ALS? And if so, how?\nA brief recap on Collaborative Filtering and Alternating Least Squares\nCollaborative Filtering is the technique of predicting the preference of a user by collecting the preferences of many users. The underlying assumption of the collaborative filtering approach is that if a person A has the same opinion as a person B on an issue, A is more likely to have B\u2019s opinion on a different issue $i$ than to have the opinion on $i$ of a person chosen randomly (Wikipedia). A preference takes the form of a (user,\u00a0item,\u00a0rating) triple. Collecting them yields a sparse matrix $R_{u \\times i}$ of known ratings of users $u$ for items $i$. The task is to fill in the missing values. In the Latent Model approach to Collaborative Filtering, we do this by decomposing $R_{u \\times i}$ into a user matrix $X_{u \\times g}$ and an items matrix $Y_{i \\times g}$ so that we can find the \u2018hidden\u2019 features $g$ of users and items. (In the case of movies one could think of these hidden features as genres.) By taking the product of the user matrix and item matrix, we can reconstruct the (complete) ratings matrix $\\hat{R} = X \\cdot Y^T$. Or for individual ratings: $\\hat{r}_{ui} = x_{u}^T y_i$ To compute these factors, we will first randomly initialize $X$ and $Y$ and iteratively recompute them by minimizing the loss function $L$: [latex] \\sum\\limits_{u,i} (r_{ui} \u2013 x_u^T y_i)^{2} + \\lambda \\Big( \\sum\\limits_u \\|x_u\\|^{2} + \\sum\\limits_i \\|y_i\\|^{2} \\Big) [/latex] The first term in $L$ is the sum of squared errors and the second term is used for regularization. In order to minimize the loss function, we will take the derivatives with respect to $x$ and $y$ and solve for 0. [latex] \\begin{aligned} \\frac{\\partial L}{\\partial x_u} &= 0 \\\\ -2\\sum\\limits_i(r_{ui} \u2013 x_u^T y_i) y_i^T + 2 \\lambda x_u^T &= 0 \\\\ -2 (r_u^T \u2013 x_u^T Y^T)Y + 2 \\lambda x_u^T &= 0 \\\\ -2 r_u^T Y + 2 x_u^T Y^T Y + 2 \\lambda x_u^T &= 0 \\\\ x_u^T Y^T Y + \\lambda x_u^T &= r_u^T Y \\\\ x_u^T \\big( Y^T Y + \\lambda I \\big) &= r_u^T Y \\\\ \\big( Y^T Y + \\lambda I \\big) x_{u} &= Y^T r_u \\\\ x_u &= \\big( Y^T Y + \\lambda I \\big)^{-1} Y^T r_u \\end{aligned} [/latex] And for y: [latex] \\begin{aligned} \\frac{\\partial L}{\\partial y_i} &= 0 \\\\ -2\\sum\\limits_i(r_{ui} \u2013 y_i^T x_u) x_u^T + 2 \\lambda y_i^T &= 0 \\\\ y_i &= \\big( X^T X + \\lambda I \\big)^{-1} X^T r_i \\end{aligned} %y_i &= \\big( X X^T + \\lambda I \\big)^{-1} X r_i^T [/latex] Recomputing $x_{u}$ and $y_i$ can be done with Stochastic Gradient Descent, but this is a non-convex optimization problem. We can convert it into a set of quadratic problems, by keeping either $x_u$ or $y_i$ fixed while optimizing the other. In that case, we can iteratively solve $x$ and $y$ by alternating between them until the algorithm converges. This is Alternating Least Squares.\nImplicit Feedback\nUnfortunately, you don\u2019t have ratings, you have click events. And a \u2018click\u2019 event does not necessarily mean a user really likes the product; the user could be curious about the product for some other reason. Even when you are using \u2018buy\u2019 events you are not in the clear, because people buy gifts for other people all the time. Furthermore, the absence of a click event does not imply a dislike for a product $i$. So can you still use ALS? Yes, you can still use ALS, but you have to take into account the fact that you have implicit ratings/feedback. Luckily, your preferred machine learning library shows there is an \u2018implicit\u2019 switch on the ALS interface and that there is an \u2018alpha\u2019 parameter involved as well.\nSo what is this $\\alpha$ character?\nTo understand alpha, we should go to the source, which is Hu et al. 2008 (1). He suggests to split each rating into a preference and a confidence level. The preference is calculated by capping the rating to a binary value. [latex] p_{ui} = \\begin{cases} 1, & r_{ui} > 0 \\\\ 0, & r_{ui} = 0 \\end{cases} [/latex] The confidence level is defined as: [latex]c_{ui} = 1 + \\alpha r_{ui}[/latex] For a rating of 0 we would have a minimum confidence of 1 and if the rating increases, the confidence increases accordingly. The rate of increase is controlled by alpha. So alpha reflects how much we value observed events versus unobserved events. Factors are now computed by minimizing the following loss function L: [latex] \\sum\\limits_{u,i} c_{ui} (p_{ui} \u2013 x_{u}^{T}y_i)^{2} + \\lambda \\Big( \\sum\\limits_u \\|x_u\\|^{2} + \\sum\\limits_i \\|y_i\\|^{2} \\Big) [/latex] Now suppose that for a given rating $r_{ui}$ that $x_{u}^T y_i$ is very large, so that the squared residual $(p_{ui} \u2013 x_{u}^{T}y_i)^{2}$ is very large, then the rating $r_{ui}$ has a big impact on our loss function. And it should! $x_{u}^T y_i$ will be drawn towards the 0-1 range, which is a good thing, because we want to predict whether the event will occur or not (0 or 1). Alternatively, suppose that for a given rating $r_{ui}$ we have observed many events, and suppose also that our initial $x_{u}^T y_i$ value is close to 0, so that the squared residual $(p_{ui} \u2013 x_{u}^{T}y_i)^{2}$ approximates 1, then the rating $r_{ui}$ will still have quite some impact on our loss function, because our confidence $c_{ui}$ is large. Again, this is a good thing, because in this case, we want $x_{u}^T y_i$ to go towards 1. If either the confidence level is low or the residual is low, there is not much impact on the loss function, so the update of $x_u$ and $y_i$ will be small.\nConclusion\nNow that we have some background on this alpha, we can safely copy-paste the recommender engine code we found online and expand it so that it includes the alpha parameter. All that is left now is some extra parameter tuning and we are done! After that, we can run our final model on the test set and we can calculate the root mean squared error\u2026 Wait.. What..? Somehow, that metric just doesn\u2019t feel right. Oh, well\u2026 Enough for today \ud83d\ude42\nReferences\n(1)\u00a0Y. Hu, Y. Koren and C. Volinsky, \u201cCollaborative Filtering for Implicit Feedback Datasets\u201d, 2008\n", "tags": [], "categories": ["Blog", "Search"]}
{"post_id": 2514, "title": "Obtaining website analytics from google using java", "url": "https://www.luminis.eu/blog-en/development-en/obtaining-website-analytics-from-google-using-java/", "updated_at": "2020-11-16T16:30:13", "body": "A lot of the websites out there use google analytics to track what users are doing on their website. Google has a very extensive dashboard to find a lot of information about your users. For one of our customers we needed a custom dashboard with statistics coming from different systems. From google analytics we needed information about the number of page views, sessions and visitors. In this blog post I am going to describe the steps to take to obtain these statistics from google analytics API using the java client.\nThere are three steps that you need to do before you have the data:\n\nConfigure google analytics to allow the java client to connect.\nCreate a java project (we use spring boot) and configure it to connect.\nCreate the query and obtain the statistics from the results.\n\nConfigure Google analytics\nOf course you first have to have a site that uses google analytics, I use my personal blog that is sleeping. It still attracts some visitors, so let us use this. If you have such a website, go to the developers console.\nhttps://console.developers.google.com\nCreate a new project (use drop down top left screen) and after that click the library button in the menu on the left.\nPush the enable button. A notification should be visible to add credentials to connect to the API. Use this button or the button in the menu on the left to get to the credentials screen.\nCreate credentials for a service account key, the next screen should be visible. Add the project viewer role.\nFinally chose the p12 key. Json is recommended, but for some reason I could not find a way to make it work with the json key. When everything is ready copy the email address, you will need it in the next steps. Format of the email address that I mean is this: analytics@website-statistics-144211.iam.gserviceaccount.com.\nWhen creating the credential a p12 file is downloaded to your machine.\nNow we are almost done. We now have credentials to connect to the google analytics API, but we have to explicitly give the email address access in the google analytics maintenance screens. So had over to that website.\nhttps://analytics.google.com/analytics/web\nFirst choose the project you want obtain the analytics for and note the web property id (format of UA-123456-1). In the screen go to the maintenance tab and push user management.\nThe result should be the following image in which we have entered the email address as created in the previous steps.\nThat is it, now we can move on to creating the java application.\nCreate the java project and configure to connect\nFirst we create a new spring-boot project, but if you prefer a plain java project that is fine as well. I would at least use something like gradle or maven. I still prefer maven. I used the spring boot started project without additional libraries and I manually added the google libraries. The following two libraries work for me:\n\r\n\r\n    com.google.api-client\r\n    google-api-client\r\n    1.22.0\r\n\r\n\r\n\r\n    com.google.apis\r\n    google-api-services-analytics\r\n    v3-rev134-1.22.0\r\n\nNow we need to create an instance of the class GoogleCredential, the following code block uses a the builder to create the instance.\nGoogleCredential credential = new GoogleCredential.Builder()\r\n        .setTransport(httpTransport)\r\n        .setJsonFactory(jsonFactory)\r\n        .setServiceAccountId(SERVICE_ACCOUNT_EMAIL)\r\n        .setServiceAccountScopes(Collections.singleton(AnalyticsScopes.ANALYTICS))\r\n        .setServiceAccountPrivateKeyFromP12File(new File(P12)) // notasecret\r\n        .build();\nThe jsonFactory and httpTransport are created by the following lines:\nJsonFactory jsonFactory = JacksonFactory.getDefaultInstance();\r\nHttpTransport httpTransport = GoogleNetHttpTransport.newTrustedTransport();\r\n\nThe SERVICE_ACCOUNT_EMAIL is the one we created when creating new credentials and the one we added to the google analytics users. P12 is a constant representing the name of the file with the p12 key in there. The one you should not share with others.\nNext step is to use the created credential and connect to the analytics API. To do that we create the Analytics object, again the with builder. This time we provide it with an application name, which does not seem to matter what value you give it.\n\nAnalytics analytics = new Analytics.Builder(httpTransport, jsonFactory, credential)\r\n         .setApplicationName(APPLICATION_NAME)\r\n         .build();\nNow that we can connect, we need to be more specific what metrics we want for what project. I have multiple websites running with the same account. Therefore I have multiple web property id\u2019s. I already showed you how to obtain the web property id in the format UA-123456-1, the account id in this case would be 123456. Using the Analytics object and these two parameters we can obtain a list of profiles. The following code block obtain the profiles and prints them to the screen.\nProfiles profiles = analytics\r\n        .management()\r\n        .profiles()\r\n        .list(ACCOUNT_ID, WEB_PROPERTY_ID).execute();\r\nprofiles.getItems().forEach(profile ->\r\n        System.out.println(profile.getId() + \" \" + profile.getName()));\r\n\nFinally we can use this profile id together with the analytics object to print analytics data for a certain period.\nSystem.out.println(\"July\");\r\nprintMonth(analytics, \"2016-07-01\", \"2016-07-31\");\r\nprivate void printMonth(Analytics analytics, String startDate, String endDate) throws IOException {\r\n    GaData period = analytics.data().ga()\r\n            .get(PROFILE_ID, startDate, endDate, \"ga:pageviews,ga:sessions,ga:visitors\")\r\n            .execute();\r\n    period.getTotalsForAllResults().entrySet().forEach(entry -> {\r\n        System.out.println(entry.getKey() + \" \" + entry.getValue());\r\n    });\r\n}\nThe output for my site would than be:\nJuly\r\nga:pageviews 7138\r\nga:sessions 6377\r\nga:visitors 5494\r\nAugust\r\nga:pageviews 7550\r\nga:sessions 6590\r\nga:visitors 5665\r\nSeptember\r\nga:pageviews 5009\r\nga:sessions 4332\r\nga:visitors 3774\nHope that this can help others to connect to google as well. There is some documentation out there, but it is hard to find and often outdated. Even the samples that come with the library did not work out of the box for me.\n", "tags": [], "categories": ["Blog", "Development"]}
{"post_id": 1411, "title": "Updatable synonyms in Elasticsearch @Bol.com", "url": "https://www.luminis.eu/blog-en/search-en/updatable-synonyms-in-elasticsearch-bol-com/", "updated_at": "2020-11-18T08:36:53", "body": "With around 6.2 million active customers, Bol.com is the number one online retailer in The Netherlands. One of the most important components of an E-commerce platform is having a good search engine. Luminis Amsterdam is helping Bol.com implement Elasticsearch for search capabilities.\nSynonyms in Elasticsearch\nA common feature of search engines is the ability to search using synonyms. In the UK people use the word\u00a0bag, whereas people in the US use the word\u00a0purse\u00a0for the same thing. What if our documents are UK-oriented and don\u2019t contain the word\u00a0purse? We would still like to help our US customers, by adding a synonym that either replaces the word\u00a0purse\u00a0with\u00a0bag\u00a0or adds\u00a0bag\u00a0to the query as an extra search term. Whether you want to use\u00a0synonym expansion\u00a0or\u00a0synonym contraction\u00a0depends on your\u00a0use-case and won\u2019t be part of this post.\nConfiguring synonyms in Elasticsearch can be done in two ways. Either by adding your synonyms directly to a\u00a0SynonymTokenFilter\u00a0like so:\n\nPUT luminis-synonyms\r\n{\r\n  \"settings\": {\r\n    \"index\": {\r\n      \"analysis\": {\r\n        \"analyzer\": {\r\n          \"synonym\": {\r\n            \"tokenizer\": \"whitespace\",\r\n            \"filter\": [\r\n              \"synonym\"\r\n            ]\r\n          }\r\n        },\r\n        \"filter\": {\r\n          \"synonym\": {\r\n            \"type\": \"synonym\",\r\n            \"synonyms\": [\r\n              \"bag, purse, handbag\"\r\n            ]\r\n          }\r\n        }\r\n      }\r\n    }\r\n  }\r\n}\nOr by placing a file on your server and referencing that file with the\u00a0synonym_path\u00a0property:\n\nPUT luminis-synonyms\r\n{\r\n  \"settings\": {\r\n    \"index\": {\r\n      \"analysis\": {\r\n        \"analyzer\": {\r\n          \"synonym\": {\r\n            \"tokenizer\": \"whitespace\",\r\n            \"filter\": [\r\n              \"synonym\"\r\n            ]\r\n          }\r\n        },\r\n        \"filter\": {\r\n          \"synonym\": {\r\n            \"type\": \"synonym\",\r\n            \"synonyms_path\": \"analysis/synonym.txt\"\r\n          }\r\n        }\r\n      }\r\n    }\r\n  }\r\n}\r\n\nPretty easy to setup and it works within minutes. Now there is one caveat here: What if we want to update the synonyms\u00a0after the analyzer has been created? Bol.com wants to have full control of their synonyms and the ability to update them at any given time. The official documentation says the following about\u00a0updating synonyms:\nIf you specify synonyms inline with the\u00a0synonyms\u00a0parameter, your only option is to close the index and update the analyzer configuration with the\u00a0update index settings API, then reopen the index.\nUpdating synonyms is easier if you specify them in a file with the\u00a0synonyms_path\u00a0parameter.\u00a0You can just update the file (on every node in the cluster) and then force the analyzers to be re-created by either of these actions:\n\n\nClosing and reopening the index (see\u00a0open/close index), or\nRestarting each node in the cluster, one by one\n\nOf course, updating the synonym list will not change any documents that have already been indexed. It will apply only to searches and to new or updated documents. To apply the changes to existing documents, you will need to reindex your data.\n\nWhat if we want to add more synonyms on our production environment? It\u2019s not possible for us to use index-time synonyms\u00a0since that requires reindexing the data, but if we would just use query-time synonyms we would have to restart our production nodes or close/open the index. Not very feasible for a production environment.\nElasticsearch has\u00a0support for creating your own plugins and effectively extending the functionality of the engine. In order to meet Bol.com\u2019s requirement to update synonyms on the fly, we created a plugin that does just that. Since we\u2019re using query-time synonyms, our plugin extends the functionality of the SynonymTokenFilter by having a updatable\u00a0synonymMap. We added\u00a0custom REST handlers that will process new synonyms after they\u2019ve been uploaded to Elasticsearch. Now people responsible for maintaining\u00a0synonyms at bol.com can add/remove synonyms on the fly, whereas with their current Endeca setup it takes one day for new synonyms are processed.\nHow the plugin works\nAs mentioned above\u00a0I created a new _synonym endpoint in Elasticsearch which allows the following functionality:\n\nGET: Retrieve the current list of synonyms\nPOST/PUT: Upload a new synonym file containing the new updated synonym file\nDELETE: Delete all known synonyms\n\nSince we\u2019re working in a distributed environment, we have to let other nodes in the cluster know that a\u00a0new file has been uploaded.\u00a0As soon as the node receiving the file has processed the new synonyms, it will store them inside an index in Elasticsearch. After indexing the new synonyms the plugin will broadcast to the other nodes in the cluster that new synonyms are available, which in turn leads to the other nodes applying the new file and confirming back to the original node if everything went ok. Once all nodes processed the new synonyms, we report back to the user that applying the new synonyms was successful. If an error happens during processing, we report back to the user that the operation was unsuccessful. At the time of writing, there is no automatic failover/rollback mechanism (yet), which means that we have to manually check what happened and re-apply the file.\nSince synonyms are being kept in memory, we need a way for new nodes joining or restarting nodes to load in the synonyms during startup. We created a service that will check for an existing synonyms index, and load these into memory. This way we make sure that all nodes in the cluster have the same version of the synonyms file in memory.\nThis is just one of the cool new things we\u2019re doing at Bol.com\n", "tags": [], "categories": ["Blog", "Search"]}
{"post_id": 1725, "title": "Faceted search with Elasticsearch", "url": "https://www.luminis.eu/blog-en/search-en/faceted-search-with-elasticsearch/", "updated_at": "2020-11-11T14:37:50", "body": "In this blog, I will be presenting two strategies for implementing faceted search with Elasticsearch. Few days back I had a discussion with my colleague Byron about implementing faceted search when Elasticsearch is being used to serve the search results. And this blog is\u00a0a culmination of our discussion. In Elasticsearch, the aggregation framework provides support for facets and executing metrics and scripts over those facet results. Following is a simple example wherein each Elastic documents contains a field color and we execute a Term Aggregation on the document set\n\n{\r\n  \"aggs\" : {\r\n    \"colors\" : {\r\n      \"terms\" : { \"field\" : \"color\" }\r\n      }\r\n    }\r\n}\nAnd we get the following\n\"buckets\": [\r\n    {\r\n        \"key\": \"red\",\r\n        \"doc_count\": 2\r\n     },\r\n     {\r\n        \"key\": \"green\",\r\n        \"doc_count\": 2\r\n     },\r\n     {\r\n        \"key\": \"blue\",\r\n        \"doc_count\": 1\r\n     },\r\n     {\r\n        \"key\": \"yellow\",\r\n        \"doc_count\": 1\r\n     }\nSo we get each of the unique color as a bucket key and doc_count is the number of documents have corresponding color field value.\nOne of the options is to keep going on with sub-aggregations which\u00a0is\u00a0also called Path hierarchy sub-aggregation but that is something which is very expensive and also not feasible beyond a certain hierarchy level, as discussed\u00a0here.\nFirst approach\nThe first approach for faceted search is when we have a unique field corresponding to a hierarchy level present in each document. For example, if we have documents pertaining to products for online webshop\u00a0and 3 levels of hierarchy then\u00a0a product document would look something like \u2013\n\n     {\r\n    ...\r\n \r\n\"categoryOneLevel\": [\r\n       \"8299\"\r\n     ],\r\n     \"categoryTwoLevel\": [\r\n       \"8299-3131\"\r\n     ],\r\n     \"categoryThreeLevel\": [\r\n       \"8299-3131-2703\",\r\n       \"8299-3131-2900\"\r\n     ]\r\n}\nOn the UI we can visualize it with something like \u2013 Computer (8299) (level 1) > laptop(3131) (level 2) > (Linux (2703) and Mac(2900)) (level 3) > further on. The reason for storing the other levels in each deeper located category is because a product can exist in multiple categories. If we take this approach then sample queries would be something like \u2013\nWhen user is on landing page (no facetting yet)\nGET document/_search\r\n  {\r\n  \"aggs\": {\r\n   \"categories\": {\r\n     \"terms\": {\r\n       \"field\": \"categoryOneLevel\",\r\n      }\r\n   }\r\n }\r\n }\nNow when the user clicks, the next query fired would be\nGET document/_search?size=0\r\n{\r\n \"query\": {\r\n   \"bool\": {\r\n     \"filter\": {\r\n       \"term\": {\r\n         \"categoryOneLevel\": \"8299\"\r\n       }\r\n     }\r\n   }\r\n },\r\n \"aggs\": {\r\n   \"categories\": {\r\n     \"terms\": {\r\n       \"field\": \"categoryTwoLevel\",\r\n       \"include\": \"8299-.*\"\r\n     }\r\n   }\r\n }\r\n\nHere the filter query selects all the documents of categoryOne and then does the aggregations on all categoryTwo values which are directly linked with categoryOne as we choose the\u00a0include : 8299-*\u00a0filter Thus you can extend this approach further that if the user clicks on categoryTwo then results from categoryThree are returned. In this\u00a0way\u00a0we can have\u00a0faceted\u00a0search\u00a0results from\u00a0ElasticSearch.\nSecond Approach\nNow, let\u2019s look at the second approach which somewhat comes out of the box. Using the built-in\u00a0Path\u00a0tokenizer.\u00a0Here a single field holds the hierarchy values and while indexing we specify the \u201cpath\u00a0tokenizer\u201d mapping setting for that field.\u00a0 For example, a single field would hold the comma separated value and path\u00a0tokenizer\u00a0would produce the token values as \u2013\nComputer,Laptop,Mac\nAnd produces tokens:\nComputer\nComputer,Laptop\nComputer,Laptop,Mac\nIn this approach, I am taking the actual user-readable values of categories instead of Ids, it\u2019s only for example sake, as taking Ids gives you the flexibility to update the category names like Computer, Laptop etc associated with that Id.\nLet\u2019s look at some elastic queries \u2013\n\nPUT blog_index/\r\n{\r\n  \"settings\": {\r\n    \"analysis\": {\r\n      \"analyzer\": {\r\n      \"path-analyzer\": {\r\n         \"type\": \"custom\",\r\n         \"tokenizer\": \"path-tokenizer\"\r\n        }\r\n     },\r\n     \"tokenizer\": {\r\n      \"path-tokenizer\": {\r\n         \"type\": \"path_hierarchy\",\r\n         \"delimiter\": \",\"\r\n         }\r\n      }\r\n   }\r\n },\r\n \"mappings\": {\r\n     \"my_type\": {\r\n       \"dynamic\": \"strict\",\r\n       \"properties\": {\r\n         \"hierarchy_path\": {\r\n             \"type\": \"string\",\r\n             \"analyzer\": \"path-analyzer\",\r\n             \"search_analyzer\": \"keyword\"\r\n             }\r\n          }\r\n       }\r\n     }\r\n   }\nOur index is ready with field \u201chierarchy_path\u201d having the path tokenzier set as the analyzer thus now the terms of this field will be tokenized based on the path_tokenizer.\nNow lets, add a document to the index\nPOST blog_index/my_type/1\r\n{\r\n\"hierarchy_path\": [\"Computer,Laptop,Mac\",\"Home,Kitchen,Cookware\"]\r\n}\nWe have added a document with field hierarchy_path having example of two set of categories and each set having comma separated values.\nIf we have execute a terms aggregation on field \u201chierarchy_path\u201d we get\nGET blog_index/my_type/_search?search_type=count\r\n{\r\n   \"aggs\": {\r\n     \"category\": {\r\n        \"terms\": {\r\n         \"field\": \"hierarchy_path\",\r\n          \"size\": 0\r\n          }\r\n       }\r\n     }\r\n     }\r\n\nWe get the following buckets\n\"buckets\": [\r\n {\r\n \"key\": \"Computer\",\r\n \"doc_count\": 1\r\n },\r\n {\r\n \"key\": \"Computer,Laptop\",\r\n \"doc_count\": 1\r\n },\r\n {\r\n \"key\": \"Computer,Laptop,Mac\",\r\n \"doc_count\": 1\r\n },\r\n {\r\n \"key\": \"Home\",\r\n \"doc_count\": 1\r\n },\r\n {\r\n \"key\": \"Home,Kitchen\",\r\n \"doc_count\": 1\r\n },\r\n {\r\n \"key\": \"Home,Kitchen,Cookware\",\r\n \"doc_count\": 1\r\n }\r\n ]\nFrom the above results, we can see that the path_tokenizer has split the comma separated values of the field \u201chierarchy_path\u201d.\nSo, now based on the user activity we can fire queries to select the documents pertaining to the category which user is looking for. The query for selecting the top level category would be\nGET blog_index/my_type/_search?search_type=count\r\n{\r\n   \"aggs\": {\r\n     \"category\": {\r\n       \"terms\": {\r\n         \"field\": \"hierarchy_path\",\r\n         \"size\": 0,\r\n         \"exclude\": \".*\\\\,.*\"\r\n        }\r\n       }\r\n       }\r\n     }\nand we get\n\"buckets\": [\r\n  {\r\n    \"key\": \"Computer\",\r\n    \"doc_count\": 1\r\n   },\r\n {\r\n   \"key\": \"Home\",\r\n   \"doc_count\": 1\r\n  }\r\n]\nWe have used the regular expression exclude\u201d: \u201c.*\\\\,.* which excludes all the sub-levels thus we get only the top hierarchy.\nIf user wants only second level then the query fired would be\nGET blog_index/my_type/_search?search_type=count \r\n{\r\n \"query\": {\r\n \"bool\" : {\r\n \"filter\": {\r\n \"prefix\" : { \"hierarchy_path\" : \"Computer\" }\r\n }\r\n }\r\n },\r\n \"aggs\": {\r\n \"category\": {\r\n \"terms\": {\r\n \"field\": \"hierarchy_path\",\r\n \"size\": 0,\r\n \"include\" : \"Computer\\\\,.*\",\r\n \"exclude\": \".*\\\\,.*\\\\,.*\"\r\n }\r\n }\r\n }\r\n}\nWherein we specify the regex for include which would mean all documents which are part of Computer hierarchy but we exclude the third level of hierarchy thus result only contains second level of hierarchy.\n\"buckets\": [\r\n {\r\n \"key\": \"Computer,Laptop\",\r\n \"doc_count\": 1\r\n }\r\n ]\nWhen the user activity requires third level of hierarchy then the query fired would be\nGET blog_index/my_type/_search?search_type=count\r\n{\r\n \"query\": {\r\n \"bool\" : {\r\n \"filter\": {\r\n \"prefix\" : { \"hierarchy_path\" : \"Computer\" }\r\n }\r\n }\r\n },\r\n \"aggs\": {\r\n \"category\": {\r\n   \"terms\": {\r\n     \"field\": \"hierarchy_path\",\r\n      \"size\": 0,\r\n     \"include\" : \"Computer\\\\,.*\\\\,.*\"\r\n    }\r\n   }\r\n  }\r\n }\r\n\nBased on the include regex \u201cComputer\\\\,.*\\\\,.*\u201d   we wil get only the documents have the third level of hierarchy as well\n\"buckets\": [\r\n{\r\n\"key\": \"Computer,Laptop,Mac\",\r\n\"doc_count\": 1\r\n}\r\n]\nIn this way based on the user activity of our application we can fetch corresponding results from Elastic and while indexing our documents we need to make sure the product documents have relevant value in the \u201chierarchy_path\u201d field based on the hierarchy level which that product would be present in.\n", "tags": [], "categories": ["Blog", "Search"]}
{"post_id": 1554, "title": "Object mapping magic with MapStruct", "url": "https://www.luminis.eu/blog-en/development-en/object-mapping-magic-with-mapstruct/", "updated_at": "2020-11-16T16:29:39", "body": "In our projects we often have several Java objects that needs to be mapped\u00a0to other objects, depending on it\u2019s purpose. One of the most common cases is a domain item that needs a representation on the frontend. Another case is that we want to store data in elasticsearch, but don\u2019t want to use the domain item there as well.\nMapping one object to another is a boring task and could easily lead to mistakes.\u00a0MapStruct\u00a0can generate bean mappings at compile-time based on annotations.\u00a0The generated mapping code uses plain method invocations and thus is fast, type-safe and easy to understand.\nHow does it work?\nDefining a mapping is really simple. First you need to create an interface with the\u00a0@Mapper\u00a0annotation. Then add a method that expects the source as parameter and the target as return type.\n\n@Mapper\r\npublic interface RelationMapper {\r\n \r\n    RelationListDTO relationToListDto(Relation relation);\r\n \r\n}\nIf your object contains another object you will need to define a mapping for that object as well. So if for example our\u00a0Relation\u00a0object contains an\u00a0Address\u00a0object and the\u00a0RelationListDto\u00a0contains a AddressListDto, the mapper has no idea how to map the properties of\u00a0Address\u00a0to our\u00a0AddressListDto.\n\npublic class Relation {\r\n \r\n    private String name;\r\n    private String email;\r\n    private Address address;\r\n \r\n    // getters and setters\r\n}\r\n \r\npublic class RelationListDTO {\r\n    private String name;\r\n    private String email;\r\n    private AddressListDto address;\r\n \r\n    // getters and setters\r\n}\nTherefor you need also need to create a mapper for\u00a0Address\u00a0and use that mapper in the RelationMapper class.\n\n@Mapper(uses = {AddressMapper.class})\r\npublic interface RelationMapper {...}\r\n\nInstead of defining a separate mapper for Address you can also specify how to map specific properties. Let us change the RelationListDto a little bit so that we only have a few properties instead of an Address object.\npublic class RelationListDTO {\r\n    private String name;\r\n    private String email;\r\n \r\n    private String street;\r\n    private String city;\r\n \r\n    // getters and setters\r\n}\nNow we can specify how to map these properties with the @Mapping annotation:\n@Mapper\r\npublic interface RelationMapper {\r\n \r\n    @Mapping(source = \"address.street\", target = \"street\")\r\n    @Mapping(source = \"address.city\", target = \"city\")\r\n    RelationListDTO relationToListDto(Relation relation);\r\n}\nDependency injection\nIf you use a dependency injection framework you can access the mapper objects via dependency injection as well. Currently they are supporting CDI and Spring Framework. \u00a0All you have to do is specify which framework you are using in the @Mapper annotation:\n\n@Mapper(componentModel = \"spring\")\r\npublic interface RelationMapper {} \nWarnings\nWhen you have a mismatch between properties in your objects, MapStruct will generate a warning that will tell you that you have unmapped properties. This can be a completely valid scenario as you maybe not want to map all properties. If this is the case you can add a mapper annotation with the property name and the ignore attribute. This way MapStruct wil skip the property from being mapped.\n\n@Mapping(target = \"somePropertyName\", ignore = true)\nAnd more..\nThese examples are just the simple use cases with MapStruct, but there is so much more. Things like implicit type conversions or referencing another mapper in a mapper. You can also customise your mapper with the\u00a0@BeforeMapping\u00a0and\u00a0@AfterMapping\u00a0which allows you to manipulate the objects before and after the mapper starts.\nSo\u00a0it\u2019s not really magic, but I like MapStruct. Just give it a try and maybe it will also help you in your project.\n", "tags": [], "categories": ["Blog", "Development"]}
{"post_id": 2516, "title": "JumpStarting Stream Processing with Kafka and Flink", "url": "https://www.luminis.eu/blog-en/development-en/jumpstarting-stream-processing-with-kafka-and-flink/", "updated_at": "2020-11-13T09:13:36", "body": "In last few months, I have come across the topic of \u201cStream Processing\u201d so much that it had become hard to ignore and definitely seemed like a new wave of Big Data technologies. We are living in the world of innovation and rapid changes thus I was cautious about investing my time in every new hype.\nAfter going through few articles, attending meetups and listening to some nice presentations from conferences, I am convinced that Stream processing is here to stay and being rapidly adopted by the industry. In this blog, I am presenting a summary of my understanding of the de-facto tools used in stream processing.\nBig Data Infrastructure since the advent of \u00a0Hadoop stack hadn\u2019t undergone a major change until very recently with new technology trends like Microservices, Internet of Things & Data localization in Apps has led to infrastructure requirements that should support processing and\u00a0aggregation of real-time events\u00a0and various consumer processes with the capability to handle those events at a high throughput . In last\u00a0few years, there were mainly 2 patterns of programming\n\nRequest/Response\nBatch\n\nRequest response is the traditional method \u00a0wherein the server gets a single request from the customer and the customer gets a response which might or might not be trivial depending on the backend of the application. \u00a0Then, came the need for processing of log files using map/reduce and coming up with intelligent analytics which gave rise to Hadoop stack based batch processing wherein the Hadoop jobs ran for hours or sometimes overnight to provide insights into log data and user activity.\nIn last, couple of years, there arose a need to develop frameworks which can perform real-time analytics on events coming from various devices or microservices or click stream from a user group (based on geography etc.) . This led to the development of in-house tools and frameworks by companies such as LinkedIn, Twitter, and Google for stream analytics. But, these tools were less mature as compared to their Batch/Hadoop counterparts. Also, there wasn\u2019t much industry consensus around tools and technologies for stream processing. Thus, the situation was something like this\n\nFig 1 . From the talk \u201cApache Kafka and the Next 700 Stream Processing Systems\u201d by Jay Kreps [1]\nHence, for the developer community, it was hard to get their hands on\u00a0mature tools which could be used for stream processing. Then few years ago, Linkedin open-sourced Apache Kafka, which is a high throughput messaging system which led to rapid innovation in this field. Kafka is now becoming the backbone of streaming pipeline and being adopted rapidly [2]. \u00a0Let\u2019s look at few of the key concept of Apache Kafka.\nApache Kafka\nKafka is a messaging system built for scalability and high throughput, sounds like JMS ? Kafka comes with a few big advantages over traditional JMS. I would choose Kafka over traditional JMS queueing solution if I have to persist my messages for longer periods of times (upto days). Another reason is that, if I have to support multiple consumers that would be reading the messaging at their own pace as different Datastores have different write speeds depending upon their core functionality. Consumers, can also \u201creplay\u201d messages if they want and if they are down then they can start from the point where they left off once they are up again. Thus, these features make Kafka highly usable in an environment where heterogeneous frameworks, libraries are involved as each can consume messages based on a polling model at a\u00a0throughput which they can handle which isn\u2019t possible with traditional messaging systems. \u00a0Let\u2019s have a closer look at why Kafka scales so well \u2013\n\nFig 2 \u2013 A Topic along with it\u2019s subset partitions\nIn Kafka, a\u00a0Topic\u00a0is an important abstraction which represents logical stream of data, this stream of data achieves parallelism when it\u2019s partitioned in parallel streams which are called partitions. The messages/events in these partitions are identified by a unique Id or message offset which\u00a0represents increasing timestamp within that partition. This image from the official docs [3] further elaborates on this \u00a0\u2013\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0\n\n Fig 3 \u2013 From the official Kafka docs[3], Anatomy of a Topic\nThus, we can see that messages are written in the sequential order and the consumer also consumes the messages sequentially.\nLet\u2019s look at the overall architecture of Kafka.\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \n\nFig 4 \u2013 Internal Architecture of Kafka\nBrokers are part of the Kafka cluster, a topic is divided into multiple partitions in order to balance load and also support parallelism. \u00a0Each consumer takes part in a consumer group, when we start a new consumer we specify a group label and based on that label grouping takes place. Each message of a topic gets delivered atleast once to a consumer instance of each subscribing group. You can see from the above image that partition 0 is sending the message of consumer 2 only, as both consumer 1 and 2 are part of the same consumer group hence the only one consumer instance of a subscribing group gets the message. If each consumer is in a different group then each message is broadcasted to all groups. Similarly, Consumer group 2 only has one consumer which gets messages from all 3 partitions. The total number of consumers in a group should never be more than total partitions of a subscribed topic. We can have a new 3rd Consumer in 1st group as there are 3 partitions but if we have a 4rth Consumer then it would be idle as there would be more Consumers in a group then number of partitions.\nBrokers in Kafka are\u00a0stateless\u00a0which basically means that the broker doesn\u2019t keep a record of how much messages a consumer has consumed, the message offset is for the consumer to keep track of not the broker. But this also makes it hard to delete redundant messages thus Kafka solves this problem by using a time-based SLA for the retention policy. Since the messages can stay for a while in Kafka thus the consumer can also rewind to an earlier offset or if a consumer instance crashes then that \u00a0partition from which it was reading from is assigned to another instance of the group. This distributed coordination\u00a0is handled by\u00a0Zookeeper, it\u2019s responsibilities include keeping track of newly added brokers and consumers, rebalancing partitions- consumer group mapping when new consumers are added or removed and keeping track of the offset consumed in each partition. \u00a0Zookeeper maintains two registries ownership registry and an offset registry. The\u00a0owner registry maintains the partition \u2013 consumer group mapping such that each consumer group has it\u2019s own corresponding ownership registry and the offset registry contains the last consumed message offset of each partition. \u00a0Thus, with Kafka, it becomes easy to integrate with various frameworks consuming at the throughput which they can handle and is required by the application. Here\u2019s how things would look once you have Kafka integrated with other tools.\n\nFig 5 \u2013 From the talk \u201cDemystifying Stream Processing with Apache Kafka\u201d- by Neha Narkhede [4]\nSo far we have discussed Kafka which is the backbone of your streaming infrastructure, now let\u2019s look at Apache Flink which is a\u00a0stream processing framework and includes multiple processing libraries to enrich incoming events and messages.\nApache Flink\nI heard about Flink very recently and have been really impressed by the traction it\u2019s gaining in the developer community. It\u2019s also one of the most active Apache Big Data projects and Flink meetups are coming up in all major tech cities of the world. Since I have been reading about Stream processing I realized that there are few key features which are important for any good stream processor to support. I was impressed to see that Flink is pretty new yet it supports all these\u00a0key features,\u00a0namely \u2013\n\nProcessing engine with support for Streaming as well as Batch\nSupporting various windowing paradigms\nSupport for Stateful Streaming\nFaul Tolerant and high throughput\nComplex Event Processing (CEP)\nBackpressure handling\nEasy Integration with existing Hadoop stack\nLibraries for doing Machine Learning and graph processing.\n\nThe existing Hadoop stack which is good at batch processing already has\u00a0so many moving parts\u00a0that trying to configure it for stream processing is a difficult task, since various components like Oozi (job scheduler), HDFS (and flume for data loading), ML and graph libraries, & Batch processing jobs all have to work in perfect harmony. On top of that Hadoop has poor Stream support and no easy way to handle backpressure spikes. This makes\u00a0Hadoop stack in streaming data processing even harder to use. Let\u2019s take a look high-level view of Flink\u2019s architecture\n\nFig 6 \u2013 Flink\u2019s architecture from the official docs [5]\nFor every submitted program a client is created which does the required pre-processing and turns the program into a parallel dataflow form which is then executed by the\u00a0TaskManagers and the JobManager\u00a0. JobManager is the primary coordinator for the whole execution cycle and is responsible for allotting tasks to TaskManager and also for resource management.\nInteresting, thing about flink is that it contains so much functionality within it\u2019s own framework that the number of moving parts in the streaming architecture goes down. Here are the \u00a0internal Flink components \u2013\n\nFig 7- \u00a0From the talk \u201cApache Flink: What, How, Why, Who, Where? \u201d by Slim Baltagi [6]\nFlink engine\u2019s which is a Distributed Streaming dataflow engine support\u00a0both Streaming and Batch processing, along with the ability to support and use existing storage and deployment infrastructure, it supports multiple of domain specific libraries like\u00a0FLinkML for machine learning, Gelly for graph analysis, Table for SQL, and FlinkCEP for complex event processing. Another interesting aspect of Flink is that existing big data jobs (Hadoop M/R, Cascading, Storm) can be\u00a0executed on the Flink\u2019s engine by means of an adapter\u00a0thus this kind of flexibility is something which makes Flink\u00a0center of the Streaming infrastructure processing.\nAs discussed above in the key feature list, two important aspects of Streaming supported by Flink are Windowing and Stateful streaming. Windowing is basically the technique of executing aggregates over streams. Windows can be broadly classified into\n\nTumbling windows (no overlap)\nSliding windows (with overlap)\n\nThe above two concepts can be explained by the following 2 images\n\nFig 8 \u2013 From the talk \u201cUnified Stream and Batch Processing with Apache Flink\u201d by Ufuk Celebi [7]\n\nIn references, I have provided link to Flink APIs that support stream aggregations i.e. windowing.\nStream processing which supports basic filtering\u00a0or simple transformation don\u2019t need state but when it comes to more advanced concepts like aggregation on streams (windowing), complex transformation, complex event processing then it becomes necessary to support\u00a0stateful streaming.\nIn the recent release of Flink, they have introduced a concept called\u00a0Savepoints.\u00a0The Flink task managers regularly create checkpoints of the job\u2019s state being processed\u00a0and under the hood Savepoints are basically pointers to any of the checkpoints, these Savepoints can be manually triggered and they never expire until discarded by the user. Let\u2019s look at an image for a more clearer understanding\n\nFig 9 \u2013 From the official docs -Savepoints [8]\nHere the checkpoints C1 and C3 have been discarded as the checkpoint C4 is the latest checkpoint and all the earlier checkpoints except C2 have been discarded. The reason C2 is still there is because a Savepoint was created when C2 was the latest checkpoint and now that Savepoint has a pointer to the C2. Initially, the job\u2019s state is stored in-memory and then checkpointed into a filesytem (like HDFS etc) and savepoint is basically a url to the HDFS location of the checkpointed state. In order to store a much larger state, Flink team is working towards providing a state backend based on RocksDB.\nHere is an overview of a Streaming architecture using Kafka and Flink\n\nFig 10 \u2013\u00a0From the talk \u201cAdvanced Streaming Analytics with Apache Flink and Apache Kafka\u201d by Stephan Ewen [9]\nSo far, we have discussed both Flink and Kafka before concluding let\u2019s just go through the Yahoo Benchmark for stream processors [10]\n\nFig 11 \u2013\u00a0From the talk \u201cUnified Stream and Batch Processing with Apache Flink\u201d by Ufuk Celebi [7]\nThe Architecture consisted of Kafka clusters feeding the stream processors and the results of stream transformation were published in Redis and via Redis available to applications outside the architecture.\u00a0As you can see that even at high throughput Storm and Flink maintained low latency. \u00a0This benchmark was further extended by Data Artisans [11], the company behind Flink, they took Yahoo\u2019s benchmark as a starting point and upgraded the Flink\u2019s cluster\u2019s node interconnect to 10GigE from 1 GigE which was used by Yahoo. The results were very interesting as Flink not only outperformed storm but also saturated the Kafka link\u00a0at around 3 million events/sec.\nConclusion\nStream processing is at an initial yet very interesting phase, and I hope after reading this blog you would give Kafka and Flink a try on your machine.\u00a0Feel free to share your feedback/comments\nReferences\n[1] \u2013\u00a0https://www.youtube.com/watch?v=9RMOc0SwRro\n[2] \u2013\u00a0http://www.confluent.io/confluent-unveils-next-generation-of-apache-kafka-as-enterprise-adoption-soars\n[3] \u2013\u00a0http://kafka.apache.org/documentation.html\n[4] \u2013\u00a0http://www.infoq.com/presentations/stream-processing-kafka\n[5] \u2013\u00a0https://ci.apache.org/projects/flink/flink-docs-release-0.7/internal_general_arch.html\n[6] \u2013\u00a0http://www.slideshare.net/sbaltagi/apacheflinkwhathowwhywhowherebyslimbaltagi-57825047\n[7] \u2013\u00a0https://www.youtube.com/watch?v=8Uh3ycG3Wew\n[8] \u2013\u00a0https://ci.apache.org/projects/flink/flink-docs-master/apis/streaming/savepoints.html \u00a0\n[9]- \u00a0http://kafka-summit.org/sessions/advanced-streaming-analytics-with-apache-flink-and-apache-kafka/\n[10] \u2013\u00a0https://yahooeng.tumblr.com/post/135321837876/benchmarking-streaming-computation-engines-at\n[11] \u2013\u00a0http://data-artisans.com/extending-the-yahoo-streaming-benchmark/\n", "tags": [], "categories": ["Blog", "Development"]}
{"post_id": 1766, "title": "Using the elasticsearch Profile API", "url": "https://www.luminis.eu/blog-en/search-en/using-the-elasticsearch-profile-api/", "updated_at": "2020-11-18T08:41:23", "body": "Some time a go I found an issue for elasticsearch explaining a Profile API that they were working on. The recently released 2.2 version of elasticsearch contains the first experimental version of the API. The API is still experimental, meaning that it could change or be removed completely in the future.\nHaving given you the warning let us have a look what we can do with the profile API. In this blogpost I give you an overview of the capabilities with a very basic example. After this blog you should have a good idea what you can do with the profile API.\nThe sample documents\nBefore we can start, we need some sample documents. Below you can find the documents that I used together with the mapping. In order to make the output easier, I configure just one shard with no replica\u2019s and a document with the standard analysed string field together with two alternatives: keyword analyser with lowercase filter and raw.\n\nPUT /companies\r\n{\r\n  \"settings\": {\r\n    \"number_of_replicas\": 0,\r\n    \"number_of_shards\": 1,\r\n    \"analysis\": {\r\n      \"analyzer\": {\r\n        \"askeyword\": {\r\n          \"type\": \"custom\",\r\n          \"tokenizer\": \"keyword\",\r\n          \"filter\": [\"lowercase\"]\r\n        }\r\n      }\r\n    }\r\n  },\r\n  \"mappings\": {\r\n    \"company\": {\r\n      \"properties\": {\r\n        \"name\": {\r\n          \"type\": \"string\", \r\n          \"index\": \"analyzed\",\r\n          \"fields\": {\r\n            \"asone\": {\r\n              \"type\": \"string\", \r\n              \"analyzer\": \"askeyword\"\r\n            },\r\n            \"raw\": {\r\n              \"type\": \"string\",\r\n              \"index\": \"not_analyzed\"\r\n            }\r\n          }\r\n        }\r\n      }\r\n    }\r\n  }\r\n}\r\n\nBelow the documents that we insert.\nPUT /companies/company/1\r\n{\r\n  \"name\": \"Luminis Amsterdam\"\r\n}\r\nPUT /companies/company/2\r\n{\r\n  \"name\": \"Elastic\"\r\n}\r\nPUT /companies/company/3\r\n{\r\n  \"name\": \"ServiceHouse\"\r\n}\r\nPUT /companies/company/4\r\n{\r\n  \"name\": \"Bol.com\"\r\n}\r\nPUT /companies/company/5\r\n{\r\n  \"name\": \"ANWB\"\r\n}\r\n\nIntroduction\n\nYou can enable the Profile API results using the top level property\u00a0profile, set it to true to enable obtaining profile results. The profile results are calculated for each shard. So, if you are searching over multiple indices, you are also searching over multiple shards. In the results a shard is identified by the unique identifier of the node, the name of the index and the number of the shard. For an example, check the id field in the next code block which shows part of the response. But first the request with a very basic query.\n\nGET /companies/_search\r\n{\r\n  \"profile\": true,\r\n  \"query\": {\r\n    \"match\": {\r\n      \"name\": \"Amsterdam Luminis\"\r\n    }\r\n  }\r\n}\n\n\"profile\": {\r\n  \"shards\": [\r\n    {\r\n      \"id\": \"[FZPSGvI8RYOGf4pzEcfU5A][companies][0]\",\r\n      \"searches\": [\r\n        {\r\n          \"query\": [],\r\n          \"rewrite_time\": 2914,\r\n          \"collector\": []\r\n        }\r\n      ]\r\n    }\r\n  ]\r\n}\n\nAt the moment the result consists of a shards array with each shard consisting of an id according to the mentioned format and an array of searches. This search array usually consists of one item. The item contains 3 elements: The query results, the rewrite time and the collector giving more information about how Lucene handled the query including the time it took for lucene. In the next sections we take a deeper dive into the different elements of the response\nQuery Section\nQueries can have a pretty complicated structure. Just like with the explain API if you break it down, it is a repetition of the same elements. The basic building blocks are:\n\nquery_type: for instance a TermQuery\nlucene: The query part in Lucene style, for example\u00a0message:lucene\ntime: number of ms it took Lucene to execute the query\nbreakdown: Has more detailed information about the query\nchildren: This is the repetition, more of the same kind of blocks\n\nThe query type is the query type as created based on the original query, but often not the same as the original query. For example, a match query with multiple terms will become a bool query consisting of a number of Term queries. Check the following example:\n\nquery\r\n  query_type: BooleanQuery\r\n  lucene: name:luminis name:amsterdam\r\n  time: 0.3146980000ms\r\n  breakdown: \r\n  children:\r\n    query_type: TermQuery\r\n    lucene: name:luminis\r\n    time: 0.1017570000ms\r\n    breakdown:\r\n \r\n    query_type: TermQuery\r\n    lucene: name:amsterdam\r\n    time: 0.02729200000ms\r\n    breakdown:\n\nThe example shows the query_type, being BooleanQuery and TermQuery, it also shows the Lucene query that is executed and the time it took in milliseconds. The final piece is the breakdown part. This piece contains more detailed information about what took so long. Check the docs in the reference section to learn more about this very advanced statistic.\nRewrite section\nAs described in the query section, the query is rewritten. In the example a match query is rewritten as a bool query containing two term queries. This section just returns the amount of milliseconds it took to rewrite the query.\nCollectors section\nCollectors are Lucene\u2019s mechanism to keep track of all other activities. Each collector has a name and a keyword description of why the specific collector was used. Finally the collector shows the time the collection took. A few example collectors are.\n\nSimpleTopScoreDocCollector: Used when asking for the documents with the highest score used as sorting\nSimpleFieldCollector: Used when returning documents sorted by their name\nTotalHitCountCollector: Used when nu documents are requested, but only the count (when size:0 is used)\nMultiCollector: Used when a query is used in combination with a aggregation for instance.\nGlobalOrdinalsStringTermsAggregator: Used when a Terms aggregation is requested.\n\nThere are more types than mentioned here, they are in the documentation. The next code block shows a sample output extracted from the Profile API result.\n\n\"collector\": [\r\n  {\r\n    \"name\": \"SimpleTopScoreDocCollector\",\r\n    \"reason\": \"search_top_hits\",\r\n    \"time\": \"0.02907200000ms\"\r\n  }\r\n]\nConcluding\n\nTo my opinion the Profile API is a very nice addition to the available API\u2019s. It enables you to learn more about what is actually happening when executing searches in elasticsearch. It is easy to use and not to hard to understand. As a warning though, using the profile API is not cheap and should not be used by default in production. Also, not everything is supported yet by the Profile API, suggestions for instance are not supported. At the moment there is no GUI for the Profile API, but elastic is working on a Kibana plugin to help you visualise the Profile results. Maybe I\u2019ll introduce it in my own GUI project as well, need to think about that.\nReference\nhttps://www.elastic.co/guide/en/elasticsearch/reference/current/search-profile.html\n", "tags": [], "categories": ["Blog", "Search"]}
{"post_id": 39613, "title": "Het Nederlands Gebarencentrum lanceert gebarenwoordenboek app", "url": "https://www.luminis.eu/blog-en/nederlands-gebarencentrum-de-app/", "updated_at": "2015-10-28T13:12:11", "body": "Gebarenwoordenboek iSignNGT voor Apple (iOS) en Android\nIn samenwerking met Luminis Arnhem lanceert het Nederlands Gebarencentrum de applicatie iSignNGT. iSignNGT is een Basis Gebarenwoordenboek met standaard gebaren uit de Nederlandse Gebarentaal (NGT), de taal van dove mensen in Nederland. De gebruiker krijgt de basis set van ruim 600 gebaren gratis. Voor alle gebaren is een filmpje beschikbaar, daarnaast kunnen er ook tekeningen van het gebaar, een pictogram en een afbeelding te zien zijn. De lijst met begrippen is alfabetisch en thematisch georganiseerd.\nDe gebruiker kan makkelijk switchen tussen het filmpje en de overige gegevens. De app is heel geschikt voor iedereen die in gebaren wil communiceren met dove kinderen en volwassen. Aan het basisgebarenwoordenboek kunnen thema\u2019s toegevoegd worden via een in-app purchase. De thema\u2019s die aangeschaft kunnen worden zijn te vinden in de themalijst. Dit zijn thema\u2019s als Museumlexicon, Sprookjes en verhalen en lexicon thema\u2019s behorend bij cursusmodules zoals NmGAB1 en NGTAB1. Het aantal gebaren en de prijs per thema verschilt.\n\n\n\n\n\n\n\n\n\nOver het Nederlandse Gebarencentrum\nHet Nederlands Gebarencentrum is opgericht in 1996 en is sinds 2004 door de overheid erkend als het Lexicografisch Instituut op het gebied van de Nederlandse Gebarentaal in al haar verschijningsvormen. Ze verzamelen gebaren van dove gebruikers van de Nederlandse Gebarentaal en maken de gegevens toegankelijk onder andere via een online gebarenwoordenboek waarin zowel standaardgebaren als regionale varianten te vinden zijn. Daarnaast ontwikkelen en produceren ze cursusmaterialen, (digitale) woordenboeken en educatief materiaal, wordt er onderzoek gedaan naar de grammatica van NGT en kennis gedeeld in de vorm van workshops NGT en NmG, cursussen NmG. Tevens geeft het Nederlandse Gebarencentrum voorlichting en advies over de toepassing van gebaren(taal) in de communicatie met diverse doelgroepen.\nOver Luminis Arnhem\n Luminis Arnhem is een software-technologiebedrijf dat business modellen van bedrijven aansluit op het internet. De kansen die het internet hiervoor biedt, diept Luminis samen met haar klanten uit. Beveiliging, schaalbaarheid en een continue veranderende markt vragen om visie en strategie in plaats van een appje of \u2018een stukje software\u2019. Bij Luminis werken gepassioneerde vakmensen aan nieuwe toepassingen. Software is overal, dat betekent dat gebruikers steeds vaker en in totaal verschillende contexten hiermee geconfronteerd worden. Luminis Arnhem heeft dan ook naast software-experts, User eXperience designers in dienst. De gebruiker staat van begin af aan centraal; de combinatie van UX en technologie in e\u0301e\u0301n team levert haalbare innovaties op!\niSignNGT Gebarenwoordenboek downloaden\n", "tags": [], "categories": ["Blog"]}
{"post_id": 2533, "title": "Infrastructure metrics with Elasticsearch stack", "url": "https://www.luminis.eu/blog-en/search-en/infrastructure-metrics-with-elasticsearch-stack/", "updated_at": "2020-11-12T17:09:56", "body": "For the operations team of any IT organisation it\u2019s of utmost importance to have an overview of it\u2019s infrastructure at any given point of time, whether it\u2019s the response time of various customer facing systems or memory consumption stats of the processes running across servers.\nTraditionally, these stats were\u00a0explored after some problem has occurred which can be anything from slow response time to certain processes taking over the CPU cycles. Thus it\u2019s better to have a real-time insight into the infrastructure stats which enables DevOps team to quickly find and fix the cause rather than waiting for the problem to occur. In this blog post we would be exploring the Elasticsearch product stack to propose a solution for effective monitoring of the infrastructure using Elasticsearch, Kibana and Beats.  Beats shippers are part of the Elasticsearch product stack, mainly classified into PacketBeat and TopBeat. These shippers integrate seamlessly with Elasticsearch and Kibana and thus are very useful for generating infrastructure metrics. PacketBeat is used for monitoring real-time network traffic for application level protocols like HTTP, MySQL etc as well as support for DNS which is based on UDP protocol. Whereas TopBeat is the new addition to the Elastic family and it\u2019s basically the visual counter-part of the Top command used in the terminal. Using TopBeat we can get system wide stats of memory usage (free/available) and also process specific stats, i.e. Top 10 memory consuming processes, disk space used by the process etc. PacketBeat It analyzes network packets in real-time and correlates requests with responses in order to log them as specific transactions. For each transaction it records the protocol specific fields in the ElasticSearch indices. Some examples \u2013 Mysql fields Mysql. affected_rows\u00a0 \u2013 Number of rows affected by the operation Mysql. num_fields \u2013 Incase of successful \u201cSelect\u201d how many fields are returned. Mysql.num_rows \u2013 Incase of successful \u201cSelect\u201d how many rows are returned. Query \u2013 complete query which was executed. Along with other fields like server and client IP, timestamp and if the query failed then related error information. Http Fields\u00a0 Http.code \u2013 Http code Http.content_length \u2013 size of the entity-body Along with other fields like path, params, query, response time, status etc Apart from the protocol specific fields there are some fields which are generic for all transactions for a complete list of fields please see\u00a0here Protocols supported by PacketBeat are \u2013\n\nHTTP\nPostgreSQL\nMySQL\nThrift-RPC\nRedis\nMongodb\nMemcache\nDNS\n\nPacketbeat can either be installed on the existing application servers or dedicated servers. Installing them on dedicated servers reduces the load on the application servers but it\u2019s also more costly specially in an cloud hosted environment. By using port mirroring or tap devices the network packets can be analyzed in case the packbeat is installed on the dedicated server and if it\u2019s on the same server as the application then by analyzing packets on network interfaces specified in the configuration file. The network stats would either be sent directly by PacketBeat to ElasticSearch for creating index or if an logstash indexing instance is being used for aggregating logs from various sources and then sending them finally to ElasticSearch thus in that case we would write the json output of Packetbeat first to Redis as currently the direct integration between logstash and Packetbeat doesn\u2019t exist.\n\nFigure 1- Taken from official documentation of Packetbeat Packetbeat installation is straightforward for linux and mac, for windows you need to have the powershell installed on your machine. Since I have a mac thus the commands and configuration would be mac specific and it\u2019s not much different from linux configuration. Before installing Packetbeat please make sure you have ElasticSearch and Kibana installed on your machine. PacketBeat ships the json data to ElasticSearch for indexing and that index data is displayed in graphs in Kibana. Once you have downloaded and unzipped the Packetbeat, following are the important sections prefilled in packetbeat.yml configuration file.\n\u00a0\ninterfaces:\r\n  device: any\nIn the device section, it\u2019s the network interfaces that need to be monitored are mentioned, using the keyword \u201cany\u201d implies that all network interfaces need to be sniffed for packets, current \u201cany\u201d is supported on Linux and\u00a0not on mac. On Mac for every individual network interface there needs to be separate Packetbeat instance running , for example If I want to monitor the internet traffic as well as the MySQL running on localhost then I would need to start 2\u00a0 instances of PacketBeat one having the device as \u201cen0\u201d (HTTP/DNS monitoring) and other as \u201clo0\u201d (MySQL monitoring).\u00a0 For each protocol to be monitored you need to mention the network port, Example \u2013\n\u00a0\nmemcache:\r\n  ports: [11211]\r\nmysql:\r\n  ports: [3306]\nThe configuration file already contains the default ports of the protocols supported, additional ports can be added by using comma separated values. Like in HTTP \u2013\nhttp:\r\n  ports: [80, 8080, 8000, 8002]\r\n  send_headers: [\"Host\"]\nAlso note, you can add the http header to the JSON o/p of the packetBeat by specifying the \u201csend_headers\u201d options which can contains HTTP headers.\u00a0 In my case I wanted the domain name of the website visited which is contained in the \u201cHost\u201d header of the request. You can further add more headers like \u201cUser-Agent\u201d, \u201cCookie\u201d etc. In the http configuration, you can specify the scenario in which you would like to hide certain parameters like \u201cpassword\u201d from being written to JSON o/p, thus the http configuration becomes \u2013\n\u00a0\nhttp:\r\n   ports: [80, 8080, 8000, 5000, 8002]\r\n   send_headers: [\"Host\"]\r\n   hide_keywords: [\"pass\", \"password\", \"passwd\"]\nApart from the application protocols, you can also customize the shipper that Packetbeat uses, following are the major options available \u2013\nshipper:\r\n  name: \u201cMyShipper\u201d\r\n  tags: [\u201cwebserver\u201d]\r\n  ignore_outgoing: true\nName\u00a0attribute specifies the name of the shipper, the\u00a0tags\u00a0attribute helps in grouping together of various shippers, for example if you have a cluster of webservers then all the different shippers should have a common binding tag data which make it easy to create grouped visualisation in Kibana based on common tag values. The \u201cignore_outgoing\u201d attribute is\u00a0basically is avoid the scenario of a single transaction across multiple servers to be logged twice, i.e. only incoming transaction are logged and outgoing transactions are ignored thus removing the chance of logging duplicate transactions. PacketBeat output PacketBeat can write to either Elasticsearch, Redis or File. Example configuration \u2013\n\u00a0\noutput:\r\n# Elasticsearch as output\r\nelasticsearch:\r\n   enabled: true\r\n   hosts: [\"localhost:9200\"]\r\n   save_topology: true\nThe value of enabled attribute identifies the output type out of the 3 supported. \u201csave_topology\u201d being set to true means that the Packetbeat shippers publish their IPs to an ElasticSearch index and all the shippers can share this information via the output plugin. You can also provide Elasticsearch authentication credentials in the yml configuration if Elasticsearch is using Shield plugin, thus Packetbeat shippers would need to be authenticated before being able to write to ES cluster. Now we are done with the configuration of Packetbeat and before you could start Packetbeat you need to make sure that the Elasticsearch cluster that you would be writing to has the template stored i.e. in Elasticsearch you can store templates such that when new indexes are created then the mapping of the fields of these new indexes conform to the template rules. The template is provided inside the file packetbeat.template.json in the installation directory to store the template on ES cluster we just need to execute the curl command (make sure ES is up and running!) \u2013\ncurl -XPUT \u2018http://localhost:9200/_template/packetbeat\u2019\u00a0-d@packetbeat.template.json\nNow we have the template store on ES cluster and new indexes would have the naming format as \u2013 packetbeat-YYYY.MM.DD Starting Packetbeat sudo ./packetbeat -e -c packetbeat.yml -d \u201cpublish\u201d (on mac) sudo /etc/init.d/packetbeat start (on linux) Now, I browsed internet while Packetbeat was running and since it was listening to http requestthus it logged those requests in the ES cluster with index name packetbeat-2015.10.01 I go to my marvel (ES plugin) console and execute \u2013\nGET packetbeat-2015.10.01/_search?size=1000\nAnd get back a total of 585 hits after few minutes of browsing which ofcourse includes all the js and css file\u2019s GET requests as well \ud83d\ude42 Here\u2019s the complete output of a single such document \u2013\n\u00a0\n{\r\n\"_index\":\"packetbeat-2015.10.01\",\r\n\"_type\":\"http\",\r\n\"_id\":\"AVAkFdJyMolTkkADd_vF\",\r\n\"_score\":1,\r\n\"_source\":{\r\n\"bytes_in\":1733,\r\n\"bytes_out\":439,\r\n\"client_ip\":\"10.108.xx.xx\",\r\n\"client_port\":53293,\r\n\"client_proc\":\"\",\r\n\"client_server\":\"Taruns-MacBook-Pro.local\",\r\n\"count\":1,\r\n\"http\":{\r\n\"code\":301,\r\n\"content_length\":178,\r\n\"phrase\":\"Permanently\",\r\n\"request_headers\":{\r\n\"host\":\"cnn.com\"\r\n},\r\n\"response_headers\":{\r\n}\r\n},\r\n\"ip\":\"157.166.226.25\",\r\n\"method\":\"GET\",\r\n\"params\":\"\",\r\n\"path\":\"/\",\r\n\"port\":80,\r\n\"proc\":\"\",\r\n\"query\":\"GET /\",\r\n\"responsetime\":104,\r\n\"server\":\"\",\r\n\"shipper\":\"Taruns-MacBook-Pro.local\",\r\n\"status\":\"OK\",\r\n\"timestamp\":\"2015-10-01T15:47:00.519Z\",\r\n\"type\":\"http\"\r\n}\r\n}\nNow, if we want to analyze network packets for the mysql connection then as I mentioned earlier that on mac we can monitor only one device interface using one packetbeat instance hence we would need to start another packetbeat instance for monitoring mysql traffic (device :lo0 is the change in the configuration file), on unix systems using \u201cany\u201d keyword in the device option of interface makes it possible to analyze data packets on all network interfaces.\n\u00a0\ninterfaces:\r\n device: lo0\nNow, packetbeat\u2019s instance is started again after making changes in the configuration file and now it will start analyzing mysql traffic and sending json data for indexing in the ES cluster.Let\u2019s look at one of document indexed Elasticsearch.\n{\"_index\": \"packetbeat-2015.10.01\",\r\n \"_type\": \"mysql\",\r\n \"_id\": \"AVAlD_IdtouTb4teCHYB\",\r\n \"_score\": 1,\r\n \"_source\": {\r\n \"bytes_in\": 51,\r\n \"bytes_out\": 52,\r\n \"client_ip\": \"127.0.0.1\",\r\n \"client_port\": 60083,\r\n \"client_proc\": \"\",\r\n \"client_server\": \"taruns-mbp.home\",\r\n \"count\": 1,\r\n \"ip\": \"127.0.0.1\",\r\n \"method\": \"UPDATE\",\r\n \"mysql\": {\r\n \"affected_rows\": 1,\r\n \"error_code\": 0,\r\n \"error_message\": \"\",\r\n \"insert_id\": 0,\r\n \"iserror\": false,\r\n \"num_fields\": 0,\r\n \"num_rows\": 0\r\n },\r\n \"path\": \"\",\r\n \"port\": 3306,\r\n \"proc\": \"\",\r\n \"query\": \"update authors set name = \\\"taruns\\\" where id =1\",\r\n \"responsetime\": 2,\r\n \"server\": \"taruns-mbp.home\",\r\n \"shipper\": \"taruns-mbp.home\",\r\n \"status\": \"OK\",\r\n \"timestamp\": \"2015-10-01T20:20:12.832Z\",\r\n \"type\": \"mysql\"\r\n }\r\n}\nNow in the above document we notice we have the mysql method \u201cUpdate\u201d the \u201caffected rows\u201d, the complete query and \u201cresponetime\u201d using responsetime and Kibana graphs we can clearly see the queries that took maximum amount of time during a certain duration. TopBeat TopBeat is the new addition to the Elastic\u2019s product stack, it\u2019s basically the visual alternative of the terminal\u2019s top command. \u00a0After downloading the zip file of the project and creating the Topbeat template, we just need to unzip it and start the executable (just make sure the that elasticsearch is running as well). Following is the sample default configuration provided in the topbeat.yml file\n\u00a0\ninput:\r\n period: 20\r\n procs: [\".*\"]\r\nshipper:\r\n name:\r\noutput:\r\nelasticsearch:\r\n  enabled: true\r\n  hosts: [\"localhost:9200\"]\nThe default name of the Topbeat shipper is the hostname, the period 20 secs basically means the time period interval of collecting stats and currently we are monitoring all the processes but we can give specific processes in a comma separated manner. This project ships json data to Elasticsearch which can later be visualised using Kibana. Since it\u2019s analogous to the top command thus it monitors the memory consumption of a process, it\u2019s state, process\u2019s cpu usage user space, system space and start-time. Following is a document from Elasticsearch sent by Topbeat shipper.\n\u00a0\n{\r\n \"_index\":\"topbeat-2015.09.23\",\r\n \"_type\":\"proc\",\r\n \"_id\":\"AU_5ns2rNiMHwRXM6TOU\",\r\n \"_score\":1,\r\n \"_source\":{\r\n \"count\":1,\r\n \"proc.cpu\":{\r\n \"user\":9620,\r\n \"user_p\":0.04,\r\n \"system\":4504,\r\n \"total\":14124,\r\n \"start_time\":\"10:48\"\r\n },\r\n \"proc.mem\":{\r\n \"size\":1635956,\r\n \"rss\":163396,\r\n \"rss_p\":0.97,\r\n \"share\":0\r\n },\r\n \"proc.name\":\"Microsoft Word\",\r\n \"proc.pid\":14436,\r\n \"proc.ppid\":1,\r\n \"proc.state\":\"running\",\r\n \"shipper\":\"Taruns-MacBook-Pro.local\",\r\n \"timestamp\":\"2015-09-23T09:52:57.283Z\",\r\n \"type\":\"proc\"\r\n }\r\n}\nThe name of the index(if not provide) is topbeat-YYYY.MM.DD. So in the above document we can see that the proc.name as \u201cMicrosoft Word\u201d , the process id, the parent process id. The start time, the process\u2019 cpu stats i.e. user space and system space and the memory stats i.e. virtual memory,\u00a0 resident memory(rss) (and percentage) and the total memory. In case of Topbeat as well we are writing the shipper output to Elasticsearch, we can also use Redis and File system just as in Packetbeat.\u00a0Since I am using my local machine for the blog, but in production environments, you would be having indexes generated on a daily basis and since Elasticsearch provides excellent cluster support thus scaling shouldn\u2019t be a problem. Let\u2019s now create some visualizations using Kibana, I am assuming you have basic Kibana understanding, in case you don\u2019t know much about it, you can always read the official\u00a0docs. We first need to add both the indexes in Kibana (settings section) \u2013\n\nMysql method breakdown \u2013\n\nHttp response code overview \u2013\n\nWe can further use sub-aggregations to find reponsetime for each code, domains visited etc. i.e. we have so many fields that the possibilities are pretty much endless when it comes to creating various charts based on fields available to us for each indexes. Similarly, for the Topbeat indexes we have lot of possibilities, on of it being top 10 process based on resident memory consumption.\n\nAll the visualizations can then be added onto a real-time dashboard based on \u201clast 24 hour\u201d condition thus we could easily have an overview of last 24 hours of activity. Conclusion \u2013\u00a0 We have looked at Packetbeat and Topbeat, two products which when used with ElasticSearch and Kibana can help generate some fantastic real-time Dashboard and give us insights into our Infrastructure.\n", "tags": [], "categories": ["Blog", "Search"]}
{"post_id": 2544, "title": "ElasticSearch 2.0 and Pipeline Aggregations", "url": "https://www.luminis.eu/blog-en/search-en/elasticsearch-2-0-and-pipeline-aggregations/", "updated_at": "2020-11-11T08:25:51", "body": "ElasticSearch 2.0.0 beta is out and apart from many performance related updates, one major addition has been the pipeline aggregations. This has been one of the most anticipated feature requests of the new version, as the name suggests it allows us to set up a pipleline aggregation which is able to perform computation on the buckets produced as a result of the earlier aggregation.\n\nElasticSearch Pipeline aggregations are broadly classified in two types \u2013\n\u2022 Parent \u2013 A pipeline aggregation computes it\u2019s output (bucket/aggregation) and this output gets added to the bucket/aggregation of the parent aggregation.\n\u2022 Sibling \u2013 An existing aggregation becomes an input of a pipeline of aggregations and you get new aggregations at the same level as the sibling aggregation instead of it becoming part existing buckets on the input aggregations.\n\nWe will look at some examples for each of these two to get a better understanding of the concept.\nPipeline Aggregations don\u2019t support sub-aggregations but they do support chaining, thus in a chain of pipeline aggregations the final output contains the output of each aggregation in the chain. In order to reference the aggregation which would be computed upon in a pipeline, the keyword used is \u201cbuckets_path\u201d . The syntax is as follows \u2013\n\u201c\u201dbuckets_path\u201d: \u201cAggs>Metric\u201d\u201d\nAs we see in the above syntax, bucket_path refers to an aggregation and the metric in that aggregation. Let\u2019s see some examples.\nLet us first create an index, based on the data provided in the\u00a0ElasticSearch \u2013 definitive guide. For all the commands, I have used the \u201csense\u201d extension of chrome,\u00a0as currently the 2.0.0 Beta version doesn\u2019t support the marvel plugin installation from command prompt.\n\nPOST /cars/transactions/_bulk\r\n{ \"index\": {}}\r\n{ \"price\" : 10000, \"color\" : \"red\", \"make\" : \"honda\", \"sold\" : \"2014-10-28\" }\r\n{ \"index\": {}}\r\n{ \"price\" : 20000, \"color\" : \"red\", \"make\" : \"honda\", \"sold\" : \"2014-11-05\" }\r\n{ \"index\": {}}\r\n{ \"price\" : 30000, \"color\" : \"green\", \"make\" : \"ford\", \"sold\" : \"2014-05-18\" }\r\n{ \"index\": {}}\r\n{ \"price\" : 15000, \"color\" : \"blue\", \"make\" : \"toyota\", \"sold\" : \"2014-07-\nFor brevity purposes I have only shared 4 documents, but in all I have inserted 16 records, you can improvise the above data to add 12 more records in a similar schema.\nLink for sample data to get started\u00a0\u2013\u00a0https://gist.github.com/tarunsapra/d2e5338bfb2cc032afe6\nAverage Bucket Aggregation\nThis is sibling aggregation as it calculates the avg. of a metric of a specified metric in a sibling aggregation. The sibling aggregation must be multi-bucket i.e. it should have multiple grouped values for a certain field (grouping of cars based on sold monthly). Now each group can have it\u2019s total sales per month and with the help of avg. bucket pipleline aggregation we can calculate the average total monthly sales.\n\nGET /cars/transactions/_search?search_type=count\r\n{\r\n   \"aggs\":{\r\n      \"sales_per_month\":{\r\n         \"date_histogram\":{\r\n            \"field\":\"sold\",\r\n            \"interval\":\"month\",\r\n            \"format\":\"yyyy-MM-dd\"\r\n         },\r\n         \"aggs\":{\r\n            \"monthly_sum\":{\r\n               \"sum\":{\r\n                  \"field\":\"price\"\r\n               }\r\n            }\r\n         }\r\n      },\r\n      \"avg_monthly_sales\":{\r\n         \"avg_bucket\":{\r\n            \"buckets_path\":\"sales_per_month>monthly_sum\"\r\n         }\r\n      }\r\n   }\r\n}\nNow we are calculating the average of monthly total in sales and the key syntax is the expression\n\u201cbuckets_path\u201d: \u201csales_per_month>monthly_sum\u201d\nHere the aggregation , \u201csales_per_month\u201d and it\u2019s metric \u201cmonthly_sum \u201cis specified using the buckets_path syntax and this aggregation of \u201csales_per_month\u201d gives us the sum of prices of cars sold on monthly basis and the sibling aggregation \u201cavg_monthly_sale\u201d generate the aggregation value of average total monthly sales.\n\n\"aggregations\": {\r\n      \"sales_per_month\": {\r\n         \"buckets\": [\r\n            {\r\n               \"key_as_string\": \"2014-01-01\",\r\n               \"key\": 1388534400000,\r\n               \"doc_count\": 3,\r\n               \"monthly_sum\": {\r\n                  \"value\": 185000\r\n               }\r\n            },\r\n           {\r\n               \"key_as_string\": \"2014-02-01\",\r\n               \"key\": 1391212800000,\r\n               \"doc_count\": 1,\r\n               \"monthly_sum\": {\r\n                  \"value\": 25000\r\n               }\r\n            },            \u2026\u2026\u2026\u2026\u2026\u2026  14 more records\r\n         ]\r\n      },\r\n      \"avg_monthly_sales\": {\r\n         \"value\": 41900\r\n      }\r\n   }\nThus we get the \u201cavg_monthly_sales\u201d which is in parallel to the aggregation \u201csales_per_month\u201d aggregation thus this aggregation is sibling aggregation. In the aggregation query we can also change the interval from \u201cmonth\u201d to \u201cquarter\u201d and we get the average of quarterly total.\n\n\"avg_quaterly_sales\": {\r\n\"value\": 104750\r\n}\nMaximum and Minimum bucket aggregations\nJust like average bucket aggregation, both Max and Min. bucket aggregations are sibling aggregation which are producing the output aggregation in parallel to the input aggregation in our case being \u201csales_per\u201dmonth\u201d. Max. and min. pipeline aggregation were eagerly awaited \u00a0by ES users as now it becomes straightforward to find the bucket with a max. or min. value based on the metric. In our previous example if we replace \u201cavg_monthly_sale\u201d \u00a0by-\n\n\"max_monthly_sales\": {\r\n          \"max_bucket\": {\r\n              \"buckets_path\": \"sales_per_month>monthly_sum\"\r\n          }\r\n      }\nand then by\n\"min_monthly_sales\": {\r\n           \"min_bucket\": {\r\n               \"buckets_path\": \"sales_per_month>monthly_sum\"\r\n           }\r\n       }\nWe get the following in the output\n\"min_monthly_sales\": {\r\n        \"value\": 10000,\r\n        \"keys\": [\r\n           \"2014-10-01\"\r\n        ]\r\n     }\nand for maximum \u2013\n\"max_monthly_sales\": {\r\n         \"value\": 185000,\r\n         \"keys\": [\r\n            \"2014-01-01\"\r\n         ]\r\n      }\nThus we get the max. and min. bucket key along with the value ( this is really cool! ).\nSum Bucket Aggregation\nThis aggregation is again a sibling aggregation and helps in calculating the sum of all the bucket\u2019s metrics.\u00a0 For example if in our original aggregation statement, we add the following query before the \u201caggs\u201d starts i.e. \u2013\n\n\"query\" : {\r\n        \"match\" : {\r\n            \"make\" : \"bmw\"\r\n        }\r\n   },\r\n\u201caggs\u201d \u2026..\r\n\u2026\u2026\nAnd now we do the aggregation \u201csum_bmw_sales\u201d for the maker BMW and then just like and max and min. bucket pipeline aggregation we can add \u2013\n\n\"sum_bmw_sales\": {\r\n      \"sum_bucket\": {\r\n            \"buckets_path\": \"sales_per_month>monthly_sum\"\r\n            }\r\n        }\nThus now we have the per monthly total sale of the BMWs and the total yearly sum of the BMW label as well, in similar manner instead of the car label we can also specify date range or color based search and sum.\nDerivative Aggregation\nThis aggregation is a\u00a0parent aggregation\u00a0as the computed derivative of the specified metric becomes part of the bucket of the input aggregation.\n\nGET /cars/transactions/_search?search_type=count\r\n{\r\n   \"aggs\": {\r\n      \"sales_per_month\": {\r\n         \"date_histogram\": {\r\n            \"field\": \"sold\",\r\n            \"interval\": \"month\",\r\n            \"format\": \"yyyy-MM-dd\"\r\n         },\r\n         \"aggs\": {\r\n            \"monthly_sum\": {\r\n               \"sum\": {\r\n                  \"field\": \"price\"\r\n               }\r\n            },\r\n            \"sales_deriv\": {\r\n               \"derivative\": {\r\n                  \"buckets_path\": \"monthly_sum\"\r\n               }\r\n            }\r\n         }\r\n      }\r\n   }\r\n}\nIn the above query we are calculating the derivative  of the \u201cmonthly_sum\u201d and output is ..\n\"aggregations\": {\r\n      \"sales_per_month\": {\r\n         \"buckets\": [\r\n            {\r\n               \"key_as_string\": \"2014-01-01\",\r\n               \"key\": 1388534400000,\r\n               \"doc_count\": 3,\r\n               \"monthly_sum\": {\r\n                  \"value\": 185000\r\n               }\r\n            },\r\n            {\r\n               \"key_as_string\": \"2014-02-01\",\r\n               \"key\": 1391212800000,\r\n               \"doc_count\": 1,\r\n               \"monthly_sum\": {\r\n                  \"value\": 25000\r\n               },\r\n               \"sales_deriv\": {\r\n                  \"value\": -160000\r\n               }\r\n            },\r\n            {\r\n               \"key_as_string\": \"2014-03-01\",\r\n               \"key\": 1393632000000,\r\n               \"doc_count\": 1,\r\n               \"monthly_sum\": {\r\n                  \"value\": 30000\r\n               },\r\n               \"sales_deriv\": {\r\n                  \"value\": 5000\r\n               }\r\n            }, \u2026\u2026..13 more records\nFor the first bucket there is no derivate as derivate needs atleast 2 points.\nCumulative Sum Derivative\nThis is another Parent pipeline aggregation and calculates the cumulative sum of the specified metric of the input aggregation. In our\u00a0case it would help in giving us the cumulative sum of the total sales over a monthly basis.\nWe can replace the \u201csales_deriv\u201d part in our pervious query with this \u2013\n\n\"cumulative_sales\": {\r\n               \"cumulative_sum\": {\r\n                  \"buckets_path\": \"monthly_sum\"\r\n               }\r\n            }\nand get the following output\n\"aggregations\": {\r\n      \"sales_per_month\": {\r\n         \"buckets\": [\r\n            {\r\n               \"key_as_string\": \"2014-01-01\",\r\n               \"key\": 1388534400000,\r\n               \"doc_count\": 3,\r\n               \"monthly_sum\": {\r\n                  \"value\": 185000\r\n               },\r\n               \"cumulative_sales\": {\r\n                  \"value\": 185000\r\n               }\r\n            },\r\n            {\r\n               \"key_as_string\": \"2014-02-01\",\r\n               \"key\": 1391212800000,\r\n               \"doc_count\": 1,\r\n               \"monthly_sum\": {\r\n                  \"value\": 25000\r\n               },\r\n               \"cumulative_sales\": {\r\n                  \"value\": 210000\r\n               }\r\n            },\r\n            {\r\n               \"key_as_string\": \"2014-03-01\",\r\n               \"key\": 1393632000000,\r\n               \"doc_count\": 1,\r\n               \"monthly_sum\": {\r\n                  \"value\": 30000\r\n               },\r\n               \"cumulative_sales\": {\r\n                  \"value\": 240000\r\n               }\r\n            }, ..13 more records..\nWith this aggregation we can easily visualize the cumulative sum over certain peak period for various products to get more insights.\nBucket Script Aggregation\nThis is s parent pipeline aggregation and uses scripts to perform arithmetic computation on specified metrics of each bucket of a multi-bucket aggregation. A use-case can be to add/subtract or calculate percentage of a sub-aggregation in context of a bucket. For example if you want to calculate monthly percentage of total sales of the BMW car then first we would need to put a sub-aggregation in place in each bucket for BMW maker and then calculate the percentage of BMWs sold monthly in context of total sales.\nThis is Pipeline Aggregation uses scripting, please read\u00a0here\u00a0for more details. Currently I would be using inline scripting which as advised by elastic is not secure for production environment. Thus to enable inline scripting please add the following line to your elasticsearch.yml file in config folder.\nscript.inline: on\n\n\"aggs\": {\r\n      \"sales_per_month\": {\r\n         \"date_histogram\": {\r\n            \"field\": \"sold\",\r\n            \"interval\": \"month\",\r\n            \"format\": \"yyyy-MM-dd\"\r\n         },\r\n         \"aggs\": {\r\n            \"monthly_sum\": {\r\n               \"sum\": {\r\n                  \"field\": \"price\"\r\n               }\r\n            },\r\n            \"bmw_car\": {\r\n               \"filter\": {\r\n                  \"term\": {\r\n                     \"make\": \"bmw\"\r\n                  }\r\n               },\r\n                \"aggs\": {\r\n                    \"sales\": {\r\n                      \"sum\": {\r\n                        \"field\": \"price\"\r\n                      }\r\n                    }\r\n                  }\r\n            },\r\n            \"bmw_percentage\": {\r\n                    \"bucket_script\": {\r\n                        \"buckets_path\": {\r\n                          \"bmwSales\": \"bmw_car>sales\",\r\n                          \"totalSales\": \"monthly_sum\"\r\n                        },\r\n                        \"script\": \"bmwSales / totalSales * 100\"\r\n                    }\r\n                }\r\n         }\r\n      }\nResponse is \u2013\n{\r\n   \"key_as_string\": \"2014-01-01\",\r\n      \"key\": 1388534400000,\r\n      \"doc_count\": 3,\r\n       \"monthly_sum\": {\r\n          \"value\": 185000\r\n         },\r\n        \"bmw_car\": {\r\n          \"doc_count\": 2,\r\n           \"sales\": {\r\n              \"value\": 160000\r\n                 }\r\n             },\r\n           \"bmw_percentage\": {\r\n                  \"value\": 86.48\r\n             }\r\n        },\nThus for the month of Jan we can see 3 cars were sold and 2 were bmw and we get the % as 86.48.\nBucket Selector Aggregation\nThis parent pipeline aggregation is very useful in scenarios wherein you don\u2019t want certain buckets in the output based on a conditions supplied by you. Total_sum greater than some X, or Percentage greater than some value, Count > X etc.\nThis is again a script based aggregation thus we would need to have scripting enabled.\u00a0We just need to add the following snippet in the exact place where we added \u201csales_deriv\u201d aggregation as this aggregation is also parent aggregation.\n\n\"sales_bucket_filter\": {\r\n    \"bucket_selector\": {\r\n        \"buckets_path\": {\r\n            \"totalSales\": \"monthly_sum\"\r\n                  },\r\n            \"script\": \"totalSales >= 30000\"\r\n           }\r\n        }\r\n     }\nand now in the output we would only see the months where monthly sale is over 30000.\nHere\u2019s the Gist for Posting the 16 initial records \u2013\u00a0https://gist.github.com/tarunsapra/d2e5338bfb2cc032afe6\nConclusion\nThere are lot of real world use-cases for the pipeline aggregations and these would surely help in getting more insights from the data store in ES. I haven\u2019t covered Moving-average aggregation as that would be covered in a separate post as that\u2019s too vast for this blog post. Feel free to share your feedback/comments.\n", "tags": [], "categories": ["Blog", "Search"]}
{"post_id": 1706, "title": "First steps with elastic watcher", "url": "https://www.luminis.eu/blog-en/search-en/first-steps-with-elastic-watcher/", "updated_at": "2020-11-11T16:10:02", "body": "Some time a go elastic introduced a new product called watcher. Watcher is product that lets you take action based on the outcome of queries executed against elasticsearch but also against other http end points. The main purpose for the product is creating notification on some condition.\nIn this blog post I will explain the basic concepts and you can follow a long using some example code that I present in the blog post. At the end of the post you should have a general idea about what you can do with the watcher product.\nInstallation\nWatcher is a commercial plugin from elasticsearch. At the moment it is still in beta and you need to register with elastic to get beta access. More information can be found here:\nhttps://www.elastic.co/downloads/watcher\nLike with other commercial offerings from elastic, watcher comes as a plugin for elasticsearch. You have to install the license plugin as well as the watcher plugin. You can install the watcher plugin in your main cluster, but you can also install it in a seperate cluster. I have choosen to follow the second path. So I run two clusters on my local machine. The commercial cluster containing the commercial plugins runs on port 9300/9200 and the other cluster on 9301/9201.\nAfter installation you can check that it works using curl.\nCheck that it works:\ncurl -XGET 'http://localhost:9200/_watcher/stats?pretty'\r\n\nThe response in my case is.\n{\r\n  \"watcher_state\" : \"started\",\r\n  \"watch_count\" : 1,\r\n  \"execution_queue\" : {\r\n    \"size\" : 0,\r\n    \"max_size\" : 20\r\n  }\r\n}\nWhat we want to accomplish\nWe are going to log the amount of snapshots that we have in the repository. We also want to have a look at the watcher history\nStructure of a watcher\nTime to take one step further with the watcher. In this section we take a first look at the different elements of a watcher.\n\nMetadata \u2013 Static data added to the watch (payload), can be used in scripts or templates\nTrigger \u2013 Determines when a watch is checked. At the moment there are only scheduled triggers, but the future will most likely also bring us document event based triggers. All sorts of scheduled triggers are available. Think about hourly, dayly, weekly, etc. Each trigger can be configured to run on certain minutes every hour, or certain hours every day, etc.\nInput \u2013 Three types of input, Simple for testing, search for querying the local cluster and http for calling a remote service which can be a remote elasticsearch instance.\nConditions \u2013 Four type of conditions: always, never, compare ad script. Compare enables you to do some basic comparisons on the watch payload values. If you need a real script you also need to enable dynamic scripting. Since elasticsearch 1.6 there is fine grained control for the scripting support. More on this late on.\nTransforms \u2013 Three type of transforms: search, script and chain. Search replaces the current payload, script transforms the current payload and with chain you can add multiple search and script transformations in a chain. A good example is to determine the amount of errors and check if there is an action required based on the number of errors. Than in the transform we obtain all the errors and replace the payload with these obtained exceptions.\nActions \u2013 Watcher supports four types of actions email, webhook, index, and logging. Important to notice that you can have multiple actions. Log to the log service as well as send an email.\n\nLet us create a watcher\nBefore we can add the watcher let us have a look at the command to obtain snapshots. The following command shows all available snapshots for the repository named \u201ctemp-bck\u201d.\ncurl \"http://localhost:9201/_snapshot/temp-bck/_all\"\r\n\nThe actual response contains more items in the array, but each item looks like this snapshot.\n\n{\r\n  \"snapshots\" : [ {\r\n    \"snapshot\" : \"conferences-20150619094018\",\r\n    \"indices\" : [ \"conferences-20150619092724\" ],\r\n    \"state\" : \"SUCCESS\",\r\n    \"start_time\" : \"2015-06-19T07:40:18.466Z\",\r\n    \"start_time_in_millis\" : 1434699618466,\r\n    \"end_time\" : \"2015-06-19T07:40:18.540Z\",\r\n    \"end_time_in_millis\" : 1434699618540,\r\n    \"duration_in_millis\" : 74,\r\n    \"failures\" : [ ],\r\n    \"shards\" : {\r\n      \"total\" : 2,\r\n      \"failed\" : 0,\r\n      \"successful\" : 2\r\n    }\r\n  }]\r\n}\r\n\nNow let us use this request and response to create the watcher\nInput\nWe are using another cluster and we use a different elastic api than the search api. Therefore we have to use the http input.\n\n{\r\n  \"input\" : {\r\n    \"http\" : {\r\n      \"request\" : {\r\n        \"host\" : \"localhost\",\r\n        \"port\" : 9201,\r\n        \"path\" : \"/_snapshot/temp-bck/_all\"\r\n      }\r\n    }\r\n  }\r\n}\nThe trigger\nEvery watch needs a trigger. Since we only have schedule based triggers at the moment, we create a schedule based trigger. The following code shows a scheduled trigger to run every 5 seconds. A bit to much for this use case, but for testing purposes better. We don\u2019t want to wait to long for results. In reality you could be better of using one of the hourly/daily/weekly/monthly/yearly triggers. For this sample we stick with the interval based schedule.\n\n{\r\n  \"trigger\" : {\r\n    \"schedule\" : {\r\n      \"interval\" : \"5s\"\r\n    }\r\n  }\r\n}\nOne feature for calling the action is called throtteling. Only one instance of a watcher can run at a time and the action is by default only performed if it did not perform the past 5 seconds. This behavior can be customized using the property:\nthrottle_period\n. There is an additional concept for throtteling called acknowledgment. With acknowledgment the action will not be performed again after the first time before the action is acknowledged. More information about this can be found here.\nactions-ack-throttle\nCondition\nFor now we use the always condition, this means that every watch is passed on to the actions. This is the default condition when not providing a condition at all.\nTransform\nBefore taking action we want to transform the payload into some interesting numbers. We want to transform using a script. Before being able to execute scripts you have to configure the cluster to support scripts. Check the following resource for more information:\nmodules-scripting\nIn this case I needed to add the following setting to the elasticsearch.yml file\nscript.engine.groovy.inline.plugin: on\n\n{\r\n  \"transform\": {\r\n    \"script\": \"return [total_snapshots : ctx.payload.snapshots.size()]\"\r\n  }\r\n}\r\n\nAction\nThe most simple action is a log action, for now let us stick with that. We want to log the amount of available snapshots at the time of running the watcher.\n\n{\r\n  \"actions\" : {\r\n    \"log\" : { \r\n      \"logging\" : {\r\n        \"text\": \"Found {{ctx.payload.total_snapshots}} snapshots at {{ctx.execution_time}}\"\r\n      }\r\n    }\r\n  }\r\n}\r\n\nIf you are learning about the response, you can also print out \u201cctx.payload\u201d or even \u201cctx\u201d using this action. That way you can get an idea what information is available in the watcher context.\nPutting it all together\n\ncurl -XPUT \"http://localhost:9200/_watcher/watch/monitor_snapshots\" -d'\r\n{\r\n  \"trigger\": {\r\n    \"schedule\": {\r\n      \"interval\": \"5s\"\r\n    }\r\n  },\r\n  \"input\": {\r\n    \"http\": {\r\n      \"request\": {\r\n        \"host\": \"localhost\",\r\n        \"port\": 9201,\r\n        \"path\": \"/_snapshot/temp-bck/_all\"\r\n      }\r\n    }\r\n  },\r\n  \"transform\": {\r\n    \"script\": \"return [total_snapshots : ctx.payload.snapshots.size()]\"\r\n  },\r\n  \"actions\": {\r\n    \"log\": {\r\n      \"logging\": {\r\n        \"text\": \"Found {{ctx.payload.total_snapshots}} snapshots at {{ctx.execution_time}}\"\r\n      }\r\n    }\r\n  }\r\n}'\nNow you can check the log of elasticsearch to see the log lines coming by.\n[2015-06-21 12:22:42,899][INFO ][watcher.actions.logging  ] [Node-jc] Found 2 snapshots at 2015-06-21T10:22:42.882Z\r\n\nDo not forget to remove the watcher if you are done, or else it keeps logging \ud83d\ude42\ncurl -XDELETE \"http://localhost:9200/_watcher/watch/monitor_snapshots\"\r\n\nLook at the watcher history\nThere is a complete REST api for the watcher. Using this api you can create new watchers, remove watchers but also get information about a specific watcher. Given the watcher we have created in this blog, the following command can be used to obtain information about the watcher.\ncurl -XGET \"http://localhost:9200/_watcher/watch/monitor_snapshots\"\r\n\nWhat I did not expect was the status information of the watcher. This status information makes it easy to read the last time the watcher was executed, whether is was succesfull or not. Below I show you only the status part of the response\n\n{\r\n  \"status\": {\r\n    \"last_checked\": \"2015-06-21T21:14:28.547Z\",\r\n    \"last_met_condition\": \"2015-06-21T21:14:28.547Z\",\r\n    \"actions\": {\r\n      \"log\": {\r\n        \"ack\": {\r\n          \"timestamp\": \"2015-06-21T21:14:18.495Z\",\r\n          \"state\": \"ackable\"\r\n        },\r\n        \"last_execution\": {\r\n          \"timestamp\": \"2015-06-21T21:14:28.547Z\",\r\n          \"successful\": true\r\n        },\r\n        \"last_successful_execution\": {\r\n          \"timestamp\": \"2015-06-21T21:14:28.547Z\",\r\n          \"successful\": true\r\n        }\r\n      }\r\n    }\r\n  }  \r\n}\r\n\nTest execution of a watcher\nThis is such a feature that you learn to appreciate from the elastic guys. They like to make it easier for the developers. There is an execution api to force execution of a watcher. There are a lot of things to configure, there is something like a dry run. But what is even more interesting is providing a watcher inline, so without actually storing it in elasticsearch first. More information can be found here.\nInline watch\nThere are lots of other endpoints to be used using the REST api. You can start/stop/restart the watcher process. You can ask for stats about the watcher, but you can also query the history of the watchers using plain queries against the following index\nwatch_history-yyyy.MM.dd\n. This index is the trace of the watcher. It contains everything you need to find out what your watcher did, when it did it and what the resutls were. Since it is a normal index you can do everything you want with it. The following query just obtains the last 10 log entries. If you want to learn more about the response try it out yourself, you won\u2019t be disappointed, I promise.\ncurl -XGET \"http://localhost:9200/.watch_history-*/_search\" -d'\r\n{\r\n  \"sort\": [\r\n    {\r\n      \"trigger_event.triggered_time\": {\r\n        \"order\": \"desc\"\r\n      }\r\n    }\r\n  ]\r\n}'\r\n\nAdditional reading and watching\nFor a product in beta there is a fair amount of documentation. Also check the video with an introduction into watcher.\nhttps://www.elastic.co/guide/en/watcher/current/introduction.html\nWebinar by Uri Boness and Steve Kearns\n", "tags": [], "categories": ["Blog", "Search"]}
{"post_id": 2605, "title": "Improve my AngularJS project with grunt", "url": "https://www.luminis.eu/blog-en/search-en/improve-my-angularjs-project-with-grunt/", "updated_at": "2020-11-12T11:17:55", "body": "Some time a go I created issue 63 for my elasticsearch gui plugin. This issue was about improving the plugin with tools like grunt and bower. The plugin is an angularjs based library and it makes use of a number of other libraries like: d3, c3js, elasticsearch.js. In this blog post I am describing the steps I took to improve my web application using grunt and bower.\nBefore I started\nThe structure of the plugin was one from a basic one page web site. The starting page is index.html, this is in the root of the project. Within this file we include all the stylesheets, the javascript files and the javascript libraries. Libraries are in the lib folder, the angularjs files are in the js folders. There are some other parts, but these are the folders we are about to change. Before we can start improving the javascript inclusion we introduce grunt.\n\nInitialise grunt\nBefore you can use grunt, you need to install it. Installing grunt is easy if you have nodejs and npm already running. If you need help installing node, please check the references. What is different from a lot of other tools is that there is a thin client for the command line, grunt-cli, this wrapper uses the grunt that you usualy install per project. Installing the command line interface (cli) is done using npm.\n\nnpm install -g grunt-cli\nNow we are ready to start using grunt for our project. First step is to setup our project using the nodejs package.json file. There are some options to generate this file, one using npm init and another using grunt-init project templates. This is discussed in the next section.\nCreate the package.json\nThis is the file used to manage dependencies and give information about the project at hand. If you are a java developer like me, you can compare this with the pom.xml for java projects. Below you\u2019ll find the package.json from my project.\n\n{\r\n  \"name\": \"elasticsearch-gui\",\r\n  \"version\": \"1.2.0\",\r\n  \"description\": \"Elasticsearch plugin to show you what data you have and learn about queries\",\r\n  \"main\": \"index.html\",\r\n  \"repository\": {\r\n    \"type\": \"git\",\r\n    \"url\": \"https://github.com/jettro/elasticsearch-gui.git\"\r\n  },\r\n  \"keywords\": [\r\n    \"elasrticsearch\",\r\n    \"plugin\",\r\n    \"gui\"\r\n  ],\r\n  \"author\": \"Jettro Coenradie\",\r\n  \"license\": \"Apache-2.0\",\r\n  \"bugs\": {\r\n    \"url\": \"https://github.com/jettro/elasticsearch-gui/issues\"\r\n  },\r\n  \"homepage\": \"https://github.com/jettro/elasticsearch-gui\",\r\n  \"devDependencies\": {\r\n    \"grunt\": \"~0.4.5\",\r\n    \"grunt-contrib-concat\": \"~0.4.0\",\r\n    \"grunt-contrib-uglify\": \"~0.5.0\",\r\n    \"grunt-contrib-watch\": \"~0.6.1\"\r\n  }\r\n}\nOf course you write this file by hand, but there is an easier way by using\u00a0npm init. This will present a wizzard to help you create the file. Than you can start adding dependencies. You need dependencies when using grunt plugins for instance but also for the actual grunt version used to build the project. Npm has a shorthand to install these development dependencies into the package.json. The following example shows how to install the grunt library into the project. The other dependencies can be installed the same way.\n\nnpm install grunt --save-dev\nYou can run npm install to install all dependencies locally as well. Next step is to start configuring the grunt project. This is discussed in the next section.\nCreate Gruntfile.js\nThe gruntfile is where we include plugins, create tasks and combine tasks in aliases. The following code block shows the empty template for a grunt file.\n\n'use strict';\r\nmodule.exports = function (grunt) {\r\n};\nBefore I am going to show what is in mine, I want to mention a wizzard like appraoch using project templates. Grunt comes with a grunt-init cli. You have to install it as a node module using the following command.\nnpm install -g grunt-init\nThan you have to add templates. This is done using a git clone of the template repository. Of course you can also create your own templates. An example template is in the following url:\nhttps://github.com/gruntjs/grunt-init-gruntfile\nJust clone it into the folder .grunt-init and you are good to go. Now execute the following command and a wizzard will be presented to generate the Gruntfile.js.\ngrunt-init gruntfile\nNow we are going to look at some of the elements in my Gruntfile. First we are looking at template support. Again templates? Yes, but now we are talking about templates or strings with placeholdes, to include in files.\nShow template support\nGrunt comes with options to use templates for files or headers. In our generated javascript file we create a header. The following code block shows the part in the configuratio file.\n\nmodule.exports = function (grunt) {\r\n    grunt.initConfig({\r\n        pkg:grunt.file.readJSON('package.json'),\r\n        banner: '/*! <%= pkg.title || pkg.name %> - v<%= pkg.version %> - ' +\r\n        '<%= grunt.template.today(\"yyyy-mm-dd\") %>\\n' +\r\n        '<%= pkg.homepage ? \"* \" + pkg.homepage + \"\\\\n\" : \"\" %>' +\r\n        '* Copyright (c) <%= grunt.template.today(\"yyyy\") %> <%= pkg.author.name %>;' +\r\n        ' Licensed <%= _.pluck(pkg.licenses, \"type\").join(\", \") %> */\\n',\r\n    });\r\n};\nNotice that we assign the package.json file to the pkg variable. This way we can read properties like pkg.homepage and reuse them in the template. The second variable we create is the banner. I think you can undertand how spring concatenation with parameters works. Now we have the string in the banner variable and we can later on use it.\nThe next step is to gather all javascript files and concatenate them in one file.\nConcatenate my own javascript files\nBefore moving into the libraries we want to gather all javascript files for controllers, services, filters, etc into one big file.\n\nmodule.exports = function (grunt) {\r\n    grunt.initConfig({\r\n        concat: {\r\n            options: {\r\n                banner: '<%= banner %>',\r\n                stripBanners: true\r\n            },\r\n            dist: {\r\n                src: [\r\n                    'js/app.js','js/controllers/*','js/directives.js','js/filters.js','js/services.js'\r\n                    ],\r\n                dest: 'assets/js/<%= pkg.name %>.js'\r\n            }\r\n        },\r\n    });\r\n    grunt.loadNpmTasks('grunt-contrib-concat');\r\n};\nNotice that we do a loadNpmTasks. This is the module system of Grunt. In the initConfig we configure the concat task with options and the tasks that can be executes. In this case there is just one task\u00a0dist. You configure the input folders and well as the destination file. That is it. Testing the task is easy by typing:\n\ngrunt concat:dist\nSince we are now generating one file, it is also easy to move all the lengthy controllers into separate files. I did not do the services yet, but this will be done in the future. I also need to extract some copy paste functions into a service or helper thing. Enough to do. In the src you can see we include\u00a0js/controllers/*\u00a0meaning all the files in the controllers directory.\nTo make the file smaller to download, we need to uglify it. That is the next plugin that we are going to use.\nUglify the generated javascript file\nThe uglify part is not so important to me, for me the fact that it makes the script a lot smaller is much more important. There is a catch though. You have to be carefull with angularjs and the dependency injection. A lot has been written on this topic, I like this\u00a0blogpost by Aaron Smith. It took me a long time to get it right, changed it and changed it back again. In the end it turned out I forgot factory in the app.js file. Most important is that you need to inject objects with a string containing the name. That way it is not changed by the minification process and the injection is not broken. Below the example of how it works with the one method that I forgot.\n\nmyApp.factory('$exceptionHandler',['$injector', function($injector) {\r\n    return function(exception, cause) {\r\n        var errorHandling = $injector.get('errorHandling');\r\n        errorHandling.add(exception.message);\r\n        throw exception;\r\n    };\r\n}]);\nIf you omit the\u00a0\u2018$injector\u2019\u00a0than it will give you problems. Now we have one big file including the banner we dicussed before. We still have a lot of other javascript files. We have all the libraries that were manually inserted into the project. Of course we want to improve this as well using bower. This is discussed in the next section.\nObtain libraries using bower\nBefore I manually checked the libraries I needed, downloaded them and put them in the lib folder. This is fine in the beginning, but after a few iteration it becomes annoying. Other already asked me to include bower for dependency inclusion.\u00a0Bower\u00a0is again a nodejs project or npm module. The following code block shows the command.\n\nnpm install -g bower\nThe next step is to create a bower.json file. This file contains the dependencies that your project requires. Below is my bower.json file.\n{\r\n  \"name\": \"elasticsearch-gui\",\r\n  \"dependencies\": {\r\n    \"ui-bootstrap\": \"~0.11.0\",\r\n    \"angular\": \"~1.2.16\",\r\n    \"angular-route\": \"~1.2.27\",\r\n    \"c3\": \"~0.4.7\",\r\n    \"elasticsearch\": \"~3.0.1\"\r\n  }\r\n}\nThe dependencies are the minimum required dependencies. Beware though what bower brings in. The ui-bootstrap project for instance needs to be build before you can use it. For now this is a manual step, but I am sure this can be automated as well. If the file is correct, run\u00a0bower install<\u00a0and watch the default folder\u00a0bower_components. Now we have all the required libraries, we have to run\u00a0npm install\u00a0and\u00a0grunt\u00a0in the ui-bootstrap folder and we are ready for the next section.\n\n\n\nAdd the libraries to the concatenated javascript file\nWe prefer as little javascript files as possible, therefore we also want to include the libraries into the minified file. Since we are first gathering all the files and only then minify the complete file. We copy the non minified version of the libraries into the big file and minify it at once. The following code block show the extended concat plugin configuration.\n\nmodule.exports = function (grunt) {\r\n    grunt.initConfig({\r\n        concat: {\r\n            options: {\r\n                banner: '<%= banner %>',\r\n                stripBanners: true\r\n            },\r\n            dist: {\r\n                src: [\r\n                    'bower_components/angular/angular.js',\r\n                    'bower_components/angular-route/angular-route.js',\r\n                    'bower_components/ui-bootstrap/dist/ui-bootstrap-0.11.2.js',\r\n                    'bower_components/elasticsearch/elasticsearch.angular.js',\r\n                    'bower_components/d3/d3.js',\r\n                    'bower_components/c3/c3.js',\r\n                    'js/c3js-directive.js',\r\n                    'js/app.js','js/controllers/*','js/directives.js','js/filters.js','js/services.js'\r\n                    ],\r\n                dest: 'assets/js/<%= pkg.name %>.js'\r\n            }\r\n        },\r\n    });\r\n    grunt.loadNpmTasks('grunt-contrib-concat');\r\n};\nThat is it, we now have improved the loading of javascript files and we make it easier to upgrade the libraries. Of course you can run grunt from the command line, but if you are an intellij user like me you can also use the grunt runner.\nShow intellij support\nIntellij comes with support for grunt. Intellij recognises the Gruntfile.js and abstracts the tasks available in the file. Using intellij you can run the grunt tasks. The following image shows the runner from within intellij.\n\nFinal thoughts\nI know it is not perfect yet, so there will be future improvement. If you spot improvement, feel free to comment and create an issue in the github project. I personally don\u2019t like the separate package.json and bower.json, so maybe there is a better option. Like I mentioned before I want to improve the copy paste code in the controllers, take apart the services. Still I really like the improvements I could made with Grunt and Bower. For me it will be standard to use them the coming projects.\nReferences\n\nAutomate with Grunt\u00a0\u2013 I used this book to get basic knowledge about Grunt, not to many pages, enough to get you started.\nElasticsearch GUI\u00a0\u2013 Source repository on github for the plugin.\nInstalling nodejs\u00a0\u2013 You can install node using this resource or on the mac just do brew install node\nBlogpost about angularjs safe minification\n\n", "tags": [], "categories": ["Blog", "Search"]}
