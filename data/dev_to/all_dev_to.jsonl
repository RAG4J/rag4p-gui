{"title": "A Look at NAT Gateways and VPC Endpoints in AWS", "published_at": 1719579640, "tags": ["aws", "cloud", "networking"], "user": "Brandon Damue", "url": "https://dev.to/aws-builders/a-look-at-nat-gateways-and-vpc-endpoints-in-aws-28pn", "details": "Every time I get the chance, I like to write articles that are geared towards enabling you make your cloud infrastructure on AWS and other cloud platforms more secure. In today\u2019s edition of writing about AWS services, we will be learning about NAT Gateways, what they are, how they work and how they enhance our cloud infrastructure. From NAT gateways we will finish it off by talking about VPC endpoints. Allons-y (FYI: that\u2019s \u201clet\u2019s go\u201d in French \ud83d\ude09)NAT GatewaysFirst and foremost, NAT stands for Network Address Translation. Let\u2019s look at what NAT really is before moving on to NAT gateways proper. Network Address Translation is a process in which private IP addresses used on a network (usually a local area network) are translated into public IP addresses that can be used to access the internet.To understand how NAT gateways work, we are going to use the example of a two-tier architecture with a web tier deployed on EC2 instance in a public subnet (a public subnet is a subnet that has a route to an Internet gateway on the route table associated with it) and an application tier deployed on EC2 instances in a private subnet ( a private subnet has no route to an internet gateway on its route table). With this architecture, the EC2 instances that make up the application tier are unable to access the internet because they the subnet in which they reside has no route to an IGW on its route table. How will the instances go about performing tasks like downloading update patches from the internet? The answer lies in using NAT gateways. For the application tier to have access to the internet, we need to provision a NAT gateway in the public subnet housing our web tier.When an instance in the application tier wants to connect to the internet, it sends a request which carries information such as the IP address of the instance and the destination of the request to the NAT gateway in the public subnet. The NAT gateway then translates the private IP address of the instance to a public elastic IP address in its address pool and uses it to forward the request to the internet via the internet gateway. One important thing to note about NAT gateways is that, they won\u2019t accept or allow any inbound communication initiated from the internet as it only allows outbound traffic originating from your VPC. This can significantly improve the security posture of your infrastructure.NAT gateways are managed by AWS. To create a NAT gateway, all you have to do is specify the subnet it will reside in and then associate an Elastic IP address (EIP). AWS handles every other configuration for you.VPC EndpointsVPC endpoints allow private access to an array of AWS services using the internal AWS network instead of having to go through the internet using public DNS endpoints. These endpoints enable you to connect to supported services without having to configure an IGW (Internet Gateway), NAT Gateway, a VPN, or a Direct Connect (DX) connection.There are two types of VPC endpoints available on AWS. They are the Interface Endpoints and Gateway EndpointsInterface Endpoints\u2014 They are fundamentally Elastic Network Interfaces (ENI) placed in a subnet where they act as a target for any traffic that is being sent to a supported service. To be able to connect to an interface endpoint to access a supported service, you use PrivateLink. PrivateLink provides a secure and private connection between VPCs, AWS services and on-premises applications through the internal AWS network.To see the suite of services that can be accessed via interface endpoints, check out thisAWS documentation.Gateway Endpoints\u2014 They are targets within your route table that enable you to access supported services thereby keeping traffic within the AWS network. At the time of writing, the only services supported by gateway endpoints are: S3 and DynamoDB. Be sure to check the appropriate AWS documentation for any addition to the list of services. One last thing to keep in mind about gateway endpoints is that they only work with IPv4ConclusionSome say the mark of a good dancer is to know when to bow out of the stage. With that, we have officially reached the end of this article about VPC endpoints and NAT gateways. I will like to implore you to keep learning and getting better at using tools such as these for you don\u2019t know when they will come in handy. That could be sooner rather than later. Thank you for riding with me to the very end. Best of luck in all your endeavors."}
{"title": "Going on an Industry Quest: Manufacturing and Auto", "published_at": 1719562351, "tags": ["beginners", "skillbuilder", "iot", "aws"], "user": "Maurice Borgmeier", "url": "https://dev.to/aws-builders/going-on-an-industry-quest-manufacturing-and-auto-125", "details": "I currently have some time on my hands and decided to dive deeper into the AWS Skill Builder. Originally, I was interested in learning about Timestream for a training that I delivered, and then I stumbled upon the AWS Industry Quests. I remember not being very interested when theylaunched a while ago, but since they're included in my subscription, I decided to give it a chance.Because my initial search for Timestream led me to it, I started the Industry Quest for Manufacturing and Automotive. Initially, I was asked to create a company name and a character, and since I didn't care too much, I just went with a random character, although there aremanycustomization options. Once you've completed the initial setup, you're dropped into your industrial area and a tutorial that guides you through the initial steps.To become a Titan of Industry, you have to solve problems in your manufacturing plant with the help of AWS services. Doing that improves your company's and your own KPIs and level. Additionally, you collect Gems, which allow you to upgrade parts of your factory.At the time of writing, the Industry Quest: Manufacturing and Auto has 18 problems to solve, i.e., 18 labs to complete. Each Lab introduces a business context and provides an overview of the AWS services you'll be interacting with.When you start the task, you get an overview of the solution architecture, including links and additional videos explaining the services and concepts involved. This allows you to familiarize yourself with the building blocks that you'll interact with.Next, the environment introduces you to the two interactive parts of the experience. A guided practice phase in which you get step-by-step instructions to build up parts of the infrastructure (you don't start from scratch) and a later DIY phase in which you have to do some things on your own.In the practice phase, you first launch the lab environment, which may take a few minutes, depending on the Lab. You get a fresh AWS account for each Lab that's available for a couple of hours. Once that's provisioned, you log in and go through the instructions. They're usually quite accurate, although sometimes the UI screenshots are slightly out of date. I experienced some problems here, but more on that later.It's a bit frustrating that you can only switch between the steps using the two arrow buttons. That means if you're at step 50 and notice that something is not working as expected and you want to double-check instruction 15, you get to click the back button 35 times (you can also enable a scrolling feature, but that was very buggy in the browsers I tried it in. The second annoyance I experienced here was that when you switch between browser windows or tabs, it always takestwoclicks for the first button press to register, but you'll get used to that after a while.Once you have completed the practice section there's a DIY part in which you're asked to complete one or more tasks on your own. These are usually not too difficult to complete and I found them useful because they forced me to think about the problem myself. If you're struggeling, there are also some hints that can help you solve the tasks.It also has a built-in validation system to check if you've actually solved the tasks. Each DIY section explains its \"scoring\" method, and you get decent feedback for partially correct solutions.For each completed Lab, you get some experience points and gems that help you expand and improve your factory. To me, it seems like another attempt at gamification because games are supposed to have in-game currencies and levels, but I don't really see the point. At least it doesn't hurt, although, about midway through the game, I stopped being able to spend my gems on anything; I'm not sure if that was a bug or the new customizations were just not available.While these game mechanisms are part of the fun, they're not the focus of the experience. The goal is to learn about building solutions for manufacturing and automotive in AWS, primarily achieved through the labs or tasks you solve. This is where the industry quest shines.I'm probably not the intended target audience here, because I've been working with AWS for 5+ years but mostly in the serverless and data spaces, which means I have a decent amount of pre-existing knowledge that may bias how I feel about the labs.Most of the labs focus on IoT use cases and the vast majority of the tasks start out with configuring some things inIoT coreor Greengrass. You learn a lot about provisioning and updating (custom) components and software on IoT devices, which I imagine is very useful to people working in that industry.Once your things are connected to IoT core, the labs teach you what you can do with all that data. You explore machine learning services for anomaly detection and predictive maintenance. You build solutions for real-time monitoring and alerting based on Timestream and Grafana. You build fancy dashboards in Grafana that visualize machine operations using AWS TwinMaker and so much more.Aside from these metrics and monitoring you also build reporting solutions using Quicksight, which teaches you how to make management happy. Moreover you learn how to handle location data and track people through hazardous areas while visualizing and alerting when they enter dangerous parts of the factory using the Amazon Location service.One of my favorite labs is called \"Using a Digital Shadow of a Connected Vehicle\", which teaches you how you can use the IoT Device Shadows to build an app that allows you to control the state of headlights, doors, and other car components from a nice webapp. While none of the graphics here are on the level of AAA games, it's always fun to see virtual doors open when you click a checkbox, especially when the underlying synchronization mechanism could be used to have the same effect on your physical car.I can't cover all of the many labs here, but for me, this process was enjoyable for the most part. The labs cover areas of AWS that I haven't interacted (much) with so far, and they have made me interested in diving deeper. Most of the labs can be completed within about an hour, some more, some less. It depends on which services are being used. Some operations, like training a machine learning model, just take a while, but the time limits on the labs are generous, so I never felt pressured to rush anything.Here are some of my favorite labs in no particular order:Using a Digital Shadow of a Connected VehicleUses Devices Shadows in IoT to sync interactions between an app and a carConnected Vehicles TelemetryBuild a fully managed streaming data pipeline from IoT to a Grafana DashboardIndustrial Personnel and Hazard TrackingDeploy an Amplify Webapp to visualize people moving through factories and send an alert when they enter hazardous areasContextualising Industrial Equipment DataMy first interaction with SiteWiseOrganizing devices into hierarchies (e.g., factories/locations) and visualizing near-real-time metricsWhere there is light, there's also shadow. In order to complete the game and get the badge, you have to finish all the labs and not all of them work out of the box. Sometimes the instructions are out of date because something in AWS has changed.Two or three labs require you to sign up for Quicksight, and the UI is entirely different now, which means that some of the steps in the instructions are in the wrong order. This makes it a bit annoying if you've never touched Quicksight before.There's another lab that asks you to create a CodePipeline v1 through the AWS Console. Unfortunately, only v2 pipelines can be created through the console, and the older ones are no longer supported. Fortunately, all the instructions work pretty much the same way for v2 pipelines, and it's possible to complete the Lab.The worst offender is also one of the coolest labs, \"Edge to Cloud Architecture for Digital Twins\". It asks you to create a scene in TwinMaker to visualize the operations of a drill press. Quite a few things are wrong here that prevent the inexperienced user from completing the Lab and gaining the badge. Since the details are probably not super interesting to most people, I've added the problems and workarounds in theappendix, which may be helpful to you if you plan to finish the game before all the issues are fixed.Based on these problems, I think AWS should grant you the badge that you receive for completing the game if you finish at least 17 of the 18 challenges; the service teams appear to move faster than the team that's trying to keep the labs up to date. Once you finish the game, you get this beautiful badge to show off on your social media accounts.ConclusionNow that I've spent more than 18h on this game, which is the longest I've spent on any video game in the last couple of years, it's time for a conclusion. While there are some hickups with wonky controls and some out of date screenshots I mostly enjoyed myself in these labs. I'm going to specifically exclude the buggy lab from theappendixbecause that's something I expect AWS to fix soon.I liked that the labs were embedded into business problems and encompassed many parts of the AWS ecosystem that I'mnotusing daily. This means I learned about lots of new services and integration patterns, which will help me make better decisions and recommendations for our customers in the future. If you're looking for a game to speed-run, this is not it, but if you want to learn something interesting, it's an excellent place to start.AWS Skill builder offers many more features and courses than this one. If you're interested in getting it for your team, we're able to offer theteam subscription for five or more people at a discount. Justget in touch.\u2014 MauriceAppendixThe UI in step 49 asks you to create a new scene unfortunately something has changed in the mean time and by default a dynamic scene will be created. The problem is that there is some permission issue that prevents you from adding the 3D-model to this scene, which I only figured out after debugging this for a while with my colleague Franck (thanks again). If you create astaticscene, it works as expected and you can use the pretty UI.In another step of that Lab, you're asked to configure the integration of TwinMaker and Grafana, which means configuring some roles. When that's done it wants you to copy the Role ARN from the interface. Unfortunately, that's no longer displayed, so you have to navigate to IAM and copy it from there. Easy,if you already know that, probably a problem if you're not experienced.The most significant bug in that Lab is that the authentication for Grafana doesn't work out of the box. The initial environment setup creates an EC2 instance with Keycloak on it and connects that to Amazon Grafana - or is supposed to connect it to that. Unfortunately, it doesn't, so there's no way to log into Grafana. I debugged that by analyzing the EC2 instance's user data and then figured out that the setup script was pulling the latest image of Keycloak (quay.io/keycloak/keycloak:latest), which seems incompatible with the rest of the configuration. Pinning that to 22.0.1, which the script was written for, and re-running it fixed it for me.The next problem in that Lab was in Step 77, where the instructions tell you to selectrpm, whereas you need to selectrpmandstatefor the visualization to work. At that point, I was frustrated enough to accept this as a minor inaccuracy.Iwas able to fix these things with some help, because I've been working with the platform for years, but anyone coming to this game to learn about AWS would most likely be out of their depth and frustrated. I'm trying to get in touch with the right people at AWS to report this issues, once they're fixed I'll update this post."}
{"title": "AWS SnapStart - Part 23 Measuring cold and warm starts with Java 17 using asynchronous HTTP clients", "published_at": 1719500746, "tags": ["aws", "java", "serverless", "coldstart"], "user": "Vadym Kazulkin", "url": "https://dev.to/aws-builders/aws-snapstart-part-23-measuring-cold-and-warm-starts-with-java-17-using-asynchronous-http-clients-5hk4", "details": "IntroductionIn the previous parts we've done many measurements with AWS Lambda using Java 17 runtime with and without using AWS SnapStart and additionally using SnapStart and priming DynamoDB invocation :cold starts usingdifferent deployment artifact sizescold starts and deployment time usingdifferent Lambda memory settingswarm startsusing different Lambda memory settingscold and  warm startsusing different compilation optionscold and warm starts withusing different synchronous HTTP clientsIn this article we'll now add another dimension to our Java 17 measurements : the choice of the asynchronous HTTP Client implementation. AWS own offering, the asynchronous CRT HTTP client has been generally available since February 2023.I will also compare it with the same measurements for Java 21 already performed in the articleMeasuring cold and warm starts with Java 21 using different asynchronous HTTP clients.Measuring cold and warm starts with Java 17 using asynchronous HTTP clientsIn our experiment we'll re-use the application introduced inpart 8for this and rewrite it to use asynchronous HTTP client. You can the find application codehere. There are basically 2 Lambda functions which both respond to the API Gateway requests and retrieve product by id received from the API Gateway from DynamoDB. One Lambda function GetProductByIdWithPureJava17AsyncLambda can be used with and without SnapStart and the second one GetProductByIdWithPureJava17AsyncLambdaAndPriming uses SnapStart and DynamoDB request invocation priming. We give both Lambda functions 1024 MB memory.There are2 asynchronousHTTP Clients implementations available in the AWS SDK for Java.NettyNioAsync (Default)AWS CRT  (asynchronous)This is the order for the look up and set of asynchronous HTTP Client in the classpath.Let's figure out how to configure such asynchronous HTTP Client.  There are 2 places to do it :pom.xmlandDynamoProductDaoLet's consider 2 scenarios:Scenario 1)NettyNioAsync HTTP Client. It's configuration looks like thisIn pom.xml the only enabled HTTP Client dependency has to be:<dependency>         <groupId>software.amazon.awssdk</groupId>         <artifactId>netty-nio-client</artifactId>      </dependency>Enter fullscreen modeExit fullscreen modeIn DynamoProductDao the DynamoDBAsyncClient should be created like this:DynamoDbAsyncClient.builder()     .region(Region.EU_CENTRAL_1)      .httpClient(NettyNioAsyncHttpClient.create())     .overrideConfiguration(ClientOverrideConfiguration.builder()       .build())     .build();Enter fullscreen modeExit fullscreen modeScenario 2)AWS CRT synchronous HTTP Client. It's configuration looks like thisIn pom.xml the only enabled HTTP Client dependency has to be:<dependency>         <groupId>software.amazon.awssdk</groupId>         <artifactId>aws-crt-client</artifactId>      </dependency>Enter fullscreen modeExit fullscreen modeIn DynamoProductDao the DynamoDBAsyncClient should be created like this:DynamoDbAsyncClient.builder()     .region(Region.EU_CENTRAL_1)      .httpClient(AwsCrtAsyncHttpClient.create())     .overrideConfiguration(ClientOverrideConfiguration.builder()       .build())     .build();Enter fullscreen modeExit fullscreen modeFor the sake of simplicity, we create all asynchronous HTTP Clients with their default settings. Of course, there is a potential to optimize there figuring out the right settings.Using the asynchronous DynamoDBClient means that we'll be using the asynchronous programming model, so the invocation ofgetItemwill returnCompletableFutureand this is the code to retrieve the item itself (for the complete codesee)CompletableFuture<GetItemResponse> getItemReponseAsync =  dynamoDbClient.getItem(GetItemRequest.builder(). key(Map.of(\"PK\",AttributeValue.builder(). s(id).build())).tableName(PRODUCT_TABLE_NAME).build()); GetItemResponse getItemResponse = getItemReponseAsync.join(); if (getItemResponse.hasItem()) { \u2002\u2002\u2002return Optional.of(ProductMapper.productFromDynamoDB(getItemResponse.item())); \u2002}  else { \u2002\u2002\u2002return Optional.empty(); }Enter fullscreen modeExit fullscreen modeThe results of the experiment below were based on reproducing more than 100 cold and approximately 100.000 warm starts with experiment which ran for approximately 1 hour. For it (and experiments from my previous article) I used the load test toolhey, but you can use whatever tool you want, likeServerless-artilleryorPostman. I ran all these experiments for all 2 scenarios using 2 different compilation options in template.yaml each:no options (tiered compilation will take place)JAVA_TOOL_OPTIONS: \"-XX:+TieredCompilation -XX:TieredStopAtLevel=1\" (client compilation without profiling)We found out in the articleMeasuring cold and warm starts with Java 17 using different compilation optionsthat with them both we've got the lowest cold and warm start times.  We\u2019ve also got good results with \"-XX:+TieredCompilation -XX:TieredStopAtLevel=2\u201d compilation option but I haven\u2019t done any measurement with this option yet.Let's look into the results of our measurements.Cold and warm start time with compilation option \"tiered compilation\" without SnapStart enabled in ms:Scenario Numberc p50c p75c p90c p99c p99.9c maxw p50w p75w p90w p99w p99.9w maxNettyNioAsync3760.753800.163898.234101.464254.094410.896.517.519.3824.3059.112475.66AWS CRT2313.422346.892399.72502.562670.432812.785.686.457.6920.3369.90975.35Cold and warm start time with compilation option \"-XX:+TieredCompilation -XX:TieredStopAtLevel=1\" (client compilation without profiling) without SnapStart enabled in ms:Scenario Numberc p50c p75c p90c p99c p99.9c maxw p50w p75w p90w p99w p99.9w maxNettyNioAsync3708.133773.563812.513854.034019.234198.236.217.168.8022.8157.272377.48AWS CRT2331.252377.142451.722598.252756.012934.435.736.518.0021.0772.661033.18Cold and warm start time with compilation option \"tiered compilation\" with SnapStart enabled without Priming in ms:Scenario Numberc p50c p75c p90c p99c p99.9c maxw p50w p75w p90w p99w p99.9w maxNettyNioAsync2324.192380.612625.602864.132892.902895.296.727.879.9926.311683.661991.13AWS CRT1206.471348.031613.741716.901778.031779.765.736.518.0022.45692.16997.82Cold and warm start time with compilation option \"-XX:+TieredCompilation -XX:TieredStopAtLevel=1\" (client compilation without profiling) with SnapStart enabled without Priming in ms:Scenario Numberc p50c p75c p90c p99c p99.9c maxw p50w p75w p90w p99w p99.9w maxNettyNioAsync2260.042338.172586.532847.012972.032972.726.517.639.5325.091657.152132.46AWS CRT1225.921306.901618.581846.861856.111857.265.646.407.8722.09703.241069.55Cold and warm start time with compilation option \"tiered compilation\" with SnapStart enabled and with DynamoDB invocation Priming in ms:Scenario Numberc p50c p75c p90c p99c p99.9c maxw p50w p75w p90w p99w p99.9w maxNettyNioAsync744.49821.10996.801130.581255.681256.496.217.168.9423.17158.16351.03AWS CRT677.05731.94983.931279.751282.321283.55.826.728.2623.92171.221169.44Cold and warm start time with compilation option \"-XX:+TieredCompilation -XX:TieredStopAtLevel=1\" (client compilation without profiling) with SnapStart enabled and with DynamoDB invocation Priming in ms:Scenario Numberc p50c p75c p90c p99c p99.9c maxw p50w p75w p90w p99w p99.9w maxNettyNioAsync697.66747.47967.351137.381338.631339.046.417.519.3823.54155.67224.87AWS CRT694.18779.511017.941234.521243.191243.385.646.417.8721.40171.22891.36ConclusionOur measurements revealed that \"tiered compilation\" and \"-XX:+TieredCompilation -XX:TieredStopAtLevel=1\" (client compilation without profiling) values are close enough. The same we observed also with Java 21.In terms of the HTTP Client choice, AWS CRT Async HTTP Client outperformed the NettyNio Async HTTP client by far for the cold start and warm start times. The only one exception was SnapStart enabled with priming where results have been quite close. The same we observed also with Java 21.In terms of the individual comparison between Java 17 and 21 when we see lower cold starts for Java 21 for the cases where SnapStart is not enabled and it is enabled but priming is not applied. If priming is applied the cold start for Java 17 and Java 21 are very close to each other.Warm start times between Java 17 and Java 21 are very close to each other for all use cases with some deviations in both directions for the higher percentiles which might depend on the experiment.To see the full measurements for Java 21 please read my articleMeasuring cold and warm starts with Java 21 using different asynchronous HTTP clients.Can we reduce the cold start a bit further? In the previous articleMeasuring cold and warm starts with Java 17 using synchronous HTTP clientsin the \"Conclusion\" section we described how to reduce the deployment artifact size and therefore the cold start time for the AWS CRT synchronous HTTP Client. The same can also be applied for the asynchronous use case. Especially this looks promising: for the AWS CRT client we can define a classifier (i.e. linux-x86_64) in our POM file to only pick the relevant binary for our platform and reduce the size of the package. Seeherefor the detailed explanation . In this article I measured the cold and warms starts only by using the uber-jar containing binaries for all platforms, so please set the classifier and re-measure it for our platform. Be aware that currently not all platforms/architectures like aarch_64 support SnapStart.The choice of HTTP Client is not only about minimizing cold and warm starts. The decision is much more complex and also depends on the functionality of the HTTP Client implementation and its settings, like whether it supports HTTP/2. AWS published the decision tree whichHTTP client to choosedepending on the criteria."}
{"title": "Events vs streaming data in AWS", "published_at": 1719496186, "tags": ["aws", "eventdriven", "datastreaming"], "user": "Jimmy Dahlqvist", "url": "https://dev.to/aws-builders/events-vs-streaming-data-in-aws-ii9", "details": "Recently I got a question regarding streaming data vs events. Can streaming data be events? Is all streaming data events? Are all events streaming data?In this post I will give my perspective on the matter, and how I see the correlation between the two. I will also introduce some AWS based architectures for different scenarios.Defining streaming data and eventsLet's start with a small definition of streaming data and events.Streaming Data: is a continuous flows of data generated by different sources that can be processed and analyzed in real-time. This type of data can be produced at a high velocity and volume. Some examples of streaming data include sensor data from IoT devices, log files from web servers, and click stream data from websites.Events: are a record of a change in state that is important within a system. Events are often discrete, encapsulated pieces of information that indicate something has happened at a particular point in time. Examples of events are a temperature sensor reaching a threshold, a user being created in a system, or a door sensor changing state.Differences between streaming data and eventsTo understand the differences between streaming data and events there a few things we can look at.Data flowStreaming data is normally compromised by a continuous flow of data points. The granularity and velocity of the data can vary. As an example, a temperature sensor that sends current reading every second even if there is no change in temperature. The flow of points are just continuous.Events are a data flow where there has been a change in the system state that is important to the system. As example, a temperature sensor that only send temperature reading when there is a change in temperature or when a set threshold is crossed. The flow of data points depends on the change in temperature.Volume and velocityStreaming data can have extreme high volume and velocity, as this is continuous data generation and transmission. Require a robust infrastructure to handle and process the influx of data. Since data is continuous loosing a reading is often not a problem.Events can also be high in volume however as the focus on changes of state, the velocity is often lower. Still require a robust infrastructure since not loosing an event can be crucial for the system, a storage first architecture pattern is a good approach to secure this.Data structuresStreaming data can have varying structures, often including raw data that needs processing and filtering to extract meaningful information. The data might be unstructured, semi-structured, or structured.Events are usually well-structured and contain predefined attributes, making them easier to interpret and act upon. Each event has a clear schema that describes its properties.Purpose and usageStreaming data is often used for real-time or near real-time analytics and monitoring. It enables immediate insights and actions, such as anomaly detection, real-time dashboards, and live analytics.Events are focused on capturing changes in state that can invoke actions within a system. Events are often used in event-driven architectures to drive workflows, notifications, and automated responses.Similarities between streaming Data and eventsTo understand the similarities between streaming data and events there a few things we can look at.Data volumeBoth streaming data and events can generate a high volume of data that require a robust and scalable infrastructure. Often the volume over the day can fluxuate adding more requirements on the infrastructure.Real-timeBoth streaming data and events can have real-time processing requirements, with different use-cases and purposes, which can be critical for applications requiring low latency and quick decision-making.Data sourcesStreaming data and events can originate from similar sources, such as IoT devices, user interactions, and system logs. The distinction lies in how the data is captured and utilized.Practical applications and use-casesThis is some practical applications and use cases that leverage both concepts, and example architectures for implementation in AWS.Real-time analyticsCombining streaming data with event processing enables businesses to gain real-time insights. For example, companies can equip factories and production lines with IoT sensors, constantly measuring things like air quality, engine temperatures, and much more. This way it's  possible to early detect problems that might impact the quality of the product.In this solution we can rely on IoT core and have IoT sensors send data directly to the cloud over MQTT. We can create business logic and analytics to alert in case of problems. We could also have sensors send data to a central hub in the factory that then send data to a kinesis data stream for analytics.Monitoring and alertingIn cloud based applications, continuous monitoring of the system can identify issues and triggering alerts. We can utilize services like CloudWatch logs, CloudTrail, and AWS Config to gain insight and take action. This approach is enables us to understand system health and security.IoT sensorsEvent-driven architectures allow for automated responses to specific events. For example, in smart homes, events like a door opening or a motion detected can trigger actions such as turning on lights or sending notifications. This automation can enhance convenience and security.To implement this scenario we rely on IoT core for sensor events, door open or motion detected. With the powerful rules engine in IoT core we send this as an event to EventBridge, that will act as our broker. IoT core don't have a direct integration to EventBridge so we rely on SQS with EventBridge pipes. We can utilize StepFunctions to implement the business logic and then send the action back through IoT core.ConclusionIn conclusion, while streaming data and events are distinct concepts, they share similarities and can often intersect. Streaming data represents continuous flows of information, whereas events are changes in state. Understanding the nuances between the two is crucial for designing systems that leverage real-time insights and enable timely actions.Almost all the time events can be seen as streaming data, while streaming data most often is not events.Final WordsThis was a post looking the the differences and similarities between streaming data and events. Streaming data is not always events, while events often can be treated as streaming data.Check outMy serverless Handbookfor some of the concepts mentioned in this post.Don't forget to follow me onLinkedInandXfor more content, and read rest of myBlogsAs Werner says! Now Go Build!"}
{"title": "9 Ways to Spin Up an EKS Cluster - Way 3 - eksctl", "published_at": 1719486029, "tags": ["kubernetes", "eks", "iac"], "user": "Ant(on) Weiss", "url": "https://dev.to/aws-builders/9-ways-to-spin-up-an-eks-cluster-way-3-eksctl-2op9", "details": "In myprevious postI showed how to spin up an EKS cluster with pure shell and AWS CLI. (All the links to other posts in this series will behere)This used to be the easiest way of getting to a cluster without leaving your terminal. But pretty early in EKS history (2017) some smart folks from a company named Weaveworks(RIP) realized it was too cumbersome to do this using theaws clisubcommand and that EKS is complex enough to deserve a command-line client of its own.  That's howeksctlwas born.A few months ago Weaveworks (who brought us a plethora of great OSS tools like Flux, Flagger and Weave) was shut down. But AWS announced full support for eksctl in 2019 - soeksctlis now the de-facto standard EKS CLI tool.The great thing abouteksctlis that it allows one to create and manage clusters not only using one-off commands with arguments but also with YAML configuration files - in a true and familiar IaC way.We'll check out both options but first let's install eksctl and generate an SSH key so we can connect to the nodes in the clusters we create if needed. Please note - I'm not endorsing SSH connections to your EKS nodes. Do avoid this if possible - so as not to cause inadvertent configuration drift. But sometimes we still need this for troubleshooting, especially in training environments. So let's have the SSH key handy.Install eksctlIf you're on Linux - here are the official instructions:# for ARM systems, set ARCH to: `arm64`, `armv6` or `armv7`ARCH=amd64PLATFORM=$(uname-s)_$ARCHcurl-sLO\"https://github.com/eksctl-io/eksctl/releases/latest/download/eksctl_$PLATFORM.tar.gz\"# (Optional) Verify checksumcurl-sL\"https://github.com/eksctl-io/eksctl/releases/latest/download/eksctl_checksums.txt\"|grep$PLATFORM|sha256sum--checktar-xzfeksctl_$PLATFORM.tar.gz-C/tmp&&rmeksctl_$PLATFORM.tar.gzsudo mv/tmp/eksctl /usr/local/binEnter fullscreen modeExit fullscreen modePlease note this doesn't install such eksctl prerequisites askubectlandaws-iam-authenticator.And if, like me -  you're on a Mac - definitely usebrewas it takes care of all dependencies. (even though the officialeksctldocs don't recommend it)brew tap weaveworks/tap brew install weaveworks/tap/eksctlEnter fullscreen modeExit fullscreen modeAnd now - let's generate that ssh key:ssh-keygen  -f ./id_rsa -N ''Enter fullscreen modeExit fullscreen modeThis will create anid_rsaandid_rsa.pubin your current directory. Make sure to run the followingeksctlcommands from the same directory and it will pick up this key by default.Sidenote - the VPCIf you've read the previous post in this series (where we created an EKS cluster using the AWS CLI), you'd notice that creating the VPC was a separate step. The added value ofeksctlis it takes care of most dependencies and add-ons for us without the need of running additional commands. The same is true for VPC creation. A new VPC with default subnet configuration is created for us each time we spin up a new cluster, unless we specifically define we want to re-use an existing VPC.1. Create an EKS cluster - eksctl with argumentsThe most straightforward way of creating an EKS cluster witheksctlis providing all the arguments on the command-line and letting the tool take care of the defaults. This approach, while limited and not repeatable enough can definitely give us a cluster.The command I provide here defines quite a number of settings I personally find important even for small toy clusters I spin up for fun and games. Buteksctlcan do its job even with less stuff defined. Look in the official \"Getting Started\" docs if you want just the bare bones.So here's what I decided to use:# First - define the environment.exportCLUSTER_NAME=way3exportAWS_REGION=eu-central-1exportK8S_VERSION=1.30exportNODE_TYPE=t2.mediumexportMIN_NODES=1exportMAX_NODES=3Enter fullscreen modeExit fullscreen modeI'm starting out with small nodes and already preparing the cluster for auto-scaling with min and max nodes definitions. It's important to note thateksctlallows us to enable the IAM policy for ASG acces and define the auto-scaling range. But it doesn't take care of installingcluster-autoscaler. We'd need to do that separately. If we wanted to... On the other hand - these days it makes total sense to start out with Karpenter. For whicheksctldoes provide support, but not on the command line. whcih means we'll see how to configure Karpenter in the next section.And now - time to spin up the cluster:eksctl create cluster--name$CLUSTER_NAME\\--region$AWS_REGION\\--with-oidc--version$K8S_VERSION\\--nodegroup-nameng-$CLUSTER_NAME-1\\--node-typet2.medium\\--nodes1--nodes-min1--nodes-max3\\--spot\\--ssh-access\\--asg-access\\--external-dns-access\\--full-ecr-access\\--alb-ingress-accessEnter fullscreen modeExit fullscreen modeThis command gives us a full-featured cluster with IAM policies for ECR access (--full-ecr-access), external dns controller (--external-dns-access) , ALB ingress controller (--alb-ingress-access), OIDC support and more. It also runs its nodes on spot instances for cost optimization. Which is totally fine for a toy cluster but may be not appropriate if the application you're planning to deploy isn't disruption-tolerant.From the command output we learn that in the background our command is converted into a couple of CloudFormation stacks:2024-06-27 12:51:47 [\u2139]  will create a CloudFormation stack for cluster itself and 0 nodegroup stack(s) 2024-06-27 12:51:47 [\u2139]  will create a CloudFormation stack for cluster itself and 1 managed nodegroup stack(s)Enter fullscreen modeExit fullscreen modeAfter about 15 minutes (depending on the weather and the region you've decided to use) CloudFormation returns and we can access our cluster:kubectl get node NAME                                             STATUS   ROLES    AGE   VERSION ip-192-168-56-76.eu-central-1.compute.internal   Ready    <none>   35m   v1.29.3-eks-ae9a62aEnter fullscreen modeExit fullscreen modeNote that the new cluster context is added to yourkubeconfigautomatically.If you want to update thekubeconfigat a later time you can use:eksctl utils write-kubeconfig-c$CLUSTER_NAME-r$AWS_REGIONEnter fullscreen modeExit fullscreen modeBut, as we already said - the CLI approach is limited. To do real IaC  we want to put the cluster definitions in a YAML config file. This gives us a lot more capabilities, and allows to commit the config file to source control for further collaboration, change tracking and  automation.But first - let's remove the cluster we just created:eksctl delete cluster --region=$AWS_REGION --name=$CLUSTER_NAMEEnter fullscreen modeExit fullscreen mode2. Create an EKS cluster - eksctl with a config file.The config file I provide here gives us everything we defined at the command line and more. As mentioned - it also allows us to install Karpenter in the sameeksctlexecution - thus giving us an industry-standard auto-scaling EKS cluster with just-in-time node provisioning. You can grab this file inGithubtoo.apiVersion:eksctl.io/v1alpha5kind:ClusterConfigmetadata:name:way3region:eu-central-1version:\"1.30\"tags:karpenter.sh/discovery:way3iam:withOIDC:truemanagedNodeGroups:-name:ng-way3-1labels:{role:worker}instanceType:t2.mediumdesiredCapacity:2minSize:1maxSize:3tags:nodegrouprole:way3volumeSize:20iam:withAddonPolicies:externalDNS:truecertManager:trueawsLoadBalancerController:truealbIngress:trueebs:trueefs:trueimageBuilder:truecloudWatch:truessh:allow:true# will use ~/.ssh/id_rsa.pub as the default ssh keykarpenter:version:'0.37.0'createServiceAccount:truewithSpotInterruptionQueue:trueEnter fullscreen modeExit fullscreen modeAn attentive eye will also notice I've also defined some additional stuff such asCloudWatch logging of the control plane,  EBS and EFS access. Consider removing these lines if you don't need them.Also you'll notice that not only it installs Karpenter, it also takes care of setting up the SpotInterruptionQueue, which allows Karpenter to replace spot instances before they die.And there are many additional options available.So yes - this is a very scalable approach, which takes care of more or less everything one might need in an EKS cluster.Execute this plan with:eksctl create cluster -f cluster.yamlEnter fullscreen modeExit fullscreen modeThis again creates a CloudFormation execution that, granted we have all the necessary permissions, should complete successfully.Let's check that Karpenter got installed:kubectl get pod-ANAMESPACE     NAME                         READY   STATUS    RESTARTS   AGE karpenter     karpenter-79db484bbf-flzzq   1/1     Running   0          32s karpenter     karpenter-79db484bbf-nfhsp   1/1     Running   0          32s kube-system   aws-node-8h4ln               2/2     Running   0          17m kube-system   aws-node-vq8wj               2/2     Running   0          18m kube-system   coredns-6f6d89bcc9-qx497     1/1     Running   0          24m kube-system   coredns-6f6d89bcc9-wwjtp     1/1     Running   0          24m kube-system   kube-proxy-8mnd2             1/1     Running   0          18m kube-system   kube-proxy-c5zkp             1/1     Running   0          17mEnter fullscreen modeExit fullscreen modeYup, here it is!The upside of using the config file is of course the ability to manage stuff in a somewhat idempotent way. So for example if we want to change our node group config - we can update the following lines:-name:ng-1labels:{role:worker}instanceType:t2.mediumdesiredCapacity:1minSize:1maxSize:5Enter fullscreen modeExit fullscreen modeand then runeksctl update nodegroup -f cluster.yaml- this will update our NodeGroup autoscaling range.And of course eksctl provides us with a plethora of addtional commands that come very handy for ongoing management of EKS clusters:eksctl-hThe official CLIforAmazon EKS  Usage: eksctl[command][flags]  Commands:   eksctl anywhere                        EKS anywhere   eksctl associate                       Associate resources with a cluster   eksctl completion                      Generates shell completion scriptsforbash, zsh or fish   eksctl create                          Create resource(s)eksctl delete                          Delete resource(s)eksctl deregister                      Deregister a non-EKS cluster   eksctl disassociate                    Disassociate resources from a cluster   eksctl drain                           Drain resource(s)eksctlenableEnable featuresina cluster   eksctl get                             Get resource(s)eksctlhelpHelp about anycommandeksctl info                            Output the version of eksctl, kubectl and OS info   eksctl register                        Register a non-EKS cluster   eksctl scale                           Scale resources(s)eksctlsetSet values   eksctlunsetUnset values   eksctl update                          Update resource(s)eksctl upgrade                         Upgrade resource(s)eksctl utils                           Various utils   eksctl version                         Output the version of eksctlEnter fullscreen modeExit fullscreen modeAll in all - eksctl is the go to tool for EKS management if you haven't already standardized your cloud platform on another IaC solution such as Terraform, Pulumi, CDK or others which we'll look into in the folowing posts.Thanks for reading and may your clusters be lean!P.S. now you got a cluster - why not start managing its cost and performance for free withPerfectScale- the leading Kubernetes cost optimization solution?Join now to build clusters you can be proud of:https://perfectscale.io."}
{"title": "Understanding Network Access Control Lists and Security Groups in AWS", "published_at": 1719450129, "tags": ["aws", "securitygroup", "accesscontrol"], "user": "Brandon Damue", "url": "https://dev.to/aws-builders/understanding-network-access-control-lists-and-security-groups-in-aws-3bk4", "details": "In an article I published exactly a year ago, I wrote about VPCs and subnets in the AWS cloud and all one needs to know about these foundational AWS networking concepts. However, I did not go into the details of Network Access Control Lists (NACLs) and Security Groups (SGs). This doesn't mean the significance of these core aspects of AWS networking is lost on me. The purpose of this write up is to provide you with an in depth examination of Security Groups and NACLs. I recommend readingthe articleI wrote on VPCs and subnets before coming back to this one. If you went to read that, welcome back and without further ado let's get to business!Network Access Control ListsAs we all know, security is a very important component of your AWS infrastructure and it is something that should always be top of mind when you are implementing solutions in the cloud.NACLs are security filters that control the flow of traffic in and out of a subnet. When you create a subnet in the AWS cloud, a default NACL is associated with it if you didn't explicitly configure one while creating the subnet. These defaults NACLs allow all inbound and outbound to and from the subnet respectively. Because of this, they pose a security threat. To eliminate this security threat you can configure your NACL by adding rules to it. These rules could either be inbound or outbound.Each inbound rule added to you NACL is made up of the following fields:ARule number(Rule #) which determines the order in which the rules are evaluated.ATypefield which determines the type of inbound traffic you want to allow or deny into the subnets the NACL is associated with.AProtocolfield which determines the protocols used by the inbound traffic.Port rangefield which determines the range of ports to be used by the inbound traffic.Sourcewhich determines the source IP address range of the inbound traffic andAnAllow / Denyfield which determines whether the rule is allowing or denying the inbound traffic.The image below shows a visual example of NACL inbound rules:For outbound rules, all the fields are the same except for the Source field which is replaced with a Destination field determining the destination of outbound traffic from the subnets associated with the NACL.NACLs arestateless. This means any response traffic generated from a request needs to be explicitly allowed else they are a denied implicitly. To put it simply, when traffic is allowed from particular source with a particular port range, type and protocol, the return traffic to that source is not allowed by default and you have explicitly allow it.Noteworthy: A subnet can only have one NACL associated with it at any point in time but a NACL can be associated with multiple subnets at a time.Now let's move on to security groups.Security GroupsSecurity Groups are much like NACLs with a few difference such as: SGs control the flow of traffic in and out of an EC2 instance, they are stateful unlike NACLs which are stateless. Let's unpack each of these aspects in more detail.Security Groups also act as traffic filters but rather than working at the subnet level like NACLs do, they work at the instance level. They have similar fields to NACL rules except for the fact that there is no Rule # and Allow / Deny fields. Since SG rules do not have rule numbers to determine the order which they are evaluated, all the rules in a security group have to be evaluated before a decision is made on the flow of traffic.SGs have only allow rules implying that any traffic that is not allowed by a security group rule is denied. Because security groups are stateful, any traffic allowed into an instance, the return traffic is allowed by default. The image below shows some examples of security group rules.As a final recap, NACLs filter traffic at the subnet level and they are stateless while SGs filter traffic at the instance level and they are stateful.ConclusionWe have seen how security groups and NACLs work together to control the flow of traffic into and out of your AWS environment. Configuring NACLs and SGs is your responsibility as stipulated by the AWS Shared Responsibility Model so learning how to use them properly will greatly improve the security posture of your AWS infrastructure. This is where this article ends but it shouldn't be where you end your journey of learning about Security Groups and NACLs. Good luck in all your endeavors."}
{"title": "Orchestrating Serverless Workflows with Ease using AWS Step Functions", "published_at": 1719425442, "tags": ["aws", "orchestration", "stepfunctions"], "user": "Brandon Damue", "url": "https://dev.to/aws-builders/orchestrating-serverless-workflows-with-ease-using-aws-step-functions-3mok", "details": "When we talk about running serverless workloads on AWS (disclaimer: serverless doesn\u2019t mean there are no servers, it just means you don\u2019t have to worry about provisioning and managing them), the service that immediately comes to mind is definitely AWS Lambda. This serverless compute service allows developers to run their code in the cloud, all without managing the underlying infrastructure.Although Lambda offers developers the ability to run their code in the cloud, it does have some constraints that limit its usability in specific scenarios and use cases. One of these constraints is Lambda\u2019s maximum execution time of 15 minutes. Unfortunately, this means developers cannot use Lambda to carry out complex operations that take more than 15 minutes to complete.However, don\u2019t let this limitation dissuade you from using AWS Lambda! Because this is where AWS Step Functions step in (see what I did there? \ud83d\ude09) to the rescue to make the execution of complex operations possible.The main objective of this article is to bring the good news of AWS Step Functions to you my dear friend. So grab your digging equipment and without further ado, let\u2019s start digging into it.What is AWS Step FunctionsSimply put, AWS Step Functions is a state machine service. But what exactly is a state machine? Let\u2019s use an analogy to explain. Imagine your office coffee maker. It sits idle in the kitchen, waiting for instructions to make coffee. When someone uses it, they select the type of coffee, quantity, and other options \u2014 these are the states the machine goes through to make a cup of coffee. Once it completes the necessary states, the coffee maker returns to its idle state, ready for the next user. AWS Step Functions allow you to create workflows just like the coffee maker, where you can have your system wait for inputs, make decisions, and process information based on the input variables. With this kind of orchestration, we are able to leverage Lambda functions in ways that are not inherently supported by the service itself. For instance, you can run processes in parallel when you have multiple tasks you want to process at one time or in sequence when order is important. In a similar fashion, you can implement retry logic if you want your code to keep executing until it succeeds, or reaches a time out of some sort. This way, we are able to conquer lambda\u2019s 15 minutes code execution limit.How does it work?Now on to how Step Functions works. It operates by getting your workflow from an Amazon State language file which is a JSON based file that is used to define your state machine and its components. This file defines the order and flow of your serverless tasks in AWS Step Functions. It\u2019s like the recipe for your code workflow. Here is an example of what State language looks like:{\"StartAt\":\"SayHello\",\"States\":{\"SayHello\":{\"Type\":\"Task\",\"Resource\":\"arn:aws:lambda:REGION:ACCOUNT_ID:function:SayHelloFunction\",\"Next\":\"Goodbye\"},\"Goodbye\":{\"Type\":\"Task\",\"Resource\":\"arn:aws:lambda:REGION:ACCOUNT_ID:function:GoodbyeFunction\",\"End\":true}}}Enter fullscreen modeExit fullscreen modeAs you can see from the code above, State Language files are written in JSON format, a language familiar to most developers. However, if you\u2019re new to JSON, there\u2019s no need to worry! AWS Step Functions lets you build your state machine by dragging and dropping components (states) to link them in the AWS Step Functions Workflow Studio. Here\u2019s a picture example of what a state machine looks like in the Workflow Studio.State Machine State TypeThere are eight commonly used core state types that you can define in your workflow to achieve a particular result. These state types are: the Pass state, Task State, Choice State, Wait, Success State, Fail, Parallel State, and Map State. We\u2019ll take a closer look at each of these in more detail.Pass State\u2014 The Pass state doesn\u2019t actually perform a specific action. Instead, it acts as a placeholder state, facilitating transitions between other states without executing any code. While it can be helpful for debugging purposes, such as testing transitions between states, it\u2019s not exclusively a debugging state.Task State\u2014 This is where the action happens. As the most common state type, it represents a unit of work, typically executed by an AWS Lambda function or another integrated service.Choice State\u2014 This state allows you to evaluate an input and then choose the next state for the workflow based on the evaluation outcome. Essentially, it\u2019s an \u201cif-then\u201d operation that enables further application logic execution based on the chosen path.Wait State\u2014 In this state, you can pause the state machine for a specified duration or until a specific time is reached. This comes in handy if you want to schedule a pause within the workflow. For example, you can use it to send out emails at 10:00 AM every day.Success State\u2014 This state is used to indicate the successful completion of a workflow. It can be part of the choice state or to end the state machine in general.Fail State\u2014 It is termination state similar to the success state but indicates that a workflow failed to complete successfully. Fail states should have an error message and a cause for better workflow understanding and troubleshooting.Parallel State\u2014 This state executes a group of states as concurrently as possible and waits for each of them to complete before moving on. Imagine you have a large dataset stored in S3 that needs to be processed. You can use a Parallel state to concurrently trigger multiple Lambda functions, each processing a portion of the data set. Doing this will significantly speed up the overall processing time.Map State\u2014 The Map state allows you to loop through a list of items and perform tasks on them. In the map state, you can define the number of concurrent items to be worked on at one time.By making use of a combination of these states, you can build dynamic and highly scalable workflows. To find the list of supported AWS service integrations for Step Functions, check out thisAWS documentation.ConclusionThis article has introduced you to AWS Step Functions and its potential to streamline your application development. Step Functions manages your application\u2019s components and logic, allowing you to write less code and focus on building and updating your application faster. It offers a wide range of use cases, including submitting and monitoring AWS Batch jobs, running AWS Fargate tasks, publishing messages to SNS topics or SQS queues, starting Glue job runs, and much more. If your workflow involves tasks like these, AWS Step Functions can be a valuable asset."}
{"title": "Back2Basics: Monitoring Workloads on Amazon EKS", "published_at": 1719394490, "tags": ["aws", "eks", "kubernetes", "grafana"], "user": "Romar Cablao", "url": "https://dev.to/aws-builders/back2basics-monitoring-workloads-on-amazon-eks-4442", "details": "OverviewWe're down to the last part of this series\u2728 In this part, we will explore monitoring solutions. Remember the voting app we've deployed? We will set up a basic dashboard to monitor each component's CPU and memory utilization. Additionally, we\u2019ll test how the application would behave under load.If you haven't read the second part, you can check it out here:Back2Basics: Running Workloads on Amazon EKSRomar Cablao for AWS Community Builders  \u30fb Jun 19#aws#eks#kubernetes#karpenterGrafana & PrometheusTo start with, let\u2019s briefly discuss the solutions we will be using. Grafana and Prometheus are the usual tandem for monitoring metrics, creating dashboards and setting up alerts. Both are open-source and can be deployed on a Kubernetes cluster - just like what we will be doing in a while.Grafanais open source visualization and analytics software. It allows you to query, visualize, alert on, and explore your metrics, logs, and traces no matter where they are stored. It provides you with tools to turn your time-series database data into insightful graphs and visualizations. Read more:https://grafana.com/docs/grafana/latest/fundamentals/Prometheusis an open-source systems monitoring and alerting toolkit. It collects and stores its metrics as time series data, i.e. metrics information is stored with the timestamp at which it was recorded, alongside optional key-value pairs called labels. Read more:https://prometheus.io/docs/introduction/overview/Alternatively, you can use an AWS native service likeAmazon CloudWatch, or a managed service likeAmazon Managed Service for PrometheusandAmazon Managed Grafana. However, in this part, we will only cover self-hostedPrometheusandGrafana, which we will host on Amazon EKS.Let's get our hands dirty!Like the previous activity, we will use thesame repository. First, make sure to uncomment all commented lines in03_eks.tf,04_karpenter.tfand05_addons.tfto enableKarpenterand other addons we used in the previous activity.Second, enableGrafanaandPrometheusby adding these lines interraform.tfvars:enable_grafana    = true enable_prometheus = trueEnter fullscreen modeExit fullscreen modeOnce updated, we have to runtofu init,tofu planandtofu apply. When prompted to confirm, typeyesto proceed with provisioning the additional resources.Accessing GrafanaWe need credentials to access Grafana. The default username isadminand the auto-generated password is stored in a Kubernetessecret. To retrieve the password, you can use the command below:kubectl -n grafana get secret grafana -o jsonpath=\"{.data.admin-password}\" | base64 -dEnter fullscreen modeExit fullscreen modeThis is what the home or landing page would look like. You have the navigation bar on the left side where you can navigate through different features of Grafana, including but not limited toDashboardsandAlerting.It's worth noting thePrometheusthat we have deployed. You might be asking - Does thePrometheusserver have a UI? Yes, it does. You can even query usingPromQLand check the health of the targets. But we will use Grafana for the visualization instead of this.Setting up our first data sourceBefore we can create dashboards and alerts, we first have to configure the data source.First, expand theConnectionsmenu and clickData Sources.ClickAdd data source. Then selectPrometheus.Set the Prometheus server URL tohttp://prometheus-server.prometheus.svc.cluster.local. SincePrometheusandGrafanareside on the same cluster, we can use the Kubernetesserviceas the endpoint.Leave other configuration as default. Once updated, clickSave & test.Now we have our first data source! We will use this to create dashboard in the next few section.Grafana DashboardsLet\u2019s start by importing an existing dashboard. Dashboards can be searched here:https://grafana.com/grafana/dashboards/For example, consider this dashboard -315: Kubernetes Cluster Monitoring via PrometheusTo import this dashboard, either copy theDashboard IDor download theJSONmodel. For this instance, use the dashboard ID315and import it into ourGrafanainstance.Select thePrometheusdata source we've configured earlier. Then clickImport.You will then be redirected to the dashboard and it should look like this:Yey\ud83c\udf89 We now have our first dashboard!Let's Create a Custom Dashboard for our Voting AppCopy thisJSONmodel and import it into our Grafana instance. This is similar to the steps above, but this time, instead of ID, we'll use theJSONfield to paste the copied template.Once imported, the dashboard should look like this:Here we have the visualization for basic metrics such ascpuandmemoryutilization for each components. Also,replica countandnode countwere part of the dashboard so we can check in later the behavior of vote-app component when it auto scale.Let's Test!If you haven't deployed thevoting-app, please refer to the command below:helm -n voting-app upgrade --install app -f workloads/helm/values.yaml thecloudspark/vote-app --create-namespaceEnter fullscreen modeExit fullscreen modeCustomize the namespacevoting-appand release nameappas needed, but update the dashboard query accordingly. I recommend to use the command above and use the same naming:voting-appfor namespace andappas the release name.Back to our dashboard: When thevote-apphas minimal load, it scales down to a single replica (1), as shown below.Horizontal Pod Autoscaling in ActionThevote-appdeployment has Horizontal Pod Autoscaler (HPA) configured with a maximum of five replicas. This means the voting app will automatically scale up to five pods to handle increased load. We can observe this behavior when we apply theseederdeployment.Now, let's test how thevote-apphandles increased load using aseederdeployment.apiVersion: apps/v1 kind: Deployment metadata:   name: seeder   namespace: voting-app spec:   replicas: 5 ...Enter fullscreen modeExit fullscreen modeTheseederdeployment simulates real user load by bombarding thevote-appwith vote requests. It has five replicas and allows you to specify the target endpoint using an environment variable. In this example, we'll target the Kubernetesservicedirectly instead of the load balancer....         env:         - name: VOTE_URL           value: \"http://app-vote.voting-app.svc.cluster.local/\" ...Enter fullscreen modeExit fullscreen modeTo apply, use the command below:kubectl apply -f workloads/seeder/seeder-app.yamlEnter fullscreen modeExit fullscreen modeAfter a few seconds, monitor your dashboard. You'll see thevote-appreplicas increase to handle the load generated by theseeder.D:\\> kubectl -n voting-app get hpa NAME                 REFERENCE                        TARGETS         MINPODS   MAXPODS   REPLICAS   AGE app-vote-hpa         Deployment/app-vote              cpu: 72%/80%   1         5         5          12mEnter fullscreen modeExit fullscreen modeSince thevote-appchart's default max value for the horizontal pod autoscaler (HPA) is five, we can see that the replica for this deployment stops at five.Stopping the Load and Scaling DownOnce you've observed the scaling behavior, delete theseederdeployment to stop the simulated load:kubectl delete -f workloads/seeder/seeder-app.yamlEnter fullscreen modeExit fullscreen modeGive the dashboard a few minutes and observe thevote-appscaling down. With no more load, the HPA will reduce replicas, down to a minimum of one. This may also lead to a node being decommissioned byKarpenterif pod scheduling becomes less demanding.You'll see that the\u00a0vote-app\u00a0eventually scales in as there is lesser load now. As you might see above, the node count also change from two to one - showing the power of Karpenter.PS D:\\> kubectl -n voting-app get hpa NAME                 REFERENCE                        TARGETS        MINPODS   MAXPODS   REPLICAS   AGE app-vote-hpa         Deployment/app-vote              cpu: 5%/80%    1         5         2          18mEnter fullscreen modeExit fullscreen modeChallenge: Scaling WorkloadsWe've successfully enabled autoscaling for thevote-appcomponent using Horizontal Pod Autoscaler (HPA). This is a powerful technique to manage resource utilization in Kubernetes. But HPA isn't limited to just one component.Tip:Explore theArtifactHub: Vote Appconfiguration in more detail. You'll find additional configurations related to HPA that you can leverage for other deployments.ConclusionYey! You've reached the end of theBack2Basics: Amazon EKS Series\ud83c\udf1f\ud83d\ude80. This series provided a foundational understanding of deploying and managing containerized applications on Amazon EKS. We covered:Provisioning an EKS cluster using OpenTofuDeploying workloads leveraging KarpenterMonitoring applications using Prometheus and GrafanaWhile Kubernetes can have a learning curve, hopefully, this series empowered you to take your first steps.Ready to level up?Let me know in the comments what Kubernetes topics you'd like to explore next!"}
{"title": "Going Pro", "published_at": 1719322512, "tags": ["career", "success", "achievement", "ambition"], "user": "Seth Orell", "url": "https://dev.to/aws-builders/going-pro-20m7", "details": "Take your career seriously.I've encountered many engineers who don't. They only think about improving from 9 to 5 at their job (and I know some who don't think about improving at all). A few are even explicit about it. They say \"Want me to get better at coding [or architecture or management]? Pay me to do it.\" Essentially, they are saying \"I'll only learn while I'm 'on the clock'.\" While it is true you can learn while earning a paycheck, should you only learn while getting paid? Is this the right attitude to take toward your career?I say no. Here is how I look at it. I will spend roughly one-third of my life doing \"work.\" I want that time to be exciting, challenging, and satisfying; I won\u2019t settle for \"punching a clock\" and counting the minutes until the weekend. On the contrary, I want a career that engages my mind so fully that I don't even notice that it's the weekend.In his recent book Effective Egoism, author Don Watkins states it this way: \"You have but one brief life, and the question you face is: What will you make of it? Will you go through the motions of living, and throw away your life doing what you're 'supposed' to do? Or will you set ambitious goals and do everything you can to make the most of your life? Will you betray your life--or honor it?\"This is what I mean by \"going pro.\" It's taking your life seriously and becoming great, potentially even world-class\"No one owes you a great career, you need to earn it--and the process won't be easy\"What does it take to be world-class?Being a professional takes effort. It takes time. It takes thought. And, if you don't choose to put in the effort/time/thought, you will not reach your professional potential.Imagine if Yo-Yo Ma, Buddy Rich, or Eddie Van Halen took the same approach as our engineer (above) and said \"I'm not going to get better at cello [or drums or guitar] unless I get paid.\" How successful do you think they'd be? Before they got good, who would have paid them for their time? Their talents were cultivated over years of exploration, experimentation, and practice, practice, practice.Or consider the absurdity of someone like Lionel Messi, Roger Clemens, or Muhammad Ali saying \"I'm not going to train unless I get paid.\" These world-class athletes spent thousands (tens of thousands) of hours on the pitch(or in the field or in the ring) before he went pro. Each of them would tell you that this preparation was critically necessary for him to become the star he is.Can you do it all?Could Yo-Yo Ma have become a professional ballet dancer and a world-class cellist? Could Lionel Messi have become a professional concert pianist and be the world's best football player1? No. While I don't take the \"Ten-Thousand-Hour Rule2\" literally, there is something real captured in the phrase.To achieve success, top success, for any endeavor that requires skill, you must invest time. And you only have so much time to spend. This requires you to choose how you spend your time. Should I go to a party with my friends or should I spend that time studying for my final exams? Should I read this new book on Software Architecture or play video games? You must choose. You cannot do it all. I think long and hard about what is in my long-term self-interest and place those values3at the top of my \"go-get-em\" list.I point this out because, to achieve greatness in one area, you will have to forgo greatness in another. You must choose where to invest your time. To be \"the greatest\"4boxer, Muhammad Ali could not have been simultaneously studying for the Bar Exam. To be the world's best footballer, Messi could not pursue a PhD in nuclear physics. You cannot do it all.The Craftsman MindsetBut it's not just time spent with the cello in your hands or a ball at your feet. It needs to be purposeful work specifically focused on the goal you are working toward. Tomorrow, it may be different. Next year, it almost certainly will be. This takes time, but the benefits come from carefully considering how you spend that time. The benefits come from thinking, from using your mind.Cal Newport's 2012 book, So Good They Can't Ignore You, refers to this as the \"craftsman mindset\" and it separates the great from the mediocre. Cal says \"There's something liberating about the craftsman mindset: It asks you to leave behind ... concerns about whether your job is 'just right,' and instead put your head down and plug away at getting really damn good. No one owes you a great career, it argues; you need to earn it--and the process won't be easy.\"5This is why just putting in 10,000 hours may not help you achieve your goal. It has to be quality time focused on your particular challenges.What Can You Do?I'm writing this article primarily for those in software technology, although everything I say here applies to being the best musician, athlete, novelist, or parent. Ultimately, everything comes down to your choices and your mind.ThinkThe first step to take is to stop for a minute and think. Ask yourself \"Am I working with the right technologies?\" \"Am I learning the right things?\" \"Am I advancing my career?\" Then ask yourself \"What actions am I taking to achieve these goals?\" Use these answers to shape your next steps.ReadAdd something small to your routine. Perhaps you can commit to finding one interesting technical article or blog post to read every day. Ask yourself \"What was the last book on technology I read?\" Try adding a goal to read a tech or leadership book every month. That's less than a chapter a day for most of them. This will give you more raw material to use as you think about your career.WritePut some thoughts down in writing. It can be a Moleskine notebook or a computer application (I'm writing this down with Obsidian6). The very act of thinking \"This is so important, I should write it down\" is crucial. You are starting to make choices, recognizing that some ideas are more valuable than others.RepeatPeriodically, go back to the \"Think\" step and reflect. Think about where you are and where you want to be. Factor in all the knowledge you've gained and adjust your routine accordingly.SummaryWhen someone says \"Want me to be a better coder? Pay me to do it\", he has just told you \"I will never be great at coding.\" If you take your career seriously and want to be the best you can be, you cannot afford to take this attitude. Get good. Live your life to the fullest. Have fun.Further ReadingOwnership Matters:What for? Owning Your CareerDon Watkins:Effective EgoismCal Newport:So Good They Can't Ignore YouMalcolm Gladwell:Outliershttps://en.wikipedia.org/wiki/Ballon_d'Or#Winners\u21a9https://www.newyorker.com/sports/sporting-scene/complexity-and-the-ten-thousand-hour-rule\u21a9I wrote about valuing and career in aprevious post\u21a9https://www.espn.com/boxing/story/_/id/15930888/muhammad-ali-10-best-quotes\u21a9Newport, Cal. So Good They Can't Ignore You: Why Skills Trump Passion in the Quest for Work You Love (p. 38). Grand Central Publishing. Kindle Edition.\u21a9https://obsidian.md/\u21a9"}
{"title": "Spring Boot 3 application on AWS Lambda - Part 8 Introduction to Spring Cloud Function", "published_at": 1719241919, "tags": ["java", "springboot", "aws", "serverless"], "user": "Vadym Kazulkin", "url": "https://dev.to/aws-builders/spring-boot-3-application-on-aws-lambda-part-8-introduction-to-spring-cloud-function-99a", "details": "During the parts2,3and4we introduced the concept behind the AWS Serverless Java Container and especially its variant for the Spring Boot (3) application, learned how to develop, deploy, run and optimize Spring Boot 3 Application on AWS Lambda using AWS Serverless Java Container. In the parts5,6and7we did the same but for AWS Lambda Web Adapter. In this part of the series we'll introduce another alternative, the framework calledSpring Cloud Function, concepts behind it and especially its integration with AWS Lambda.Spring Cloud FunctionSpring Cloud Function is a project with the following high-level goals:Promote the implementation of business logic via functions.Decouple the development lifecycle of business logic from any specific runtime target so that the same code can run as a web endpoint, a stream processor, or a task. One of these specific runtime targets can be AWS Lambda which will be focus of this article.Support a uniform programming model across serverless providers, as well as the ability to run standalone (locally or in a PaaS).Enable Spring Boot features (auto-configuration, dependency injection, metrics) on serverless providers.It abstracts away all of the transport details and infrastructure, allowing the developer to keep all the familiar tools and processes, and focus firmly on business logic.A simple function application (in context or Spring) is an application that contains beans of type Supplier,Java 8 Function interfaceor Consumer.Spring Cloud Function on AWS LambdaWithSpring Cloud Function on AWS Lambda,AWS adapter takes a Spring Cloud Functionapp and converts it to a form that can run in AWS Lambda. So, with AWS it means that a simple function bean should somehow be recognized and executed in AWS Lambda environment.  Which very well fits into the AWS Lambda model with Amazon API Gateway in front which similar to the Java 8 function receives the (HTTP) request, executes some business logic and then sends the (HTTP) response to the caller.@SpringBootApplication public class FunctionConfiguration {  \u2002\u2002\u2002\u2002\u2002\u2002public static void main(String[] args) { \u2002\u2002\u2002\u2002\u2002\u2002\u2002\u2002\u2002\u2002\u2002\u2002SpringApplication.run(FunctionConfiguration.class, args); \u2002\u2002\u2002\u2002\u2002\u2002}  \u2002\u2002\u2002\u2002\u2002\u2002@Bean \u2002\u2002\u2002\u2002\u2002\u2002public Function<String, String> uppercase() { \u2002\u2002\u2002\u2002\u2002\u2002\u2002\u2002\u2002\u2002\u2002\u2002return value -> value.toUpperCase(); \u2002\u2002\u2002\u2002\u2002\u2002} }Enter fullscreen modeExit fullscreen modeThe diagram below describes the request flow in the typical web application on AWS. AWS Request Adapter converts the JSON coming from Lambda function to theHttpServletRequestwhich then invokes theSpring Dispatcher Servletwhich then interacts with our Spring Boot application on API level without starting web server (i.e. Tomcat). Then response flows back and AWS Response Adapter convertsHttpServletResponseto JSON which Lambda function sends back to API Gateway.ConclusionIn this part of the series, we introduced Spring Cloud Function and especially its integration with AWS Lambda and concepts behind it. In next part we'll learn how to develop and deploy application using this framework."}
{"title": "Amazon EC2 or Amazon RDS, when to choose?", "published_at": 1719235157, "tags": ["ec2", "rds", "aws", "communitybuilder"], "user": "Carlos Filho", "url": "https://dev.to/aws-builders/amazon-ec2-or-amazon-rds-when-to-choose-25o9", "details": "The video is in Portuguese (BR), but I'll leave the content below in English of the same thing I covered in the video! I hope you enjoy it.Today I will discuss two important AWS services: Amazon EC2 and Amazon RDS. I'll explain when you should choose one or the other, with tips to help you make the best decision for your cloud projects. Let's go!Amazon EC2Amazon EC2 (Elastic Compute Cloud) is a service that provides scalable computing capacity in the cloud. It is ideal for:Flexibility and control -> If you need total control over the operating system, software, and execution environment, EC2 is the right choice. You can install, configure, and manage everything according to your needs.Customized applications -> Use EC2 when your applications require a highly customized environment that cannot be achieved with managed services. Examples include game servers, custom development environments, and legacy applications.Variable workloads -> If your workloads are variable and unpredictable, EC2 offers the flexibility to adjust computing capacity quickly. This is useful for e-commerce sites with seasonal peaks, marketing campaigns, and product launches.High performance -> For applications that require high performance, such as scientific computing, large-scale data analysis, and machine learning, EC2 instances optimized for computing are ideal.Amazon RDSAmazon RDS (Relational Database Service) is a managed service that facilitates the configuration, operation, and scalability of relational databases in the cloud. Choose RDS for:Simple management -> RDS automates time-consuming tasks such as hardware provisioning, database configuration, patching, and backups. It's perfect for companies that want to focus on application development rather than infrastructure management.High availability -> RDS offers high availability and disaster recovery options with Multi-AZ replicas, which replicate your data in multiple availability zones. Ideal for critical applications that cannot afford downtime.Automatic scalability -> RDS allows you to easily adjust storage and computing capacity according to your business needs. It's great for applications that need to grow quickly without the complexity of managing the underlying hardware.Security and compliance: RDS integrates with AWS Identity and Access Management (IAM) and offers encryption at rest and in transit, meeting strict compliance and security requirements. Perfect for financial, healthcare, and other applications that handle sensitive data.Examples of useEC2 -> A startup developing a new online game can use EC2 to create game servers that need a highly customized environment and total control over configuration.RDS -> An e-commerce store can use RDS to manage its database of products and transactions, taking advantage of automatic scalability and high availability.In summary, use Amazon EC2 when you need total control and flexibility over your computing environment, and choose Amazon RDS to simplify database management, guaranteeing high availability and scalability. I hope these tips have helped you better understand when to use each of these services."}
{"title": "How to Cloud: IaC", "published_at": 1719234000, "tags": ["aws", "beginners", "terraform"], "user": "Joseph", "url": "https://dev.to/aws-builders/how-to-cloud-iac-3ifl", "details": "In my last post I intentionally jumped the gun a little bit when it came to deploying containers into your cloud account. It should be your first priority to get your team to learn containerization, but deploying to the cloud requires allocating container registry infrastructure. That brings us to our next topic which is Infrastructure as Code, and it\u2019s a very close second in priority.Don\u2019t do it manuallyDo not, under any circumstances, try to allocate infrastructure in your account manually through the console. I have tried to think of any situation where it makes sense to just use the console, but there really just isn\u2019t one unless you are truly just toying around. Any product you intend to build and make money on, you can\u2019t afford the windfall of problems you will encounter if you go down that path. Allow me to highlight a couple of these just to give you an idea.Your team will forget what they were doingIt may seem like it\u2019s faster to just jump into the console and allocate something, especially like a container registry, really quickly and just get it over with. After all, it\u2019s so easy. 6 months later, your team has forgotten how many things they allocated manually, and they don\u2019t remember where to find them, and all of the sudden you are going to have to build it all over again because it\u2019s actually lost in your account and there\u2019s too many other things in your account to find the one thing you need to find. Having your infrastructure written in code completely solves this problem.Fear of making changesEven if your team can find the infrastructure they made 6 months ago, they will be terrified to change it, because they won\u2019t be able to easily tell what depends on it, and how it depends on it. Their solution will just be to make another one while keeping the original around for legacy reasons, and your cloud bill cost will suffer the consequences.Speed of DeliveryThe reality is that, if you are using the console, you think you are going faster, but you are actually going at a minimum 4 times slower than the way I do it. I know this from experience. I\u2019ve watched other teams that did it manually because they were convinced they could get it done quicker. They took a month and had nothing to show for it because everything kept breaking, and my team came in and got their entire account working from scratch in 1 week using scripts. Allocating an entire compute cluster stack for my teams today takes less than an hour, from start to finish. I\u2019ve never seen a team allocate a web service on any compute cluster cloud product in the console in that time frame. You think you\u2019re going faster in the console because of an illusion. That illusion is the self gratifying feedback loop that the console gives you from seeing a little bit of progress every 10 minutes or so. It is a complete falsehood.Knowing what\u2019s necessary and what\u2019s flavorBefore I get into the next steps for IaC, I want to take a second and point something out that\u2019s really important for decision making as a leader. You need to be able to tell the difference between what is the right call vs what\u2019s just your own preference or the team\u2019s preference. What I said earlier about manual vs scripting? That\u2019s the right call. You want to make sure that\u2019s something you handle at your level as a leader. Which tech stack you choose to do the scripting? That\u2019s preference. You want to help facilitate your team into making the best decision here, but I like to leave this area up to my teams, collectively. Empower them to do this for themselves and work together on this decision, not in silos for those of you that manage multiple teams. There are, of course, pros and cons to what you choose, and some situations will call for one solution more than another, but you can be successful using a lot of different tech stacks.In the realm of IaC you have a few options. I\u2019ll cover the main points how I see them today and explain why I choose what I choose, but that doesn\u2019t mean it\u2019s going to be the right choice for your team. This is the preference part. You need to choose what\u2019s right for your situation, and you need to get good at making that choice quickly.AWS CLIThe OG for AWS IaC scripting. It was the best and most comprehensive solution for allocating AWS infrastructure for a long time. It isn\u2019t hosted inside AWS, however, because it\u2019s just a cli you run yourself, but it can be extremely versatile. It used to be that AWS would roll out commands to the cli before any other IaC solution so you had to leverage it for the newly released tech on your account.AWS CloudFormationThe first true IaC language developed by AWS. It\u2019s declarative, runs inside your AWS account which is great for tracking and maintainability. CF was always lagging with features in the past because the cli would get them first, but AWS has done some work to try to close that gap so it may not be that much of an issue anymore. Being declarative has it\u2019s drawbacks, though.Azure ARMFor those of you that use Azure, ARM is the equivalent tech for that cloud vendor.AWS CDKWhich is why AWS made a new language for IaC called AWS CDK. This is the next evolution in AWS\u2019s offering for IaC. It is programmatic, getting away from being declarative, which allows for much more flexibility in defining your infrastructure. It is, however, still vendor locked into only leveraging AWS cloud technology. If you are 100% using only AWS Cloud offerings, then any of AWS\u2019s options should be fine, including CDK, but before you make that choice let\u2019s talk about what I mean by 100%.TerraformTerraform offers two varieties of IaC, one declarative and one programmatic, but the main distinction that Terraform has over the AWS offerings above is that it\u2019s built to be multi-cloud, multi-provider. Let\u2019s talk about what multi-provider actually means.Most people getting into the cloud and learning about IaC will latch onto the idea of scripting out whatever they are building in their cloud account of choice pretty quickly. It\u2019s a pretty natural process. What will happen to a lot of teams, however, is that you will come to realize that you actually purchase or own licenses for all kinds of other services you need to build the product your team is focused on. Things like\u2026. maybe you use fastly as a cdn provider, MongoDB Atlas for a database solution, or datadog as your observability platform. Now, you may be asking yourself, why do any of those things matter? Well, with terraform, all of those products are connected in the terraform ecosystem as providers. So you can script one IaC solution together that ties all of those things, including your cloud account infrastructure, all in one place. So when I was talking earlier about 100%, I was talking about everything your team uses.Point of note, I didn\u2019t really bring up GCP anywhere in this discussion, but that\u2019s because they leverage APIs for a lot of their allocation, and those APIs feed into terraform providers so if you are in GCP land you are probably looking to adopt terraform. In a lot of ways, the same may be true for folks looking to adopt Azure, even though they have their own templating stack. Terraform is a choice my teams tend to make these days because of all the flexibility I mentioned before. It tends to be the selection of choice, although I have managed teams in the past that used AWS CloudFormation extremely successfully.Getting backOk so getting back to the example we started, let\u2019s now talk about how you actually create that infrastructure you need for you container registry. My teams use Terraform CDK so my examples will be with that, but our approach translates well to whatever choice you make. And again, I\u2019m not going to get into the details of how to do all things terraform, rather I want to point out some key decisions that you want to make when you start building out your IaC.Organizing your stackWhen we start this build out you are going to want to start organizing where you put your scripts, and from my experience you want to immediately split up infrastructure you make for your account, and what you make for the product you are deploying, and their subsequent environments. Things you need to do in your account would be things like allocating your VPC, networking, domain management, stuff that you don\u2019t normally handle on a product by product basis. For this example I\u2019m going to actually put my ECR allocation at the account level because I\u2019m going into this with the mindset that the overall stack here is going to be rather small, so allocating an ECR for each product is probably overkill. Think the difference between a start-up vs an enterprise corporate team. This stack is more of a start-up scene right now, whereas in a more enterprise environment you might allocate an ECR for each product, or maybe each team.This is what allocating the ECR looks like right now in my repo, it\u2019s nothing fancy:import{Construct}from\"constructs\";import{App,TerraformStack,TerraformOutput}from\"cdktf\";import{AwsProvider}from\"@cdktf/provider-aws/lib/provider\";import{EcrRepository}from\"@cdktf/provider-aws/lib/ecr-repository\";classMyStackextendsTerraformStack{constructor(scope:Construct,name:string){super(scope,name);newAwsProvider(this,\"AWS\",{region:\"us-east-1\",});newEcrRepository(this,\"how-to-cloud\",{name:\"how-to-cloud-ecr\",});newTerraformOutput(this,\"testing\",{value:\"hello world\",});}}constapp=newApp();newMyStack(app,\"how-to-cloud\");app.synth();Enter fullscreen modeExit fullscreen modeThe code is actually way less important here than the folder structure I\u2019ve started. We started with setting up docker and we have that over in a docker folder, and now that we\u2019re allocating infrastructure we put that over in an iac folder, with an account folder inside that just for the account stuff. Later we\u2019ll make more scripts in product folders that are separate from the account, so building out new infrastructure will be really easy.But that\u2019s for a later post."}
{"title": "Amazon GuardDuty Malware Protection for Amazon S3", "published_at": 1719224820, "tags": ["aws", "security"], "user": "Mark Laszlo", "url": "https://dev.to/aws-builders/amazon-guardduty-malware-protection-for-amazon-s3-2oe1", "details": "Amazon GuardDuty Malware Protection for Amazon S3is a feature that automatically scans newly uploaded objects in S3 buckets for potential malware. This service provides a seamless, scalable solution to enhance security within AWS environments, particularly focusing on preventing the ingress of malicious files.Key FeaturesAutomated Malware Detection:GuardDuty Malware Protection for S3 scans new objects or new versions of existing objects as they are uploaded to your S3 buckets. This automated process ensures that any potential malware is detected in real-time, mitigating risks before the files are accessed or processed downstream.Event-Driven Architecture:The service uses an event-driven approach, which means that every time an object is uploaded to a bucket or a new version is added, a malware scan is automatically initiated. This timely detection mechanism is crucial for maintaining security without manual intervention.Scanning Scope:GuardDuty Malware Protection for S3 focuses on newly uploaded objects. It does not retroactively scan existing objects in a bucket prior to the feature being enabled. If there is a need to scan existing objects, they must be re-uploaded to trigger the scan process.Operational Simplicity and Scalability:By being fully managed by AWS, this feature alleviates the need for customers to maintain their own scanning infrastructure. This reduces operational complexity and ensures that scanning operations do not impact the performance and scalability of S3 operations.Integration with AWS Services:Results from the malware scans can be integrated with Amazon EventBridge and Amazon CloudWatch. This enables automated workflows such as tagging, quarantine, or notification setups based on scan results. However, currently, the Malware Protection for S3 finding typedoes not integrate with AWS Security Hub and Amazon Detective.Getting Started and UsageTo enable GuardDuty Malware Protection for S3:Configure the feature through the GuardDuty console.Select the specific S3 buckets to protect and set up necessary permissions through AWS Identity and Access Management (IAM).Choose whether to scan all objects in a bucket or only those with a specific prefix.Configure post-scan actions like tagging objects based on their scan status.Organizational-Level ControlsCurrently, there are no direct organizational-level controls to enable malware protection for all buckets simultaneously. Each bucket must be enabled individually. Furthermore, delegated GuardDuty administrators cannot enable this feature on buckets belonging to member accounts.Security Findings and NotificationsDetailed security findings are generated for each scanned object, categorizing them based on the presence of threats. These findings are visible in the GuardDuty console and can trigger automated responses through EventBridge, ensuring timely handling of detected threats.PricingThe pricing for GuardDuty Malware Protection for S3 is based on thevolume of data scanned and the number of objects evaluated. AWS offers a limited free tier that includes 1,000 requests and 1 GB of scanned data per month for the first year or until June 11, 2025, for existing accounts."}
{"title": "Issue 49 and 50 of AWS Cloud Security Weekly", "published_at": 1719155464, "tags": [], "user": "AJ", "url": "https://dev.to/aws-builders/issue-49-and-50-of-aws-cloud-security-weekly-f3n", "details": "(This is just the highlight of Issue 49 and 50 of AWS Cloud Security weekly @https://aws-cloudsec.com/p/issue-49-and-50<< Subscribe to receive the full version in your inbox weekly for free!!).What happened in AWS CloudSecurity & CyberSecurity last week June 10-June 20, 2024?IAM Access Analyzer now provides actionable recommendations to assist you in addressing unused access. For roles, access keys, and passwords that are not in use, IAM Access Analyzer offers convenient console links to facilitate their deletion. Regarding unused permissions, IAM Access Analyzer evaluates your current policies and suggests refined versions customized to your access patterns.AWS has launched Amazon GuardDuty Malware Protection for Amazon S3 which enables scanning of newly uploaded objects to Amazon S3 buckets for potential malware, viruses, and suspicious uploads so that you can action to isolate these objects before they impact downstream processes.AWS Private Certificate Authority (AWS Private CA) introduces the Connector for SCEP, enabling secure and scalable enrollment of mobile devices using a managed cloud certificate authority (CA). Simple Certificate Enrollment Protocol (SCEP) is widely adopted by mobile device management (MDM) solutions for obtaining digital identity certificates from a CA and enrolling both corporate-issued and bring-your-own-device (BYOD) mobile devices. With the Connector for SCEP, organizations can leverage a managed private CA and SCEP solution to streamline operations, reduce costs, and optimize their public key infrastructure (PKI). Furthermore, this connector allows integration of AWS Private CA with leading SCEP-compatible MDM solutions such as Microsoft Intune and Jamf Pro.AWS Identity and Access Management (IAM) now introduces passkeys for multi-factor authentication. Built on FIDO standards and utilizing public key cryptography, passkeys provide robust authentication that is resistant to phishing attacks, surpassing traditional password security measures. The support is compatible with built-in authenticators such as Touch ID on Apple MacBooks and facial recognition via Windows Hello on PCs. Passkeys can be generated using a hardware security key or through a chosen passkey provider, utilizing methods like fingerprint, facial recognition, or device PIN.Amazon EKS has released the Pod Identity agent as open source that you can package and deploy the agent within EKS clusters. Pod Identity is a feature designed to streamline the configuration of Kubernetes applications with AWS IAM permissions for cluster administrators. To leverage the Pod Identity feature, it is necessary to run the Pod Identity agent on the worker nodes of the cluster. By open sourcing the Pod Identity agent, users now have the ability to independently build the agent. This grants a range of options for packaging and deploying the agent, allowing alignment with organizational deployment practices.AWS KMS has introduced support for Elliptic Curve Diffie-Hellman (ECDH) key agreement. This feature enables two parties to establish a shared secret securely over a public channel. With ECDH in AWS KMS, you can use another party's public key along with your own elliptic-curve KMS key hosted within the FIPS 140-2 validated hardware security module (HSM) of AWS Key Management Service (KMS) to derive this shared secret. Subsequently, the shared secret can be utilized to derive a symmetric key for encrypting and decrypting data between the parties using a symmetric encryption algorithm within your application.AWS introduced natural language query generation powered by generative AI in AWS CloudTrail Lake (preview) which equips you to analyze AWS activity events without needing to write intricate SQL queries and just simply ask questions in plain English. (Note: I did have some errors at times- \"Query generator failed to generate a query. A valid SQL statement could not be generated using the given prompt. Reword your prompt and try again\u201d and this feature is in early phase so you should double check the generated SQL queries to make sure it\u2019s generating what you are investigating.)Trending on the news & advisories (Subscribe to the newsletter for details):Panera disclosed security incident.Advanced Auto Parts confirms data breach."}
{"title": "AWS: Karpenter and SSH for Kubernetes WorkerNodes", "published_at": 1719153396, "tags": ["security", "aws", "devops", "kubernetes"], "user": "Arseny Zinchenko", "url": "https://dev.to/aws-builders/aws-karpenter-and-ssh-for-kubernetes-workernodes-16km", "details": "We have an AWS EKS cluster with WorkerNodes/EC2 created with Karpenter.The process of creating the infrastructure, cluster, and launching Karpenter is described in previous posts:Terraform: Building EKS, part 1\u200a\u2014\u200aVPC, Subnets and EndpointsTerraform: Building EKS, part 2\u200a\u2014\u200aan EKS cluster, WorkerNodes, and IAMTerraform: Building EKS, part 3\u200a\u2014\u200aKarpenter installationWhat this system really lacks, is access to servers via SSH, without which you feel like\u2026 Well, like DevOps, not Infrastructure Engineer. In short, SSH access is sometimes necessary, but\u200a\u2014\u200asurprise\u200a\u2014\u200aKarpenter does not allow you to add a key to the WorkerNodes it manages out of the box.Although, what\u2019s the problem to add in theEC2NodeClassa way to pass an SSH key, as it is done in the Terraform'sresource \"aws_instance\" with the parameterkey_name?But it\u2019s okay. If it\u2019s not there, it\u2019s not there. Maybe they will add it later.Instead, Karpenter\u2019s documentationCan I add SSH keys to a NodePool?suggests using eitherAWS Systems Manager Session ManagerorAWS EC2 Instance Connect, or \u201cthe old school way\u201d\u200a\u2014\u200aadd the public part of the key viaAWS EC2 User Data, and connect via a bastion host or a VPN.So what we\u2019re going to do today is:try all three solutions one by onefirst we\u2019ll do each one by hand, then we\u2019ll see how to add it to our automation with Terraformand then we\u2019ll decide which option will be the easiestOption 1: AWS Systems Manager Session Manager and SSH on EC2AWS Systems Manager Session Manager is used to manage EC2 instances. In general, it can do quite a lot, for example, keep track of patches and updates for packages that are installed on instances.Currently, we are only interested in it as a system that will allow us to have an SSH to a Kubernetes WorkerNode.It requires an SSM agent, which is installed by default on all instances with Amazon Linux AMI.Find nodes created by Karpenter (we have a dedicated label for them):$ kubectl get node -l created-by=karpenter NAME STATUS ROLES AGE VERSION ip-10-0-34-239.ec2.internal Ready <none> 21h v1.28.8-eks-ae9a62a ip-10-0-35-100.ec2.internal Ready <none> 9m28s v1.28.8-eks-ae9a62a ip-10-0-39-0.ec2.internal Ready <none> 78m v1.28.8-eks-ae9a62a ...Enter fullscreen modeExit fullscreen modeFind an Instance ID:$ kubectl get node ip-10-0-34-239.ec2.internal -o json | jq -r \".spec.providerID\" | cut -d \\/ -f5  i-011b1c0b5857b0d92Enter fullscreen modeExit fullscreen modeAWS CLI: TargetNotConnected when calling the StartSession operationTry to connect and get the error \u201cTargetNotConnected\u201d:$ aws --profile work ssm start-session --target i-011b1c0b5857b0d92  An error occurred (TargetNotConnected) when calling the StartSession operation: i-011b1c0b5857b0d92 is not connected.Enter fullscreen modeExit fullscreen modeOr through the AWS Console:But here, too, we have the connection error\u200a\u2014\u200a\u201cSSM Agent is not online\u201d:The error occurs because:either the IAM role that is connected to the instance does not have SSM permissionsor EC2 is running on a private network and the agent cannot connect to an external endpointSessionManager and IAM PolicyLet\u2019s check. Find the IAM Role attached to this EC2:And the policies connected to it\u200a\u2014\u200athere is nothing about SSM:Edit the Role by hand for now, then we will do it in Terraform code:Attach theAmazonSSMManagedInstanceCore:And in a minute or two, try again:SessionManager and VPC EndpointAnother possible reason for the problems connecting the SSM agent to AWS is that the instance does not have access to SSM endpoints:ssm.region.amazonaws.comssmmessages.region.amazonaws.comec2messages.region.amazonaws.comIf the subnet is private, and has limits on external connections, then you may need to create aVPC Endpointfor SSM.SeeSSM Agent is not onlineandTroubleshooting Session Manager.AWS CLI: SessionManagerPlugin is not foundHowever, after the IAM fix, when connecting from a workstation using the AWS CLI, we can get the \u201cSessionManagerPlugin is not found\u201d error:$ aws --profile work ssm start-session --target i-011b1c0b5857b0d92 SessionManagerPlugin is not found. Please refer to SessionManager Documentation here: http://docs.aws.amazon.com/console/systems-manager/session-manager-plugin-not-foundEnter fullscreen modeExit fullscreen modeInstall it locally\u200a\u2014\u200asee the documentationInstalling the Session Manager Plugin for the AWS CLI.For Arch Linux there is aaws-session-manager-pluginpackage in AUR:$ yay -S aws-session-manager-pluginEnter fullscreen modeExit fullscreen modeAnd now we can connect:$ aws --profile work ssm start-session --target i-011b1c0b5857b0d92 Starting session with SessionId: arseny-33ahofrlx7bwlecul2mkvq46gy sh-4.2$Enter fullscreen modeExit fullscreen modeAll that\u2019s left is to add it to the automation.Terraform: EKS module, and adding an IAM PolicyFor the Terraform EKS module from Anton Babenko, we can add a policy through theiam_role_additional_policiesparameter - see thenode_groups.tf, and in the examples of theAWS EKS Terraform module.In the 20.0 module, the parameter name has changed\u200a\u2014iam_role_additional_policies=>node_iam_role_additional_policies, but we are still using version 19.21.0, and the role is added in this way:... module \"eks\" {   source = \"terraform-aws-modules/eks/aws\"   version = \"~> 19.21.0\"   cluster_name = local.env_name   cluster_version = var.eks_version   ...   vpc_id = local.vpc_out.vpc_id   subnet_ids = data.aws_subnets.private.ids   control_plane_subnet_ids = data.aws_subnets.intra.ids   manage_aws_auth_configmap = true    eks_managed_node_groups = {       ...       # allow SSM       iam_role_additional_policies = {         AmazonSSMManagedInstanceCore = \"arn:aws:iam::aws:policy/AmazonSSMManagedInstanceCore\"       } ...Enter fullscreen modeExit fullscreen modeRemove what we did manually, deploy the Terraform code, and check that the Policy has been added:And the connection is working:$ aws --profile work ssm start-session --target i-011b1c0b5857b0d92 Starting session with SessionId: arseny-pt7d44xp6ibvqcezj2oqjaxv5q sh-4.2$ bash [ssm-user@ip-10-0-34-239 bin]$ pwd /usr/binEnter fullscreen modeExit fullscreen modeOption 2: AWS EC2 Instance Connect and SSH on EC2Another way to connect is through EC2 Instance Connect. Documentation\u200a\u2014Connect to your Linux instance with EC2 Instance Connect.It also requires an agent, which is also installed by default on the Amazon Linux.For instances on private networks, EC2 Instance Connect VPC Endpoint is required for connection.SecurityGroup and SSHInstance Connect through the endpoint requires access to port 22, SSH (as opposed to SSM, which opens a connection through the agent itself).Open the port for all addresses in the VPC:EC2 Instance Connect VPC EndpointGo to the VPC Endpoints and create an endpoint:Select theEC2 Instance Connect Endpointtype, the VPC itself, and the SecurityGroup:Choose a Subnet\u200a\u2014\u200awe have most of the resources inus-east-1a, so we\u2019ll use it to avoid unnecessary cross-AvailabilityZone traffic (seeAWS: Cost optimization\u200a\u2014\u200aservices expenses overview and traffic costs in AWS):Wait a few minutes for the Active status:And connect using AWS CLI by specifying--connection-type eice, because the instances are on a private network:$ aws --profile work ec2-instance-connect ssh --instance-id i-011b1c0b5857b0d92 --connection-type eice ... [ec2-user@ip-10-0-34-239 ~]$Enter fullscreen modeExit fullscreen modeTerraform: EC2 Instance Connect, EKS, and VPCFor the Terraform, here you will need to add thenode_security_group_additional_rulesin theEKS modulefor SSH access, and create an EC2 Instance Connect Endpoint for the VPC, as in my case we create VPC and EKS separately.... module \"eks\" {   source = \"terraform-aws-modules/eks/aws\"   version = \"~> 19.21.0\"    cluster_name = local.env_name   cluster_version = var.eks_version    ...       # allow SSM       iam_role_additional_policies = {         AmazonSSMManagedInstanceCore = \"arn:aws:iam::aws:policy/AmazonSSMManagedInstanceCore\"       }       ...   }    node_security_group_name = \"${local.env_name}-node-sg\"   cluster_security_group_name = \"${local.env_name}-cluster-sg\"    # to use with EC2 Instance Connect   node_security_group_additional_rules = {     ingress_ssh_vpc = {       description = \"SSH from VPC\"       protocol = \"tcp\"       from_port = 22       to_port = 22       cidr_blocks = [local.vpc_out.vpc_cidr]       type = \"ingress\"     }   }    node_security_group_tags = {     \"karpenter.sh/discovery\" = local.env_name   }   ... } ...Enter fullscreen modeExit fullscreen modeIf you created it manually, as described above, then remove the rule from SecurityGroup with SSH and deploy it from Terraform.For the VPC EC2 Instance Connect Endpoint, I did not find how to do this through Anton Babenko\u2019s moduleterraform-aws-modules/vpc, but you can make it a separate resource throughaws_ec2_instance_connect_endpoint:resource \"aws_ec2_instance_connect_endpoint\" \"example\" {   subnet_id = module.vpc.private_subnets[0]   security_group_ids = [\"sg-0b70cfd6019c635af\"] }Enter fullscreen modeExit fullscreen modeHowever, here you need to pass the SecurityGroup ID from the cluster, and the cluster is created after the VPC, so there is a chicken-and-egg problem.In general, the Instance Connect seems to be a little more complicated than SSM in the automation, because there are more changes in the code, and in different modules.However, it is a working option, and if your automation allows it, you can use it.Option 3: the old-fashioned way with SSH Public Key via EC2 User DataAnd the oldest and perhaps the simplest option is to create an SSH key yourself and add its public part to EC2 when creating an instance.The disadvantages here are that it will be difficult to add many keys in this way, and EC2 User Data can sometimes go sideways, but if you need to add only one key, a kind of \u201csuper-admin\u201d in case of emergency, then this is a perfectly valid option.Moreover, if you have a VPN to the VPC (seePritunl: Launching a VPN in AWS on EC2 with Terraform), then the connection will be even easier.Create a key:$ ssh-keygen  Generating public/private ed25519 key pair. Enter file in which to save the key (/home/setevoy/.ssh/id_ed25519): /home/setevoy/.ssh/atlas-eks-ec2 Enter passphrase (empty for no passphrase):  Enter same passphrase again:  Your identification has been saved in /home/setevoy/.ssh/atlas-eks-ec2 Your public key has been saved in /home/setevoy/.ssh/atlas-eks-ec2.pub ...Enter fullscreen modeExit fullscreen modeThe public part can be stored in a repository\u200a\u2014\u200acopy it:$ cat ~/.ssh/atlas-eks-ec2.pub  ssh-ed25519 AAA***VMO setevoy@setevoy-wrk-laptopEnter fullscreen modeExit fullscreen modeNext, a few crutches: theEC2NodeClassin my case is created from the Terraform code through thekubectl_manifestresource. The easiest option that has come to mind so far is to add the public key to thevariables, and then use it in thekubectl_manifest.Later, I will probably move such resources to a dedicated Helm chart.For now, let\u2019s create a new variable:variable \"karpenter_nodeclass_ssh\" {   type = string   default = \"ssh-ed25519 AAA***VMO setevoy@setevoy-wrk-laptop\"   description = \"SSH Public key for EC2 created by Karpenter\" }Enter fullscreen modeExit fullscreen modeIn theEC2NodeClassconfiguration, add thespec.userData:resource \"kubectl_manifest\" \"karpenter_node_class\" {   yaml_body = <<-YAML     apiVersion: karpenter.k8s.aws/v1beta1     kind: EC2NodeClass     metadata:       name: default     spec:       amiFamily: AL2       role: ${module.eks.eks_managed_node_groups[\"${local.env_name_short}-default\"].iam_role_name}       subnetSelectorTerms:         - tags:             karpenter.sh/discovery: \"atlas-vpc-${var.environment}-private\"       securityGroupSelectorTerms:         - tags:             karpenter.sh/discovery: ${local.env_name}       tags:         Name: ${local.env_name_short}-karpenter         environment: ${var.environment}         created-by: \"karpneter\"         karpenter.sh/discovery: ${local.env_name}       userData: |         #!/bin/bash         mkdir -p ~ec2-user/.ssh/         touch ~ec2-user/.ssh/authorized_keys         echo \"${var.karpenter_nodeclass_ssh}\" >> ~ec2-user/.ssh/authorized_keys         chmod -R go-w ~ec2-user/.ssh/authorized_keys         chown -R ec2-user ~ec2-user/.ssh          YAML    depends_on = [     helm_release.karpenter   ] }Enter fullscreen modeExit fullscreen modeIf you are usingnotAmazon Linux, then change theec2-userto the desired one.Important Note: Keep in mind that changes to EC2NodeClass will recreate all instances, and that your services are configured for stable operation, seeKubernetes: Providing High Availability for Pods.Deploy it and check it:$ kk get ec2nodeclass -o yaml ...     userData: #!/bin/bash\\nmkdir -p ~ec2-user/.ssh/\\ntouch ~ec2-user/.ssh/authorized_keys\\necho       \\\"ssh-ed25519 AAA***VMO setevoy@setevoy-wrk-laptop\\\" >> ~ec2-user/.ssh/authorized_keys\\nchmod -R go-w       ~ec2-user/.ssh/authorized_keys\\nchown -R ec2-user ~ec2-user/.ssh \\n ...Enter fullscreen modeExit fullscreen modeWait for Karpenter to create a new WorkerNode and try SSH:$ ssh -i ~/.ssh/hOS/atlas-eks-ec2 ec2-user@10.0.39.73 ... [ec2-user@ip-10-0-39-73 ~]$Enter fullscreen modeExit fullscreen modeDone.Conclusions.AWS SessionManager: looks like the easiest option in terms of automation, recommended by AWS itself, but you need to think about how to usescpthrough it (although it seems to be possible through additional moves, see.SSH and SCP with AWS SSM)AWS EC2 Instance Connect: a cool feature from Amazon, but somehow more troublesome to automate, so not our option\u201cgrandfathered\u201d SSH: well, the old one is tried and true :-) but I don\u2019t really like User Data, because sometimes it can lead to problems with launching instances; however, it is also simple in terms of automation, and gives you the usual SSH without additional movementsOriginally published atRTFM: Linux, DevOps, and system administration."}
{"title": "Automatic Golden Image Generation using CI/CD", "published_at": 1719126956, "tags": ["aws", "devops", "gitlab", "terraform"], "user": "Anirban Das", "url": "https://dev.to/aws-builders/automatic-golden-image-generation-using-cicd-12e2", "details": "Introduction:Everyone, In every organization, security and compliance guardrails are measured in order to maintain the things are aligned with client expectations and agreement. There are many types of guardrails or compliance parameters out of which golden image creation is one of them. Before going into deep dive, let's under stand what is Golden Image.Golden Image is basically an image that has all required or supporting packages to be installed like agent packages, software or utilities packages, vulnerability agent package etc. there can be other packages installed which are approved by client. So when you're going to build a golden image for the first time, you just have to make sure that all required tools are installed and running fine in that server(windows/linux) to support the environment. After all this needs to be aligned with approved SOE parameters document. Along with making sure all packages are installed, another thing which is OS needs to be updated with latest patches for current month. Once these all are done, then take a snapshot of that instance and consider as base image which is known as Golden Image. This image would be used for further server build activity in future.Diagram:Prerequisites:GitLabTerraformAnsible(optional)AWS Cloud PlatformGuidelines:In this project, I have planned to build golden image for the first time as I didn't have any image earlier, so it's kind of we are starting from scratch. So, let me tell you guys first, that below are the planned action items to be done for this project ->Build AWS EC2 instance using Terraform.Provision EC2 instance using Ansible.Created CICD pipeline to build sequence of activities.Once entire provisioning is completed, then take an AMI of that instance.Lastly, terminate the instance.Note:As this is done for the first time, so ansible is required because there is no OS hardening parameters implemented. After instance provisioning with latest patches and implementing all security standards, once image is created, then for next month activity, Ansible will not be required because OS hardening parameters would have baked up in last month.Build an Instance using TerraformI have taken a sample base image (not last month golden image) as a reference, fetched this image using terraform and created a new EC2 instance.var.tfvariable \"instance_type\" {   description = \"ec2 instance type\"   type        = string   default     = \"t2.micro\" }Enter fullscreen modeExit fullscreen modedata.tf:## fetch AMI ID ## data \"aws_ami\" \"ami_id\" {   most_recent = true   filter {     name   = \"tag:Name\"     values = [\"Golden-Image_2024-06-13\"]   } }  ## Fetch SG and Keypair ## data \"aws_key_pair\" \"keypair\" {   key_name           = \"keypair3705\"   include_public_key = true }  data \"aws_security_group\" \"sg\" {   filter {     name   = \"tag:Name\"     values = [\"management-sg\"]   } }  ## Fetch IAM role ## data \"aws_iam_role\" \"instance_role\" {   name = \"CustomEC2AdminAccess\" }  ## Fetch networking details ## data \"aws_vpc\" \"vpc\" {   filter {     name   = \"tag:Name\"     values = [\"custom-vpc\"]   } }  data \"aws_subnet\" \"subnet\" {   filter {     name   = \"tag:Name\"     values = [\"management-subnet\"]   } }Enter fullscreen modeExit fullscreen modeinstance.tfresource \"aws_iam_instance_profile\" \"test_profile\" {   name = \"InstanceProfile\"   role = data.aws_iam_role.instance_role.name }  resource \"aws_instance\" \"ec2\" {   ami                         = data.aws_ami.ami_id.id   instance_type               = var.instance_type   associate_public_ip_address = true   availability_zone           = \"us-east-1a\"   key_name                    = data.aws_key_pair.keypair.key_name   security_groups             = [data.aws_security_group.sg.id, ]   iam_instance_profile        = aws_iam_instance_profile.test_profile.name   subnet_id                   = data.aws_subnet.subnet.id   user_data                   = file(\"userdata.sh\")    root_block_device {     volume_size = 15     volume_type = \"gp2\"   }   tags = {     \"Name\" = \"GoldenImageVM\"   } }Enter fullscreen modeExit fullscreen modeoutput.tfoutput \"ami_id\" {   value = {     id               = data.aws_ami.ami_id.image_id     arn              = data.aws_ami.ami_id.arn     image_loc        = data.aws_ami.ami_id.image_location     state            = data.aws_ami.ami_id.state     creation_date    = data.aws_ami.ami_id.creation_date     image_type       = data.aws_ami.ami_id.image_type     platform         = data.aws_ami.ami_id.platform     owner            = data.aws_ami.ami_id.owner_id     root_device_name = data.aws_ami.ami_id.root_device_name     root_device_type = data.aws_ami.ami_id.root_device_type   } }  output \"ec2_details\" {   value = {     arn         = aws_instance.ec2.arn     id          = aws_instance.ec2.id     private_dns = aws_instance.ec2.private_dns     private_ip  = aws_instance.ec2.private_ip     public_dns  = aws_instance.ec2.public_dns     public_ip   = aws_instance.ec2.public_ip    } }  output \"key_id\" {   value = {     id          = data.aws_key_pair.keypair.id     fingerprint = data.aws_key_pair.keypair.fingerprint   } }  output \"sg_id\" {   value = data.aws_security_group.sg.id }  output \"role_arn\" {   value = {     arn = data.aws_iam_role.instance_role.arn     id  = data.aws_iam_role.instance_role.id   } }Enter fullscreen modeExit fullscreen modeuserdata.sh#!/bin/bash sudo yum install jq -y ##Fetching gitlab password from parameter store GITLAB_PWD=`aws ssm get-parameter --name \"gitlab-runner_password\" --region 'us-east-1' | jq .Parameter.Value | xargs`  ##Set the password for ec2-user PASSWORD_HASH=$(openssl passwd -1 $GITLAB_PWD) sudo usermod --password \"$PASSWORD_HASH\" ec2-user  ## Create gitlab-runner user and set password USER='gitlab-runner' sudo useradd -m -u 1001 -p $(openssl passwd -1 $GITLAB_PWD) $USER  ##Copy the Gitlab SSH Key to gitlab-runner server sudo mkdir /home/$USER/.ssh sudo chmod 700 /home/$USER/.ssh Ansible_SSH_Key=`aws ssm get-parameter --name \"Ansible-SSH-Key\" --region 'us-east-1' | jq .Parameter.Value | xargs` sudo echo $Ansible_SSH_Key > /home/$USER/.ssh/authorized_keys sudo chown -R $USER:$USER /home/$USER/.ssh/ sudo chmod 600 /home/$USER/.ssh/authorized_keys sudo echo \"StrictHostKeyChecking no\" >> /home/$USER/.ssh/config sudo echo \"$USER  ALL=(ALL) NOPASSWD  : ALL\" > /etc/sudoers.d/00-$USER sudo sed -i 's/^#PermitRootLogin.*/PermitRootLogin yes/; s/^PasswordAuthentication no/PasswordAuthentication yes/' /etc/ssh/sshd_config sudo systemctl restart sshd sleep 40Enter fullscreen modeExit fullscreen modeHere, we have used a shell script to get prerequisites installed for Ansible like user creation and providing sudo access etc.Provision EC2 instance using Ansible:Note:Before triggering ansible job in GitLab, please make sure you login to the server you built from gitlab runner as gitlab-runner is going to login to new server for ansible provisioning and that time it will get an error if we don't perform below one ->main.yml--- - name: Set hostname   hosts: server   become: true   gather_facts: false   vars_files:     - ../vars/variable.yml   roles:     - ../roles/hostnamectl  - name: Configure other services   hosts: server   become: true   roles:     - ../roles/ssh     - ../roles/login_banner     - ../roles/services     - ../roles/timezone     - ../roles/fs_integrity     - ../roles/firewalld     - ../roles/log_management     - ../roles/rsyslog     - ../roles/cron     - ../roles/journald  - name: Start Prepatch   hosts: server   become: true   roles:     - ../roles/prepatch  - name: Start Patching   hosts: server   become: true   roles:     - ../roles/patch  - name: Start Postpatch   hosts: server   become: true   roles:     - ../roles/postpatch  - name: Reboot the server   hosts: server   become: true   tasks:     - reboot:         msg: \"Rebooting machine in 5 seconds\"Enter fullscreen modeExit fullscreen modePrepare GitLab CI/CD Pipeline:There are 4 stages created for entire deployment activity. Initially it will start with validation to make sure if all required services are running fine as expected.If yes, then it will proceed for resource(EC2) build using Terraform. Here, I have used Terraform Cloud to make things more reliable and store state file in managed memory location provided by Hashicorp. But terraform cli can be used without any issues.After successful resource build, provisioning needs to be performed to implement basic security standards and complete OS hardening process using Ansible CLI.At last, once provisioning with patching is completed, pipeline job will take an AMI using AWS CLI commands.Below are the required stages for this pipeline ->ValidationInstanceBuildInstancePatchingTakeAMI.gitlab-ci.ymldefault:   tags:     - anirban  stages:   - Validation   - InstanceBuild   - InstancePatching   - TakeAMI   - Terminate  job1:   stage: Validation   script:     - sudo chmod +x check_version.sh     - source check_version.sh   except:     changes:       - README.md   artifacts:     when: on_success     paths:       - Validation_artifacts  job2:   stage: InstanceBuild   script:     - sudo chmod +x BuildScript/1_Env.sh     - source BuildScript/1_Env.sh     - python3 BuildScript/2_CreateTFCWorkspace.py -vvv    except:     changes:       - README.md   artifacts:     paths:       - Validation_artifacts       - content.tar.gz  job3:   stage: InstancePatching   script:     - INSTANCE_PRIVATEIP=`aws ec2 describe-instances --filters \"Name=tag:Name, Values=GoldenImageVM\" --query Reservations[0].Instances[0].PrivateIpAddress | xargs`     - echo -e \"[server]\\n$INSTANCE_PRIVATEIP\" > ./Ansible/inventory     - ansible-playbook ./Ansible/playbook/main.yml -i ./Ansible/inventory     - sudo chmod +x BuildScript/7_Cleanup.sh   when: manual   except:     changes:       - README.md   artifacts:     when: on_success     paths:       - Validation_artifacts       - ./Ansible/inventory  job4:   stage: TakeAMI   script:     - echo '------------Fetching Instance ID------------'     - INSTANCE_ID=`aws ec2 describe-instances --filters \"Name=tag:Name, Values=GoldenImageVM\" --query Reservations[0].Instances[0].InstanceId | xargs`     - echo '----------Taking an Image of Instance-----------'     - aws ec2 create-image --instance-id $INSTANCE_ID --name \"GoldenImage\" --description \"Golden Image created on $(date -u +\"%Y-%m-%dT%H:%M:%SZ\")\" --no-reboot --tag-specifications \"ResourceType=image, Tags=[{Key=Name,Value=GoldenImage}]\" \"ResourceType=snapshot,Tags=[{Key=Name,Value=DiskSnaps}]\"   when: manual   except:     changes:       - README.md  job5:   stage: Terminate   script:     - echo '------------Fetching Instance ID------------'     - INSTANCE_ID=`aws ec2 describe-instances --filters \"Name=tag:Name, Values=GoldenImageVM\" --query Reservations[0].Instances[0].InstanceId | xargs`     - echo '--------------------Terminating the Instance--------------------'     - aws ec2 terminate-instances --instance-ids $INSTANCE_ID   when: manual   except:     changes:       - README.mdEnter fullscreen modeExit fullscreen modeValidation:As per below images, we can see instances has been launched and provisioned successfully, post that AMI has been taken.Conclusion:So, after all we are at the end of this blog, I hope we all get an idea or approach how pipeline can be set up to build image without any manual intervention. However, in the pipeline I have referred Continuous Delivery approach, hence few stages are set to be trigged manually. There is one thing to highlight mandatorily which is \"Do not set Ansible stage(job3) in gitlab as automatic. Use when: manual key to set this stage manual. As I mentioned on the top, ansible stage requires gitlab runner to login to newly build server which I could have mentioned as a command in the pipeline, but I didn't, thought of lets verify things by entering into the server from gitlab runner.Hopefully you have enjoyed this blog, please go through this one and do the hands-on for sure\ud83d\ude42\ud83d\ude42. Please let me know how did you feel, what went well and how and where I could have done little better. All responses are welcome\ud83d\udc97\ud83d\udc97.For upcoming updates, please stay tuned and get in touch. In the meantime, let's dive into below GitHub repository -> \ud83d\udc47\ud83d\udc47Thanks Much!!Anirban Das."}
{"title": "What has changed since becoming AWS Community Builders", "published_at": 1719126884, "tags": ["awscommunitybuilders", "pankration", "jawsug"], "user": "Yasuhiro Matsuda", "url": "https://dev.to/aws-builders/what-has-changed-since-becoming-aws-community-builders-26ba", "details": "This is the English version ofthe articleI recently contributed to.I was recently selected as an AWS Community Builders for the third year in a row.The first time I was elected was in 2022, and at that time, with the new coronavirus spreading, I was mainly organizing online workshops. I was selected in the area of Front-End Web & Mobile, partly because of a joint project with the LINE Developer Community in that area.Before I became an AWS Community Builders, I was a speaker at a 24-hour event calledJAWS Pankration 2021. This event involved AWS user groups from all over the world and was a JAWS-UG event that was regarded as crazy even from overseas, held in a follow-the-sun format.And now, for the first time in three years, it will be held in the form ofJAWS Pankration 2024, and in applying for the Call for Proposals, I have decided to write a summary of the AWS Community I wanted to summarize the differences between before and after I became an AWS Community Builders.I also decided to apply for the CFP for2024 AWS Community and Career Day Taiwan, which was guided at the 2024 AWS Japan Community Leaders Meetup at 22/6/2024, with the same theme. There are 18 Community Builders in Taiwan, and we would like to encourage our user group members to become the next Community Builders.What is AWS Community Builders?Theofficial websiteintroduces them as follows and theAWS Community Directory. (120 are Japanese) have been published as of 6/6/2024. Builders who have already been selected must submit their activities on the annual renewal form, and if their activities are not recognized, they will not be selected for the next year.The AWS Community Builders program offers technical resources, education, and networking opportunities to AWS technical enthusiasts and emerging thought leaders who are passionate about sharing knowledge and connecting with the technical community.Throughout the program, AWS subject matter experts will provide informative webinars, share insights \u2014 including information about the latest services \u2014 as well as best practices for creating technical content, increasing reach, and sharing AWS knowledge across online and in-person communities. The program will accept a limited number of members per year. All AWS builders are welcome and encouraged to apply.The official website describes the benefits of becoming an AWS Community Builders as follows.Members of the program will receive:Access to AWS product teams and information about new services and features via weekly webinarsLearning from AWS subject matter experts on a variety of non-technical topics, including content creation and support for submitting CFPs and securing speaking engagementsAWS Promotional Credits and other helpful resources to support content creation and community-based workSome surprises!Why did I decide to apply for AWS Community Builders?I am a core member ofJAWS-UG Kanazawa Chapter. While serving on the executive committee of AWS Community Day Kanazawa and JAWS DAYS 2021, I was asked by an AWS Community I decided to apply for the AWS Community Builders program after being invited by AWS employees in charge of AWS community and influenced by Mr.Masayuki Kato and Mr.Michael Tedder, who had already become AWS Community Builders.Since AWS Community Builders is an application system, I had to enter my own activities in English through the form, and I was not selected in FY2021. The reason was that I could not appeal our activities in a way that a third party could understand.I continued our activities for six months without giving up, and by reviewing our appeal methods, I was successfully elected in 2022.About the changes after being selected as AWS Community BuildersI realized the following 4 points.Being exposed to the latest informationMeetings for AWS Community Builders are held online, and I can get in touch with the latest service information.Due to the time zone, the meetings are held in two parts, often at 1:00 am and 7:00 am Japan time, making it difficult for AWS Community Builders in APAC (Asia Pacific) to participate in the real meetings.However, it is very stimulating to see the excitement of the chat through real participation.Resistance to English is somewhat easedAs I can see in \"the You Belong Here video\", I can barely speak English. Because of this, I have attended re:Invent in Las Vegas twice, and both times I experienced considerable difficulties due to my inability to speak English after getting into trouble at the hotel.All AWS Community Builders announcements are communicated in English on a dedicated Slack workspace, so I am forced to interact with English to understand the information.Fortunately, only the lang-japanese channel is in Japanese, so I can get help from AWS Community Builders in Japan if I have trouble understanding the details.Since we don't usually speak or write in English, this is a good opportunity to ease our resistance to the language.Experience AWS re:Invent 2023As described inmy first self-funded re:Invent experience in 5 years, I attended AWS re:. Invent 2023 made me realize how different it was from AWS re:Invent 2018, which I went to before becoming an AWS Community Builders.Partly because I have a greater understanding of AWS and more members to get involved with through the community, but also because I was able to network with people I would not have come in contact with had I not become an AWS Community Builders.The information that comes in through connecting with other people can be quite different. Since I do not have any strengths in the technical area, the information and ideas that come in from newly connected people are often things that I am not exposed to on a daily basis, and this motivates me to make changes.Interaction with AWS Community BuildersWhile overseas connections are the best part of being an AWS Community Builder, we try to actively interact with domestic members through domestic events.Last year's efforts includedAWS Summit Tokyo 2023andJAWS Festa 2023, we organized the AWS Community Builder Meetup, a social event exclusively for AWS Community Builders.Each of them has their own technical strengths, but the fact is that there is a lot to learn from the words that come out of these people who are supporting the community.Changing ConsciousnessIn addition to JAWS-UG activities, I am involved in several community activities. One of the major concerns that I have with my community activities is why I invest my time and effort in these activities.I feel that the larger the events we hold, and the fewer the core members, the greater this feeling becomes.It is often said that people gather where there is a high level of enthusiasm and where people are having fun, and by working with people like the AWS Community Bilders, who have high centripetal force, I have found the answer to the question of \"why invest my time and effort in our activities\".SummaryIf you are a regular user of AWS technology and like output activities, just taking one step forward can make a world of difference. If you are interested in trying it through what you have seen, please applyJAWS Pankration 2024and2024 AWS Community and Career Day Taiwan."}
{"title": "AWS Firewalls 101: Stateful vs. Stateless", "published_at": 1719121470, "tags": [], "user": "J.R. de Guzman", "url": "https://dev.to/aws-builders/aws-firewalls-101-stateful-vs-stateless-3k1h", "details": "AWS Firewalls 101: Stateful vs. StatelessHey there, fellow cloud enthusiast! Today, let's dive into the basics of stateful and stateless firewalls in AWS.Firewalls are the unsung heroes of network security, keeping the bad stuff out while letting the good stuff in.But did you know there are different types? Let's break it down.Stateful FirewallsThink of stateful firewalls as the smart gatekeepers of your network. They remember past interactions. If you let someone in, they remember and let them out too without you having to tell them again. This is super handy because you set fewer rules, and it keeps things simple.Why They're Awesome:Connection Savvy- They track ongoing connections, making life easier by allowing return traffic automatically.Less Work- Fewer rules to manage means less hassle.In AWS,Security Groupsare your go-to stateful firewalls. It allows incoming traffic on port 80 for your web server, and the return traffic flows back out without additional configuration.Stateless FirewallsOn the flip side, stateless firewalls are like diligent security guards checking every single packet without any memory of the past. They need explicit instructions for everything, both coming in and going out.Why They're Cool:Super Fast- They can handle lots of traffic quickly because they don't track connections.Detailed Control- You get to set detailed rules for everything, giving you granular control.AWS Network ACLs (Access Control Lists)are your typical stateless firewalls. You'll need to write specific rules for both inbound and outbound traffic, which gives you precise control but requires more setup.In a nutshell, most AWS setups use a combination of both. Security Groups manage traffic to your instances, while Network ACLs add an extra layer of subnet-level control.Let's have a quick demo on the next blog post about the concept of stateful and stateless firewalls."}
{"title": "Unleashing the Power of CDK and Terraform in Cloud Deployments", "published_at": 1719043142, "tags": ["cdk", "terraform", "aws", "iac"], "user": "Sidath Munasinghe", "url": "https://dev.to/aws-builders/unleashing-the-power-of-cdk-and-terraform-in-cloud-deployments-5680", "details": "IntroductionDeploying applications to the cloud has become a cornerstone of modern software development. AWS offers CloudFormation as a service to facilitate cloud deployments and tools like the AWS Cloud Development Kit (CDK). At the same time, Terraform has emerged as a powerful solution for Infrastructure as Code (IaC), enabling faster deployments to multiple cloud providers. In this article, we\u2019ll explore the benefits of using AWS CDK and Terraform together and walk through a practical example of creating a REST API with CDK in TypeScript.What is Terraform and CDK?Terraform and CDK are prominent tools that empower the definition of infrastructure as code. Each solution possesses its own set of advantages and disadvantages. Let's delve into a bit more information on both.TerraformTerraform is a tool created by HashiCorp that allows you to define your infrastructure in a high-level configuration language called HCL (HashiCorp Configuration Language). Terraform is cloud-agnostic and can manage infrastructure across various cloud providers, including AWS, Azure, and Google Cloud Platform. It also enables faster deployments when compared to CloudFormation, specifically for AWS.AWS CDKThe AWS Cloud Development Kit (CDK) is an open-source software development framework for defining cloud infrastructure in code and provisioning it through AWS CloudFormation. CDK uses familiar programming languages, including TypeScript, to model your applications. Underneath, CDK generates plain CloudFormation templates to create the infrastructure using the code we implement with CDK. The advantage is due to this abstraction, we could generate very lengthy CloudFormation templates within a few lines using high-level CDK constructs. So, it helps developers implement and maintain infrastructure code conveniently with their favourite programming language.Benefits of Using Terraform and CDK TogetherUsing both tools together, we can enjoy the benefits of both worlds. Although Terraform uses HCL, it may not be very convenient for developers. CDK solves this by providing high-level reusable CDK constructs to implement the infrastructure within a few lines. Also, since we use a very familiar programming language, it feels so close to the developers.On the other hand, CDK uses CloudFormation behind the scenes, which is usually slower than Terraform. However, when we use CDK and Terraform together, we can make much faster cloud deployments since we use Terraform to perform the deployments.We can achieve this through the use ofCDK for Terraform, which is introduced as Cloud Development Kit for Terraform (CDKTF), allowing us to utilise familiar programming languages to define and provision infrastructure.Setting up a ProjectLet\u2019s set up a Terraform project with CDK using Typescript as the language. We need to set up a few prerequisites for using CDK for Terraform.Terraform CLINodeJSTypeScriptCDKTF CLIOnce the setup is complete, we can initiate a project. First, let\u2019s create a folder to set up the initial code.Then we can initiate a project with below CLI command. Here we are going to use TypeScript.Once the project is initialized, we can update the main.ts file to define the infrastructure we need. Within the main.ts file, it has created a CDK app as well as a Stack. We can update the resources within the stack as needed to deploy. Let\u2019s build a simple hello world REST API using API Gateway and a Lambda function.Building a REST APIBefore adding any AWS resources, let\u2019s configure the AWS provider in Terraform since we will use AWS as the cloud provider. Further, we can use a S3 bucket to store the Terraform backend and track the deployment states.We can simply configure this by adding the necessary CDK constructs (AwsProvider, S3Backend) with the required parameters like below.Here, we have configured the AWS provider by providing the AWS account ID we need to deploy with the region. Similarly, we have configured the S3 backend by providing the bucket name and other configurations.Now, let\u2019s create an IAM role as the execution role for the Lambda function, including the permissions for a basic lambda execution role.Now, it\u2019s time to create the Lambda function. Let\u2019s add the below code for the Lambda function code inside index.ts file within the src folder. Since we are building a simple hello-world application, the Lambda function is returning a simple hello-world response.Once we have added the Lambda function handler implementation, we can add the CDK implementation to refer to that and create the Lambda function resource.The above definition will create a S3 bucket to hold the function code and create the Lambda function. The role we defined earlier is also provided as the execution role for the function.Once the Lambda function is ready, we can now create the API Gateway REST API and integrate the Lambda function with it.Here, we are defining the constructs for the API Gateway, a resource for the/hellopath and the GET method under that for the/helloGETendpoint. Finally, we have integrated it with the lambda function we created earlier as a proxy integration.Since things are integrated correctly, we can create a stage in the API Gateway and create a deployment like the one below.We have provided the stage name and API we want to create in the configurations.Now, we have created all the resources we need. But there is one more thing we need to do. We need to ensure that the API Gateway service can invoke the provided lambda function. To do that, we must create and attach a resource-based policy to allow that action within the Lambda function. We can do it like below easily using theLambdaPermissionconstruct.This construct will add the required permissions to the Lambda function to be invoked by the API we created earlier. With this, we are complete with the implementation.The full implementation of this project can be found on thisGitHub repository.Now everything is ready to deploy. Ensure you have configured your AWS credentials correctly so the Terraform can access AWS to provision the infrastructure. We can first build the code and then deploy it using the command below.Once the above command is executed, CDK for Terraform will install if there are any missing packages and start the deployment. After the deployment is complete, you can verify the created resources and play with the API.Moreover, we can discern that Terraform is executing the deployment significantly faster than CloudFormation, which is incredibly advantageous.To delete the resources you created, you can run the cdktf destroy command. This will ensure that all the resources created by the project are properly cleaned up.ConclusionUsing AWS CDK with Terraform offers several notable benefits for managing cloud infrastructure. CDK\u2019s deep integration with AWS and support for familiar programming languages like TypeScript make defining AWS resources intuitive and maintainable. Terraform\u2019s cloud-agnostic capabilities complement CDK by allowing for seamless management across multiple cloud providers. This combination provides flexibility, ease of use, and modularity, enhancing the overall infrastructure management workflow. By leveraging both tools, you can streamline deployments, improve efficiency, and achieve a more robust and versatile infrastructure management solution."}
{"title": "Deep Dive on AWS Clean Rooms with Integration to AWS Glue", "published_at": 1719034104, "tags": ["awsglue", "awscleanrooms", "cloudwatch", "s3bucket"], "user": "GargeeBhatnagar", "url": "https://dev.to/aws-builders/deep-dive-on-aws-clean-rooms-with-integration-to-aws-glue-3dbf", "details": "\u201c I have checked the documents of AWS to deep dive on aws clean rooms with integration to aws glue. So I checked for aws documents to get an idea on how aws clean rooms works for querying the analysis and used in machine learning modeling. Pricing of aws clean rooms depends on sql compute cost and type of analysis rule.\u201dAWS Glue is a serverless data integration service that makes it easy to discover, prepare, and combine data for analytics, machine learning, and application development. AWS Glue provides all of the capabilities needed for data integration so that you can start analyzing your data and putting it to use in minutes instead of months. AWS Glue provides both visual and code-based interfaces to make data integration easier. Users can easily find and access data using the AWS Glue Data Catalog.AWS Clean Rooms helps you and your partners analyze and collaborate on your collective datasets to gain new insights without revealing underlying data to one another. You can use AWS Clean Rooms, a secure collaboration workspace, to create your own clean rooms in minutes, and start analyzing your collective datasets with just a few steps. You can choose the partners with whom you want to collaborate, select their datasets and configure restrictions for participants.In this post, you will get to know how to deep dive on aws clean rooms with integration to aws glue. Here I have created a s3 bucket, glue database and table to further integrate with aws clean rooms service.PrerequisitesYou\u2019ll need an Amazon Simple Storage Service for this post.Getting started with Amazon Simple Storage Serviceprovides instructions on how to create a bucket in simple storage service.You\u2019ll need an AWS Glue for this post.Getting started with AWS Glueprovides instructions on how to create a glue database and table.Architecture OverviewThe architecture diagram shows the overall deployment architecture with data flow, amazon s3, aws glue, aws clean rooms, cloudwatch.Solution overviewThe blog post consists of the following phases:Create of Members Collaboration in AWS Clean RoomsIntegrate of Glue Database and Table in AWS Clean Rooms and Associate It To CollaborationOutput of Queries Analysis in CollaborationI have a s3 bucket, glue database and table created as below \u2192Phase 1: Create of Members Collaboration in AWS Clean RoomsOpen the aws clean rooms console and create a collaboration with required parameters such as name, members you want to add include yourself and other aws account members, specify of member abilities, who pay for queries, query logging info and optional settings. Also specify configure membership info query logging settings, query results store destination and result format. Review the details and create it.Phase 2: Integrate of Glue Database and Table in AWS Clean Rooms and Associate It To CollaborationConfigure a new table with a glue database and tables already created. Choose of schema and allowed columns with specify of new table name. Also configure the Analysis rule with certain parameters such as rule type, aggregate function, join controls and soon\u2026 Specify query results controls with review and configure it. Associate of table created to collaboration.Phase 3: Output of Queries Analysis in CollaborationClean-upDelete S3 Bucket, Glue Database and Table, AWS Clean Rooms, CloudwatchLog Group.PricingI review the pricing and estimated cost of this example.Cost of Data Transfer = $0.0Cost of Cloudwatch = $0.0Cost of Glue = $0.0Cost of S3 = $0.02Cost of AWS Clean Rooms (SQL Compute per CRPU hour is $2.00) = $0.0Total Cost = $0.02SummaryIn this post, I showed \u201chow to deep dive on aws clean rooms with integration to aws glue\u201d.For more details on AWS Clean Rooms, Checkout Get started AWS Clean Rooms, open theAWS Clean Rooms console. To learn more, read theAWS Clean Rooms documentation.Thanks for reading!Connect with me:Linkedin"}
{"title": "My Flow and Productivity has Improved with the Simplicity of Neovim", "published_at": 1719007006, "tags": ["programming", "development"], "user": "Benjamen Pyle", "url": "https://dev.to/aws-builders/my-flow-and-productivity-has-improved-with-the-simplicity-of-neovim-2lhm", "details": "I don't think it's a surprise if you've been following along with me lately that I've pivoted my daily programming setup to Neovim.  What might surprise you is that I started my career working on HP-UX and remoting into servers because the compilers and toolchains only existed on those servers I was building for.  When working through a terminal, you've got to leverage a terminal editor or do something with x11 and that was just super clunky.  Enter my first experience with Vi.  I loved the motions, the simplicity, and the ubiquity of it.  But those are things that have been talked about in great detail.  What I want to explore in this article is my experience in moving to Neovim.Why the ChangeI've been working with code going back into the mid-90s across multiple operating systems and too many languages to keep track of.  The question that many people have asked me is why would you \"go back\" to a terminal-based workflow.  Almost as if to say it's a step backward.  With tools like VSCode and Jetbrains (insert any of their editors), why jump into something that is a community port of something so old? Before I jump into the process of converting, here are my top 3 reasons.Simplification of my tooling.  I need a Terminal Emulator and Neovim.  With that setup, I can work on Rust, Go, TypeScript, HTML, YAML, and any language or markup that I encounter in my day-to-day job.  No VSCode for this and Jetbrains for that. No switching between keybindings and the mouse.  Now, I do have many plugins that enhance my Neovim experience, but at its core, my setup is 2 things.Improved flow and productivity.  By staying in my terminal to work with Git, code, compilation, filesystems, and other pieces of my chain, I keep myself from having to jump out to another tool.  The way my brain works, every time I jump, I lose flow. I lose my place.  When I find my flow, I'm much more productive.Stripped-down experience.  In a world where code documentation, auto-completion, and AI-code generation are taking over, I'm going back to just crafting things with hand tools. Sure, I have automation in my Neovim setup, but for me, when I have to hit the docs to read code, and then type that code in, it sticks in my brain better.  This might not apply to you, but I was gifted with a great memory that is close to photogenic.  My memory further cements when I see, and then type, write, or speak.  By passing on some of the more fancy automation, I find myself learning more.  I might not write as much code, but the code I write feels better crafted.  No judgment here whatsoever, I'm simply stating how coding with Neovim makes me feel.What was My Experience Like?With all of the above as the foundation for my move, where did I start?  Honestly, not quite at the bottom but pretty close to it with TJ DeVries'KickStartproject.  I went down the path of wanting to understand exactly how my setup is working and only add in the plugins that I wanted.  Looking back, I just didn't have the time to fully understand exactly what this meant.  However, the act of failing with KickStart did give me some solid background in how Neovim, Lua, and Lazy (the plugin manager) work.I honestly was at a breaking point a few weeks into my conversion and reached out to my friendRakeshthat I was ready to give up.  As much as I enjoyed theNeovim Motions, I just couldn't live with the discombobulated experience that I was getting.  He rightly recommended that I give it another try, but this time with a prebuilt configuration.  The Neovim world calls them \"distros\".My first attempt at a distro wasNvChad.  NvChad is well-liked, polished, and a really good place to start. I know as of this writing, Rakesh is still flying high with NvChad and enjoys it very much.  Something about it felt too proprietary though.  Custom loaders, dealing with packages in certain ways, and that sort of thing.  I wanted something prebuilt but felt more like KickStart in that plugin adds and configurations felt more Neovim \"native\".This leads me to where I am now with usingLazyVim.  Landing in LazyVim was exactly what I needed to start building my developer flow and productivity.  It has some nice defaults, but the extensions and adding of plugins feel closer to what my journey started within KickStart.  I'd like to spend the balance of the article walking through my workflow and favorite plugins.The TerminalI don't want to skip over some foundational parts that are key to my development workflow and productivity.  Neovim is the editor, but it starts with my terminal emulator AND terminal multiplexer.I was a heavy and long-term user of iTerm2 coming into this change.  I figured it would serve me just fine.  And it did until it didn't.  I noticed it starting to get a little bogged down as I was running tmux and now Neovim.  More on tmux in a minute.I tried Kitty.  There was success but ultimately font rendering just felt off.  I then moved over to Alacrity.  Loved it, but found the configuration to be a little strange.  So on the prompting of some other friends (AJandDarko), Wezterm is where I landed.  It honestly feels like a blend of all 3 of the previous ones I listed but yet still super snappy.Multiplex or Multiverse?I said multiplexer didn't I?tmuxto be exact.  Another game-changer for me.  The beauty of using tmux is that I can create sessions, panes, and windows that can then be moved, split, detached, and everything in between.  I also have Neovim shortcuts built in so that I can easily move withhjklwhich if you know Neovim, that's life.With panes, I can split my terminal however I want, navigate between, hide, zoom, and dispatch right from the keyboard.  Super powerful.And if I want to have multiple windows going, I can switch with a keystroke that cycles through previous and next, by window number, or I get a selection screen.And as I mentioned, I can navigate with Neovim keybindings.With tmux and Wezterm, I'm in a position to get my editor fired up.Neovim Tour and PluginsThis article could get pretty lengthy if I went all through my setup, configuration, and plugin usage. So the plan is, that I'msharingmy dot-files and will touch on a few of the pieces I use or love the most.What I enjoy the most about using Neovim is that my fingers are glued to the keyboard.  It's getting to the point that I'm not even having to think about which key pairs do what.  I can't understate how much that improves my flow and productivity.  I don't want this to be a big Neovim vs VSCode vs IntelliJ as I know they all support Neovim bindings, but having specific keys for specific tasks that don't conflict with my Mac is so freeing.So let's get into a tour, starting with theOutlineplugin.OutlineWhat I like about Outline is that I have a nice heads-up view of my file.  I can see functions, structs, fields, properties, or whatever your language calls them.  There's nothing magical about the plugin, but it does a great job of doing just what it says.  Acting as an Outline.  I have mine bound to the simple<leader>o.  In Neovim, theleaderkey is a special key that you map to kick off commands.  For me, I haveleader == space.TroubleIn a similar spirit to Outline, there is a plugin calledTrouble.  This was created and maintained by the creator of LazyVim as well.  Think of Trouble as having two functions for me.Function one is to interact with the Language Server Protocol (LSP) in a way that yields diagnostics.  Those error messages, warnings, and other items that show up. Trouble makes them available in a single window.  A much broader topic is what is an LSP.  Think of it as the brains behind many of the coding actions you find useful like symbol lookup, reference finding, and filling in your favorite struct of class.The next thing that Trouble does is provide a view into all of the places a particular symbol is used.  This is determined by my cursor position and looks like the image below.With code organization covered, let's move into some functionality.TelescopeI don't think many Neovim users could live withoutTelescope.  Maintained by TJ DeVries, this is a fuzzy find, LSP integrator, and so many other things.  I use it constantly to find open buffers, grep my codebase, look through Git logs, and pull up references.  The image below shows how I'm using it to find Workspace Symbols.I could spend an entire article on just Telescope as it's something I could not live without.  The best I can compare it to is the finder in IntelliJ that greps symbols, types, and other items.  Only this can search through so much more.Code Completion and TypesSometimes it's helpful to have code completion and an easy view of the types that you are working with.  Coming from an IDE, these were things that I enjoyed and while I'm not super reliant on them, I still do use them.Like many using Neovim, I'm leveraging theNvim-Cmpplugin.  With this plugin, I get the snippets, code completion, and documentation on functions that I'm used to that help me out when my brain slows.And while code completion is nice, sometimes I just want to see the hints of the type in-lay next to my variables.  And with that latest version of Neovim, that's possible.And, if hit<leader>uh, they disappear.So many options.TestingThe last three parts of my setup that I want to dive into speak specifically to functions that are integral to my build process.GitUnit TestsDebuggingGitUsing tmux, I could just have a shell to pivot into when I want to work with Git.  Fine, and I could do that.  But I'm using the Neovim plugin forLazyGit. Which takes advantage of thisLazyGit UI.What I like about using LazyGit is that I can stay in my editor, and use my normal keybindings to navigate the buffer just like I do in every other buffer I work with.  This whole journey wasn't about feature for feature, but how I could increase my flow and productivity.  And staying in the terminal does that for me.TestingWhat developer flow is complete without a unit test runner?  For that work, I rely onNeotest.  Neotest launches a Neovim buffer that sits on the side of my terminal. I don't have to pop up the summary.  I can trigger Neotest in the background, get some notifications, and then move on. It also feels just like the other buffers I've mentioned above that can slide in and out as I need them.DebuggingThe final piece of the experience for me was \"Could I use a debugger in Neovim?\".  This was a big thing for me because I use a lot ofRustand Golang, and having a debugger available is critical.  The Debugger Adapter Protocol or DAP can plug into popular debuggers like LLDB or GDB which then can be managed by a plugin calledDAP UI.The UI is exactly what you'd think it would be.  Symbols, threads, watches, breakpoints, and then the common Continue, Step Over, Step Out that you would be accustomed to.  The below shows how I'm using it to debug a Rust Web API.Wrap Up ThoughtsI feel like I've written too little but my editor is showing me that I'm into the normal length article I've been producing lately.  I could keep going, go back up, and dive deeper into the plugins, but I'm going to stop here.  The point of this was to introduce my current setup, why I chose it, and what I'm doing with it.  I am not using any other editor for my coding or debugging tasks.  I still use VSCode to write my blog, because I like the Markdown preview mode and the Grammarly feature.  I am toying with using LaTex and Neovim and seeing about a Markdown plugin, but I bounce so much while writing that my hand reaches for the mouse in natural ways.  Maybe I'll switch in the future, but I'm not sure.My closing thought though is that in a world that is looking for more instant gratification, more code, more output, and using AI to bounce prompts and thoughts off of, I like the feeling that I can read and write my code on purpose without distractions.  Generated code is a distraction to me.  I've said it before, but the act of learning is why I like coding, not the act of producing.  Sure I love to finish things.  But I love coding because it's an art to me.  There is science for sure, but I like writing code like I like writing books and articles.  I'm not in a rush to complete it and move to the next thing.  I often romanticize the work I do.  It's just who am.And I'd be remiss if I didn't include links to my font and color scheme in case anyone is looking to make the switch.Font -Jetbrains Mono Nerdfont-- I can't get away from Jetbrains!Colors - The soothing pastels forCatppuccinAnd the last thing, if you ever get lost,Which-Keyis always there to help!Thanks for reading and happy building!"}
{"title": "Install Keycloak on ECS(with Aurora Postgresql)", "published_at": 1718967987, "tags": ["aws", "keyclock", "ecs", "cdk"], "user": "Jakub Wo\u0142ynko", "url": "https://dev.to/aws-builders/install-keycloak-on-ecs-5dbo", "details": "WelcomeIf you have read my latest post about accessing RHEL in the cloud,you may notice that we\u2019re accessing the cockpit console,via SSM Session manager port forwarding.That\u2019s not an ideal solution. I\u2019m not talking in bed,it\u2019s just not ideal(but cheap). Today I realised that usingAmazon WorkSpaces Secure Browser could be interesting, and fun as well.Unfortunately, this solution required an Identity Provider,which can serve us with SAML 2.0. The problem is, that most of theproviders likeOktaorPing,are enterprise-oriented, and you can\u2019t play with them easily.Of course, you can request a free trial, but it\u2019s 30 days only,and there is no single-user account.That is why I decided to useKeycloak.Then I just need to decide where to deploy it, in the cloud or in the home lab.Thankfully we have a mind map.That is why, today we will install Keycloak on AWS,and in part two, we will connect it with WorkSpacesand access the RHEL console over the cockpit, as an enterprise-like user.Keycloak introBut hey! What is the Keycloak? In simple words it isOpen Source Identity and Access Management tool.Also, Keycloak provides user federation, strong authentication,user management, fine-grained authorization, and more.You can read more on the GitHub projectpage, and star it as well.Services listTo make it a bit more interesting, I will deploy it with:ECS(Fargate)AWS Application Load Balancerseparate database, where today we will play with Serverless Aurora in MySQL modeRoute53 for domain managementOk, good. Finally, we will deploy something different right?First stepsAt the beginning, I\u2019d recommend building basic projects with networks,ECS components, database, and ALB.  Let\u2019s call itan almost dry test.Just take a look at the code, and break it into smaller parts.import*ascdkfrom'aws-cdk-lib';import{Construct}from'constructs';import*asec2from'aws-cdk-lib/aws-ec2';import*asrdsfrom'aws-cdk-lib/aws-rds';import*asecsfrom'aws-cdk-lib/aws-ecs';import*aselbv2from'aws-cdk-lib/aws-elasticloadbalancingv2'Enter fullscreen modeExit fullscreen modeOn top, as you can see, I have imports. It\u2019s worth to mention,that AWS Aurora Serverless V2 is usingaws-rdspackage, and modern load balancers,like ALB is included inaws-elasticloadbalancingv2, not standardaws-elasticloadbalancing.Firstly we have a network. Today we will have regular 3-tier architecture,with a really small mask -/28, which means 11 IPs; regular 16 - 5 (AWS specific).Besides of that two AZs for Aurora HA, so 2 NAT Gateways as well.constvpc=newec2.Vpc(this,'VPC',{ipAddresses:ec2.IpAddresses.cidr(\"10.192.0.0/20\"),maxAzs:2,enableDnsHostnames:true,enableDnsSupport:true,restrictDefaultSecurityGroup:true,subnetConfiguration:[{cidrMask:28,name:\"public\",subnetType:ec2.SubnetType.PUBLIC},{cidrMask:28,name:\"private\",subnetType:ec2.SubnetType.PRIVATE_WITH_EGRESS},{cidrMask:28,name:\"database\",subnetType:ec2.SubnetType.PRIVATE_ISOLATED}]});Enter fullscreen modeExit fullscreen modeNext we very simple setup of the application load balancer.It\u2019s listed on port 80(no TLS), it\u2019s internet facing, and exists in our VPC,inside subnets with type \u201cPUBLIC\u201d.constalb=newelbv2.ApplicationLoadBalancer(this,'ApplicationLB',{vpc:vpc,vpcSubnets:{subnetType:ec2.SubnetType.PUBLIC,},internetFacing:true});constlistener=alb.addListener('Listener',{port:80,open:true,});});Enter fullscreen modeExit fullscreen modeHere the same, we have a very basic setup of Postgres Aurora, without security groupsor deletion protection.However, we\u2019re setting it in the correct set of subnets,and what is more important we\u2019re checking our Postgres engine version,asthe latestis not always supported. If you would like to check iton our own, here is the command, which shows you available engines.aws rds describe-db-engine-versions\\--engineaurora-mysql\\--filtersName=engine-mode,Values=serverlessEnter fullscreen modeExit fullscreen modeBUT, if you would like to use Aurora Serverless V2, you need to userds.DatabaseCluster,so our code will be, and as you can see we can use a higher version of Postgres:newrds.DatabaseCluster(this,'AuroraCluster',{engine:rds.DatabaseClusterEngine.auroraMysql({version:rds.AuroraMysqlEngineVersion.VER_2_11_4}),vpc:vpc,credentials:{username:'keycloak',// WARNING: This is wrong, do not work this waypassword:cdk.SecretValue.unsafePlainText('password')},// NOTE: use this rather for testingdeletionProtection:false,securityGroups:[auroraSecurityGroup],vpcSubnets:{subnetType:ec2.SubnetType.PRIVATE_ISOLATED},writer:rds.ClusterInstance.serverlessV2('ClusterInstance',{scaleWithWriter:true})});Enter fullscreen modeExit fullscreen modeGreat, now we can configure:ECS Cluster with Fargate Capacity ProvidersFargate Task DefinitioncontainerECS ServiceALB TargetGroup registrationand check if we will be able toaccess our container from the regular Internet.constecsCluster=newecs.Cluster(this,'EcsCluster',{clusterName:'keycloak-ecs-cluster',// NOTE: add some loggingcontainerInsights:true,enableFargateCapacityProviders:true,vpc:vpc})constecsTaskDefinition=newecs.FargateTaskDefinition(this,'TaskDefinition',{memoryLimitMiB:512,cpu:256,runtimePlatform:{operatingSystemFamily:ecs.OperatingSystemFamily.LINUX,cpuArchitecture:ecs.CpuArchitecture.X86_64}});constcontainer=ecsTaskDefinition.addContainer('keycloak',{image:ecs.ContainerImage.fromRegistry('nginx'),portMappings:[{containerPort:80,protocol:ecs.Protocol.TCP}]});constecsService=newecs.FargateService(this,'EcsService',{cluster:ecsCluster,taskDefinition:ecsTaskDefinition,vpcSubnets:{subnetType:ec2.SubnetType.PRIVATE_WITH_EGRESS}});ecsService.registerLoadBalancerTargets({containerName:'keycloak',containerPort:80,newTargetGroupId:'ECS',listener:ecs.ListenerConfig.applicationListener(listener,{protocol:elbv2.ApplicationProtocol.HTTP})},);Enter fullscreen modeExit fullscreen modeAs you see the logic here is straightforward.First, we need a cluster, with enabled FargateCapacityProvider,and we would like to place it in our VPC.Then we have a task, that needs to be an instance of classecs.FargateTaskDefinition.Next, we\u2019re adding a container object with a Nginx docker image and port mapping.Additionally, we need a service that combines all elements.At the end, we register our service, as Load Balancer Target.This configuration should allow us to validate our application skeleton,and make sure that basic components are up, and we don\u2019t mess something.That is how I like to work, make a simple landscape, then tweak it according to the needs.Keycloak configurationNow, we need to clarify a few things.First, as we\u2019re planning to run Keycloak in container, that is why we needto go throughthis doc,then as we\u2019re using Aurora, we needthis.Second, there is no support for IAM Roles, which is why we need to use a username/password.However, if you feel stronger than me with AWS SDK for Java,and have some time, I\u2019m sure that the Keycloak team will be more than happy to review your PR.So after reading the documentation, looks like I need to add the JDBC driver to the image,which requires building a custom image. It will be very simplebut require some project modification. Additionally, we need some environment variables:constcontainer=ecsTaskDefinition.addContainer('keycloak',{image:ecs.ContainerImage.fromRegistry('quay.io/keycloak/keycloak:24.0'),environment:{KEYCLOAK_ADMIN:\"admin\",KEYCLOAK_ADMIN_PASSWORD:\"admin\",KC_DB:\"postgres\",KC_DB_USERNAME:\"keycloak\",KC_DB_PASSWORD:\"password\",KC_DB_SCHEMA:\"public\",KC_LOG_LEVEL:\"INFO, org.infinispan: DEBUG, org.jgroups: DEBUG\"},portMappings:[{containerPort:80,protocol:ecs.Protocol.TCP}]});Enter fullscreen modeExit fullscreen modeStop. Why it doesn\u2019t work?Look good right? No, no, and no! Making this project workforced me to spend much more time than I expected. Why? That\u2019s simple,Keycloak is complex software, which is why a lot of configuration isneeded. However, let\u2019s start from the beginning.Building custom docker imageI decided, that a public docker image from Aurora support would be great,and maybe helpful for someone. That is why you can find it onquay.Also whole project repo is public, and accessiblehere.Besides GitHub Actions, Dockerfile is the most important part, so here we go:ARGVERSION=25.0.1ARGBUILD_DATE=todayFROMquay.io/keycloak/keycloak:${VERSION}asbuilderLABELvendor=\"3sky.dev\" \\maintainer=\"Kuba Wolynko <kuba@3sky.dev>\" \\     name=\"Keyclock for Aurora usage\" \\     arch=\"x86\" \\     build-date=${BUILD_DATE}  # Enable health and metrics supportENVKC_HEALTH_ENABLED=trueENVKC_METRICS_ENABLED=trueENVKC_DB=postgresENVKC_DB_DRIVER=software.amazon.jdbc.DriverWORKDIR/opt/keycloak# use ALB on top, self-sign is fine hereRUNkeytool-genkeypair\\-storepasspassword\\-storetypePKCS12\\-keyalgRSA\\-keysize2048\\-dname\"CN=server\"\\-aliasserver\\-ext\"SAN:c=DNS:localhost,IP:127.0.0.1\"\\-keystoreconf/server.keystoreADD--chmod=0666 https://github.com/awslabs/aws-advanced-jdbc-wrapper/releases/download/2.3.6/aws-advanced-jdbc-wrapper-2.3.6.jar /opt/keycloak/providers/aws-advanced-jdbc-wrapper.jarRUN/opt/keycloak/bin/kc.sh buildFROMquay.io/keycloak/keycloak:${VERSION}COPY--from=builder /opt/keycloak/ /opt/keycloak/ENTRYPOINT[\"/opt/keycloak/bin/kc.sh\"]Enter fullscreen modeExit fullscreen modeWorth mentioning is the fact, that thebuildcommand needs to beexecuted after gettingaws-advanced-jdbc-wrapper,or enabling health checks. Also, I\u2019m using a self-sign certificateas AWS will provide public cert on the ALB level.DocumentationWhen you are going through official Keycloak documentationyou will be able to read something like this:I\u2019m not a native speaker, so for me, it means:Putaws-wrapperinto your JDBC URL. But, it\u2019s not true.It means that the result will bejdbc:aws-wrapper:postgresql://,which is not what the application can consume!To work with Aurora Postgresql, our connection string must be:Edit: I was wrong.aws_wrapperneeds to be included in JDBC URL,to use advanced wrapper's functions. However environment variableKC_DB_DRIVER=software.amazon.jdbc.Driver,need to be present in Dockerfile before running/opt/keycloak/bin/kc.sh buildcommand.KC_DB_URL:'jdbc:aws-wrapper:postgresql://'+theAurora.clusterEndpoint.hostname+':5432/keycloak',Enter fullscreen modeExit fullscreen modeNote:keycloakat the end is the database name, and can be customized.Believe it or not, this misunderstanding costs me around the week of debugging...Cookie not foundWhen after a while I was able to start an app and type admin/admin,I was welcomed with this error message:Cookie not found. Please make sure cookies are enabled in your browser.After some investigation, and readingstackoverflowthreads,the decision was to add TLS and see what would happen. To do that in rather a simplemanner, a hosted zone with a public registered domain was needed. And you can read about itsetting it uphere. In case of adding it to project we can simply write:constzone=route53.HostedZone.fromLookup(this,'Zone',{domainName:DOMAIN_NAME});newroute53.CnameRecord(this,'cnameForAlb',{recordName:'sso',zone:zone,domainName:alb.loadBalancerDnsName,ttl:cdk.Duration.minutes(1)});constalbcert=newacme.Certificate(this,'Certificate',{domainName:'sso.'+DOMAIN_NAME,certificateName:'Testing keyclock service',// Optionally provide an certificate namevalidation:acme.CertificateValidation.fromDns(zone)});this.listener=alb.addListener('Listener',{port:443,open:true,certificates:[albcert]});Enter fullscreen modeExit fullscreen modeHealth checkThis one is tricky. The target Group attached to the Application Load Balancer requires healthy targets.Using the native ECS method as described above does not meet our needs, as we expected:dedicated path/healthdifferent port(9000), than main container port(8433)if we decide to stick with/path, we need to accept port302As you see we need to change a method to a more traditional one:theListner.addTargets('ECS',{port:8443,targets:[ecsService],healthCheck:{port:'9000',path:'/health',interval:cdk.Duration.seconds(30),timeout:cdk.Duration.seconds(5),healthyHttpCodes:'200'}});Enter fullscreen modeExit fullscreen modeNOTE:default HC port could be changed, but it adds additional complexity.Build timePlaying with ECS and database in the same CDK stack is not the best possible idea. Why?Let's imagine the situation when your health checks are failing.Deployment is in progress and still rolling. During this period,you can't change your stack. But you can delete it\u2026, and recreate it.Even if the networking part is fast, spanning Aurora up and down,could add to our change time by around 12 minutes.That's not a bed, but still, it's quite easy to avoid it.The change was simple. I created a new stack, dedicated to ECS,and split the app into two parts:infra (network and database)ECS (container and services)This simple action shows that the modularity of Infrastructure as Code could be moreimportant than we're usually thinking.Final declarationSlowly going to the end, the ECS dedicated part will look like that:ecsTaskDefinition.addContainer('keycloak',{image:ecs.ContainerImage.fromRegistry(CUSTOM_IMAGE),environment:{KEYCLOAK_ADMIN:'admin',KEYCLOAK_ADMIN_PASSWORD:'admin',KC_DB:'postgres',KC_DB_URL:'jdbc:postgresql://'+theAurora.clusterEndpoint.hostname+':5432/keycloak',KC_DB_USERNAME:'keycloak',KC_DB_PASSWORD:theSecret.secretValueFromJson('password').toString(),KC_HOSTNAME_STRICT:'false',KC_HTTP_ENABLED:'false',KC_DB_DRIVER:'software.amazon.jdbc.Driver',KC_HEALTH_ENABLED:'true'},portMappings:[{containerPort:8443,protocol:ecs.Protocol.TCP},{containerPort:9000,protocol:ecs.Protocol.TCP}],logging:newecs.AwsLogDriver({streamPrefix:'keycloak'}),command:['start','--optimized']});Enter fullscreen modeExit fullscreen modeFinal architectureAs we all love images, that's the final digram of our setup:SummaryThat's the end of setting keycloak on ECS with the usage of Aurora Postgres.I didn't see it anywhere on the Internet, so for at least a moment, that isthe only working example and blog post available publicly.Additionally, a solution was tested with versions 24.0 and the latest 25.0.1 ofKeycloak base images. What will be next? As I mentioned at the beginning.Setting up Worksapces with my own SSO solution, however, firstly we need toconfigure Keyclock, which I\u2019m almost sure, will be fun as well.Ah and to be fully open, source code could be found onGitHub."}
{"title": "Deploying AWS Guard Duty Malware Protection for S3 Buckets (Step-by-Step Guide)", "published_at": 1718967978, "tags": ["guardduty", "awscommunity", "s3", "malwareprotection"], "user": "amalkabraham001", "url": "https://dev.to/aws-builders/deploying-aws-guard-duty-malware-protection-for-s3-buckets-step-by-step-guide-15l4", "details": "Keep your S3 buckets safe from malware! GuardDuty scans new and updated files uploaded to your chosen Amazon Simple Storage Service (S3) bucket. This automatic scanning helps identify potential malware threats before they can cause harm.In this blog post, I will walk you through a step-by-step guide on how to deploy AWS Guard Duty malware protection for S3.Configuring Guard Duty Malware ProtectionNavigate to the AWS portal, and search for guard duty.On the guard duty home page, click on \u201cGet Started\u201d after selecting the radio button next to the \u201cGuard Duty malware protection for S3 only\u201d.You will be redirected to the Guard duty console where we need to enable the S3 bucket configuration.Under the \u201cprotected buckets\u201d section click on \u201cEnable malware protection for S3\u201d.In the \u201cEnable malware protection for S3\u201d wizard, select the S3 bucket you need to protect using Guard Duty. You can click on \u201cBrowse S3\u201d and select the bucket you need to protect.You need to do this exercise repeatedly if you have multiple buckets to add.Under the prefix, you can select the radio button if you want to scan all the files or any specific types of files.Under \u201cTag scanned objects\u201d select the radio button next to the \u201cTag objects\u201d.The objects will be tagged as \u201cTHREATS_FOUND\u201d if guard duty detects any file containing malware. It will tag the devices without malware as \u201cNO_THREATS_FOUND\u201d.There are additional object tags like ACCESS_DENIED or FAILED based on the scan result.Guard Duty needs IAM permissions such as Read/Tag S3 objects, send to event bridge, and more. The pre-configured permission JSON file can be copied from the \u201cView permissions\u201d option under \u201cpermissions\u201d.First, create an IAM policy and copy the contents from the JSON file. Once you create the policy, the policy needs to be linked to an IAM role.The IAM Role trust entities need to be configured to allow the \u201cmalware-protection-plan.guardduty.amazonaws.com\u201d Service as shown in the screenshot below.Click on \u201cEnable\u201d to enable the protection.Testing the Functionality.\"Important Note: Uploading actual malware is not recommended as it could be harmful. To test GuardDuty functionality, we can leverage a safe test file provided by the European Institute for Computer Anti-Virus Research (EICAR) specifically designed for this purpose. Download the test file from the EICAR website.I uploaded the test malware file to my S3 bucket. The guard duty scanned the file and created a tag with the key pair as {\"GuardDutyMalwareScanStatus\": \"THREATS_FOUND\"}.I tested by uploading a normal file without any malware and the guardduty created a tag with the key pair as {\"GuardDutyMalwareScanStatus\": \"NO_THREATS_FOUND\"}.We can configure Quarantine rules via event bridge and lambda to move the malware files to a different S3 bucket created for Quarantine or to delete them immediately.Guard Duty malware Protection for S3 PricingGuard duty malware protection for S3 charges you $0.60 per GB and also $0.215 per 1000 PUT requests.Hope the blog is informative, please feel free to share your comments"}
{"title": "Monitoring Underutilized Storage Resources on AWS", "published_at": 1718917736, "tags": ["aws", "storage", "cloudcomputing", "awscommunity"], "user": "Brandon Damue", "url": "https://dev.to/aws-builders/monitoring-underutilized-storage-resources-on-aws-1gnf", "details": "When cloud professionals embark on a journey to fish out underutilized resources that may be driving costs up, they rarely pay attention to doing some cost optimization in the direction of storage resources and often focus solely on optimizing their compute resources. In this article, we will go through some tools and strategies you can leverage in monitoring your AWS storage resources. Before moving on to that main event, let\u2019s start by talking briefly about the different storage types available on AWS. If you are ready to roll, let\u2019s go!!Storage TypesWhen it comes storage, AWS has a wide array of services you can choose from. You will agree with me that having these many options can add some level of confusion to your decision making process especially when you don\u2019t have an understanding of what the options are and which ones are suitable for what use case. To provide you with guidance for when you have to pick a storage service on AWS, let\u2019s talk about some of the storage types available.On AWS, storage is primarily divided into three categories depending on the type of data you intend to store. These categories are: Block storage, Object storage and File storage. We will go over them one after the other, exploring examples of each as we go.Block StorageTo put it simply, a block storage device is a type of storage device that stores data in fixed-size chunks called blocks. The size of each block is dependent on the amount of data the device can read or write in a single input/output (IO) request. So when you want to store data on a block storage device and the size of the data surpasses the size of a single block of data that the device can read or write in single I/O request, the data is broken down into equal-size chunks before it is stored on the underlying storage device. As it is always important to understand the Why behind actions, let me tell you the performance benefit of block storage devices handling data in the manner that they do.When data is broken down into blocks, it allows for fast access and retrieval of the data. In addition to fast access, when data is on a block storage device and changes are made to the data, only the blocks affected by the change are re-written. All other blocks remain unchanged which helps to further enhance performance and speed. In AWS, the block storage options include Elastic Block Storage (EBS) volumes and Instance Store volumes. Check outthis articleI wrote to learn more about EBS and Instance Store Volumes.Object StorageWith object storage, data is not broken down into fixed-sized chunks as is the case with block storage. In object storage, data (files) are stored as single objects no matter their size. This kind of storage is suitable for huge amounts of unstructured data. The object storage service of AWS is S3. With all data being stored as single objects, when some part of that object is updated, the entire object has to be rewritten. You can access data stored in S3 via HTTP, HTTPS or APIs through the AWS CLI or SDK. Some pros that come with using S3 are: it is highly available, tremendously durable, low cost and can scale infinitely not forgetting the fact that you can replicate your data in the same or across regions for disaster recovery purposes. Check outthis articleI wrote on S3 to learn more about object storage.File StorageFile Storage is fundamentally an abstraction of block storage using a file system such as NFS (Network File System) and SMB (Server Message Block). With File storage, the hierarchy of files is maintained with the use of folders and subfolders. The main file storage services of AWS are Amazon EFS and Amazon FSx. File storage is the most commonly used storage type for network shared file systems.Monitoring Underutilized Storage ResourcesThe opening sentence of this paragraph is a lamentation so to speak on how storage resources are seldom considered when organizations and individuals take cost optimization actions. Even though they often fail to do this, it is just as important to pick the right storage option for your use case and also provision them appropriately. You can right size your storage resources by monitoring, modifying and even deleting those that are underutilized. Let\u2019s examine some of the ways in which you can monitor your storage resources.Amazon CloudwatchCloudwatch provides out-of-box metrics for monitoring storage services such as S3, DynamoDB, EBS and more. For EBS volumes you can use a metric such as, VolumeIdleTime which specifies the number of seconds there are no read or write requests to the volume within a given time period. With the information that Cloudwatch provides through this metric, you can decide on the action you want to take to manage the underutilized volume. In addition to the metrics that CloudWatch ships with for EBS volumes for example, you can create custom metrics to do things like find under provisioned or over provisioned volumes.For S3 buckets, you can use the BucketSizeByte CloudWatch metric which gives you the size of your bucket in bytes. This comes in handy if you have stray S3 buckets that aren\u2019t holding much data. Using this metric, you can quickly find and clean up those buckets.S3 Object Logs & S3 AnalyticsWith S3 you can use S3 object access logs as well. These will help you track requests that are made to your bucket. Using this, you can find buckets that aren\u2019t accessed frequently, and then determine if you still need the data in that bucket, or if you can move it to a lower cost storage tier or delete it. This is a manual process of determining access patterns. You make use of S3 Analytics if you are interested in a service that provides an automated procedure.S3 Analytics can help you determine when to transition data to a different storage class. Using the analytics provided by this service, you can then leverage S3 lifecycle configurations to move data to lower cost storage tiers or delete it, ultimately reducing your spend over time. You can also optionally use the S3 Intelligent-tiering class to analyze when to move your data and automate the movement of the data for you. This is best for data that has unpredictable storage patterns.Compute Optimizer and Trusted AdvisorTo monitor for situations such as under-provisioned or over-provisioned EBS volumes, you can also make use of Compute Optimizer and Trusted Advisor for an easier and more automated experience. Compute Optimizer will make throughput and IOPS recommendations for General Purpose SSD volumes and IOPs recommendations for Provisioned IOPs volumes. However, it will identify a list of optimal EBS volume configurations that provide cost savings and potentially better performance. With Trusted Advisor, you can identify a list of underutilized EBS volumes. Trusted Advisor also ingests data from Compute Optimizer to identify volumes that may be over-provisioned as well.ConclusionAs a self appointed disciple preaching the gospel of optimizing AWS resources for better cost saving and performance, I hope you have taken a lesson or two from this article to implement in your resources monitoring and optimizing strategies. There are services such as CloudWatch, Trusted Advisor, Compute Optimizer, S3 Analytics and much more for you to add to your bag of tools. To make sure you don\u2019t overwhelm yourself, learn more about each service you intend to make use of, start small and then move up from there. Good luck in your cloud endeavors."}
{"title": "Hands-On: Escalonamento autom\u00e1tico com EKS e Cluster Autoscaler utilizando Terraform e\u00a0Helm", "published_at": 1718895545, "tags": ["devops", "aws", "kubernetes"], "user": "Rodrigo Fernandes", "url": "https://dev.to/aws-builders/hands-on-escalonamento-automatico-com-eks-e-cluster-autoscaler-utilizando-terraform-e-helm-51ki", "details": "Introdu\u00e7\u00e3oO escalonamento autom\u00e1tico de clusters \u00e9 uma funcionalidade essencial em ambientes de computa\u00e7\u00e3o em nuvem, especialmente quando se trata de gerenciar recursos de forma eficiente e econ\u00f4mica.Nesse contexto o Cluster Autoscaler (CA) \u00e9 uma ferramenta vital para ajustar dinamicamente o n\u00famero de inst\u00e2ncias de n\u00f3 em um cluster Kubernetes, garantindo que as cargas de trabalho tenham recursos suficientes enquanto minimiza os custos.Este artigo t\u00e9cnico explora o processo de configura\u00e7\u00e3o e uso do Amazon EKS e do Cluster Autoscaler utilizando Terraform e Helm para implementar o escalonamento autom\u00e1tico.Informa\u00e7\u00f5es geraisAs configura\u00e7\u00f5es abaixo s\u00e3o para ambientes de testes, workshops e demos. N\u00e3o utilizar em ambientes de produ\u00e7\u00e3o.Caso j\u00e1 conhe\u00e7a o Cluster Autoscaler e quer fazer testes, clique nesselinke use o reposit\u00f3rio completo.Se quer fazer o passo-a-passo para entender em detalhes, siga as instru\u00e7\u00f5es abaixo.Setup do\u00a0ClusterPara o setup do cluster, iremos utilizar um reposit\u00f3rio em Terraform com o c\u00f3digo de um cluster b\u00e1sico j\u00e1 pronto.Acesse o reposit\u00f3rio clicandoaqui, no readme existe o passo-a-passo para o setup completo do cluster.Ap\u00f3s a execu\u00e7\u00e3o dos passos, aguarde at\u00e9 conclus\u00e3o, o output ser\u00e1 conforme imagem abaixo:Pronto, o setup do cluster est\u00e1 concluido, vamos acessar o cluster e fazer alguns testes iniciais para analisar a integridade do cluster.Acessando o\u00a0ClusterPara acessar o cluster vamos utilizar o AWS Cloud9 e para a configura\u00e7\u00e3o vamos seguir o artigoBoosting AWS Cloud9 to Simplify Amazon EKS Administrationclicandoaqui.Ap\u00f3s seguir os passos do artigo teremos o Cloud9 e o script de ferramentas para Kubernetes configurados.Copie o comando abaixo, altere a regi\u00e3o e o nome do cluster e execute o comando para acessar o cluster EKS\u00a0.$ aws eks --region <sua-regi\u00e3o> update-kubeconfig --name <nome-do-cluster>Vamos fazer alguns testes iniciais para verificar a integridade do cluster.Coletando algumas informa\u00e7\u00f5es.kubectl cluster-infoVerificando os Worker Nodes.kubectl get nodes -o wideAnalisando todos os recursos criados.kubectl get all -ACom isso podemos concluir que nosso cluster est\u00e1 funcionando corretamente.Com o cluster configurado vamos ao Cluster Autoscaler.O que \u00e9 o Cluster AutoscalerO Cluster Autoscaler \u00e9 uma ferramenta de gerenciamento autom\u00e1tico de recursos em clusters Kubernetes.Ele ajusta automaticamente o tamanho de um cluster Kubernetes, aumentando ou diminuindo o n\u00famero de Worker Nodes conforme a necessidade de execu\u00e7\u00e3o das cargas de trabalho.O Cluster Autoscaler toma decis\u00f5es com base na quantidade de pods em execu\u00e7\u00e3o e nas suas respectivas necessidades de recursos.Para saber mais sobre o Cluster Autoscaler acesse a documenta\u00e7\u00e3o oficial clicandoaqui.Instala\u00e7\u00e3o do Cluster AutoscalerVamos dividir os arquivos de insrala\u00e7\u00e3o e configura\u00e7\u00e3o do cluster em 3 partes:cluster_autoscaler_iam.tfcluster_autoscaler_chart.tfcluster_autoscaler_values.yamlVamos come\u00e7ar configurando as permiss\u00f5es.Primeiramente temos que pegar  o id da conta AWS e o id do OIDC Provider criado pelo cluster EKS.Para pegar o id do OIDC Provider execute o comando abaixo, alterando a vari\u00e1velcluster_name.aws eks describe-cluster --name <cluster_name> --query \"cluster.identity.oidc.issuer\" --output textCom o id da conta aws e o id do OIDC Provider, vamos criar o arquivocluster_autoscaler_iam.tfe colar o trecho do c\u00f3digo abaixo.Lembrado de alterar as vari\u00e1veis id-da-conta-aws e oidc.# Cria\u00e7\u00e3o da pol\u00edtica IAM para o Cluster Autoscaler resource \"aws_iam_policy\" \"cluster_autoscaler_policy\" {   name        = \"ClusterAutoscalerPolicy\"   description = \"Policy for Kubernetes Cluster Autoscaler\"   policy      = jsonencode({     Version = \"2012-10-17\",     Statement = [       {         Effect = \"Allow\",         Action = [           \"autoscaling:DescribeAutoScalingGroups\",           \"autoscaling:DescribeAutoScalingInstances\",           \"autoscaling:DescribeLaunchConfigurations\",           \"autoscaling:DescribeTags\",           \"autoscaling:SetDesiredCapacity\",           \"autoscaling:TerminateInstanceInAutoScalingGroup\",           \"ec2:DescribeInstances\",           \"ec2:DescribeLaunchTemplateVersions\",           \"ec2:DescribeTags\"         ],         Resource = \"*\"       }     ]   }) }  # Criar a IAM Role resource \"aws_iam_role\" \"cluster_autoscaler\" {   name = \"eks-cluster-autoscaler-role\"    assume_role_policy = jsonencode({     Version = \"2012-10-17\",     Statement = [       {         Effect = \"Allow\",         Principal = {           Federated = \"arn:aws:iam::<id-da-conta-aws>:oidc-provider/oidc.eks.us-east-1.amazonaws.com/id/<iodc>\"          },         Action = \"sts:AssumeRoleWithWebIdentity\",         Condition = {           StringEquals = {             \"oidc.eks.${var.region}.amazonaws.com/id/<iodc>:aud\" = \"sts.amazonaws.com\"             \"oidc.eks.${var.region}.amazonaws.com/id/<iodc>:sub\" = \"system:serviceaccount:kube-system:cluster-autoscaler\"           }         }       },     ],   }) }  # Criar a service account resource \"kubernetes_service_account\" \"cluster_autoscaler\" {   metadata {     name      = \"cluster-autoscaler\"     namespace = \"kube-system\"     annotations = {       \"eks.amazonaws.com/role-arn\" = aws_iam_role.cluster_autoscaler.arn     }   } }  # Atachar a policy na role resource \"aws_iam_role_policy_attachment\" \"cluster_autoscaler_policy_attachment\" {   policy_arn = aws_iam_policy.cluster_autoscaler_policy.arn  #\"arn:aws:iam::${data.aws_caller_identity.current.account_id}:policy/ClusterAutoscalerPolicy\"   role       = aws_iam_role.cluster_autoscaler.name }  # (Opcional) Se voc\u00ea estiver usando uma inst\u00e2ncia EC2 para executar o Cluster Autoscaler, crie um profile para a inst\u00e2ncia resource \"aws_iam_instance_profile\" \"cluster_autoscaler_instance_profile\" {   name = \"ClusterAutoscalerInstanceProfile\"   role = aws_iam_role.cluster_autoscaler.name }Enter fullscreen modeExit fullscreen modeCriamos uma IAM Policy chamada ClusterAutoscalerPolicy com as permiss\u00f5es necess\u00e1rias para o Cluster Autoscaler funcionar.Criamos uma IAM Role com as permiss\u00f5es necess\u00e1rias para o OIDC Provider.Criamos uma Service Account e \"atachamos\" a role criada.Opcional, se voc\u00ea estiver usando uma inst\u00e2ncia EC2 para executar o Cluster Autoscaler, crie um instance profile.Agora vamos configurar o Helm Chart, para isso crie um arquivo chamadocluster_autoscaler_chart.tfe cole o trecho de c\u00f3digo abaixo:resource \"helm_release\" \"cluster_autoscaler\" {   name       = \"cluster-autoscaler\"   repository = \"https://kubernetes.github.io/autoscaler\"   chart      = \"cluster-autoscaler\"   namespace  = \"kube-system\"   timeout    = 300   version = \"9.34.1\"    values = [     \"${file(\"cluster_autoscaler_values.yaml\")}\"   ]    set {     name  = \"autoDiscovery.clusterName\"     value = data.aws_eks_cluster.cluster.name   }    set {     name  = \"awsRegion\"     value = var.region   }    set {     name  = \"rbac.serviceAccount.create\"     value = \"false\"   }    set {     name  = \"rbac.serviceAccount.name\"     value = \"cluster-autoscaler\"   }  }Enter fullscreen modeExit fullscreen modePara configurar o Cluster Autoscaler com op\u00e7\u00f5es avan\u00e7adas do Helm chart, voc\u00ea pode ajustar v\u00e1rios par\u00e2metros que controlam o comportamento do autoscaler.O arquivo_ values.yaml_ permite configurar op\u00e7\u00f5es como escalonamento m\u00ednimo e m\u00e1ximo de Worker Nodes, controle de toler\u00e2ncias, m\u00e9tricas, intervalos de checagem, e muito mais.Agora crie o arquivocluster_autoscaler_values.yamle cole o trecho abaixo.Temos que ajustar alguns par\u00e2metros:clusterName-\u200aInserir o nome do cluster EKSawsRegion-\u200aInserir a regi\u00e3o da AWS## Ref: https://kubernetes.io/docs/concepts/configuration/assign-pod-node/#affinity-and-anti-affinity # affinity -- Affinity for pod assignment affinity: {}  # additionalLabels -- Labels to add to each object of the chart. additionalLabels: {}  autoDiscovery:   # cloudProviders `aws`, `gce`, `azure`, `magnum`, `clusterapi` and `oci` are supported by auto-discovery at this time   # AWS: Set tags as described in https://github.com/kubernetes/autoscaler/blob/master/cluster-autoscaler/cloudprovider/aws/README.md#auto-discovery-setup    # autoDiscovery.clusterName -- Enable autodiscovery for `cloudProvider=aws`, for groups matching `autoDiscovery.tags`.   # autoDiscovery.clusterName -- Enable autodiscovery for `cloudProvider=azure`, using tags defined in https://github.com/kubernetes/autoscaler/blob/master/cluster-autoscaler/cloudprovider/azure/README.md#auto-discovery-setup.   # Enable autodiscovery for `cloudProvider=clusterapi`, for groups matching `autoDiscovery.labels`.   # Enable autodiscovery for `cloudProvider=gce`, but no MIG tagging required.   # Enable autodiscovery for `cloudProvider=magnum`, for groups matching `autoDiscovery.roles`.   clusterName: cluster-workshop    # autoDiscovery.namespace -- Enable autodiscovery via cluster namespace for for `cloudProvider=clusterapi`   namespace:  # default    # autoDiscovery.tags -- ASG tags to match, run through `tpl`.   tags:     - k8s.io/cluster-autoscaler/enabled     - k8s.io/cluster-autoscaler/{{ .Values.autoDiscovery.clusterName }}   # - kubernetes.io/cluster/{{ .Values.autoDiscovery.clusterName }}    # autoDiscovery.roles -- Magnum node group roles to match.   roles:     - worker    # autoDiscovery.labels -- Cluster-API labels to match  https://github.com/kubernetes/autoscaler/blob/master/cluster-autoscaler/cloudprovider/clusterapi/README.md#configuring-node-group-auto-discovery   labels: []     # - color: green     # - shape: circle # autoscalingGroups -- For AWS, Azure AKS or Magnum. At least one element is required if not using `autoDiscovery`. For example: # <pre> # - name: asg1<br /> #   maxSize: 2<br /> #   minSize: 1 # </pre> # For Hetzner Cloud, the `instanceType` and `region` keys are also required. # <pre> # - name: mypool<br /> #   maxSize: 2<br /> #   minSize: 1<br /> #   instanceType: CPX21<br /> #   region: FSN1 # </pre> autoscalingGroups: [] # - name: asg1 #   maxSize: 2 #   minSize: 1 # - name: asg2 #   maxSize: 2 #   minSize: 1  # autoscalingGroupsnamePrefix -- For GCE. At least one element is required if not using `autoDiscovery`. For example: # <pre> # - name: ig01<br /> #   maxSize: 10<br /> #   minSize: 0 # </pre> autoscalingGroupsnamePrefix: [] # - name: ig01 #   maxSize: 10 #   minSize: 0 # - name: ig02 #   maxSize: 10 #   minSize: 0  # awsAccessKeyID -- AWS access key ID ([if AWS user keys used](https://github.com/kubernetes/autoscaler/blob/master/cluster-autoscaler/cloudprovider/aws/README.md#using-aws-credentials)) awsAccessKeyID: \"\"  # awsRegion -- AWS region (required if `cloudProvider=aws`) awsRegion: us-east-1  # awsSecretAccessKey -- AWS access secret key ([if AWS user keys used](https://github.com/kubernetes/autoscaler/blob/master/cluster-autoscaler/cloudprovider/aws/README.md#using-aws-credentials)) awsSecretAccessKey: \"\"  # azureClientID -- Service Principal ClientID with contributor permission to Cluster and Node ResourceGroup. # Required if `cloudProvider=azure` azureClientID: \"\"  # azureClientSecret -- Service Principal ClientSecret with contributor permission to Cluster and Node ResourceGroup. # Required if `cloudProvider=azure` azureClientSecret: \"\"  # azureResourceGroup -- Azure resource group that the cluster is located. # Required if `cloudProvider=azure` azureResourceGroup: \"\"  # azureSubscriptionID -- Azure subscription where the resources are located. # Required if `cloudProvider=azure` azureSubscriptionID: \"\"  # azureTenantID -- Azure tenant where the resources are located. # Required if `cloudProvider=azure` azureTenantID: \"\"  # azureUseManagedIdentityExtension -- Whether to use Azure's managed identity extension for credentials. If using MSI, ensure subscription ID, resource group, and azure AKS cluster name are set. You can only use one authentication method at a time, either azureUseWorkloadIdentityExtension or azureUseManagedIdentityExtension should be set. azureUseManagedIdentityExtension: false  # azureUseWorkloadIdentityExtension -- Whether to use Azure's workload identity extension for credentials. See the project here: https://github.com/Azure/azure-workload-identity for more details. You can only use one authentication method at a time, either azureUseWorkloadIdentityExtension or azureUseManagedIdentityExtension should be set. azureUseWorkloadIdentityExtension: false  # azureVMType -- Azure VM type. azureVMType: \"vmss\"  # azureEnableForceDelete -- Whether to force delete VMs or VMSS instances when scaling down. azureEnableForceDelete: false  # cloudConfigPath -- Configuration file for cloud provider. cloudConfigPath: \"\"  # cloudProvider -- The cloud provider where the autoscaler runs. # Currently only `gce`, `aws`, `azure`, `magnum` and `clusterapi` are supported. # `aws` supported for AWS. `gce` for GCE. `azure` for Azure AKS. # `magnum` for OpenStack Magnum, `clusterapi` for Cluster API. cloudProvider: aws  # clusterAPICloudConfigPath -- Path to kubeconfig for connecting to Cluster API Management Cluster, only used if `clusterAPIMode=kubeconfig-kubeconfig or incluster-kubeconfig` clusterAPICloudConfigPath: /etc/kubernetes/mgmt-kubeconfig  # clusterAPIConfigMapsNamespace -- Namespace on the workload cluster to store Leader election and status configmaps clusterAPIConfigMapsNamespace: \"\"  # clusterAPIKubeconfigSecret -- Secret containing kubeconfig for connecting to Cluster API managed workloadcluster # Required if `cloudProvider=clusterapi` and `clusterAPIMode=kubeconfig-kubeconfig,kubeconfig-incluster or incluster-kubeconfig` clusterAPIKubeconfigSecret: \"\"  # clusterAPIMode --  Cluster API mode, see https://github.com/kubernetes/autoscaler/blob/master/cluster-autoscaler/cloudprovider/clusterapi/README.md#connecting-cluster-autoscaler-to-cluster-api-management-and-workload-clusters # Syntax: workloadClusterMode-ManagementClusterMode # for `kubeconfig-kubeconfig`, `incluster-kubeconfig` and `single-kubeconfig` you always must mount the external kubeconfig using either `extraVolumeSecrets` or `extraMounts` and `extraVolumes` # if you dont set `clusterAPIKubeconfigSecret`and thus use an in-cluster config or want to use a non capi generated kubeconfig you must do so for the workload kubeconfig as well clusterAPIMode: incluster-incluster  # incluster-incluster, incluster-kubeconfig, kubeconfig-incluster, kubeconfig-kubeconfig, single-kubeconfig  # clusterAPIWorkloadKubeconfigPath -- Path to kubeconfig for connecting to Cluster API managed workloadcluster, only used if `clusterAPIMode=kubeconfig-kubeconfig or kubeconfig-incluster` clusterAPIWorkloadKubeconfigPath: /etc/kubernetes/value  # containerSecurityContext -- [Security context for container](https://kubernetes.io/docs/tasks/configure-pod-container/security-context/) containerSecurityContext: {}   # capabilities:   #   drop:   #   - ALL  deployment:   # deployment.annotations -- Annotations to add to the Deployment object.   annotations: {}  # dnsPolicy -- Defaults to `ClusterFirst`. Valid values are: # `ClusterFirstWithHostNet`, `ClusterFirst`, `Default` or `None`. # If autoscaler does not depend on cluster DNS, recommended to set this to `Default`. dnsPolicy: ClusterFirst  # envFromConfigMap -- ConfigMap name to use as envFrom. envFromConfigMap: \"\"  # envFromSecret -- Secret name to use as envFrom. envFromSecret: \"\"  ## Priorities Expander # expanderPriorities -- The expanderPriorities is used if `extraArgs.expander` contains `priority` and expanderPriorities is also set with the priorities. # If `extraArgs.expander` contains `priority`, then expanderPriorities is used to define cluster-autoscaler-priority-expander priorities. # See: https://github.com/kubernetes/autoscaler/blob/master/cluster-autoscaler/expander/priority/readme.md expanderPriorities: {}  # extraArgs -- Additional container arguments. # Refer to https://github.com/kubernetes/autoscaler/blob/master/cluster-autoscaler/FAQ.md#what-are-the-parameters-to-ca for the full list of cluster autoscaler # parameters and their default values. # Everything after the first _ will be ignored allowing the use of multi-string arguments. extraArgs:   logtostderr: true   stderrthreshold: info   v: 4   # write-status-configmap: true   # status-config-map-name: cluster-autoscaler-status   # leader-elect: true   # leader-elect-resource-lock: endpoints   # skip-nodes-with-local-storage: true   # expander: random   # scale-down-enabled: true   # balance-similar-node-groups: true   # min-replica-count: 0   # scale-down-utilization-threshold: 0.5   # scale-down-non-empty-candidates-count: 30   # max-node-provision-time: 15m0s   # scan-interval: 10s   # scale-down-delay-after-add: 10m   # scale-down-delay-after-delete: 0s   # scale-down-delay-after-failure: 3m   # scale-down-unneeded-time: 10m   # skip-nodes-with-system-pods: true   # balancing-ignore-label_1: first-label-to-ignore   # balancing-ignore-label_2: second-label-to-ignore  # extraEnv -- Additional container environment variables. extraEnv: {}  # extraEnvConfigMaps -- Additional container environment variables from ConfigMaps. extraEnvConfigMaps: {}  # extraEnvSecrets -- Additional container environment variables from Secrets. extraEnvSecrets: {}  # extraVolumeMounts -- Additional volumes to mount. extraVolumeMounts: []   # - name: ssl-certs   #   mountPath: /etc/ssl/certs/ca-certificates.crt   #   readOnly: true  # extraVolumes -- Additional volumes. extraVolumes: []   # - name: ssl-certs   #   hostPath:   #     path: /etc/ssl/certs/ca-bundle.crt  # extraVolumeSecrets -- Additional volumes to mount from Secrets. extraVolumeSecrets: {}   # autoscaler-vol:   #   mountPath: /data/autoscaler/   # custom-vol:   #   name: custom-secret   #   mountPath: /data/custom/   #   items:   #     - key: subkey   #       path: mypath  # fullnameOverride -- String to fully override `cluster-autoscaler.fullname` template. fullnameOverride: \"\"  # hostNetwork -- Whether to expose network interfaces of the host machine to pods. hostNetwork: false  image:   # image.repository -- Image repository   repository: registry.k8s.io/autoscaling/cluster-autoscaler   # image.tag -- Image tag   tag: v1.30.0   # image.pullPolicy -- Image pull policy   pullPolicy: IfNotPresent   ## Optionally specify an array of imagePullSecrets.   ## Secrets must be manually created in the namespace.   ## ref: https://kubernetes.io/docs/tasks/configure-pod-container/pull-image-private-registry/   ##   # image.pullSecrets -- Image pull secrets   pullSecrets: []   # - myRegistrKeySecretName  # kubeTargetVersionOverride -- Allow overriding the `.Capabilities.KubeVersion.GitVersion` check. Useful for `helm template` commands. kubeTargetVersionOverride: \"\"  # kwokConfigMapName -- configmap for configuring kwok provider kwokConfigMapName: \"kwok-provider-config\"  # magnumCABundlePath -- Path to the host's CA bundle, from `ca-file` in the cloud-config file. magnumCABundlePath: \"/etc/kubernetes/ca-bundle.crt\"  # magnumClusterName -- Cluster name or ID in Magnum. # Required if `cloudProvider=magnum` and not setting `autoDiscovery.clusterName`. magnumClusterName: \"\"  # nameOverride -- String to partially override `cluster-autoscaler.fullname` template (will maintain the release name) nameOverride: \"\"  # nodeSelector -- Node labels for pod assignment. Ref: https://kubernetes.io/docs/user-guide/node-selection/. nodeSelector: {}  # podAnnotations -- Annotations to add to each pod. podAnnotations:   cluster-autoscaler.kubernetes.io/safe-to-evict: \"false\"  # podDisruptionBudget -- Pod disruption budget. podDisruptionBudget:   maxUnavailable: 1   # minAvailable: 2  # podLabels -- Labels to add to each pod. podLabels: {}  # priorityClassName -- priorityClassName priorityClassName: \"system-cluster-critical\"  # priorityConfigMapAnnotations -- Annotations to add to `cluster-autoscaler-priority-expander` ConfigMap. priorityConfigMapAnnotations: {}   # key1: \"value1\"   # key2: \"value2\"  ## Custom PrometheusRule to be defined ## The value is evaluated as a template, so, for example, the value can depend on .Release or .Chart ## ref: https://github.com/coreos/prometheus-operator#customresourcedefinitions prometheusRule:   # prometheusRule.enabled -- If true, creates a Prometheus Operator PrometheusRule.   enabled: false   # prometheusRule.additionalLabels -- Additional labels to be set in metadata.   additionalLabels: {}   # prometheusRule.namespace -- Namespace which Prometheus is running in.   namespace: monitoring   # prometheusRule.interval -- How often rules in the group are evaluated (falls back to `global.evaluation_interval` if not set).   interval: null   # prometheusRule.rules -- Rules spec template (see https://github.com/prometheus-operator/prometheus-operator/blob/master/Documentation/api.md#rule).   rules: []  rbac:   # rbac.create -- If `true`, create and use RBAC resources.   create: true   # rbac.pspEnabled -- If `true`, creates and uses RBAC resources required in the cluster with [Pod Security Policies](https://kubernetes.io/docs/concepts/policy/pod-security-policy/) enabled.   # Must be used with `rbac.create` set to `true`.   pspEnabled: false   # rbac.clusterScoped -- if set to false will only provision RBAC to alter resources in the current namespace. Most useful for Cluster-API   clusterScoped: true   serviceAccount:     # rbac.serviceAccount.annotations -- Additional Service Account annotations.     annotations: {}     # rbac.serviceAccount.create -- If `true` and `rbac.create` is also true, a Service Account will be created.     create: true     # rbac.serviceAccount.name -- The name of the ServiceAccount to use. If not set and create is `true`, a name is generated using the fullname template.     name: \"\"     # rbac.serviceAccount.automountServiceAccountToken -- Automount API credentials for a Service Account.     automountServiceAccountToken: true  # replicaCount -- Desired number of pods replicaCount: 1  # resources -- Pod resource requests and limits. resources: {}   # limits:   #   cpu: 100m   #   memory: 300Mi   # requests:   #   cpu: 100m   #   memory: 300Mi  # revisionHistoryLimit -- The number of revisions to keep. revisionHistoryLimit: 10  # securityContext -- [Security context for pod](https://kubernetes.io/docs/tasks/configure-pod-container/security-context/) securityContext: {}   # runAsNonRoot: true   # runAsUser: 1001   # runAsGroup: 1001  service:   # service.create -- If `true`, a Service will be created.   create: true   # service.annotations -- Annotations to add to service   annotations: {}   # service.labels -- Labels to add to service   labels: {}   # service.externalIPs -- List of IP addresses at which the service is available. Ref: https://kubernetes.io/docs/user-guide/services/#external-ips.   externalIPs: []    # service.loadBalancerIP -- IP address to assign to load balancer (if supported).   loadBalancerIP: \"\"   # service.loadBalancerSourceRanges -- List of IP CIDRs allowed access to load balancer (if supported).   loadBalancerSourceRanges: []   # service.servicePort -- Service port to expose.   servicePort: 8085   # service.portName -- Name for service port.   portName: http   # service.type -- Type of service to create.   type: ClusterIP  ## Are you using Prometheus Operator? serviceMonitor:   # serviceMonitor.enabled -- If true, creates a Prometheus Operator ServiceMonitor.   enabled: false   # serviceMonitor.interval -- Interval that Prometheus scrapes Cluster Autoscaler metrics.   interval: 10s   # serviceMonitor.namespace -- Namespace which Prometheus is running in.   namespace: monitoring   ## [Prometheus Selector Label](https://github.com/helm/charts/tree/master/stable/prometheus-operator#prometheus-operator-1)   ## [Kube Prometheus Selector Label](https://github.com/helm/charts/tree/master/stable/prometheus-operator#exporters)   # serviceMonitor.selector -- Default to kube-prometheus install (CoreOS recommended), but should be set according to Prometheus install.   selector:     release: prometheus-operator   # serviceMonitor.path -- The path to scrape for metrics; autoscaler exposes `/metrics` (this is standard)   path: /metrics   # serviceMonitor.annotations -- Annotations to add to service monitor   annotations: {}   ## [RelabelConfig](https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#monitoring.coreos.com/v1.RelabelConfig)   # serviceMonitor.metricRelabelings -- MetricRelabelConfigs to apply to samples before ingestion.   metricRelabelings: {}  # tolerations -- List of node taints to tolerate (requires Kubernetes >= 1.6). tolerations: []  # topologySpreadConstraints -- You can use topology spread constraints to control how Pods are spread across your cluster among failure-domains such as regions, zones, nodes, and other user-defined topology domains. (requires Kubernetes >= 1.19). topologySpreadConstraints: []   # - maxSkew: 1   #   topologyKey: topology.kubernetes.io/zone   #   whenUnsatisfiable: DoNotSchedule   #   labelSelector:   #     matchLabels:   #       app.kubernetes.io/instance: cluster-autoscaler  # updateStrategy -- [Deployment update strategy](https://kubernetes.io/docs/concepts/workloads/controllers/deployment/#strategy) updateStrategy: {}   # rollingUpdate:   #   maxSurge: 1   #   maxUnavailable: 0   # type: RollingUpdate  # vpa -- Configure a VerticalPodAutoscaler for the cluster-autoscaler Deployment. vpa:   # vpa.enabled -- If true, creates a VerticalPodAutoscaler.   enabled: false   # vpa.updateMode -- [UpdateMode](https://github.com/kubernetes/autoscaler/blob/vertical-pod-autoscaler/v0.13.0/vertical-pod-autoscaler/pkg/apis/autoscaling.k8s.io/v1/types.go#L124)   updateMode: \"Auto\"   # vpa.containerPolicy -- [ContainerResourcePolicy](https://github.com/kubernetes/autoscaler/blob/vertical-pod-autoscaler/v0.13.0/vertical-pod-autoscaler/pkg/apis/autoscaling.k8s.io/v1/types.go#L159). The containerName is always et to the deployment's container name. This value is required if VPA is enabled.   containerPolicy: {}  # secretKeyRefNameOverride -- Overrides the name of the Secret to use when loading the secretKeyRef for AWS and Azure env variables secretKeyRefNameOverride: \"\"Enter fullscreen modeExit fullscreen modeAlgumas configura\u00e7\u00f5es que podem ser personalizadas no arquivo de values:autoDiscovery: Configura o nome do cluster para descoberta autom\u00e1tica de grupos de Auto Scaling.extraArgs: Define argumentos adicionais para o Cluster Autoscaler, como pol\u00edticas de escalonamento e thresholds.rbac: Configura a conta de servi\u00e7o e as permiss\u00f5es RBAC.image: Define a vers\u00e3o da imagem do Cluster Autoscaler.resources: Especifica os recursos solicitados e limites para o pod do Cluster Autoscaler.nodeSelector, tolerations, affinity: Configura\u00e7\u00f5es para especificar onde os pods do Cluster Autoscaler podem ser agendados.replicaCount: Define o n\u00famero de r\u00e9plicas do Cluster Autoscaler.podAnnotations: Adiciona anota\u00e7\u00f5es ao pod do Cluster Autoscaler.Ap\u00f3s criar todos os arquivos acima, vamos aplica-lo\u00b4s com o Terraform executando o comando abaixo:terraform apply --auto-approveAcompanhe os logs do Cluster Autoscaler para avaliar se o deploy ocorreu com sucesso.kubectl -n kube-system logs -f deployment/cluster-autoscaler-aws-cluster-autoscalerCaso esteja tudo certo o Cluster Autoscaler est\u00e1 operacional e pronto para testes de escalonamento.Teste o escalonamento autom\u00e1ticoVamos iniciar os teste o escalonamento autom\u00e1tico para isso vamos obter algumas informa\u00e7\u00f5es, criar alguns recursos e acompanhar os resultados.Observe a quantidade de Worker Nodes atuais com o comando abaixo:kubectl get nodesObserve que nesse momento temos somente 1 Worker Node dispon\u00edvel.Vamos criar um deployment para os testes de stress.Crie um arquivo com o nome decpu-stress-deployment.yamle cole o c\u00f3digo abaixo:apiVersion: apps/v1 kind: Deployment metadata:   name: cpu-stress spec:   replicas: 5   selector:     matchLabels:       app: cpu-stress   template:     metadata:       labels:         app: cpu-stress     spec:       containers:       - name: cpu-stress         image: vish/stress         resources:           requests:             cpu: \"1\"         args:         - -cpus         - \"1\"Enter fullscreen modeExit fullscreen modeAplique o deployment com o comando:kubectl apply -f cpu-stress-deployment.yamlObserve o comportamento do Cluster Autoscaler, que deve aumentar o n\u00famero de Worker Nodes para acomodar o workload adicional.Acopanhe os logs do Cluster Autoscaler.kubectl -n kube-system logs -f deployment/cluster-autoscaler-aws-cluster-autoscalerAcompanhe os Worker Nodes escalando e nota-se que ele escalou v\u00e1rios Worker Nodes para acomodar o novo workload.kubectl get nodesVamos simular a redu\u00e7\u00e3o do workload excedente, voltando o ambiente normal.Vamos zerar a quantidade de pods no deployment e acompanhe os Worker Nodes sendo desprovisionados da infraestrutura e voltando ao seu estado original.Conclus\u00e3oUsar Terraform e Helm para configurar um cluster EKS e o Cluster Autoscaler proporciona uma solu\u00e7\u00e3o robusta e automatizada para gerenciar a escalabilidade dos clusters Kubernetes.Este artigo detalhado fornece os passos necess\u00e1rios para implementar e gerenciar o escalonamento autom\u00e1tico, garantindo que os recursos sejam utilizados de forma eficiente e econ\u00f4mica.Com estas ferramentas, voc\u00ea pode otimizar os custos e melhorar o desempenho das suas aplica\u00e7\u00f5es em um ambiente Kubernetes gerenciado pela AWS."}
{"title": "The Fool's Journey (through AWS)", "published_at": 1718888984, "tags": ["aws", "awscommunitybuilder"], "user": "Marty Henderson", "url": "https://dev.to/aws-builders/the-fools-journey-through-aws-52c0", "details": "TarotTarot cards are sometimes used as a form divination, but they are historically based on several suits or Arcana. Although you can study Tarot for quite a while and come up with different ways of interpreting a reading of Tarot, in Cartomancy Tarot the \"main\" suits, or Major Arcana, tell a life's journey. Numerically, the \"0\" or first card in the journey is called the Fool.The FoolThe Fool, contrary to English colloquialism, doesn't represent idiocy or stupidity, but rather ignorance. We all begin as a fool in life, lacking experience or going through the other Arcana. Indeed, most journeys start with a total lack of knowledge, but not a lack of common sense or will. The Fool starts the journey.Your AWS cloud journey will start the same way and go through the various Major Arcana as you climb to total AWS knowledge - a literally impossible task, but a goal to strive for nonetheless. In tarot, this last step is known as The World. With the World, you have all the knowledge and experience possible. Quite a grand goal in life, let alone on your AWS journey.(It should be noted that there are different interpretations of the cards, in various languages, and that I will mostly stick to the Rider-Waite terms, but in some, the World may be known as the Universe but have similar interpretations)Going through all 22 Major Arcana and what they mean for your journey would be a book onto itself, but I'd like to highlight some places you will go and show you that it is definitely okay.The MagicianThe first card after the Fool in is the Magician. The Magician itself can represent a few things, but the focus here is on potential. The Magician represents focused potential in the field, and this is where everyone starts.It doesn't matter if you've never opened a bash shell before or if you're able to do subnetting in your sleep. Every cloud journey begins with potential and having it focused on AWS learning is important.Some tips to get started into The Magician step of your journey is to commit to a goal, whether it's attaining a certification (such as the Cloud Practitioner), doing a specific task (such as setting up a blog), or simply measuring your knowledge (such as teaching a friend about what you learned). This helps your potential have a solid focus which lets your journey begin.The EmperorThe fifth card is called the Emperor. The Emperor represents authority and structure, which is a hard thing to accept on your journey. AWS specifically makes it a bit easier, as they offer white papers and blogs on specific topics or how things are supposed to be done, but accepting the \"how things are done\" step is important. Eventually, you might have a good reason to deviate from authority, but at this phase of your journey, learning, knowing, and following best practices is critical. For example, AWS recommends using accounts to isolate workloads. Instead of having all your cloud functions in one shared account, this isolation protects other workloads (in what is usually referred to as \"blast radius\"). Violating this best practice can lead to major issues when something starts to blow up or mutate over time and creates a potentially risky or fragile structure.StrengthStrength Arcana, the 12th one, represents something a little different and something we all want - a success. Eventually, in your journey, you will overcome a series of obstacles and eventually deploy a project to production.Enjoy it! It's a beautiful moment when you can see your work live, whether it's a certification, a website you can click on, watching metrics as others use what you've built, or something between. Your first major success is an incredibly fulfilling moment, knowing your journey has led there.Personally, I recall deploying an EKS Cluster on AWS (back when EKS was still in preview!) and putting my first confluent control plane on it. Years and many deployments later, I still remember the moment an application did it's first message in a queue on the Kafka setup I deployed. It's an incredible step on your journey and one you should hold tightly to on your journey - but acknowledge your journey is far from done. One success does not unlock all AWS knowledge to you!TemperanceTemperance, the 15th Arcana, can be about many things, but primarily about moderation. At some point, you will architect beautiful, stable designs, following best practices and show them to reviewers that will complain about expense, cognitive load, engineering overhead, or straight up complexity. Even though your design is probably the ideal one for a theoretical situation and solution, it will need to be tempered by frugality, both fiscal and labor, as well as moderation of usage.I've seen a number of web-based services use EC2 as their primary engine when ECS or even Lambdas would be a more elegant and faster solution. However, it will come to a point that the cost of an EC2 instance is worth it to an organization to reduce the engineering labor. Perhaps they have an excellent patching policy and don't yet have an understanding of how serverless truly works and worry about noisy neighbors or having their data left behind for another function to scoop up. There are also times where we might want a good service over another good one - such as leveraging Bedrock's models instead of making your own with SageMaker, but the fear of Generative AI forces usage of SageMaker instead.On my journey, I spent a lot of time in Temperance, and honestly, probably come back to it the most for when I talk about the journey most people take. Don't expect to rush through your journey, the experience is more important than becoming the perfect architect or engineer. After all, journey before destination.The TowerComing in at number 17, the Tower is an Arcana that represents upheaval or trouble. Eventually, you will own a solution and it will fail. Perhaps it simply goes down or failover fails or a malicious actor breaks it. Regardless of how it happens, your design will one day fail because you didn't consider something. This usually happens once you're more senior and can own complex architectures and designs.This step in the journey is not one unique to your cloud journey, either. There's always the ongoing joke that you aren't a senior developer until you've taken down production environments. However, this is also the greatest learning experience on your journey - not only learning about why your implementation failed, but also learning how those around you act. What happens if your design broke something serious enough to involve a C-level officer or Legal? How the people interact with your failure is just as important, perhaps more, than your cloud service's failure.The Tower is also one of the most stressful as it can lead to being removed from a project or being let go from a job. This is not the end of the journey! You not only can, but must learn how to recover from trouble to continue down the path you want to walk. The focus you learned during the Magician step of your journey should help you persist through the Tower, but it is never easy.The StarThe Star, the 19th Arcana, is where I feel I am at - it represents hope and opportunity. As you walk your journey, there eventually comes a time where you've built the previous Arcana and what you've learned from them to a bright spot. Opportunities happen, whether they are careers or others, such as being an AWS Community Builder. It is here that you can truly hold great titles in Cloud and work on complex and overwhelming projects with success and deliver to expectations to others around you and also your self-held expectations.The Star is an exciting part of your journey and one to definitely enjoy. I am not sure how long I will be on the Star step - hopefully longer than Temperance! - but I have yet to understand when you're stepping to the next part of the journey.Beyond the StarThis is where I leave you.There is more to learn and more things that need to happen with me to keep growing through the journey. Everyone's journey - yours, mine, your colleagues, your friends, everyone's - will be different. Perhaps you will move faster than I did. Or slower. Maybe you will have your great failure earlier than anyone else, but perhaps you will have years and years before the Tower calls for you. You don't really know until you get there.In summary, your journey will have many steps and many things to learn at each juncture. There are upsides and downsides along the way, but they all build you to be a better engineer or architect.Special ThanksMy knowledge of things such as tarot are limited, and I owe a great deal of gratitude to Dean ofNullsheen.comfor help with understanding of such concepts. Dean is a pretty cool guy working on custom tooling for Shadowrun, discussions about things such as cyberpunk, and other super-interesting topics. His site is definitely worth checking out!"}
{"title": "The Fastest Way to Download from Amazon S3 (using Raycast)", "published_at": 1718878938, "tags": ["aws", "productivity", "webdev", "programming"], "user": "Chris Cook", "url": "https://dev.to/aws-builders/the-fastest-way-to-download-from-amazon-s3-using-raycast-1d40", "details": "If you are a heavy Amazon S3 user like me, you might have wondered if there is a quick way to download a file from S3. After all, AWS created this wonderfuls3://protocol that contains all the information needed to get the file.Usually, I have to log in to the AWS console, fight with the multi-factor authentication, navigate to S3, search for the bucket, and then find the right file to download. It's annoying!I have created aRaycast Script Commandto optimize this process. Raycast is a shortcut manager that allows you to quickly launch an application or make a search query. It has replaced the Spotlight search on my macOS, which is usually opened withCMD+Space. A Script Command is a programmatic way to add a new shortcut to Raycast. In this case, my Script Command downloads a file from Amazon S3.Here is a short video of this action: I open Raycast withCMD+Space, typeS3to find the script, paste the S3 URL and hit Enter. The Script downloads the file from S3 and opens the folderDownloads:The Script Command is available on the officialGitHub repository, but I have pasted it here for convenience:#!/bin/bash# Required parameters:# @raycast.schemaVersion 1# @raycast.title S3 Download# @raycast.mode fullOutput# Optional parameters:# @raycast.icon images/amazon-s3.png# @raycast.packageName AWS# Documentation:# @raycast.description Download from Amazon S3 via URL# @raycast.argument1 { \"type\": \"text\", \"placeholder\": \"S3 URL\" }# @raycast.author Chris Cook# @raycast.authorURL https://github.com/zirkelcURL=$1# Ensure AWS CLI is installedif!command-vaws &> /dev/null;thenecho\"AWS CLI not found. Please install AWS CLI.\"echo\"Installation instructions: https://docs.aws.amazon.com/cli/latest/userguide/install-cliv2.html\"exit1fi# Try matching different S3 URL patterns# https://dev.to/aws-builders/format-and-parse-amazon-s3-url-5e10if[[$URL=~ ^https?://s3\\.([a-z0-9-]+)\\.amazonaws\\.com/([^/]+)/(.*)$]];then# Regional hostname with path-styleBUCKET=\"${BASH_REMATCH[2]}\"KEY=\"${BASH_REMATCH[3]}\"elif[[$URL=~ ^https?://([^/]+)\\.s3\\.([a-z0-9-]+)\\.amazonaws\\.com/(.*)$]];then# Regional hostname with virtual-hosted-styleBUCKET=\"${BASH_REMATCH[1]}\"KEY=\"${BASH_REMATCH[3]}\"elif[[$URL=~ ^https?://s3\\.amazonaws\\.com/([^/]+)/(.*)$]];then# Legacy hostname with path-styleBUCKET=\"${BASH_REMATCH[1]}\"KEY=\"${BASH_REMATCH[2]}\"elif[[$URL=~ ^https?://([^/]+)\\.s3\\.amazonaws\\.com/(.*)$]];then# Legacy hostname with virtual-hosted-styleBUCKET=\"${BASH_REMATCH[1]}\"KEY=\"${BASH_REMATCH[2]}\"elif[[$URL=~ ^s3://([^/]+)/(.*)$]];then# S3 URIBUCKET=\"${BASH_REMATCH[1]}\"KEY=\"${BASH_REMATCH[2]}\"elseecho\"Invalid URL:$URL\"echo\"URL must match recognized S3 patterns.\"exit1fi# Check for empty bucket or keyif[[-z\"$BUCKET\"||-z\"$KEY\"]];thenecho\"Error extracting bucket and key from URL:$URL\"exit1fi# Remove trailing slash if present and set recursive download flagRECURSIVE=\"\"if[[\"$KEY\"=~ .*/$]];thenKEY=\"${KEY%/}\"# Remove trailing slash for directory keyRECURSIVE=\"--recursive\"fi# Set download pathDOWNLOAD_FOLDER=\"$HOME/Downloads\"DOWNLOAD_PATH=\"$DOWNLOAD_FOLDER/$KEY\"if[[-n\"$RECURSIVE\"]];thenecho\"Downloading directory$URLto$DOWNLOAD_PATH...\"elseecho\"Downloading file$URLto$DOWNLOAD_PATH...\"fi# Print bucket and keyecho\"Bucket:$BUCKET\"echo\"Key:$KEY\"# Use recursive if necessaryifaws s3cp\"s3://$BUCKET/$KEY\"\"$DOWNLOAD_PATH\"$RECURSIVE;thenecho\"Downloaded successfully.\"open\"$DOWNLOAD_FOLDER\"elseecho\"Download failed.\"exit1fiEnter fullscreen modeExit fullscreen modeI assume the code with the comments is explanatory enough. The cool thing is, it uses the AWS CLI installed on your local machine. So you don't have to expose your access keys to a third-party service; it stays on your machine. It also supports differentS3 URL formats, including the HTTPS endpoints, and you can even download an entire folder.If you want to use it yourself, follow these steps to add the Script to your own Raycast:Install script commands from this repository."}
{"title": "EC2 Snapshot Management: How to get AWS EC2 Snapshot Information with Python", "published_at": 1718850194, "tags": ["aws", "python"], "user": "Oluwasegun Adedigba", "url": "https://dev.to/aws-builders/ec2-snapshot-management-how-to-get-aws-ec2-snapshot-information-with-python-753", "details": "IntroductionAmazon Elastic Compute Cloud (EC2) snapshots are integral to data backup and disaster recovery strategies within AWS. They provide point-in-time copies of your EC2 instance volumes, allowing you to restore data quickly and reliably in the event of failures, data loss, or system corruption. As organizations scale their cloud infrastructure, managing these snapshots becomes increasingly complex and time-consuming. Automation is the key to simplifying this process, ensuring data integrity, and optimizing operational efficiency.In this blog post, we'll walk through a Python script that automates the extraction of snapshot information, including associated instance details. This script exports the gathered data to a CSV file for easy analysis and documentation. By leveraging this automated approach, you can streamline your workflow, maintain a robust backup strategy, and gain valuable insights into your AWS environment.PrerequisitesBefore diving into the script, ensure you have the following prerequisites:AWS Account: You need an active AWS account with EC2 instances and associated snapshots.AWS CLI and Boto3: The AWS Command Line Interface (CLI) and Boto3 (the AWS SDK for Python) should be installed and configured on your machine.Python Environment: Make sure you have Python installed on your local machine.IAM Permissions: The IAM user or role you use must have the necessary permissions to describe EC2 instances and snapshots. Typically,AmazonEC2ReadOnlyAccessis sufficient.Setting Up AWS CLI and Boto3First, install the AWS CLI and Boto3. Open your terminal and run:pipinstallawscli boto3Enter fullscreen modeExit fullscreen modeNext, configure the AWS CLI with your credentials:aws configureEnter fullscreen modeExit fullscreen modeYou'll be prompted to enter your AWS Access Key ID, Secret Access Key, default region, and output format. This configuration is essential for Boto3 to interact with your AWS environment.Automating Snapshot Information ExtractionTo automate the extraction of EC2 snapshot information, we need to perform the following steps:Retrieve the names of EC2 instances.Extract EC2 instance IDs from snapshot descriptions.Gather snapshot information and export it to a CSV file.1. Retrieving Instance NamesEach EC2 instance can have multiple tags, one of which is typically theNametag. This tag is crucial for identifying instances more easily.importboto3defget_instance_name(ec2,instance_id):response=ec2.describe_instances(InstanceIds=[instance_id])forreservationinresponse['Reservations']:forinstanceinreservation['Instances']:fortagininstance.get('Tags',[]):iftag['Key']=='Name':returntag['Value']return'N/A'Enter fullscreen modeExit fullscreen modeTheget_instance_namefunction queries AWS to describe the specified instance by its ID and iterates through the tags to find theNametag. If theNametag is not present, it returns 'N/A'.2. Extracting Instance IDs from Snapshot DescriptionsSnapshots in AWS often contain the instance ID in their descriptions. We can use a regular expression to extract these IDs.importredefextract_instance_id(description):match=re.search(r'i-[a-f0-9]+',description)ifmatch:returnmatch.group(0)return'N/A'Enter fullscreen modeExit fullscreen modeTheextract_instance_idfunction uses a regular expression to search for instance IDs (which match the patterni-[a-f0-9]+) within the snapshot description. If a match is found, it returns the instance ID; otherwise, it returns 'N/A'.3. Exporting Snapshot Information to CSVCombining the previous functions, we can now gather the snapshot information and export it to a CSV file.importcsvimportboto3defexport_snapshots_info_to_csv():ec2=boto3.client('ec2')# Connect to EC2 servicesnapshots=ec2.describe_snapshots(OwnerIds=['self'])['Snapshots']withopen('ec2_snapshots.csv',mode='w',newline='')ascsv_file:fieldnames=['Instance Name','Snapshot ID','Volume Size (GiB)','Snapshot Date Started']writer=csv.DictWriter(csv_file,fieldnames=fieldnames)writer.writeheader()forsnapshotinsnapshots:instance_id=extract_instance_id(snapshot['Description'])instance_name=get_instance_name(ec2,instance_id)snapshot_id=snapshot['SnapshotId']volume_size=snapshot['VolumeSize']snapshot_date=snapshot['StartTime'].strftime(\"%Y-%m-%d %H:%M:%S\")writer.writerow({'Instance Name':instance_name,'Snapshot ID':snapshot_id,'Volume Size (GiB)':volume_size,'Snapshot Date Started':snapshot_date})print(\"Snapshot information has been written to ec2_snapshots.csv.\")Enter fullscreen modeExit fullscreen modeTheexport_snapshots_info_to_csvfunction performs the following steps:Connect to the EC2 Service: Initializes a connection to the EC2 service using Boto3.Retrieve Snapshots: Fetches a list of snapshots owned by the account.Open CSV File: Opens a CSV file for writing.Iterate Through Snapshots: For each snapshot, it extracts the instance ID, retrieves the instance name, and collects other snapshot details.Write to CSV: Writes the gathered information to the CSV file.Running the ScriptTo run the script, save it to a file (e.g.,ec2_snapshot_info.py) and execute it using Python:python ec2_snapshot_info.pyEnter fullscreen modeExit fullscreen modeThis command will generate a CSV file (ec2_snapshots.csv) in the same directory, containing detailed information about your EC2 snapshots.Detailed Explanation of Script ComponentsAWS EC2 Client InitializationTheboto3.client('ec2')call initializes a client to interact with the EC2 service. This client will be used to make API calls to AWS.ec2=boto3.client('ec2')Enter fullscreen modeExit fullscreen modeDescribing SnapshotsThedescribe_snapshotsmethod fetches details about the snapshots. We specifyOwnerIds=['self']to retrieve snapshots owned by the account.snapshots=ec2.describe_snapshots(OwnerIds=['self'])['Snapshots']Enter fullscreen modeExit fullscreen modeWriting to CSVThe CSV module in Python simplifies writing tabular data to files. We usecsv.DictWriterto write rows of dictionaries to the CSV file. Each dictionary represents a row in the CSV.withopen('ec2_snapshots.csv',mode='w',newline='')ascsv_file:fieldnames=['Instance Name','Snapshot ID','Volume Size (GiB)','Snapshot Date Started']writer=csv.DictWriter(csv_file,fieldnames=fieldnames)writer.writeheader()forsnapshotinsnapshots:instance_id=extract_instance_id(snapshot['Description'])instance_name=get_instance_name(ec2,instance_id)snapshot_id=snapshot['SnapshotId']volume_size=snapshot['VolumeSize']snapshot_date=snapshot['StartTime'].strftime(\"%Y-%m-%d %H:%M:%S\")writer.writerow({'Instance Name':instance_name,'Snapshot ID':snapshot_id,'Volume Size (GiB)':volume_size,'Snapshot Date Started':snapshot_date})Enter fullscreen modeExit fullscreen modeHandling Missing InformationThe script gracefully handles missing information. If an instance ID or name is not found, it returns 'N/A'. This ensures that the script does not break and provides a complete CSV output.Time FormattingThestrftimemethod formats the snapshot start time into a human-readable string. This makes it easier to interpret the snapshot creation dates in the CSV file.snapshot_date=snapshot['StartTime'].strftime(\"%Y-%m-%d %H:%M:%S\")Enter fullscreen modeExit fullscreen modeConclusionAutomating EC2 snapshot management with Python significantly enhances your AWS infrastructure management. This script provides a reliable, repeatable process for documenting and analyzing your snapshots, ensuring your backup strategy is robust and well-documented.Incorporate this script into your regular AWS maintenance routines to gain better visibility into your snapshot strategy, optimize your backup processes, and free up time for more critical tasks. By leveraging automation, you can ensure your data is secure and readily available, enhancing the overall efficiency and reliability of your AWS environment.Feel free to customize and expand this script to suit your specific needs, such as adding more snapshot details or integrating with other AWS services. Automation is a powerful tool, and this script is just the beginning of what you can achieve with AWS and Python."}
{"title": "Integrating a Basic TensorFlow Model on AWS", "published_at": 1718813569, "tags": ["tensorflow", "aws", "model", "ai"], "user": "Mursal Furqan Kumbhar", "url": "https://dev.to/aws-builders/integrating-a-basic-tensorflow-model-on-aws-81a", "details": "Welcome to the exciting world of integrating machine learning models with cloud computing! In this article, we'll guide you through the process of deploying a basic TensorFlow model on Amazon Web Services (AWS). We'll explore the services you can leverage, discuss some practical use cases, and provide a hands-on example of a TensorFlow model that converts voice into text. Let's dive in!IntroductionTensorFlow is a powerful open-source library for machine learning and deep learning applications. AWS offers a suite of services that make it easier to deploy, manage, and scale your machine-learning models. By integrating TensorFlow with AWS, you can take advantage of the cloud's scalability, security, and ease of use to bring your models to production.AWS Services for TensorFlow IntegrationTo successfully integrate a TensorFlow model on AWS, you'll need to familiarize yourself with several key services:Amazon SageMaker: A fully managed service that provides every developer and data scientist with the ability to build, train, and deploy machine learning models quickly.AWS Lambda: A serverless compute service that lets you run code without provisioning or managing servers, ideal for running lightweight TensorFlow models.Amazon S3: A scalable object storage service that you can use to store data and models.AWS API Gateway: A service to create, publish, maintain, monitor, and secure APIs at any scale, which can be used to expose your TensorFlow model as an API.Amazon Polly: A service that turns text into lifelike speech, useful if you need to create interactive voice applications.Amazon Transcribe: A service that automatically converts speech into text, which can be used in conjunction with your TensorFlow model for voice recognition tasks.Use Cases for TensorFlow on AWSHere are some practical use cases for integrating TensorFlow models on AWS:1. Real-Time Voice TranscriptionUse a TensorFlow model to convert spoken language into text in real-time, which is useful for applications like live captioning, transcription services, and voice-controlled interfaces.2. Sentiment AnalysisDeploy a TensorFlow model to analyze customer reviews or social media posts to determine the sentiment (positive, negative, neutral), helping businesses understand customer feedback better.3. Image RecognitionUse TensorFlow to build image recognition models for applications in security, retail (like recognizing products on shelves), and healthcare (such as identifying anomalies in medical images).4. Predictive MaintenanceImplement predictive maintenance solutions by analyzing data from sensors and predicting when equipment will fail, allowing businesses to perform maintenance before issues occur.Example: Voice-to-Text Conversion Using TensorFlow on AWSNow, let's walk through an example of integrating a basic TensorFlow model that listens to voice and converts it into text.Step 1: Setting Up Your Environment1.1 Create an S3 BucketStore your TensorFlow model and any other necessary files in an S3 bucket.aws s3 mb s3://your-bucket-nameEnter fullscreen modeExit fullscreen mode1.2 Prepare Your TensorFlow ModelTrain your TensorFlow model locally and save it in the S3 bucket.# Example of saving a trained modelmodel.save('model.h5')Enter fullscreen modeExit fullscreen mode1.3 Upload the Model to S3aws s3cpmodel.h5 s3://your-bucket-name/model.h5Enter fullscreen modeExit fullscreen modeStep 2: Deploying the Model with Amazon SageMaker2.1 Create a SageMaker Notebook InstanceUse the SageMaker console to create a notebook instance for deploying your model.2.2 Load and Deploy the ModelOpen the SageMaker notebook and run the following code:importboto3importsagemakerfromsagemaker.tensorflowimportTensorFlowModelsagemaker_session=sagemaker.Session()role='your-iam-role'model=TensorFlowModel(model_data='s3://your-bucket-name/model.h5',role=role,framework_version='2.3.0')predictor=model.deploy(initial_instance_count=1,instance_type='ml.m4.xlarge')Enter fullscreen modeExit fullscreen modeStep 3: Creating a Lambda Function3.1 Create a Lambda FunctionUse the AWS Lambda console to create a new function. This function will load the TensorFlow model and process audio input.3.2 Write the Lambda Codeimportjsonimportboto3importtensorflowastfs3_client=boto3.client('s3')deflambda_handler(event,context):# Get the audio file from the eventbucket=event['Records'][0]['s3']['bucket']['name']key=event['Records'][0]['s3']['object']['key']audio_file=s3_client.get_object(Bucket=bucket,Key=key)['Body'].read()# Load the TensorFlow modelmodel=tf.keras.models.load_model('/tmp/model.h5')withopen('/tmp/audio.wav','wb')asf:f.write(audio_file)# Process the audio file and convert it to text# Placeholder for actual audio processing and predictiontext=\"predicted text from model\"return{'statusCode':200,'body':json.dumps(text)}Enter fullscreen modeExit fullscreen modeStep 4: Setting Up API Gateway4.1 Create a REST APIUse API Gateway to create a new REST API.4.2 Create a Resource and MethodCreate a resource (e.g.,/transcribe) and a POST method that triggers the Lambda function.Step 5: Testing the Integration5.1 Upload an Audio File to S3Upload an audio file that you want to transcribe to the S3 bucket.5.2 Invoke the APISend a POST request to the API Gateway endpoint with the audio file information.curl-XPOST https://your-api-id.execute-api.region.amazonaws.com/prod/transcribe-d'{\"bucket\": \"your-bucket-name\", \"key\": \"audio-file.wav\"}'Enter fullscreen modeExit fullscreen modeConclusionIntegrating TensorFlow models with AWS services opens up a world of possibilities for deploying scalable and efficient machine learning applications. Whether you're working on voice transcription, sentiment analysis, image recognition, or predictive maintenance, AWS provides the tools and services to bring your models to life. We hope this guide has given you a clear roadmap to start your journey with TensorFlow on AWS.Happy coding!"}
{"title": "Back2Basics: Running Workloads on Amazon EKS", "published_at": 1718787941, "tags": ["aws", "eks", "kubernetes", "karpenter"], "user": "Romar Cablao", "url": "https://dev.to/aws-builders/back2basics-running-workloads-on-amazon-eks-5e68", "details": "OverviewWelcome back to theBack2Basicsseries! In this part, we'll explore howKarpenter, a just-in-time node provisioner, automatically manages nodes based on your workload needs. We'll also walk you through deploying a voting application to showcase this functionality in action.If you haven't read the first part, you can check it out here:Back2Basics: Setting Up an Amazon EKS ClusterRomar Cablao for AWS Community Builders  \u30fb Jun 12#aws#eks#kubernetes#opentofuInfrastructure SetupIn the previous post, we covered the fundamentals of cluster provisioning usingOpenTofuand simple workload deployment. Now, we will enable additional addons includingKarpenterfor automatic node provisioning based on workload needs.First we need to uncomment these lines in03_eks.tfto create taints on the nodes managed by the initial node group.# Uncomment this if you will use Karpenter       # taints = {       #   init = {       #     key    = \"node\"       #     value  = \"initial\"       #     effect = \"NO_SCHEDULE\"       #   }       # }Enter fullscreen modeExit fullscreen modeTaints ensure that only pods configured to tolerate these taints can be scheduled on those nodes. This allows us to reserve the initial nodes for specific purposes whileKarpenterprovisions additional nodes for other workloads.We also need to uncomment the codes in04_karpenterand05_addonsto activateKarpenterand provision other addons.Once updated, we have to runtofu init,tofu planandtofu apply. When prompted to confirm, typeyesto proceed with provisioning the additional resources.KarpenterKarpenter is an open-source project that automates node provisioning in Kubernetes clusters. By integrating with EKS, Karpenter dynamically scales the cluster by adding new nodes when workloads require additional resources and removing idle nodes to optimize costs. The Karpenter configuration defines different node classes and pools for specific workload types, ensuring efficient resource allocation. Read more:https://karpenter.sh/docs/The template04_karpenterdefines several node classes and pools categorized by workload type. These include:critical-workloads: for running essential cluster addonsmonitoring: dedicated to Grafana and other monitoring toolsvote-app: for the voting application we'll be deployingWorkload SetupThe voting application consists of several components:vote,result,worker,redis, andpostgresql. While we'll deploy everything on Kubernetes for simplicity, you can leverage managed services likeAmazon ElastiCache for RedisandAmazon RDSfor a production environment.ComponentDescriptionVoteHandles receiving and processing votes.ResultProvides real-time visualizations of the current voting results.WorkerSynchronizes votes between Redis and PostgreSQL.RedisStores votes temporarily, easing the load on PostgreSQL.PostgreSQLStores all votes permanently for secure and reliable data access.Here's the Voting App UI for both voting and results.Deployment Using Kubernetes ManifestIf you explore theworkloads/manifestdirectory, you'll find separate YAML files for each workload. Let's take a closer look at the components used for stateful applications likepostgresandredis:apiVersion: v1 kind: Secret ... --- apiVersion: v1 kind: PersistentVolumeClaim ... --- apiVersion: apps/v1 kind: StatefulSet ... --- apiVersion: v1 kind: Service ...Enter fullscreen modeExit fullscreen modeAs you may see,Secret,PersistentVolumeClaim,StatefulSetandServicewere used forpostgresandredis. Let's take a quick review of the following API objects used:Secret- used to store and manage sensitive information such as passwords, tokens, and keys.PersistentVolumeClaim- a request for storage, used to provision persistent storage dynamically.StatefulSet- manages stateful applications with guarantees about the ordering and uniqueness ofpods.Service- used for exposing an application that is running as one or morepodsin the cluster.Now, lets viewvote-app.yaml,results-app.yamlandworker.yaml:apiVersion: v1 kind: ConfigMap ... --- apiVersion: apps/v1 kind: Deployment ... --- apiVersion: v1 kind: Service ...Enter fullscreen modeExit fullscreen modeSimilar topostgresandredis, we have used a service for stateless workloads. Then we introduce the use ofConfigmapandDeployment.Configmap- stores non-confidential configuration data in key-value pairs, decoupling configurations from code.Deployment- used to provide declarative updates forpodsandreplicasets, typically used for stateless workloads.And lastly theingress.yaml. To make our service accessible from outside the cluster, we'll use anIngress. This API object manages external access to the services in a cluster, typically in HTTP/S.apiVersion: networking.k8s.io/v1 kind: Ingress ...Enter fullscreen modeExit fullscreen modeNow that we've examined the manifest files, let's deploy them to the cluster. You can use the following command to apply all YAML files within theworkloads/manifest/directory:kubectl apply -f workloads/manifest/Enter fullscreen modeExit fullscreen modeFor more granular control, you can apply each YAML file individually. To clean up the deployment later, simply runkubectl delete -f workloads/manifest/While manifest files are a common approach, there are alternative tools for deployment management:Kustomize: This tool allows customizing raw YAML files for various purposes without modifying the original files.Helm: A popular package manager for Kubernetes applications. Helm charts provide a structured way to define, install, and upgrade even complex applications within the cluster.Deployment Using KustomizeLet's checkKustomize. If you haven't installed it's binary, you can refer toKustomize Installation Docs. This example utilizes an overlay file to make specific changes to the default configuration. To apply the builtkustomization, you can run the command:kustomize build .\\workloads\\kustomize\\overlays\\dev\\ | kubectl apply -f -Enter fullscreen modeExit fullscreen modeHere's what we've modified:Added an annotation:note: \"Back2Basics: A Series\".Set the replicas for both thevoteandresultdeployments to3.To check you can refer to the commands below:D:\\> kubectl get pod -o custom-columns=NAME:.metadata.name,ANNOTATIONS:.metadata.annotations NAME                          ANNOTATIONS postgres-0                    map[note:Back2Basics: A Series] redis-0                       map[note:Back2Basics: A Series] result-app-6c9dd6d458-8hxkf   map[note:Back2Basics: A Series] result-app-6c9dd6d458-l4hp9   map[note:Back2Basics: A Series] result-app-6c9dd6d458-r5srd   map[note:Back2Basics: A Series] vote-app-cfd5fc88-lsbzx       map[note:Back2Basics: A Series] vote-app-cfd5fc88-mdblb       map[note:Back2Basics: A Series] vote-app-cfd5fc88-wz5ch       map[note:Back2Basics: A Series] worker-bf57ddcb8-kkk79        map[note:Back2Basics: A Series]   D:\\> kubectl get deploy NAME         READY   UP-TO-DATE   AVAILABLE   AGE result-app   3/3     3            3           5m vote-app     3/3     3            3           5m worker       1/1     1            1           5mEnter fullscreen modeExit fullscreen modeTo remove all the resources we created, run the following command:kustomize build .\\workloads\\kustomize\\overlays\\dev\\ | kubectl delete -f -Enter fullscreen modeExit fullscreen modeDeployment Using Helm ChartNext to check isHelm. If you haven't installed helm binary, you can refer toHelm Installation Docs. Once installed, lets add a repository and update.helm repo add thecloudspark https://thecloudspark.github.io/helm-charts helm repo updateEnter fullscreen modeExit fullscreen modeNext, create avalues.yamland add some overrides to the default configuration. You can also use existing config inworkloads/helm/values.yaml. This is how it looks like:ingress:   enabled: true   className: alb   annotations:     alb.ingress.kubernetes.io/scheme: internet-facing     alb.ingress.kubernetes.io/target-type: instance  # Vote Handler Config vote:   tolerations:     - key: app       operator: Equal       value: vote-app       effect: NoSchedule   nodeSelector:     app: vote-app   service:     type: NodePort  # Results Handler Config result:   tolerations:     - key: app       operator: Equal       value: vote-app       effect: NoSchedule   nodeSelector:     app: vote-app   service:     type: NodePort  # Worker Handler Config worker:   tolerations:     - key: app       operator: Equal       value: vote-app       effect: NoSchedule   nodeSelector:     app: vote-appEnter fullscreen modeExit fullscreen modeAs you may see, we addednodeSelectorandtolerationsto make sure that thepodswill be scheduled on the dedicated nodes where we wanted them to run. This Helm chart offers various configuration options and you can explore them in more detail onArtifactHub: Vote App.Now install the chart and apply overrides from values.yaml# Install helm install app -f workloads/helm/values.yaml thecloudspark/vote-app  # Upgrade helm upgrade app -f workloads/helm/values.yaml thecloudspark/vote-appEnter fullscreen modeExit fullscreen modeWait for the pods to be up and running, then access the UI using the provisioned application load balancer.To uninstall just run the command below.helm uninstall appEnter fullscreen modeExit fullscreen modeGoing back to KarpenterUnder the hood,Karpenterprovisioned nodes used by the voting app we've deployed. The sample logs you see here provide insights into it's activities:{\"level\":\"INFO\",\"time\":\"2024-06-16T10:15:38.739Z\",\"logger\":\"controller.provisioner\",\"message\":\"found provisionable pod(s)\",\"commit\":\"fb4d75f\",\"pods\":\"default/result-app-6c9dd6d458-l4hp9, default/worker-bf57ddcb8-kkk79, default/vote-app-cfd5fc88-lsbzx\",\"duration\":\"153.662007ms\"} {\"level\":\"INFO\",\"time\":\"2024-06-16T10:15:38.739Z\",\"logger\":\"controller.provisioner\",\"message\":\"computed new nodeclaim(s) to fit pod(s)\",\"commit\":\"fb4d75f\",\"nodeclaims\":1,\"pods\":3} {\"level\":\"INFO\",\"time\":\"2024-06-16T10:15:38.753Z\",\"logger\":\"controller.provisioner\",\"message\":\"created nodeclaim\",\"commit\":\"fb4d75f\",\"nodepool\":\"vote-app\",\"nodeclaim\":\"vote-app-r9z7s\",\"requests\":{\"cpu\":\"510m\",\"memory\":\"420Mi\",\"pods\":\"8\"},\"instance-types\":\"m5.2xlarge, m5.4xlarge, m5.large, m5.xlarge, m5a.2xlarge and 55 other(s)\"} {\"level\":\"INFO\",\"time\":\"2024-06-16T10:15:41.894Z\",\"logger\":\"controller.nodeclaim.lifecycle\",\"message\":\"launched nodeclaim\",\"commit\":\"fb4d75f\",\"nodeclaim\":\"vote-app-r9z7s\",\"provider-id\":\"aws:///ap-southeast-1b/i-028457815289a8470\",\"instance-type\":\"t3.small\",\"zone\":\"ap-southeast-1b\",\"capacity-type\":\"spot\",\"allocatable\":{\"cpu\":\"1700m\",\"ephemeral-storage\":\"14Gi\",\"memory\":\"1594Mi\",\"pods\":\"11\"}} {\"level\":\"INFO\",\"time\":\"2024-06-16T10:16:08.946Z\",\"logger\":\"controller.nodeclaim.lifecycle\",\"message\":\"registered nodeclaim\",\"commit\":\"fb4d75f\",\"nodeclaim\":\"vote-app-r9z7s\",\"provider-id\":\"aws:///ap-southeast-1b/i-028457815289a8470\",\"node\":\"ip-10-0-206-99.ap-southeast-1.compute.internal\"} {\"level\":\"INFO\",\"time\":\"2024-06-16T10:16:23.631Z\",\"logger\":\"controller.nodeclaim.lifecycle\",\"message\":\"initialized nodeclaim\",\"commit\":\"fb4d75f\",\"nodeclaim\":\"vote-app-r9z7s\",\"provider-id\":\"aws:///ap-southeast-1b/i-028457815289a8470\",\"node\":\"ip-10-0-206-99.ap-southeast-1.compute.internal\",\"allocatable\":{\"cpu\":\"1700m\",\"ephemeral-storage\":\"15021042452\",\"hugepages-1Gi\":\"0\",\"hugepages-2Mi\":\"0\",\"memory\":\"1663292Ki\",\"pods\":\"11\"}}Enter fullscreen modeExit fullscreen modeAs shown in the logs, whenKarpenterfound pod/s that needs to be scheduled, a new node claim was created, launched and initialized. So whenever there is a need for additional resources, this component is responsible in fulfilling it.Additionally,Karpenterautomatically labels nodes it provisions withkarpenter.sh/initialized=true. Let's usekubectlto see these nodes:kubectl get nodes -l karpenter.sh/initialized=trueEnter fullscreen modeExit fullscreen modeThis command will list all nodes that have this specific label. As you can see in the output below, three nodes have been provisioned byKarpenter:NAME                                              STATUS   ROLES    AGE   VERSION ip-10-0-208-50.ap-southeast-1.compute.internal    Ready    <none>   10m   v1.30.0-eks-036c24b ip-10-0-220-238.ap-southeast-1.compute.internal   Ready    <none>   10m   v1.30.0-eks-036c24b ip-10-0-206-99.ap-southeast-1.compute.internal    Ready    <none>   1m    v1.30.0-eks-036c24bEnter fullscreen modeExit fullscreen modeLastly, let's check related logs for node termination. This process involves removing nodes from the cluster. Decommissioning typically involves tainting the node first to prevent furtherpodscheduling, followed by node deletion.{\"level\":\"INFO\",\"time\":\"2024-06-16T10:35:39.165Z\",\"logger\":\"controller.disruption\",\"message\":\"disrupting via consolidation delete, terminating 1 nodes (0 pods) ip-10-0-206-99.ap-southeast-1.compute.internal/t3.small/spot\",\"commit\":\"fb4d75f\",\"command-id\":\"5e5489a6-a99d-4b8d-912c-df314a4b5cfa\"} {\"level\":\"INFO\",\"time\":\"2024-06-16T10:35:39.483Z\",\"logger\":\"controller.disruption.queue\",\"message\":\"command succeeded\",\"commit\":\"fb4d75f\",\"command-id\":\"5e5489a6-a99d-4b8d-912c-df314a4b5cfa\"} {\"level\":\"INFO\",\"time\":\"2024-06-16T10:35:39.511Z\",\"logger\":\"controller.node.termination\",\"message\":\"tainted node\",\"commit\":\"fb4d75f\",\"node\":\"ip-10-0-206-99.ap-southeast-1.compute.internal\"} {\"level\":\"INFO\",\"time\":\"2024-06-16T10:35:39.530Z\",\"logger\":\"controller.node.termination\",\"message\":\"deleted node\",\"commit\":\"fb4d75f\",\"node\":\"ip-10-0-206-99.ap-southeast-1.compute.internal\"} {\"level\":\"INFO\",\"time\":\"2024-06-16T10:35:39.989Z\",\"logger\":\"controller.nodeclaim.termination\",\"message\":\"deleted nodeclaim\",\"commit\":\"fb4d75f\",\"nodeclaim\":\"vote-app-r9z7s\",\"node\":\"ip-10-0-206-99.ap-southeast-1.compute.internal\",\"provider-id\":\"aws:///ap-southeast-1b/i-028457815289a8470\"}Enter fullscreen modeExit fullscreen modeWhat's Next?We've successfully deployed our voting application! And thanks toKarpenter, new nodes are added automatically when needed and terminates when not - making our setup more robust and cost effective. In the final part of this series, we'll delve into monitoring the voting application we've deployed withGrafanaandPrometheus, providing us the visibility into resource utilization and application health."}
{"title": "Unlocking Secure Web Access with Amazon WorkSpaces Secure Browser", "published_at": 1718774586, "tags": ["aws", "workspaces", "enduser", "community"], "user": "amalkabraham001", "url": "https://dev.to/aws-builders/unlocking-secure-web-access-with-amazon-workspaces-secure-browser-16m8", "details": "Amazon Workspaces Secure BrowserAmazon Workspaces Secure Browser offers a secure environment for users to access private websites, SaaS applications, and the public internet. It operates within the user\u2019s local browser, streaming encrypted pixels from a remote session hosted in the highly secure AWS cloud. Starting at just $7 per month, it eliminates the need for managing specialized client software, infrastructure, and VPN connections.This service is particularly beneficial for organizations implementing Bring-Your-Own-Device (BYOD) policies, as it ensures sensitive web content never directly touches the end user\u2019s device while providing cost-effective, secure access. Additionally, it\u2019s ideal for scenarios like customer support, analytics environments, and safe browsing for high-security networksEnd User ExperienceThe login page for Amazon WorkSpaces Secure Browser is seamlessly integrated with the user\u2019s identity provider (idP). When users click \u2018Sign In,\u2019 they are redirected to their respective idP provider\u2019s page (e.g., Entra login page). After successful authentication, users are redirected back to the Secure Browser page, where the application is initiated.As depicted in the screenshot below, we securely publish an intranet site using Amazon WorkSpaces Secure Browser. Within this environment, you can enforce controls such as disabling clipboard functionality and file transfers. By default, internet access is restricted for Secure Browser instances.Configuring WorkSpaces Secure BrowserIn the AWS Console, navigate to WorkSpaces secure browser. In the Secure Browser home page, click on \u201ccreate portal\u201d to initiate the creation of new Workspaces Secure Browser.In the \u201cSpecify networking connection\u201d page, select your VPC, private subnets and security group where the micro instances will get spin up to handle the workspaces browser and Click next.In the \u201cConfigure portal settings\u201d page, select the Display name of the portal, instance type and the maximum concurrent users/instance.The user access logging is captured into a kinesis data stream. Make sure you create the Kinesis data stream with the naming convention \u201camazon-workspaces-web-*\u201d and also the server authentication is disabled.The below screenshot shows the Kinesis data stream created for storing the user access logging details.You can also enable IP access control to ensure that the users from a particular network is only allowed to access the browser.The policy settings allow you to configure your browser startup page, URL filtering to allow and deny specific URLs and also to configure pre-build browser bookmarks.The \u201cselect user settings\" page allows the administrator to configure clipboard permissions, file transfer permissions and also session timeout settings.The \u201cconfigure identity provider\u201d settings allow administrators to integrate the secure browser via idp providers like Entra, OKTA etc.In this blog post, I used Microsoft Entra for authentication purposes.Select the radio button next to the \u201cStandard (external IdP) and click on \u201cContinue with standard IdP.Download the SP metadata file which we will be importing to our enterprise application in Entra.To configure the enterprise application, login to Azure portal and navigate to Entra ID.Under Enterprise applications, select \u201c+ new application\u201d.In the Browse Microsoft Entra Gallery page, select \"+ Create your own application\u201d.In the \u201cCreate your own application\u201d page, select the name of the application and select the radio button next to \u201cIntegrate any other application you don\u2019t find in the gallery\u201d and Click Finish.Go to the newly created enterprise application and click on Single sign on.In the single sign on method, select SAML option.In the SAML based sign-on page, click on \u201cupload metadata file, and select the SP metadata file we downloaded from the aws console.After successfully uploading the metadata file, the App federation metadata URL will get generated. Go back to the Workspaces IdP configuration page and click on the radio button next to \u201cEnter metadata endpoint URL\u201d and provide the URL copied from Entra App page.In the \u201cReview and launch\u201d page, verify the configuration and click on finish to launch the secure browser.The portal page looks like below and the status will become Active.Monitoring the Secure Browser.CloudWatch metrics can be utilized to monitor Amazon Workspaces Secure Browser. These metrics are defined under \u2018WorkSpacesWeb\u2019 and include five default portal metrics.Session AttemptSession successSession failureGlobal memory percentageglobal Cpu percentageThe kinesis data streams metrics can be also monitored using Cloudwatch.Auditing the WorkSpaces Secure Browser Admin activitiesCloudTrail can be used to monitor the admin activities related to the Secure browserThe End User access logging can be monitored via Kinesis data stream logs.The WorkSpaces secure browser is a good option for remote workers who only leverages Web/SaaS Applications as part of the day to day activities.Hope this blog helps you to understand how secure browser can be configured in a real scenario."}
{"title": "AWS Community Day 2024: A Landmark Event in Kenya's Tech Landscape", "published_at": 1718657023, "tags": [], "user": "Mark Orina", "url": "https://dev.to/aws-builders/aws-community-day-2024-a-landmark-event-in-kenyas-tech-landscape-5073", "details": "Nearly 50 days later, the excitement and inspiration from AWS Community Day 2024 still resonate with me. This event has left a lasting impact on Kenya\u2019s tech community, offering a blend of insightful learnings and the chance to meet incredible people. It was a remarkable milestone for everyone involved.AWS Community Day was the culmination of over 30 virtual meet-ups and 4 physical meet-ups, bringing together a dynamic community that\u2019s growing every day! I joined the AWS User Group in 2021 and became part of the organizing team in 2022. Since then, the community has expanded exponentially to over 3000 members, underscoring the importance of sharing AWS knowledge and advancing cloud computing.Here\u2019s a recap of this inaugural event that brought our vibrant community together.TL;DRTheme: \"Learn and Be Curious\"The event's theme was inspired by Amazon\u2019s leadership principles, emphasizing the importance of continuous learning and exploring new possibilities.Keynote Speakers:Jeff Barr - Chief Evangelist for AWS.Watch it hereDr. Aminah Zawedde - PhD, CISA, Permanent Sec. of Uganda's Ministry of ICT.Watch it hereEng. John Kipchumba Tanui, MBS - Principal Secretary - State Department for ICT and Digital Economy.Watch it hereRobin Njiru -  AWS Public Sector Lead for Sub-Saharan Africa.Watch it hereVenue: KCA UniversityEvent by the Numbers:Planning the Event:We began planning in November 2023. Securing the date and venue were the most critical aspects, we were initially aiming for April 13th but later settling on Saturday, April 20th, 2024. We had an amazing team that pulled together to get the venue locked.The organizing committee comprised AWS community builders, well-wishers, and cloud captains leading various key aspects of the event.Sponsors:Sponsors played a key role in ensuring the event was a success. Their generous support from financial resources to experts for panels & exhibiting during the event helped see our 1st community day be one for the books. This support did not come easy as potential sponsors needed to see the value they would get from the event.We had an AWS Community Day concept paper that communicated the event's aim and the attendee profile we targeted to attend for the event.Below are our esteemed sponsors who supported us. We could not be more grateful for walking with us.Call for Papers and Agenda:The most crucial aspect of the event was the Call for Papers and the crafting of the agenda. The organizing committee aimed to spark curiosity and provide a platform for learning and showcasing Kenya and Africa's amazing talent and knowledge. The call for presentations resulted in 38 submissions, of which 28 were accepted.Tracks and Submissions:DevOps:18 submissions, 13 acceptedGen AI/ML:9 submissions, 6 acceptedCybersecurity:5 submissions, all acceptedSustainability and Environment: 6 submissions, 4 acceptedTypes of Presentations:Paper Presentations: 17Technical Demonstrations: 21Below are the various Presentations and Panels:Gen AI/ML:Farah Abdirahman: Building Generative AI Applications Using Amazon BedrockChibuike Nwachukwu: Unleashing RAG: Serverless Q&A with Kendra, Bedrock & AmplifyNicabed Gathaba: Intro to Quantum ComputingShadab Hussain: Unveiling the Quantum Leap in Financial Modeling using AWS BraketAlvin Kamau Ndirangu: Augmenting Human Intelligence with AIDevOps:Peter Muchemi: Building Multitenant Application with AWSElsie Marion: Containers Don't Contain: Container Security 101Lewis Sawe: Decentralized Authentication with AWS CognitoKadima Samuel: DevOps: The Big PictureKelvin Kaoka: AWS SAM For Event-Driven Architectures: Building Serverless ArchitecturesMoracha Jacob: Building Serverless Application With AmplifyChris Otta: AWS IAM Roles for Secure GitOpsDaniel Kimani: SDLC AutomationAbby Nduta: Demystifying AWS Billing for BeginnersKevin Kiruri: Serverless Architecture on AWSWanjohi Christopher: Best Practices for Database Migration to AWS: A Technical Deep DiveEvans Kiprotich: Optimizing Your DR Strategy: Leveraging AWS DRS for Seamless RecoveryCybersecurity:Kurtis Bengo: Beyond the Perimeter: Safeguarding Your AWS Cloud Fortress with Battle-Tested PracticesAdonijah Kiplimo: Deepfakes: The Looming Threat & Building Cloud Security DefensesEmmanuel Mingle: AWS Certification Paths: Which Certification is Well Suited for My Future Role?Albertini Francis: Hacked!!...Not again!!... A Guide on Reducing Your Attack Surface on AWS CloudZipporah Wachira: A Defense in Depth Approach to Cloud SecuritySustainability and Environment:Diana Chepkirui/Clive Kamaliki: IoT & SustainabilitySumaiya Nalukwago: Developing Transferable/Soft Skills for a Sustainable Tech CareerVanessa Alumasa: AWS Cloud: Innovation for SustainabilityJudith Soi: Successful AWS TrainingThese are the various Panels that were held:Cybersecurity Panel DiscussionHost: Nelly NyadzuaExperts: Yinka Daramola, Washington,Licio Lentimo, Purity Njeri GachuhiDevOps Panel DiscussionHost: Mark OrinaExperts: Antony Wanyugi , Yinka Daramola ,Lemayian Nakolah, Kevin Karanga KiruriWomen in Tech Panel DiscussionHost: Linet KendiExperts: Zipporah Wachira, Ms. Cherin Onyango, Dr. Aminah Zawedde, Diana MuthoniAI/ML Panel Discussion HOSTHost: DR Kevin MugoyeExperts: Mr David Opondo, Daniells Adebimp, Lawrence Muema, Chris OttaAWS DeepRacer League:The Deep Racer League exemplified the power of teamwork, resilience, and continuous learning.The team organized virtual tracks of the AWS DeepRacer 3D racing simulator where each team put their models to the test.The rounds are virtual with the single fastest lap. The racers who were consistent progressed on to the final race.The team that cut were:1.MMU2.DEKUT3.KCA4.TUK5.Strath6.emobilisThe final round was held on the day of the community day which saw the winning team receive various gifts and prizes.The winning team was the MMU team.You can watch their presentation on how they were able to win the league:How to get into AWS Deep Racer & AWS AI & ML Scholarship programThanks to all the hard work of the AWS DeepRacer team, mentors, competition administrators, and emobilis for making this Race happen.AWS Community Day 2024 was a testament to the power of community and the endless possibilities unlocked by learning and curiosity. The event not only provided a platform for knowledge sharing and skill development but also reinforced the collaborative spirit that defines the AWS User Group - Kenya. We look forward to the next AWS Community Day, where we will once again come together to learn, share, and innovate. Until then, let's keep the spirit of curiosity alive and continue to explore the vast potential of AWS.We are looking forward to AWS Community DAY Kenya 2024.Join our community here :MeetUpsTwitter"}
{"title": "Amazon DevOps Guru for the Serverless applications - Part 11 Anomaly detection on SNS (kind of)", "published_at": 1718637290, "tags": ["aws", "serverless", "devops", "aiops"], "user": "Vadym Kazulkin", "url": "https://dev.to/aws-builders/amazon-devops-guru-for-the-serverless-applications-part-11-anomaly-detection-on-sns-kind-of-388", "details": "IntroductionIn the1st part of the serieswe introduced the Amazon DevOps Guru service, described its value proposition, the benefits of using it and explain how to configure it. We also need to go through all the steps in the2nd part of the seriesto set everything up. In the subsequent parts we saw DevOps Guru in action detecting anomalies on DynamoDB and Aurora Serverless v2, API Gateway and Lambda alone and also in conjunction with other AWS Serverless Services like SQS, Kinesis, Step Functions and Aurora Serverless v2.In this part of the series I'd like to explore whether DevOps Guru will recognize anomalies with Amazon Simple Notification Service (SNS)Detecting anomalies with SNSLet's enhance our architecture so that in case of creation of the new product we send the notification to the SNS Topic which then delivers this notification to other (external) HTTP(s) endpoint.Not let's imagine that this HTTP(s) endpoint was moved or answers with the 500 error code, so that SNS will consider the notification as not being delivered.I was able reproduce this scenario on AWS but deploying temporary API Gateway endpoint and configured as SNS subscription. I needed to confirm the subscription, so I put the Lambda behind my temporary API Gateway endpoint which was triggered for POST request (this is what SNS sends to the configured HTTP(s) endpoint as confirmation request). Then I logged the whole HTTP body of the POST request in my Lambda function and copied the subscription URL (which is a part of the HTTP body) which I entered in the browser. With SNS subscription being confirmed, I then deleted my temporary API Gateway endpoint so that SNS HTTP(s) subscription was sent but couldn't be delivered to the endpoint anymore.Then I sent several hundreds create product requests via thehey toollike :hey -q 1 -z 15m -c 1 -m PUT -d '{\"id\": 1, \"name\": \"Print 10x13\", \"price\": 0.15}' -H \"X-API-Key: XXXa6XXXX\" https://XXX.execute-api.eu-central-1.amazonaws.com/prod/productsEnter fullscreen modeExit fullscreen modewhich all failed to be delivered and have been retried (without success) 3 times by default, seeAmazon SNS message delivery retries.Despite seeing the NumberOfNotificationsFailed in CloudWatch metrics (see the blue line), no DevOpsGuru insight has been created even after re-trying this experiment several times.Then directly after this experiment and I immediately started another experiment to fetch not existing product from the database which then caused HTTP Error 404 (Not Found) on API Gateway. I was then surprised that the following insight has been created by DevOps Guru right away:with the following anomalous metricsNumberOfNotificationsFailed Average(for anomaly with SNS) and4XXX Error Average(for anomaly with API Gateway):and the following graphed anomalies :ConclusionIn this article we explored whether DevOps Guru will recognize anomalies with Amazon Simple Notification Service (SNS)  like the HTTPs Subscription which endpoint doesn't exist anymore (no connection can be established) or answers with HTTP 500.  We saw that DevOps Guru seemed not to react on the anomalous metric NumberOfNotificationsFailed Average alone as DevOps Guru considers this not to be an anomaly (which is wrong on my opinion). It only seems to create DevOps Guru insight then at least another anomalous metric will be detected. I will approach the DevOps Guru team with my insights so that they can verify the experiment and look behind the scenes what's happening and hopefully improve DevOps Guru service to correctly handle also this SNS anomaly."}
{"title": "Tagging AWS resources the right way using Terraform", "published_at": 1718605089, "tags": ["aws", "devops", "terraform", "platformengineering"], "user": "saifeddine Rajhi", "url": "https://dev.to/aws-builders/tagging-aws-resources-the-right-way-using-terraform-2872", "details": "\ud83d\udd16 Introduction:Keeping your AWS resources organized and tracking costs can be a challenge, especially as your infrastructure grows.Tagging resources is a simple yet great solution, but doing it effectively requires following best practices.In this blog post, we\u2019ll show you how to tag AWS resources using the Infrastructure as Code (IaC) tool Terraform.Introduction to Tagging AWS Resources:Tagging AWS resources is important for maintaining an organized and cost-effective cloud infrastructure.Tags arekey-value pairsthat allow you to categorize and manage resources based on criteria like environment, application, team, etc.Consistent tagging provides benefits like betterresource organization, cost allocation, automation, security,andlifecycle management.In Terraform, you can tag resources during provisioning. For example, to tag an S3 bucket withenvironmentandteamtags:resource \"aws_s3_bucket\" \"example\" {   bucket = \"my-bucket\"   tags = {     Environment = \"Production\"     Team = \"DevOps\"   } }Enter fullscreen modeExit fullscreen modeYou can also define default tags at the provider level, which apply to all resources:provider \"aws\" {   default_tags {     tags = {       Environment = \"Production\"       ManagedBy = \"Terraform\"     }   } }Enter fullscreen modeExit fullscreen modeOverriding Default Tags at Resource Level:Defining default tags at the provider level using thedefault_tagsblock promotes consistency and reduces manual work by automatically applying common tags to all resources provisioned by that provider.Benefits of Default Tags:Consistency: Ensures all resources have a base set of tags applied.Less Manual Work: Avoids repetitive tag definitions across resources.For example, setting default tags in the AWS provider:provider \"aws\" {   default_tags {     tags = {       Environment = \"Production\"       ManagedBy   = \"Terraform\"     }   } }Enter fullscreen modeExit fullscreen modeHow to Override Default Tags or Add Extra Tags you can override the default tags or add new tags at the resource level by specifying the**tags**argument.This argument takes precedence over the default tags defined at the provider level.resource \"aws_s3_bucket\" \"example\" {   bucket = \"my-bucket\"    # Override default Environment tag and add Purpose tag   tags = {     Environment = \"Staging\"     Purpose     = \"Data Processing\"   } }Enter fullscreen modeExit fullscreen modeIn this example, theEnvironmenttag is overridden with the valueStaging, while thePurposetag is added specifically for this S3 bucket resource.TheManagedBydefault tag is still applied.Use Cases for Resource-Level Tag Customization:Environment-specific tags (dev, staging, prod, etc.).Application/project-specific tags.Resource-specific metadata tags (e.g., purpose, owner, expiration).Compliance or regulatory tags based on data sensitivity.Cost allocation tags for specific resources.By allowing tag overrides at the resource level, you maintain the benefits of default tags while gaining the flexibility to customize tags based on the specific needs of individual resources.Using Variables and Functions for Flexible Tagging:Terraform allows you to define tags as variables and use functions likemerge()to combine them with other tags, promoting reusability and flexibility.Defining tags as variables.variable \"default_tags\" {   default = {     Environment = \"Production\"     ManagedBy   = \"Terraform\"   } }Enter fullscreen modeExit fullscreen modeUsing themerge()function to combine tags:resource \"aws_instance\" \"example\" {   ami           = \"ami-0c94855ba95c71c99\"   instance_type = \"t2.micro\"    tags = merge(     var.default_tags,     {       Name    = \"ExampleInstance\"       Project = \"MyApp\"     }   ) }Enter fullscreen modeExit fullscreen modeThemerge()function combines thedefault_tagsvariable with additional resource-specific tags, resulting in all four tags being applied to the EC2 instance.Handling Special Cases:Some AWS resources require specific tagging configurations or have limitations on how tags can be applied.Tagging Auto Scaling Groups:Auto Scaling GroupsASGand Launch TemplatesLTare tricky to tag correctly.Without the right configuration, the EC2 instance and attached storage volumes launched by the ASG and LT will not have the default tags attached.ASGs require thepropagate_at_launchtag configuration.Tagging Launch Templates:Launch templates require thetag_specificationsconfiguration:resource \"aws_launch_template\" \"example\" {   # ...   tag_specifications {     resource_type = \"instance\"     tags = {       Environment = \"Production\"       ManagedBy   = \"Terraform\"     }   }   tag_specifications {     resource_type = \"volume\"     tags = {       Persistence = \"Permanent\"     }   } }Enter fullscreen modeExit fullscreen modeTagging EBS Volumes:When you create Elastic Compute EC2 instances via Terraform, the Elastic Block Store EBS volumes attached to the EC2 are not automatically tagged. Untagged EBS volumes are cumbersome to administer.You assign the EC2 default tags to the attached EBS storage volume with the aws_instancevolume_tags.resource \"aws_instance\" \"example\" {   # ...   volume_tags = {     Name        = \"DataVolume\"     Persistence = \"Permanent\"   } }Enter fullscreen modeExit fullscreen modeOther Special Cases:Certain resources like AMIs, NAT Gateways, or VPC Endpoints may have specific tagging requirements or limitations.Always refer to the Terraform provider documentation for the latest guidance on tagging configurations for different resource types.Avoiding Common Pitfalls:Inconsistent tag naming conventions:Using inconsistent tag keys likeappid, app_role,andAppPurposemakes tags harder to use and manage.resource \"aws_s3_bucket\" \"example\" {   bucket = \"my-bucket\"   tags = {     appid     = \"myapp\"     app_role  = \"data-processing\"     AppPurpose = \"logs\"   } }Enter fullscreen modeExit fullscreen modeInstead, definean explicit rulesetfor tag key naming and stick with it.Not tagging all resources (including secondary resources):Failing to tag all AWS resources, including secondary or complementary resources like EBS volumes, leads to incomplete visibility and cost tracking.resource \"aws_instance\" \"example\" {   ami           = \"ami-0c94855ba95c71c99\"   instance_type = \"t2.micro\"   tags = {     Environment = \"Production\"   } }Enter fullscreen modeExit fullscreen modeIdentical default and resource tags issue:Having identical tag keys and values in bothdefault_tagsand resourcetagscauses an error in Terraform, requiring deduplicating tags or using workarounds.provider \"aws\" {   default_tags {     tags = {       Name = \"Example\"     }   } }  resource \"aws_vpc\" \"example\" {   tags = {     Name = \"Example\" # Error: tags are identical   } }Enter fullscreen modeExit fullscreen modePerpetual diff for partial tag matches:Whendefault_tagsand resourcetagshave some matching and some differing tags, Terraform shows a perpetual diff trying to update the matching tags on every plan, requiring workarounds.provider \"aws\" {   default_tags {     tags = {       Match1 = \"A\"       Match2 = \"B\"        NoMatch = \"X\"     }   } }  resource \"aws_vpc\" \"example\" {   tags = {     Match1 = \"A\" # Perpetual diff trying     Match2 = \"B\" # to update these     NoMatch = \"Y\"    } }Enter fullscreen modeExit fullscreen modeInfrastructure drift and tag loss:Losing tags due to infrastructure drift when resources are modified outside of Terraform.Using IaC consistently helps mitigate this issue.Best Practices and Tips:_Establish a clear tagging strategy and naming convention:_Define a consistent set of tag keys and naming conventions to use across your infrastructure.variable \"tag_names\" {   default = {     environment = \"Environment\"     application = \"Application\"     team        = \"Team\"     costcenter  = \"CostCenter\"   } }Enter fullscreen modeExit fullscreen modeTag resources as you provision them (not after):Apply tags to resources during the provisioning process, not after the fact, to ensure consistent tagging from the start.resource \"aws_s3_bucket\" \"example\" {   bucket = \"my-bucket\"   tags = {     (var.tag_names.environment) = \"Production\"     (var.tag_names.application) = \"MyApp\"   } }Enter fullscreen modeExit fullscreen modeRegularly review and audit tags:Periodically review and audit resource tags to ensure compliance with your tagging strategy and identify any missing or incorrect tags.Automate tagging where possible:Leverage Terraform\u2019s features likedefault_tags, variables, and functions to automatically apply tags during provisioning, reducing manual effort and promoting consistency.AWS Resource Groups and Tag Editor:Have you ever wanted to do the following:\u201cFind all AWS resources in all regions that have the tag team='platform engineering' \u201c?AWS Resource Groups and Tag Editor are powerful tools that allow you to manage tags across multiple AWS resources and regions effectively.Resource Groups:Resource Groups provide a centralized way to organize and manage collections of AWS resources based on shared tags. With Resource Groups, you can:Find resources across regions that have specific tags applied, such as team='platform engineering'.Identify resources that are missing tags or have incorrect tag values.Automate operations like starting/stopping instances or applying configurations based on resource group membership.View consolidated information about resource status, costs, and configurations within a group.Tag Editor:The Tag Editor is a component of Resource Groups that enables bulk tagging operations across supported AWS services and regions. Using the Tag Editor, you can:Search for resources based on resource types and existing tags, allowing queries like \u201cFind all EC2 instances with team='platform engineering'.Add, modify, or remove tags on multiple resources simultaneously, streamlining tagging efforts.Preview the changes before applying them, ensuring accuracy and avoiding unintended modifications.Use tag-based access control policies to manage resource access based on tag values.\ud83d\udd1a Conclusion:Proper tagging is substantial for organized, cost-friendly AWS setups, make sure to use Terraform\u2019s tagging tools and the best ways and avoid common mistakes.Thanks for reading and I hope you learned something about tagging AWS resources in Terraform!Until next time \ud83c\udf89Thank you for Reading !! \ud83d\ude4c\ud83c\udffb\ud83d\ude01\ud83d\udcc3, see you in the next blog.\ud83e\udd18\ud83c\uddf5\ud83c\uddf8\ud83d\ude80 Thank you for sticking up till the end. If you have any questions/feedback regarding this blog feel free to connect with me :\u267b\ufe0f LinkedIn:https://www.linkedin.com/in/rajhi-saif/\u267b\ufe0f Twitter:https://twitter.com/rajhisaifeddineThe end \u270c\ud83c\udffb\ud83d\udd30 Keep Learning !! Keep Sharing !! \ud83d\udd30References:https://support.hashicorp.com/hc/en-us/articles/4406026108435-Known-issues-with-default-tags-in-the-Terraform-AWS-Provider-3-38-0-4-67-0https://medium.com/@leslie.alldridge/how-to-tag-aws-resources-in-terraform-effectively-f4f12bc2416bhttps://engineering.deptagency.com/best-practices-for-terraform-aws-tags"}
{"title": "Github - Teams(Demo)", "published_at": 1718604667, "tags": ["github", "teams"], "user": "Srinivasulu Paranduru", "url": "https://dev.to/aws-builders/github-teamsdemo-2n4n", "details": "Step 1 :GitHub Teams can be created only under the organization. Make sure if you have selected your organization firstStep 2 :Click on TeamsStep 3:Click on new team to create  new teamStep 4:Enter/select the fieldsTeam name (mandatory)Description (mandatory)Parent team (optional) : You can select other github team if team needs to be child team.Team Visibility : Visible / Secret       [Questions will come in this area, github teams can be visible /secret options]Team Notifications : Enabled / DisabledStep 5: By default GitHub organization owner will be added as a maintainer for GitHub teamStep 6: Different roles can be assigned to the uses in GitHub teamsMaintainerMemberStep 7: We can add child team under the team we have created just nowStep 8 :SummaryFlexible repository access: You can add repositories to your teams with more flexible levels of access (Admin, Write, Read).Request to join teams :Members can quickly request to join any team. An owner or team maintainer can approve the requestTeam mentions: Use team @mentions (ex. @github/design for the entire team) in any comment, issue, or pull request.Conclusion:\ud83d\udcac If you enjoyed reading this blog post about GitHub Teams and found it informative, please take a moment to share your thoughts by leaving a review and liking it \ud83d\ude00 and follow me indev.to,linkedin"}
{"title": "Leveraging Amazon Titan Text Premier for RAG AI in Software Testing", "published_at": 1718604446, "tags": [], "user": "Adeline Makokha", "url": "https://dev.to/aws-builders/leveraging-amazon-titan-text-premier-for-rag-and-agent-based-ai-in-software-testing-3b81", "details": "IntroductionAmazon Titan Text Premier, now available through Amazon Bedrock, is a state-of-the-art generative AI model that can revolutionize various fields, including software testing. This article provides a detailed guide on how to implement Retrieval-Augmented Generation (RAG) and agent-based generative AI applications to enhance software testing processes, optimizing outcomes with these advanced technologies.Understanding RAG and Agent-Based Generative AIRetrieval-Augmented Generation (RAG)RAG combines retrieval-based techniques with generative models to create systems capable of fetching relevant information from extensive data sets and using this context to generate high-quality responses. This is particularly useful for tasks requiring detailed and contextually accurate outputs, such as creating comprehensive test cases or documentation.Agent-Based Generative AIAgent-based generative AI employs autonomous agents powered by generative models to perform tasks like test case creation, scenario simulation, and software interaction. These agents can learn and adapt from their interactions, making software testing more efficient and effective.How to Implement RAG and Agent-Based Generative AI in Software TestingStep 1: Setting Up the Environment1.1 Accessing Amazon BedrockLog into your AWS account and go to the Amazon Bedrock service.Ensure you have the necessary permissions to use the Amazon Titan Text Premier model.1.2 Provisioning the Titan Text Premier ModelFollow the AWS documentation to set up the Titan Text Premier model in your AWS environment.Configure the model to meet your specific software testing needs.Step 2: Creating a RAG System for Test Case Generation2.1 Preparing the DataCollect a comprehensive set of documents, including user manuals, past test cases, and bug reports.Use a retrieval system like Elasticsearch or Amazon Kendra to index this data for efficient searching.2.2 Implementing the RAG FrameworkDevelop a retrieval component that queries the indexed data based on test requirements.Integrate the Titan Text Premier model to generate test cases using the retrieved information.2.3 Automating Test Case GenerationCreate automation scripts to streamline the process of retrieving and generating test cases.Use these generated test cases to enhance your existing test suite for broader and more thorough testing.Step 3: Deploying Agent-Based Generative AI for Dynamic Testing3.1 Defining Agent Roles and ScenariosIdentify the types of agents needed, such as UI testers, API testers, and performance testers.Define scenarios for these agents to cover, including edge cases and common user interactions.3.2 Developing Agent LogicUse the Titan Text Premier model to enable agents to dynamically generate and execute test scripts.Implement logic for agents to adapt and learn from test results, improving their effectiveness over time.3.3 Integrating with CI/CD PipelinesConnect the agent-based testing system to your Continuous Integration/Continuous Deployment (CI/CD) pipeline.Ensure agents can autonomously start tests, analyze results, and report issues, supporting continuous testing.Benefits of Using Amazon Titan Text Premier in Software TestingComprehensive Test CoverageRAG and generative AI allow for the creation of a wide range of test scenarios, including those that might be overlooked by human testers, ensuring thorough test coverage.Enhanced EfficiencyAutomating test case generation and execution reduces manual effort and speeds up the testing process, enabling testers to focus on more complex issues.Continuous ImprovementGenerative AI models learn from test results, continuously improving the accuracy and relevance of generated test cases and scenarios.ScalabilityAgent-based systems can easily scale to handle large test suites and extensive applications, providing robust testing capabilities without significant additional resources.ConclusionIntegrating Amazon Titan Text Premier into your software testing framework with RAG and agent-based generative AI greatly enhances testing efficiency and effectiveness. By automating and optimizing test processes, organizations can achieve higher-quality software products with faster release cycles. Amazon Bedrock's advanced infrastructure and capabilities make it feasible and highly beneficial to implement these innovative AI techniques.Embrace the future of software testing with Amazon Titan Text Premier and transform your testing strategies for superior results."}
{"title": "AWS Lambda Layer", "published_at": 1718547511, "tags": ["aws", "devops", "lambda", "cicd"], "user": "Ashutosh Singh", "url": "https://dev.to/aws-builders/aws-lambda-layer-5g00", "details": "This is something I was stuck on while deploying the lambda function, even though the code was only a few 100 lines I had to deploy the function with a zip file including all the node dependencies unnecessarily, it was quite annoying and I'm having this feeling you felt the same way that why you landed over here.Let's get started...This is quite simple, it is made just for the problem I've discussed above i.e. Take away the dependencies and focus on code.Official Definitions:A Lambda layer is a .zip file archive that contains supplementary code or data. Layers usually contain library dependencies, a custom runtime, or configuration files.If you are interested in reading more about it here's thelink.We can start using it now that we know what the lambda layer is.STEP ISearch lambda in the service list and go to the layer on the left side.STEP IINow we gonna create a package that we'll upload in layersNOTICE: The path of the folders is important you can't mess this up otherwise it won't work. Different runtimes have different path like Im using nodeMy folder structure is like this/layers |------- /nodejs             |----- /node_modules             |----- package.json             |----- package-lock.jsonEnter fullscreen modeExit fullscreen modeIf you wanna check out other runtime paths I suggest you should check the table givenhereMake sure any modules you're adding are executable in Linux as lambda layers are executed in Amazon-Linux(Linux).Now Zip the folderlayers.zipor anything you wanna name it.STEP IIIHead towards the lambda page. Now we will upload the zip to lambda layers by creating a new layer.Name the layer anything you want.Don't forget to add the other details like architecture and runtime these 2 are the most important.After you're done click on Create and doneNow you can attach this layer to any of your lambda functions and just call the package as you write normally. You don't have to do anything to integrate it. Enjoy the freedom of writing the code on the browser or make any changes to it on the go without worrying about the package or zipping it.Thank you"}
{"title": "Mind Boggling Speed when Caching with Momento and Rust", "published_at": 1718469915, "tags": [], "user": "Benjamen Pyle", "url": "https://dev.to/aws-builders/mind-boggling-speed-when-caching-with-momento-and-rust-i1d", "details": "Summer is here and in the northern hemisphere, temperatures are heating up.  Living in North Texas, you get used to the heat and humidity but somehow it still always seems to sneak up on me.  As I start this new season (which happens to be my favorite) I wanted to reflect a touch and remember the summer of 2023.  That summer, I looked at6 different aspectsof serverless development from the perspective of things I wish I had known when I was getting started.  Fast forward to this summer when I started withDoes Serverless Still Matter?.  What a year it's been for sure.  And as I look forward to the next few hot months, I'm going to explore my current focus which is highly performant serverless patterns.  And to kick things off, let's get started with caching with Momento andRust.ArchitectureI always like to start by describing what it is that I'm going to be building throughout the article.  When designing for highly performant Lambda-based solutions, I like to keep things as simple as possible.  Since all of these transitions require HTTP requests, latency only grows as more requests enter the mix.  Additionally, by choosing Rust as the language for the Lambda Function, I can be assured that I'm getting the best compute performance that is possible.Project SetupAs I mentioned above, I'm going to be using Rust to build out my Lambda Function.  And as I explore caching with Momento and Rust, I'll be usingMomento's SDK for Rust.  In addition to Rust, I'm building the infrastructure with SAM instead of my usual CDK.  I tend to go back and forth.  When working in purely serverless setups, I tend to favor SAM for its simplicity.  But when I've got more complexity, I lean towards CDK.SAM TemplateThe architecture diagram above highlights a few pieces of AWS infrastructure.  The template below sets up those necessary pieces for getting started as we dive deeper into caching with Momento and Rust.Pay close attention to the Rust Lambda Function piece which requires the naming of the handler to bebootstrap.  Also to note is that the path in the CodUri points to where theCargo.tomlmanifest file is for the Lambda Function handler.Resources:KinesisStream:Type:AWS::Kinesis::StreamProperties:RetentionPeriodHours:24StreamModeDetails:StreamMode:ON_DEMANDDynamoDBTable:Type:AWS::DynamoDB::TableProperties:TableName:LocationsAttributeDefinitions:-AttributeName:locationAttributeType:SKeySchema:-AttributeName:locationKeyType:HASHBillingMode:PAY_PER_REQUESTRustConsumerFunction:Type:AWS::Serverless::FunctionMetadata:BuildMethod:rust-cargolambdaProperties:FunctionName:kinesis-consumer-model-one-rustEnvironment:Variables:RUST_LOG:kinesis_consumer=debugCodeUri:./kinesis-consumer-model-one-rust/rust_app# Points to dir of Cargo.tomlHandler:bootstrap# Do not change, as this is the default executable name produced by Cargo LambdaRuntime:provided.al2023Architectures:-arm64Policies:-AmazonDynamoDBFullAccess-Version:\"2012-10-17\"Statement:-Effect:AllowAction:-ssm:*Resource:\"*\"Events:Stream:Type:KinesisProperties:Stream:!GetAttKinesisStream.ArnStartingPosition:LATESTBatchSize:10Enter fullscreen modeExit fullscreen modeMomento SDKDiving into the Momento piece of the caching with Momento and Rust, I need to first establish an account, a cache, and an API key.  Instead of demonstrating that here,I'll refer you to wonderful documentationthat will guide you through that process.With an API key and cache all configured, I'm going to store that key in an AWS SSM parameter.  That can be demonstrated through this code.  Feel free to change this if you are following along, but if you don't want to make any adjustments, you'll need this value in SSMletparameter=client.get_parameter().name(\"/keys/momento-pct-key\").send().await?;Enter fullscreen modeExit fullscreen modeCaching with Momento and RustFirst off, the Momento SDK is still less than v1.0 so I'd expect some changes along the way.  But in that same thought, it's well-polished for being so new.  It has a very AWS SDK feel to it which I LOVE.  It's one of the things that I appreciate about working with AWS and the Momento Rust SDK has that same vibe.I first need to establish a connection or client into the Momento API.// create a new Momento clientletcache_client=matchCacheClient::builder().default_ttl(Duration::from_secs(10)).configuration(configurations::Laptop::latest()).credential_provider(CredentialProvider::from_string(api_key).unwrap()).build(){Ok(c)=>c,Err(_)=>panic!(\"error with momento client\"),};Enter fullscreen modeExit fullscreen modeWith the client established, I can then make requests against the control plane and data plane APIs. For the balance of the article, I'll be using the data plane API to make gets and sets.GetsIssuing a get on a cache dictionary is straightforward.// use the client to execute a Getmatchcache_client.get(\"sample-a\".to_string(),location.clone()).await{Ok(r)=>matchr{// match on OK or ErrorGetResponse::Hit{value}=>{// A Cache Hittracing::info!(\"Cache HIT\");letcached:String=value.try_into().expect(\"Should have been a string\");letmodel=serde_json::from_str(cached.as_ref()).unwrap();Ok(Some(model))}GetResponse::Miss=>{// A Cache Misstracing::info!(\"Cache MISS, going to DDB\");// Code ommitted but included in the main repository ...}},Err(e)=>{tracing::error!(\"(Error)={:?}\",e);Ok(None)}}Enter fullscreen modeExit fullscreen modeAs shown above, thegetoperation will return aResultwith the inner value being anEnumthat holds information about whether the request was aHitor aMiss.  What I like about this is that theHitalso includes the value retrieved.  This is a nice touch as then deserializing into myCacheModelis as simple as executingserde_json::from_str.  Again, really nice feature.SetsCaching with Momento and Rust was easy and clean with gets, and sets work the same way.  Think of it as almost the reverse of the get.  Instead of deserializing, I now serialize.  Instead of querying, I'm now writing.lets=serde_json::to_string(cache_model).unwrap();matchcache_client.set(\"sample-a\".to_string(),cache_model.location.clone(),s).await{Ok(_)=>Ok(()),Err(e)=>{tracing::error!(\"(Error)={:?}\",e);Ok(())}}Enter fullscreen modeExit fullscreen modeFinal Momento SDK ThoughtsConsider me impressed at my first go with the SDK.  The code worked the very first time without having to dive into documentation.  The SDK API is based on the commonBuilder Patternwhich makes the configuration of a request simple and readable.  There is a common error enum that I then can easily work around withthiserrorto take advantage of the Rust?operator. And lastly, it is highly performant.  And that brings me back to this summer exploration.  I've executed roughly 65K requests through Kinesis to be processed through my Lambda Function which also makes 65K Momento requests.  I consistently saw Momento return me either a hit with the value or a miss at an average of 1.8ms.Running the SampleLet's dive into how to run this sample and see what happens when I do.  Caching with Momento and Rust is such a powerful pattern but sometimes a picture can tell more than words.  I've written aboutRust's performance with Lambdabefore so you either agree with that data or you don't.  I've never steered away from the fact that if you want the maximum amount of speed you can get, then maybe you shouldn't be running in the cloud, using HTTP, and a host of other decisions. If that's the camp you fall in, then 7ms is going to seem slow to you.  But for most of us who enjoy the speed and scale of the cloud without the overhead of management and the ability to iterate quickly at a low cost, then 7ms is much better than what you are going to get with another runtime and setup.Rust's performance shines when paired with Kinesis and Momento.The ProducerIn the repository's root directory, there is aproducerdirectory that holds a Rust program which will load as many Kinesis records as you want.  It will run several threads to loop for a specified duration and write those values into Kinesis.  This is a test harness so to speak.Themainfunction has the below code to handle the threads.  I can configure how many, but by default, I'm just going to kick off 1.// THREAD_COUNT defaults to 1 but can be changed to support multiple threads that'll execute// the thread_runner function as many times as defined in the RECORD_COUNTletthread_count_var:Result<String,VarError>=std::env::var(\"THREAD_COUNT\");letthread_count:i32=thread_count_var.as_deref().unwrap_or(\"1\").parse().expect(\"THREAD_COUNT must be an int\");whileloop_counter<thread_count{// create as many threads as definedletcloned_client=client.clone();lethandle=tokio::spawn(async{thread_runner(cloned_client).await;});handles.push(handle);loop_counter+=1;}whileletSome(h)=handles.pop(){h.await.unwrap();}Enter fullscreen modeExit fullscreen modeIt then contains athread_runnerfunction that will loop some number of times (defaults to 10) and write a record into Kinesis.  The record has alocationfield which is selected from an array at random.asyncfnthread_runner(client:Client){// record count default to 10letrecord_count_var:Result<String,VarError>=std::env::var(\"RECORD_COUNT\");letrecord_count:i32=record_count_var.as_deref().unwrap_or(\"10\").parse().expect(\"RECORD_COUNT must be an int\");// this is where it publishes.// RUN the SAM code in the publisher and take the Stream Name and put that in an environment// variable to make this workletkinesis_stream=std::env::var(\"KINESIS_STREAM_NAME\").expect(\"KINESIS_STREAM_NAME is required\");letmuti=0;whilei<record_count{letmodel_one=ModelOne::new(String::from(\"Model One\"));// create a new model in the loop and push into kinesisletmodel_one_json=serde_json::to_string(&model_one);letmodel_one_blob=Blob::new(model_one_json.unwrap());letkey=model_one.get_id();letresult=client.put_record().data(model_one_blob).partition_key(key).stream_name(kinesis_stream.to_string()).send().await;matchresult{Ok(_)=>{println!(\"Success!\");}Err(e)=>{println!(\"Error putting\");println!(\"{:?}\",e);}}i+=1;}}Enter fullscreen modeExit fullscreen modeI can then run this program by doing the following.cdpublisher cargo buildexportKINESIS_STREAM_NAME=<the name of the stream> cargo runEnter fullscreen modeExit fullscreen modeYou'll seeSuccessprinted into the terminal output and records will start showing up in the Lambda Function.The ConsumerI'm getting to the end of this sample so let's dive into the consumer.  There is a single Lambda Function that brings together caching with Momento and Rust by hooking up to the Kinesis stream and processing the records.The function handler takes aKinesisEvent, loops the records, and then works with the cache.asyncfnfunction_handler(cache_client:&CacheClient,ddb_client:&aws_sdk_dynamodb::Client,event:LambdaEvent<KinesisEvent>,)->Result<(),Error>{info!(\"Starting the loop ...\");// loop the kinesis recordsforeinevent.payload.records{// convert the data into a ModelOne// ModelOne implements the From traitletmutmodel_one:ModelOne=e.into();info!(\"(ModelOne BEFORE)={:?}\",model_one);// grab the item from storageletresult=fetch_item(ddb_client,cache_client,model_one.read_location.clone()).await;matchresult{Ok(r)=>{model_one.location=r;info!(\"(ModelOne AFTER)={:?}\",model_one);}Err(e)=>{error!(\"(Err)={:?}\",e);}}}Ok(())}Enter fullscreen modeExit fullscreen modeThe main operation inside of the loop is thefetch_item.  I've written a good bit aboutRust and DynamoDBso I'm not going to highlight the code below, but the way it works is if the item isn't found in the fetch to Momento, it then goes to DynamoDB to grab the record and then execute the set operation that I showed above.  The key to making this work in this sample is to have the records in DynamoDB so that I have something to set.MyModelOnestruct has a location field which is one of the three values.['Car', 'House', 'Diner'].  Insert the following records into the Locations table created by the SAM infrastructure template.{\"location\":\"Car\",\"description\":\"Car description\",\"notes\":\"Car notes\"}{\"location\":\"Diner\",\"description\":\"Diner description\",\"notes\":\"Diner notes\"}{\"location\":\"House\",\"description\":\"House description\",\"notes\":\"House notes\"}Enter fullscreen modeExit fullscreen modeAnd that'll do it.  When you run the producer above, you'll see a host of output into CloudWatch that highlights the Hits, Misses, DynamoDB queries, and the printing out of a large number of ModelOne structs.Wrapping UpI wrote a few blocks above that 7ms might not be the speed you are looking for, but I'd present you with another opinion.  With serverless, I don't stress over the infrastructure, the durability, reliability, or the fact that I might need 10x more capacity today than I needed yesterday.  Yes, that comes at a premium but as builders, we need to know how tools and know when they are right and when they are wrong.  Serverless to me is still the right solution more than it is the wrong one.  And paired with Momento and Rust, I can get a highly performant and extremely scalable solution with very little investment.  That will stretch a long way for so many that are shipping value.To demonstrate that, here's a comparison of when the record was written to Kinesis and when it was read and processed.  I'm more than happy with 16ms from write to read. That'll take care of the performance criteria I have in so many requirements.This is just the first of many scenarios I plan to look at this summer.  High performance and serverless aren't at odds.  They go hand in hand.  And by using the right tools, you can even further enhance your user's experience.  Because speed does just that.  Enhance user experience.  I hope you've enjoyed Caching with Momento and Rust.And as always,here is the GitHub repository I've been working throughThanks for reading and happy building!"}
{"title": "Cognito Inception: How to add Cognito as OIDC Identity Provider in Cognito", "published_at": 1718439552, "tags": ["aws", "cloud", "tutorial", "security"], "user": "Emma Moinat", "url": "https://dev.to/aws-builders/cognito-inception-how-to-add-cognito-as-oidc-identity-provider-in-cognito-1bk1", "details": "What?Amazon Cognito is an identity platform for web and mobile apps. With Amazon Cognito, you can authenticate and authorise users from a built-in user directory, from your enterprise directory, or from consumer identity providers like Google and Facebook.This post will look at how to setup AWS Cognito to use an OpenID Connect (OIDC) identity provider of another Cognito user pool.Open ID Connect (OIDC) is an authentication protocol built on top of OAuth 2.0. It is designed to verify an existing account (identity of an end user) by a third party application using an Identity Provider site (IDP). It complements OAuth 2.0 which is an authorisation protocol.In this case we are using Cognito as the IDP but you could replace this with many other providers like Salesforce, Github or Azure AD etc. etc.Why?You might wonder why you would want to integrate 2 Cognitos? \ud83e\udd14In this Cognito hosted UI login screen you can see various authentication options are offered, including an alternative Cognito user pool. There's even an option to log directly into this Cognito user pool, which is all configurable.Integrating two Cognito user pools can be beneficial if you have a product linked to a Cognito user pool and a customer who has their own Cognito user pool with their user base. This setup allows the customer's user base to access your product without needing to migrate users to your product's user pool.These 2 Cognito user pools can exist in different accounts and regions.Why not?I feel obliged to mention before you go any further with this setup that it will cost you!Cognito generally is known to be an inexpensive alternative to many other auth providers with one of the major benefits being that there is a free tier of 50,000 monthly active users per account or per AWS organisation. However, this is only the case for users who sign in directly to the user pool or through a social identity provider. So what about users who log in through an OIDC federation like this example. Well...For users federated through SAML 2.0 or an OpenID Connect (OIDC) identity provider, Amazon Cognito user pools has a free tier of 50 MAUs per account or per AWS organization.For users who sign in through SAML or OIDC federation, the price for MAUs above the 50 MAU free tier is $0.015.Cognito pricingI would recommend doing a quick estimate of the cost of this approach for your use case. Head over toAWS' pricing calculatorbefore you continue any further as you may be surprised by the price. And if you never come back to this blog I will understand why! \ud83d\ude02How?Here you can see each step in the authentication process. This is a very standard flow when using an external OIDC provider.Let's set it upTo keep things clear, we'll refer to the Cognito with the user base as the \"Customer user pool\" and the other one as the \"Product user pool\". Our product will first interact with its own user pool (Product user pool) before being redirected to the Customer user pool.Customer User PoolIn this tutorial we will look at how to set this up from A to Z but in reality the Customer user pool may already exist with its user base. In that case you may just need to create a new client in your existing customer user pool so you can skip some of the following steps.Let's first set up the Cognito user pool with the user base (i.e. the customer's user pool).Head to AWS Cognito and clickCreate user pool.SelectProvider typesto be onlyCognito user pooland sign-in options to be whatever suits your use case (I chose email):Follow through the next steps setting up your password policy, MFA, User account recovery and Sign-up experience as you desire.On theIntegrate your apppage enter your desired user pool name.TickUse the Cognito Hosted UISelect the domain setup you want but using a cognito domain is fine if you don't have a custom domain.Set up the client app as follows:Notice here I have generated aclient secret- in this case we need a secret to use this client later as an identity provider. If you don't include it at setup time then you will have to create a new client as this cannot be changed after creation.Also for now I have entered a placeholder allowed callback url ofhttps://example.combut we will come back to change this later.In theAdvanced app client settingsyou can leave everything as it is except adjust the scope as follows:Review and create your user pool!Let's get a user added to this customer's user base when we are still in the area. Keep note of the user's details as you will need them later of course.Product User PoolLet's set up the \"Product\" Cognito user pool, i.e. the instance that your product will interact directly with.Head to AWS Cognito and clickCreate user poolOn theConfigure sign-in experiencescreen selectFederated identity providersas an option and the sign-in options whatever suits you:ForFederated sign-in optionstickOpenID Connect (OIDC)Follow through the next steps setting up your password policy, MFA, User account recovery and Sign-up experience as you desire.Next you will be presented with aConnect federated identity providersscreen - this is where the magic happens. Here fill in the client id and client secret from yourcustomer'suser pool's app client. (i.e. the client app we created in the steps above)You'll find those details in theApp Integrationtab of your Customer's user pool and then selecting the client you created:Enter them as follows (where the provider name will be what is displayed to the user in the hosted UI later):KeepAttribute request methodasGETSetup the issuer url where the url will be:https://cognito-idp.{region}.amazonaws.com/{customerUserPoolId}Add theemailattribute andemail_verifiedas shown here:You can add as many other attributes as you want or need here. Each attribute in a user pool with match exactly to the same attribute in the other user pool, logically.Name your user pool, for example, product-user-pool.Setup your app client as you require. It is not required at this point to generate a client secret for this user pool. You can add one if you want but I wouldn't recommend it if you plan to use this user pool in a webapp or mobile app etc.In the advanced settings, ensure the following:Set theIdentity providersto include your newly created IDP:If you do not want the user to be able to log in directly to your product user pool via the hosted UI, here you can remove the option of Cognito user pool and have the IDP as the only option.Set the scopes to match what we set in the other user pool and in the Identity Provider:Review and create your second user pool!Final integrationOne last step, we need to go to the Customer user pool and adjust the allowed callbacks for the client.Head to theApp integrationtab and then click into your client and go to the hosted UI settings.Set the allowed callbacks to be the following:https://{productCognitoDomain}/oauth2/idpresponseResultNow if you head to the Hosted UI of the Product user pool you will see this:If you click on the button to login to the customer's user pool you will see this:And if you look at the url you can see you are on the customer's user pool hosted UI.You can now log in with the details you set up earlier in the customer's user pool. You are then redirected to the product user pool's redirect url, authenticated and all. Magic. \ud83e\ude84ResourcesThanks to Daniel Kim and his original post which you can read hereUsing Cognito User Pool as an OpenID Connect Provider"}
{"title": "AWS GameDay: Frugality Fest", "published_at": 1718375562, "tags": ["gameday", "awsusergroup", "awscommunity", "communitybuilder"], "user": "Lucian Patian", "url": "https://dev.to/aws-builders/aws-gameday-frugality-fest-4889", "details": "AWS GameDay is an engaging, hands-on learning event where participants tackle real-world technical challenges using AWS solutions in a team-based environment. Unlike traditional workshops, GameDays are open-ended and encourage creative problem-solving.To foster collaboration among AWS User Groups in Romania, we organized a GameDay event focused on building cost-effective applications and mastering cost-efficiency strategies. We chose the \"Frugality Fest\" theme, a trending topic for 2024, because it aligns well with our evening meetup schedule, typically starting at 18:30. This ensured the event wouldn't stretch late into the night, making it convenient for participants who join after their workday.Each AWS Usergroup from Romania (Timisoara,Cluj-Napoca,Bucuresti,Iasi) andMoldovahosted on-site events coordinated remotely by AWS Romania. Each location had a dedicated AWS Romania representative to help with organization and coordination.Teams of four participated, with some pre-formed and others assembled ad-hoc. The event kicked off with a 30 minute introduction covering the event's objectives, the scenario and the rules.At 18:30, the game began and all 17 teams accessed their resources. Each venue featured a live scoreboard displayed prominently, updating in real-time.The game ran for 2.5 hours, filled with intense communication, laughter, frustration and calls for help. After the clock stopped, we captured a screenshot of the final scoreboard. The top three teams were the most experienced but everyone enjoyed the event and eagerly asked about the next GameDay.While I won't spoil the specific tasks, they involved AWS services like EC2, RDS, S3, VPC and CloudWatch. A tip from the winners: prioritize database optimization!If you'd like more details about this event, feel free to reach out to me onLinkedIn. If you\u2019re interested in organizing a similar event, make sure to contact your local AWS Solutions Architect or Technical Account Manager.Event photos:"}
{"title": "Unlocking the Power of EC2 Auto Scaling using Lifecycle Hooks", "published_at": 1718287262, "tags": ["aws", "autoscaling", "ec2", "cloud"], "user": "Brandon Damue", "url": "https://dev.to/aws-builders/unlocking-the-power-of-ec2-auto-scaling-using-lifecycle-hooks-12fk", "details": "In a previous article in which I wrote about EC2 auto scaling, I failed to talked about instance lifecycle hooks and how AWS practitioners can utilize them to optimize their infrastructure. This article is my way of showing you that I have learned from that mistake.A little recap of what auto scaling is: It's a procedure or mechanism that helps you automatically (as the \"auto\" in auto scaling suggests) increase or decrease the size of your IT resources based on predefined thresholds and metrics. In the context of AWS, there is EC2 auto scaling and a service called AWS Auto Scaling, which is used for scaling ECS, DynamoDB, and Aurora resources. However, the focus of this article is on EC2 auto scaling and how to effectively leverage lifecycle hooks during scaling.Before I move on with this article, I give you a real-world example of why auto scaling is important to get you to continue reading this article with an increased level of attention.Imagine a popular social media app. Every Sunday evening, after a weekend filled with adventures, users rush to the app to upload and share their photos. Without auto scaling, the app's servers would be overwhelmed during this rush, causing slow loading times or even crashes. However, with auto scaling in place, the app can automatically scale up by launching additional EC2 instances to handle the increased traffic. This ensures a smooth user experience even during peak times, leading to greater customer satisfaction and retention. But auto scaling doesn't stop there. Once the Sunday rush subsides, auto scaling can intelligently scale back in, terminating unused instances. This frees up valuable resources and reduces costs. This automatic provisioning and de-provisioning not only saves money, but also frees up the IT professionals who would otherwise be manually managing server capacity (a very tedious task).Now that you are sold on the importance of auto scaling, let's move on to the other parts of this article.Lifecycle HooksAny frontend developer who has used a library like React.js already has an understanding of what a lifecycle hook is. The concept is similar in the context of EC2 instances on AWS. Lifecycle hooks give you the ability to perform custom actions on instances in an Auto Scaling group from the time they are launched through to their termination. They provide a specified amount of time (one hour by default) to wait for the action to complete before the instance transitions to the next state. Let's talk about the different stages in the lifecycle of an EC2 instance during scaling.When an EC2 instance is launched during a scale out event, it enters apendingstate allowing time for the instance to run any bootstrapping scripts specified in the user data section of the launch configuration or template of the Auto Scaling group. Once all this is complete, the instance immediately goes into service that is therunningstate. On the flip side of things, when an instance is being removed from an Auto Scaling group during a scale in event or because it has failed health checks, it moves to theterminatingorshutting-downstate until it finally enters theterminatedstate. Even though this looks like a pretty robust set up, it can constitute some problems. For example, when an instance is launched and the user data script has finished running and the instance enters the in-service (running) state, it doesn't necessarily mean the application to be served by the instance is ready to start receiving and processing requests because it might still need more time to perform tasks such as processing configuration files, loading custom resources or connecting to backend databases amongst others. While all this is still trying to complete, the instance might already be receiving health check requests from a load balancer. What do you think will the result of the health check when this happens? You are right if your answer to that question is that the health checks will likely fail because the application is still loading. How then do we inform an auto scaling group that an instance that has been launched is not ready to start receiving any type of requests yet and needs more time before it is ready to start receiving requests? We will come back to this question in a minute.There is another pertinent problem. During a scale-in event, an instance scheduled for termination may still be in the middle of processing requests and may even contain some important logs needed for troubleshooting issues in the future. If the instance is suddenly terminated, both the in-progress requests and logs will be lost. How do you tell your auto scaling group to delay the termination of the instance until it has finished processing pending requests and important log files have been collected into a permanent storage service like Amazon S3? The answer to this question, and the one asked a couple of sentences ago, is, as you might have guessed, lifecycle hooks.Using an instance launching lifecycle hook, you can prevent an instance from moving from the pending state straight into service by first moving it into thepending:waitstate to ensure the application on the instance can finish loading and is ready to start processing requests. When that event ends, the instance moves to thepending:proceedstate where the Auto Scaling group can then attempt to put it in service (running state)In a similar manner, you can also make use of instance termination on the flip side of things that is, when an instance is targeted for termination, an instance terminating lifecycle hook will put your instance in aterminating:waitstate. During which you can do your final cleanup tasks such as preserving copies of logs by moving them to S3 for example. And once you're done, or a preset timer (one hour by default) expires, the instance will move toterminating:proceedstate, and then the Auto Scaling group will take over and proceed to terminate the instance.There are many other use cases for lifecycle hooks, such as managing configurations with tools like Chef or Puppet, among others. We won't go into the details of these to avoid making this article too long. Before I conclude this article, let's look at some implementation considerations for lifecycle hooks.Implementation Considerations for Lifecycle HooksBefore making use of lifecycle hooks you should always consider factors such as:Timeout\u2014 The default timeout for a lifecycle hook as I have already mentioned is one hour (3600 seconds). This may be sufficient for most initialization or cleanup tasks. You can set a custom timeout duration based on your specific needs. The timeout should be long enough to complete necessary actions but not so long that it delays scaling operations unnecessarily.Action Success/Failure\u2014 You have to clearly define what constitutes a successful completion of the lifecycle hook action. This might include successful software installation, configuration setup, or data backup. You will also need to identify conditions that would result in a failure, such as timeout expiration, script errors, or failed installations. In a similar fashion, you should configure your system to send notifications (e.g., via SNS or CloudWatch) upon completion of lifecycle hook actions. This helps in tracking and auditing.Always keep in mind that lifecycle hooks can add latency to scaling events so you should optimize all actions for efficiency.Final ThoughtsIn this article, we explored the concept of EC2 auto scaling and then looked at lifecycle hooks, illustrating how they enhance the efficiency of Auto Scaling groups. We also discussed key implementation considerations to ensure the effective use of lifecycle hooks in your scaling strategy. By combining auto scaling with lifecycle hooks, you gain a powerful and automated approach to managing your cloud infrastructure. Auto scaling ensures your application has the resources it needs to handle fluctuating demands, while lifecycle hooks provide the control to tailor instance behavior during launch and termination. This gives you the ability to optimize resource utilization, streamline deployments, and ultimately deliver a highly available and scalable application experience. Thank you for taking the time to read this and learn more about EC2 Auto Scaling with me."}
{"title": "Using Amazon Bedrock, Claude, and the Converse API to remove PII", "published_at": 1718283921, "tags": ["aws", "bedrock", "generativeai", "python"], "user": "Faye Ellis", "url": "https://dev.to/aws-builders/using-amazon-bedrock-claude-and-the-converse-api-to-remove-pii-53ki", "details": "The Amazon Bedrock Converse API can be used to interact with AI models available in Bedrock, in a consistent way.Here's how to get started using Amazon Bedrock and Converse to Remove Personally Identifiable Information (PII).This exercise should cost < $1 as long as you remember to perform the clean-up step at the end!Before you begin, be sure to check that the following models are available in your region, and that you have enabled access to them:anthropic.claude-v2anthropic.claude-3-haiku1) Create a Cloud9 instance, making sure to select Ubuntu - not Amazon Linux2) After the Cloud9 instance is created, log in, and install the AWS SDK for Python (boto3)pip install boto33) Create a new file on your Cloud9 instance, named converse.py the contents of the file should be as follows (alternatively download fromGitHub) :#first we import boto3 and json   import boto3, json  #create a boto3 session - stores config state and allows you to create service clients  session = boto3.Session()  #create a Bedrock Runtime Client instance - used to send API calls to AI models in Bedrock  bedrock = session.client(service_name='bedrock-runtime')  #define an empty message list - to be used to pass the messages to the model  message_list = []  #here\u2019s the message that I want to send to the model. So in this prompt, I\u2019m providing some text, and asking the AI model to redact any personally identifiable information from the text I provided.  initial_message = {     \"role\": \"user\",     \"content\": [         {           \"text\": \"\\n\\nHuman:\\n<text>\\n Faye: Hi Lydia!\\n Lydia: Hi Faye! Have you recieved the replacement cable that I ordered for you? \\n Faye: No I did not, did you send to my new address? \\n Lydia: I mailed it to 41 Oak Street, Bristol, U.K.\\n Faye: That is my old address, my new address since last week is 105 Lionel Road, London, W8 9YD, U.K.\\n Lydia: Please can you give me your new phone number as well?\\n Faye: Sure, it's 019377464944 </text> \\n\\nRemove all personally identifying information from the text and replace it with \u201cxxx\u201d. All names, phone numbers, and email addresses must get replaced with xxx. \\n\\nPlease provide the sanitized version of the text with all PII removed in <response></response> XML tags.\\n\\nAssistant:\"         }     ], }  #the message above is appended to the message_list  message_list.append(initial_message)  #make an API call to the Bedrock Converse API, we define the model to use, the message, and inference parameters to use as well  response = bedrock.converse(     modelId=\"anthropic.claude-v2\",     messages=message_list,     inferenceConfig={         \"maxTokens\": 2048,         \"temperature\": 0,         \"topP\": 1     }, )  #invoke converse with all the parameters we provided above and then print the result   response_message = response['output']['message'] print(json.dumps(response_message, indent=4))Enter fullscreen modeExit fullscreen mode4) Run the Python code like this:python converse.pyEnter fullscreen modeExit fullscreen modeIt should have removed all of the Personally Identifiable Information from the text. You should see a response similar to this:5) We can also run this same code using different model, by replacing the model ID in our code as follows (Claude 3 Haiku is another model that also supports removing PII):anthropic.claude-3-haiku-20240307-v1:0To clean-up, be sure to delete your Cloud9 instance if you no longer need it, to avoid unneccessary charges.So the Converse API gives you a simple, consistent API, that works with all Amazon Bedrock models that support messages. And this means that you can write your code once and use it with different models to compare the results!"}
{"title": "Building a serverless connected BBQ as SaaS - Part 2 - User Creation", "published_at": 1718278046, "tags": ["aws", "serverless", "iot", "saas"], "user": "Jimmy Dahlqvist", "url": "https://dev.to/aws-builders/building-a-serverless-connected-bbq-as-saas-part-2-user-creation-2n2k", "details": "The time has come for Part 2 in the series of creating a Serverless Connected BBQ as SaaS. In this second post we'll look into user creation, authentication, and authorization. We'll setup idP using Cognito User Pool, and create an event-driven system to store data in our user service. We'll also start building out the frontend part so users can interact with the solution.If you have not already checked it out, here ispart 1.User in a SaaSIn the connected BBQ IoT SaaS solution, I have opted in for a user management strategy that ensure secure and efficient handling of user data. We leverage AWS Cognito User Pools, enhanced with custom attributes to store tenant-specific information, coupled with DynamoDB for external metadata storage. This approach streamlines user authentication and authorization and maintains the scalability and flexibility needed.Single User Pool with Custom AttributesOur primary strategy involves using a single Cognito User Pool, enriched with custom attributes to capture tenant information. Each user is assigned attributes that identify their tenant, enabling the system to differentiate and manage users across various organizations within the same pool. This approach simplifies user management by centralizing all users in one pool while still allowing for tenant-specific operations.External Metadata StorageTo complement our user pool strategy, we store metadata about users in an external DynamoDB table. This can include information such as user preferences, and additional tenant-specific data that might not be suitable for storage within Cognito. This also enables a easy listing of users per tenant, and a quick way to fetch and display user information, instead of querying Cognito. In this solution users will update information in the user service, that stores it in DynamoDB, changes are then reflected into Cognito.One User Pool per TenantAnother common approach in SaaS user management is to use one Cognito User Pool per tenant. This method provides a very strong isolation between tenants, simplifying access control and data segregation.ThoughtsBy using a single user pool with custom attributes and external metadata storage, we have a balanced approach that combines the advantages of centralized management and flexible.Architecture OverviewWe'll create two parts when it comes to user management, the idP which consists of Cognito User Pool and a user service that will be storing user information and relationships. When a user sign up for our solution the user pool will invoke a Lambda function when the user has been confirmedPost Confirmation. The function will put a an event on the application event-bus that a user was created. The user service will react on this event and store information about the user in a DynamoDB table. User service ends by posting a new event on the bus saying a new user was created.We will also start creating our dashboard, which is a React application. We'll let users sign up for our solution, login / logout, and see some basic information about their profile.Create EventBridgeWe will use the event-bus design with a single central bus, this design pattern is a good start which makes it easy to expand with more services, and in a later stage maybe move to a multi-bus approach. Starting with a single central bus setup is normally what I recommend. So let's introduce our common stack that will contain our centrally managed resources.AWSTemplateFormatVersion:\"2010-09-09\"Transform:\"AWS::Serverless-2016-10-31\"Description:Connected BBQ Application Common InfraParameters:Application:Type:StringDescription:Name of owning applicationDefault:bbq-iotResources:EventBridgeBus:Type:AWS::Events::EventBusProperties:Name:!Sub${Application}-application-eventbusTags:-Key:ApplicationValue:!RefApplicationOutputs:EventBridgeName:Description:The EventBus NameValue:!RefEventBridgeBusExport:Name:!Sub${AWS::StackName}:eventbridge-bus-nameEventBridgeArn:Description:The EventBus ARNValue:!GetAttEventBridgeBus.ArnExport:Name:!Sub${AWS::StackName}:eventbridge-bus-arnEnter fullscreen modeExit fullscreen modeCreate idP setupFirst of all we need to create our idP, for this we use Cognito User Pool. E-mail will be used as username, which also need to be verified. Password policy is created and also a schema where the user need to specify e-mail and name, in the schema we also add a field tenant that will be populated by our system.When a user sign up, e-mail, password, and name will be added by the user. Cognito will then validate the e-mail and when that is done a Lambda function will be invoked that adds a message on the event-bus.So let's start by creating the User PoolAWSTemplateFormatVersion:\"2010-09-09\"Transform:\"AWS::Serverless-2016-10-31\"Description:Connected BBQ Application idP setup AuthenticationParameters:ApplicationName:Type:StringDescription:The application that owns this setup.HostedAuthDomainPrefix:Type:StringDescription:The domain prefix to use for the UserPool hosted UI <HostedAuthDomainPrefix>.auth.[region].amazoncognito.comCommonStackName:Type:StringDescription:The name of the common stack that contains the EventBridge Bus and moreResources:UserPool:Type:AWS::Cognito::UserPoolProperties:UserPoolName:!Sub${ApplicationName}-user-poolUsernameConfiguration:CaseSensitive:falseUsernameAttributes:-\"email\"AutoVerifiedAttributes:-emailPolicies:PasswordPolicy:MinimumLength:12RequireLowercase:trueRequireUppercase:trueRequireNumbers:trueRequireSymbols:trueAccountRecoverySetting:RecoveryMechanisms:-Name:\"verified_email\"Priority:1-Name:\"verified_phone_number\"Priority:2Schema:-Name:emailAttributeDataType:StringMutable:falseRequired:true-Name:nameAttributeDataType:StringMutable:trueRequired:true-Name:tenantAttributeDataType:StringDeveloperOnlyAttribute:trueMutable:trueRequired:falseEnter fullscreen modeExit fullscreen modeTo be able to interact with the User Pool from our Webb application we also need to create a User Pool Client. In the webb application we will use Amplify and Amplify UI for user sign up and sign in. For this to work properly it's important that we don't generate an secret, as that will then block Amplify UI. So we needGenerateSecret: Falseset. Now let's add the client to the template from before.UserPoolClient:Type:AWS::Cognito::UserPoolClientProperties:UserPoolId:!RefUserPoolGenerateSecret:FalseAllowedOAuthFlowsUserPoolClient:trueCallbackURLs:-http://localhost:3000#- !Sub https://${DomainName}/signinAllowedOAuthFlows:-code-implicitAllowedOAuthScopes:-phone-email-openid-profileSupportedIdentityProviders:-COGNITOEnter fullscreen modeExit fullscreen modeThe final part is to add the Lambda function for the post confirmation hook and integrate that with the User Pool. When posting a event to the event-bus we will use the metadata / data pattern.{\"metadata\":{\"domain\":\"idp\",\"application\":\"application_name\",\"event_type\":\"signup\",\"version\":\"1.0\",},\"data\":{\"email\":\"user e-mail\",\"userName\":\"user name\",\"name\":\"name\",\"verified\":\"verified\",\"status\":\"status\",},}Enter fullscreen modeExit fullscreen modeNow let's add the Lambda function to the template and set the User Pool to call it. We also need to add Lambda Permission so the User Pool is allowed to invoke the function.PostSignUpHook:Type:AWS::Serverless::FunctionProperties:AutoPublishAlias:\"true\"CodeUri:./PostSignUpLambdaHandler:hook.handlerAssumeRolePolicyDocument:Version:2012-10-17Statement:-Effect:AllowPrincipal:Service:-lambda.amazonaws.comAction:-sts:AssumeRolePolicies:-EventBridgePutEventsPolicy:EventBusName:Fn::ImportValue:!Sub${CommonStackName}:eventbridge-bus-nameEnvironment:Variables:EventBusName:Fn::ImportValue:!Sub${CommonStackName}:eventbridge-bus-nameApplicationName:!RefApplicationNamePostSignUpHookPermission:Type:AWS::Lambda::PermissionProperties:Action:lambda:InvokeFunctionFunctionName:!GetAttPostSignUpHook.ArnPrincipal:cognito-idp.amazonaws.comUserPool:Type:AWS::Cognito::UserPoolProperties:.....LambdaConfig:PostConfirmation:!GetAttPostSignUpHook.ArnEnter fullscreen modeExit fullscreen modeThe code for the Lambda function is not that complicated, it will just post a message to the event-bus.importboto3importosimportjsondefhandler(event,context):application_name=os.environ[\"ApplicationName\"]event_bus=os.environ[\"EventBusName\"]event_bus_client=boto3.client(\"events\")user_event={\"metadata\":{\"domain\":\"idp\",\"application\":application_name,\"event_type\":\"signup\",\"version\":\"1.0\",},\"data\":{\"email\":event[\"request\"][\"userAttributes\"][\"email\"],\"userName\":event[\"userName\"],\"name\":event[\"request\"][\"userAttributes\"][\"name\"],\"verified\":event[\"request\"][\"userAttributes\"][\"email_verified\"],\"status\":event[\"request\"][\"userAttributes\"][\"cognito:user_status\"],},}response=event_bus_client.put_events(Entries=[{\"Source\":f\"{application_name}.idp\",\"DetailType\":\"signup\",\"Detail\":json.dumps(user_event),\"EventBusName\":event_bus,},])returneventEnter fullscreen modeExit fullscreen modeWith that created the sign up flow for the User Pool is completed.Create User ServiceThe next part in the user handling is the User Service that will be used to store additional metadata about the users in the system. It will also be a crucial part in the permission and data isolation, that will be discussed in later parts.When a user has signed up, we like to react on the event sent by the User Pool Lambda integration, and create a user in the user database. When user is stored we send an event about that on the bus for other services to react on.So lets go ahead and create the state machine and user DynamoDB table.AWSTemplateFormatVersion:\"2010-09-09\"Transform:\"AWS::Serverless-2016-10-31\"Description:Connected BBQ Application User ServiceParameters:ApplicationName:Type:StringDescription:Name of owning applicationDefault:bbq-iotCommonStackName:Type:StringDescription:The name of the common stack that contains the EventBridge Bus and moreResources:UserSignUpHookStateMachineLogGroup:Type:AWS::Logs::LogGroupProperties:LogGroupName:!Sub${ApplicationName}/userservice/signuphookstatemachineRetentionInDays:5UserSignUpHookExpress:Type:AWS::Serverless::StateMachineProperties:DefinitionUri:statemachine/statemachine.asl.yamlTracing:Enabled:trueLogging:Destinations:-CloudWatchLogsLogGroup:LogGroupArn:!GetAttUserSignUpHookStateMachineLogGroup.ArnIncludeExecutionData:trueLevel:ALLDefinitionSubstitutions:EventBridgeBusName:Fn::ImportValue:!Sub${CommonStackName}:eventbridge-bus-nameUserTable:!RefUserTableApplicationName:!RefApplicationNamePolicies:-Statement:-Effect:AllowAction:-logs:*Resource:\"*\"-EventBridgePutEventsPolicy:EventBusName:Fn::ImportValue:!Sub${CommonStackName}:eventbridge-bus-name-DynamoDBCrudPolicy:TableName:!RefUserTableEvents:UserSignUp:Type:EventBridgeRuleProperties:EventBusName:Fn::ImportValue:!Sub${CommonStackName}:eventbridge-bus-namePattern:source:-!Sub${ApplicationName}.idpdetail-type:-signupType:EXPRESSUserTable:Type:AWS::DynamoDB::TableProperties:TableName:!Sub${ApplicationName}-usersAttributeDefinitions:-AttributeName:useridAttributeType:SKeySchema:-AttributeName:useridKeyType:HASHBillingMode:PAY_PER_REQUESTEnter fullscreen modeExit fullscreen modeThe definition for the state machine is not that complicated.Comment:User service - User Signup Hook State MachineStartAt:DebugStates:Debug:Type:PassNext:Create UserCreate User:Type:TaskResource:arn:aws:states:::dynamodb:putItemParameters:TableName:${UserTable}Item:userid:S.$:$.detail.data.userNamename:S.$:$.detail.data.nameemail:S.$:$.detail.data.emailstatus:S.$:$.detail.data.statusverified:S.$:$.detail.data.verifiedResultPath:nullNext:Post EventPost Event:Type:TaskResource:arn:aws:states:::events:putEventsParameters:Entries:-Source:${ApplicationName}.userDetailType:createdDetail.$:$EventBusName:${EventBridgeBusName}End:trueEnter fullscreen modeExit fullscreen modeCreate DashboardLet us now start creating our dashboard, that we will continue building on in this series. The dashboard is a react app created withcreate-react-app. For styling we will useTailwind CSS.For user login and signup we will rely on Amplify, so first of all, let's create a small utils class that will check if a user is already logged in.import{getCurrentUser}from\"aws-amplify/auth\";exportconstisAuthenticated=async()=>{try{awaitgetCurrentUser();returntrue;}catch{returnfalse;}};Enter fullscreen modeExit fullscreen modeNext let's create our Login page, that we will route users to when they are not logged in.importReact,{useEffect}from\"react\";import{Navigate,Route,Routes,useNavigate}from\"react-router-dom\";import{isAuthenticated}from\"../utils/auth\";importHeaderfrom\"../components/Header\";importFooterfrom\"../components/Footer\";import{Authenticator}from\"@aws-amplify/ui-react\";import\"@aws-amplify/ui-react/styles.css\";constLogin=()=>{constnavigate=useNavigate();useEffect(()=>{isAuthenticated().then((loggedIn)=>{if(loggedIn){navigate(\"/dashboard\");}});},[navigate]);return(<divclassName=\"min-h-screen flex flex-col\"><Header/><mainclassName=\"flex-grow flex items-center justify-center\"><AuthenticatorsignUpAttributes={[\"name\"]}loginMechanisms={[\"email\"]}>{({signOut,user})=>(<Routes><Routepath=\"/\"element={<Navigatereplaceto=\"/dashboard\"/>}/></Routes>)}</Authenticator></main><Footer/></div>);};exportdefaultLogin;Enter fullscreen modeExit fullscreen modeThis will now create a UI and flow like this, which is the Amplify UI for Cognito User Pools.To sign up the user clickCreate Accountand fill in e-mail and password, in the next step the e-mail address must be verified.After successful login it's possible to view user attributes on theProfiletab, also not that the login button now changes to logout.Get the codeThe complete setup with all the code is available onServerless HandbookFinal WordsThis was the second part in building a connected BBQ as a SaaS solution. Where we start to create the user sign up and registration using Cognito User Pool.Check outMy serverless Handbookfor some of the concepts mentioned in this post.Don't forget to follow me onLinkedInandXfor more content, and read rest of myBlogsAs Werner says! Now Go Build!"}
{"title": "Infra as GitHub Actions - AWS Serverless Function for nodejs", "published_at": 1718252854, "tags": ["javascript", "aws", "githubactions", "terraform"], "user": "Alonso Suarez", "url": "https://dev.to/aws-builders/infra-as-github-actions-aws-serverless-function-for-nodejs-25j2", "details": "In the lastpostwe talked about the need to simplify infra while also moving it back to the application repoAs I started to work on the next infra as GitHub actions, which was a secured website withauthentication@edge. It became clear that AWS lambda was a fundamental building block in the journeyIntroducingactions-aws-function-node\ud83c\udf89Now with very few dependencies, you can provision your node backend in literally a minute \ud83c\udfce\ufe0fGetting startedLet's start with familiar code// src/index.jsexports.handler=async(event,context)=>{return{\"statusCode\":200,\"headers\":{\"Content-Type\":\"*/*\"},\"body\":\"hello world\"}}Enter fullscreen modeExit fullscreen modeAdd the workflow# .github/workflows/on-push-main.yml name: demo on:   push:     branches:       - main jobs:   deploy:     environment:       name: main       url: ${{ steps.backend.outputs.url }}     permissions:        id-token: write     runs-on: ubuntu-latest     steps:       - name: Check out repo         uses: actions/checkout@v4       - uses: aws-actions/configure-aws-credentials@v4         with:           aws-region: us-east-1           role-to-assume: ${{ secrets.ROLE_ARN }}           role-session-name: ${{ github.actor }}       - uses: alonch/actions-aws-backend-setup@main         with:            instance: sample       - uses: alonch/actions-aws-function-node@main         with:            name: actions-aws-function-node-sample           entrypoint-file: index.js           entrypoint-function: handler           artifacts: src           allow-public-access: trueEnter fullscreen modeExit fullscreen modeAdd the secretROLE_ARNwith access to AWS and that's it, after pushing to main you have a GitHub deployment with you backend running \ud83c\udf89You can clone thissamplefrom Github tooOf course, there are a lot more optionsPermissionsYou can allow access to services by just adding the resource name and the access, either read or writeFor example:- uses: alonch/actions-aws-function-node@main         with:            name: actions-aws-function-node-demo           entrypoint-file: index.js           entrypoint-function: handler           artifacts: src           allow-public-access: true           permissions: |             s3: read             dynamodb: writeEnter fullscreen modeExit fullscreen modeThis configuration will attach AmazonS3ReadOnly and AmazonDynamoDBFullAccess managed policies to the function's roleEnvironment VariablesSimilar to permissions, you can attach function variables as follow:- uses: alonch/actions-aws-function-node@main         with:            name: actions-aws-function-node-demo           entrypoint-file: index.js           entrypoint-function: handler           artifacts: src           allow-public-access: true           env: |             DD_ENV: production              DD_SERVICE: demo             DD_VERSION: ${{ github.sha }}Enter fullscreen modeExit fullscreen modeThe rest of the options are standard attributes like memory, timeout or selecting ARM architectureThe best part is that it takes a minute to provision it and even less time to destroy \ud83d\udc4fI\u2019m excited about the future developments and improvements that can be made to this workflow. If you have any feedback, questions, or suggestions, feel free to leave a comment below or reach out directly. Let\u2019s continue this journey of simplifying infrastructure together!Thank you for reading, and happy coding!"}
{"title": "Deploy Docker Image to AWS EC2 in 5 minutes", "published_at": 1718217305, "tags": ["aws", "ec2", "docker", "containers"], "user": "Ahmed Srebrenica", "url": "https://dev.to/aws-builders/deploy-docker-image-to-aws-ec2-in-5-minutes-2g37", "details": "From Development to Staging in 5 Minutes\u2026IntroductionAs we know, there are many ways to run your Docker image on the Cloud. I have previously written about this and introduced two methods:\u201cHow to Deploy a Docker Image to Amazon ECR and Run It on Amazon EC2\u201d&\u201cHow to Deploy a Docker Image on AWS and Run it on Fargate\u201d.The first method involves managing the infrastructure, security patches, network security, etc., while the second method allows AWS to handle the infrastructure, simplifying many processes for us.However,I will write about this in future articles.In this article, I will show you the easiest way to move your Docker image from theDevelopmentto theStagingenvironment. This process takes approximately5 minutes. We will useDockeron our local machine,DockerHubandAWS.PrerequisitesCreateAWS Account. I already have an AWS Account and I won\u2019t be creating a new one.Create aDockerHubaccount I already have and I won\u2019t be creating a new one.Create aGitHub AccountI already have GitHub and I won\u2019t be creating a new one.DownloadVisual Studio Codeor another code editor.Download and installDocker.Clone the Application to your Local machineTo make it easier, we will use mysimple-node-app, which you need to clone to your local machine.Go to your terminal and follow these steps:git clone https://github.com/srebreni3/simple-nodejs-appEnter fullscreen modeExit fullscreen modeThen navigate to the project directory.Go to your code editor, and you will see this:We haveapp.js, package.json,andDockerfile, it\u2019s all we need.I don\u2019t want to explain the code of the application again, go to thisarticleand you will find everything about it.Let\u2019s summarize:If you follow these steps, you will have an application on your Local machine, and that Local machine will be your Development environment. Feel free to change the code if you want.Push the Docker image to Docker HubWe need to create a Docker image for our application. Navigate to the directory where the application is located.If you are using an M1 or newer chip, use the following command to create the Docker image:docker buildx build--platformlinux/amd64-tnode-app.Enter fullscreen modeExit fullscreen modeIf you are not using M1 or newer, use this command to create a Docker image:docker build-tnode-app.Enter fullscreen modeExit fullscreen modeWhen you have successfully created a Docker image, our next step is to push that image on Docker Hub.Go to Docker Hub and create a new repository.Let\u2019s summarize: With these steps, you can push your Docker image to Docker Hub. It\u2019s preferable to push it to the Private repository rather than the Public repository.Create an EC2 Instance and install Docker on itGo to the EC2 dashboard in AWS Console and click the Launch instance button.2. Name your instance, for example: node-docker-server. The image should be Amazon Linux 2023. If you want a free tier-eligible instance, choose t2.micro.3. We would like to login to the instance, and we will create a key pair. Security group should have open inbound ports 22 and 3000.4. For storage, I will leave default settings, I don\u2019t need more than 8 GiB of storage. Don\u2019t forget: Free tier eligible customers can get up to 30 GB of EBS General Purpose (SSD) or Magnetic storage.5. Under Configure Storage, click the Advanced details.Go to the bottom of the page, and you will see the User data window. Paste this code into User data:#!/bin/bashsudoyuminstalldocker-ysudosystemctl start dockersudosystemctlenabledockerEnter fullscreen modeExit fullscreen modeFrom the right side, click theLaunch instancebutton.Let\u2019s summarize:With these steps, you can create an EC2 instance on AWS to set up your staging environment using User data to install Docker on the instance.Deploy an image from Docker Hub to AWS EC2The first step is to connect to the EC2 instance. I will use AWS Console this time, but you can also connect through your Local machine from the terminal. Check out my older articles and you will find how to connect through a terminal from a Local machine.Switch to sudo mode with this command:sudosuEnter fullscreen modeExit fullscreen modeCheck the Docker with:docker--versionEnter fullscreen modeExit fullscreen modeEverything is set up, the next step is to deploy the image from Docker Hub. Use this command:docker run-d-p3000:3000 <docker-hub-repo-name>Enter fullscreen modeExit fullscreen modeThe image has been successfully created. Check the Public IP of your Instance with a 3000 port.We got our Docker application on the EC2 instance.Let\u2019s summarize:Thanks to AWS, we can create a server and deploy whatever we want to that server in just a few seconds. This is an example of deploying from Docker Hub to staging environment (EC2 instance) in a few seconds.ConclusionIf you prefer to run your production on EC2 instead of ECS using Docker images, one of the easiest ways to transfer images from the Development environment to the Staging environment is by using this method. Although this method has its advantages and disadvantages, it is, in my opinion, the simplest way so far. I hope it proves useful to someone."}
{"title": "Back2Basics: Setting Up an Amazon EKS Cluster", "published_at": 1718176767, "tags": ["aws", "eks", "kubernetes", "opentofu"], "user": "Romar Cablao", "url": "https://dev.to/aws-builders/back2basics-setting-up-an-amazon-eks-cluster-2ep1", "details": "OverviewThis blog post kicks off a three-part series exploring Amazon Elastic Kubernetes Service (EKS) and how builders like ourselves can deploy workloads and harness the power of Kubernetes.Throughout this series, we'll delve into the fundamentals of Amazon EKS. We'll walk through the process of cluster provisioning, workload deployment, and monitoring. We'll leverage various solutions along the way, includingKarpenterandGrafana.As mentioned, this series aims to empower fellow builders to explore the exciting world of containerization.Kubernetes And It's ComponentsBefore we dive into provisioning our first cluster, let's take a quick look at Kubernetes and its components.Control Plane Componentskube-apiserver- the central API endpoint for Kubernetes, handling requests for cluster management.etcd- a consistent and highly-available key value store used as Kubernetes' backing store for all cluster data.kube-scheduler- the automated scheduler responsible for assigning pods to available nodes in the cluster.kube-controller-manager- component that runs controller processes (e.g. Node controller, Job controller, etc.)cloud-controller-manager- component that embeds cloud-specific control logic.Node Componentskubelet- an agent that runs on each node in the cluster that makes sure that containers are running in a Pod.kube-proxy- is a network proxy that runs on each node in the cluster, implementing part of the Kubernetes service concept.Container runtime- is responsible for managing the execution and lifecycle of containers within Kubernetes.That's a quick recap of Kubernetes components. We will talk more about the different things that make up Kubernetes, like pods and services, later on in this series.Worth noting \u2013 this month marks a significant milestone! June 2024 marks the 10th anniversary of Kubernetes\ud83e\udd73\ud83c\udf82. Over the past decade, it has established itself as the go-to platform for container orchestration. This widespread adoption is evident in its integration with major cloud providers like AWS.Amazon Elastic Kubernetes Service (EKS)Amazon Elastic Kubernetes Service (Amazon EKS) is a managed Kubernetes service to run Kubernetes in the AWS cloud and on-premises data centers. In the cloud, Amazon EKS automatically manages the availability and scalability of the Kubernetes control plane nodes responsible for scheduling containers, managing application availability, storing cluster data, and other key tasks. Read more:https://aws.amazon.com/eks/There are several ways to provision an EKS cluster in AWS:AWS Management Console- provides a user-friendly interface for creating and managing clusters.Usingeksctl- a simple command-line tool for creating and managing clusters on EKS.Infrastructure as Code (IaC) tools- tools likeCloudFormation,TerraformandOpenTofu.In this series will useOpenTofuto provision an EKS cluster along with all the necessary resources to create a platform ready for workload deployment. So if you already knowTerraform, learningOpenTofuwill be easy as it is an open-source, community-driven fork ofTerraformmanaged by the Linux Foundation. It offers similar functionalities while being actively developed and maintained by the open-source community.Let's Get Our Hands Dirty!Our first goal is to setup a cluster. For this activity, we will be using this repository:romarcablao/back2basics-working-with-amazon-eksBack2Basics: Working With Amazon Elastic Kubernetes Service (EKS)Back2Basics: Working With Amazon Elastic Kubernetes Service (EKS)Read the series here:Back2Basics: Amazon EKSInstallationDepending on your OS, select the installation method here:https://opentofu.org/docs/intro/install/Provision the infrastructureMake necessary adjustment on the variables.Runtofu initto initialize the modules and other necessary resources.Runtofu planto check what will be created/deleted.Runtofu applyto apply the changes. Typeyeswhen asked to proceed.Fetchkubeconfigto access the clusteraws eks update-kubeconfig --region$REGION--name$CLUSTER_NAMEEnter fullscreen modeExit fullscreen modeCheck what's inside the cluster#List all pods in all namespaceskubectl get pods -A#List all deployments in kube-systemkubectl get deployment -n kube-system#List all daemonsets in kube-systemkubectl get daemonset -n kube-system#List all nodeskubectl get nodesEnter fullscreen modeExit fullscreen modeLet's try to deploy a simple app#Create a deploymentkubectl create deployment my-app --image nginx#Scale the replicas of my-app deployment\u2026Enter fullscreen modeExit fullscreen modeView on GitHubPrerequisiteMake sure you haveOpenTofuinstalled. If not, head over to theOpenTofu Docsfor a quick installation guide.Steps1. Clone the repositoryFirst things first, let's grab a copy of the code:git clone https://github.com/romarcablao/back2basics-working-with-amazon-eks.gitEnter fullscreen modeExit fullscreen mode2. Configureterraform.tfvarsModify theterraform.tfvarsdepending on your need. As of now, it is set to use Kubernetes version 1.30 (the latest at the time of writing), but feel free to adjust this and the region based on your needs. Here's what you might want to change:environment     = \"demo\"cluster_name    = \"awscb-cluster\"cluster_version = \"1.30\"region          = \"ap-southeast-1\"vpc_cidr        = \"10.0.0.0/16\"Enter fullscreen modeExit fullscreen mode3. Initialize and install plugins (tofu init)Once you've made your customizations, runtofu initto get everything set up and install any necessary plugins.4. Preview the changes (tofu plan)Before applying anything, let's see what OpenTofu is about to do withtofu plan. This will give you a preview of the changes that will be made.5. Apply the changes (tofu apply)Runtofu applyand when prompted, typeyesto confirm the changes.Looks familiar? You're not wrong!OpenTofuworks very similarly as it shares a similar core setup withTerraform. And if you ever need to tear down the resources, just runtofu destroy.Now, lets check the resources provisioned!Once provisioning is done, we should be able to see a new cluster. But where can we find it? You can simply use the search box inAWS Management Console.Click the cluster and you should be able to see something like this:Do note that we enable a couple of addons in the template hence we should be able to see these three core addons.CoreDNS- this enable service discovery within the cluster.Amazon VPC CNI- this enable pod networking within the cluster.Amazon EKS Pod Identity Agent- an agent used for EKS Pod Identity to grant AWS IAM permissions to pods through Kubernetes service accounts.Accessing the ClusterNow that we have the cluster up and running, the next step is to check resources and manage them usingkubectl.By default, the cluster creator has full access to the cluster. First, we need to fetch thekubeconfigfile by running:aws eks update-kubeconfig--region$REGION--name$CLUSTER_NAMEEnter fullscreen modeExit fullscreen modeNow, let's list all pods in all namespaceskubectl get pods-AEnter fullscreen modeExit fullscreen modeHere's a sample output from the command above:NAMESPACE     NAME                           READY   STATUS    RESTARTS   AGE kube-system   aws-node-5kvd4                 2/2     Running   0          2m49s kube-system   aws-node-n2dqb                 2/2     Running   0          2m51s kube-system   coredns-5765b87748-l4mj5       1/1     Running   0          2m7s kube-system   coredns-5765b87748-tpfnx       1/1     Running   0          2m7s kube-system   eks-pod-identity-agent-f9hhb   1/1     Running   0          2m7s kube-system   eks-pod-identity-agent-rdbzs   1/1     Running   0          2m7s kube-system   kube-proxy-8khgq               1/1     Running   0          2m51s kube-system   kube-proxy-p94w7               1/1     Running   0          2m49sEnter fullscreen modeExit fullscreen modeLet's check a couple of objects and resources:# List all deployments in kube-systemkubectl get deployment-nkube-system# List all daemonsets in kube-systemkubectl get daemonset-nkube-system# List all nodeskubectl get nodesEnter fullscreen modeExit fullscreen modeHow about deploying a simple workload?# Create a deploymentkubectl create deployment my-app--imagenginx# Scale the replicas of my-app deploymentkubectl scale deployment/my-app--replicas2# Check the podskubectl get pods# Delete the deploymentkubectl delete deployment my-appEnter fullscreen modeExit fullscreen modeWhat's Next?Yay\ud83c\udf89, we're able to provision an EKS cluster, check resources and objects usingkubectland create a simple nginx deployment. Stay tuned for the next part in this series, where we'll dive into deployment, scaling and monitoring of workloads in Amazon EKS!"}
{"title": "Announcements from AWS re:Inforce 2024 Keynote", "published_at": 1718172778, "tags": ["aws", "security", "ai", "news"], "user": "Eyal Estrin", "url": "https://dev.to/aws-builders/announcements-from-aws-reinforce-2024-keynote-481f", "details": "AWS re:Inforce is the Amazon Web Services annual event focused on security.The event was led byChris Betz, CISO of AWS.During the keynote, Chris shared some of the insights that AWS embed security as part of their company's culture.He talked about theSecurity Guardians program, a mechanism for distributing security ownership, and a culture of escalation \u2013 a process of making sure that the right people know about the problem at the right time.AWS infrastructure is secured by design at all layers \u2013 from hardware, virtualization, compute, networking, storage, and finally at the apps and data layer.AWS Graviton4was designed with built-in security features such as:Pointer authenticationNo simultaneous multi-threading (SMT)Full encryption of all high-speed physical interfacesBranch target identificationAWS Nitro System supports full isolation of customer's AI data from AWS operators:Encrypt sensitive AI data using keys that customers own and controlStore data in a location of the customer's choiceSecurely transfer the encrypted data to the enclave for inferencingEncryption for ML accelerator, to Nitro, to the network, and back Reference:A secure approach to generative AI with AWSAWS is usingAutomated Reasoningfor multiple purposes, such as:Verify the correctness of cryptographic protocols, authorization logic, and consistency of storage systems (such asAmazon S3 ShardStore)Verify security mechanisms such as firewalls, detection, and coding practicesZero Trust challenges:A strong identity and access managementHybrid environmentsComplex network segmentationExpanding application landscape and workforce mobilityAnnouncement - AWS Private CA Connector for SCEP (Currently in Preview)Simple Certificate Enrollment Protocol (SCEP), lets you use a managed and secure cloud certificate authority (CA) to enroll mobile devices securely and at scale.References:AWS Private CA introduces Connector for SCEP for mobile devices (Preview)AWS Private CA Connector for SCEP documentationAnnouncement - Passkeys as 2nd Factor Authenticators in AWS IAMAWS now allows customers the options for strong authentication by launching support for FIDO2 passkeys as a method for multi-factor authentication (MFA) as we expand our MFA capabilities. Passkeys deliver a highly secure, user-friendly option to enable MFA for many of our customers.References:AWS Identity and Access Management now supports passkey as a second authentication factorAWS adds passkey multi-factor authentication (MFA) for root and IAM usersPasskeys enhance security and usability as AWS expands MFA requirementsAnnouncement - IAM Access Analyzer unused access findings recommendation (Currently in Preview)AWS IAM Access Analyzer provides tools to set, verify, and refine permissions. With the new announcement, IAM Access Analyzer offers actionable recommendations to guide you to remediate unused access.References:AWS IAM Access Analyzer now offers recommendations to refine unused accessIAM Access Analyzer updates: Find unused access, check policies before deploymentIAM Access Analyzer - Unused access findingsAnnouncement - Malware Protection for S3 Amazon GuardDutyAmazon GuardDuty is a threat detection service that continuously monitors, analyzes, and processes specific AWS data sources and logs in the AWS environment. This expansion of GuardDuty Malware Protection allows scanning newly uploaded objects to Amazon S3 buckets for potential malware, viruses, and other suspicious uploads and taking action to isolate them before they are ingested into downstream processes.References:Detect malware in new object uploads to Amazon S3 with Amazon GuardDutyIntroducing Amazon GuardDuty Malware Protection for Amazon S3GuardDuty Malware Protection for S3AWS Generative AI stack and built-in security controls:Amazon Q - Tools and services to write secure and robust code (Amazon Q Developer,Amazon Q Business)Amazon Bedrock- Helps keep data secure and private. All data is encrypted in transit and at rest. Data used for customization is securely transferred through the customer's VPCAWS Nitro System- Allows customers to secure AI infrastructure includes zero trust access to sensitive AI dataAnnouncement - Generative AI-powered query generation AWS CloudTrail Lake (Currently in Preview)AWS CloudTrail Lakelets customers run SQL-based queries on your events. This new feature empowers users who are not experts in writing SQL queries or who do not have a deep understanding of CloudTrail events.References:AWS CloudTrail Lake announces AI-powered natural language query generation (preview)Simplify AWS CloudTrail log analysis with natural language query generation in CloudTrail Lake (preview)Create CloudTrail Lake queries from English language promptsSteve Schmidt, the Chief Security Officer of Amazon, shared some of the experiences Amazon has had using generative AI.Generative AI security scoping matrix:Consumer App - Using \"public\" generative AI servicesEnterprise App - Using an app or SaaS with generative AI featuresPre-trained models - Building an app on a versioned modelFine-tuned models - Fine-tuning a model based on customer's dataSelf-trained models - Training a model from scratch, based on the customer's data References:An Introduction to the Generative AI Security Scoping MatrixSecuring generative AI: Applying relevant security controlsHandling service data properly:Know what you have, where is it, how it is stored, who has access for what purposes, and how that data is used over timeTrust boundaries for retrieval-augmented generation (RAG)Continued testingSecurity guardrails (such asGuardRails for Amazon Bedrock)The full keynote is available at:https://www.youtube.com/watch?v=skH3Q90llssAbout the AuthorEyal Estrin is a cloud and information security architect, and the author of the booksCloud Security Handbook, andSecurity for Cloud Native Applications, with more than 20 years in the IT industry.You can connect with him onTwitter.Opinions are his own and not the views of his employer.\ud83d\udc47Help to support my authoring\ud83d\udc47\u2615Buy me a coffee\u2615"}
{"title": "DevSecOps with AWS- IaC at scale - Building your own platform - Part 2 - CI for IaC", "published_at": 1718077042, "tags": ["devops", "aws", "iac", "terraform"], "user": "Alejandro Velez", "url": "https://dev.to/aws-builders/devsecops-with-aws-iac-at-scale-building-your-own-platform-part-2-ci-for-iac-275c", "details": "Level 300In this post the idea of the pipeline as SaaS product is explored and applied in detail. Consider the scenario explained in the previous blogs. First, some main questions could be clear before started:- What is the reference pipeline for IaC as Code?There are many sources and references architectures for IaC CI/CD pipeline, books like Patterns and practices for Infrastructure as Code expose patterns and consideration in deep. For other hand, some vendors suggest best practices and workflows according to their services and tools, the Figure 1 depicts the general steps for agnostic CI process for IaC.Figure 1. CI for IaCKey Points1-  Decoupling the CI from CD.2-  Apply Trunk Base Development.3-  Do Risk management evaluation to apply changes.4-  Reduce Blass radius and apply microstacks for your infrastructure code.5-  Introduce cost and drift detection before applying some changes.6-  Verify custom policies and compliance practices such policy tags, environments values using policy as code.7-  Integrate practices like SBOM and manage vulnerabilities at scale.8-  To ensure that the IaC remains aligned with the constant changes apply Drift detection as an imperative practice.9-  Don\u2019t forget to consider that not all infrastructure is related to containers and orchestration, modern applications and cloud automation involve networking, serverless, multicloud deployments and complex dependencies.10- Remember that the repository\u2019s structure, pipelines, and platforms represent a structural or your organization, teams, culture, and communication practices. (Conway\u2019s law).- What are the security and governance concerts based on AWS Security Best practices?The well Architecture framework Security pillar resume the best practices, many tools likecheckovorkicsincorporated those practices into them checks and translate de definitions into configuration properties for the infrastructure components, you can also create a custom library with your requirements and apply policy as code into your CI/CD process and prioritize the preventive over detection controls using tools likeOpen Policy Agent (OPA).- How can create self service capabilities for the builder\u2019s team?It\u2019s a best practice to create aservice catalogfor your development team using service likeAWS Service Catalog, customblueprints in code catalystor enterprise catalog using open source likebackstage, the main point isapplying the well engineering practices abstracting your patterns are use cases around the end user(Builders team).- When is this approach useful?Implementing internal SaaS capabilities is useful on the condition that you have aplatform teamand enablement the lean product practices for create, keep, maintain, and release versions for your artifacts and portals, don\u2019t forget your mission, make possible that the builder teams will becentric applicationand keep in mind that too many variations increase maintenance costs and reduced the opportunity for sharing between teams. For this scenario, remember that you are doing\u201cDevOps for DevOps\u201dand it\u2019s practical *due to the size of builder\u2019s teams *(Fewer than 50 builders).- How can you manage the Multi environment deployments based on terraform on AWS?This is a critical point, a best practice iskeeping a central account to assume roles to apply the changes into other accounts according to AWS multi account structure (deployment account), for example, you can have an account for development, other for testing, other for staging, and another for production,map each account with a workspace or folder inside your project structure. For this scenario the approach is an account per workspace.- What are the tools for each critical step?In addition to the challenge of this blog, there are some tools available to make these tasks for each step-in Figure 1. For automating local development, you must havepre-commitwith the base hooks, in the pipelineKics **will be used for SAST to IaC you can include **Infracostfor cost detection, applying native terraform testing framework for tests orterratestand export test results in junix format to visualize with native codebuild reports or sending to a central test ops suite. For drift detection you can usedrifctl. If you want to enable automatic pull request review can introduceAtlantis.Figure 2. CI IaC some tools.Hands OnIts time to create!The project setup:According with the first post:DevSecOps with AWS- IaC at scale - Building your own platform - Part 1the terragrunt code will suffer additional changes.First, theterragrunt.hclfile change to pass the optionaloverwrite.auto.tfvarsas variable file to terraform CLI, this file will be generated into the pipeline usingoptional_var_filesblock.Second, the an environment variablepipelinewas introduce to assign the profile name for remote state due the dynamic profile name cross local environments and the pipeline.locals{common_vars=read_terragrunt_config(\"${get_parent_terragrunt_dir()}/common/common.hcl\")environment=read_terragrunt_config(\"${get_parent_terragrunt_dir()}/common/environment.hcl\")}terraform{extra_arguments\"init_arg\"{commands=[\"init\"]arguments=[\"-reconfigure\"]env_vars={TERRAGRUNT_AUTO_INIT=true}}extra_arguments\"common_vars\"{commands=get_terraform_commands_that_need_vars()required_var_files=[\"${get_parent_terragrunt_dir()}/common/common.tfvars\"]optional_var_files=[\"${get_parent_terragrunt_dir()}/overwrite.auto.tfvars\"]}}remote_state{backend=\"s3\"generate={path=\"remotebackend.tf\"if_exists=\"overwrite_terragrunt\"}config={profile=\"false\"==local.environment.locals.pipeline?local.common_vars.locals.backend_profile:\"backend_profile\"region=local.common_vars.locals.backend_regionbucket=local.common_vars.locals.backend_bucket_namekey=\"${local.common_vars.locals.project_folder}/${local.environment.locals.workspace}/${path_relative_to_include()}/${local.common_vars.locals.backend_key}\"dynamodb_table=local.common_vars.locals.backend_dynamodb_lockencrypt=local.common_vars.locals.backend_encrypt}}generate=local.common_vars.generateEnter fullscreen modeExit fullscreen modeThe CDK project has de common structure for python projects, here there are some key files:The pipeline steps definitions:Figure 3. Repository files - Steps Definitions.Create planFirst create a plan to move through the pipeline steps. You can use assume role block to setup the AWS providers for backend and infrastructure deployments, however,to keep the same code in the pipeline and in IDE the necessary profiles are created with AWS CLI and post build command clean the folder $HOME/.aws.Inpre-build commandtheworkspaceis setup as environment variable and newoverwrite.auto.tfvarsfile is created to use the previous profiles. Finally, the plan is created and saved in/tmp/allfolder.version:'0.2'env:variables:pipeline:'true'parameter-store:{}phases:install:runtime-versions:python:'3.12'commands:# Get the caller identity-caller_identity=$(aws sts get-caller-identity --output json)# Extract the ARN from the response-account_backend_role=$(echo $caller_identity | jq -r '.Account')-echo \"backend Role $backend_role\"-backend_role=arn:aws:iam::$account_backend_role:role/$backend_role-# Set the role ARN and the source profile name# Assume the role and get the temporary credentials-response=$(aws sts assume-role --role-arn \"$backend_role\" --role-session-name \"$project_name\" )# Extract the necessary values from the response-access_key_id=$(echo $response | jq -r '.Credentials.AccessKeyId')-secret_access_key=$(echo $response | jq -r '.Credentials.SecretAccessKey')-session_token=$(echo $response | jq -r '.Credentials.SessionToken')# Create the temporary profile in the AWS credentials file-aws configure set aws_access_key_id \"$access_key_id\" --profile backend_profile-aws configure set aws_secret_access_key \"$secret_access_key\" --profile backend_profile-aws configure set aws_session_token \"$session_token\" --profile backend_profile-echo \"Temporary backend profile 'backend_profile' created successfully.\"# create deployment profile-response=$(aws sts assume-role --role-arn arn:aws:iam::$deployment_account:role/$read_role --role-session-name \"$project_name\" --profile backend_profile )-access_key_id=$(echo $response | jq -r '.Credentials.AccessKeyId')-secret_access_key=$(echo $response | jq -r '.Credentials.SecretAccessKey')-session_token=$(echo $response | jq -r '.Credentials.SessionToken')# Create the temporary profile in the AWS credentials file-aws configure set aws_access_key_id \"$access_key_id\" --profile deployment_profile-aws configure set aws_secret_access_key \"$secret_access_key\" --profile deployment_profile-aws configure set aws_session_token \"$session_token\" --profile deployment_profile-aws sts get-caller-identity --output json --profile deployment_profile-echo \"Temporary backend profile 'deployment_profile' created successfully.\"-ls -allrun-as:rootpre_build:commands:-export TF_VAR_env=$workspace-printf \"profile={\\n\\\"%s\\\"={\\n \\\"profile\\\"=\\\"deployment_profile\\\" \\n \\\"region\\\"= \\\"%s\\\"\\n}\\n}\" $workspace $deployment_region > overwrite.auto.tfvars-echo \"Creating tfplan in plan folder for scanning\"-ls -R --ignore=venvbuild:commands:-terragrunt run-all plan -input=false -no-color -lock=false --terragrunt-out-dir /tmp/tfplan --terragrunt-json-out-dir /tmp/tfplan --terragrunt-exclude-dir .post_build:commands:-mv  /tmp/tfplan ./tfplan-rm -rf ~/.awsreports:report_group_name:files:-/tmp/tfplan/*file-format:JunitXmlartifacts:files:-'tfplan/**/*'exclude-paths:-'./venv/**/*'-'**/.terraform/**/*'Enter fullscreen modeExit fullscreen modeRun SASTIn this step **KICS **es the tool for the demonstration. Just simple docker step to run checks and create reports.The same approach is applied to use more tools like checkov, tfsec, trivy, terrascan and more.version:'0.2'env:variables:pipeline:'true'parameter-store:{}phases:install:runtime-versions:python:'3.12'commands:-ls -allrun-as:rootpre_build:commands:-ls -R-docker --versionbuild:commands:-echo \"Running kics\"-docker run -t -v ./tfplan:/path checkmarx/kics --ci scan -p /path --report-formats \"all\" --exclude-gitignore --output-path  /path  --fail-on \"critical\"run-as:rootpost_build:commands:-echo \"finished!\"reports:report_group_name:files:-./tfplan/junit-results.xmlfile-format:JunitXmlartifacts:files:-'**/*'exclude-paths:-'./venv/**/*'-'**/.terraform/**/*'Enter fullscreen modeExit fullscreen modeDeployment stepThis deployment step is to apply indevenvironment, get the currenttagand update a parameter store with the value for current infrastructure version.version:'0.2'env:variables:pipeline:'true'phases:install:runtime-versions:python:'3.12'commands:-# Get the caller identity-caller_identity=$(aws sts get-caller-identity --output json)# Extract the ARN from the response-account_backend_role=$(echo $caller_identity | jq -r '.Account')-echo \"backend Role $backend_role\"-backend_role=arn:aws:iam::$account_backend_role:role/$backend_role-# Set the role ARN and the source profile name# Assume the role and get the temporary credentials-response=$(aws sts assume-role --role-arn \"$backend_role\" --role-session-name \"$project_name\" )# Extract the necessary values from the response-access_key_id=$(echo $response | jq -r '.Credentials.AccessKeyId')-secret_access_key=$(echo $response | jq -r '.Credentials.SecretAccessKey')-session_token=$(echo $response | jq -r '.Credentials.SessionToken')# Create the temporary profile in the AWS credentials file-aws configure set aws_access_key_id \"$access_key_id\" --profile backend_profile-aws configure set aws_secret_access_key \"$secret_access_key\" --profile backend_profile-aws configure set aws_session_token \"$session_token\" --profile backend_profile-echo \"Temporary backend profile 'backend_profile' created successfully.\"# create deployment profile-response=$(aws sts assume-role --role-arn arn:aws:iam::$deployment_account:role/$write_role --role-session-name \"$project_name\" --profile backend_profile )-access_key_id=$(echo $response | jq -r '.Credentials.AccessKeyId')-secret_access_key=$(echo $response | jq -r '.Credentials.SecretAccessKey')-session_token=$(echo $response | jq -r '.Credentials.SessionToken')# Create the temporary profile in the AWS credentials file-aws configure set aws_access_key_id \"$access_key_id\" --profile deployment_profile-aws configure set aws_secret_access_key \"$secret_access_key\" --profile deployment_profile-aws configure set aws_session_token \"$session_token\" --profile deployment_profile-aws sts get-caller-identity --output json --profile deployment_profile-echo \"Temporary backend profile 'deployment_profile' created successfully.\"-ls -allpre_build:commands:-export TF_VAR_env=$workspace-printf \"profile={\\n\\\"%s\\\"={\\n \\\"profile\\\"=\\\"deployment_profile\\\" \\n \\\"region\\\"= \\\"%s\\\"\\n}\\n}\" $workspace $deployment_region > overwrite.auto.tfvars-ls -R --ignore=venvbuild:commands:-echo 'Deploying ... '-terragrunt run-all apply  --terragrunt-non-interactive  --terragrunt-use-partial-parse-config-cache --terragrunt-exclude-dir .-export current_tag=`git describe --abbrev=0 --tag`-echo \"Updating parameter with new version\"-aws ssm put-parameter --name /$project_name/$workspace/version --type \"String\" --value $current_tag  --overwriteEnter fullscreen modeExit fullscreen modeIn the pipeline you can add manual approve for peer review steps and watch the state in Slack Channel or custom notification in Microsoft.The Pipeline in AWS CodePipelineFigure 4. Deployment AWS Code tools.The code!In the next post the code will be shared, thanks for reading and sharing!  \u263a\ufe0f\u2b50"}
{"title": "Issue 47 and 48 of AWS Cloud Security Weekly", "published_at": 1718068325, "tags": ["security", "news", "aws"], "user": "AJ", "url": "https://dev.to/aws-builders/issue-47-and-48-of-aws-cloud-security-weekly-4c0i", "details": "(This is just the highlight of Issue 47 and 48 of AWS Cloud Security weekly @https://aws-cloudsec.com/p/issue-47-and-48<< Subscribe to receive the full version in your inbox weekly for free!!).What happened in AWS CloudSecurity & CyberSecurity last week May 28-June 10, 2024?Amazon Verified Permissions has enhanced support for securing Amazon API Gateway APIs by enabling fine-grained access controls using an OpenID Connect (OIDC) compliant identity provider. Developers can now manage access based on user attributes and group memberships without writing any code.AWS WAF now lets you choose specific versions of Bot Control and Fraud Control managed rule groups within your web ACLs, providing ability to manage traffic when AWS releases updates to these rule groups. With versioning, you can test new and updated bot and fraud rules before deploying them to production. For example, you can apply a new version of a managed rule group to a staging environment to assess its effectiveness. Then, you can gradually roll out the version in production to monitor its impact closely before fully enabling it. If a new version causes issues, you can quickly revert to the previous version to restore the original behavior. By default, you will be configured to use version 1.0 of the Bot Control and Fraud Control managed rule groups and will continue to receive periodic AWS updates. If you prefer not to receive automatic updates, you can select a specific version and remain on that version until you manually update or it reaches end of life.Previously, Amazon Cognito user pools introduced the ability to enrich identity and access tokens with custom attributes through OAuth 2.0 scopes and claims. Now, this functionality has been expanded to include complex custom attributes such as arrays, maps, and JSON objects in both identity and access tokens. This enhancement allows for fine-grained authorization decisions based on these complex custom attributes. The feature supports enhanced personalization and increased access control while simplifying the migration and modernization of your applications to use Amazon Cognito with minimal or no changes.You can now enable Route 53 Profiles in the AWS GovCloud (US-East) and AWS GovCloud (US-West) Regions, allowing you to define a standard DNS configuration as a Profile, which may include Route 53 private hosted zone (PHZ) associations, Route 53 Resolver rules, and Route 53 Resolver DNS Firewall rule groups. You can apply this configuration to multiple VPCs in your account. Profiles also help enforce DNS settings for your VPCs, including DNSSEC validations, Resolver reverse DNS lookups, and DNS Firewall failure mode. Additionally, you can share Profiles with AWS accounts in your organization using AWS Resource Access Manager (RAM). Route 53 Profiles streamline the process of associating Route 53 resources and VPC-level DNS settings across VPCs and AWS accounts within a Region, reducing the complexity of managing each resource and setting individually for each VPC.AWS Audit Manager has introduced a common control library to streamline automating risk and compliance assessments against enterprise controls. This library allows Governance, Risk, and Compliance (GRC) teams to efficiently map their controls into Audit Manager for evidence collection. The common control library includes predefined and pre-mapped AWS data sources, removing the need to identify specific AWS resources for various controls. It features AWS-managed common controls, determined by extensive mapping and reviews by AWS-certified auditors, to ensure the correct data sources are used for evidence collection. With this launch, Audit Manager also provides additional evidence mappings for controls, including support for 140 new API calls. You can customize and update all evidence mappings to fit your specific objectives.Amazon Inspector now provides native integration with Amazon CodeCatalyst and GitHub Actions for container image scanning. This enables customers to assess their container images for software vulnerabilities within their Continuous Integration and Continuous Delivery (CI/CD) tools, enhancing security earlier in the software development lifecycle. With this expansion, Inspector now integrates natively with four developer tools: Jenkins, TeamCity, GitHub Actions, and Amazon CodeCatalyst for container image scanning. This feature is compatible with CI/CD tools hosted on AWS, on-premises, or in hybrid cloud environments, offering developers a consistent solution across all their development pipelines.Trending on the news & advisories (Subscribe to the newsletter for details):AWS new cohort of AWS Heroes.HuggingFace- Space secrets leak disclosure.Apple to Debut Passwords App in Challenge to 1Password, LastPass.FBI announced 7,000 LockBit decryption keys.Kali Linux 2024.2 Released.BBC- data security incident announcement.Cloudflare acquires BastionZero.Fortinet to Acquire Lacework.Mandiant- UNC5537 Targets Snowflake Customer Instances for Data Theft and Extortion.Report of New York Times source code leak:"}
{"title": "Introduction to Git", "published_at": 1718044779, "tags": ["git", "github", "vcs", "scm"], "user": "Srinivasulu Paranduru", "url": "https://dev.to/aws-builders/introduction-to-git-ga9", "details": "Quality control is critical, and developers work in small teams using Git for version control.Introduction to Git.What is version control?Version control system (VCS) is a program or set of programs that tracks changes to a collection of files.Another goal is to allow several team members to work on a project, even on the same files, at the same time without affecting each other's work.Another name for a VCS is a software configuration management (SCM) system.To learn more about git andofficial documentationWith VCSYou can see who made the changes and their comments at the time of committing filesRetrieve past versions of the entire projectCreate branchesAttach a tag to a version\u2014for example, to mark a new release.Distributed version controlEarlier instances of VCSes, including CVS, Subversion (SVN) used a centralized server to store a project's history. This centralization meant that the one server was also potentially a single point of failure.Git is distributed, which means that a project's complete history is stored both on the client and on the server. You can edit files without a network connection, check them in locally, and sync with the server when a connection becomes available.Git TerminologyRepository (repo): The directory, located at the top level of a working tree, where Git keeps all the history and metadata for a project. Repositories are almost always referred to as repos.Commit: When used as a verb, commit means to make a commit object.Branch: A branch is a named series of linked commits. The most recent commit on a branch is called the head. The default branch, which is created when you initialize a repository, is called main, often named master in Git. The head of the current branch is named HEAD.Remote: A remote is a named reference to another Git repository. When you create a repo, Git creates a remote named origin that is the default remote for push and pull operations.Git Command line: Different GUIs available for GitGit DesktopVisual Studio CodeDifferences between Git and GitHubGitGitHubGit is a distributed version control system (DVCS) that multiple developers and other contributors can use to work on a project.GitHub is a cloud platform that uses Git as its core technology.GitHub act s as the remote repositoryKey features provided by GitHub include:IssuesDiscussionsPull requestsNotificationsLabelsActionsForksProjectsTry out-https://learn.microsoft.com/en-us/training/modules/intro-to-git/2-exercise-configure-gitReferences :Introduction to GitHubGetting Started with GitHubBasic Git commandsgit status : git status displays the state of the working treegit add : git add is the command you use to tell Git to start keeping track of changes in certain files. You'll use git add to stage changes to prepare for a commit. All changes in files that have been added but not yet committed are stored in the staging area.git commandgit log : The git log command allows you to see information about previous commits.git help : Each command comes with its own help page, too. You can find these help pages by typing git  --help. For example,git commit --helpbrings up a page that tells you more about the git commit command and how to use it.References:Every day git\ud83d\udcac If you enjoyed reading this blog post and found it informative, please take a moment to share your thoughts by leaving a review and liking it \ud83d\ude00 and follow me indev.to,linkedinandbuy me a coffee"}
{"title": "GitHub Foundation Certification Preparation", "published_at": 1718033839, "tags": ["github", "foundation", "git"], "user": "Srinivasulu Paranduru", "url": "https://dev.to/aws-builders/github-foundation-certification-preparation-4ojm", "details": "GitHub is where over 100 million developers shape the future of software, together. Contribute to the open source community, manage your Git repositoriesGitHub Fundamentals Exam References :Github Fundamentals Study GuideMicrosoft learnLinkedin learningFree Practise test:https://ghcertified.com/practice_tests/How to register for github certification exams:Create a Github account using theurlUse the linkto register hereLogin using Github accountCurrently there are 4 exams available from GithubGitHub Foundations - Consists of 75 questions in examGitHub Actions  - Consists of 75 questions in examGitHub Admin  - Consists of 66 questions in examGitHub Advanced SecurityInitially all exams are disabled and links will be enabled with in 24 hours after clicking on the above links and then you can book the exam based on your plan which exam you are planning to give.After clearing your exam :You will get an email from credly to accept badge using the link shared in the email. If you dont have login, create a login then accept the badge.Click on the badge in credly and then click on share button highlighted in the image belowYou will get multiple options to share credly badge in linkedin, download badge image and download certificateClick on download certificate- Will be adding more blogs towards Github foundations exam in the next couple of days.References :https://github.com/cloudteachable/github_certificationsFAQS -https://dev.to/learnwithsrini/import-topics-in-github-fundamentals-2ph8Conclusion:\ud83d\udcac If you enjoyed reading this blog post and found it informative, please take a moment to share your thoughts by leaving a review and liking it \ud83d\ude00 and follow me indev.to,linkedin"}
{"title": "Spring Boot 3 application on AWS Lambda - Part 7 Measuring cold and warm starts with AWS Lambda Web Adapter", "published_at": 1718031697, "tags": ["aws", "serverless", "java", "springboot"], "user": "Vadym Kazulkin", "url": "https://dev.to/aws-builders/spring-boot-3-application-on-aws-lambda-part-7-measuring-cold-and-warm-starts-with-aws-lambda-web-adapter-310h", "details": "IntroductionIn thepart 5of the series we introduced AWS Lambda Web Adapter tool, and in thepart 6we explained how to develop Lambda function with AWS Lambda Web Adapter using Java 21 and Spring Boot 3.In this article of the series, we'll measure the cold and warm start time including enabling SnapStart on the Lambda function but also applying various priming techniques like priming the DynamoDB invocation and priming the whole web request. We'll use Spring Boot 3.2sample applicationfor our measurements, and for all Lambda functions use JAVA_TOOL_OPTIONS: \"-XX:+TieredCompilation -XX:TieredStopAtLevel=1\" and give them Lambda 1024 MB memory. Current Spring Boot version 3.3. should work out of the box by switching the version in the POM file.Measuring cold starts and warm time with AWS Lambda Web Adapter and using Java 21 and Spring Boot 3Enabling SnapStart on Lambda function is only a matter of configuration like :SnapStart:   ApplyOn: PublishedVersionsEnter fullscreen modeExit fullscreen modeapplied in The Lambda Function Properties or Global Functions section of the SAM template, I'd like to dive deeper to how to use priming on top of it for our use case. I explained the ideas behind priming (in our case of DynamoDB request) in my articleAWS Lambda SnapStart - Part 5 Measuring priming, end to end latency and deployment timeThe code for priming of DynamoDB request can be foundhere.This class Priming is additionally annotated with@Configuration so it will be initialized as well before the initialization of bean, repositories and controllers is completed.It implements import org.crac.Resourceinterface of theCraC project.With this invocationCore.getGlobalContext().register(this);Enter fullscreen modeExit fullscreen modePriming class registers itself as CRaC resource. We additionally prime the DynamoDB invocation by implementingbeforeCheckpointmethod from theCRaC API.@Override \u2002\u2002\u2002\u2002\u2002\u2002public void beforeCheckpoint(org.crac.Context<? extends Resource> context) throws Exception { \u2002\u2002\u2002\u2002\u2002\u2002\u2002\u2002\u2002\u2002\u2002\u2002 productDao.getProduct(\"0\"); \u2002\u2002\u2002\u2002\u2002\u2002}Enter fullscreen modeExit fullscreen modewhich we'll be invoked during the deployment phase of the Lambda function and before Firecracker microVM snapshot is taken.Web request invocation priming that we explored with AWS Serverless Java Container in the articleSpring Boot 3 application on AWS Lambda - Part 4 Measuring cold and warm starts with AWS Serverless Java Containeris not applicable to AWS Lambda Web Adapter as the latter doesn't offer the formal API for the request invocation as AWS Serverless Java Container does.The results of the experiment below were based on reproducing more than 100 cold and approximately 100.000 warm starts with Lambda function with 1024 MB memory setting for the duration of 1 hour. For it I used the load test toolhey, but you can use whatever tool you want, likeServerless-artilleryorPostman.I ran all these experiments on ourGetProductByIdWithSpringBoot32WithLambdaWebAdapterLambda function with 3 different scenarios with compilation option \"-XX:+TieredCompilation -XX:TieredStopAtLevel=1\" (client compilation without profiling). For further explanation please read my article about this topicAWS SnapStart - Part 14 Measuring cold starts and deployment time with Java 21 using different compilation options1) No SnapStart enabledin template.yaml use the following configuration:Globals:   Function:     Handler: run.sh     CodeUri: target/aws-spring-boot-3.2-lambda-web-adapter-1.0.0-SNAPSHOT.jar     Runtime: java21     ....     #SnapStart:       #ApplyOn: PublishedVersionsEnter fullscreen modeExit fullscreen mode2) SnapStart enabled but no priming appliedin template.yaml use the following configuration:Globals:   Function:     Handler: run.sh     CodeUri: target/aws-spring-boot-3.2-lambda-web-adapter-1.0.0-SNAPSHOT.jar     Runtime: java21     ....     SnapStart:       ApplyOn: PublishedVersionsEnter fullscreen modeExit fullscreen modeIf we'd like to measure cold and warm starts, so the priming effect won't take place for the SnapStart enabled Lambda function we have to additionally remove@Configurationannotation from thePriming classor delete the entire Priming class.3) SnapStart enabled with DynamoDB invocation primingin template.yaml use the following configuration:Globals:   Function:     Handler: run.sh     CodeUri: target/aws-spring-boot-3.2-lambda-web-adapter-1.0.0-SNAPSHOT.jar     Runtime: java21     ....     SnapStart:       ApplyOn: PublishedVersionsEnter fullscreen modeExit fullscreen modeand leave the Priming class described above as it is, so the priming effect takes place.I will refer to those scenarios by their numbers in the tables below, for example scenario number 3) stays for \"SnapStart enabled with DynamoDB invocation priming\". Abbreviationcis for the cold start andwis for the warm start.Cold (c) and warm (w) start time in ms:Scenario Numberc p50c p75c p90c p99c p99.9c maxw p50w p75w p90w p99w p99.9w max14777.944918.465068.175381.415392.185392.338.609.7711.9225.82887.921061.5421575.491649.612132.752264.572273.642274.458.269.3811.0923.461136.711761.123674.35709.61974.141114.871195.661196.798.008.8010.1621.84317.66624.33ConclusionBy enabling SnapStart on the Lambda function alone, it reduces the cold start time of the Lambda function significantly. By additionally using DynamoDB invocation priming we are be able to achieve cold starts only slightly higher than cold starts described in my articleAWS SnapStart -Measuring cold and warm starts with Java 21 using different memory settingswhere we measured cold and warm starts for the pure Lambda function without the usage of any frameworks including 1024MB memory setting like in our scenario.Comparing the cold and warm start times we measured with AWS Serverless Java Container in the articleMeasuring cold and warm starts with AWS Serverless Java Containerwe observe that AWS Lambda Web Adaptor offers much lower cold start times for all scenarios. Even priming DynamoDB invocation with AWS Lambda Web Adapter offers lower cold start times than the web request invocation priming for the Serverless Java Container. Warm start times are very similar for both approaches being very slightly lower for Serverless Java Container.In the next part of the series, I'll introduce Spring Cloud Function project and concepts behind it as another alternative to develop and run Spring Boot applications on AWS Lambda."}
{"title": "Everything you need to know about monitoring CoreDNS for DNS performance", "published_at": 1717992431, "tags": ["dns", "eks", "troubleshooting", "monitoring"], "user": "saifeddine Rajhi", "url": "https://dev.to/aws-builders/everything-you-need-to-know-about-monitoring-coredns-for-dns-performance-5hi9", "details": "\ud83d\udcda Introduction:Running DNS-intensive workloads can sometimes lead to intermittentCoreDNSfailures caused by DNS throttling. These issues can have a significant impact on your applications.Such disruptions can hinder the reliability and performance of your services, making it mandatory to have a monitoring solution in place.AWS offers a suite of open-source tools\u200a-\u200aCloudWatch, Fluentd, and Grafana\u200a-\u200athat can be integrated to monitor CoreDNS.Introduction to Kubernetes DNS:Kubernetes relies onDNSfor service discovery within clusters. When applications running in pods need to communicate with each other, they often refer to services by their domain names rather than IP addresses.This is whereKubernetes DNS comes into play. It ensures that these domain names are resolved to the correct IP addresses, allowing pods and services to communicate.In Kubernetes, each pod is assigned a temporary IP address. However, these IP addresses are dynamic and can change over time, making it challenging for applications to keep track of them.Kubernetes addresses this challenge by assigning fully qualified domain namesFQDNsto pods and services.CoreDNS, the default DNS provider in Kubernetes, is responsible for handling DNS queries within the cluster. It maps these FQDNs to the corresponding IP addresses, enabling communication between pods and services.Why DNS Issues Are\u00a0Common:DNS issues are a common source of frustration in network troubleshooting. DNS plays a big role in translating human-readable domain names into machine-readable IP addresses.However, DNS problems can arise due to many factors such as misconfigurations, network issues, or server failures. When DNS fails to resolve domain names correctly, applications may experience connectivity issues or fail to access external services.CoreDNS in Kubernetes:CoreDNSplays an important role in providing DNS services within Kubernetes clusters. As the default DNS provider since Kubernetes v1.13, CoreDNS simplifies cluster networking by enabling clients to access services using DNS names rather than IP addresses. It resolves domain name requests and facilitates service discovery within the cluster.How CoreDNS Operates:CoreDNS operates as a resolver and forwarder for DNS requests within Kubernetes clusters. When a pod needs to communicate with another service, it sends a DNS query to CoreDNS, specifying the domain name of the target service. CoreDNS then resolves this query by mapping the domain name to the corresponding IP address using its internal records.For external domain names that CoreDNS is not authoritative for, it forwards the DNS query to public resolvers or upstream DNS servers for resolution.To enhance performance and reduce latency, CoreDNS can cache DNS responses for frequently accessed domain names. This caching mechanism improves the responsiveness of DNS queries and reduces the load on upstream DNS servers.CoreDNS achieves this functionality through its modular architecture and extensible plugin system, allowing operators to customize and optimize DNS resolution according to their specific requirements.Mitigating CoreDNS Throttling in Amazon\u00a0EKS:In Amazon EKS clusters, CoreDNS and DNS throttling issues can be challenging to identify and troubleshoot.While many users focus on monitoring CoreDNS logs and metrics, they often overlookthe hard limit of 1024 packets per second (PPS) enforcedat theElastic Network Interface (ENI)level. Understanding how this limit can lead to throttling issues requires insight into the typical DNS resolution flow of a Kubernetes pod.In a Kubernetes environment, pods must resolve domain names for both internal and external services to enable communication. This resolution process involves routing DNS queries through theworker node's ENI, particularly when resolving external endpoints. Even for internal endpoints, if the CoreDNS pod is not co-located with the querying pod, DNS packets still traverse the worker node's ENI.Consider a scenario where there is a suddensurgein DNS queries, causing the PPS to approach the hard limit of 1024. This situation can result in DNS throttling, impacting all microservices running on the affected worker node. Unfortunately, troubleshooting such issues can be hard because the focus tends to be on CoreDNS pods rather than ENI metrics.To mitigate DNS throttling issues in EKS clusters, it is important to monitor packet drops occurring at the ENI level continuously. This monitoring allows for early detection and prevention of potential outages. In this blog post, we introduce a solution that leverages network performance metrics to identify DNS throttling issues effectively.Solution: \ud83c\udf89An easy way to identify the DNS throttling issues in worker nodes is by capturinglinklocal_allowance_exceededmetric provided bythe Elastic Network Adapter (ENA) driverand other metrics also obviously.Thelinklocal_allowance_exceededis number of packets dropped because the PPS of the traffic to local proxy services exceeded the maximum for the network interface. This impacts traffic to the DNS service, the Instance Metadata Service, and the Amazon Time Sync Service.Instead of tracking this event in real-time, we can stream this metric toAmazon Managed Service for Prometheusas well and can have them visualized inAmazon Managed Grafana.Hands-on: collect and visualize CoreDNS metrics in AWS\u00a0EKS:TheCoreDNSprometheusplugin exposes metrics in theOpenMetricsformat, a text-based standard that evolved from the Prometheus format. In a Kubernetes cluster, the plugin is enabled by default, so you can begin monitoring many key metrics as soon as you launch your cluster.By default, theprometheusplugin writes metrics to a/metricsendpoint on port 9153 on each CoreDNS pod.Create an Amazon Managed Service for Prometheus workspace and Managed Service for Grafana:In this step, we will create a workspace for Amazon Managed Service for Prometheus and Managed Service for Grafana:the configuration in these files creates:AMP workspaceAMP alert manager definition.main.tf:module \"prometheus\" {   source = \"terraform-aws-modules/managed-service-prometheus/aws\"    workspace_alias = \"demo-coredns\"    alert_manager_definition = <<-EOT   alertmanager_config: |     route:       receiver: 'default'     receivers:       - name: 'default'   EOT    rule_group_namespaces = {} }Enter fullscreen modeExit fullscreen modeversions.tf:terraform {   required_version = \">= 1.3\"    required_providers {     aws = {       source  = \"hashicorp/aws\"       version = \">= 5.32\"     }   } }Enter fullscreen modeExit fullscreen modeTo run the terraform, you need to execute:$ terraform init $ terraform plan $ terraform applyEnter fullscreen modeExit fullscreen modethe below configuration files create will create:Default Grafana workspace (using defaults provided by the module).main.tf:provider \"aws\" {   region = local.region }  data \"aws_availability_zones\" \"available\" {}  locals {   region      = \"eu-west-1\"   name        = \"amg-ex-${replace(basename(path.cwd), \"_\", \"-\")}\"   description = \"AWS Managed Grafana service for ${local.name}\"    vpc_cidr = \"10.0.0.0/16\"   azs      = slice(data.aws_availability_zones.available.names, 0, 3) }  ################################################################################ # Managed Grafana Module ################################################################################  module \"managed_grafana\" {   source = \"../..\"    # Workspace   name                      = local.name   associate_license         = false   description               = local.description   account_access_type       = \"CURRENT_ACCOUNT\"   authentication_providers  = [\"AWS_SSO\"]   permission_type           = \"SERVICE_MANAGED\"   data_sources              = [\"CLOUDWATCH\", \"PROMETHEUS\", \"XRAY\"]   notification_destinations = [\"SNS\"]   stack_set_name            = local.name   grafana_version           = \"9.4\"    configuration = jsonencode({     unifiedAlerting = {       enabled = true     },     plugins = {       pluginAdminEnabled = false     }   })    # vpc configuration   vpc_configuration = {     subnet_ids = module.vpc.private_subnets   }   security_group_rules = {     egress_postgresql = {       description = \"Allow egress to PostgreSQL\"       from_port   = 5432       to_port     = 5432       protocol    = \"tcp\"       cidr_blocks = module.vpc.private_subnets_cidr_blocks     }   }    # Workspace API keys   workspace_api_keys = {     viewer = {       key_name        = \"viewer\"       key_role        = \"VIEWER\"       seconds_to_live = 3600     }     editor = {       key_name        = \"editor\"       key_role        = \"EDITOR\"       seconds_to_live = 3600     }     admin = {       key_name        = \"admin\"       key_role        = \"ADMIN\"       seconds_to_live = 3600     }   }    # Workspace IAM role   create_iam_role                = true   iam_role_name                  = local.name   use_iam_role_name_prefix       = true   iam_role_description           = local.description   iam_role_path                  = \"/grafana/\"   iam_role_force_detach_policies = true   iam_role_max_session_duration  = 7200   iam_role_tags                  = { role = true }    tags = local.tags }  module \"managed_grafana_default\" {   source = \"../..\"    name              = \"${local.name}-default\"   associate_license = false    tags = local.tags }  module \"managed_grafana_disabled\" {   source = \"../..\"    name   = local.name   create = false }  ################################################################################ # Supporting Resources ################################################################################  module \"vpc\" {   source  = \"terraform-aws-modules/vpc/aws\"   version = \"~> 5.0\"    name = local.name   cidr = local.vpc_cidr    azs             = local.azs   private_subnets = [for k, v in local.azs : cidrsubnet(local.vpc_cidr, 4, k)]   public_subnets  = [for k, v in local.azs : cidrsubnet(local.vpc_cidr, 8, k + 48)]    enable_nat_gateway = false    single_nat_gateway = true    tags = local.tags }Enter fullscreen modeExit fullscreen modeversions.tf:terraform {   required_version = \">= 1.0\"    required_providers {     aws = {       source  = \"hashicorp/aws\"       version = \">= 5.0\"     }   } }Enter fullscreen modeExit fullscreen modeTo run this code you need to execute:$ terraform init $ terraform plan $ terraform applyEnter fullscreen modeExit fullscreen modeDeploying Prometheus ethtool exporter:Ethtoolis a Linux tool for configuring and gathering information about Ethernet devices on worker nodes. We will use ethtool's output to detect packet loss and convert it to Prometheus format with a Prometheus ethtool exporter utility.The deployment contains a Python script that pulls information from ethtool and publishes it in Prometheus format.kubectl apply -f https://raw.githubusercontent.com/Showmax/prometheus-ethtool-exporter/master/deploy/k8s-daemonset.yamlEnter fullscreen modeExit fullscreen modeDeploy ADOT collector to scrape ethtool metrics:In this step we will deploy the ADOT collector and configure the ADOT collector to ingest metrics to Amazon Managed Service for Prometheus.We will be using theAmazon EKS add-on for ADOT operatorto send the metrics \"linklocal_allowance_exceeded\" to Amazon Managed Service for Prometheus for monitoring CoreDNS.Create an IAM role and Amazon EKS Service Account:We will be deploying the ADOT collector to run under the identity of a Kubernetes service account \"adot-collector\".IAM roles for service accounts (IRSA)lets you associate theAmazonPrometheusRemoteWriteAccessrole with a Kubernetes service account, thereby providing IAM permissions to any pod utilizing the service account to ingest the metrics to Amazon Managed Service for Prometheus.You need kubectl and eksctl CLI tools to run the script. They must be configured to access your Amazon EKS cluster.eksctl create iamserviceaccount \\ --name adot-collector \\ --namespace default \\ --region eu-west-1\\ --cluster coredns-monitoring-demo\\ --attach-policy-arn arn:aws:iam::aws:policy/AmazonPrometheusRemoteWriteAccess \\ --approve \\ --override-existing-serviceaccountsEnter fullscreen modeExit fullscreen modeInstall ADOT add-on:You can check the list of add-ons enabled for different versions of Amazon EKS using the following command:Determine the ADOT versions that are available that are supported by your cluster's version.aws eks describe-addon-versions --addon-name adot --kubernetes-version 1.28 \\   --query \"addons[].addonVersions[].[addonVersion, compatibilities[].defaultVersion]\" --output textEnter fullscreen modeExit fullscreen modeRun the following command to install the ADOT add-on, replace the \u2013addon-version flag based on your Amazon EKS cluster version as show in step above.aws eks create-addon --addon-name adot --addon-version v0.66.0- eksbuild.1 --cluster-name coredns-monitoring-demoEnter fullscreen modeExit fullscreen modeVerify that ADOT add-on is ready using the following command.kubectl get po -n opentelemetry-operator-systemEnter fullscreen modeExit fullscreen modeThe following procedure uses an example YAML file with deployment as the mode value. This is the default mode and deploys the ADOT Collector similarly to a standalone application. This configuration receives OTLP metrics from the sample application and Amazon Managed Service for Prometheus metrics scraped from pods on the clustercurl -o collector-config-amp.yaml https://raw.githubusercontent.com/aws-observability/aws-otel-community/master/sample-configs/operator/collector-config-amp.yamlEnter fullscreen modeExit fullscreen modeIn collector-config-amp.yaml, replace the following with your own values:*** mode: deployment * serviceAccount: adot-collector * endpoint: \"\" * region: \"\" * name: adot-collector**kubectl apply -f collector-config-amp.yamlEnter fullscreen modeExit fullscreen modeOnce the adot collector is deployed, the metrics will be stored successfully in Amazon Prometheus.Visualize ethtool metrics in Amazon Managed Grafana:Configure the Amazon Managed Service for Prometheus workspace as a datasource inside the Amazon Managed Grafana console.Let's explore the metrics in Amazon Managed Grafana now: Click the explore button, and search for ethtool:Let's build a dashboard for the linklocal_allowance_exceeded metric by using the query:rate(node_net_ethtool{device=\"eth0\",type=\"linklocal_allo wance_exceeded\"} [30s])Enter fullscreen modeExit fullscreen modeWe can see that there were no packets dropped as the value is zero. You can further extend this by configuring alerts in the alert manager in Amazon Managed Service for Prometheus to send notifications.Conclusion:In this post, we showed how to monitor and create alerts for CoreDNS throttling issues using AWS Distro for OpenTelemetry (ADOT), Amazon Managed Service for Prometheus, and Amazon Managed Grafana. By monitoring the coreDNS metrics, customers can proactively detect packet drops and take preventive actions.Until next time  \ud83c\udf89Thank you for Reading\u00a0!! \ud83d\ude4c\ud83c\udffb\ud83d\ude01\ud83d\udcc3, see you in the next blog.\ud83e\udd18\ud83d\ude80 Thank you for sticking up till the end. If you have any questions/feedback regarding this blog feel free to connect with me:\u267b\ufe0f LinkedIn:https://www.linkedin.com/in/rajhi-saif/\u267b\ufe0fTwitter\u00a0:https://twitter.com/rajhisaifeddineThe end \u270c\ud83c\udffb**_\ud83d\udd30 Keep Learning\u00a0!! Keep Sharing\u00a0!!\u00a0\ud83d\udd30References:_**https://cilium.io/blog/2019/12/18/how-to-debug-dns-issues-in-k8s/https://sysdig.com/blog/how-to-monitor-coredns/https://www.datadoghq.com/blog/coredns-metrics/https://www.datadoghq.com/blog/coredns-monitoring-tools/https://aws.amazon.com/blogs/mt/monitoring-coredns-for-dns-throttling-issues-using-aws-open-source-monitoring-services/"}
{"title": "V\u00eddeo de apresenta\u00e7\u00e3o e artigo do projeto de identifica\u00e7\u00e3o facial com AWS Rekognition", "published_at": 1717985861, "tags": ["aws", "cloud", "communitybuilder", "architecture"], "user": "Carlos Filho", "url": "https://dev.to/aws-builders/video-de-apresentacao-e-artigo-do-projeto-de-identificacao-facial-com-aws-rekognition-4efa", "details": "Hoje vou apresentar um projeto que envolve a implementa\u00e7\u00e3o de pontos de identifica\u00e7\u00e3o facial em 30 locais de monitoramento. Esse projeto ser\u00e1 realizado utilizando diversos servi\u00e7os da AWS para garantir seguran\u00e7a, efici\u00eancia e escalabilidade. Vamos come\u00e7ar com uma vis\u00e3o geral do projeto e, em seguida, detalhar cada componente da arquitetura.Descri\u00e7\u00e3o do projetoO projeto tem como objetivo implementar um sistema de reconhecimento facial para monitoramento em 30 locais diferentes. Estimamos que o sistema atender\u00e1 entre 2000 a 3000 pessoas. A arquitetura do sistema precisa ser robusta, segura e capaz de lidar com o grande volume de dados gerado. Para isso, utilizaremos uma combina\u00e7\u00e3o de servi\u00e7os da AWS.Locais de monitoramentoCada um dos 30 locais de monitoramento ser\u00e1 equipado com c\u00e2meras que capturam imagens faciais. Essas c\u00e2meras enviar\u00e3o as imagens para um dispositivo de borda ou uma inst\u00e2ncia local que processar\u00e1 e encaminhar\u00e1 essas imagens para a AWS.Edge device/Inst\u00e2ncia localOs dispositivos de borda ou inst\u00e2ncias locais s\u00e3o respons\u00e1veis por receber as imagens das c\u00e2meras e envi\u00e1-las para a AWS. Essa conex\u00e3o ser\u00e1 feita de forma segura, utilizando AWS Direct Connect ou VPN para garantir que os dados sejam transmitidos com seguran\u00e7a e efici\u00eancia.Amazon S3Uma vez que as imagens chegam \u00e0 AWS, elas s\u00e3o armazenadas no Amazon S3. O Amazon S3 \u00e9 um servi\u00e7o de armazenamento altamente escal\u00e1vel e seguro, ideal para armazenar grandes volumes de dados, como as imagens capturadas. Al\u00e9m disso, o S3 permite que a gente configure notifica\u00e7\u00f5es de eventos, que ser\u00e3o usadas para acionar outras partes do nosso sistema.AWS LambdaQuando uma nova imagem \u00e9 carregada no Amazon S3, um evento \u00e9 disparado para uma fun\u00e7\u00e3o AWS Lambda. O AWS Lambda \u00e9 um servi\u00e7o de computa\u00e7\u00e3o serverless que permite executar c\u00f3digo em resposta a eventos sem a necessidade de provisionar ou gerenciar servidores. O Lambda processa a imagem e a envia para o Amazon Rekognition para an\u00e1lise.Amazon RekognitionO Amazon Rekognition \u00e9 um servi\u00e7o de an\u00e1lise de imagens que utiliza aprendizado de m\u00e1quina para realizar o reconhecimento facial. Ele compara a imagem recebida com as fotos armazenadas no nosso banco de dados e retorna os resultados. Esse processo \u00e9 r\u00e1pido e altamente preciso, garantindo que as identifica\u00e7\u00f5es sejam feitas corretamente.Amazon RDSOs resultados do Amazon Rekognition, juntamente com as informa\u00e7\u00f5es pessoais e fotos das pessoas, s\u00e3o armazenados no Amazon RDS. O Amazon RDS \u00e9 um servi\u00e7o de banco de dados relacional gerenciado que facilita a configura\u00e7\u00e3o, opera\u00e7\u00e3o e escalabilidade de um banco de dados na nuvem. Ele oferece alta disponibilidade e seguran\u00e7a para os nossos dados.Amazon API GatewayPara permitir que os dispositivos locais consultem os resultados do reconhecimento facial, utilizamos o Amazon API Gateway. O API Gateway facilita a cria\u00e7\u00e3o, publica\u00e7\u00e3o, manuten\u00e7\u00e3o, monitoramento e prote\u00e7\u00e3o de APIs em escala. Ele exp\u00f5e endpoints que os dispositivos locais podem usar para obter informa\u00e7\u00f5es do sistema.AWS CloudWatchFinalmente, utilizamos o AWS CloudWatch para monitorar todos os servi\u00e7os e garantir que o sistema esteja funcionando corretamente. O CloudWatch coleta m\u00e9tricas e logs, permitindo que configuremos alertas e tomemos a\u00e7\u00f5es proativas em caso de problemas.Sincroniza\u00e7\u00e3o de dadosPara garantir que os dados estejam sempre atualizados, utilizamos o AWS Database Migration Service (DMS) para sincronizar o Amazon RDS com o banco de dados local. Isso assegura que quaisquer mudan\u00e7as feitas no banco de dados local sejam refletidas no RDS e vice-versa, mantendo a integridade e a consist\u00eancia dos dados.Desenho da arquiteturaAgora, vamos ver como todos esses componentes se conectam em um diagrama de arquitetura.C\u00e2meras nos locais de monitoramento capturam imagens faciais.As imagens s\u00e3o enviadas para os dispositivos de borda ou inst\u00e2ncias locais.Esses dispositivos transmitem as imagens para o Amazon S3 atrav\u00e9s de uma conex\u00e3o segura (Direct Connect ou VPN).Amazon S3 armazena as imagens e dispara eventos para o AWS Lambda.AWS Lambda processar\u00e1 as imagens e as envia para o Amazon Rekognition.Amazon Rekognition realizar\u00e1 o reconhecimento facial e compara as imagens com o banco de dados no Amazon RDS.AWS Lambda atualizar\u00e1 os resultados no Amazon RDS e fornece os resultados para o Amazon API Gateway.Os dispositivos locais podem consultar os resultados atrav\u00e9s do Amazon API Gateway.AWS DMS sincronizar\u00e1 o Amazon RDS com o banco de dados local, garantindo que os dados estejam sempre atualizados.AWS CloudWatch monitorar\u00e1 todos os servi\u00e7os, garantindo a integridade e a performance do sistema.Conclus\u00e3oE assim, temos um sistema completo de reconhecimento facial utilizando os servi\u00e7os da AWS. Essa arquitetura garante seguran\u00e7a, escalabilidade e efici\u00eancia, atendendo \u00e0s necessidades do cliente para monitoramento em m\u00faltiplos locais."}
{"title": "Differences between primary, core and task nodes in Amazon EMR\u00a0cluster", "published_at": 1717929822, "tags": ["aws", "emr", "cluster", "nodes"], "user": "Nowsath", "url": "https://dev.to/aws-builders/differences-between-primary-core-and-task-nodes-in-amazon-emr-cluster-pc6", "details": "The key differences between primary, core, and task nodes in an Amazon EMR cluster are:Primary Node (also known as Master Node):The primary node is responsible for coordinating the cluster and managing the execution of jobs.It runs the main Hadoop services, such as the JobTracker, NameNode, and ResourceManager.There is only one primary node in an EMR cluster.The primary node cannot be terminated during the lifetime of the cluster, as it is essential for the cluster's operation.Core Nodes:Core nodes host the Hadoop Distributed File System (HDFS) and run the DataNode and TaskTracker services.They are responsible for storing and processing data in the cluster.Core nodes cannot be removed from the cluster without risking data loss, as they contain the persistent data in HDFS.You should reserve core nodes for the capacity that is required until your cluster completes.Task Nodes:Task nodes are used for running tasks and do not host HDFS. They can be added or removed from the cluster as needed, without the risk of data loss.Task nodes are ideal for handling temporary or burst workloads, as you can launch task instance fleets on Spot Instances to increase capacity while minimizing costs.The cluster will never scale below the minimum constraints set in the managed scaling policy.Here's a table summarizing the key differences:More details regarding,Selecting and deploying an Amazon EMR cluster: clickhereEstimating Amazon EMR cluster capacity: clickhere"}
{"title": "How To Manage Amazon Inspector in AWS Organizations Using Terraform", "published_at": 1717916572, "tags": ["aws", "terraform", "security"], "user": "Anthony Wat", "url": "https://dev.to/aws-builders/how-to-manage-amazon-inspector-in-aws-organizations-using-terraform-1ecc", "details": "IntroductionOver the past two months, I have publishednumerous blog posts on managing different AWS security services in AWS Organizations using Terraform. In this blog post, I will cover one remaining AWS service, AWS Inspector, for native vulnerability management. The Terraform resources for Inspector are a bit quirky, so I will show some slightly more advanced techniques to keep the configuration neat and configurable. With that said, let's review the objective.About the use caseAmazon Inspectoris a vulnerability management service that continuously scans AWS workloads for software vulnerabilities and unintended network exposure. Supported compute services include Amazon EC2 instances, container images in Amazon ECR, and AWS Lambda functions.Similar to other AWS security services, Inspector supportsmanaging multiple accounts with AWS Organizationsvia the delegated administrator feature. Once an account in the organization is designated as a delegated administrator, it can manage member accounts and view aggregated findings.Since it is increasingly common to establish an AWS landing zone usingAWS Control Tower, we will use thestandard account structurein a Control Tower landing zone to demonstrate how to configure Inspector in Terraform:The relevant accounts for our use case in the landing zone are:TheManagementaccount for the organization where AWS Organizations is configured. For details, refer toManaging multiple accounts in Amazon Inspector with Organizations.TheAuditaccount where security and compliance services are typically centralized in a Control Tower landing zone.The objective is to delegate Inspector administrative duties from theManagementaccount to theAuditaccount, after which all organization configurations are managed in theAuditaccount. Let's walk through how to do this using Terraform.Designating an Inspector administrator accountThe Inspector delegated administrator is configured in theManagementaccount, so we need a provider associated with it in Terraform. To keep things simple, we will take a multi-provider approach by defining two providers, one for theManagementaccount and another for theAuditaccount, using AWS CLI profiles as follows:provider\"aws\"{alias=\"management\"# Use \"aws configure\" to create the \"management\" profile with the Management account credentialsprofile=\"management\"}provider\"aws\"{alias=\"audit\"# Use \"aws configure\" to create the \"audit\" profile with the Audit account credentialsprofile=\"audit\"}Enter fullscreen modeExit fullscreen mode\u26a0 Since Inspector is a regional service, you must apply this Terraform configuration on each region that you are using. Consider using theregionargument in your provider definition and a variable to make your Terraform configuration rerunnable in other regions.We can designate the delegated administrator using theaws_inspector2_delegated_admin_accountresource. However, this does not enable Inspector in the delegated administrator account, so we also need to use theaws_inspector2_enablerresource. What I learned from testing theaws_inspector2_enablerresource is that you cannot provide both the delegated account and the member accounts in theaccount_idsargument, so we need a dedicatedaws_inspector2_enablerresource for theAuditaccount. According to the resource source code, this is to address a legacy Inspector issue.The resulting Terraform configuration should look like the following (pay special attention to theproviderargument in each resource):data\"aws_caller_identity\"\"audit\"{provider=aws.audit}resource\"aws_inspector2_enabler\"\"audit\"{provider=aws.auditaccount_ids=[data.aws_caller_identity.audit.account_id]}resource\"aws_inspector2_delegated_admin_account\"\"audit\"{provider=aws.managementaccount_id=data.aws_caller_identity.audit.account_iddepends_on=[aws_inspector2_enabler.audit]}Enter fullscreen modeExit fullscreen modeConfiguring Inspector activation for new member accountsTo allow more control over which scan types are enabled, we can define the following variables and use them with the relevant resources:# Variable definition (.tfvars)variable\"enable_ec2\"{description=\"Whether Amazon EC2 scans should be enabled for both existing and new member accounts in the organization.\"type=booldefault=true}variable\"enable_ecr\"{description=\"Whether Amazon ECR scans should be enabled for both existing and new member accounts in the organization.\"type=booldefault=true}variable\"enable_lambda\"{description=\"Whether Lambda Function scans should be enabled for both existing and new member accounts in the organization.\"type=booldefault=true}variable\"enable_lambda_code\"{description=\"Whether Lambda code scans should be enabled for both existing and new member accounts in the organization.\"type=booldefault=true}Enter fullscreen modeExit fullscreen modeIn an organizational setup, Inspector can auto-enable on new member accounts. In Terraform, this can be configured using theaws_inspector2_organization_configurationresource. Leveraging the variables above, the resource can be defined as follows:resource\"aws_inspector2_organization_configuration\"\"this\"{provider=aws.auditauto_enable{ec2=var.enable_ec2ecr=var.enable_ecrlambda=var.enable_lambdalambda_code=var.enable_lambda_code&&var.enable_lambda}depends_on=[aws_inspector2_delegated_admin_account.audit]}Enter fullscreen modeExit fullscreen modeNote that for AWS Lambda code scanning (lambda_code), AWS Lambda standard scanning (lambda) is a prerequisite, so we need to check both variables to enable it.Now let's address the existing member accounts.Activating scanning for existing member accountsUnlike GuardDuty, the Inspector organization configuration does not support auto-enablement for existing member accounts, so we need to separately manage the member accounts. The strategy is to get the list ofactivemember accounts from the organization, which we can use with the Inspector Terraform resources, including theaws_inspector2_enablerresource. We can exclude theAuditaccount since that is managed separately. To get the list of member accounts in the organization, we can use theaws_organizations_organizationdata source.Furthermore, theaws_inspector2_enablerresource'sresource_typesargument takes a list of strings that represent the scan types to enable. Since the variables we defined earlier are boolean variables, we need a bit of function magic to create the list of scans to enable based on the variables.The Terraform configuration that addresses the above requirements can be defined as follows:data\"aws_organizations_organization\"\"this\"{provider=aws.management}locals{enabler_resource_types=compact([var.enable_ec2?\"EC2\":null,var.enable_ecr?\"ECR\":null,var.enable_lambda?\"LAMBDA\":null,var.enable_lambda_code&&var.enable_lambda?\"LAMBDA_CODE\":null,])member_account_ids=[foraccountindata.aws_organizations_organization.this.accounts:account.idifaccount.status==\"ACTIVE\"&&account.id!=data.aws_caller_identity.audit.account_id]}Enter fullscreen modeExit fullscreen modeMember accounts are not automatically associated with the delegated administrator account, so they must first be associated using theaws_inspector2_member_associationresource.Using thefor_eachmeta-argument, we can define a single resource to associate all member accounts with the previously definedmember_account_idslocal value:resource\"aws_inspector2_member_association\"\"members\"{provider=aws.auditfor_each=toset(local.member_account_ids)account_id=each.keydepends_on=[aws_inspector2_delegated_admin_account.audit]}Enter fullscreen modeExit fullscreen modeLastly, we can enable Inspector scans in the member accounts using theaws_inspector2_enablerresource. Although theaccount_idsargument can take the list of member accounts, it is more flexible to have one resource per account. Thus, usingfor_eachand the local values, the resource can be defined as follows:resource\"aws_inspector2_enabler\"\"members\"{provider=aws.auditfor_each=toset(local.member_account_ids)account_ids=[each.key]resource_types=local.enabler_resource_typesdepends_on=[aws_inspector2_member_association.members]}Enter fullscreen modeExit fullscreen mode\u2705 You can find the complete Terraform in theGitHub repositorythat accompanies this blog post.Now that the Terraform configuration is fully defined, you can apply it to establish theAuditaccount as the delegated administration and centrally manage Inspector settings for both new and existing accounts.Caveats about deactivating Inspector in member accountsAmong the AWS security services, Inspector has the least sophisticated API for organizational management. The mix between auto-enablement for new member accounts and explicit enablement for existing member accounts complicates how they are managed in Terraform, particularly if you are trying to disable Inspector viaterraform destroy.Consider the case where a new member account is added and auto-enablement is applied to this account. If you runterraform destroyas-is, Terraform is not aware of the new member account, and thus Inspector cannot be deactivated in this account. You must manually deactivate the account in each applied region.Alternatively, you can first runterraform applyso that theaws_inspector2_member_associationandaws_inspector2_enablerresource instances are created, then runterraform destroyto properly clean up. While this method works, you must keep track of when new member accounts are added so that you know when to runterraform applyto reconcile the Terraform resources with the updated organization.In any case, be aware of this caveat and take one of the two approaches if you ever need to clean up Inspector resources.SummaryIn this blog post, you learned how to manage Amazon Inspector in AWS Organizations using Terraform. With a delegated administrator, Inspector can be auto-enabled for new member accounts, while existing member accounts are dynamically associated and configured with the desired scan types. If you have alsoconfigured AWS Security Hub to operate at the organization level, you can manage Inspector findings across accounts and regions, thereby streamlining your security operations.If you are interested in this type of content, be sure to read other posts on theAvangards Blog, where I share tips and deep dives on AWS, Terraform, and beyond. Thank you, and enjoy the rest of your day!"}
{"title": "Next Generation SQL Injection: Github Actions Edition", "published_at": 1717791413, "tags": ["githubactions", "security", "devops"], "user": "Alonso Suarez", "url": "https://dev.to/aws-builders/next-generation-sql-injection-github-actions-edition-36a4", "details": "In the evolving landscape of software development, Continuous Integration and Continuous Deployment (CI/CD) tools like GitHub Actions have become indispensable. However, with great power comes great responsibility, and it's crucial to be aware of potential security pitfalls. One such vulnerability is treating untrusted inputs, such as branch names or pull request (PR) titles, as static parameters.Spoiler alert,they should NOT be trustedI enjoy puzzles, so here goes one:steps:   - name: Generate summary     run: |       echo \"Pull Request for [${{ github.event.pull_request.title }}](https://github.com/${{ github.repository }}/pull/${{ github.event.pull_request.number }}) has been updated \ud83c\udf89\" >> $GITHUB_STEP_SUMMARY       echo \"Image tagged **v${{ needs.determine_app_version.outputs.app_version }}** has been built and pushed to the registry.\" >> $GITHUB_STEP_SUMMARY This will generate a workflow summary like:Enter fullscreen modeExit fullscreen modeCan you spot the vulnerability?The ProblemWhen a developer clicks on the Revert PR button in GitHub, the new PR title isRevert \"<OLD TITLE>\". This breaks the Generate summary step.Try again if you haven\u2019t spotted the vulnerabilityThe vulnerability lies in howgithub.event.pull_request.titleinput is handled.GitHub Actions expressions dynamically generate code. Unfortunately, if the PR title contains any special characters or unexpected input, it can cause the script to fail or behave unexpectedly.The ImpactWhen developers click on Revert PR button, the PR title isRevert \"<OLD TITLE>\", the echo command in the Generate summary step generates the bash command:echo \"Pull Request for [Revert \"<OLD TITLE>\"](https://github.com/${{ github.repository }}/pull/${{ github.event.pull_request.number }}) has been updated \ud83c\udf89\" >> $GITHUB_STEP_SUMMARYEnter fullscreen modeExit fullscreen modeThe presence of the double quotes within the PR title closes the echo early. However, this allows for bad actors to craft a title that could be used take full control of the runnerThe SolutionTo prevent this issue, one approach is to use env as an input \u201cencoder\u201d and avoid directly using GitHub Actions${{ }}expressions. For example:env:   TITLE: ${{ github.event.pull_request.title }} steps:   - name: Generate summary     run: |       echo \"Pull Request for [$TITLE](https://github.com/${{ github.repository }}/pull/${{ github.event.pull_request.number }}) has been updated \ud83c\udf89\" >> $GITHUB_STEP_SUMMARYEnter fullscreen modeExit fullscreen modeBy using theenvcontext to store untrusted inputs, you avoid potential script-breaking issues caused by special characters pre-rendered by Github Actions.Leave a comment if you can think on a different way to solve this problem!"}
{"title": "AWS Step Functions\u306b\u5165\u9580\u3059\u308b", "published_at": 1717754188, "tags": ["aws", "stepfunctions", "japanese"], "user": "Yasuhiro Matsuda", "url": "https://dev.to/aws-builders/aws-step-functionsniru-men-suru-2b7p", "details": "AWS Step Functions\u3092\u5229\u7528\u3057\u305f\u3053\u3068\u306f\u3042\u308a\u307e\u3059\u304b\uff1f\u79c1\u306f\u30b5\u30fc\u30d3\u30b9\u304c\u51fa\u3066\u304b\u3089\u4f7f\u3044\u6240\u304c\u3088\u304f\u308f\u304b\u3063\u3066\u304a\u3089\u305a\u5229\u7528\u3092\u907f\u3051\u3066\u304d\u307e\u3057\u305f\u304c\u3001\u975e\u540c\u671f\u51e6\u7406\u306e\u30b5\u30fc\u30d3\u30b9\u547c\u3073\u51fa\u3057\u3057\u305f\u5f8c\u306b\u5f85\u3061\u53d7\u3051\u3059\u308b\u51e6\u7406\u3092AWS Lambda\u3067\u5b9f\u88c5\u3057\u3088\u3046\u3068\u3057\u305f\u969b\u306b\u82e6\u616e\u3057\u305f\u30b1\u30fc\u30b9\u304c\u3042\u308a\u307e\u3057\u305f\u3002\u305d\u3053\u3067\u3001AWS Certified Solutions Architect - Professional\u306b\u3082\u51fa\u3066\u304f\u308bAWS Step Functions\u3092\u5229\u7528\u3059\u308c\u3070\u5b9f\u73fe\u3067\u304d\u308b\u306e\u3067\u306f\u306a\u3044\u304b\u3068\u601d\u3063\u3066\u5b9f\u88c5\u3057\u3066\u307f\u308b\u3068\u610f\u5916\u306b\u7c21\u5358\u306b\u5b9f\u73fe\u3067\u304d\u308b\u3053\u3068\u304c\u308f\u304b\u3063\u305f\u306e\u3067\u305d\u306e\u7d4c\u9a13\u3092\u307e\u3068\u3081\u3066\u307f\u307e\u3057\u305f\u3002AWS Step Functions\u3068\u306fAWS Step Functions\u306e\u7d39\u4ecb\u30da\u30fc\u30b8\u3088\u308a\u30c7\u30d9\u30ed\u30c3\u30d1\u30fc\u304c AWS \u306e\u30b5\u30fc\u30d3\u30b9\u3092\u5229\u7528\u3057\u3066\u5206\u6563\u578b\u30a2\u30d7\u30ea\u30b1\u30fc\u30b7\u30e7\u30f3\u3092\u69cb\u7bc9\u3057\u3001\u30d7\u30ed\u30bb\u30b9\u3092\u81ea\u52d5\u5316\u3057\u3001\u30de\u30a4\u30af\u30ed\u30b5\u30fc\u30d3\u30b9\u306e\u30aa\u30fc\u30b1\u30b9\u30c8\u30ec\u30fc\u30b7\u30e7\u30f3\u3001\u30c7\u30fc\u30bf\u3068\u6a5f\u68b0\u5b66\u7fd2\u306e\u30d1\u30a4\u30d7\u30e9\u30a4\u30f3\u3092\u69cb\u7bc9\u3067\u304d\u308b\u3088\u3046\u306b\u3059\u308b\u30d3\u30b8\u30e5\u30a2\u30eb\u30ef\u30fc\u30af\u30d5\u30ed\u30fc\u30b5\u30fc\u30d3\u30b9\u3067\u3059\u3002\u5b9f\u88c5\u3057\u3066\u307f\u305f\u4eca\u56de\u5b9f\u88c5\u3057\u305f\u30b9\u30c6\u30fc\u30c8\u30de\u30b7\u30f3\u56f3\u306f\u4ee5\u4e0b\u306e\u901a\u308a\u3067\u3059\u3002EFS\u3092\u30de\u30a6\u30f3\u30c8\u3057\u3066\u3044\u308b\u8907\u6570\u53f0\u306eEC2\u3067\u69cb\u6210\u3057\u3066\u3044\u308bWeb\u30b5\u30fc\u30d0\u306b\u5bfe\u3057\u3066\u3001\u8ee2\u9001\u5143\u306eS3\u306e\u30d0\u30b1\u30c3\u30c8\u3088\u308a\u30c7\u30fc\u30bf\u3092\u8ee2\u9001\u3059\u308b\u4ed5\u7d44\u307f\u3092DataSync\u3092\u4f7f\u3063\u3066\u5b9f\u73fe\u3057\u3066\u304a\u308a\u3001\u30c7\u30fc\u30bf\u306e\u8ee2\u9001\u3092\u5b8c\u4e86\u3059\u308b\u3068CloudFront\u306e\u30ad\u30e3\u30c3\u30b7\u30e5\u3092\u524a\u9664\u3059\u308b\u3068\u3068\u3082\u306b\u5b8c\u4e86\u901a\u77e5\u3092SNS\u3092\u4f7f\u3063\u3066\u901a\u77e5\u3059\u308b\u3068\u3044\u3046\u3082\u306e\u3067\u3059\u3002\u306a\u304a\u3001\u3053\u306e\u30b9\u30c6\u30fc\u30c8\u30de\u30b7\u30f3\u306f\u3001EventBridge\u30eb\u30fc\u30eb\u306b\u3066S3\u306e\u30d0\u30b1\u30c3\u30c8\u306b\u30c8\u30ea\u30ac\u30fc\u30d5\u30a1\u30a4\u30eb\u3092\u914d\u7f6e\u3059\u308b\u3053\u3068\u306b\u3088\u308a\u8d77\u52d5\u3055\u308c\u308b\u3088\u3046\u306b\u306a\u3063\u3066\u3044\u307e\u3059\u3002\u5b9f\u884c\u7d50\u679c\u306f\u4ee5\u4e0b\u306e\u901a\u308a\u3067\u3001\u72b6\u614b\u9077\u79fb\u6570\u306f50\u3067\u3057\u305f\u30021\u30f5\u6708\u3042\u305f\u308a4,000\u56de\u306e\u72b6\u614b\u9077\u79fb\u307e\u3067\u7121\u6599\u306a\u306e\u3067\u300180\u56de\u5b9f\u884c\u3057\u3066\u3082\u7121\u6599\u306e\u8a08\u7b97\u306b\u306a\u308a\u307e\u3059\u3002(AWS Lambda\u3067\u306f\u306a\u304f)AWS Step Functions\u3067\u5b9f\u88c5\u3059\u308b\u30e1\u30ea\u30c3\u30c8\u4eca\u56de\u306e\u5b9f\u88c5\u3092\u901a\u3057\u3066AWS Step Functions\u3067\u5b9f\u88c5\u3059\u308b\u30e1\u30ea\u30c3\u30c8\u3092\u6d17\u3044\u51fa\u3057\u3057\u3066\u307f\u307e\u3057\u305f\u3002AWS Lambda\u3092\u4f7f\u3063\u3066\u4ed6\u306eAWS\u30b5\u30fc\u30d3\u30b9\u3092\u547c\u3073\u51fa\u3059\u3053\u3068\u304c\u591a\u3044\u3068\u601d\u3044\u307e\u3059\u304c\u3001\uff08\u30d0\u30c3\u30c1\u5b9f\u884c\u7684\u306a\u51e6\u7406\u306e\u5834\u5408\u306b\u306f\u7279\u306b\uff09\u307e\u305a\u306fAWS Step Functions\u3067\u547c\u3073\u51fa\u3057\u3067\u304d\u306a\u3044\u304b\u3092\u5148\u306b\u691c\u8a0e\u3057\u305f\u65b9\u304c\u826f\u3044\u3068\u8003\u3048\u307e\u3059\u3002\u307e\u305f\u3001\u5b9f\u884c\u7d50\u679c\u3092\u898b\u308b\u3068\u3001\u79c1\u304c\u904e\u53bb\u30aa\u30f3\u30d7\u30ec\u30df\u30b9\u74b0\u5883\u3067\u5229\u7528\u3057\u3066\u3044\u305f System Walker\u3084Hinemos\u306a\u3069\u904b\u7528\u7ba1\u7406\u30bd\u30d5\u30c8\u30a6\u30a8\u30a2(JP1\u3082\u6709\u540d\u3067\u3059\u306d)\u3092\u5f77\u5f7f\u3068\u3055\u305b\u3001Amazon EventBridge\u3068\u9023\u643a\u3055\u305b\u308b\u3053\u3068\u3067\u3001\u904b\u7528\u7ba1\u7406\u306e\u89b3\u70b9\u3067\u3082\u5229\u7528\u3057\u3084\u3059\u3044\u4ed5\u7d44\u307f\u3060\u3068\u611f\u3058\u3066\u3044\u307e\u3059\u3002\uff08\u30b5\u30fc\u30d3\u30b9\u306e\u547c\u3073\u51fa\u3057\u3060\u3051\u3092\u884c\u3063\u3066\u3044\u308b\u3088\u3046\u306a\u5b9f\u88c5\u3067\u3042\u308c\u3070\uff09AWS Lambda\u306b\u3042\u308b\u30e9\u30f3\u30bf\u30a4\u30e0\u30b5\u30dd\u30fc\u30c8\u306b\u5408\u308f\u305b\u3066\u30ea\u30d5\u30a1\u30af\u30bf\u30ea\u30f3\u30b0\u305b\u305a\u306b\u6e08\u3080\u3088\u3046\u306b\u306a\u308b\u30b9\u30ed\u30c3\u30c8\u30ea\u30f3\u30b0\u306e\u5f71\u97ff\u3092\u8efd\u6e1b\u3067\u304d\u308bLambda\u306b\u3042\u308b15\u5206\u306e\u5b9f\u884c\u6642\u9593\u306e\u5236\u7d04\u3092\u53d7\u3051\u306a\u304f\u306a\u308b\uff08\u500b\u5225\u306b\u547c\u3073\u51fa\u3057\u304c\u5fc5\u8981\u306a\u7b87\u6240\u3060\u3051AWS Step Functions\u3067\u547c\u3073\u51fa\u3059\u3053\u3068\u306b\u3088\u308a\u3001\u547c\u3073\u51fa\u3057\u5143\u306e\u6642\u9593\u5236\u9650\u3092\u53d7\u3051\u305a\u306b\u6e08\u3080\uff09\u5b9f\u884c\u72b6\u6cc1\u304c\u308f\u304b\u308a\u3084\u3059\u3044\u4e26\u5217\u51e6\u7406\u306a\u3069\u304c\u3084\u308a\u3084\u3059\u304f\u306a\u308b\u30c6\u30b9\u30c8\u72b6\u614b\u3068\u3044\u3046\u6a5f\u80fd\u3092\u4f7f\u3063\u3066\u5404\u30b9\u30c6\u30c3\u30d7\u3054\u3068\u306e\u5358\u4f53\u30c6\u30b9\u30c8\u304c\u5bb9\u6613"}
{"title": "AWS SnapStart - Part 22 Measuring cold and warm starts with Java 17 using synchronous HTTP clients", "published_at": 1717685025, "tags": ["aws", "java", "serverless", "coldstart"], "user": "Vadym Kazulkin", "url": "https://dev.to/aws-builders/aws-snapstart-part-22-measuring-cold-and-warm-starts-with-java-17-using-synchronous-http-clients-2k0l", "details": "IntroductionIn the previous parts we've done many measurements with AWS Lambda using Java 17 runtime with and without using AWS SnapStart and additionally using SnapStart and priming DynamoDB invocation :cold starts usingdifferent deployment artifact sizescold starts and deployment time usingdifferent Lambda memory settingswarm startsusing different Lambda memory settingscold and  warm startsusing different compilation optionsIn this article we'll now add another dimension to our Java 17 measurements : the choice of HTTP Client implementation.  Starting from AWS SDK for Java version 2.22 AWS added support for their own implementation of thesynchronous CRT HTTP Client. The asynchronous CRT HTTP client has been generally available since February 2023. In this article we'll explore synchronous HTTP clients first and leave asynchronous ones for the next article.I will also compare it with the same measurements for Java 21 already performed in the articleMeasuring cold and warm starts with Java 21 using different synchronous HTTP clientsMeasuring cold and warm starts with Java 17 using synchronous HTTP clientsIn our experiment we'll re-use the application introduced inpart 8for this. Here is the code for thesample application. There are basically 2 Lambda functions which both respond to the API Gateway requests and retrieve product by id received from the API Gateway from DynamoDB. One Lambda function GetProductByIdWithPureJava17Lambda can be used with and without SnapStart and the second one GetProductByIdWithPureJava17LambdaAndPriming uses SnapStart and DynamoDB request invocation priming.As we did our measurements for Java 17 in the previous articles of the series, we have always used the default HTTP Client implementation which isApache HTTP Client(we'll use the measurements for the comparison in this article), now we'll explore 2 other options as well.There are now3 synchronousHTTP Clients implementations available in the AWS SDK for Java.Url ConnectionApache (Default)AWS CRTThis is the order for the look up and set of synchronous HTTP Client in the classpath.Let's figure out how to configure the HTTP Client. There are 2 places to do it :pom.xmlandDynamoProductDaoLet's consider 3 scenarios:Scenario 1)Url Connection HTTP Client. Its configuration looks like this:In the pom.xml the only enabled HTTP Client dependency has to be:<dependency>         <groupId>software.amazon.awssdk</groupId>         <artifactId>url-connection-client</artifactId>      </dependency>Enter fullscreen modeExit fullscreen modeIn DynamoProductDao the DynamoDBClient should be created like this:DynamoDbClient.builder()     .region(Region.EU_CENTRAL_1)      .httpClient(UrlConnectionHttpClient.create())     .overrideConfiguration(ClientOverrideConfiguration.builder()       .build())     .build();Enter fullscreen modeExit fullscreen modeScenario 2)Apache HTTP Client. Its configuration looks like this:In the pom.xml the only enabled HTTP Client dependency has to be:<dependency>         <groupId>software.amazon.awssdk</groupId>         <artifactId>apache-client</artifactId>      </dependency>Enter fullscreen modeExit fullscreen modeIn DynamoProductDao the DynamoDBClient should be created like this:DynamoDbClient.builder()     .region(Region.EU_CENTRAL_1)      .httpClient(ApacheHttpClient.create())     .overrideConfiguration(ClientOverrideConfiguration.builder()       .build())     .build();Enter fullscreen modeExit fullscreen modeScenario 3)AWS CRT synchronous HTTP Client. Its configuration looks like this:In the pom.xml the only enabled HTTP Client dependency has to be:<dependency>         <groupId>software.amazon.awssdk</groupId>         <artifactId>aws-crt-client</artifactId>      </dependency>Enter fullscreen modeExit fullscreen modeIn DynamoProductDao the DynamoDBClient should be created like this:DynamoDbClient.builder()     .region(Region.EU_CENTRAL_1)      .httpClient(AwsCrtHttpClient.create())     .overrideConfiguration(ClientOverrideConfiguration.builder()       .build())     .build();Enter fullscreen modeExit fullscreen modeFor the sake of simplicity, we create all HTTP Clients with their default settings. Of course, there is the optimization potential there to figure out the right HTTP Client settings.The results of the experiment below were based on reproducing more than 100 cold and approximately 100.000 warm starts with experiment which ran for approximately 1 hour. For it (and experiments from my previous article) I used the load test toolhey, but you can use whatever tool you want, likeServerless-artilleryorPostman. I ran all these experiments for all 3 scenarios using 2 different compilation options in the AWS SAM template.yaml each:no options (tiered compilation will take place)JAVA_TOOL_OPTIONS: \"-XX:+TieredCompilation -XX:TieredStopAtLevel=1\" (client compilation without profiling)We found out in the articleMeasuring cold and warm starts with Java 17 using different compilation optionsthat with them both we've got the lowest cold and warm start times.  We\u2019ve also got good results with \"-XX:+TieredCompilation -XX:TieredStopAtLevel=2\u201d compilation option but I haven\u2019t done any measurement with this option yet.Let's look into the results of our measurements.Cold (c) and warm (m) start time with compilation option \"tiered compilation\" without SnapStart enabled in ms:Scenario Numberc p50c p75c p90c p99c p99.9c maxw p50w p75w p90w p99w p99.9w maxUrl Connection2615.482672.782726.223660.93817.733993.286.827.578.8022.0150.822172.5Apache2831.332924.852950.123120.343257.033386.675.736.507.8820.4949.621355.08AWS CRT2340.712406.52482.012578.712721.062890.885.736.618.0021.0770.39980.93Cold (c) and warm (m) start time with compilation option \"-XX:+TieredCompilation -XX:TieredStopAtLevel=1\" (client compilation without profiling) without SnapStart enabled in ms:Scenario Numberc p50c p75c p90c p99c p99.9c maxw p50w p75w p90w p99w p99.9w maxUrl Connection2610.592700.552800.533028.363184.083326.097.047.889.3122.5555.041286.36Apache2880.532918.792974.453337.293515.863651.656.117.058.9423.5462.991272.96AWS CRT2268.782314.492341.292461.232613.982754.085.556.307.5720.4975.701010.95Cold (c) and warm (m) start time with compilation option \"tiered compilation\" with SnapStart enabled without Priming in ms:Scenario Numberc p50c p75c p90c p99c p99.9c maxw p50w p75w p90w p99w p99.9w maxUrl Connection1510.721566.071797.682006.602012.632014.236.937.879.3823.92935.811343.25Apache1506.201577.061845.012010.622280.4622815.826.728.3922.81798.461377.54AWS CRT1196.861313.441584.961781.581872.881873.525.556.417.8721.40681.261164.44Cold (c) and warm (m) start time with compilation option \"-XX:+TieredCompilation -XX:TieredStopAtLevel=1\" (client compilation without profiling) with SnapStart enabled without Priming in ms:Scenario Numberc p50c p75c p90c p99c p99.9c maxw p50w p75w p90w p99w p99.9w maxUrl Connection1567.631647.961889.802026.762075.972076.577.108.009.6925.41953.931190.54Apache1521.331578.641918.352113.652115.772117.426.017.058.9423.92101.411077.45AWS CRT11176.701259.451621.821854.251856.111857.595.556.307.6321.40670.53990.96Cold (c) and warm (m) start time with compilation option \"tiered compilation\" with SnapStart enabled and with DynamoDB invocation Priming in ms:Scenario Numberc p50c p75c p90c p99c p99.9c maxw p50w p75w p90w p99w p99.9w maxUrl Connection666.97745.23965.421084.101108.201108.667.218.079.6124.22145.49377.43Apache708.90790.50960.611041.611148.801149.915.646.618.3821.07141.53373.37AWS CRT679.76851.181026.111102.681111.531111.645.926.728.2622.09171.221065.32Cold (c) and warm start (m) time with compilation option \"-XX:+TieredCompilation -XX:TieredStopAtLevel=1\" (client compilation without profiling) with SnapStart enabled and with DynamoDB invocation Priming in ms:Scenario Numberc p50c p75c p90c p99c p99.9c maxw p50w p75w p90w p99w p99.9w maxUrl Connection673.67748.22946.311184.961213.731214.347.168.139.8325.89141.53275.35Apache692.79758.001003.801204.061216.151216.886.217.279.3825.09103.03256.65AWS CRT640.19693.491022.021229.601306.901307.145.646.518.1322.81171.22877.24ConclusionIn terms of the HTTP Client choice for Java 17, AWS CRT HTTP Client is preferred choice in case SnapStart isn't enabled or SnapStart is enabled but no priming is applied. In case of priming of the DynamoDB invocation, the results in terms of the cold starts for all 3 HTTP Clients are close to each other as the initialization of the DynamoDB Client with the HTTP Client and the most expensive first invocation (priming) happens already during the deployment phase of the Lambda function and doesn't impact the further invocations that much. The Apache HTTP Client is probably the most powerful choice, but it shows the worst results for the cold starts for SnapStart not being enabled.We observed the same also for Java 21, see the measurements in the articleMeasuring cold and warm starts with Java 21 using different synchronous HTTP clients.The warm execution times are more or less close to each other for all 3 HTTP clients and compilation options and very a bit in favor of one or another HTTP Clients depending on the percentile and compilation option. We observed the same also for Java 21Can we reduce the cold start a bit further? From our articleMeasuring cold starts with Java 17 using different deployment artifact sizeswe know that smaller deployment artifact sizes lead to the lower cold start times. The usage of AWS CRT HTTP Client adds 18 MB to the deployment artifact size for our sample application (total size 32MB versus 14 MB for URL Connection and Apache HTTP Clients). If we look into the deployment artifact with AWS CRT HTTP Client, we'll discover the following additional packages for each operating system : linux, osx and windows.If we take a look into those folders, we'll see for example the following content for the linux folder (the same applies for windows and osx folders) :As we see the content of such folders is natives file for each operating system and processor architecture: for osx it's libaws-crt-jni.dylib file, for windows - aws-crt-jni.dll and for linux - libaws-crt-jni.so. If we already know that we'll run our Lambda only on Linux x86 architecture, we can delete the osx and windows folders completely and subfolders for arm architecture in the linux folder. This will reduce the deployment artifact size from 32 to 19 MB for AWS CRT HTTP Client and further reduce the cold start time a bit.The choice of HTTP Client is not only about minimizing cold and warm starts. The decision is much more complex end also depends on the functionality of the HTTP Client implementation and its settings, like whether it supports HTTP/2. AWS publshed the decision tree whichHTTP client to choosedepending on the criteria.In the next article of the series we'll make the same measurements for Java 17 but using the asynchronous HTTP Clients.Update on 06.06.2024.  For the CRT client we can set classifier (i.e. linux-x86_64) in our POM file to only pick the relevant binary for our platform. Seehere. Big thanks toMaximilian Schellhornfor the hint!"}
{"title": "AWS Community Builders\u306b\u306a\u3063\u3066\u5909\u308f\u3063\u305f\u3053\u3068", "published_at": 1717665099, "tags": ["awscommunitybuilders", "japanese", "pankration", "jawsug"], "user": "Yasuhiro Matsuda", "url": "https://dev.to/aws-builders/aws-community-buildersninatutebian-watutakoto-pkj", "details": "\u5148\u65e5\u3001\u79c1\u306fAWS Community Builders\u306b3\u5e74\u9023\u7d9a\u3067\u9078\u51fa\u3055\u308c\u307e\u3057\u305f\u3002\u6700\u521d\u306b\u9078\u51fa\u3055\u308c\u305f\u306e\u306f2022\u5e74\u3067\u5f53\u6642\u306f\u65b0\u578b\u30b3\u30ed\u30ca\u30a6\u30a4\u30eb\u30b9\u304c\u8513\u5ef6\u3057\u3066\u3044\u308b\u4e2d\u3067\u3001\u30aa\u30f3\u30e9\u30a4\u30f3\u3067\u306e\u52c9\u5f37\u4f1a\u3092\u4e2d\u5fc3\u306b\u958b\u50ac\u3057\u3066\u3044\u307e\u3057\u305f\u3002\u305d\u306e\u4e2d\u3067LINE Developer Community\u3068\u306e\u5171\u540c\u4f01\u753b\u3092\u884c\u3063\u3066\u3044\u305f\u3053\u3068\u3082\u3042\u308a\u3001Front-End Web & Mobile\u306e\u5206\u91ce\u3067\u9078\u51fa\u3055\u308c\u307e\u3057\u305f\u3002AWS Community Builders\u306b\u306a\u308b\u524d\u306bJAWS Pankration 2021\u3068\u3044\u304624\u6642\u9593\u30a4\u30d9\u30f3\u30c8\u306b\u767b\u58c7\u3057\u307e\u3057\u305f\u3002\u3053\u306e\u30a4\u30d9\u30f3\u30c8\u306f\u4e16\u754c\u4e2d\u306eAWS\u30e6\u30fc\u30b6\u30fc\u30b0\u30eb\u30fc\u30d7\u3092\u5dfb\u304d\u8fbc\u3093\u3060\u30a4\u30d9\u30f3\u30c8\u3067\u3001\u30d5\u30a9\u30ed\u30fc\u30fb\u30b6\u30fb\u30b5\u30f3\u5f62\u5f0f\u3067\u958b\u50ac\u3055\u308c\u305f\u6d77\u5916\u304b\u3089\u3082\u30af\u30ec\u30a4\u30b8\u30fc\u3068\u8a55\u4fa1\u3055\u308c\u305fJAWS-UG\u306e\u30a4\u30d9\u30f3\u30c8\u3067\u3057\u305f\u3002\u305d\u3057\u3066\u3001\u4eca\u56de3\u5e74\u3076\u308a\u306bJAWS Pankration 2024\u3068\u3044\u3046\u5f62\u3067\u958b\u50ac\u3055\u308c\u308b\u3053\u3068\u3068\u306a\u308a\u3001Call for Proposals\u306b\u5fdc\u52df\u3059\u308b\u306b\u3042\u305f\u308a\u3001AWS Community Builders\u306b\u306a\u308b\u524d\u3068\u306a\u3063\u305f\u5f8c\u3067\u3069\u306e\u3088\u3046\u306a\u9055\u3044\u304c\u3042\u3063\u305f\u304b\u306b\u3064\u3044\u3066\u307e\u3068\u3081\u3066\u307f\u307e\u3057\u305f\u3002AWS Community Builders\u3068\u306f\u516c\u5f0f\u30b5\u30a4\u30c8\u3067\u306f\u4ee5\u4e0b\u306e\u3088\u3046\u306b\u7d39\u4ecb\u3055\u308c\u3066\u304a\u308a\u3001AWS Community Directory\u306b\u3066\u9078\u51fa\u3055\u308c\u305f\u65b9\u304c\u691c\u7d22\u3067\u304d\u308b\u3088\u3046\u306b\u306a\u3063\u3066\u3044\u307e\u3059\u30022024/6/6\u73fe\u5728\u3067\u5168\u4e16\u754c\u30672,593\u4eba\uff08\u65e5\u672c\u4eba\u306f120\u4eba\uff09\u304c\u516c\u958b\u3055\u308c\u3066\u3044\u307e\u3059\u3002\u4ee5\u524d\u306f\u534a\u5e74\u306b1\u5ea6\u9078\u51fa\u3055\u308c\u3066\u3044\u307e\u3057\u305f\u304c\u3001\u73fe\u5728\u306f1\u5e74\u306b\uff11\u56de\u306e\u9078\u51fa\u30b5\u30a4\u30af\u30eb\u3068\u306a\u3063\u3066\u304a\u308a\u3001\u3059\u3067\u306b\u9078\u51fa\u3055\u308c\u3066\u3044\u308b\u30d3\u30eb\u30c0\u30fc\u3067\u3082\u6bce\u5e74\u306e\u66f4\u65b0\u306e\u30d5\u30a9\u30fc\u30e0\u306b\u6d3b\u52d5\u3092\u30a2\u30d4\u30fc\u30eb\u3059\u308b\u5fc5\u8981\u304c\u3042\u308a\u3001\u6d3b\u52d5\u304c\u8a55\u4fa1\u3055\u308c\u306a\u3044\u5834\u5408\u306b\u306f\u6b21\u5e74\u5ea6\u306e\u9078\u51fa\u306f\u884c\u308f\u308c\u307e\u305b\u3093\u3002AWS \u30b3\u30df\u30e5\u30cb\u30c6\u30a3\u30d3\u30eb\u30c0\u30fc\u30ba\u30d7\u30ed\u30b0\u30e9\u30e0\u306f\u3001\u77e5\u8b58\u306e\u5171\u6709\u3084\u6280\u8853\u30b3\u30df\u30e5\u30cb\u30c6\u30a3\u3068\u306e\u9023\u643a\u306b\u71b1\u5fc3\u306a AWS \u6280\u8853\u611b\u597d\u5bb6\u3084\u65b0\u8208\u306e\u30bd\u30fc\u30c8\u30ea\u30fc\u30c0\u30fc\u306b\u3001\u6280\u8853\u30ea\u30bd\u30fc\u30b9\u3001\u6559\u80b2\u3001\u30cd\u30c3\u30c8\u30ef\u30fc\u30ad\u30f3\u30b0\u306e\u6a5f\u4f1a\u3092\u63d0\u4f9b\u3059\u308b\u30d7\u30ed\u30b0\u30e9\u30e0\u3067\u3059\u3002\u3053\u306e\u30d7\u30ed\u30b0\u30e9\u30e0\u3067\u306f\u3001AWS \u306e\u5185\u5bb9\u9818\u57df\u5c02\u9580\u5bb6\u304c\u6709\u76ca\u306a\u30a6\u30a7\u30d3\u30ca\u30fc\u3092\u63d0\u4f9b\u3057\u3001\u6700\u65b0\u30b5\u30fc\u30d3\u30b9\u306b\u95a2\u3059\u308b\u60c5\u5831\u3092\u542b\u3081\u3001\u6280\u8853\u30b3\u30f3\u30c6\u30f3\u30c4\u306e\u4f5c\u6210\u3001\u30ea\u30fc\u30c1\u306e\u62e1\u5927\u3001\u30aa\u30f3\u30e9\u30a4\u30f3\u304a\u3088\u3073\u5bfe\u9762\u3067\u306e\u30b3\u30df\u30e5\u30cb\u30c6\u30a3\u306b\u304a\u3051\u308b AWS \u77e5\u8b58\u306e\u5171\u6709\u306b\u95a2\u3059\u308b\u30d9\u30b9\u30c8\u30d7\u30e9\u30af\u30c6\u30a3\u30b9\u306a\u3069\u3092\u5171\u6709\u3057\u307e\u3059\u3002\u3053\u306e\u30d7\u30ed\u30b0\u30e9\u30e0\u3067\u306f\u3001\u5e74\u9593\u9650\u5b9a\u6570\u306e\u30e1\u30f3\u30d0\u30fc\u3092\u53d7\u3051\u5165\u308c\u307e\u3059\u3002AWS \u30d3\u30eb\u30c0\u30fc\u306e\u7686\u69d8\u306f\u3001\u305c\u3072\u3054\u5fdc\u52df\u304f\u3060\u3055\u3044\u3002\u516c\u5f0f\u30b5\u30a4\u30c8\u306b\u306fAWS Community Builders\u306b\u306a\u308b\u3053\u3068\u306b\u3088\u308b\u30e1\u30ea\u30c3\u30c8\u304c\u4ee5\u4e0b\u306e\u901a\u308a\u8a18\u8f09\u3055\u308c\u3066\u3044\u307e\u3059\u3002\u30d7\u30ed\u30b0\u30e9\u30e0\u30e1\u30f3\u30d0\u30fc\u306f\u4ee5\u4e0b\u3092\u53d7\u3051\u53d6\u308b\u3053\u3068\u304c\u3067\u304d\u307e\u3059:AWS \u30d7\u30ed\u30c0\u30af\u30c8\u30c1\u30fc\u30e0\u3078\u306e\u30a2\u30af\u30bb\u30b9\u3001\u9031 1 \u56de\u306e\u30a6\u30a7\u30d3\u30ca\u30fc\u306b\u3088\u308b\u65b0\u30b5\u30fc\u30d3\u30b9\u3084\u65b0\u6a5f\u80fd\u306e\u60c5\u5831\u3092\u53d7\u3051\u53d6\u308b\u3053\u3068\u304c\u3067\u304d\u307e\u3059\u3002AWS \u306e\u5185\u5bb9\u9818\u57df\u5c02\u9580\u5bb6\u304b\u3089\u3001\u6280\u8853\u4ee5\u5916\u306e\u69d8\u3005\u306a\u30c8\u30d4\u30c3\u30af\u306b\u3064\u3044\u3066\u5b66\u3073\u3001\u30b3\u30f3\u30c6\u30f3\u30c4\u4f5c\u6210\u3001CFP \u306e\u63d0\u51fa\u3084\u8b1b\u6f14\u3092\u3059\u308b\u6a5f\u4f1a\u3092\u78ba\u4fdd\u3059\u308b\u30b5\u30dd\u30fc\u30c8\u3092\u53d7\u3051\u308b\u3053\u3068\u304c\u3067\u304d\u307e\u3059\u3002AWS \u30d7\u30ed\u30e2\u30fc\u30b7\u30e7\u30f3\u30af\u30ec\u30b8\u30c3\u30c8\u3001\u30b3\u30f3\u30c6\u30f3\u30c4\u5236\u4f5c\u3084\u5730\u57df\u5bc6\u7740\u578b\u306e\u6d3b\u52d5\u3092\u652f\u63f4\u3059\u308b\u4ed6\u306e\u6709\u7528\u306a\u30ea\u30bd\u30fc\u30b9\u3092\u5f97\u308b\u3053\u3068\u304c\u3067\u304d\u307e\u3059\u3002\u30b5\u30d7\u30e9\u30a4\u30ba\u3067\u3059!\u306a\u305cAWS Community Builders\u306b\u7533\u8acb\u3057\u3088\u3046\u3068\u3057\u305f\u306e\u304b\u79c1\u306fJAWS-UG\u91d1\u6ca2\u652f\u90e8\u306e\u30b3\u30a2\u30e1\u30f3\u30d0\u30fc\u3092\u3057\u3066\u3044\u307e\u3059\u3002AWS Community Day Kanazawa\u3084JAWS DAYS 2021\u306e\u5b9f\u884c\u59d4\u54e1\u306a\u3069\u3092\u884c\u3046\u4e2d\u3067\u3001AWS\u306e\u30b3\u30df\u30e5\u30cb\u30c6\u30a3\u62c5\u5f53\u306e\u793e\u54e1\u306e\u65b9\u304b\u3089\u3082\u304a\u58f0\u304c\u3051\u3092\u3044\u305f\u3060\u304f\u3068\u3068\u3082\u306b\u3001\u3059\u3067\u306bAWS Community Builders\u306b\u306a\u3089\u308c\u3066\u3044\u305fMasayuki Kato\u3055\u3093\u3084Michael Tedder\u3055\u3093\u306e\u5f71\u97ff\u3092\u53d7\u3051\u3066\u7533\u8acb\u3059\u308b\u3053\u3068\u306b\u6c7a\u3081\u307e\u3057\u305f\u3002AWS Community Builders\u306f\u7533\u8acb\u5236\u5ea6\u306a\u306e\u3067\u3001\u81ea\u5206\u3067\u6d3b\u52d5\u3092\u30d5\u30a9\u30fc\u30e0\u3092\u901a\u3058\u3066\u82f1\u8a9e\u3067\u5165\u529b\u3059\u308b\u5fc5\u8981\u304c\u3042\u308a\u30012021\u5e74\u5ea6\u306f\u9078\u51fa\u3055\u308c\u307e\u305b\u3093\u3067\u3057\u305f\u3002\u6d3b\u52d5\u5185\u5bb9\u3092\u7b2c\u4e09\u8005\u304c\u7406\u89e3\u3067\u304d\u308b\u3088\u3046\u306b\u30a2\u30d4\u30fc\u30eb\u3059\u308b\u3053\u3068\u304c\u3067\u304d\u306a\u304b\u3063\u305f\u3053\u3068\u304c\u539f\u56e0\u3067\u3057\u305f\u3002\u8ae6\u3081\u305a\u306b\u534a\u5e74\u9593\u6d3b\u52d5\u3092\u7d9a\u3051\u3001\u30a2\u30d4\u30fc\u30eb\u3059\u308b\u65b9\u6cd5\u3082\u898b\u76f4\u3059\u3053\u3068\u3067\u7121\u4e8b2022\u5e74\u306b\u9078\u51fa\u3055\u308c\u307e\u3057\u305f\u3002AWS Community Builders\u306b\u9078\u51fa\u3055\u308c\u305f\u5909\u5316\u306b\u3064\u3044\u3066\u79c1\u306e\u4e2d\u3067\u5b9f\u611f\u3057\u305f\u3053\u3068\u306f\u4ee5\u4e0b\u306e\uff14\u70b9\u3067\u3059\u3002\u6700\u65b0\u60c5\u5831\u306b\u89e6\u308c\u308b\u3053\u3068\u304c\u3067\u304d\u308bAWS Community Builders\u5411\u3051\u306e\u30df\u30fc\u30c6\u30a3\u30f3\u30b0\u306a\u3069\u304c\u30aa\u30f3\u30e9\u30a4\u30f3\u3067\u958b\u50ac\u3055\u308c\u3066\u304a\u308a\u3001\u305d\u306e\u4e2d\u3067\u6700\u65b0\u306e\u30b5\u30fc\u30d3\u30b9\u60c5\u5831\u306b\u89e6\u308c\u308b\u3053\u3068\u304c\u3067\u304d\u307e\u3059\u3002\u30df\u30fc\u30c6\u30a3\u30f3\u30b0\u306f\u30bf\u30a4\u30e0\u30be\u30fc\u30f3\u306e\u90fd\u5408\u4e0a\u3001\uff12\u56de\u306b\u5206\u3051\u3066\u958b\u50ac\u3055\u308c\u3066\u3044\u308b\u306e\u3067\u3059\u304c\u3001\u65e5\u672c\u6642\u9593\u3067\u306f\u5348\u524d1\u6642\u3068\u5348\u524d\uff17\u6642\u306b\u884c\u308f\u308c\u308b\u3053\u3068\u304c\u591a\u304f\u3001APAC(\u30a2\u30b8\u30a2\u592a\u5e73\u6d0b\u5730\u57df)\u306eAWS Community Builders\u306b\u3068\u3063\u3066\u306f\u30ea\u30a2\u30eb\u53c2\u52a0\u3059\u308b\u306e\u306f\u306a\u304b\u306a\u304b\u5927\u5909\u3067\u3059\u3002\u305f\u3060\u3001\u30ea\u30a2\u30eb\u53c2\u52a0\u3092\u901a\u3058\u3066\u30c1\u30e3\u30c3\u30c8\u306e\u76db\u308a\u4e0a\u304c\u308a\u306a\u3069\u3092\u898b\u308b\u3068\u3068\u3066\u3082\u523a\u6fc0\u306b\u306a\u308a\u307e\u3059\u3002\u82f1\u8a9e\u3078\u306e\u62b5\u6297\u611f\u304c\u591a\u5c11\u548c\u3089\u3050You Belong Here\u306e\u52d5\u753b\u306b\u3042\u308b\u901a\u308a\u3001\u79c1\u306f\u82f1\u8a9e\u304c\u307b\u3068\u3093\u3069\u3067\u304d\u307e\u305b\u3093\u3002\u305d\u306e\u305f\u3081\u3001\u30e9\u30b9\u30d9\u30ac\u30b9\u3067\u958b\u50ac\u3055\u308c\u3066\u3044\u308bre:Invent\u306b\uff12\u56de\u53c2\u52a0\u3057\u307e\u3057\u305f\u304c\u3001\u3044\u305a\u308c\u3082\u30db\u30c6\u30eb\u3067\u30c8\u30e9\u30d6\u30eb\u306b\u5dfb\u304d\u8fbc\u307e\u308c\u3066\u82f1\u8a9e\u304c\u8a71\u305b\u306a\u3044\u3053\u3068\u306b\u3088\u3063\u3066\u304b\u306a\u308a\u82e6\u52b4\u3057\u305f\u7d4c\u9a13\u3092\u3057\u3066\u3044\u307e\u3059\u3002AWS Community Builders\u306e\u304a\u77e5\u3089\u305b\u306f\u5c02\u7528\u306eSlack\u30ef\u30fc\u30af\u30b9\u30da\u30fc\u30b9\u306b\u3066\u5168\u3066\u82f1\u8a9e\u3067\u3084\u308a\u53d6\u308a\u3055\u308c\u3066\u3044\u308b\u305f\u3081\u3001\u60c5\u5831\u3092\u7406\u89e3\u3059\u308b\u305f\u3081\u306b\u82f1\u8a9e\u306b\u89e6\u308c\u3056\u308b\u3092\u5f97\u306a\u3044\u74b0\u5883\u306b\u7f6e\u304b\u308c\u307e\u3059\u3002\u305f\u3060\u3001\u5e78\u3044\u306a\u3053\u3068\u306blang-japanese\u30c1\u30e3\u30cd\u30eb\u3060\u3051\u306f\u65e5\u672c\u8a9e\u3067\u3084\u308a\u53d6\u308a\u3055\u308c\u3066\u3044\u308b\u306e\u3067\u3001\u7d30\u304b\u3044\u7406\u89e3\u304c\u3067\u304d\u306a\u3044\u5834\u5408\u306b\u306f\u65e5\u672c\u306eAWS Community Builders\u306e\u52a9\u3051\u3092\u501f\u308a\u308b\u3053\u3068\u3082\u3067\u304d\u307e\u3059\u3002\u65e5\u9803\u82f1\u8a9e\u3067\u4f1a\u8a71\u3057\u305f\u308a\u3001\u6587\u7ae0\u3092\u66f8\u3044\u305f\u308a\u3059\u308b\u3053\u3068\u306f\u306a\u3044\u305f\u3081\u3053\u306e\u3088\u3046\u306a\u6a5f\u4f1a\u306b\u63a5\u3059\u308b\u3053\u3068\u306f\u62b5\u6297\u611f\u304c\u5c11\u3057\u3067\u3082\u548c\u3089\u3050\u3053\u3068\u306b\u3064\u306a\u304c\u308a\u307e\u3059\u3002AWS re:Invent 2023\u306e\u4f53\u9a135\u5e74\u3076\u308a\u306b\u81ea\u8cbb\u3067\u884c\u3063\u305fre:Invent\u4f53\u9a13\u8a18\u3067\u8a18\u8f09\u3057\u3066\u3044\u307e\u3059\u304c\u3001\u79c1\u306fAWS re:Invent 2023\u3067AWS Community Builders\u306b\u306a\u308b\u524d\u306b\u884c\u3063\u305fAWS re:Invent 2018\u3068\u306e\u9055\u3044\u3092\u5b9f\u611f\u3057\u307e\u3057\u305f\u3002AWS\u3078\u306e\u7406\u89e3\u304c\u5e83\u304c\u3063\u305f\u308a\u3001\u30b3\u30df\u30e5\u30cb\u30c6\u30a3\u3092\u901a\u3058\u3066\u95a2\u308f\u308b\u30e1\u30f3\u30d0\u30fc\u304c\u5897\u3048\u305f\u304b\u3089\u3068\u3044\u3046\u3053\u3068\u3082\u3042\u308a\u307e\u3059\u304c\u3001AWS Community Builders\u306b\u306a\u3063\u3066\u3044\u306a\u3051\u308c\u3070\u63a5\u3059\u308b\u3053\u3068\u306e\u306a\u304b\u3063\u305f\u4eba\u3068\u30cd\u30c3\u30c8\u30ef\u30fc\u30ad\u30f3\u30b0\u3059\u308b\u3053\u3068\u304c\u3067\u304d\u307e\u3057\u305f\u3002\u4eba\u3068\u4eba\u304c\u7e4b\u304c\u308b\u3053\u3068\u306b\u3088\u3063\u3066\u5165\u3063\u3066\u304f\u308b\u60c5\u5831\u3082\u304b\u306a\u308a\u5909\u308f\u308b\u3053\u3068\u306b\u306a\u308a\u307e\u3059\u3002\u79c1\u306f\u6280\u8853\u7684\u306a\u9818\u57df\u3067\u306e\u5f37\u307f\u3092\u6301\u3063\u3066\u3044\u308b\u308f\u3051\u3067\u306f\u306a\u3044\u305f\u3081\u3001\u65b0\u305f\u306b\u7e4b\u304c\u3063\u305f\u65b9\u304b\u3089\u5165\u3063\u3066\u304f\u308b\u60c5\u5831\u3084\u8003\u3048\u65b9\u306f\u65e5\u9803\u89e6\u308c\u308b\u3053\u3068\u304c\u3067\u304d\u306a\u3044\u3082\u306e\u304c\u591a\u304f\u3001\u81ea\u5206\u3092\u5909\u5316\u3055\u305b\u3066\u304f\u308c\u308b\u52d5\u6a5f\u3065\u3051\u306b\u3064\u306a\u304c\u308a\u307e\u3059\u3002AWS Community Builders\u3068\u306e\u4ea4\u6d41\u6d77\u5916\u3068\u306e\u7e4b\u304c\u308a\u304cAWS Community Builder\u306e\u918d\u9190\u5473\u3067\u3059\u304c\u3001\u56fd\u5185\u306e\u30e1\u30f3\u30d0\u30fc\u3068\u306e\u7e4b\u304c\u308a\u306b\u3064\u3044\u3066\u306f\u56fd\u5185\u306e\u30a4\u30d9\u30f3\u30c8\u3092\u901a\u3058\u3066\u7a4d\u6975\u7684\u306b\u4ea4\u6d41\u3092\u6301\u3064\u3088\u3046\u306b\u3057\u3066\u3044\u307e\u3059\u3002\u6628\u5e74\u306e\u53d6\u308a\u7d44\u307f\u3067\u306f\u3001AWS Summit\u3000Tokyo 2023\u3084JAWS Festa 2023\u3067\u306eAWS Community Builders\u5c02\u7528\u306e\u4ea4\u6d41\u30a4\u30d9\u30f3\u30c8\u3000AWS Community Builder Meetup\u3000\u3092\u4f01\u753b\u3057\u307e\u3057\u305f\u3002\u305d\u308c\u305e\u308c\u6280\u8853\u7684\u306a\u5f37\u307f\u3092\u6301\u3064\u3068\u3044\u3046\u3068\u3053\u308d\u3082\u3042\u308a\u307e\u3059\u304c\u3001\u30b3\u30df\u30e5\u30cb\u30c6\u30a3\u3092\u652f\u3048\u3066\u3044\u308b\u4eba\u305f\u3061\u3060\u304b\u3089\u3053\u305d\u51fa\u3066\u304f\u308b\u8a00\u8449\u306b\u5b66\u3073\u304c\u591a\u3044\u3053\u3068\u304c\u4e8b\u5b9f\u3067\u3059\u3002\u610f\u8b58\u306e\u5909\u9769JAWS-UG\u306e\u6d3b\u52d5\u3060\u3051\u3067\u306a\u304f\u3001\u79c1\u306f\u3044\u304f\u3064\u304b\u306e\u30b3\u30df\u30e5\u30cb\u30c6\u30a3\u6d3b\u52d5\u3092\u884c\u3063\u3066\u3044\u307e\u3059\u304c\u3001\u305d\u3093\u306a\u79c1\u304c\u30b3\u30df\u30e5\u30cb\u30c6\u30a3\u6d3b\u52d5\u3092\u3084\u3063\u3066\u3044\u3066\u3001\u300c\u306a\u305c\u81ea\u5206\u306e\u6642\u9593\u3068\u52b4\u529b\u3092\u6295\u5165\u3057\u3066\u6d3b\u52d5\u3092\u884c\u3046\u306e\u304b\u300d\u3068\u3044\u3046\u5927\u304d\u306a\u60a9\u307f\u4e8b\u304c\u3042\u308a\u307e\u3059\u3002\u3053\u306e\u601d\u3044\u306f\u958b\u50ac\u3059\u308b\u30a4\u30d9\u30f3\u30c8\u304c\u5927\u304d\u304f\u306a\u308c\u3070\u306a\u308b\u307b\u3069\u3001\u305d\u3057\u3066\u4e2d\u5fc3\u7684\u306a\u30e1\u30f3\u30d0\u30fc\u304c\u5c11\u3051\u308c\u3070\u5c11\u306a\u3044\u307b\u3069\u3001\u3053\u306e\u601d\u3044\u306f\u5927\u304d\u304f\u306a\u308b\u3088\u3046\u306b\u611f\u3058\u3066\u3044\u307e\u3059\u3002\u71b1\u91cf\u306e\u9ad8\u3044\u4eba\u3001\u697d\u3057\u304f\u3057\u3066\u3044\u308b\u3068\u3053\u308d\u306b\u306f\u4eba\u306f\u96c6\u307e\u308b\u3068\u3088\u304f\u8a00\u308f\u308c\u307e\u3059\u304c\u3001AWS Community Bilders\u306e\u3088\u3046\u306b\u6c42\u5fc3\u529b\u304c\u9ad8\u3044\u65b9\u3068\u4e00\u7dd2\u306b\u30b3\u30df\u30e5\u30cb\u30c6\u30a3\u306e\u6d3b\u52d5\u3092\u884c\u3046\u3053\u3068\u306b\u3088\u3063\u3066\u3001\u300c\u306a\u305c\u81ea\u5206\u306e\u6642\u9593\u3068\u52b4\u529b\u3092\u6295\u5165\u3057\u3066\u6d3b\u52d5\u3092\u884c\u3046\u306e\u304b\u300d\u306e\u7b54\u3048\u304c\u898b\u51fa\u305b\u307e\u3057\u305f\u3002\u307e\u3068\u3081AWS\u306e\u6280\u8853\u3092\u65e5\u9803\u304b\u3089\u5229\u7528\u3055\u308c\u3066\u3044\u308b\u65b9\u3067\u30a2\u30a6\u30c8\u30d7\u30c3\u30c8\u6d3b\u52d5\u304c\u597d\u304d\u306a\u65b9\u3067\u3042\u308c\u3070\u3001\u4e00\u6b69\u524d\u306b\u8e0f\u307f\u51fa\u3059\u3060\u3051\u3067\u4e16\u754c\u306f\u5927\u304d\u304f\u5909\u308f\u308a\u307e\u3059\u3002\u3054\u89a7\u3044\u305f\u3060\u3044\u305f\u5185\u5bb9\u3092\u901a\u3058\u3066\u3084\u3063\u3066\u307f\u3088\u3046\u3068\u601d\u308f\u308c\u305f\u65b9\u306f\u3001\u305c\u3072JAWS Pankration 2024\u306b\u5fdc\u52df\u3057\u3066\u307f\u3066\u304f\u3060\u3055\u3044\u3002"}
{"title": "Como a AWS Cloud Control API e o novo Terraform provider impactam sua vida?", "published_at": 1717646170, "tags": [], "user": "Marcelo Andrade", "url": "https://dev.to/aws-builders/como-a-aws-cloud-control-api-e-o-novo-terraform-provider-impactam-sua-vida-3jg8", "details": "Recentemente, a Hashicorp anunciou que o provider AWS Cloud Control agora \u00e9 consider\u00e1vel pronto para uso em produ\u00e7\u00e3o (tamb\u00e9m conhecido como \"Generally Available\", ouGA):A not\u00edcia \u00e9 bem explicativa, mas percebi bastante confus\u00e3o dentre meus conhecidos a respeito sobre o assunto, ent\u00e3o vamos falar sobre! O que \u00e9 AWS Cloud Control, como funciona, o que faz este novo provider e qual o impacto na sua vida!Terraform Provider AWSSe voc\u00ea usa Terraform (ou OpenTofu) e AWS, ent\u00e3o provavelmente voc\u00ea usa oTerraform Provider AWSpara provisionar sua infraestrutura.Dispon\u00edvel desde oTerraform 0.1(2014!), este possivelmente \u00e9 o provider mais antigo da  solu\u00e7\u00e3o e de longe o mais conhecido e usado.O que a not\u00edcia compartilhada tem a ver com este provider?Nada!Este provider usa a mesma API que aAWS SDKe oAWS CLI, por exemplo.Por falar em AWS CLI...AWS API e suas inconsist\u00eanciasSe voc\u00ea j\u00e1 usou aAWS CLI, provavelmente j\u00e1 percebeu algumas \"particularidades\".As APIs da AWS foram constru\u00eddas ao longo de quase 20 anos; servi\u00e7os de \u00e9pocas diferentes certamente foram desenvolvidos por equipes diferentes a partir de modelos e ideias diferentes. Al\u00e9m disso, servi\u00e7os antigos s\u00e3o extendidos possivelmente por outras equipes que implementam recursos do seu jeito.Combinado ao tamanho e variedade de ofertas, essa caracter\u00edstica torna torna a APIextremamente inconsistenteem seu comportamento. Vamos a um exemplo bem simpl\u00f3rio mas que ilustra adequadamente o que queremos dizer.Para listar as m\u00e1quinas virtuais de uma determinada regi\u00e3o, voc\u00ea executa:$aws ec2 describe-instancesEnter fullscreen modeExit fullscreen modePara listar seus bancos gerenciados do servi\u00e7o RDS:$aws rds describe-db-instancesEnter fullscreen modeExit fullscreen modePor outro lado, se voc\u00ea quiser  os buckets S3 de uma conta, voc\u00ea usa o seguinte comando:aws s3lsEnter fullscreen modeExit fullscreen modePara listar todas as Kinesis streams:$aws kinesis list-streamsEnter fullscreen modeExit fullscreen modeO subcomandos3da AWS CLI \u00e9 t\u00e3o esquisito que a AWS optou por implementar uma nova API para trabalhar com s3 e n\u00e3o quebrar a retrocompatibilidade - o subcomandos3api. Este subcomando usa a vers\u00e3o mais \"moderna\", parecida com o Kinesis, que usalistcomo verbo em vez dedescribe:$aws s3api list-bucketsEnter fullscreen modeExit fullscreen modeSe voc\u00ea achou bobagem essas diverg\u00eancias, saiba que isso tem seu custo, em especial se voc\u00ea vai al\u00e9m das opera\u00e7\u00f5es triviais. Como essas particularidades n\u00e3o s\u00e3o da linha de comando e sim da API, elas se apresentam tamb\u00e9m naAWS SDK, e isso pode ser um problema, em especial para quem desenvolve solu\u00e7\u00f5esthird-partyque integram com servi\u00e7os AWS. Observe opost abaixo:Ent\u00e3o eu n\u00e3o sou o primeiro a apontar que isso incomoda!AWS Cloud ControlCuriosamente, no mesmo ano da not\u00edcia anterior, aAWS anunciou aAPI Cloud Control, cujo prop\u00f3sito \u00e9 resolver justamente esse problema de consist\u00eancia:A pr\u00f3pria AWS admite os problemas:As applications and infrastructures become increasingly sophisticated and you work across more AWS services, it becomes increasingly difficult to learn and manage distinct APIs. This challenge is exacerbated when you also use third-party services in your infrastructure, since you have to build and maintain custom code to manage both the AWS and third-party services together.Qual era a ideia ent\u00e3o? Oferecer uma \"interface padr\u00e3o\" para as opera\u00e7\u00f5es em todos os servi\u00e7os da AWS. No caso, implementar as opera\u00e7\u00f5es de um CRUDL (o mesmo que um CRUD, mas com umListjunto!):CreateResource;GetResource;UpdateResource;DeleteResource;ListResource.Como funciona na pr\u00e1tica? Vamos criar umbucket S3:$aws cloudcontrol create-resource\\--type-nameAWS::S3::Bucket\\--desired-state'{         \"BucketName\": devsres-cloudcontrol-lab         }'{\"ProgressEvent\":{\"TypeName\":\"AWS::S3::Bucket\",\"Identifier\":\"devsres-cloudcontrol-lab\",\"RequestToken\":\"6b47f322-d9af-4d08-8faa-6d530f33bcff\",\"Operation\":\"CREATE\",\"OperationStatus\":\"IN_PROGRESS\",\"EventTime\":\"2024-06-05T22:16:06.220000-03:00\"}}Enter fullscreen modeExit fullscreen modeCriando umastream Kinesis:$aws cloudcontrol create-resource\\--type-nameAWS::Kinesis::Stream\\--desired-state'{       \"Name\": \"ResourceExample\",       \"RetentionPeriodHours\":168,        \"ShardCount\\\":3       }'Enter fullscreen modeExit fullscreen modeQuer criar um CloudWatch LogGroup?$aws cloudcontrol create-resource\\--type-nameAWS::Logs::LogGroup\\--desired-state'{    \"LogGroupName\": \"DevSREsCloudControlLG\",    \"RetentionInDays\":30    }'{\"ProgressEvent\":{\"TypeName\":\"AWS::Logs::LogGroup\",\"Identifier\":\"DevSREsCloudControlLG\",\"RequestToken\":\"7fe86619-dda1-4698-beb0-7bcf29c22868\",\"Operation\":\"CREATE\",\"OperationStatus\":\"IN_PROGRESS\",\"EventTime\":\"2024-06-04T00:56:17.364000-03:00\"}}Enter fullscreen modeExit fullscreen modePara listar os buckets existentes:$aws cloudcontrol list-resources--type-nameAWS::S3::Bucket{\"ResourceDescriptions\":[{\"Identifier\":\"devsres-cloudcontrol-lab\",\"Properties\":\"{\\\"BucketName\\\":\\\"devsres-cloudcontrol-lab\\\"}\"},{\"Identifier\":\"outro-bucket-criado-de-outro-jeito\",\"Properties\":\"{\\\"BucketName\\\":\\\"outro-bucket-criado-de-outro-jeito\\\"}\"},{\"Identifier\":\"terceiro-bucket-criado-de-outro-jeito\",\"Properties\":\"{\\\"BucketName\\\":\\\"terceiro-bucket-criado-de-outro-jeito\\\"}\"}],\"TypeName\":\"AWS::S3::Bucket\"}Enter fullscreen modeExit fullscreen modeQuer listar os CloudWatch LogGroups?$aws cloudcontrol list-resources--type-nameAWS::Logs::LogGroup{\"ResourceDescriptions\":[{\"Identifier\":\"DevSREsCloudControlLG\",\"Properties\":\"{\\\"RetentionInDays\\\":30,\\\"LogGroupClass\\\":\\\"STANDARD\\\",\\\"LogGroupName\\\":\\\"DevSREsCloudControlLG\\\",\\\"Arn\\\":\\\"arn:aws:logs:us-west-2:211125357951:log-group:DevSREsCloudControlLG:*\\\"}\"},{\"Identifier\":\"OutroLogGroup\",\"Properties\":\"{\\\"RetentionInDays\\\":30,\\\"LogGroupClass\\\":\\\"STANDARD\\\",\\\"LogGroupName\\\":\\\"OutroLogGroup\\\",\\\"Arn\\\":\\\"arn:aws:logs:us-west-2:211125357951:log-group:OutroLogGroup:*\\\"}\"},{\"Identifier\":\"TerceiroLogGroup\",\"Properties\":\"{\\\"RetentionInDays\\\":30,\\\"LogGroupClass\\\":\\\"STANDARD\\\",\\\"LogGroupName\\\":\\\"TerceiroLogGroup\\\",\\\"Arn\\\":\\\"arn:aws:logs:us-west-2:211125357951:log-group:TerceiroLogGroup:*\\\"}\"}],\"TypeName\":\"AWS::Logs::LogGroup\"}Enter fullscreen modeExit fullscreen modeN\u00e3o sei se voc\u00ea prestou aten\u00e7\u00e3o, mas a solicita\u00e7\u00e3o de cria\u00e7\u00e3o n\u00e3o \u00e9 conclu\u00edda de maneira s\u00edncrona:$aws cloudcontrol create-resource\\--type-nameAWS::S3::Bucket\\--desired-state'{         \"BucketName\": \"ele-nao-espera-concluir\"       }'{\"ProgressEvent\":{\"TypeName\":\"AWS::S3::Bucket\",\"Identifier\":\"ele-nao-espera-concluir\",\"RequestToken\":\"e871792e-1467-45cb-bd0d-9ac8041825e2\",\"Operation\":\"CREATE\",\"OperationStatus\":\"IN_PROGRESS\",\"EventTime\":\"2024-06-05T23:52:02.801000-03:00\"}}Enter fullscreen modeExit fullscreen modeOOperationStatusIN_PROGRESSindica que a solicita\u00e7\u00e3o est\u00e1 sendo processada. Para acompanhar o status, usa-se outro comando passando oRequestToken:$aws cloudcontrol get-resource-request-status\\--request-tokene871792e-1467-45cb-bd0d-9ac8041825e2{\"ProgressEvent\":{\"TypeName\":\"AWS::S3::Bucket\",\"Identifier\":\"ele-nao-espera-concluir\",\"RequestToken\":\"e871792e-1467-45cb-bd0d-9ac8041825e2\",\"Operation\":\"CREATE\",\"OperationStatus\":\"SUCCESS\",\"EventTime\":\"2024-06-05T23:52:25.022000-03:00\"}}Enter fullscreen modeExit fullscreen modeA solicita\u00e7\u00e3o foi atendida com sucesso.E se tentarmos criar o mesmo bucket novamente?$aws cloudcontrol create-resource\\--type-nameAWS::S3::Bucket\\--desired-state'{         \"BucketName\": \"ele-nao-espera-concluir\"       }'{\"ProgressEvent\":{\"TypeName\":\"AWS::S3::Bucket\",\"Identifier\":\"ele-nao-espera-concluir\",\"RequestToken\":\"c965b56a-7a25-4b70-b084-6e733db0c6c3\",\"Operation\":\"CREATE\",\"OperationStatus\":\"IN_PROGRESS\",\"EventTime\":\"2024-06-05T23:57:55.038000-03:00\"}}$aws cloudcontrol get-resource-request-status\\--request-tokenc965b56a-7a25-4b70-b084-6e733db0c6c3{\"ProgressEvent\":{\"TypeName\":\"AWS::S3::Bucket\",\"Identifier\":\"ele-nao-espera-concluir\",\"RequestToken\":\"c965b56a-7a25-4b70-b084-6e733db0c6c3\",\"Operation\":\"CREATE\",\"OperationStatus\":\"FAILED\",\"EventTime\":\"2024-06-05T23:57:55.311000-03:00\",\"StatusMessage\":\"ele-nao-espera-concluir already exists (Service: S3, Status Code: 0, Request ID: null)\",\"ErrorCode\":\"AlreadyExists\"}}Enter fullscreen modeExit fullscreen modeComo era esperado, o retorno da requisi\u00e7\u00e3o \u00e9 uma falha, eStatusMEssagete explica o porqu\u00ea.Genial! Quando migramos?Parece legal, certo? E realmente \u00e9.Mas voc\u00ea precisa se perguntar da relev\u00e2ncia dessa mudan\u00e7a na sua vida e no seu c\u00f3digo.Faz sentido voc\u00eamigrartodas as suasstacksde c\u00f3digo Terraform para usar o novoprovider?A resposta, provavelmente, \u00e9n\u00e3o- o ganho \u00e9 m\u00ednimo para a infraestrutura pr\u00e9-existente, em especial se levado em considera\u00e7\u00e3o a massiva quantidade de trabalho que essa migra\u00e7\u00e3o demandaria.Faz sentido voc\u00ea passar a escrever todos os c\u00f3digos Terraform daqui pra frente usando \u00fanica e exclusivamente a API Cloud Control com seu novoprovider?Aqui temos algo pass\u00edvel de discuss\u00e3o. Mas,na minha opini\u00e3o, n\u00e3o vejo como imperativa a troca. O provider acabou de ser lan\u00e7ado comoGA; a maturidade do software e a base de usu\u00e1rios que o adotou nem se comparam com o provider original.A pr\u00f3pria API Cloud Control n\u00e3o tem tanta popularidade assim - em uma r\u00e1pida pesquisa que fiz no meu perfil, quase ningu\u00e9m havia sequer ouvido falar a respeito. Uma base menor de usu\u00e1rios no m\u00ednimo aumenta a possibilidade de incorrer em maior quantidades de bugs e comportamentos inesperados - tanto do provider quanto da pr\u00f3pria API Cloud Control.Esse fato, por si s\u00f3, n\u00e3o deveria desencorajar a ado\u00e7\u00e3o de um software, mas sabemos que nem todos est\u00e3o prontos para se tornaremEarly Adopters.Para quem \u00e9 o Cloud Control ent\u00e3o?A AWS responde sua pergunta no seuan\u00fancio de 2021: pessoas que construam solu\u00e7\u00f5es que operam usando servi\u00e7os da AWS (especialmente se usaremmuitos servi\u00e7os!). Trouxe o excerto em ingl\u00eas original para c\u00e1:Builders- The first community is builders using AWS Services APIs to manage their infrastructure or their customer\u2019s infrastructure. The ones requiring usage of low-level AWS Services APIs rather than higher level tools. For example, I know companies that manages AWS infrastructures on behalf of their clients. Many developed solutions to list and describe all resources deployed in their client\u2019s AWS Accounts, for management and billing purposes. Often, they built specific tools to address their requirements, but find it hard to keep up with new AWS Services and features. Cloud Control API simplifies this type of tools by o\ufb00ering a consistent, resource-centric approach. It makes easier to keep up with new AWS Services and features.APN Partners- The second community that benefits from Cloud Control API is APN Partners, such as HashiCorp (maker of Terraform) and Pulumi, and other APN Partners offering solutions that relies on AWS Services APIs. When AWS releases a new service or feature, our partner\u2019s engineering teams need to learn, integrate, and test a new set of AWS Service APIs to expose it in their offerings. This is a time consuming process and often leads to a lag between the AWS release and the availability of the service or feature in their solution. With the new Cloud Control API, partners are now able to build a unique REST API code base, using unified API verbs, common input parameters, and common error types. They just have to merge the standardized pre-defined uniform resource model to interact with new AWS Services exposed as REST resources.Se voc\u00ea n\u00e3o l\u00ea em ingl\u00eas (melhore isso j\u00e1!), vamos usar um exemplo.A AWS anunciou, no fim de abril, um novo servi\u00e7o (n\u00ba 356?) de \"render farms\" gerenciado com o pouco sugestivo\u00b9 nomeDeadline Cloud. Suponha que voc\u00ea trabalhe em uma empresa que faz trailers de filmes e tem interesse em usar este servi\u00e7o; voc\u00ean\u00e3o pode usar o terraform provider awssimplesmente porque eleainda n\u00e3o \u00e9 suportado:Isso quer dizer que voc\u00ea n\u00e3o pode usar sua infra-estrutura de IaC com Terraform para usar este servi\u00e7o, certo?Ou pode?Como a pr\u00f3pria Hashicorp conta em seucomunicado do post original:Initially launched in 2021 as a tech preview, the Terraform AWS Cloud Control provider is automatically generated based on the Cloud Control API published by AWS, which means the latest features and services on AWS can be supported right away. The AWSCC provider gives developers access with several new AWS services such as: AWS Billing Conductor, AWS Chatbot, Amazon Personalize, Amazon Q Business, and more.Em uma tradu\u00e7\u00e3o livre, o provider Cloud Control para Terraform \u00e9 gerado automaticamente usando como base a API disponibilizada pela AWS.Teoricamenteisso quer dizer que qualquer funcionalidade disponibilizada pela AWS estar\u00e1 dispon\u00edvel quase que imediatamente.Mas h\u00e1 detalhes adicionais! A lista de servi\u00e7os acima do excerto em ingl\u00eas lista os servi\u00e7osAmazon Q Business(lan\u00e7ado em maio de 2024),AWS Billing Conductor(lan\u00e7ado em 2022),AWS Chatbot(lan\u00e7ado em 2020!) eAmazon Personalize, que foi lan\u00e7ado h\u00e1 quase 5 anos atr\u00e1s!Isso d\u00e1 uma ideia que n\u00e3o \u00e9 s\u00f3 uma quest\u00e3o de \"usar os servi\u00e7os lan\u00e7ados ontem\", mas tamb\u00e9m usar servi\u00e7os mais obscuros e menos usados, cuja baixa popularidade faz com que a implementa\u00e7\u00e3o no provider principal n\u00e3o atinja massa cr\u00edtica o suficiente para se justificar o tempo gasto (como o Amazon Personalize), oferecendo uma alternativa \u00e0s pessoas que n\u00e3o conseguiam usar o mesmo padr\u00e3o e conven\u00e7\u00f5es de IaC para 100% de sua infraestrutura na AWS.Conclus\u00e3oPara a Hashicorp e Pulumi, naturalmente seria muito melhor se todos abandonassem seus providers oiginais e migrassem para os novos baseados em Cloud Control.Mas esse desejo provavelmente n\u00e3o significa que ir\u00e3o descontinuar os providers originais e s\u00f3 oferecer manuten\u00e7\u00e3o nos novos - eles certamente n\u00e3o s\u00e3o loucos o suficiente para faz\u00ea-lo (ou s\u00e3o?).Se sua empresa est\u00e1 interessada em usar os servi\u00e7os mais novos (em especial o festival de ofertas sobre generative AI), servi\u00e7os mais obscuros (como o Amazon Personalize) ou simplesmente tem o perfil \"early adopter\", em que os funcion\u00e1rios contam com espa\u00e7o para depurar e contribuir com projetos, lembre-se: a API Cloud Control existe, e agora temos inclusive um Terraform Provider considerado pronto para uso em produ\u00e7\u00e3o.Aproveitem!\u00b9 O nome do servi\u00e7o n\u00e3o \u00e9 t\u00e3o pouco sugestivo assim; na verdade n\u00e3o \u00e9 sugestivo para voc\u00ea! Em 2017, a AWScomprou uma empresa chamada Thinkbox Software, que constru\u00eda, entre outras coisas, solu\u00e7\u00f5es para gerenciamento e p\u00f3s processamento de renderiza\u00e7\u00f5es. A mais popular das ferramentas se chamaDeadline; logo,AWS Deadline Cloud\u00e9 o deploy automatizado deste software na nuvem."}
{"title": "Using a custom domain name in a Private REST API Gateway", "published_at": 1717642479, "tags": ["aws", "apigateway", "route53", "lambda"], "user": "Matias Kreder", "url": "https://dev.to/aws-builders/using-a-custom-domain-name-in-a-private-rest-api-gateway-1c2h", "details": "When working on internal networks, particularly within a VPC, developers often encounter the need to interact with a private API gateway. A common scenario is when a network resource, which must make non-internet HTTPS calls without involving the AWS API, requires access to a specific lambda function. While using the API gateway assigned hostname is an option, opting for a private DNS name can provide a more consistent approach across environments.According to theAWS Documentation:\"Custom domain names are not supported for private APIs.\"However, there is a simple hack to get this to work.TL;DR; ArchitectureFull SolutionOn the VPC, create a\"execute-api\" VPC endpoint for API GatewayOn API Gateway, create a private REST API and all necessary/ resource methods. Create a resource policythat only allow access through the VPC EndpointOn the VPC Endpoints, explore the Subnets section of the VPC endpoint created in step 1 and grab the IPsCreate a TLS target group using the IPs from step 3.Create a TLS internal NLB, using the target group from step 4.Create a custom domain name in API Gateway (Regional type) but point it to the private API gateway.On Route53, configure a private zone attached to the same VPC with a CNAME record that points to the NLB DNS address.Once this is done, it should work. I have done this many times in different projects but keep forgetting about it, so I figured it was a good time to document it to be useful for someone else."}
{"title": "How to detect Forest fires using Kinesis Video Streams and Amazon Rekognition", "published_at": 1717624915, "tags": ["aws", "kinesis", "rekognition", "awsiot"], "user": "Nikitas Gargoulakis", "url": "https://dev.to/aws-builders/how-to-detect-forest-fires-using-kinesis-video-streams-and-rekognition-4he8", "details": "IntroductionOn a hot summer night, while we were enjoying our food and drinks, the dogs suddenly began barking and staring at a certain direction. We got outside to have a better look and noticed that the sky had started to turn orange. We immediately knew what it was happening, there was a huge fire at a beautiful forest a few miles away. This was happening almost every summer, at different places, wiping out forests and destroying homes, with a massive impact on the environment and people's lives.Having seen the aftermath and the years it took for the burnt areas and people to recover, I decided to build something to detect smoke and fire and help reduce the destructive impact. After all, early detection plays a crucial role when it comes to forest fires.ChallengesWaiting for a Real-time scenario, like the one described above, was not an option or desirable for testing my solution. To overcome this challenge, i decided to simulate the required conditions.I used my laptop and played YouTube videos of forest fires as the source. This approach allowed me to consistently recreate the visual characteristics of forest fires, use specific scenes, thus ensuring that my solution was tested thoroughly under different conditions. This approach provided a reliable and efficient way to validate my solution and demonstrate how it could possibly handle similar real-time scenarios.PrerequisitesHere is a brief overview of the AWS services and components used in the solution:RTSP CameraAn IP/CCTV cameraRaspberry PiThis acts as a local gateway to connect the camera and manage the video stream up to Amazon Kinesis Video Streams. It is using certificates generated by AWS IoT Core to authenticate itself securely to AWS services.AWS IoTSet up an IoT Thing to represent my IP camera. This involved configuring the certificates and policies for secure communication between the IP camera and AWS IoT. It is an important component in creating a secure and manageable architecture for streaming video from an RTSP camera through a Raspberry Pi to Kinesis Video Streams.Kinesis Video Stream KVSKinesis Video Stream to ingest live video from the RTSP camera (with a matching name to the IoT Thing).Amazon RekognitionTrained a Rekognition Custom Labels model to detect smoke and fire in images. Training takes some time, depending on the size of the dataset. (The ARN is used in Lambda functions).S3Created an S3 bucket to store the extracted images from the IP camera, with the appropriate bucket policies to allow read/write access from the AWS services used.LambdaWrote a Lambda function to processes images stored in S3, detect smoke and fire using Rekognition, and trigger an SNS notification.SNSIf smoke or fire is detected by the Rekognition Custom Labels model, the Lambda function triggers a notification using Amazon Simple Notification Service (SNS). SNS can then deliver the notification to subscribed endpoints, such as email, SMS, or mobile push notifications.IAM RolesCreated the required  IAM roles and policies for Kinesis Video Streams, Rekognition, Lambda, IoT, S3, and SNS. As per best practices, least privilege principles were applied.Producer SDK - GStreamer pluginThe GStreamer plugin for Kinesis Video Streams is a component that integrates GStreamer with Amazon Kinesis Video Streams.Solution Overview and walkthroughHere is a brief overview about how the solution works.The first thing to do is to start the Amazon Rekognition Model that we trained.Next, we need to setup the RTSP camera and test the stream, using VLC. Then we move on and configure the GStreamer plugin in the Raspberry-Pi.We have to transfer the certificates to the Raspberry Pi and place them in a specific directory.Obtain the IoT credential endpoint using AWS CloudShell or awscli:aws iot describe-endpoint --endpoint-type iot:CredentialProviderEnter fullscreen modeExit fullscreen modeThe next step is to set the environment variables for the region, certificate paths, and role alias:export AWS_DEFAULT_REGION=eu-west-1 export CERT_PATH=certs/certificate.pem.crt export PRIVATE_KEY_PATH=certs/private.pem.key export CA_CERT_PATH=certs/AmazonRootCA1.pem export ROLE_ALIAS=CameraIoTRoleAlias export IOT_GET_CREDENTIAL_ENDPOINT=cxxxxxxxxxxs.credentials.iot.eu-west-1.amazonaws.comEnter fullscreen modeExit fullscreen modeNow we can execute the GStreamer command and start streaming to Kinesis Video Streams:./kvs_gstreamer_sample FireDetection rtsp://username:password@192.168.1.100/stream1Enter fullscreen modeExit fullscreen modeWith the video feed successfully streaming to Kinesis Video Streams, it's time to start extracting the images from the stream.Kinesis Video Streams simplifies this process by automatically transcoding and delivering images. It extracts images from video data in real-time based on tags and delivers them to a specified S3 bucket.To use that feature, we need to create a JSON file namedupdate-image-generation-input.jsonwith the required config.{  \"StreamName\": \"FireDetection\",  \"ImageGenerationConfiguration\":  {   \"Status\": \"ENABLED\",   \"DestinationConfig\":   {    \"DestinationRegion\": \"eu-west-1\",    \"Uri\": \"s3://images-bucket-name\"   },   \"SamplingInterval\": 200,   \"ImageSelectorType\": \"PRODUCER_TIMESTAMP\",   \"Format\": \"JPEG\",   \"FormatConfig\": {                 \"JPEGQuality\": \"80\"        },   \"WidthPixels\": 1080,   \"HeightPixels\": 720  } }Enter fullscreen modeExit fullscreen modeand run the following command in awscliaws kinesisvideo update-image-generation-configuration \\ --cli-input-json file://./update-image-generation-input.json \\Enter fullscreen modeExit fullscreen modeIf we check our S3 bucket we can see the extracted imagesOur Lambda function is now going to be triggered and will start processing them using Amazon Rekognition. This allows for identifying smoke/fire objects within the images and triggering notifications based on detected objects.ConclusionWe now have a solution where our IP camera streams video to a Kinesis Video Stream. AWS Lambda processes frames from this stream, using Amazon Rekognition Custom Labels to detect smoke and fire. Detected events are then triggering SNS.By integrating Amazon Rekognition with custom labels, Kinesis Video Streams, S3, and AWS IoT, we can create a powerful image recognition system for many use cases.For a more detailed walkthrough, feel free to contact me."}
{"title": "Amazon Inspector", "published_at": 1717577550, "tags": [], "user": "Manu Muraleedharan", "url": "https://dev.to/aws-builders/amazon-inspector-4fk4", "details": "Inspector is a Vulnerability scanning tool for AWS workloads.Here is an over view from AWS:https://www.youtube.com/watch?v=viAn4E7uwRUPersonal note: In the context of other law-enforcement terms used to name AWS Security services(Detective, Guard etc), Inspector is a bit different. In my country, a police inspector (the inspector that comes to my mind when I hear the title) is a law enforcement officer, who conducts investigations. I would say AWS Inspector is more like the vehicle inspector, who would verify if your vehicle is configured fine and is not causing pollution.For an instance to be scanned by Inspector, it needs to be a managed instance in SSM, and the below prerequisites need to be met:SSM Agent is installed on EC2 and it is runningThe instance has an IAM role with the required permissions to talk to SSM.443 port is open outbound from EC2 instance so it can talk to SSM service.Steps to follow if your EC2 is not coming as SSM Managed instance:https://repost.aws/knowledge-center/systems-manager-ec2-instance-not-appearTypes of scanningEC2 scanning - An agent would be installed on the EC2 (SSM Agent) which would scan the EC2 for any vulnerability.ECR Scanning - Scan images in ECRLambda scanning - Scan packages used in lambdaLambda code scanning - scan code in lambdaFor the vulnerabilities found, it will give relevant details like remediation steps, CVSS (Common Vulnerability Scoring System) Score, Inspector score etc which shows how critical this vulnerability is.Suppression RulesSay you have a web server. Then port 80, 443 being open is expected and you dont want to see warnings for that.You can create suppression rules to avoid seeing specific vulnerabilities reported.Vulnerability Database SearchSearch CVE ID in vulnerability databases to get more info on the reported vulnerability.Deep scan of EC2In addition to OS packages, application packages would be inspected for vulnerabilities. You can specify which paths you need to be scanned.Center of Internet Security (CIS) Benchmark assessmentsCenter of Internet Security, offers a suite of security benchmarks that serve as authoritative guidelines for securing IT systems, which are utilized extensively in the industry. On-demand/Scheduled scans can be run with CIS benchmark for specific operating systems. This will select resources based on the tags you specify.Export Software BOMSExport the Bill of Materials in the software packages analysed by inspector into industry standard formats. These BOMs contain a hierarchical list of all the individual components in a package. This functionality helps to check for vulnerabilities in a system not reachable from AWS.Inspector DemoEC2 scanningCreate an EC2 instance with an older version of Debian (version 10) from the marketplace.https://aws.amazon.com/marketplace/pp/prodview-vh2uh3o4pdfow#pdp-overview(This AMI is free to use)Create a security group allowing traffic from anywhere(0.0.0.0/0) to 22. Attach this security group to the EC2.SSH into the EC2, then install and start the SSM Agent.Steps for this:https://docs.aws.amazon.com/systems-manager/latest/userguide/manually-install-ssm-agent-linux.htmlNow the vulnerabilities from this EC2 would be detected by Inspector and shown in the console.Security group with SSH from anywhere will also be detected as a vulnerability.Container scanningPull a Debian 10 image from dockerhub and push it to the Amazon ECR.docker pull debian:10.0How to push to ECR:https://docs.aws.amazon.com/AmazonECR/latest/userguide/docker-push-ecr-image.htmlNow vulnerabilities from this docker image would be detected by Inspector and shown in the console. You can look at vulnerability per container repo or by container image.Lambda scanningCreate a lambda with some vulnerabilities. For example, lambda below updates a reserved variable and has an insecure socket connection.import os import json import socket  def lambda_handler(event, context):      # print(\"Scenario 1\");     os.environ['_HANDLER'] = 'hello'     # print(\"Scenario 1 ends\")     # print(\"Scenario 2\");     s = socket.socket(socket.AF_INET, socket.SOCK_STREAM)     s.bind(('',0))     # print(\"Scenario 2 ends\")      return {         'statusCode': 200,         'body': json.dumps(\"Inspector Code Scanning\", default=str)     }Enter fullscreen modeExit fullscreen modeCode needs to use runtimes supported by Inspector to be able to scan them. Many times this is not the latest, but the -1 version. For python, it is 3.11 as of writing this article, whereas latest version for lambda is 3.12.You can check the supported versions here:https://docs.aws.amazon.com/inspector/latest/user/supported.htmlNow the vulnerabilities from the lambda code can be seen in the console."}
{"title": "Building in Public", "published_at": 1717437171, "tags": ["aws", "buildinginpublic", "cloud", "webdev"], "user": "Ntombizakhona Mabaso", "url": "https://dev.to/aws-builders/building-in-public-4e91", "details": "Building in Publicis rather quite self-explanatory:Building. In. Public, need I say more?Building in Public basically emphasizes and hopes to encourage you to develop the habit of documenting and sharing your journey, as it happensin near real time, which is why I have decided to document the development of my 'Pet Project' site using AWS services (mostly).I say mostly, because I had intended to build a 100% AWS Cloud Native site, but I ended up registering my domain with a local provider which was four times cheaper than registering with Amazon Route53 (AWS's DNS Service), so technically, my future site has been optimized for cost already...For more details, it is a.co.zadomain which isZAR 90, which roughly equates toUSD 5, and on Amazon Route53 it isUSD 13, so hopefully, down the line, I hope Route53 becomes the cheaper option.What Are the Benefits of Building in Public?There are numerous benefits of building in public but primarily:Feedback: You get the opportunity to receive feedback in real time. This will help you make better choices instantaneously, especially if you haven\u2019t realized that there\u2019s a better alternative way to do something.Community: You get the chance to engage with like-minded individuals who may be on the same journey. You also get to interact with people in more advanced positions. By learning from their mistakes, successes, and experiences, you gain valuable, actionable insights. These insights can help you progress faster by being part of something, rather than being a directionless, confused lone wolf.Enhanced Communication Skills: Since building in public requires you to explain and document your steps, it encourages you to communicate more. As a result, you learn more as you transform your thoughts and actions into nuanced and structured words, either written or spoken, that others can effectively consume.Knowledge Acquisition and Retention:  There is a difference between active learning, which involves building and documenting, and passive learning, which involves reading and watching videos. By building in public, you engage in active learning, which enables you to retain the knowledge you acquire through the mistakes you make. Furthermore, by documenting your steps, you learn while teaching yourself and others. Thus, you are continuously and iteratively acquiring and retaining knowledge on another level.Accountability: When you build in public, it means that you\u2019re no longer the only one aware of your project. With others watching, you might feel more accountable and be more determined to see your project through. Accountability places responsibility on your shoulders. Therefore, you won\u2019t quit as easily as you would when you lack accountability. The faster you fail, the faster you get up and try again.ConclusionWhen you are building in public, you are not only building your project and your skills, but you are also building a real time feedback system, community, accountability, communication skills and enhanced knowledge acquisition.You get the opportunity to learn, unlearn and relearn - live.Let's Build...In Public."}
{"title": "Step by Step Troubleshooting WAFv2 - With Pictures", "published_at": 1717433638, "tags": ["aws", "cloud", "waf", "security"], "user": "Ali Ogun", "url": "https://dev.to/aws-builders/step-by-step-troubleshooting-wafv2-with-pictures-4hnp", "details": "You suspect that the Web Application Firewall (WAF) may be obstructing the functionality of your application. This article provides guidance on diagnosing the issue and adjusting/removing WAF rules as necessary.OpenWEB ACLstab under the WAF dashboard as shown in figure.Make sure you choose the correct region. In this example my WAF is deployed in Ohio(us-east-2). And then click your WAF rule, here in this example the name is managed_rules.In this dashboard, you can access comprehensive metrics related to your WAF. Take a moment to review the graphics and metrics to become acquainted with them. Make sure that Blocked button is selected only. However, the information we require is located at the bottom of the page, so please scroll down accordingly.4.At the bottom, we can see at the Attack Types graph that our WAF detected 3 different attack types as SQL Injection, NoUserAgent and BadBots.  We see those 3 because previously I tried to do an SQL injection to my application and also I made HTTPS requests without proper headers. In Requests terminated by managed rule groups you can see which rule groups are blocking your requests. In my case they are AWSManagedRulesSQLiRuleSet (for SQL injection) and AWSManagedRulesCommonRuleSet (for missing and bad agent header). Now that we have gathered some information, our focus needs to shift to the individual rules rather than the rule groups. Let's direct our attention to the graphic located at the bottom left Top 10 managed rule labels.If we take a closer look at this graph, we can see exactly when each rule kicks in and stops your access. Just match up the times when you tried to access with the points on the graph. The pink line on the graph shows which rule is stopping you. Below the graph, you'll see a list of rules that were triggered, each with its own color. In my case, it looks like the rule that's causing the trouble is aws:core-rule-set:NoUserAgent_Header. So what does it mean?The rule label is aws:core-rule-set:NoUserAgent_Header. After the last column, we see the name of the rule we want to exclude, which is NoUserAgent_Header. I actually sent a request with an empty user agent header, triggering this rule. Now, let's proceed and see how to exclude the rule programmatically.The rule NoUserAgent_Header belongs to the core-rule-set, and we need to refer to the corresponding AWS documentation to verify the actual name of NoUserAgent_Header because it is case-sensitive. As shown in the image below, we find the corresponding LABEL and Role Name. So in our case, the rule name I need to use in the exclude list is NoUserAgent_HEADER, not NoUserAgent_Header, because it is case-sensitive. Please take a look at the table below to find corresponding AWS page for your label name that you see in CloudWatch.Label from CloudWatchCorresponding AWS Pagecore-rule-setAWSManagedRulesCommonRuleSetsql-databaseAWSManagedRulesSQLiRuleSetwindows-osAWSManagedRulesWindowsRuleSetknown-bad-inputsAWSManagedRulesKnownBadInputsRuleSetamazon-ip-listAWSManagedRulesAmazonIpReputationListNow we can go to our root module and define our variables as shown below. We wanted to exclude NoUserAgent_HEADER and I place it under the rules_to_exclude_common list because this rule is part of the AWSManagedRulesCommonRuleSet. As you can see I also exclude as I wish other rules. Here I also excluded SQLi_QUERYARGUMENTS which is a rule blocks SQL injections and also I excluded UserAgent_BadBots_HEADER which inspects for common User-Agent header values that indicate that the request is a bad bot. You can see the code snippet at the bottom of the page.But why did we place NoUserAgent_HEADER under the rules_to_exclude_commonblock? In the table below, you can identify the corresponding Terraform variable to utilize in your code by comparing it with the Label from the CloudWatch column.| Label from CloudWatch | Corresponding TF Variable      ||-----------------------|--------------------------------|| core-rule-set         | rules_to_exclude_common        || sql-database          | rules_to_exclude_sql           || windows-os            | rules_to_exclude_windows       || known-bad-inputs      | rules_to_exclude_bad           || amazon-ip-list        | rules_to_exclude_reputation    |Now we can provision our infrastructure with that code (or whichever IaC Tool you use):$terraform applyEnter fullscreen modeExit fullscreen modeNow we can check whether our changes have been applied with going back to the dashboard from STEP 3. In that screen you need to click to Rules.Here I will click common. Because I wanted to exclude NoUserAgent_HEADER , and I know that I declared it in exclusion list under the rules_to_exclude_common block and this rule is part of AWSManagedRulesCommonRuleSet.Below, you can observe that NoUserAgent_HEADER is explicitly allowed now. Additionally, UserAgent_BadBots_HEADER is allowed, as both have been excluded. Remember that SQLi_QUERYARGUMENTS rule has also been excluded, which you can confirm under the sql section rather than common.So finally, you can see that again I send a request with an empty user agent header, but this time I receive successful response rather than403 Forbidden."}
{"title": "Data API for Amazon Aurora Serverless v2 with AWS SDK for Java - Part 7 Data API meets SnapStart", "published_at": 1717427658, "tags": ["aws", "serverless", "java", "database"], "user": "Vadym Kazulkin", "url": "https://dev.to/aws-builders/data-api-for-amazon-aurora-serverless-v2-with-aws-sdk-for-java-part-7-data-api-meets-snapstart-22eb", "details": "IntroductionIn thepart 5we measured cold and warm starts of our sample application which uses Data API for Amazon Aurora Serverless v2 with AWS SDK for Java and in thepart 6we did the same measurements using the standard connection management solutions like JDBC including the usage of the Amazon RDS Proxy service and compared the result. In this part of the series we'll enable AWS SnapStart to the Lambda function communicating with Aurora Serverless v2 PostgreSQL via Data API and also apply optimization technique called priming.Measuring cold and warm starts with SnapStart enabled and primingI released a separate series aboutAWS Lambda SnapStartwhere I talked about its benefits and also measured warm and cold starts for the similarapplicationwhich also used Java 21 managed Lambda runtime but DynamoDB database instead of Aurora Serverless v2 with Data API.In our experiment we'll re-use the application introduced in thepart 1for this which you can findhere.We will measure cold and warm start for 2 approaches:Enable SnapStart for Lambda functionGetProductByIdViaAuroraServerlessV2DataApiby addingSnapStart:         ApplyOn: PublishedVersionsEnter fullscreen modeExit fullscreen modeto the Properties: section of the Lambda functionAdditionally apply priming technique to the Lambda function with SnapStart enabled by priming the database request.  I explained priming in my articleMeasuring priming, end to end latency and deployment time with Java. In our case I implemented additional Lambda functionGetProductByIdViaAuroraServerlessV2DataApiWithPriming.  In its implementationGetProductByIdViaAuroraServerlessV2DataApiWithPrimingHandleryou can see how priming works in action :public void beforeCheckpoint(org.crac.Context<? extends Resource> context) throws Exception { \u2002\u2002\u2002\u2002\u2002\u2002\u2002\u2002\u2002\u2002\u2002\u2002auroraServerlessV2DataApiDao.getProductById(\"0\"); \u2002\u2002\u2002\u2002\u2002\u2002}Enter fullscreen modeExit fullscreen modeIn thebeforeCheckpointLambda runtime hook method (which usesCRaC API) we prime database invocation by retrieving the product with id equals to 0 from the database using Data API for Aurora Serverless v2. With that we pre-initialize all the classes involved in the invocation chain and pre-initilizing synchronous HTTP client (we use the default one which is Apache) which all will be directly available after the Firecracker microVM is restored.For that our Lambda handler needs to implementorg.crac.Resourceinterface, and register this class to be CRaC-aware with:public GetProductByIdViaAuroraServerlessV2DataApiWithPrimingHandler() { \u2002\u2002\u2002\u2002\u2002\u2002\u2002\u2002\u2002\u2002\u2002\u2002Core.getGlobalContext().register(this); \u2002\u2002\u2002\u2002\u2002\u2002}Enter fullscreen modeExit fullscreen modeAdditionally we need to include the following dependecy<dependency>   <groupId>io.github.crac</groupId>   <artifactId>org-crac</artifactId>   <version>0.1.3</version> </dependency>Enter fullscreen modeExit fullscreen modein thepom.xml.The results of the experiments to retrieve the existing product from the database by its id see below were based on reproducing more than 100 cold and at least 30.000 warm starts with experiment which ran for approximately 1 hour. For it (and experiments from my previous article) I used the load test toolhey, but you can use whatever tool you want, likeServerless-artilleryorPostman.Cold (c) and warm (m) start time in ms:Approachc p50c p75c p90c p99c p99.9c maxw p50w p75w p90w p99w p99.9w maxNo SnapStart enabled3154.3532373284.913581.493702.123764.92104.68173.96271.32572.111482.892179.7SnapStart enabled without priming1856.111994.612467.833229.113238.803241.7561.02113.32185.37639.351973.302878.5SnapStart enabled with priming of database invocation via Data API990.841069.041634.842120.002285.032286.960.06106.35185.37581.271605.372658.24ConclusionIn this part of the series, we applied SnapStart to the Lambda function and also convinced that it significantly reduced the cold and the warm start of our Lambda function. It was especially true when we also applied priming of Data API invocation.In the next part of the series we'll introduce various optimization strategies for the cold and warm starts."}
{"title": "Does Serverless Still Matter?", "published_at": 1717419593, "tags": ["serverless"], "user": "Benjamen Pyle", "url": "https://dev.to/aws-builders/does-serverless-still-matter-2jag", "details": "No.  Short, simple, and direct.  The answer to the question is that serverless at this point and time doesn't matter.  Now I'm not saying that it's never mattered.  But what I am saying is that it's just a tool in a developer's toolchain.  It's not some sweeping \"movement\" that it was and I firmly believe that this is all OK.  I don't see this as doom and gloom.  It's more about WOW, that happened, now what's next. With that, let's look at how we got here and where I think we go from here.The Serverless ArcThere are always people coming in and out of a community or technology ecosystem.  Serverless is no different.  And while some might say serverless is new and unproven, they'd be mistaken.  Google began shipping pay-as-you-go compute in the late 2000s but it wasn't until AWS released Lambda in 2014 that the serverless banner was hung in the cloud.  That gives the services and patterns more than 10 years of real-world production deployments.  There's been time to learn, fail, and harden so that its use cases can be clearly defined and exploited.  Additionally, as computing continues to improve, the lines have gotten blurry when having to decide to choose always-on vs event-driven serverless computing.  The performance of serverless compute is almost on par with those in the full time workload camp.  Anyone who says differently hasn't been paying attention.  Serverless has been deployed successfully in some of the most demanding of cloud-native businesses.Let's pretend for a minute that you need even more convincing, thenhere's a great whitepaperthat drives these points home even further.  Spoiler, you might recognize the author.Serverless has a story and with all stories, there is a beginning, middle, and end.The Early DaysMy perspective doesn't come from the excitement of launching new products and services in one of the big vendors but from that of a cloud builder and community member.  Sure, I haven't always been as active as I am now, but I am an early adopter who approaches things with a healthy dose of skepticism.  I don't want to make bets on things that end up having to be replaced because the tech was abandoned.  I feel like serverless was born during a time when service buses were dying and the birth of microservices and containers was happening as well.  I lived through the container wars and container orchestration discussions and remember how easy it was for serverless to slide under the radar.  It wasn't until 2015 that I actually got my hands on Lambda and then in 2016 when I put something in production powered by this thing called serverless compute.  If you've heard me say serverless is more than just compute.  That's true now, but it wasn't always that way.From a community and builder standpoint, AWS didn't make quite the push that I remember.  I believe that early practitioners and precursors to the Developer Advocate explosion were building patterns and materials to onslaught the market with what their engineering teams had produced.  Again, this isn't backed by specific inside knowledge, only my perception and what I imagined would have happened.  Serverless to me had gotten lost in the shuffle while the architecture and developer community leaned into container-based distributed APIs.The early days though just like any set of early days were filled with hope, promise, and a chance at changing the status quo.Mid-LifeAt this point in the serverless arc, things really started to pick up.  If I was going to put a time on things, I'd say mid-life started in late 2016 and we are currently living in these same times.  From my vantage point, there was a massive energy that was released from AWS and others to saturate the market with quality materials, samples, and patterns so that any builder looking to jump on the train had an easy on-ramp.  The serverless energy was almost like the early days of the iPhone.  You almost couldn't help but buy into the hype.  Because honestly, it was hype.  There were limited runtimes, not nearly as many connection points that exist now, DynamoDB modeling was proven in-house but not so much in industry, and Lambda itself suffered massive spin-up times.  Some of these issues limited a builder to using Lambda in only asynchronous type workflows.  I know it's hard to believe, but there was no EventBridge or Step Functions either.  Seriously, early times had a bunch of hype mixed in with a great deal of promise.It was that promise that fueled a movement.  A movement that AWS and others invested heavily in by encouraging community and online discord to the point of it being everywhere.  I've been doing this since the mid-nineties and I've never seen a push and rally behind something quite like this.  Docker, Ruby on Rails, Java, .NET, and the current version of AI are the only things that I remember in my career that have come this close.If I take a step back and look at why serverless is so interesting to me, it's because it's not like Docker, RoR, or Java.  Those were open source projects that had tremendous support from community members.  We all know how passionate open source contributors can be.  And yes, I remember Java started with Sun but it did get released as open source.  It also was heavily supported by people who wanted to work in Free and Open Source technologies.  At the time, Java == Linux and Linux wasn't Microsoft.  So why am I digressing here?  Because serverless has nothing to do with open source.  I get it, AWS Firecracker which powers Lambda is open source, but serverless in and of itself is not a technology.  It's a description or an umbrella that a capability lives under.  When I look at these facts, I find the whole thing so interesting.  The communities that stood up around serverless were quasi-corporate sponsored communities and that hadn't happened before in my memory.   Not at the scale and the force that serverless did it.Zeroing back in on the present, I do believe that we are at the tail end of the mid-life arc for serverless. For clarity, I don't think serverless is done by any stretch.  Capabilities still need to be added, integrations built, continued work on observability, and generally more undifferentiated heavy lifting to take care of.  But it feels to me like we've entered into a new space.  The ones that launched this \"run code without worrying about infrastructure\" movement have gotten distracted by the next disruptor.  In all fairness, this happens in any industry and with any technology.  Innovation is like breathing.  But what I don't like about what I see with the serverless ecosystem is this.  If the iPhone is analogous to an appliance at this point and it can only take quality-of-life updates, then I believe that serverless is getting close to that point.  I don't think it has to reach that point but with a lack of innovation from the big providers, the movement will begin to lose steam. Enter the end-of-life phase.End of LifeEverything gets here.  Software, animals, people.  We are born, we live, we die.  As I mentioned above, serverless will eventually get to the point that it's like the iPhone.  It will receive quality-of-life updates and those that market and sell will continue to make each release cycle sound like the next best thing is here.  ElastiCache and OpenSearch Serverless sound familiar?  But truthfully, builders can smell and feel what's not all the way real.  This isn't a bad thing honestly at its core.  All of the serverless code and applications in production can't just \"go away\".  AWS, Google, and Microsoft will continue to run these workloads for us and the software systems we've built will continue to live on.  Code spends more time in maintenance than in any other phase of its life.However, what will happen is that the energy, content, and communities will also slowly spin down and we will leave the era of \"run code without thinking about servers\" and move into the world of what's next.  If current trends follow, it'll be the world of AI and the creation of code without servers as well.  So we went from running and not caring about the infrastructure to now generating code that we don't care what infrastructure created it.  If we enter the end-of-life phase and you don't realize the impact that serverless has had, you truly haven't been paying attention.The point of acknowledging this though is important.  Serverless won't end because it wasn't impactful, meaningful, or real.  All things go out of style especially something that was corporate-backed.  They will move on to what's next because that's how you innovate, make money, and generate more value.  Serverless was important.What's the Point?Now that we've taken that detour through what I believe is the arc of serverless, how can I possibly say that serverless doesn't matter?  If you look me up online, you'll see that I'm an AWS Community Builder focused on serverless, I'm an active writer and code producer who is very often serverless, and I'm a Champion in theBelieve in Serverless community.  I can believe in the arc I shared above and believe in serverless itself.  Those things aren't at odds.  And here's why.The PhoenixIf I was casting a vision for the future, here's what I think.  Serverless the big corporate-sponsored version is on the slide towards the end of life era.  But just like the Phoenix from mythology, serverless has a chance to be reborn and rise from its ashes.  You can see that happening actually now.  Kind of weird that death and life are happening at the same time, but they are.  The most amazing thing that AWS, Google, and Microsoft have given to the world is the gift of obscene amounts of compute and wonderfully built infrastructure.  That infrastructure provides us as builders compute power beyond our wildest dreams.  But not beyond the visions of new leaders in the serverless product space.New products are being created seemingly overnight.  Products likeMomentoare not building serverless cache, they are reimagining caching and application performance while solving the problems with a serverless mindset.Serverless Postgresis now a thing.  And companies that have traditionally been installed are now embracing serverless.  Just look atInfluxDBwhich is now offering a serverless version.Serverless is going to be reborn because the promises it makes are sound and good for developers and businesses.  Businesses that are buying products built with serverless and good for businesses that are building serverless offerings.  If I had to look forward 5 years ahead, I see a world where more companies like this are spinning up and filling the gaps that AWS, Google, and Microsoft are leaving by being so heavily invested in AI.  And by the time those giants spin back around, maybe they can buy their way back in, or maybe they won't want to, but we as builders will have moved on as well.  Not without serverless, but without big corporate serverless.Value over DogmaI mentioned it above, but other successful \"movements\" in tech were fostered and cared for by passionate open source contributors.  Serverless doesn't share those roots as like I've mentioned, was born out of companies.  But what has happened is that the serverless movement, along with a boost from these new upstart vendors has the human capability to carry forward in this new world.  Communities likeBelieve in Serverlessare fostering collaboration and engagement regardless of your flavor of serverless or programming language of choice.  What I find so interesting about what I see right now is that the online discourse has moved passed talking about the far left or far right of serverless and is just talking about delivering value and solving problems.  The word serverless rarely comes up.  The focus has found its way to value, users, and developer experience. Which is right where it always is down the center of the tech world.What I also find unique to this version of the serverless community is that it's open not just to a vendor but not even to being serverless.  The concept of serverless only pushed too far into the dialogue and I believe that was true because of where it was coming from.  The truth is, it can be serverless only, but almost every deployment is going to be serverless plus.  And the most responsible thing a serverless architect can do is be serverless first but not serverless always.  It just doesn't make sense.  And this community gets that.  It's different than what it was like in years previous.  Version x.0 of Serverless is a much more moderate and tempered crowd.  Which ultimately is a great thing.So again, serverless doesn't matter.  Value has always mattered.  And what's being shown is that serverless plays a role in shipping value.  But honestly, it always had.PeopleI always end up back here, don't I?  I believe strongly that there is more humanity in tech than people want to acknowledge.  Sure, algorithms, data structures, transistors, power, and everything in between are very scientific.  But just like there are physical aspects to a building, if it didn't deliver a solid user experience, the building wouldn't sell.  Software is like this but at a higher level. You can't build good software without the help of others.  And you can't build a good community or support a movement like serverless without amazing people.I've said this from day 1 as being public in the serverless and tech community, that I wouldn't be doing or sharing most of this content if it wasn't for the people. Serverless doesn't matter to me because I could be writing COBOL code with the people I've met as a part of this movement and it would be A-OK by me.  25 years ago I was involved in Linux communities because the people were awesome to hang out with.  And serverless to me has that same feel.  And it's something I give AWS credit for even beyond the software and the marketing that launched serverless into the world like the Hulk Ride at Universal Orlando.  They launched serverless with amazing people.  And they recruited heavily to build communities and groups that also had quality humans at the core.Those are facts that just underscore for me that serverless doesn't matter.  People do.  They always have and they always will long after the world realizes that GenAI is just the next fad.Wrapping UpAnd even though these times are fading, the people aren't.  We are just finding other ways to organize and collaborate.  And if the computers and the AI take away this craft called programming that I love dearly, I'll still have the friends and relationships that I've made through being in this community.  And then perhaps we'll have more time on our hands to do things IRL vs always being virtual.  Who knows.But I do know this.  Serverless mattered. A computing movement was built that helped shape this next phase and the world is better for having had this happen.But I also know that it mattered for reasons behind the compute.  It mattered because of the community that was born from it.  Artificial, manufactured, or cultivated, who cares?  It happened.  And what happens next is also why serverless no longer matters.  Because the people that came together matter more and the future is brighter than it's ever been.Thanks so much for reading this different piece.  And happy building!"}
{"title": "How To Manage an Amazon Bedrock Knowledge Base Using Terraform", "published_at": 1717358396, "tags": ["aws", "terraform", "ai"], "user": "Anthony Wat", "url": "https://dev.to/aws-builders/how-to-manage-an-amazon-bedrock-knowledge-base-using-terraform-2688", "details": "IntroductionIn the previous blog post,Adding an Amazon Bedrock Knowledge Base to the Forex Rate Assistant, I explained how to create a Bedrock knowledge base and associate it with a Bedrock agent using the AWS Management Console, with a forex rate assistant as the use case example.We also covered how to manage Bedrock agents with Terraform in another blog post,How To Manage an Amazon Bedrock Agent Using Terraform. In this blog post, we will extend that setup to also manage knowledge bases in Terraform. To begin, we will first examine the relevant AWS resources in the AWS Management Console.Taking inventory of the required resourcesUpon examining the knowledge base we previously built, we find that it comprises the following AWS resources:Theknowledge baseitself;Theknowledge base service rolethat provides the knowledge base access to Amazon Bedrock models, data sources in S3, and the vector index;TheOpenSearch Serverless policies, collection, and the vector index;The S3 bucket that acts as thedata sourceWith this list of resources, along with those required by the agent to which the knowledge base will be attached, we can begin creating the Terraform configuration. Before diving into the setup, let's first take care of the prerequisites.Defining variables for the configurationFor better manageability, we define some variables in avariables.tffile that we will reference throughout the Terraform configuration:variable\"kb_s3_bucket_name_prefix\"{description=\"The name prefix of the S3 bucket for the data source of the knowledge base.\"type=stringdefault=\"forex-kb\"}variable\"kb_oss_collection_name\"{description=\"The name of the OSS collection for the knowledge base.\"type=stringdefault=\"bedrock-knowledge-base-forex-kb\"}variable\"kb_model_id\"{description=\"The ID of the foundational model used by the knowledge base.\"type=stringdefault=\"amazon.titan-embed-text-v1\"}variable\"kb_name\"{description=\"The knowledge base name.\"type=stringdefault=\"ForexKB\"}Enter fullscreen modeExit fullscreen modeDefining the S3 and IAM resourcesThe knowledge base requires a service role, which can be created using theaws_iam_roleresourceas follows:data\"aws_caller_identity\"\"this\"{}data\"aws_partition\"\"this\"{}data\"aws_region\"\"this\"{}locals{account_id=data.aws_caller_identity.this.account_idpartition=data.aws_partition.this.partitionregion=data.aws_region.this.nameregion_name_tokenized=split(\"-\",local.region)region_short=\"${substr(local.region_name_tokenized[0],0,2)}${substr(local.region_name_tokenized[1],0,1)}${local.region_name_tokenized[2]}\"}resource\"aws_iam_role\"\"bedrock_kb_forex_kb\"{name=\"AmazonBedrockExecutionRoleForKnowledgeBase_${var.kb_name}\"assume_role_policy=jsonencode({Version=\"2012-10-17\"Statement=[{Action=\"sts:AssumeRole\"Effect=\"Allow\"Principal={Service=\"bedrock.amazonaws.com\"}Condition={StringEquals={\"aws:SourceAccount\"=local.account_id}ArnLike={\"aws:SourceArn\"=\"arn:${local.partition}:bedrock:${local.region}:${local.account_id}:knowledge-base/*\"}}}]})}Enter fullscreen modeExit fullscreen modeWith the service role in place, we can now proceed to define the corresponding IAM policy. As we define the configuration for creating resources that the knowledge base service role needs to access, we will consequently define the corresponding IAM policy using theaws_iam_role_policyresource. First, we create the IAM policy that provides access to the embeddings model. Since the foundation model is not created but referenced, we can use theaws_bedrock_foundation_modeldata sourceto obtain the ARN which we need:data\"aws_bedrock_foundation_model\"\"kb\"{model_id=var.kb_model_id}resource\"aws_iam_role_policy\"\"bedrock_kb_forex_kb_model\"{name=\"AmazonBedrockFoundationModelPolicyForKnowledgeBase_${var.kb_name}\"role=aws_iam_role.bedrock_kb_forex_kb.namepolicy=jsonencode({Version=\"2012-10-17\"Statement=[{Action=\"bedrock:InvokeModel\"Effect=\"Allow\"Resource=data.aws_bedrock_foundation_model.kb.model_arn}]})}Enter fullscreen modeExit fullscreen modeNext, we create the Amazon S3 bucket that acts as thedata sourcefor the knowledge base using theaws_s3_bucketresource. To adhere to security best practices, we also enable S3-SSE using theaws_s3_bucket_server_side_encryption_configurationresourceand bucket versioning with theaws_s3_bucket_versioningresourceas follows:resource\"aws_s3_bucket\"\"forex_kb\"{bucket=\"${var.kb_s3_bucket_name_prefix}-${local.region_short}-${local.account_id}\"force_destroy=true}resource\"aws_s3_bucket_server_side_encryption_configuration\"\"forex_kb\"{bucket=aws_s3_bucket.forex_kb.idrule{apply_server_side_encryption_by_default{sse_algorithm=\"AES256\"}}}resource\"aws_s3_bucket_versioning\"\"forex_kb\"{bucket=aws_s3_bucket.forex_kb.idversioning_configuration{status=\"Enabled\"}depends_on=[aws_s3_bucket_server_side_encryption_configuration.forex_kb]}Enter fullscreen modeExit fullscreen modeNow that the S3 bucket is available, we can create the IAM policy that gives the knowledge base service role access to files for indexing:resource\"aws_iam_role_policy\"\"bedrock_kb_forex_kb_s3\"{name=\"AmazonBedrockS3PolicyForKnowledgeBase_${var.kb_name}\"role=aws_iam_role.bedrock_kb_forex_kb.namepolicy=jsonencode({Version=\"2012-10-17\"Statement=[{Sid=\"S3ListBucketStatement\"Action=\"s3:ListBucket\"Effect=\"Allow\"Resource=aws_s3_bucket.forex_kb.arnCondition={StringEquals={\"aws:PrincipalAccount\"=local.account_id}}},{Sid=\"S3GetObjectStatement\"Action=\"s3:GetObject\"Effect=\"Allow\"Resource=\"${aws_s3_bucket.forex_kb.arn}/*\"Condition={StringEquals={\"aws:PrincipalAccount\"=local.account_id}}}]})}Enter fullscreen modeExit fullscreen modeDefining the OpenSearch Serverless policy resourcesThe Bedrock console offers a quick create option that provisions an OpenSearch Serverless vector store on our behalf as the knowledge base is created. Since thedocumentationfor creating the vector index in OpenSearch Serverless is a bit open-ended, we can refer to the resources from the quick create option to supplement.First, weconfigure permissionsby defining adata access policyfor the vector search collection. The data access policy from the quick create option is defined as follows:This data access policy provides read and write permissions to the vector search collection and its indices to the knowledge base execution role and the creator of the policy.Using the correspondingaws_opensearchserverless_access_policyresource, we can define the policy as follows:resource\"aws_opensearchserverless_access_policy\"\"forex_kb\"{name=var.kb_oss_collection_nametype=\"data\"policy=jsonencode([{Rules=[{ResourceType=\"index\"Resource=[\"index/${var.kb_oss_collection_name}/*\"]Permission=[\"aoss:CreateIndex\",\"aoss:DeleteIndex\",\"aoss:DescribeIndex\",\"aoss:ReadDocument\",\"aoss:UpdateIndex\",\"aoss:WriteDocument\"]},{ResourceType=\"collection\"Resource=[\"collection/${var.kb_oss_collection_name}\"]Permission=[\"aoss:CreateCollectionItems\",\"aoss:DescribeCollectionItems\",\"aoss:UpdateCollectionItems\"]}],Principal=[aws_iam_role.bedrock_kb_forex_kb.arn,data.aws_caller_identity.this.arn]}])}Enter fullscreen modeExit fullscreen modeNote thataoss:DeleteIndexwas added to the list because this is required for cleanup by Terraform viaterraform destroy.Next, we need anencryption policythat assigns an encryption key to a collection for data protection at rest. The encryption policy from the quick create option is defined as follows:This encryption policy simply assigns an AWS-owned key to the vector search collection. Using theaws_opensearchserverless_security_policyresourcewith an encryption type, we can define the policy as follows:resource\"aws_opensearchserverless_security_policy\"\"forex_kb_encryption\"{name=var.kb_oss_collection_nametype=\"encryption\"policy=jsonencode({Rules=[{Resource=[\"collection/${var.kb_oss_collection_name}\"]ResourceType=\"collection\"}],AWSOwnedKey=true})}Enter fullscreen modeExit fullscreen modeLastly, we need anetwork policywhich defines whether a collection is accessible publicly or privately. The network policy from the quick create option is defined as follows:his network policy allows public access to the vector search collection's API endpoint and dashboard over the internet. Using theaws_opensearchserverless_security_policyresourcewith an network type, we can define the policy as follows:resource\"aws_opensearchserverless_security_policy\"\"forex_kb_network\"{name=var.kb_oss_collection_nametype=\"network\"policy=jsonencode([{Rules=[{ResourceType=\"collection\"Resource=[\"collection/${var.kb_oss_collection_name}\"]},{ResourceType=\"dashboard\"Resource=[\"collection/${var.kb_oss_collection_name}\"]}]AllowFromPublic=true}])}Enter fullscreen modeExit fullscreen modeWith the prerequisite policies in place, we can now create the vector search collection and the index.Defining the OpenSearch Serverless collection and index resourcesCreating the collection in Terraform is straightforward using theaws_opensearchserverless_collectionresource:resource\"aws_opensearchserverless_collection\"\"forex_kb\"{name=var.kb_oss_collection_nametype=\"VECTORSEARCH\"depends_on=[aws_opensearchserverless_access_policy.forex_kb,aws_opensearchserverless_security_policy.forex_kb_encryption,aws_opensearchserverless_security_policy.forex_kb_network]}Enter fullscreen modeExit fullscreen modeThe knowledge base service role also needs access to the collection, which we can provide using theaws_iam_role_policysimilar to before:resource\"aws_iam_role_policy\"\"bedrock_kb_forex_kb_oss\"{name=\"AmazonBedrockOSSPolicyForKnowledgeBase_${var.kb_name}\"role=aws_iam_role.bedrock_kb_forex_kb.namepolicy=jsonencode({Version=\"2012-10-17\"Statement=[{Action=\"aoss:APIAccessAll\"Effect=\"Allow\"Resource=aws_opensearchserverless_collection.forex_kb.arn}]})}Enter fullscreen modeExit fullscreen modeCreating the index in Terraform is however more complex, since it is not an AWS resource but an OpenSearch construct. Looking at CloudTrail events, there wasn't any event that correspond to an AWS API call that would create the index. However, observing the network traffic in the Bedrock console did reveal a request to the OpenSearch collection's API endpoint to create the index. This is what we want to port to Terraform.Luckily, there is anOpenSearch Providermaintained by OpenSearch that we can use. To connect to the vector search collection, we provide the endpoint URL and credentials in theproviderblock. The provider has first-class support for AWS, so credentials can be provided implicitly similar to the Terraform AWS Provider. The resulting provider definition is as follows:provider\"opensearch\"{url=aws_opensearchserverless_collection.forex_kb.collection_endpointhealthcheck=false}Enter fullscreen modeExit fullscreen modeNote that thehealthcheckargument is set tofalsebecause the client health check does not really work with OpenSearch Serverless.To get the index definition, we can examine the collection in the OpenSearch Service Console:We can create the index using theopensearch_indexresourcewith the same specifications:resource\"opensearch_index\"\"forex_kb\"{name=\"bedrock-knowledge-base-default-index\"number_of_shards=\"2\"number_of_replicas=\"0\"index_knn=trueindex_knn_algo_param_ef_search=\"512\"mappings=<<-EOF{       \"properties\": {         \"bedrock-knowledge-base-default-vector\": {           \"type\": \"knn_vector\",           \"dimension\": 1536,           \"method\": {             \"name\": \"hnsw\",             \"engine\": \"faiss\",             \"parameters\": {               \"m\": 16,               \"ef_construction\": 512             },             \"space_type\": \"l2\"           }         },         \"AMAZON_BEDROCK_METADATA\": {           \"type\": \"text\",           \"index\": \"false\"         },         \"AMAZON_BEDROCK_TEXT_CHUNK\": {           \"type\": \"text\",           \"index\": \"true\"         }       }     }EOFforce_destroy=truedepends_on=[aws_opensearchserverless_collection.forex_kb]}Enter fullscreen modeExit fullscreen modeNote that the dimension is set to 1536, which is the value required for theTitan G1 Embeddings - Textmodel.Before we move on, you must know about an issue with the Terraform OpenSearch provider that caused me a lot of headache. When I was testing the Terraform configuration, theopensearch_indexresource kept failing because the provider could not seemingly authenticate against the collection's endpoint URL. After a long debugging session, I was able to find aGitHub issuein the Terraform OpenSearch Provider repository that mentions the cryptic \"EOF\" error that was present. The issue mentions that the bug is related to OpenSearch Serverless and an earlier provider version, v2.2.0, does not have the problem. Consequently, I was able to work around the problem by using this specific version of the provider:terraform{required_providers{aws={source=\"hashicorp/aws\"version=\"~> 5.48\"}opensearch={source=\"opensearch-project/opensearch\"version=\"= 2.2.0\"}}required_version=\"~> 1.5\"}Enter fullscreen modeExit fullscreen modeHopefully letting you in on this tip will save you hours of troubleshooting.Defining the knowledge base resourceWith all dependent resources in place, we can now proceed to create the knowledge base. However, there is the matter ofeventual consistency with IAM resourcesthat we first need to address. Since Terraform creates resources in quick succession, there is a chance that the configuration of the knowledge base service role is not propagated across AWS endpoints before it is used by the knowledge base during its creation, resulting in temporary permission issues. What I observed during testing is that the permission error is usually related to the OpenSearch Serverless collection.To mitigate this, we add a delay using thetime_sleepresourcein the Time Provider. The following configuration will add a 20-second delay after the IAM policy for the OpenSearch Serverless collection is created:resource\"time_sleep\"\"aws_iam_role_policy_bedrock_kb_forex_kb_oss\"{create_duration=\"20s\"depends_on=[aws_iam_role_policy.bedrock_kb_forex_kb_oss]}Enter fullscreen modeExit fullscreen mode\ud83d\udca1 If you still encounter permission issues when creating the knowledge base, try increasing the delay to 30 seconds.Now we can create the knowledge base using theaws_bedrockagent_knowledge_baseresourceas follows:resource\"aws_bedrockagent_knowledge_base\"\"forex_kb\"{name=var.kb_namerole_arn=aws_iam_role.bedrock_kb_forex_kb.arnknowledge_base_configuration{vector_knowledge_base_configuration{embedding_model_arn=data.aws_bedrock_foundation_model.kb.model_arn}type=\"VECTOR\"}storage_configuration{type=\"OPENSEARCH_SERVERLESS\"opensearch_serverless_configuration{collection_arn=aws_opensearchserverless_collection.forex_kb.arnvector_index_name=\"bedrock-knowledge-base-default-index\"field_mapping{vector_field=\"bedrock-knowledge-base-default-vector\"text_field=\"AMAZON_BEDROCK_TEXT_CHUNK\"metadata_field=\"AMAZON_BEDROCK_METADATA\"}}}depends_on=[aws_iam_role_policy.bedrock_kb_forex_kb_model,aws_iam_role_policy.bedrock_kb_forex_kb_s3,opensearch_index.forex_kb,time_sleep.aws_iam_role_policy_bedrock_kb_forex_kb_oss]}Enter fullscreen modeExit fullscreen modeNote thattime_sleep.aws_iam_role_policy_bedrock_kb_forex_kb_ossis in thedepends_onlist - this is how the aforementioned delay is enforced before the knowledge base is created by Terraform.We also need to add the data source to the knowledge base using theaws_bedrock_data_source resourceas follows:resource\"aws_bedrockagent_data_source\"\"forex_kb\"{knowledge_base_id=aws_bedrockagent_knowledge_base.forex_kb.idname=\"${var.kb_name}DataSource\"data_source_configuration{type=\"S3\"s3_configuration{bucket_arn=aws_s3_bucket.forex_kb.arn}}}Enter fullscreen modeExit fullscreen modeVoila! We have created a stand-alone Bedrock knowledge base using Terraform! All that remains is to attach the knowledge base to an agent (the forex assistant in our case) to extend the solution.Integrating the knowledge base and agent resourcesFor your convenience, you can use the Terraform configuration from the blog postHow To Manage an Amazon Bedrock Agent Using Terraformto create the rate assistant. It can be found in the1_basicdirectory inthis GitHub repository.Once you incorporate this Terraform configuration with the knowledge base you\u2019ve been developing, we use the newaws_bedrockagent_agent_knowledge_base_associationresourceto associate the knowledge base with the agent:resource\"aws_bedrockagent_agent_knowledge_base_association\"\"forex_kb\"{agent_id=aws_bedrockagent_agent.forex_asst.iddescription=file(\"${path.module}/prompt_templates/kb_instruction.txt\")knowledge_base_id=aws_bedrockagent_knowledge_base.forex_kb.idknowledge_base_state=\"ENABLED\"}Enter fullscreen modeExit fullscreen modeFor better organization, we will keep the knowledge base description in a text file calledkb_instruction.txtin theprompt_templatesfolder. The file contains the following text:Use this knowledge base to retrieve information on foreign currency exchange, such as the FX Global Code.Enter fullscreen modeExit fullscreen modeLastly, we explained in the previous blog post that the agent must be prepared after changes are made. We used anull_resourceto trigger the prepare action, so we will continue to use the same strategy for the knowledge base association by adding an explicit dependency:resource\"null_resource\"\"forex_asst_prepare\"{triggers={forex_api_state=sha256(jsonencode(aws_bedrockagent_agent_action_group.forex_api))forex_kb_state=sha256(jsonencode(aws_bedrockagent_knowledge_base.forex_kb))}provisioner\"local-exec\"{command=\"aws bedrock-agent prepare-agent --agent-id${aws_bedrockagent_agent.forex_asst.id}\"}depends_on=[aws_bedrockagent_agent.forex_asst,aws_bedrockagent_agent_action_group.forex_api,aws_bedrockagent_knowledge_base.forex_kb]}Enter fullscreen modeExit fullscreen modeTesting the configurationNow, the moment of truth. We can apply the full Terraform configuration and make sure that it is working properly. My run took several minutes, with the majority of the time spent on creating the OpenSearch Serverless collection. Here is an excerpt of the output for reference:In the Bedrock console, we can see that the agentForexAssistantis ready for testing. But we first need to upload theFX Global Code PDF fileto the S3 bucket and do a data source sync. For details on these steps, refer to the blog postAdding an Amazon Bedrock Knowledge Base to the Forex Rate Assistant.Using the test chat interface, I asked:What is the FX Global Code?It responded with an explanation that contains citations, indicating that the information was obtained from the knowledge base.For good measure, we will also ask the forex assistant for an exchange rate:What is the exchange rate from US Dollar to Canadian Dollar?It responded with the latest exchange rate as expected:And that's a wrap! Don't forget to runterraform destroywhen you are done, since there is a running cost for the OpenSearch Serverless collection.\u2705 For reference, I've dressed up the Terraform solution a bit and checked in the final artifacts to the2_knowledge_basedirectory inthis repository. Feel free to check it out and use it as the basis for your Bedrock experimentation.SummaryIn this blog post, we developed the Terraform configuration for the knowledge base that enhances the forex rate assistant which we created interactively in the blog postAdding an Amazon Bedrock Knowledge Base to the Forex Rate Assistant. I hope the explanations on key points and solutions to various issues in this blog post help you fast-track your IaC development for Amazon Bedrock solutions.I will continue to evaluate different features of Amazon Bedrock, such asGuardrails for Amazon Bedrock, and streamlining the data ingestion process for knowledge bases. Please look forward for more helpful content on this topic as well as many others in theAvangards Blog. Happy learning!"}
{"title": "A step-by-step guide on how to use the Amazon Bedrock Converse API", "published_at": 1717349936, "tags": ["python", "aws", "tutorial", "ai"], "user": "Thomas Taylor", "url": "https://dev.to/aws-builders/a-step-by-step-guide-on-how-to-use-the-amazon-bedrock-converse-api-2mnl", "details": "On May 30th, 2024, Amazon announced therelease of the Bedrock Converse API. This API is designed to provide a consistent experience for \"conversing\" with Amazon Bedrock models.The API supports:Conversations with multiple turnsSystem messagesTool useImage and text inputIn this post, we'll walk through how to use the Amazon Bedrock Converse API with the Claude Haiku foundational model.Please keep in mind that not all foundational models may support all the features of the Converse API. For more details per model, see theAmazon Bedrock documentation.Getting startedTo get started, let's install theboto3package.pipinstallboto3Enter fullscreen modeExit fullscreen modeNext, we'll create a new Python script and import the necessary dependencies.importboto3client=boto3.client(\"bedrock-runtime\")Enter fullscreen modeExit fullscreen modeStep 1 - Starting a conversationTo start off simple, let's send a single message to Claude.importboto3client=boto3.client(\"bedrock-runtime\")messages=[{\"role\":\"user\",\"content\":[{\"text\":\"What is your name?\"}]}]response=client.converse(modelId=\"anthropic.claude-3-haiku-20240307-v1:0\",messages=messages,)print(response)Enter fullscreen modeExit fullscreen modeThe response is a dictionary containing the response and other metadata information from the API. For more information regarding the output, please refer to theAmazon Bedrock documentation.For the purposes of this post, I'll print the response as JSON.{\"ResponseMetadata\":{\"RequestId\":\"6984dcf2-c6aa-4000-a3d6-22e34a43df12\",\"HTTPStatusCode\":200,\"HTTPHeaders\":{\"date\":\"Sun, 02 Jun 2024 14:54:08 GMT\",\"content-type\":\"application/json\",\"content-length\":\"222\",\"connection\":\"keep-alive\",\"x-amzn-requestid\":\"6984dcf2-c6aa-4000-a3d6-22e34a43df12\"},\"RetryAttempts\":0},\"output\":{\"message\":{\"role\":\"assistant\",\"content\":[{\"text\":\"My name is Claude. It's nice to meet you!\"}]}},\"stopReason\":\"end_turn\",\"usage\":{\"inputTokens\":12,\"outputTokens\":15,\"totalTokens\":27},\"metrics\":{\"latencyMs\":560}}Enter fullscreen modeExit fullscreen modeTo retrieve the contents of the message, we can access theoutputkey in the response.importboto3client=boto3.client(\"bedrock-runtime\")messages=[{\"role\":\"user\",\"content\":[{\"text\":\"What is your name?\"}]}]response=client.converse(modelId=\"anthropic.claude-3-haiku-20240307-v1:0\",messages=messages,)ai_message=response[\"output\"][\"message\"]output_text=ai_message[\"content\"][0][\"text\"]print(output_text)Enter fullscreen modeExit fullscreen modeOutput:My name is Claude. It's nice to meet you!Enter fullscreen modeExit fullscreen modeStep 2 - Continuing a conversationLet's continue the conversation by appending the AI's message to the original list of messages. This will allow us to have a multi-turn conversation.importboto3client=boto3.client(\"bedrock-runtime\")messages=[{\"role\":\"user\",\"content\":[{\"text\":\"What is your name?\"}]}]response=client.converse(modelId=\"anthropic.claude-3-haiku-20240307-v1:0\",messages=messages,)ai_message=response[\"output\"][\"message\"]messages.append(ai_message)# Let's ask another questionmessages.append({\"role\":\"user\",\"content\":[{\"text\":\"Can you help me?\"}]})response=client.converse(modelId=\"anthropic.claude-3-haiku-20240307-v1:0\",messages=messages,)print(response[\"output\"][\"message\"][\"content\"][0][\"text\"])Enter fullscreen modeExit fullscreen modeOutput:Yes, I'd be happy to try and help you with whatever you need assistance with. What can I help you with?Enter fullscreen modeExit fullscreen modeStep 3 - Using imagesThe Amazon Bedrock Converse API supports images as input. Let's send an image to Claude and see how it responds. I'll download animage of a cat from Wikipediaand send it to claude.For this example, I used therequestslibrary to download the image. If you don't have it installed, you can install it usingpip install requests.pipinstallrequestsEnter fullscreen modeExit fullscreen modeimportboto3importrequestsclient=boto3.client(\"bedrock-runtime\")messages=[{\"role\":\"user\",\"content\":[{\"text\":\"What is your name?\"}]}]response=client.converse(modelId=\"anthropic.claude-3-haiku-20240307-v1:0\",messages=messages,)ai_message=response[\"output\"][\"message\"]messages.append(ai_message)messages.append({\"role\":\"user\",\"content\":[{\"text\":\"Can you help me?\"}]})response=client.converse(modelId=\"anthropic.claude-3-haiku-20240307-v1:0\",messages=messages,)ai_message=response[\"output\"][\"message\"]messages.append(ai_message)image_bytes=requests.get(\"https://upload.wikimedia.org/wikipedia/commons/4/4d/Cat_November_2010-1a.jpg\").contentmessages.append({\"role\":\"user\",\"content\":[{\"text\":\"What is in this image?\"},{\"image\":{\"format\":\"jpeg\",\"source\":{\"bytes\":image_bytes}}},],})response=client.converse(modelId=\"anthropic.claude-3-haiku-20240307-v1:0\",messages=messages,)ai_message=response[\"output\"][\"message\"]print(ai_message)Enter fullscreen modeExit fullscreen modeOutput:{\"role\":\"assistant\",\"content\":[{\"text\":\"The image shows a domestic cat. The cat appears to be a tabby cat with a striped coat pattern. The cat is sitting upright and its green eyes are clearly visible, with a focused and alert expression. The background suggests an outdoor, snowy environment, with some blurred branches or vegetation visible behind the cat.\"}]}Enter fullscreen modeExit fullscreen modeAs you can see, the AI was able to identify the image as a cat and provide a detailed description of the image within a conversational context.Step 4 - Using a single toolFor this section, let's start a new conversation with Claude and provide tools it can use.importboto3client=boto3.client(\"bedrock-runtime\")tools=[{\"toolSpec\":{\"name\":\"get_weather\",\"description\":\"Get the current weather in a given location\",\"inputSchema\":{\"json\":{\"type\":\"object\",\"properties\":{\"location\":{\"type\":\"string\",\"description\":\"The city and state, e.g. San Francisco, CA\",},\"unit\":{\"type\":\"string\",\"enum\":[\"celsius\",\"fahrenheit\"],\"description\":\"The unit of temperature, either'celsius'or'fahrenheit'\",},},\"required\":[\"location\"],}},}},]messages=[{\"role\":\"user\",\"content\":[{\"text\":\"What is the weather like right now in New York?\"}],}]response=client.converse(modelId=\"anthropic.claude-3-haiku-20240307-v1:0\",messages=messages,toolConfig={\"tools\":tools},)print(response[\"output\"])Enter fullscreen modeExit fullscreen modeOutput:{\"message\":{\"role\":\"assistant\",\"content\":[{\"text\":\"Okay, let me check the current weather for New York:\"},{\"toolUse\":{\"toolUseId\":\"tooluse_rRwaOoldTeiRiDZhTadP0A\",\"name\":\"get_weather\",\"input\":{\"location\":\"New York, NY\",\"unit\":\"fahrenheit\"}}}]}}Enter fullscreen modeExit fullscreen modeThe output includes atoolUseobject that indicates to us, the developers, that the AI is using theget_weathertool to fetch the current weather in New York. We must now fulfill the tool request by responding with the weather information.But first, let's build a simplistic router that can handle the tool request and respond with the weather information.defget_weather(location:str,unit:str=\"fahrenheit\")->dict:return{\"temperature\":\"78\"}deftool_router(tool_name,input):matchtool_name:case\"get_weather\":returnget_weather(input[\"location\"],input.get(\"unit\",\"fahrenheit\"))case_:raiseValueError(f\"Unknown tool:{tool_name}\")Enter fullscreen modeExit fullscreen modeNow, let's update the code to handle the tool request and respond with the weather information.importboto3defget_weather(location:str,unit:str=\"fahrenheit\")->dict:return{\"temperature\":\"78\"}deftool_router(tool_name,input):matchtool_name:case\"get_weather\":returnget_weather(input[\"location\"],input.get(\"unit\",\"fahrenheit\"))case_:raiseValueError(f\"Unknown tool:{tool_name}\")client=boto3.client(\"bedrock-runtime\")tools=[{\"toolSpec\":{\"name\":\"get_weather\",\"description\":\"Get the current weather in a given location\",\"inputSchema\":{\"json\":{\"type\":\"object\",\"properties\":{\"location\":{\"type\":\"string\",\"description\":\"The city and state, e.g. San Francisco, CA\",},\"unit\":{\"type\":\"string\",\"enum\":[\"celsius\",\"fahrenheit\"],\"description\":\"The unit of temperature, either'celsius'or'fahrenheit'\",},},\"required\":[\"location\"],}},}},]messages=[{\"role\":\"user\",\"content\":[{\"text\":\"What is the weather like right now in New York?\"}],}]response=client.converse(modelId=\"anthropic.claude-3-haiku-20240307-v1:0\",messages=messages,toolConfig={\"tools\":tools},)ai_message=response[\"output\"][\"message\"]messages.append(ai_message)ifresponse[\"stopReason\"]==\"tool_use\":contents=response[\"output\"][\"message\"][\"content\"]forcincontents:if\"toolUse\"notinc:continuetool_use=c[\"toolUse\"]tool_id=tool_use[\"toolUseId\"]tool_name=tool_use[\"name\"]input=tool_use[\"input\"]tool_result={\"toolUseId\":tool_id}try:output=tool_router(tool_name,input)ifisinstance(output,dict):tool_result[\"content\"]=[{\"json\":output}]elifisinstance(output,str):tool_result[\"content\"]=[{\"text\":output}]# Add more cases, such as images, if neededelse:raiseValueError(f\"Unsupported output type:{type(output)}\")exceptExceptionase:tool_result[\"content\"]=[{\"text\":f\"An unknown error occurred:{str(e)}\"}]tool_result[\"status\"]=\"error\"message={\"role\":\"user\",\"content\":[{\"toolResult\":tool_result}]}messages.append(message)response=client.converse(modelId=\"anthropic.claude-3-haiku-20240307-v1:0\",messages=messages,toolConfig={\"tools\":tools},)print(response[\"output\"])Enter fullscreen modeExit fullscreen modeOutput:{\"message\":{\"role\":\"assistant\",\"content\":[{\"text\":\"According to the weather data, the current temperature in New York, NY is 78 degrees Fahrenheit.\"}]}}Enter fullscreen modeExit fullscreen modeGreat! We have successfully responded to the AI tool request and provided the weather information for New York.Step 5 - Using multiple toolsFor reference, I'm adapting theAnthropic AI Tool examplesfrom their documentation to the Bedrock Converse API.In the previous example, we only used one tool to fetch the weather information. However, we can use multiple tools in a single conversation.Let's add another tool to the conversation to fetch the current time and introduce a loop to handle multiple tool requests.importboto3defget_weather(location:str,unit:str=\"fahrenheit\")->dict:return{\"temperature\":\"78\"}defget_time(timezone:str)->str:return\"12:00PM\"deftool_router(tool_name,input):matchtool_name:case\"get_weather\":returnget_weather(input[\"location\"],input.get(\"unit\",\"fahrenheit\"))case\"get_time\":returnget_time(input[\"timezone\"])case_:raiseValueError(f\"Unknown tool:{tool_name}\")client=boto3.client(\"bedrock-runtime\")tools=[{\"toolSpec\":{\"name\":\"get_weather\",\"description\":\"Get the current weather in a given location\",\"inputSchema\":{\"json\":{\"type\":\"object\",\"properties\":{\"location\":{\"type\":\"string\",\"description\":\"The city and state, e.g. San Francisco, CA\",},\"unit\":{\"type\":\"string\",\"enum\":[\"celsius\",\"fahrenheit\"],\"description\":\"The unit of temperature, either'celsius'or'fahrenheit'\",},},\"required\":[\"location\"],}},}},{\"toolSpec\":{\"name\":\"get_time\",\"description\":\"Get the current time in a given timezone\",\"inputSchema\":{\"json\":{\"type\":\"object\",\"properties\":{\"timezone\":{\"type\":\"string\",\"description\":\"The IANA time zone name, e.g. America/Los_Angeles\",}},\"required\":[\"timezone\"],}},}},]messages=[{\"role\":\"user\",\"content\":[{\"text\":\"What is the weather like right now in New York and what time is it there?\"}],}]response=client.converse(modelId=\"anthropic.claude-3-haiku-20240307-v1:0\",messages=messages,toolConfig={\"tools\":tools},)ai_message=response[\"output\"][\"message\"]messages.append(ai_message)tool_use_count=0whileresponse[\"stopReason\"]==\"tool_use\":ifresponse[\"stopReason\"]==\"tool_use\":contents=response[\"output\"][\"message\"][\"content\"]forcincontents:if\"toolUse\"notinc:continuetool_use=c[\"toolUse\"]tool_id=tool_use[\"toolUseId\"]tool_name=tool_use[\"name\"]input=tool_use[\"input\"]tool_result={\"toolUseId\":tool_id}try:output=tool_router(tool_name,input)ifisinstance(output,dict):tool_result[\"content\"]=[{\"json\":output}]elifisinstance(output,str):tool_result[\"content\"]=[{\"text\":output}]# Add more cases such as images if neededelse:raiseValueError(f\"Unsupported output type:{type(output)}\")exceptExceptionase:tool_result[\"content\"]=[{\"text\":f\"An unknown error occurred:{str(e)}\"}]tool_result[\"status\"]=\"error\"message={\"role\":\"user\",\"content\":[{\"toolResult\":tool_result}]}messages.append(message)response=client.converse(modelId=\"anthropic.claude-3-haiku-20240307-v1:0\",messages=messages,toolConfig={\"tools\":tools},)ai_message=response[\"output\"][\"message\"]messages.append(ai_message)tool_use_count+=1print(tool_use_count)print(response[\"output\"])Enter fullscreen modeExit fullscreen modeOutput:{\"message\":{\"role\":\"assistant\",\"content\":[{\"text\":\"The current time in New York is 12:00 PM.\\n\\nSo in summary, the weather in New York right now is 78 degrees Celsius, and the time is 12:00 PM.\"}]}}Enter fullscreen modeExit fullscreen modeTool use count:2We have successfully responded to the AI's tool requests and provided the weather and time information for New York.ConclusionIn this post, we learned how to use the Amazon Bedrock Converse API to augment converations with AI models. Not only did we leverage text and images, but we also used tools to simulate fetching external data.The Amazon Bedrock Converse API is a powerful tool that can be used to build conversational AI applications!"}
{"title": "Executing long running tasks with AppSync", "published_at": 1717330317, "tags": ["lambda", "eventbridge", "appsync", "serverless"], "user": "Mohammed", "url": "https://dev.to/aws-builders/executing-long-running-tasks-with-appsync-1ikk", "details": "AWS recentlyannounceda small (but notable) feature for the AppSync service where a data source associated with a Lambda can invoke the function asynchronously. Before this, AppSync was to only process all requests synchronously.How does it helpThe current update opens the door to handling certain situations which weren\u2019t possible earlier (or at least they were not as straight-forward to implement).Imagine when you have specific AppSync mutations that take more than 30 seconds to process owing to technical constraints. Since you always need to return a synchronous response, the way to deal with this would typically involve offloading the actual processing of the request to another function either by passing the payload via an SQS (provided payload is under 256KB) or doing a direct asynchronous invocation of the other lambda function and then returning a generic response by the Lambda function associated with the resolver. By offloading the actual processing to another function, the 30 second timeout limitation has been handled.Assuming the caller needs the final result of that long-running task, it would also need to make a subscription call so that it can receive the response once it has been completed. Triggering a mutation call (without an actual data source) is needed to deliver the subscription response.Specifying \u201cEvent\u201d invocation method type now allows calling the function asynchronously. So now the additional step of handing off the actual processing through an SQS/Lambda can be eliminated.The setupWith in the JS resolver request function, all we need to do is specify the invocationType attribute. You would already be familiar with this when a function is invoked using the AWS SDK/CLI.The response function of the resolver can return a static response to indicate to the caller that AppSync has received the request for processing.export function request(ctx) {     return {       operation: \"Invoke\",       invocationType: \"Event\",       payload: ctx,     }; }  export function response(ctx) {   return \"Received request\"; }Enter fullscreen modeExit fullscreen modeThings to keep in mindAsynchronous lambda invocations cannot have payload sizes more than 256KB. So if your existing synchronous AppSync request has a payload size beyond this value, switching to async mode is not going to be possible.Likewise, for the response, an AppSync request (query/mutation) can return a payload with a size limit of 5MB.The flow indicated above assumes the caller needs the result after the long-running task has finished processing. Eventbridge pipes is a great tool leveraging AWS infrastructure for triggering the mutation call whose response is the result for a Subscription request.With the use of Subscriptions we are able to deliver the final payload, but the payload size cannot exceed 240 KB. This isn\u2019t something new but just to keep in mind that even though AWS enabled asynchronous request processing with AppSync, the final act of delivering a large payload is still a catch."}
{"title": "Deep Dive on Amazon Elastic MapReduce Service Platform with Amazon EC2 Instance", "published_at": 1717323049, "tags": ["amazonemr", "s3bucket", "iamrole", "ec2instance"], "user": "GargeeBhatnagar", "url": "https://dev.to/aws-builders/deep-dive-on-amazon-elastic-mapreduce-service-platform-with-amazon-ec2-instance-1ohi", "details": "\u201c I have checked the documents of AWS to get into deep dive on amazon elastic mapreduce service platform with amazon ec2 instance. In terms of cost, need to pay for emr service, amount of storage  and data transferred in and out of service for s3 bucket, amazon ec2 instance.\u201dAmazon Elastic MapReduce is a managed cluster platform that simplifies running big data frameworks, such as Apache Hadoop and Apache Spark, on AWS to process and analyze vast amounts of data. Using these frameworks and related open-source projects, you can process data for analytics purposes and business intelligence workloads. Amazon EMR also lets you transform and move large amounts of data into and out of other AWS data stores and databases, such as Amazon Simple Storage Service and Amazon Dynamodb.In this post, you will experience how to deep dive on amazon elastic mapreduce service platform with amazon ec2 instance. Here I have created an amazon emr service cluster with iam roles, key pair and s3 bucket.Architecture OverviewThe architecture diagram shows the overall deployment architecture with data flow, amazon emr service, s3 bucket, iam service role, ec2 instances.Solution overviewThe blog post consists of the following phases:Create of Amazon EMR Service Cluster with Required ConfigurationsOutput of Emr Cluster as Submit of Spark Application as a Step OptionPhase 1: Create of Amazon EMR Service Cluster with Required ConfigurationsCreate a key pair, iam roles and s3 bucket with required data. Open the console of Amazon emr service, create a cluster with amazon emr running on amazon ec2 option. Specify the cluster name as Emr Cluster and choose the required parameters as choice of application, instance type, ebs volume, networking, cluster logs s3 location, key pair, emr service role, ec2 instance profile for emr role.Phase 2: Output of Emr Cluster as Submit of Spark Application as a Step OptionClean-upDelete of Amazon EMR Cluster, IAM Roles, S3 Bucket, Key Pair.PricingI review the pricing and estimated cost of this example.Cost of Amazon Elastic MapReduce service = $0.048 per hour for EMR m5.xlarge = $(0.048x1.086) = $0.05Cost of Amazon Elastic Compute Cloud = $0.192 per On Demand Linux m5.xlarge Instance Hour = $(0.192x1.334) = $0.26Cost of Amazon Simple Storage Service = $0.0Total Cost = $0.31SummaryIn this post, I showed \u201chow to deep dive on amazon elastic mapreduce service platform with amazon ec2 instance\u201d.For more details on Amazon EMR Service, Checkout Get started Amazon EMR Service, open theAmazon EMR Service console. To learn more, read theAmazon EMR Service documentation.Thanks for reading!Connect with me:Linkedin"}
{"title": "How to package and deploy a Lambda function as a container image", "published_at": 1717314879, "tags": ["containers", "aws", "lambda", "devops"], "user": "saifeddine Rajhi", "url": "https://dev.to/aws-builders/how-to-package-and-deploy-a-lambda-function-as-a-container-image-3d1a", "details": "Introduction:AWS Lambda allows you to run your code inside Docker containers! This feature opens up a lot of possibilities. You can use any programming language or framework by packaging it into a container image.There are no limitations from Lambda's built-in runtimes. Deploying Lambda functions also becomes much easier.Instead of zipping up code and dependencies, we can build a Docker image with everything our function needs. Just push the image to Amazon's container registry and configure Lambda to use it.In this blog post, we'll show you how to create a Lambda function that runs in a Docker container.Amazon ECR:Amazon Elastic Container Registry (ECR) is a fully managed container registry provided by AWS. It allows you to store, manage, and deploy container images securely.Key points about ECR:Eliminates the need to operate your own container repository.Integrates natively with other AWS services like ECS, EKS, and Lambda.Provides high-performance hosting for your images.Automatically encrypts images at rest and transfers them over HTTPS.Offers resource-based permissions and lifecycle policies for images.Using ECR with AWS\u00a0Lambda:Traditionally, you deployed Lambda functions by uploading a ZIP file containing your code and dependencies. However, AWS now allows you to package your Lambda function code as a container image and deploy it from ECR.The main benefits of using container images for Lambda functions include:Use any programming language, framework, or base image by customizing the container.Larger deployment packages up to 10GB compared to 50MB zipped archives.Easier dependency management and code organization within the container.Consistent build and deployment process across different environments.To use a container image for your Lambda function, you first build and push the Docker image to an ECR repository. Then, you create the Lambda function and specify the ECR image URI as the deployment package.Hands-On walkthrough:To show how this works in practice, this walkthrough uses an environment with the AWS CLI, python, and Docker installed.Create aDockerfilefile and paste the following code:FROM amazon/aws-lambda-python:3.10  COPY app.py ./  CMD [\"app.handler\"]Enter fullscreen modeExit fullscreen modeCreateapp.pyas lambda handler:import sys def handler(event, context):     return 'Hello from AWS Lambda using Python container image' + sys.version + '!'Enter fullscreen modeExit fullscreen modeNext, run the following command in the console to authenticate to the ECR registry.Replace with the account ID of the AWS account you are using and with the code of the region you are using.aws ecr get-login-password --region <Region> | docker login --username AWS --password-stdin <Account-ID>.dkr.ecr.<Region>.amazonaws.comEnter fullscreen modeExit fullscreen modeCreate a new repository in ECR and push the Docker image to the repo.aws ecr create-repository --repository-name demo-lambda --image-scanning-configuration scanOnPush=trueEnter fullscreen modeExit fullscreen modeAfter that, run the following command to create the container image:docker build -t demo-lambda .Enter fullscreen modeExit fullscreen modeThen, run the following commands to tag and push the image:docker tag get-customer:latest <Account-ID>.dkr.ecr.<Region>.amazonaws.com/get-customer:latest  docker push <Account-ID>.dkr.ecr.<Region>.amazonaws.com/get-customer:latestEnter fullscreen modeExit fullscreen modeNext step is to create the Lambda function:aws lambda create-function \\     --function-name demo-lambda \\     --package-type Image \\     --code ImageUri=<Account-ID>.dkr.ecr.<Region>.amazonaws.com/demo-lambda:latest \\     --role arn:aws:iam::<Account-ID>:role/execution_role \\     --region <Region> \\     --timeout 15 \\     --memory-size 512 \\     --description \"A Lambda function created using a container image.\"Enter fullscreen modeExit fullscreen mode--function-name: The name you want to assign to your Lambda function (demo-lambda).--package-type Image: Specifies that the deployment package is a container image.--code ImageUri=...: The URI of the container image in ECR.--role: The ARN of the IAM role that Lambda assumes when it executes your function. Ensure this role has the necessary permissions.--region: The AWS region where you want to create the Lambda function.--timeout: The maximum execution time in seconds for the function.--memory-size: The amount of memory available to the function at runtime.--description: A description of the function.Make sure to replace placeholders like with your actual values.After, you can invoke the container image as a Lambda function:$ aws lambda invoke --function-name demo-lambda --region <Region> outfile                                                                                                                           {     \"StatusCode\": 200,     \"ExecutedVersion\": $LATEST\" } $ cat outfile                                                                                                                                                                                                     \"Hello from AWS Lambda using Python3.10.2 (default, June 01 2024, 09:22:02) \\n[GCC 8.3.0]!\"Enter fullscreen modeExit fullscreen modeConclusionThis blog post showed an easy way to use Docker containers with AWS Lambda functions.By building a Docker image with your code, you can run any programming language or tools on Lambda. Deploying is simple\u200a-\u200ajust upload your container image to AWS. Give it a try to make Lambda more flexible for your apps!Thank you for Reading\u00a0!! \ud83d\ude4c\ud83c\udffb\ud83d\ude01\ud83d\udcc3, see you in the next blog.\ud83e\udd18\ud83d\udd30 Keep Learning\u00a0!! Keep Sharing\u00a0!!\u00a0\ud83d\udd30References:https://dev.to/vumdao/deploy-python-lambda-functions-with-container-image-5hgjhttps://www.pluralsight.com/resources/blog/cloud/packaging-aws-lambda-functions-as-container-imageshttps://community.aws/content/2Z4KyWJP5qXDD6StOWJZFXzoRZq/creating-a-lambda-function-with-a-container-based-runtime"}
{"title": "Provisioning AWS solutions in minutes with Infra as Github Actions", "published_at": 1717191600, "tags": ["aws", "githubactions", "terraform", "devops"], "user": "Alonso Suarez", "url": "https://dev.to/aws-builders/provisioning-aws-solutions-in-minutes-with-infra-as-github-actions-c25", "details": "I remember those days when I created infra by clicking in the console \ud83d\ude2c eventually that became a nightmare to manage and infra as code came to save the day \ud83e\uddb8 but with that, it also started an awkward phase of coupling infra code and app codeSome platform teams decided to move infra code to their own repos, that worked, specially for access control, but that required exceptions like:Ignoring the image version since it was going to be managed by the app pipelineThe awkward environment variables setupI've been searching for ways to bring infra back infra code to app reposThe first experiment was to keep network and LBs and move ECS services with Fargate to the app repo, this continuous to work surprisingly well after 3 years, environment variables changes are in the same PR as app that depended on it, another advantage was that terraform itself managed the newly built docker image tag. However, app developers rarely touched most of the terraform code, as terraform requires significant effort to learn, there has to be a better way \ud83e\udd14Entering Infra as GitHub ActionsWith just a few line of YML, we can create pretty complex depended workflows, what if we can use Github Actions to compose infra?I write a couple of actions that will provision the S3, CloudFront and Route53 with fairly simple steps:Login to AWSSetup the BackendProvision the websiteThe whole workflow relies on only 3 inputs- Instance name: an identifier for the infra- Domain: DNS for the website, you need to own it on Route53- Path: content to publish as root on the websitepermissions:    id-token: write jobs:   deploy:     runs-on: ubuntu-latest     steps:       - name: Check out repo         uses: actions/checkout@v4       - uses: aws-actions/configure-aws-credentials@v4         with:           aws-region: us-east-1           role-to-assume: ${{ secrets.ROLE_ARN }}           role-session-name: ${{ github.actor }}       - uses: alonch/actions-aws-backend-setup@main         with:            instance: demo       - uses: alonch/actions-aws-website@main         with:            domain: ${{ env.DOMAIN }}           content-path: publicEnter fullscreen modeExit fullscreen modeHow it worksWhen a GitHub Action job is initialized, it checks out its source code from the repo, this unlocks interesting capabilities, for example: an action could apply terraform as if the terraform code is part of the current repoactions-aws-backend-setup (repo)- uses: alonch/actions-aws-backend-setup@main     with:        instance: demoEnter fullscreen modeExit fullscreen modeThis action requires a custom instance name to query AWS by tag, we need to find the S3 and Dynamodb to setup the terraform backendIn case where it doesn\u2019t exist, it will provision it and setup the environment variables:TF_BACKEND_s3: bucket nameTF_BACKEND_dynamodb: table nameactions-aws-website (repo)- uses: alonch/actions-aws-website@main         with:            domain: ${{ env.DOMAIN }}           content-path: publicEnter fullscreen modeExit fullscreen modeThis action assumes the backend setup has run, and it requires a domain and the path to the content that needs to be publish as the websiteSimilar to the backend action, GitHub checks the source code which includes the terraform code to provision a Bucket, Cloudfront, Certificate and route53 routesUsing Github Actions is incredible flexible as the only dependency is on the AWS role and the backend instance name, in the case were we need to upgrade the infra, we just need to tag the new action version and terraform will sync the resources to the latest desired stateNew degree of freedomWith Infra as Github Actions we basically can achieve ephemeral environments with a 31 lines of YMLname: Deploy Ephemeral Environment on:   pull_request:     types: [opened, synchronize, reopened, closed] env:    DOMAIN: ${{github.head_ref}}.test.realsense.ca permissions:    id-token: write jobs:   deploy:     environment:       url: \"https://${{ env.DOMAIN }}\"       name: ${{github.head_ref}}     runs-on: ubuntu-latest     steps:       - name: Check out repo         uses: actions/checkout@v4       - uses: aws-actions/configure-aws-credentials@v4         with:           aws-region: us-east-1           role-to-assume: ${{ secrets.ROLE_ARN }}           role-session-name: ${{ github.actor }}       - uses: alonch/actions-aws-backend-setup@main         with:            instance: demo       - uses: alonch/actions-aws-website@main         with:            domain: ${{ env.DOMAIN }}           content-path: public           # destroy when PR closed           action: ${{github.event.action == 'closed' && 'destroy' || 'apply'}}Enter fullscreen modeExit fullscreen modeThis will create a new infra for that PR, update it in sync and destroy it when the PR is closedWhat\u2019s next?This is just the beginning, I believe GitHub actions dependency could be use to compose and orchestre complex infra with simple interfacesI'm considering building:actions-aws-http-lambda: Serverless API from folder pathactions-aws-edge-auth: Website behind social media login from client secretsactions-aws-http-server: Web hosting from docker imageWhat do you think? What should I focus on next?"}
{"title": "AWS DevOps Projects List 2024", "published_at": 1717113600, "tags": ["devops", "docker", "kubernetes", "aws"], "user": "Yashvi Kothari", "url": "https://dev.to/aws-builders/aws-devops-projects-list-2024-41fn", "details": "Here are some mini projects and exercises related to various areas in DevOps, SRE, Platform Engineering, Cloud Infrastructure, and Security.Linux Projects:Linux System Administration:Story Problem: You work for a web hosting company. A client's website is experiencing slow performance. Investigate the server logs, identify bottlenecks, optimize configurations, and improve overall system performance.AWS Solution Architect Projects:Story Problem: A startup wants to deploy a scalable web application on AWS. Design an architecture that ensures high availability, fault tolerance, and efficient resource utilization. Consider EC2, S3, RDS, VPC, Route 53, and CloudWatch.Jenkins OR Circle CI CI/CD Projects:Story Problem: A software development team wants to automate their deployment process. Set up a CI/CD pipeline using Jenkins or CircleCI. Include stages for building, testing, and deploying code to production.IT Resource Monitoring and Alerting: Incident Management:Story Problem: Your company's critical application experiences downtime. Implement monitoring using New Relic APM or AppDynamics. Set up alerts for performance degradation and incident response workflows.APM Tools: New Relic APM, AppDynamics, AWS CloudWatch, Sensu, Nagios, Zabbix, Icinga, Pingdom, Pagerduty (call and SMS), AWS SNS/SES:Story Problem: A large e-commerce platform faces intermittent outages. Choose an APM tool and configure it to monitor critical services. Set up alerting and incident management.Logging Security: Wazuh, Elasticsearch, Logstash, Kibana Stack, Grafana, and Prometheus Tools:Story Problem: A security breach occurs. Implement Wazuh for intrusion detection, centralize logs using the ELK stack (Elasticsearch, Logstash, Kibana), and visualize security metrics with Grafana and Prometheus.Docker Projects:Story Problem: A development team wants to containerize their application. Create Docker images for a web app and a database. Set up a Docker Compose file to orchestrate the containers.AWS ECS Service Projects (How to build the Docker image, push to Docker repo, configure Task definition, Service, CloudWatch log, Scaling of Docker service, Request route from Route53 to Load balance (Path-based routing) to EC2 server to Docker container):Story Problem: A company plans to migrate its microservices to AWS ECS (Elastic Container Service). Build Docker images, set up ECS services, configure auto-scaling, and ensure proper routing using Route 53.Kubernetes Projects:Story Problem: A startup wants to deploy applications on Kubernetes. Set up a local Minikube cluster for development. Explore Kubernetes concepts like pods, services, and deployments. Prepare for the Certified Kubernetes Administrator (CKA) or Certified Kubernetes Application Developer (CKAD) exam.Feel free to share github repos and make projects opensource and contribute for hands-on experience! \ud83d\ude0a"}
{"title": "Amazon OpenSearch", "published_at": 1716998985, "tags": ["aws", "opensearch", "analytics", "observability"], "user": "Vijayaraghavan Vashudevan", "url": "https://dev.to/aws-builders/amazon-opensearch-2di6", "details": "\ud83c\udf40 Overview of Amazon OpenSearch\ud83d\udd17Amazon OpenSearch is a managed service provided by AWS that makes it easy to deploy, operate, and scale OpenSearch clusters. OpenSearch is an open-source search and analytics suite derived from Elasticsearch and Kibana, offering full-text search capabilities, structured search, and various data visualization tools.\ud83c\udf32 How does it work?\ud83d\udcb2 PricingFor customers in the AWS Free Tier, OpenSearch Service provides free usage of up to 750 hours per month of a t2.small.search or t3.small.search instancePricing\u2708\ufe0f Hands-on Demo\u267b\ufe0fInstructions to clean up AWS resource to avoid Billing\u2714\ufe0f Delete the domain cluster you had createdThanks for being patient and followed me. Keep supporting \ud83d\ude4fGive \ud83d\udc9a if you liked the blogFor more exercises \u2014 pls do follow me below \u2705!https://www.linkedin.com/in/vijayaraghavanvashudevan/"}
{"title": "Robust API Retry Mechanism with AWS Step Functions and Lambda", "published_at": 1716985875, "tags": ["aws", "lambda", "stepfunctions", "softwaredevelopment"], "user": "Somil Gupta", "url": "https://dev.to/aws-builders/robust-api-retry-mechanism-with-aws-step-functions-and-lambda-4lap", "details": "I have been working with external API calls for a while and have noticed they can sometimes fail for various reasons, such as network issues, server downtime, or rate limits on the server. So, I have built this solution to have a robust system to tackle this problem.In this solution, we will leverage the AWS Step Function and Lambda Functions to construct a reliable retry mechanism. The State Machine will consist of a collection of Lambda functions invoked and stitched together to produce results. This article will walk you through the step-by-step guide.The main objective we are trying to solve:While Step Functions inherently support retries within tasks, our specific challenge involves handling API rate limits from the server we are communicating with. The server imposes a rate limit and responds with a 429 status code if too many requests are made from the same IP address within a short period.PrerequisitesAWS AccountBasic understanding of AWS Lambda and Step Functions1. Architecture:Workflow ExplanationUser Invokes Step Function State Machine: The process begins when a user initiates the step function state machine. This could be triggered through an API call, a scheduled event, or another AWS service.Step Function Invokes Lambda (1st Attempt): The step function invokes the first Lambda function (Lambda 1). This Lambda function is responsible for making the API call.Response: Status: Lambda 1 Executes the API call and returns a status response. This response indicates whether the API call was successful (e.g., status code 200) or failed (e.g., any status code other than 200).If Failure Status \u2260 200 (2nd Attempt): If the response from Lambda 1 If it indicates a failure (status code not equal to 200), the step function will proceed to invoke a retry mechanism. This could involve retrying the same Lambda function or invoking a different Lambda function (Lambda 2) to handle the retry attempt.Response: Status: Lambda 2 It attempts to execute the API call and returns a status response. Similar to the first Attempt, this response will indicate whether the retry was successful.If Success Status = 200: If either Lambda 1 or Lambda 2 Successfully executes the API call and returns a status code of 200, the step function completes successfully, and the user is notified of the success.If Failure Even After Retries: Then we will fail the step function and forward the API error to the user with the appropriate status code.To explain the architecture easily, I have created the above diagram with one retry only, but we will build the solution with two retries. Below is the state machine diagram.2. Step-by-Step GuideCreate a base lambda function:This lambda function will help us in orchestrating the state machine. Executing the state machine and handling logic based on the execution status.importboto3importjsonimporttimedefstart_state_machine(body):# Create a session with AWS credentialssession=boto3.Session(aws_access_key_id='',aws_secret_access_key='',region_name='')# Create a client to interact with AWS Step Functionsstep_functions_client=session.client('stepfunctions')# Define the ARN of the Step Function that you want to startstate_machine_arn='arn:aws:states::stateMachine:apiProxyStateMachine'# Define the input to pass to the Step Functioninput_data=body# Start the Step Function with the specified inputresponse=step_functions_client.start_execution(stateMachineArn=state_machine_arn,input=json.dumps(input_data))# Wait for the execution to completewhileTrue:execution_status=step_functions_client.describe_execution(executionArn=response['executionArn'])['status']ifexecution_statusin['SUCCEEDED','FAILED','ABORTED']:breakexecution_output=step_functions_client.describe_execution(executionArn=response['executionArn'])if(execution_output['status']=='SUCCEEDED'):returnexecution_output['output']else:returnexecution_output['status']deflambda_handler(event,context):event=event[\"body\"]data=start_state_machine(event)response=json.loads(data)return{\"statusCode\":response[\"statusCode\"],\"body\":response[\"body\"]}Enter fullscreen modeExit fullscreen modeCreate a function URL for the lambda function:Now that the lambda function is ready, we can set up a function URL to trigger/send a request to the lambda function using it. Refer to the article below to turn any lambda function into an API with a function URL.How to use AWS Lambda to trigger \u201cany\u201d script as an API call | by Somil Gupta | Technology Hits | MediumSomil Gupta \u30fbNov 10, 2023\u30fbMediumCreate child lambda functions:These will be simple lambda functions acting as a proxy; they will not handle any logic.importjsonimportrequestsdeflambda_handler(event,context):api_url=\"https://api.example.com/data\"try:response=requests.get(api_url)response.raise_for_status()return{'statusCode':200,'body':json.dumps(response.json())}exceptrequests.exceptions.RequestExceptionase:return{'statusCode':response.status_codeifresponseelse500,'body':json.dumps({'error':str(e)})}Enter fullscreen modeExit fullscreen modeWe have to create the same three lambda functions using the above code.Define Step Function State Machine:Next, we'll create a Step Functions state machine with a retry mechanism. Here is an example definition in JSON.Click to expand{\"Comment\":\"A description of my state machine\",\"StartAt\":\"Proxy 1\",\"States\":{\"Proxy 1\":{\"Type\":\"Task\",\"Resource\":\"arn:aws:states:::lambda:invoke\",\"OutputPath\":\"$.Payload\",\"Parameters\":{\"Payload.$\":\"$\",\"FunctionName\":\"arn:aws:lambda:ap-south-1:378343485419:function:apiProxy1:$LATEST\"},\"Retry\":[{\"ErrorEquals\":[\"Lambda.ServiceException\",\"Lambda.AWSLambdaException\",\"Lambda.SdkClientException\",\"Lambda.TooManyRequestsException\"],\"IntervalSeconds\":2,\"MaxAttempts\":6,\"BackoffRate\":2}],\"Next\":\"Pass\"},\"Pass\":{\"Type\":\"Pass\",\"Next\":\"Choice\"},\"Choice\":{\"Type\":\"Choice\",\"Choices\":[{\"Variable\":\"$.statusCode\",\"NumericEquals\":200,\"Next\":\"Pass (1)\"}],\"Default\":\"Pass (7)\"},\"Pass (7)\":{\"Type\":\"Pass\",\"Next\":\"Wait\"},\"Wait\":{\"Type\":\"Wait\",\"Seconds\":5,\"Next\":\"Proxy 2\"},\"Pass (1)\":{\"Type\":\"Pass\",\"Next\":\"Success - 1\"},\"Success - 1\":{\"Type\":\"Succeed\"},\"Proxy 2\":{\"Type\":\"Task\",\"Resource\":\"arn:aws:states:::lambda:invoke\",\"OutputPath\":\"$.Payload\",\"Parameters\":{\"Payload.$\":\"$\",\"FunctionName\":\"arn:aws:lambda:ap-south-1:378343485419:function:apiProxy2:$LATEST\"},\"Retry\":[{\"ErrorEquals\":[\"Lambda.ServiceException\",\"Lambda.AWSLambdaException\",\"Lambda.SdkClientException\",\"Lambda.TooManyRequestsException\"],\"IntervalSeconds\":2,\"MaxAttempts\":6,\"BackoffRate\":2}],\"Next\":\"Pass (8)\"},\"Pass (8)\":{\"Type\":\"Pass\",\"Next\":\"Choice (1)\"},\"Choice (1)\":{\"Type\":\"Choice\",\"Choices\":[{\"Variable\":\"$.statusCode\",\"NumericEquals\":200,\"Next\":\"Pass (2)\"}],\"Default\":\"Pass (3)\"},\"Pass (3)\":{\"Type\":\"Pass\",\"Next\":\"Proxy 3\"},\"Pass (2)\":{\"Type\":\"Pass\",\"Next\":\"Success -2\"},\"Success -2\":{\"Type\":\"Succeed\"},\"Proxy 3\":{\"Type\":\"Task\",\"Resource\":\"arn:aws:states:::lambda:invoke\",\"OutputPath\":\"$.Payload\",\"Parameters\":{\"Payload.$\":\"$\",\"FunctionName\":\"arn:aws:lambda:ap-south-1:378343485419:function:apiProxy3:$LATEST\"},\"Retry\":[{\"ErrorEquals\":[\"Lambda.ServiceException\",\"Lambda.AWSLambdaException\",\"Lambda.SdkClientException\",\"Lambda.TooManyRequestsException\"],\"IntervalSeconds\":2,\"MaxAttempts\":6,\"BackoffRate\":2}],\"Next\":\"Pass (4)\"},\"Pass (4)\":{\"Type\":\"Pass\",\"Next\":\"Choice (2)\"},\"Choice (2)\":{\"Type\":\"Choice\",\"Choices\":[{\"Variable\":\"$.statusCode\",\"NumericEquals\":200,\"Next\":\"Pass (5)\"}],\"Default\":\"Pass (6)\"},\"Pass (6)\":{\"Type\":\"Pass\",\"Next\":\"Failure\"},\"Pass (5)\":{\"Type\":\"Pass\",\"Next\":\"Success - 3\"},\"Success - 3\":{\"Type\":\"Succeed\"},\"Failure\":{\"Type\":\"Succeed\"}}}Enter fullscreen modeExit fullscreen mode3. Testing the State\u00a0MachineTrigger the state machine execution using the first lambda function URL and monitor it through the AWS State Machine Console. You should see the retries and the final result, whether it succeeds or fails.Conclusion -Implementing a robust API retry mechanism using AWS Step Functions and Lambda is a powerful way to enhance the reliability of your API integrations. I have worked too much with the vendor APIs, and their reliability is something you can not trust. They have rate limits, server IP-based wait times, and so on. This retry using different lambda functions will give us different server URLs, preventing IP-based wait time blocking plus the retry mechanism.This solution provides a visual workflow to monitor and debug your API calls. With AWS Step Functions and Lambda, you can build a fault-tolerant API integration with minimal effort.Thanks for reading the tutorial. I hope you learn something new today. If you want to read more stories like this, I invite you to follow me.Till then, Sayonara! I wish you the best in your learning journey."}
{"title": "Building a serverless connected BBQ as SaaS - Part 1", "published_at": 1716979835, "tags": ["aws", "serverless", "iot"], "user": "Jimmy Dahlqvist", "url": "https://dev.to/aws-builders/building-a-serverless-connected-bbq-as-saas-part-1-59gn", "details": "This post is the start of a series of post on how I built a IoT connected BBQ as SaaS. This first post will kick everything off by briefly introducing the used hardware and AWS GreenGrass, which will be used to connect and send data to the cloud. In the coming post the architecture will be introduced followed be deep dive technical posts on each part of the architecture.In the end I will have created a Serverless connected BBQ smoker as a SaaS solution. There will be deep dives into IoT setup, on-boarding, user management, data and tenant isolation.HW ComponentsFirst, let's go over the required HW and what I'll be using in this series. I will be using aRaspberry Pi 3 model Bthat will connect to aInkbird IBT-6xsover bluetooth LE.However in this first post we'll setup a simulated device running on a small EC2 instance.AWS Greengrass CoreAWS IoT Greengrass is an edge runtime and cloud service for building, deploying, and managing device software in an easy way. It support creation of custom software components that can easily be deployed to any device running Greengrass.Greengrass Development KitTo make development a bit easier I will be usingGreengrass Development Kitto build new versions of the component. GDK however doesn't support AWS Identity Center and CLI profiles, making it impossible to publish new versions using this tool. Instead I will be manually creating new versions.Initialize a componentFirst of I use GDK to initialize a new component. To do this run theinit command. There is a need to specify the used language and template to base the component of. Available templates can be found inGitHubI will be using Python and base the component of HelloWorld template. I will also supply a name for it, this will create a new folder where all files are stored. Make sure the folder do not exist.gdk component init-lpython-tHelloWorld-nHelloBBQEnter fullscreen modeExit fullscreen modeIn theHelloBBQfolder there is now a configuration filegdk-config.jsonthis need to be updated and the PLACEHOLDER values changed, and the component name specified.{\"component\":{\"com.example.hellobbq\":{\"author\":\"<PLACEHOLDER_NAME>\",\"version\":\"1.0.0\",\"build\":{\"build_system\":\"zip\",\"options\":{\"zip_name\":\"\"}},\"publish\":{\"bucket\":\"<PLACEHOLDER_BUCKET>\",\"region\":\"<PLACEHOLDER_REGION>\"}}},\"gdk_version\":\"1.3.0\"}Enter fullscreen modeExit fullscreen modeAdd device codeUpdate the main.py file and add code to connect and publish fake temperatures to IoT Core on a topice/{THING_NAME}/dataimportosimporttimeimportuuidimportrandomimportjsonimportawsiot.greengrasscoreipc.clientv2asclientV2fromawsiot.greengrasscoreipc.clientv2importGreengrassCoreIPCClientV2fromdecimalimportDecimal# MQTTIPC_CLIENT:GreengrassCoreIPCClientV2=clientV2.GreengrassCoreIPCClientV2()THING_NAME=os.getenv(\"AWS_IOT_THING_NAME\")OPERATIONS_TOPIC=f\"c/{THING_NAME}/operation\"DATA_TOPIC=f\"e/{THING_NAME}/data\"QOS=\"1\"SESSION=str(uuid.uuid4())defpublishToTopic(topic,payload):IPC_CLIENT.publish_to_iot_core(topic_name=topic,qos=QOS,payload=payload)defcreateRandomDecimal():returnrandom.randint(0,1000)/10defgenerateSimulatedTemperature():tempProbe0=str(round(Decimal(createRandomDecimal()),2))tempProbe1=str(round(Decimal(createRandomDecimal()),2))tempProbe2=str(round(Decimal(createRandomDecimal()),2))tempProbe3=str(round(Decimal(createRandomDecimal()),2))tempProbe4=str(round(Decimal(createRandomDecimal()),2))tempProbe5=str(round(Decimal(createRandomDecimal()),2))# Add the temperatures to the arraytemps=[tempProbe0,tempProbe1,tempProbe2,tempProbe3,tempProbe4,tempProbe5,]temp_dict={\"session\":SESSION,\"temperatures\":temps}returntemp_dictdefmain():# Continually request information from the iBBQ devicewhileTrue:try:temps=generateSimulatedTemperature()publishToTopic(DATA_TOPIC,json.dumps(temps))time.sleep(5)exceptExceptionase:print(f\"ERROR IN WHILE LOOP, TRY AGAIN!{e.message}\")if__name__==\"__main__\":main()Enter fullscreen modeExit fullscreen modeCreate the recipeIn the same folder the GreenGrass recipe file is also created. This need to be updated to match GDK configuration. In the recipe it's possible to variables like{artifacts:decompressedPath}, full reference can befound hereTo send data over MQTT to AWS IoT Core we need to add permissions for the mqttproxy and specify which topics it can publish to. W'll only allow the device to publish to topics it \"owns\" that is include the thing name. For this we can use the variable{iot:thingName}---RecipeFormatVersion:\"2020-01-25\"ComponentName:\"com.example.hellobbq\"ComponentVersion:\"1.0.0\"ComponentDescription:\"ComponentforsendingtemperaturedatatoCloud\"ComponentPublisher:\"YOUR-NAME\"ComponentConfiguration:DefaultConfiguration:Topic:\"e/{iot:thingName}/data\"accessControl:aws.greengrass.ipc.mqttproxy:com.example.hellobbq:mqttproxy:1:policyDescription:Allows access to publish to device topics.operations:-aws.greengrass#PublishToIoTCoreresources:-\"e/{iot:thingName}/data\"Manifests:-Platform:os:allArtifacts:-URI:\"s3://PLACEHOLDER_BUCKET/com.example.hellobbq/1.0.0/com.example.hellobbq.zip\"Unarchive:ZIPLifecycle:Run:\"python3-u{artifacts:decompressedPath}/com.example.hellobbq/main.py{configuration:/Topic}\"Enter fullscreen modeExit fullscreen modeBuild componentWith the GDK config and recipe update it's time to build the component. That is done with command.gdk component buildEnter fullscreen modeExit fullscreen modeThe build will produce a zip file located in foldergreengrass-build/COMPONENT_NAME/VERSION/With build completed copy the zip file to the S3 bucket used in the recipe.aws s3cpgreengrass-build/artifacts/com.example.HelloBBQ/1.0.0/com.example.hellobbq.zip s3://PLACEHOLDER_BUCKET/com.examle.hellobbq/1.0.0/com.example.hellobbq.zipEnter fullscreen modeExit fullscreen modeWith the zip file uploaded the component can be created in the AWS Console.Create componentTo create the component we navigate to the IoT Core part of the console, expandGreengrass devicesand selectComponents, click onCreate Componentbutton to the right.Paste the recipe and click onCreate ComponentCreate simulation deviceTo simulate the device we can use a small EC2 instance. I will create a smallt3.microinstance that I will be using.We need create an IAM Role, that we assign to the EC2 Instance, this role will be used to provision the instance as an IoT Thing and all the needed resources. The minimum access needed can be foundin the aws documentationbelow is an example.{\"Version\":\"2012-10-17\",\"Statement\":[{\"Sid\":\"CreateTokenExchangeRole\",\"Effect\":\"Allow\",\"Action\":[\"iam:AttachRolePolicy\",\"iam:CreatePolicy\",\"iam:CreateRole\",\"iam:GetPolicy\",\"iam:GetRole\",\"iam:PassRole\"],\"Resource\":[\"arn:aws:iam::ACCOUNT-ID:role/GreengrassV2TokenExchangeRole\",\"arn:aws:iam::ACCOUNT-ID:policy/GreengrassV2TokenExchangeRoleAccess\",\"arn:aws:iam::aws:policy/GreengrassV2TokenExchangeRoleAccess\"]},{\"Sid\":\"CreateIoTResources\",\"Effect\":\"Allow\",\"Action\":[\"iot:AddThingToThingGroup\",\"iot:AttachPolicy\",\"iot:AttachThingPrincipal\",\"iot:CreateKeysAndCertificate\",\"iot:CreatePolicy\",\"iot:CreateRoleAlias\",\"iot:CreateThing\",\"iot:CreateThingGroup\",\"iot:DescribeEndpoint\",\"iot:DescribeRoleAlias\",\"iot:DescribeThingGroup\",\"iot:GetPolicy\"],\"Resource\":\"*\"},{\"Sid\":\"DeployDevTools\",\"Effect\":\"Allow\",\"Action\":[\"greengrass:CreateDeployment\",\"iot:CancelJob\",\"iot:CreateJob\",\"iot:DeleteThingShadow\",\"iot:DescribeJob\",\"iot:DescribeThing\",\"iot:DescribeThingGroup\",\"iot:GetThingShadow\",\"iot:UpdateJob\",\"iot:UpdateThingShadow\"],\"Resource\":\"*\"}]}Enter fullscreen modeExit fullscreen modeI create a Role namedSimulatedBBQDeviceInstanceRolewith the above permissions, and assign this the EC2 instance used to simulated the BBQ device.When the EC2 instance is running I connect to it usingEC2 Instance Connect.Install RequirementsFirst of all Java need to be installed on the instance, since i use a Amazon Linux 2023 based instance Java is installed with command.sudodnfinstalljava-11-amazon-corretto-yjava-versionEnter fullscreen modeExit fullscreen modeNext Python and PIP are needed. Amazon Linux 2023 should come with Python 3.9 installed, verify this by runningwhich python3. If Python should not be installed run:sudodnfinstallpython3.9-yEnter fullscreen modeExit fullscreen modeNext install PIP for the corresponding Python versionsudodnfinstallpython3.9-pip-yEnter fullscreen modeExit fullscreen modeFinally install AWS IoT SDK fpr Python, but before you do this ensure you have the latest version of AWS CLI installed,follow this guide.sudopip3installawsiotsdk--userEnter fullscreen modeExit fullscreen modeSetup GreenGrass CoreTo setup GreenGrass Core on the device navigate back to the IoT Console, selectGreengrassandCore devicesthen click onSet up one Greengrass core deviceI give it a name and don't assign any Thing group.The rest of the steps are in the wizard but summarized we should:1: Download the installercurl-shttps://d2s8p88vqu9w66.cloudfront.net/releases/greengrass-nucleus-latest.zip>greengrass-nucleus-latest.zip&&unzip greengrass-nucleus-latest.zip-dGreengrassInstallerEnter fullscreen modeExit fullscreen mode2: Run installersudo-Ejava-Droot=\"/greengrass/v2\"-Dlog.store=FILE-jar./GreengrassInstaller/lib/Greengrass.jar--aws-regioneu-west-1--thing-nameSimulatedBBQDeviceOne--component-default-userggc_user:ggc_group--provisiontrue--setup-system-servicetrue--deploy-dev-toolstrueEnter fullscreen modeExit fullscreen modeThere should be a print in the end indicating successSuccessfully set up Nucleus as a system serviceand the device should now be visible in the IoT Core Console.Install componentsNow let's install the component that is needed. Navigate toDeploymentsin the GreenGrass section of IoT Core console, and locate the Deployment for the Simulated device.From the Actions menu selectReviseand theRevise Deploymentbutton.Leave target as is and just click next.Select the HelloBBQ component, LogManager, CloudWatch, and Nucleus, and click next.Next there is a need to configureLogManagerandNucleuscomponents.In the Configuration screen select LogManager and click Configure, add the below JSON to the merge config.{\"logsUploaderConfiguration\":{\"systemLogsConfiguration\":{\"uploadToCloudWatch\":\"true\"},\"componentLogsConfigurationMap\":{\"com.example.hellobbq\":{}}}}Enter fullscreen modeExit fullscreen modeNow repeat the process forNucleuswhere we need to setinterpolateComponentConfigurationotherwise{iot:thingName}will not be inflated to a proper value.{\"reset\":[],\"merge\":{\"reset\":[],\"merge\":{\"interpolateComponentConfiguration\":\"true\"}}}Enter fullscreen modeExit fullscreen modeNow just click next in rest of the dialog and confirm the deployment to start the process.To verify that everything now is running navigate to Core Devices, select the simulated device, and check that all components are running.Verify dataTo verify that data is sent as expected open the MQTT test client in the console and create a subscription toe/#. Data should start flowing in the format we defined in the code.Cloudwatch LogsTo check the logs from the device, navigate to CloudWatch Logs, there should a Log group with the component name e.g./aws/greengrass/UserComponent/eu-west-1/com.example.hellobbqDevice LogsIt's also possible to get the logs on the device it self. Connect to the instance, using Instance Connect. Logs can be found in folder/greengrass/v2/logsFinal WordsThis was the first part in building a connected BBQ as a SaaS solution. We looked at creating a GreenGrass Core component and device running as a simulation on an EC2 instance.Check outMy serverless Handbookfor some of the concepts mentioned in this post.Don't forget to follow me onLinkedInandXfor more content, and read rest of myBlogsAs Werner says! Now Go Build!"}
{"title": "Contours on MYSQL Aurora Database with Blue/Green Deployment and Switch Over", "published_at": 1716890197, "tags": ["aws", "amazonrds", "cloudwatch", "bluegreendeployment"], "user": "GargeeBhatnagar", "url": "https://dev.to/aws-builders/contours-on-mysql-aurora-database-with-bluegreen-deployment-and-switch-over-3i7k", "details": "\u201c I have checked the documents of AWS to get into a contours on mysql aurora database with blue/green deployment and switch over. In terms of cost, need to pay for amazon rds and cloudwatch.\u201dAmazon Aurora is a fully managed relational database engine that's compatible with Mysql and Postgresql. As already know how mysql and postgresql combine the speed and reliability of high end commercial databases with the simplicity and cost effectiveness of open source databases.When you create a blue/green deployment, you specify the DB cluster to copy in the deployment. The DB cluster you choose is the production DB cluster and it becomes the DB cluster in the blue environment. RDS copies the blue environment's topology to a staging area, along with its configured features. The db cluster is copied to the green environment and rds configures replication from the db cluster in the blue environment to the db cluster in the green environment. RDS also copies all of the db instances in the db cluster.A switchover promoted the db cluster, including its db instances, in the green environment to be the production db cluster. Before you switch over, production traffic is routed to the cluster in the blue environment. After you switch over production traffic is routed to the db cluster in the green environment.In this post, you will experience how to contours on mysql aurora database with blue/green deployment and switch over. Here I have created an amazon rds mysql aurora database with a custom db parameter group and blue/green deployment. Also the addition of a switch over feature on aurora database.Architecture OverviewThe architecture diagram shows the overall deployment architecture with data flow, amazon rds mysql aurora, cloudwatch, blue/green deployment.Solution overviewThe blog post consists of the following phases:1.Create of Amazon RDS Mysql Aurora with Custom DB Cluster Parameter Group2.Create of Blue/Green Deployment on Aurora RDS3.Output of Blue/Green Deployment with Switch Over OptionPhase 1: Create of Amazon RDS Mysql Aurora with Custom DB Cluster Parameter GroupOpen the console of Amazon RDS, create a db cluster parameter group. Update the value of binlog_format as ROW in the parameter group. Create aurora mysql rds with a custom db cluster parameter group.Phase 2: Create of Blue/Green Deployment on Aurora RDSSelect the database and choose the blue/green deployment option from action drop down. Start creating the deployment as a staging environment with specifying identifiers and other required parameters.Phase 3: Output of Blue/Green Deployment with Switch Over OptionClean-upDelete of Amazon RDS, DB Cluster Parameter Group and Cloudwatch Log Group.PricingI review the pricing and estimated cost of this example.Cost of Amazon Relational Database Service for Aurora MYSQL in US East (N. Virginia) = ($0.29 per RDS db.r5.large Single-AZ instance hour (or partial hour) running Aurora MySQL) x (6.342 Hrs) = $1.84Cost of Amazon Aurora Storage and I/O = $0.20 per 1 million I/O requests (Aurora) x 82,413 IOs = $0.02Cost of Cloudwatch = $0.0Total Cost = $1.86SummaryIn this post, I showed \u201chow to contours on mysql aurora database with blue/green deployment and switch over\u201d.For more details on Amazon RDS, Checkout Get started Amazon RDS, open theAmazon RDS console. To learn more, read theAmazon RDS documentation.Thanks for reading!Connect with me:Linkedin"}
{"title": "GenAI Use Cases JP\u3092\u8a66\u3057\u3066\u307f\u305f", "published_at": 1716886124, "tags": ["japanese", "bedrock", "ai", "aws"], "user": "Yasuhiro Matsuda", "url": "https://dev.to/aws-builders/genai-use-caseswoshi-sitemita-4img", "details": "5/15\u306bJAWS-UG\u306e\u30a4\u30d9\u30f3\u30c8\u3067\u958b\u50ac\u3055\u308c\u305fAWS Expert Online\u300e\u751f\u6210 AI \u30e6\u30fc\u30b9\u30b1\u30fc\u30b9\u3092\u8003\u3048\u5012\u3059\u305f\u3081\u306e Generative AI Use Cases JP \u306e\u9b45\u529b\u3068\u4f7f\u3044\u65b9\u300f\u306b\u53c2\u52a0\u3057\u3066\u3001\u5b9f\u969b\u306b\u8a66\u3057\u3066\u307f\u305f\u7d50\u679c\u3092\u307e\u3068\u3081\u3066\u307f\u305f\u3002\u30b9\u30e9\u30a4\u30c9\u8cc7\u6599\u751f\u6210AI\u30e6\u30fc\u30b9\u30b1\u30fc\u30b9\u3092\u8003\u3048\u5012\u3059\u305f\u3081\u306eGenerative AI Use Cases JP (GenU)\u306e\u9b45\u529b\u3068\u4f7f\u3044\u65b9 - Speaker DeckAWS Expert Online for JAWS-UG #33 (2024\u5e745\u670815\u65e5\u5b9f\u65bd)\u306e\u767a\u8868\u8cc7\u6599\u3067\u3059\u3002speakerdeck.com\u914d\u4fe1\u52d5\u753bAWS Expert Online for JAWS-UG #33 - YouTube\u65e5\u6642\uff1a2024-05-15\uff08\u6c34\uff0919:00 - 20:00\u767b\u58c7\u8cc7\u6599\uff1a https://speakerdeck.com/okamotoaws/sheng-cheng-aiyusukesuwokao-edao-sutamenogenerative-ai-use-cases-jp-genu-nomei-li-toshi...youtube.comGenerative AI Use Cases JP\u3068\u306f\uff1fCDK\u3067\u30c7\u30d7\u30ed\u30a4\u3059\u308b\u3060\u3051\u3067\u7c21\u5358\u306b\u4ee5\u4e0b\u306e\u30e6\u30fc\u30b9\u30b1\u30fc\u30b9\u3092\u7c21\u5358\u306b\u8a66\u3059\u3053\u3068\u304c\u3067\u304d\u308b\u3002\u30c1\u30e3\u30c3\u30c8\u6587\u7ae0\u751f\u6210\uff08\u6587\u7ae0\uff09\u8981\u7d04\u6821\u6b63\u7ffb\u8a33Web\u30b3\u30f3\u30c6\u30f3\u30c4\u62bd\u51fa\u753b\u50cf\u751f\u6210\u6620\u50cf\u5206\u6790\u4f8b\u3048\u3070\u3001\u30c1\u30e3\u30c3\u30c8\u3067\u306f\u4ee5\u4e0b\u306e\u3088\u3046\u306b\u88dc\u52a9\u91d1\u306b\u3064\u3044\u3066\u805e\u3044\u3066\u307f\u308b\u3068\u305d\u308c\u306a\u308a\u306b\u7d50\u679c\u304c\u51fa\u3066\u304f\u308b\u3002\u77f3\u5ddd\u770c\u3067\u5275\u696d\u3057\u305f\u4e8b\u696d\u8005\u304c\u6d3b\u7528\u3067\u304d\u308b\u88dc\u52a9\u91d1\u306b\u3064\u3044\u3066\u6559\u3048\u3066\u304f\u3060\u3055\u3044\u3002\u77f3\u5ddd\u770c\u3067\u5275\u696d\u3057\u305f\u4e8b\u696d\u8005\u304c\u6d3b\u7528\u3067\u304d\u308b\u4e3b\u306a\u88dc\u52a9\u91d1\u3092\u4ee5\u4e0b\u306b\u6319\u3052\u307e\u3059\u3002\u77f3\u5ddd\u770c\u5275\u696d\u652f\u63f4\u4e8b\u696d\u8cbb\u88dc\u52a9\u91d1\u770c\u5185\u3067\u5275\u696d\u3059\u308b\u500b\u4eba\u4e8b\u696d\u4e3b\u3084\u6cd5\u4eba\u3092\u5bfe\u8c61\u3068\u3057\u305f\u5275\u696d\u652f\u63f4\u88dc\u52a9\u91d1\u88dc\u52a9\u7387\u306f2/3\u4ee5\u5185\u3001\u4e0a\u9650100\u4e07\u5186\u91d1\u6ca2\u5e02\u5275\u696d\u652f\u63f4\u4e8b\u696d\u88dc\u52a9\u91d1\u91d1\u6ca2\u5e02\u5185\u3067\u5275\u696d\u3059\u308b\u500b\u4eba\u4e8b\u696d\u4e3b\u3084\u6cd5\u4eba\u3092\u5bfe\u8c61\u3068\u3057\u305f\u5275\u696d\u652f\u63f4\u88dc\u52a9\u91d1\u88dc\u52a9\u7387\u306f2/3\u4ee5\u5185\u3001\u4e0a\u9650100\u4e07\u5186\u4e2d\u5c0f\u4f01\u696d\u5e81\u306e\u5275\u696d\u88dc\u52a9\u91d1\u5168\u56fd\u306e\u5275\u696d\u8005\u3092\u5bfe\u8c61\u3068\u3057\u305f\u56fd\u306e\u88dc\u52a9\u91d1\u6700\u5927200\u4e07\u5186\u306e\u88dc\u52a9\u304c\u53d7\u3051\u3089\u308c\u308b\u65e5\u672c\u653f\u7b56\u91d1\u878d\u516c\u5eab\u306e\u5275\u696d\u652f\u63f4\u8cc7\u91d1\u5275\u696d\u6642\u306e\u8a2d\u5099\u8cc7\u91d1\u3084\u904b\u8ee2\u8cc7\u91d1\u3068\u3057\u3066\u4f4e\u5229\u306e\u878d\u8cc7\u3092\u53d7\u3051\u3089\u308c\u308b\u88dc\u52a9\u91d1\u306e\u5185\u5bb9\u3084\u7533\u8acb\u6761\u4ef6\u3001\u52df\u96c6\u6642\u671f\u306a\u3069\u306f\u5e74\u5ea6\u306b\u3088\u3063\u3066\u5909\u66f4\u306b\u306a\u308b\u5834\u5408\u304c\u3042\u308a\u307e\u3059\u3002\u8a73\u7d30\u306f\u5404\u81ea\u6cbb\u4f53\u3084\u652f\u63f4\u6a5f\u95a2\u306e\u30a6\u30a7\u30d6\u30b5\u30a4\u30c8\u3084\u30d1\u30f3\u30d5\u30ec\u30c3\u30c8\u3067\u6700\u65b0\u60c5\u5831\u3092\u78ba\u8a8d\u3059\u308b\u5fc5\u8981\u304c\u3042\u308a\u307e\u3059\u3002\u5275\u696d\u6642\u306e\u8cc7\u91d1\u8abf\u9054\u3067\u306f\u3001\u3053\u3046\u3057\u305f\u516c\u7684\u652f\u63f4\u3092\u7a4d\u6975\u7684\u306b\u6d3b\u7528\u3059\u308b\u3053\u3068\u3092\u304a\u3059\u3059\u3081\u3057\u307e\u3059\u3002\u307e\u305f\u3001\u6620\u50cf\u5206\u6790\u3082\u6587\u5b57\u3092\u8b58\u5225\u3067\u304d\u3066\u304a\u308a\u3001\u304b\u306a\u308a\u6b63\u78ba\u306a\u6587\u7ae0\u304c\u751f\u6210\u3067\u304d\u3066\u3044\u308b\u3002\u753b\u50cf\u306b\u306f\u3001\u30e4\u30de\u30cf\u306e\u30ea\u30b3\u30fc\u30c0\u30fc\u3092\u624b\u306b\u6301\u3063\u305f\u7537\u6027\u306e\u4e0a\u534a\u8eab\u304c\u5199\u3063\u3066\u3044\u307e\u3059\u3002\u7537\u6027\u306f\u683c\u5b50\u67c4\u306e\u30b7\u30e3\u30c4\u3092\u7740\u3066\u304a\u308a\u3001\u30ea\u30b3\u30fc\u30c0\u30fc\u3092\u53e3\u306b\u5f53\u3066\u308b\u30dd\u30fc\u30ba\u3092\u3057\u3066\u3044\u307e\u3059\u3002\u80cc\u666f\u306b\u306f\u7a93\u3068\u984d\u7e01\u304c\u898b\u3048\u307e\u3059\u3002JAWS-UG\u3068\u306f\uff1f\u516c\u5f0f\u30b5\u30a4\u30c8\u3092\u6307\u5b9a\u3057\u3066\u3001 Web\u30b3\u30f3\u30c6\u30f3\u30c4\u62bd\u51fa \u306e\u30e6\u30fc\u30b9\u30b1\u30fc\u30b9\u3092\u5b9f\u884c\u3057\u305f\u7d50\u679c\u304c\u4ee5\u4e0b\u306e\u901a\u308a\u3002\uff08\u8ffd\u52a0\u30b3\u30f3\u30c6\u30ad\u30b9\u30c8\u306b\u300c\u8981\u7d04\u3057\u3066\u300d\u3092\u6307\u5b9a\u3057\u307e\u3057\u305f\uff09JAWS-UG(AWS User Group - Japan)\u306f\u3001AWS\u306e\u30af\u30e9\u30a6\u30c9\u30b3\u30f3\u30d4\u30e5\u30fc\u30c6\u30a3\u30f3\u30b0\u3092\u5229\u7528\u3059\u308b\u4eba\u3005\u306e\u96c6\u307e\u308a(\u30b3\u30df\u30e5\u30cb\u30c6\u30a3)\u3067\u3059\u3002\u30dc\u30e9\u30f3\u30c6\u30a3\u30a2\u306b\u3088\u308b\u52c9\u5f37\u4f1a\u306e\u958b\u50ac\u3084\u4ea4\u6d41\u30a4\u30d9\u30f3\u30c8\u306a\u3069\u3092\u884c\u3044\u3001\u4e00\u4eba\u3067\u306f\u5f97\u3089\u308c\u306a\u3044\u5b66\u3073\u3084\u4ea4\u6d41\u3092\u76ee\u7684\u3068\u3057\u3066\u3044\u307e\u3059\u3002\u65e5\u672c\u5168\u56fd\u306b\u652f\u90e8\u306e\u5f62\u3067\u30b0\u30eb\u30fc\u30d7\u3092\u6301\u3061\u3001\u305d\u308c\u305e\u308c\u306e\u30c6\u30fc\u30de\u306b\u57fa\u3065\u3044\u3066\u6d3b\u52d5\u3057\u3066\u3044\u307e\u3059\u3002\u975e\u55b6\u5229\u76ee\u7684\u3067\u6d3b\u52d5\u3057\u3066\u3044\u307e\u3059\u3002\u79c1\u306f\u91d1\u6ca2\u652f\u90e8\u306e\u30b3\u30a2\u30e1\u30f3\u30d0\u30fc\u3068\u3057\u3066\u3001\u4eca\u56de\u306e\u914d\u4fe1\u52d5\u753b\u306b\u3082\u767b\u5834\u3057\u3066\u3044\u307e\u3059\u3002\u30c7\u30d7\u30ed\u30a4\u65b9\u6cd5GitHub\u306b\u624b\u9806\u304c\u3042\u308a\u3001\u7c21\u5358\u306b\u30c7\u30d7\u30ed\u30a4\u3059\u308b\u3053\u3068\u304c\u3067\u304d\u308b\u3002\u307e\u305f\u3001\u30bd\u30fc\u30b9\u30b3\u30fc\u30c9\u3092\u4fee\u6b63\u3057\u3066\u8a66\u3057\u306a\u304c\u3089\u5b9f\u88c5\u3092\u691c\u8a0e\u3057\u3066\u3044\u304f\u3053\u3068\u304c\u3067\u304d\u308b\u3002\u306a\u304a\u3001\u7b2c\u4e09\u8005\u306b\u5229\u7528\u3055\u308c\u306a\u3044\u3088\u3046\u306b\u3059\u308b\u305f\u3081\u306b\u3001\u30b5\u30a4\u30f3\u30a2\u30c3\u30d7\u3067\u304d\u308b\u30e1\u30fc\u30eb\u30a2\u30c9\u30ec\u30b9\u306e\u30c9\u30e1\u30a4\u30f3\u3092\u5236\u9650\u3059\u308b\u306f\u8a2d\u5b9a\u3057\u305f\u307b\u3046\u304c\u826f\u3044\u3002\u30c8\u30e9\u30d6\u30eb\u30b7\u30e5\u30fc\u30c6\u30a3\u30f3\u30b0\u5b9f\u969b\u306b\u8a66\u3057\u3066\u307f\u305f\u3068\u3053\u308d\u4ee5\u4e0b\u306e\u30c8\u30e9\u30d6\u30eb\u306b\u906d\u9047\u3057\u305f\u3002\u30e6\u30fc\u30b9\u30b1\u30fc\u30b9\u3092\u8a66\u305d\u3046\u3068\u601d\u3063\u3066\u3082\u30a8\u30e9\u30fc\u304c\u767a\u751f\u3057\u3066\u5229\u7528\u3067\u304d\u306a\u3044\u4ee5\u4e0b\u306e\u3088\u3046\u306a\u30a8\u30e9\u30fc\u304c\u767a\u751f\u3059\u308b\u5834\u5408\u306b\u306f\u3001\u30c9\u30ad\u30e5\u30e1\u30f3\u30c8\u306b\u8a18\u8f09\u306e\u3042\u308b\u901a\u308a\u3001Claude3 Sonnet\u306e\u30e2\u30c7\u30eb\u30ea\u30af\u30a8\u30b9\u30c8\u3092\u6709\u52b9\u5316\u3059\u308b\u5fc5\u8981\u304c\u3042\u308b\u3002\u30e2\u30c7\u30eb\u306e\u6709\u52b9\u5316\u6642\u306b\u30a8\u30e9\u30fc\u304c\u767a\u751f\u3057\u3066\u6709\u52b9\u5316\u3067\u304d\u306a\u3044\u6709\u52b9\u5316\u3059\u308b\u5834\u5408\u306b\u306f\u30af\u30ec\u30b8\u30c3\u30c8\u30ab\u30fc\u30c9\u306e\u6709\u52b9\u6027\u3092\u78ba\u8a8d\u3055\u308c\u308b\u3002\u3082\u3057\u671f\u9650\u304c\u5207\u308c\u3066\u3044\u308b\u306a\u3069\u306e\u5834\u5408\u306b\u306f INVALID_PAYMENT_INSTRUMENT:A valid payment instrument must be provided. \u306e\u30a8\u30e9\u30fc\u304c\u767a\u751f\u3059\u308b\u3002\u652f\u6255\u8a2d\u5b9a\u753b\u9762\u3088\u308a\u78ba\u8a8d\u3057\u3066\u3001\u6709\u52b9\u671f\u9650\u5185\u306e\u30af\u30ec\u30b8\u30c3\u30c8\u30ab\u30fc\u30c9\u3092\u8a2d\u5b9a\u3059\u308b\u3002\u753b\u50cf\u751f\u6210\u306e\u30e6\u30fc\u30b9\u30b1\u30fc\u30b9\u3092\u8a66\u305d\u3046\u3068\u601d\u3063\u3066\u3082\u30a8\u30e9\u30fc\u304c\u767a\u751f\u3057\u3066\u5229\u7528\u3067\u304d\u306a\u3044\u753b\u50cf\u751f\u6210\u3067You don't have access to the model with the specified model ID.\u304c\u767a\u751f\u3059\u308b\u5834\u5408\u306b\u306f\u3001Stability AI SDXL 1.0\u306e\u30e2\u30c7\u30eb\u30ea\u30af\u30a8\u30b9\u30c8\u3092\u6709\u52b9\u5316\u3059\u308b\u5fc5\u8981\u304c\u3042\u308b\u3002\u6c17\u306b\u306a\u308b\u304a\u5024\u6bb5\u306f\uff1f\u6599\u91d1\u8a66\u7b97\u306fAWS\u306e\u30b5\u30a4\u30c8\u306b\u516c\u958b\u3055\u308c\u3066\u3044\u308b\u304c\u3001\u91d1\u984d\u304c\u9ad8\u3044\u306e\u3067\u8a66\u3059\u306e\u306b\u8e8a\u8e87\u3055\u308c\u3066\u3044\u308b\u65b9\u3082\u591a\u3044\u3068\u601d\u3046\u3002\u305d\u3053\u3067\u3001\u30c7\u30d5\u30a9\u30eb\u30c8\u306e\u72b6\u614b\u306e\u307e\u307e\uff08Kendora\u3092\u4f7f\u308f\u306a\u3044\uff09\u3067\u5c11\u3057\u904a\u3093\u3067\u30016\u65e5\u9593\u653e\u7f6e\u3057\u3066\u307f\u305f\u3002Claude 3 Sonnet\u3001SDXL V1.0\u4ee5\u5916\u306f\u30b3\u30b9\u30c8\u304c\u304b\u304b\u3063\u3066\u3044\u306a\u3044\u72b6\u6cc1\u3067\u3001\u5b9f\u969b\u306b\u4f7f\u3063\u305f\u5206\u3060\u3051\u306e\u8ab2\u91d1\u306b\u3067\u304d\u305d\u3046\u306a\u611f\u899a\u3092\u3082\u3063\u305f\u3002\u5b9f\u969b\u306b\u8a66\u3059\u969b\u306b\u306f\u8ab2\u91d1\u72b6\u6cc1\u3092\u78ba\u8a8d\u3057\u306a\u304c\u3089\u5c11\u3057\u305a\u3064\u904a\u3093\u3067\u307f\u308b\u3053\u3068\u3092\u30aa\u30b9\u30b9\u30e1\u3059\u308b\u3002"}
{"title": "Process S3 Objects with Step Functions using CDK + TS", "published_at": 1716826372, "tags": ["aws", "typescript", "cdk"], "user": "Marcos Henrique", "url": "https://dev.to/aws-builders/process-s3-objects-with-step-functions-using-cdk-ts-20ab", "details": "PrefaceWhenever a user needed to launch automation, our tech lead had to dive into an extensive manual process that took about two days to configure everything.Picture this: an Excel file with over six tabs and data manipulation using Excel functions (a spreadsheet nightmare \ud83d\udc7b).Seeing this madness, I realised we could automate this process (trust me, it was a heated debate to reach this conclusion). Thus, the architecture below was born.Remember, we've got some proprietary code, so this is a simplified and slightly fictionalised version to illustrate the solution.The SolutionWe needed to set up a process to trigger aStep Functionevery time an object was created in S3. However, direct invocation of a Step Function is not currently supported. Therefore, I created an EventBridge rule to monitor S3 for object creation. Once an object was created, the rule would invoke the Step Function to process the object using one of the Lambda tasks.I used the Express type since I needed this to be a quick process. With this decision, I had to parallelise some steps (yes, my automation allowed for that), so, I used theParallel state. Finally, if any errors occurred, I set up a notification using a topic for that.Express x StandardStandard: For long-running processes with exactly-once execution guarantees and advanced recovery features.Express: For fast, high-throughput processes with at least one execution guarantee and billing based on time and memory.Getting your hands dirtyLet's implement it using CDK with Typescript!Let's start by creating our bucket and importing a topic to notify us of any errorsconstuserNotificationTopic=Topic.fromTopicArn(this,'userNotificationTopic',Fn.importValue('infrastructure::sns::user-notification-topic::arn'),)constbucket=newBucket(this,'bucket',{bucketName:'automation-configuration',removalPolicy:RemovalPolicy.DESTROY,})Enter fullscreen modeExit fullscreen modeNow, let's create our Lambdas and grant read permissions to the Lambda that will access the S3 object:constetlLambda=newNodejsFunction(this,'etl-lambda',{entry:'src/lambda/etl/etl.handler.ts',functionName:'etl',timeout:Duration.seconds(30),architecture:Architecture.ARM_64,})bucket.grantReadWrite(newAlias(this,id.concat('alias'),{aliasName:'current',version:etlLambda.currentVersion,}),)constcategoriesLambda=newNodejsFunction(this,'insert-categories-lambda',{entry:'src/lambda/categories/categories.handler.ts',functionName:'insert-categories',timeout:Duration.seconds(30),architecture:Architecture.ARM_64,})consthospitalsLambda=newNodejsFunction(this,'hospitals-categories-lambda',{entry:'src/lambda/categories/categories.handler.ts',functionName:'insert-categories',timeout:Duration.seconds(30),architecture:Architecture.ARM_64,})Enter fullscreen modeExit fullscreen modeTo call a Lambda within our Step Function, we need to create aLambda Invokefor each of the Lambdas. So, I created a function to avoid repeating this for every Lambda:privatemountLambdaInvokes(lambdasInvoke:Array<{function:IFunctionname?:stringoutput?:string}>,){returnlambdasInvoke.map(lambdaInvoke=>{returnnewLambdaInvoke(this,`${lambdaInvoke.name||lambdaInvoke.function.functionName}-task`,{lambdaFunction:lambdaInvoke.function,inputPath:'$',outputPath:lambdaInvoke?.output||'$',})})}Enter fullscreen modeExit fullscreen modeUsingmountLambdaInvokes:const[etlTask,categoriesTask,hospitalsTask]=this.mountLambdaInvokes([{function:etlLambda,output:'$.Payload'},{function:categoriesLambda},{function:hospitalsLambda},])Enter fullscreen modeExit fullscreen modeWe need to create our failure step and theSnsPublishto send failed events to the topic we imported earlier:consterrorTopicConfig={topic:userNotificationTopic,subject:'Automation Config Failed \ud83d\ude25',message:TaskInput.fromObject({message:'Automation Config Failed due to an unexpected error.',cause:'Unexpected Error',channel:'email',destination:{ToAddresses:['suporte@somemail.com']},}),}constpublishFailed=(publishFailedId:string)=>newSnsPublish(this,`automation-config-sns-failed-${publishFailedId}`,errorTopicConfig)constjobFailed=newFail(this,'automation-config-job-failed',{cause:'Unexpected Error',})Enter fullscreen modeExit fullscreen modeHaving done this, let's set up ourParallelstate. Each branch will be a Lambda processed simultaneously. We'll also add a retry to attempt again if any issues arise and a catch to handle any failures in this parallel process:consthospitalsCategoriesParallel=newParallel(this,'auto-config-exams-parallel-map').branch(categoriesTask).branch(hospitalsTask).addRetry({errors:['States.ALL'],interval:Duration.seconds(5),maxAttempts:1}).addCatch(publishFailed('exams').next(jobFailed),{errors:['States.ALL'],})Enter fullscreen modeExit fullscreen modeAt this point, we'll create the definition of our tasks, a log group, and the State Machine, which is essentially our Step Function:constdefinition=etlTask.next(hospitalsCategoriesParallel)constlogGroup=newLogGroup(this,'automation-configuration-log-group',{retention:RetentionDays.ONE_WEEK,removalPolicy:RemovalPolicy.DESTROY,})conststateMachine=newStateMachine(this,`${id}-state-machine`,{definition,timeout:Duration.minutes(5),stateMachineName:'automation-configuration',stateMachineType:StateMachineType.EXPRESS,logs:{destination:logGroup,includeExecutionData:true,level:LogLevel.ALL,},})Enter fullscreen modeExit fullscreen modeAfter this, we need to create the rules to associate EventBridge with S3 and the execution of our Step Function:consts3EventRule=newRule(this,'automation-config-s3-event-rule',{ruleName:'automation-config-s3-event-rule',})consteventRole=newRole(this,'eventRole',{assumedBy:newServicePrincipal('events.amazonaws.com'),})stateMachine.grantStartExecution(eventRole)s3EventRule.addTarget(newSfnStateMachine(stateMachine,{input:RuleTargetInput.fromObject({detail:EventField.fromPath('$.detail'),}),role:eventRole,}),)s3EventRule.addEventPattern({source:['aws.s3'],detailType:['Object Created'],detail:{bucket:{name:[bucket.bucketName],},object:{key:[{wildcard:'csv/automation-configuration/*.csv',},],},},})Enter fullscreen modeExit fullscreen modeFinally, let's create a rule to listen to any unexpected failures that occur in our Step Function through EventBridge as well, thus maintaining the event-driven nature that we love so much:constunexpectedFailRule=newRule(this,'exam-automation-config-unexpected-fail-rule',{ruleName:'exam-automation-config-unexpected-fail-rule',})unexpectedFailRule.addTarget(newSnsTopic(userNotificationTopic,{message:RuleTargetInput.fromObject({subject:'Exam Automation Config Failed \ud83d\ude25',message:'Exam Automation Config Failed due to an unexpected error.',cause:'Unexpected Error',channel:'email',destination:{ToAddresses:['it@somemail.com']},}),}),)unexpectedFailRule.addEventPattern({source:['aws.states'],detailType:['Step Functions Execution Status Change'],detail:{stateMachineArn:[stateMachine.stateMachineArn],status:['FAILED','TIMED_OUT','ABORTED'],},})Enter fullscreen modeExit fullscreen modePutting everything together in a class so you can understand the entire unified flow:import{Duration,Fn,RemovalPolicy}from'aws-cdk-lib'import{Rule,RuleTargetInput,EventField}from'aws-cdk-lib/aws-events'import{SfnStateMachine,SnsTopic}from'aws-cdk-lib/aws-events-targets'import{Role,ServicePrincipal}from'aws-cdk-lib/aws-iam'import{Alias,Architecture,IFunction}from'aws-cdk-lib/aws-lambda'import{NodejsFunction}from'aws-cdk-lib/aws-lambda-nodejs'import{LogGroup,RetentionDays}from'aws-cdk-lib/aws-logs'import{Bucket}from'aws-cdk-lib/aws-s3'import{Topic}from'aws-cdk-lib/aws-sns'import{Fail,LogLevel,Parallel,StateMachine,StateMachineType,TaskInput}from'aws-cdk-lib/aws-stepfunctions'import{LambdaInvoke,SnsPublish}from'aws-cdk-lib/aws-stepfunctions-tasks'import{Construct}from'constructs'exportdefaultclassAutomationConfigurationextendsConstruct{constructor(scope:Construct,id:string){super(scope,id)constuserNotificationTopic=Topic.fromTopicArn(this,'userNotificationTopic',Fn.importValue('infrastructure::sns::user-notification-topic::arn'),)constbucket=newBucket(this,'bucket',{bucketName:'automation-configuration',removalPolicy:RemovalPolicy.DESTROY,})constetlLambda=newNodejsFunction(this,'etl-lambda',{entry:'src/lambda/etl/etl.handler.ts',functionName:'etl',timeout:Duration.seconds(30),architecture:Architecture.ARM_64,})bucket.grantRead(newAlias(this,id.concat('alias'),{aliasName:'current',version:etlLambda.currentVersion,}),)constcategoriesLambda=newNodejsFunction(this,'insert-categories-lambda',{entry:'src/lambda/categories/categories.handler.ts',functionName:'insert-categories',timeout:Duration.seconds(30),architecture:Architecture.ARM_64,})consthospitalsLambda=newNodejsFunction(this,'hospitals-categories-lambda',{entry:'src/lambda/categories/categories.handler.ts',functionName:'insert-categories',timeout:Duration.seconds(30),architecture:Architecture.ARM_64,})const[etlTask,categoriesTask,hospitalsTask]=this.mountLambdaInvokes([{function:etlLambda,output:'$.Payload'},{function:categoriesLambda},{function:hospitalsLambda},])consterrorTopicConfig={topic:userNotificationTopic,subject:'Automation Config Failed \ud83d\ude25',message:TaskInput.fromObject({message:'Automation Config Failed due to an unexpected error.',cause:'Unexpected Error',channel:'email',destination:{ToAddresses:['suporte@somemail.com']},}),}constpublishFailed=(publishFailedId:string)=>newSnsPublish(this,`automation-config-sns-failed-${publishFailedId}`,errorTopicConfig)constjobFailed=newFail(this,'automation-config-job-failed',{cause:'Unexpected Error',})consthospitalsCategoriesParallel=newParallel(this,'auto-config-exams-parallel-map').branch(categoriesTask).branch(hospitalsTask).addRetry({errors:['States.ALL'],interval:Duration.seconds(5),maxAttempts:1}).addCatch(publishFailed('exams').next(jobFailed),{errors:['States.ALL'],})constdefinition=etlTask.next(hospitalsCategoriesParallel)constlogGroup=newLogGroup(this,'automation-configuration-log-group',{retention:RetentionDays.ONE_WEEK,removalPolicy:RemovalPolicy.DESTROY,})conststateMachine=newStateMachine(this,`${id}-state-machine`,{definition,timeout:Duration.minutes(5),stateMachineName:'automation-configuration',stateMachineType:StateMachineType.EXPRESS,logs:{destination:logGroup,includeExecutionData:true,level:LogLevel.ALL,},})consts3EventRule=newRule(this,'automation-config-s3-event-rule',{ruleName:'automation-config-s3-event-rule',})consteventRole=newRole(this,'eventRole',{assumedBy:newServicePrincipal('events.amazonaws.com'),})stateMachine.grantStartExecution(eventRole)s3EventRule.addTarget(newSfnStateMachine(stateMachine,{input:RuleTargetInput.fromObject({detail:EventField.fromPath('$.detail'),}),role:eventRole,}),)s3EventRule.addEventPattern({source:['aws.s3'],detailType:['Object Created'],detail:{bucket:{name:[bucket.bucketName],},object:{key:[{wildcard:'csv/automation-configuration/*.csv',},],},},})constunexpectedFailRule=newRule(this,'exam-automation-config-unexpected-fail-rule',{ruleName:'exam-automation-config-unexpected-fail-rule',})unexpectedFailRule.addTarget(newSnsTopic(userNotificationTopic,{message:RuleTargetInput.fromObject({subject:'Exam Automation Config Failed \ud83d\ude25',message:'Exam Automation Config Failed due to an unexpected error.',cause:'Unexpected Error',channel:'email',destination:{ToAddresses:['it@somemail.com']},}),}),)unexpectedFailRule.addEventPattern({source:['aws.states'],detailType:['Step Functions Execution Status Change'],detail:{stateMachineArn:[stateMachine.stateMachineArn],status:['FAILED','TIMED_OUT','ABORTED'],},})}privatemountLambdaInvokes(lambdasInvoke:Array<{function:IFunctionname?:stringoutput?:string}>,){returnlambdasInvoke.map(lambdaInvoke=>{returnnewLambdaInvoke(this,`${lambdaInvoke.name||lambdaInvoke.function.functionName}-task`,{lambdaFunction:lambdaInvoke.function,inputPath:'$',outputPath:lambdaInvoke?.output||'$',})})}}Enter fullscreen modeExit fullscreen mode[\ud83c\udde7\ud83c\uddf7 PT-BR]Pref\u00e1cioSempre que um usu\u00e1rio precisava subir uma automa\u00e7\u00e3o, o nosso tech lead tinha que realizar um trabalho manual bem extensivo do qual levava cerca de dois dias para configurar tudo, usando um arquivo xls com mais de 6 abas e manipulando os dados com algumas fun\u00e7\u00f5es do excel, ao ver esta situa\u00e7\u00e3o percebi que pod\u00edamos sim automatizar este fluxo (inclusive foi uma discuss\u00e3o acalorada para chegarmos nesta conclus\u00e3o), assim surgiu a arquitetura abaixo (temos algumas quest\u00f5es de c\u00f3digo propriet\u00e1rio ent\u00e3o esta arquitetura \u00e9 uma vis\u00e3o simplificada e com items ficticios para demonstrar esta solu\u00e7\u00e3o)A Solu\u00e7\u00e3oTinhamos que triggar umStep Functionquando um objeto fosse criado no S3, infelizmente hoje n\u00e3o temos como chamar de forma direta um step function, ent\u00e3o tive que criar um event bridge rule para ouvir a cria\u00e7\u00e3o de um objeto no meu S3, e quando isso fosse criado chamar o step function e consumir o objeto em quest\u00e3o por um dos lambdas tasks, como eu precisava que fosse um processo mais r\u00e1pido utilizei o tipo express, com esta decis\u00e3o eu precisava paralelizar alguns steps (sim, minha automatiza\u00e7\u00e3o possibilitava isso) assim utilizei o state Parallel, por fim caso eu tivesse algum erro eu notifico utilizando um t\u00f3pico para isso.Express x StandardStandard: Para processos longos, com execu\u00e7\u00f5es garantidas exatamente uma vez e recursos avan\u00e7ados de recupera\u00e7\u00e3o.Express: Para processos r\u00e1pidos e de alta taxa de transfer\u00eancia, com execu\u00e7\u00f5es garantidas pelo menos uma vez e cobran\u00e7a baseada em tempo e mem\u00f3ria.Colocando a m\u00e3o na massaBora implementar isto com CDK e Typescript!Vamos come\u00e7ar criando nosso bucket e importanto um t\u00f3pico para notificar nossos errosconstuserNotificationTopic=Topic.fromTopicArn(this,'userNotificationTopic',Fn.importValue('infrastructure::sns::user-notification-topic::arn'),)constbucket=newBucket(this,'bucket',{bucketName:'automation-configuration',removalPolicy:RemovalPolicy.DESTROY,})Enter fullscreen modeExit fullscreen modeAgora vamos criar nossos lambdas e dar a permiss\u00e3o de leitura para o lambda que ir\u00e1 acessar o objeto do S3:constetlLambda=newNodejsFunction(this,'etl-lambda',{entry:'src/lambda/etl/etl.handler.ts',functionName:'etl',timeout:Duration.seconds(30),architecture:Architecture.ARM_64,})bucket.grantReadWrite(newAlias(this,id.concat('alias'),{aliasName:'current',version:etlLambda.currentVersion,}),)constcategoriesLambda=newNodejsFunction(this,'insert-categories-lambda',{entry:'src/lambda/categories/categories.handler.ts',functionName:'insert-categories',timeout:Duration.seconds(30),architecture:Architecture.ARM_64,})consthospitalsLambda=newNodejsFunction(this,'hospitals-categories-lambda',{entry:'src/lambda/categories/categories.handler.ts',functionName:'insert-categories',timeout:Duration.seconds(30),architecture:Architecture.ARM_64,})Enter fullscreen modeExit fullscreen modePara que possamos chamar uma lambda dentro do nosso Step Function temos que criar umLambda Invokepara cada um dos lambdas, assim criei uma fun\u00e7\u00e3o para n\u00e3o precisar ficar repetindo isto para cada lambda:privatemountLambdaInvokes(lambdasInvoke:Array<{function:IFunctionname?:stringoutput?:string}>,){returnlambdasInvoke.map(lambdaInvoke=>{returnnewLambdaInvoke(this,`${lambdaInvoke.name||lambdaInvoke.function.functionName}-task`,{lambdaFunction:lambdaInvoke.function,inputPath:'$',outputPath:lambdaInvoke?.output||'$',})})}Enter fullscreen modeExit fullscreen modeUtilizando a fun\u00e7\u00e3omountLambdaInvokes:const[etlTask,categoriesTask,hospitalsTask]=this.mountLambdaInvokes([{function:etlLambda,output:'$.Payload'},{function:categoriesLambda},{function:hospitalsLambda},])Enter fullscreen modeExit fullscreen modePrecisamos criar nosso step de falha e o SnsPublish para enviar os eventos que falharam para o t\u00f3pico que importamos antes:consterrorTopicConfig={topic:userNotificationTopic,subject:'Automation Config Failed \ud83d\ude25',message:TaskInput.fromObject({message:'Automation Config Failed due to an unexpected error.',cause:'Unexpected Error',channel:'email',destination:{ToAddresses:['suporte@somemail.com']},}),}constpublishFailed=(publishFailedId:string)=>newSnsPublish(this,`automation-config-sns-failed-${publishFailedId}`,errorTopicConfig)constjobFailed=newFail(this,'automation-config-job-failed',{cause:'Unexpected Error',})Enter fullscreen modeExit fullscreen modeFeito isto vamos montar nossoParallel, onde cada branch \u00e9 um lambda que ser\u00e1 processado simultaneamente, adicionaremos tamb\u00e9m um retry para q seja feita a tentativa novamente caso ocorra algum problema e um catch para capturar as falhas desse processo paralelo:consthospitalsCategoriesParallel=newParallel(this,'auto-config-exams-parallel-map').branch(categoriesTask).branch(hospitalsTask).addRetry({errors:['States.ALL'],interval:Duration.seconds(5),maxAttempts:1}).addCatch(publishFailed('exams').next(jobFailed),{errors:['States.ALL'],})Enter fullscreen modeExit fullscreen modeNeste ponto vamos criar a definition das nossas tasks, um log group e a State Machine que nada mais \u00e9 que nossa Step Function de fatoconstdefinition=etlTask.next(hospitalsCategoriesParallel)constlogGroup=newLogGroup(this,'automation-configuration-log-group',{retention:RetentionDays.ONE_WEEK,removalPolicy:RemovalPolicy.DESTROY,})conststateMachine=newStateMachine(this,`${id}-state-machine`,{definition,timeout:Duration.minutes(5),stateMachineName:'automation-configuration',stateMachineType:StateMachineType.EXPRESS,logs:{destination:logGroup,includeExecutionData:true,level:LogLevel.ALL,},})Enter fullscreen modeExit fullscreen modeAp\u00f3s isso precisamos criar as rules para associar o event bridge com S3 e com a execu\u00e7\u00e3o do nosso step function:consts3EventRule=newRule(this,'automation-config-s3-event-rule',{ruleName:'automation-config-s3-event-rule',})consteventRole=newRole(this,'eventRole',{assumedBy:newServicePrincipal('events.amazonaws.com'),})stateMachine.grantStartExecution(eventRole)s3EventRule.addTarget(newSfnStateMachine(stateMachine,{input:RuleTargetInput.fromObject({detail:EventField.fromPath('$.detail'),}),role:eventRole,}),)s3EventRule.addEventPattern({source:['aws.s3'],detailType:['Object Created'],detail:{bucket:{name:[bucket.bucketName],},object:{key:[{wildcard:'csv/automation-configuration/*.csv',},],},},})Enter fullscreen modeExit fullscreen modeE por fim vamos criar uma regra para ouvir as falhas inesperadas que ocorrerem em nosso step function atrav\u00e9s do event bridge tamb\u00e9m, assim mantendo o car\u00e1ter de orienta\u00e7\u00e3o a eventos que tanto adoramos:constunexpectedFailRule=newRule(this,'exam-automation-config-unexpected-fail-rule',{ruleName:'exam-automation-config-unexpected-fail-rule',})unexpectedFailRule.addTarget(newSnsTopic(userNotificationTopic,{message:RuleTargetInput.fromObject({subject:'Exam Automation Config Failed \ud83d\ude25',message:'Exam Automation Config Failed due to an unexpected error.',cause:'Unexpected Error',channel:'email',destination:{ToAddresses:['it@somemail.com']},}),}),)unexpectedFailRule.addEventPattern({source:['aws.states'],detailType:['Step Functions Execution Status Change'],detail:{stateMachineArn:[stateMachine.stateMachineArn],status:['FAILED','TIMED_OUT','ABORTED'],},})Enter fullscreen modeExit fullscreen modeColocando tudo junto em uma classe para que voc\u00ea possa entender todo o fluxo unificado:import{Duration,Fn,RemovalPolicy}from'aws-cdk-lib'import{Rule,RuleTargetInput,EventField}from'aws-cdk-lib/aws-events'import{SfnStateMachine,SnsTopic}from'aws-cdk-lib/aws-events-targets'import{Role,ServicePrincipal}from'aws-cdk-lib/aws-iam'import{Alias,Architecture,IFunction}from'aws-cdk-lib/aws-lambda'import{NodejsFunction}from'aws-cdk-lib/aws-lambda-nodejs'import{LogGroup,RetentionDays}from'aws-cdk-lib/aws-logs'import{Bucket}from'aws-cdk-lib/aws-s3'import{Topic}from'aws-cdk-lib/aws-sns'import{Fail,LogLevel,Parallel,StateMachine,StateMachineType,TaskInput}from'aws-cdk-lib/aws-stepfunctions'import{LambdaInvoke,SnsPublish}from'aws-cdk-lib/aws-stepfunctions-tasks'import{Construct}from'constructs'exportdefaultclassAutomationConfigurationextendsConstruct{constructor(scope:Construct,id:string){super(scope,id)constuserNotificationTopic=Topic.fromTopicArn(this,'userNotificationTopic',Fn.importValue('infrastructure::sns::user-notification-topic::arn'),)constbucket=newBucket(this,'bucket',{bucketName:'automation-configuration',removalPolicy:RemovalPolicy.DESTROY,})constetlLambda=newNodejsFunction(this,'etl-lambda',{entry:'src/lambda/etl/etl.handler.ts',functionName:'etl',timeout:Duration.seconds(30),architecture:Architecture.ARM_64,})bucket.grantRead(newAlias(this,id.concat('alias'),{aliasName:'current',version:etlLambda.currentVersion,}),)constcategoriesLambda=newNodejsFunction(this,'insert-categories-lambda',{entry:'src/lambda/categories/categories.handler.ts',functionName:'insert-categories',timeout:Duration.seconds(30),architecture:Architecture.ARM_64,})consthospitalsLambda=newNodejsFunction(this,'hospitals-categories-lambda',{entry:'src/lambda/categories/categories.handler.ts',functionName:'insert-categories',timeout:Duration.seconds(30),architecture:Architecture.ARM_64,})const[etlTask,categoriesTask,hospitalsTask]=this.mountLambdaInvokes([{function:etlLambda,output:'$.Payload'},{function:categoriesLambda},{function:hospitalsLambda},])consterrorTopicConfig={topic:userNotificationTopic,subject:'Automation Config Failed \ud83d\ude25',message:TaskInput.fromObject({message:'Automation Config Failed due to an unexpected error.',cause:'Unexpected Error',channel:'email',destination:{ToAddresses:['suporte@somemail.com']},}),}constpublishFailed=(publishFailedId:string)=>newSnsPublish(this,`automation-config-sns-failed-${publishFailedId}`,errorTopicConfig)constjobFailed=newFail(this,'automation-config-job-failed',{cause:'Unexpected Error',})consthospitalsCategoriesParallel=newParallel(this,'auto-config-exams-parallel-map').branch(categoriesTask).branch(hospitalsTask).addRetry({errors:['States.ALL'],interval:Duration.seconds(5),maxAttempts:1}).addCatch(publishFailed('exams').next(jobFailed),{errors:['States.ALL'],})constdefinition=etlTask.next(hospitalsCategoriesParallel)constlogGroup=newLogGroup(this,'automation-configuration-log-group',{retention:RetentionDays.ONE_WEEK,removalPolicy:RemovalPolicy.DESTROY,})conststateMachine=newStateMachine(this,`${id}-state-machine`,{definition,timeout:Duration.minutes(5),stateMachineName:'automation-configuration',stateMachineType:StateMachineType.EXPRESS,logs:{destination:logGroup,includeExecutionData:true,level:LogLevel.ALL,},})consts3EventRule=newRule(this,'automation-config-s3-event-rule',{ruleName:'automation-config-s3-event-rule',})consteventRole=newRole(this,'eventRole',{assumedBy:newServicePrincipal('events.amazonaws.com'),})stateMachine.grantStartExecution(eventRole)s3EventRule.addTarget(newSfnStateMachine(stateMachine,{input:RuleTargetInput.fromObject({detail:EventField.fromPath('$.detail'),}),role:eventRole,}),)s3EventRule.addEventPattern({source:['aws.s3'],detailType:['Object Created'],detail:{bucket:{name:[bucket.bucketName],},object:{key:[{wildcard:'csv/automation-configuration/*.csv',},],},},})constunexpectedFailRule=newRule(this,'exam-automation-config-unexpected-fail-rule',{ruleName:'exam-automation-config-unexpected-fail-rule',})unexpectedFailRule.addTarget(newSnsTopic(userNotificationTopic,{message:RuleTargetInput.fromObject({subject:'Exam Automation Config Failed \ud83d\ude25',message:'Exam Automation Config Failed due to an unexpected error.',cause:'Unexpected Error',channel:'email',destination:{ToAddresses:['it@somemail.com']},}),}),)unexpectedFailRule.addEventPattern({source:['aws.states'],detailType:['Step Functions Execution Status Change'],detail:{stateMachineArn:[stateMachine.stateMachineArn],status:['FAILED','TIMED_OUT','ABORTED'],},})}privatemountLambdaInvokes(lambdasInvoke:Array<{function:IFunctionname?:stringoutput?:string}>,){returnlambdasInvoke.map(lambdaInvoke=>{returnnewLambdaInvoke(this,`${lambdaInvoke.name||lambdaInvoke.function.functionName}-task`,{lambdaFunction:lambdaInvoke.function,inputPath:'$',outputPath:lambdaInvoke?.output||'$',})})}}Enter fullscreen modeExit fullscreen mode"}
{"title": "How To Manage IAM Access Analyzer in AWS Organizations Using Terraform", "published_at": 1716825595, "tags": ["aws", "terraform", "security"], "user": "Anthony Wat", "url": "https://dev.to/aws-builders/how-to-manage-iam-access-analyzer-in-aws-organizations-using-terraform-34fi", "details": "IntroductionSince I began covering theimplementation of security controls in AWS, I have provided walkthroughs on configuringAmazon GuardDutyandAWS Security Hubin a centralized setup using Terraform. In this blog post, we will explore another security service: AWS IAM Access Analyzer. This service helps identify unintended external access or unused access within your organization. Setting up IAM Access Analyzer is simpler than the other services, so let's dive right in!About the use caseAWS Identity and Access Management (IAM) Access Analyzeris a feature of AWS IAM that identifies resources shared with external entities and detects unused access, enabling you to mitigate any unintended or obsolete permissions.IAM Access Analyzercan be used in AWS Organizations, allowing analyzers that use the organization as the zone of trust to be managed by either the management account or adelegated administrator account. This enables the consolidation of findings, which can then be ingested by AWS Security Hub in a centralized setup.Since it is increasingly common to establish an AWS landing zone usingAWS Control Tower, we will use thestandard account structurein a Control Tower landing zone to demonstrate how to configure IAM Access Analyzer in Terraform:The relevant accounts for our use case in the landing zone are:TheManagementaccount for the organization where AWS Organizations is configured. For details, refer toSettings for IAM Access Analyzer.TheAuditaccount where security and compliance services are typically centralized in a Control Tower landing zone.The objective is to delegate IAM Access Analyzer administrative duties from theManagementaccount to theAuditaccount, after which all organization configurations are managed in theAuditaccount. With that said, let's see how we can achieve this using Terraform!Designating an IAM Access Analyzer administrator accountThe IAM Access Analyzer delegated administrator is configured in theManagementaccount, so we need a provider associated with it in Terraform. To simplify the setup, we will use a multi-provider approach by defining two providers: one for theManagementaccount and another for theAuditaccount. We will use AWS CLI profiles as follows:provider\"aws\"{alias=\"management\"# Use \"aws configure\" to create the \"management\" profile with the Management account credentialsprofile=\"management\"}provider\"aws\"{alias=\"audit\"# Use \"aws configure\" to create the \"audit\" profile with the Audit account credentialsprofile=\"audit\"}Enter fullscreen modeExit fullscreen modeUnlike other security services that have specific Terraform resources for designating a delegated administrator, this is done using the more generalaws_organizations_delegated_administratorresourceas follows:data\"aws_caller_identity\"\"audit\"{provider=aws.audit}resource\"aws_organizations_delegated_administrator\"\"this\"{provider=aws.managementaccount_id=data.aws_caller_identity.audit.account_idservice_principal=\"access-analyzer.amazonaws.com\"}Enter fullscreen modeExit fullscreen modeWith theAuditaccount designated as the IAM Access Analyzer administrator, we can now create the analyzers for the organization.Creating analyzers with organizational zone of trustAs mentioned earlier, there are two types of analyzers: external access and unused access. To make the setup more configurable, we will add some variables and keep them in a separate file calledvariables.tf. To create the external access analyzer with the organization as the zone of trust, we can define the Terraform configuration as follows:# Defined in variables.tfvariable\"org_external_access_analyzer_name\"{description=\"The name of the organization external access analyzer.\"type=stringdefault=\"OrgExternalAccessAnalyzer\"}Enter fullscreen modeExit fullscreen mode# Defined in main.tfresource\"aws_accessanalyzer_analyzer\"\"org_external_access\"{provider=aws.auditanalyzer_name=var.org_external_access_analyzer_nametype=\"ORGANIZATION\"depends_on=[aws_organizations_delegated_administrator.this]}Enter fullscreen modeExit fullscreen modeSince the unused access analyzer is a paid feature, we ought to make it optional. The Terraform configuration can be defined in the following manner:# Defined in variables.tfvariable\"org_unused_access_analyzer_name\"{description=\"The name of the organization unused access analyzer.\"type=stringdefault=\"OrgUnusedAccessAnalyzer\"}variable\"eanble_unused_access\"{description=\"Whether organizational unused access analysis should be enabled.\"type=booldefault=false}variable\"unused_access_age\"{description=\"The specified access age in days for which to generate findings for unused access.\"type=numberdefault=90}Enter fullscreen modeExit fullscreen moderesource\"aws_accessanalyzer_analyzer\"\"org_unused_access\"{provider=aws.auditcount=var.eanble_unused_access?1:0analyzer_name=var.org_unused_access_analyzer_nametype=\"ORGANIZATION_UNUSED_ACCESS\"configuration{unused_access{unused_access_age=var.unused_access_age}}depends_on=[aws_organizations_delegated_administrator.this]}Enter fullscreen modeExit fullscreen mode\u2705 You can find the complete Terraform in theGitHub repositorythat accompanies this blog post.With the complete Terraform configuration, you can now apply it with the appropriate variable values to establish theAuditaccount as the delegated administrator and create the analyzers with the organization as the zone of trust.Additional considerationsIAM Access Analyzer is a regional service, so you must create an analyzer in each region. However, it primarily applies to external access analysis, which examines the policies of regional resources such as S3 buckets and KMS keys. Since unused access analysis works with IAM users and roles, which are global resources, creating multiple unused access analyzers would only increase costs without adding value. Therefore, it is recommended to create one external access analyzer per region and only one unused access analyzer in the home region.Another consideration is that there are times when the organizational zone of trust is not desirable. For example, if you wish to have full segregation of member accounts because they represent different tenants, then you would actually want analyzers created in each member account with itself as the zone of trust. This unfortunately would have to be managed at a per-account level.SummaryIn this blog post, you learned how to manage IAM Access Analyzer in AWS Organizations using Terraform by defining a delegated administrator and using analyzers with the organization as the zone of trust. If you have alsoconfigured AWS Security Hub to operate at the organization level, you can manage IAM Access Analyzer findings across accounts and regions, thereby streamlining your security operations.I hope you find this blog post helpful. Be sure to keep an eye out for more how-to articles on configuring other AWS security services in Terraform, or learn about other topics likegenerative AI, on theAvangards Blog."}
{"title": "Spring Boot 3 application on AWS Lambda - Part 6 Develop application with AWS Lambda Web Adapter", "published_at": 1716822103, "tags": ["aws", "java", "serverless", "springboot"], "user": "Vadym Kazulkin", "url": "https://dev.to/aws-builders/spring-boot-3-application-on-aws-lambda-part-6-develop-application-with-aws-lambda-web-adapter-h88", "details": "IntroductionIn thepart 5we introduced AWS Lambda Web Adapter. In this article we'll take a look into how to write AWS Lambda function with Java 21 runtime and AWS Lambda Web Adapter using Spring Boot 3.2 version. To use the newer version of Spring Boot (i.e. 3.3) it should be enough to update the version in pom.xml.How to write AWS Lambda with AWS Lambda Web Adapter using Spring Boot 3.2For the sake of explanation, we'll use our Spring Boot 3.2sample applicationand use Java 21 runtime for our Lambda functions.In this application we'll create and retrieveproductsand use DynamoDB as the NoSQL database. You can find the DynamoProductDao.java implementationhere. We also put Amazon API Gateway in front of it as defined inAWS SAM template.Spring BootProduct Controllerannotated with@RestControllerand@EnableWebMvcdefinesgetProductByIdandcreateProductmethods.@RequestMapping(path = \"/products/{id}\", method = RequestMethod.GET, produces = MediaType.APPLICATION_JSON_VALUE) \u2002\u2002\u2002\u2002\u2002\u2002public Optional<Product> getProductById(@PathVariable(\"id\") String id) { \u2002\u2002\u2002\u2002\u2002\u2002\u2002\u2002\u2002\u2002\u2002\u2002return productDao.getProduct(id); \u2002\u2002\u2002\u2002\u2002\u2002}  @RequestMapping(path = \"/products/{id}\", method = RequestMethod.PUT, consumes = MediaType.APPLICATION_JSON_VALUE) \u2002\u2002\u2002\u2002\u2002\u2002public void createProduct(@PathVariable(\"id\") String id, @RequestBody Product product) { \u2002\u2002\u2002\u2002\u2002\u2002\u2002\u2002\u2002\u2002\u2002\u2002product.setId(id); \u2002\u2002\u2002\u2002\u2002\u2002\u2002\u2002\u2002\u2002\u2002\u2002productDao.putProduct(product); \u2002\u2002\u2002\u2002\u2002\u2002}Enter fullscreen modeExit fullscreen modeUntil this point in time the application based on AWS Lambda Web Adapter looks exactly the same as based on AWS Serverless Java Container as we can re-use our Spring Boot based application.Now come the differences. They are mainly in theAWS SAM template.1)  We need to attach Lambda Web Adapter as a Lambda layer to our Lambda functions. Here is the example for GetProductByIdWithSpringBoot32WithLambdaWebAdapter  Lambda function :GetProductByIdFunction:     Type: AWS::Serverless::Function     Properties:       FunctionName: GetProductByIdWithSpringBoot32WithLambdaWebAdapter       Layers:         - !Sub arn:aws:lambda:${AWS::Region}:753240598075:layer:LambdaAdapterLayerX86:20Enter fullscreen modeExit fullscreen modeFor Lambda function running on the arm64 architecture (which currently doesn't support Lambda SnapStart) there is another Lambda Layer to be attached: arn:aws:lambda:${AWS::Region}:753240598075:layer:LambdaAdapterLayerArm64:202) In the Globals: Function: section of the SAM template we need to define the following for all Lambda functions:Globals:   Function:     Handler: run.sh     CodeUri: target/aws-spring-boot-3.2-lambda-web-adapter-1.0.0-SNAPSHOT.jar     Runtime: java21      ....     Environment:       Variables:         ....         RUST_LOG: info         REMOVE_BASE_PATH: /v1         AWS_LAMBDA_EXEC_WRAPPER: /opt/bootstrapEnter fullscreen modeExit fullscreen modeWhat we are doing here is to configure Lambda environment variable AWS_LAMBDA_EXEC_WRAPPER to /opt/bootstrap.  When we add a layer to a Lambda function, Lambda extracts the layer contents into the /optdirectory in our function\u2019s execution environment. All natively supported Lambda runtimes include paths to specific directories within the/optdirectory. This gives our Lambda function access to our layer content. For more information about these specific paths, seePackaging your layer content.We also set function handler to our web application start up script (instead of the Java Lambda Handler class). e.g. run.sh.In therun.sh scriptwe put everything (jars) in the lib folder into the class path and start our Spring Boot application main classsoftware.amazonaws.Application. In case of Spring Boot this is the class annotated with@SpringBootApplication.#!/bin/sh  exec java -cp \"./:lib/*\" \"software.amazonaws.Application\"Enter fullscreen modeExit fullscreen modeAWS_LWA_REMOVE_BASE_PATH / REMOVE_BASE_PATH - The value of this environment variable tells the adapter whether the application is running under a base path. For the detailed explanation of this parameter please read the documentation on the AWS Lambda Web Adaptermain page.Because of this definition of theREMOVE_BASE_PATH: /v1the Path variable of the API Gateway mapping to each Lambda function also needs to start with/v1likePath: /v1/products/{id}for the sample for the Lambda function GetProductByIdWithSpringBoot32WithLambdaWebAdapter belowGetProductByIdFunction:     Type: AWS::Serverless::Function     Properties:       FunctionName: GetProductByIdWithSpringBoot32WithLambdaWebAdapter       ....       Events:         GetRequestById:           Type: Api           Properties:             RestApiId: !Ref MyApi             Path: /v1/products/{id}             Method: getEnter fullscreen modeExit fullscreen modeThen we need to deploy the application withsam deploy -gand to retrieve the existing product we have to invoke the following:curl -H \"X-API-Key: a6ZbcDefQW12BN56WED2\"  https://{$API_GATEWAY_URL}/prod/v1/products/1Enter fullscreen modeExit fullscreen modeConclusionIn this article we took a look into how to write AWS Lambda functions with Java 21 runtime with AWS Lambda Web Adapter using Spring 3.2 version. As we explored, we can re-use the Spring Boot Rest Controller as well as it was the case with AWS Serverless Java Container.In the next article of the series, we'll measure the cold and warm start times for this sample application including enabling SnapStart on the Lambda function but also applying priming for the DynamoDB invocation."}
{"title": "Dead Letter Queue (DLQ) for AWS Step Functions", "published_at": 1716794918, "tags": ["aws", "serverless", "stepfunctions", "sqs"], "user": "Pubudu Jayawardana", "url": "https://dev.to/aws-builders/dead-letter-queue-dlq-for-aws-step-functions-25eo", "details": "IntroWhen it comes to building a software system, one of the most critical components is error handling, because \u201ceverything fails all the time\u201d. While it's impossible to anticipate every possible failure scenario, having a plan in place when an unexpected thing happens can always be helpful for robustness and resilience of the system.This \u2018plan\u2019 can be a simpleDead Letter Queue (DLQ)!Dead Letter Queues can act as the safety net where you can keep the messages when unexpected issues arise. This helps you to isolate problematic messages, debug the issue withoutdisrupting the rest of your workflow, and then retry or reprocess the message as needed.Many AWS Serverless services support SQS queues as the dead letter queue natively. However, Step Functions - one of the main Serverless services offered by AWS for workflow orchestration -does not supportdead letter queuesnatively(yet).In this blog post, I am going to discuss a couple of workarounds to safely capture any messages that have failed to process by a Step Function into a dead letter queue.ScenarioLet\u2019s consider a very common use case where we have a message in a SQS source queue, which needs to be processed by Step Functions. First the messages are read by a Lambda function that starts a step function execution for each message.Here, once Step function execution lambda reads the message from Source Queue and successfully starts the Step function execution, the message will be marked as successfully processed and will be deleted from the queue. If there\u2019s any errors in the Step Function execution, the message will be lost.Solution 01In order to retain the message even when there is a failure in Step Function execution, we can add a2nd SQS queue which acts as a dead letter queueas follows.The state machine will look like this:How it worksWithin the State machine, we can create a step namely \u201cSend message to DLQ\u201d which is a SDK integration for SQS SendMessage functionality.In this step, the message to be sent to the DLQ is built based on the execution input retrieved from the context parameters.In the other steps of StateMachine, as required, we can configure the Catch errors in Error handling settings where we use the above \u201cSend message to DLQ\u201d step as the catcher.This way, when an error happens in a state, we can send the message to the DLQ and re-process from there.Try this yourselfHere is a Github repository of a sample project I created to try out this solution.https://github.com/pubudusj/dlq-for-stepfunctionsYou can deploy this to your AWS account using CDK.Once deployed you can test the functionality by sending a message to the source SQS queue in below expected format.{\"metadata\":{},\"data\":{\"foo\":\"bar\"}}Enter fullscreen modeExit fullscreen modeAnd you can see the message ends up in the DLQ and the \"metadata\" object now includes the \"attempt_number\" as well.As an alternative for using the metadata section of the message to set the attempt number, you may use SQS message attribute as well.Please note:In this approach, the DLQ is not a \"real\" DLQ and is not configured with the source SQS queue. However, it will help to capture any messages that failed to process by the Step Function execution.Solution 02In this method, we will use a real DLQ that is configured with the source queue.The state machine will look like this:How it worksThere is a source SQS queue and a DLQ configured to it.In the DLQ settings, the max receive count is set as > 1 so the message will be available in the DLQ immediately after the first failure.There is a Lambda function which has the trigger set up to process messages from the source queue and initialize SF execution.In the Event Source mapping setting of this lambda function, it is required to set the report_batch_item_failures to True.First, when the message is processed by the lambda function, we set the visibility timeout of the message to a larger value. This must be larger than the time it takes to complete the Step Function execution.Then, the step function execution will be started. Here, we pass the SQS queue url and the message receipt handler values along with the original values from the message from SQS.In the example above, in order to determine if we need to simulate the failure, we use a simple choice state.If it is a successful execution, we will call the step - Delete SQS Message.  Here we use the SQS SDK integration to delete the message using the SQS queue url and the receipt handle values received in the input payload.If it is a failure, we will call a step named - \u201cSet message visibility timeout 5s\u201d.  Here we will use SQS SDK integration for the action:  \u201cchangeMessageVisibility\u201d to set the SQS message\u2019s visibility to 5 seconds. For this SDK integration, we use the SQS queue url and the SQS receipt handle values passed in the execution input.Once the message visibility is set to 5 seconds, it will again appear on the source queue after 5 seconds. However, since we have the rule \u2018max receive count\u2019 set to more than 1, the message will be immediately put into the DLQ of the source queue.Try this yourselfI have another Github repository for  you to try this in your own AWS environment. You can set it up using CDK/Python.https://github.com/pubudusj/dlq-for-stepfunctions-2To simulate a failure scenario, send a message into the source queue with a \"failed\" value as True.{\"foo\":\"bar\",\"failed\":true}Enter fullscreen modeExit fullscreen modeThis will make the step function execution fail and the message will be immediately available in the DLQ of the source queue.With this approach, you can use the native DLQ functionality when we cannot process messages in the Step Function execution.SummaryStep Functions is one of the widely used AWS Serverless services. However it doesn\u2019t support dead letter queues (DLQs) natively yet. Still there are workarounds to achieve this with few simple steps. This blog post explained two of such workarounds which help to build a better resilient system."}
{"title": "Easy AWS permissions for your EKS workloads: Pod Identity\u200a-\u200aAn easy way to grant AWS\u00a0access", "published_at": 1716786780, "tags": ["eks", "aws", "kubernetes", "security"], "user": "saifeddine Rajhi", "url": "https://dev.to/aws-builders/easy-aws-permissions-for-your-eks-workloads-pod-identity-an-easy-way-to-grant-aws-access-13oj", "details": "\ud83d\udcda Introduction:Running applications on Kubernetes is great, but sometimes they need to talk to other AWS services like S3 or DynamoDB. In the past, setting up the right permissions for your Kubernetes apps to access these AWS services was a bit of a headache. You had to jump through hoops and follow complex instructions.But now, there's a new feature called EKS Pod Identity that makes granting AWS permissions to your Kubernetes apps a breeze. With just a few clicks (or commands), you can give your apps the AWS access they need, without any complicated setup.EKS Pod Identity is a part of Amazon's Elastic Kubernetes Service (EKS), and it's designed to make your life as a Kubernetes user much easier. It's a simple, straightforward way to manage AWS permissions for your Kubernetes workloads, saving you time and effort.In this blog post, we'll explore what EKS Pod Identity is, how it works, and why you should consider using it for your Kubernetes applications running on EKS.Grant AWS Permissions to Your K8S\u00a0Apps:When you're running Kubernetes apps on Amazon EKS (Elastic Kubernetes Service), you have two main options to give them the ability to access other AWS services like S3 or DynamoDB:1. IAM Roles for Service Accounts (IRSA):This method allows associating IAM roles with Kubernetes service accounts. It supports various Kubernetes environments on AWS like EKS, EKS Anywhere, OpenShift, and self-managed clusters. IRSA uses core AWS services like IAM and doesn't directly depend on the EKS service.2. EKS Pod Identity:This EKS-specific feature simplifies how cluster admins can configure IAM permissions for Kubernetes apps. It allows directly mapping an IAM role to a Kubernetes service account through EKS APIs. Pods under the associated service account can automatically obtain temporary AWS credentials.Both options achieve the same goal\u200a-\u200agranting your Kubernetes workloads on EKS the necessary AWS permissions. However, EKS Pod Identity is a more EKS-specific and simplified approach, while IRSA is a more general solution that works across different Kubernetes environments on AWS.Pod Identity hands-on:Deploy the cluster:Execute the following commands  to provision the EKS Cluster:git clone https://github.com/seifrajhi/aws-eks-terraform.git cd aws-eks-terraform terraform init  terraform plan  terraform auto-approveEnter fullscreen modeExit fullscreen modeDeploy Pod Identity agent:To use EKS Pod Identity in your cluster, the EKS Pod Identity Agent addon must be installed on your EKS cluster. Let's install it using the below command.aws eks create-addon --cluster-name eks-pod-identity-demo --addon-name eks-pod-identity-agent aws eks wait addon-active --cluster-name eks-pod-identity-demo --addon-name eks-pod-identity-agentEnter fullscreen modeExit fullscreen modeGo to EKS Console and view the eks-pod-identity-agent under the Add-on tab.You can also take a look at what has been created in your EKS cluster by the new addon:$ kubectl -n kube-system get daemonset eks-pod-identity-agent  # Or   $ kubectl -n kube-system get pods -l app.kubernetes.io/name=eks-pod-identity-agentEnter fullscreen modeExit fullscreen modeDeploy the sample app:Below is the manifest we will be using:apiVersion: v1 kind: Namespace metadata:   name: demo-ns --- apiVersion: v1 kind: ServiceAccount metadata:   name: demo-sa   namespace: demo-ns ---  apiVersion: v1 kind: Pod metadata:   name: demo-app   namespace: demo-ns   labels:      app: demo-app spec:   serviceAccountName: demo-sa   containers:     - name: demo-app       image: amazon/aws-cli:latest       command: ['sleep', '36000']   restartPolicy: NeverEnter fullscreen modeExit fullscreen modeRun the below command to deploy the app:kubectl  apply -f manifests.yamlEnter fullscreen modeExit fullscreen modeConfigure Amazon EKS Pod Identity:Create a trust policy and configure the principal topods.eks.amazonaws.com.IAM_ROLE_TRUST_POLICY.json:{     \"Version\": \"2012-10-17\",     \"Statement\": [         {             \"Effect\": \"Allow\",             \"Principal\": {                 \"Service\": \"pods.eks.amazonaws.com\"             },             \"Action\": [                 \"sts:AssumeRole\",                 \"sts:TagSession\"             ]         }     ] }Enter fullscreen modeExit fullscreen modeUsing the above trust policy, create the IAM role.aws iam create-role \\         --role-name eks-pod-s3-readonly-access \\         --description  \"allow EKS pods readonly acces to S3\" \\         --assume-role-policy-document file://IAM_ROLE_TRUST_POLICY.json \\         --output text \\         --query 'Role.Arn'Enter fullscreen modeExit fullscreen modeThen create the IAM Policy for S3 to list buckets and get Objects.IAM_POLICY.json:{     \"Version\": \"2012-10-17\",     \"Statement\": [         {             \"Effect\": \"Allow\",             \"Action\": [                 \"s3:ListAllMyBuckets\"             ],             \"Resource\": \"*\"         },         {             \"Effect\": \"Allow\",             \"Action\": [                 \"s3:GetObject\",                 \"s3:GetObjectTagging\"             ],             \"Resource\": \"*\"         }     ] }Enter fullscreen modeExit fullscreen modeCreate the IAM Policy:aws iam create-policy --policy-name eks-pod-s3-readonly-access-policy --policy-document file://IAM_POLICY.json --output text --query Policy.ArnEnter fullscreen modeExit fullscreen modeAttach the policy to the IAM Role:aws iam attach-role-policy --role-name eks-pod-s3-readonly-access \\   --policy-arn eks-pod-s3-readonly-access-policyEnter fullscreen modeExit fullscreen modeCreate Pod Identity association:Create the EKS Pod Identity association for the Service accountdemo-sain Namespacedemo-nsfor the IAM Roleeks-pod-s3-readonly-access:$ export IAM_ROLE_ARN=$(aws iam get-role --role-name eks-pod-s3-readonly-access | jq -r '.Role.Arn')  $ aws eks create-pod-identity-association \\   --cluster-name eks-pod-identity-demo \\   --namespace demo-ns\\   --service-account demo-sa \\   --role-arn $IAM_ROLE_ARNEnter fullscreen modeExit fullscreen modeWe can get the list of current EKS Pod Identity associations using the below command:aws eks list-pod-identity-associations --cluster-name eks-pod-identity-demoEnter fullscreen modeExit fullscreen modeTest AWS EKS Pod Identity:Run the below command:kubectl -n demo-ns exec -it demo-app -- aws s3 lsEnter fullscreen modeExit fullscreen modeThe App can list of S3 Buckets \ud83c\udf89.Conclusion:In short, EKS Pod Identity is a great solution that lets you easily give your Kubernetes apps running on Amazon EKS the AWS permissions they need.Thank you for Reading\u00a0!! \ud83d\ude4c\ud83c\udffb\ud83d\ude01\ud83d\udcc3, see you in the next blog.\ud83d\ude80 Thank you for sticking up till the end. If you have any questions/feedback regarding this blog feel free to connect with me\u00a0:\u267b\ufe0f LinkedIn:https://www.linkedin.com/in/rajhi-saif/\u267b\ufe0f Twitter\u00a0:https://twitter.com/rajhisaifeddineThe end \u270c\ud83c\udffb\ud83d\udd30 Keep Learning\u00a0!! Keep Sharing\u00a0!!\u00a0\ud83d\udd30References:https://securitylabs.datadoghq.com/articles/eks-pod-identity-deep-dive/https://docs.aws.amazon.com/eks/latest/userguide/pod-identities.htmlhttps://www.eksworkshop.com/docs/security/amazon-eks-pod-identity/use-pod-identity/"}
{"title": "Amazon Forecast : Best Practices and Anti-Patterns implementing AIOps", "published_at": 1716738937, "tags": ["aiops", "sre", "forecasting", "aws"], "user": "Indika_Wimalasuriya", "url": "https://dev.to/aws-builders/amazon-forecast-best-practices-and-anti-patterns-implementing-aiops-2p5a", "details": "AIOps leverages artificial intelligence for IT operations.Forecastingis one of the most leveraged use cases in AIOps. It typically involves making predictions or estimates of a dataset based on historical data, patterns, and various quantitative and qualitative factorsTypical Forecasting use cases are:Traffic: Predicting traffic or volume fluctuations to anticipate demand and optimize infrastructure accordingly.Error Rate: Forecasting error rates to proactively identify and mitigate potential issues, ensuring system reliability.Latency: Predicting latency metrics to maintain optimal performance and enhance user experience.Resources: Forecasting resource usage patterns to optimize allocation and prevent bottlenecks.Business Metrics: Forecasting various business metrics such as sales, revenue, or customer engagement to inform strategic decision-making.Error Budget Burn Rate: Forecasting the rate at which error budget is consumed to manage risk and prioritize improvements effectively.SLA Adherence: Predicting SLA adherence to ensure service level commitments are met and customer satisfaction is maintained.Time series dataAmazon Forecast works with time series data. Time series data is a sequence of data points collected, recorded, or observed over a period of time, where each data point is associated with a timestamp or time index. It is characterized by its chronological order, intervals (regular or irregular), trends (such as increasing, decreasing, or cyclical patterns), seasonality (repeating patterns with fixed periodicity), and the presence of noise, all of which influence its analysis and forecastingAmazon ForecastIt's a fully managed service offered by Amazon for metric forecasting. It's easy to use, as it allows you to integrate historical or related data by uploading them to Amazon Forecast. Once the data is uploaded, Forecast automatically inspects the data, identifies key attributes, and selects the right algorithms needed for forecasting. It then trains and optimizes your custom model. Once generated, forecasts can be visualized via the console or downloaded. Amazon Forecast also provides APIs that allow you to build solutionsGetting started is relatively easy.Import Your DataCreate your dataset group.Select the forecasting domain. AWS supports multiple domains out of the box:Retail domainInventory planningEC2 capacityWorkforceWeb trafficMetric (for forecasting metrics such as revenue, sales, and cash flow)Custom (if your requirement does not match any of the above).Then select the frequency of data.Create your data schema - the data definition of your sample data. You can use the schema builder or JSON schema.Select your data file from S3.Create an IAM role providing access to S3.Train a predictorYou can select your predictor, essentially the metric you want to forecast.Generate ForecastsYou can use the predictor to generate the forecast.For all items: Generate forecasts for all items in the input dataset.For selected items: Generate forecasts for selected items in the input dataset.Query Forecast : You can generate your query to visualize the forecastAmazon Forecast output snapshotIn Amazon Forecast, P10, P50, and P90 represent the 10th, 50th, and 90th percentiles of the forecast distribution, indicating that there is a 10%, 50%, and 90% probability, respectively, that the actual value will be less than the forecasted value.Explore InsightsExplore Insights consists of two parts:Explore Explainability- As per the AWS-provided definition, Explainability insights identify the attributes that impact your forecasts, quantify their impact relative to other attributes, and determine whether they decrease or increase forecast values.Explore What-if Analysis- As per the AWS-provided definition, What-if analysis explores modifications to the related time series, quantifies the impact of those modifications, and determines how those modifications can impact forecast valuesFew things to note about Amazon Forecast:It supports forecasting via Console, AWS CLI, or Python notebook.Ability to customize forecast parameters.Modularized - meaning able to track model drift, what-if scenarios, or forecast explainability.Pricing:Pricing is based on imported dataset and the time taken for predictor training, as well as the number of forecast data points.Best practices to follow when using Amazon Forecast:Data quality: Ensure your dataset is clean and accurate, removing outliers or noise that could mislead your forecasts.Feature engineering: Include relevant factors in your dataset to improve forecast accuracy.Fine-tune model parameters: Adjust forecast horizon, frequency, and other parameters to optimize model performance.Select the forecasting algorithm: Choose the appropriate algorithm (e.g., ARIMA, CNN-QR, DeepAR+, ETS, NPTS, Prophet) based on your dataset and needs.Continuous evaluation, validation, and monitoring: Regularly assess and improve model performance.Enable explainability and predictor monitoring: Activate these features to gain insights and track model performance.Updating data: Choose between replacement and incremental updates based on data volume and changes.Handling missing data: Use filling methods like middle, back, and future filling to address missing values and ensure accurate forecasting.Follow dataset guidelines: Adhere to guidelines to ensure optimal model performance.Use predictor monitor: Track model performance over time and make adjustments as needed.Leverage Forecast Explainability: Gain insights into how dataset attributes influence forecasts.Leverage what-if analysis: Explore the impact of altering related time series on baseline forecasts.Pitfalls to avoid:Overfitting Models: Occurs when over-configuring or fine-tuning to prevent noise, instead focus on identifying key patterns.Complex Algorithms: Stick to Amazon Auto Model selection unless there are specific reasons to use a different algorithm.Seasonality and Trends: Ignoring seasonality can lead to unsatisfactory forecasts; ensure relevant features and data are included to uncover patterns.Improper Data Preparation: Prepare data meticulously by clearing, normalizing, and using feature engineering techniques for optimal forecasting results.Not Spending Enough Time: Forecasting is challenging; allocate sufficient time to understand, evaluate, validate, and make necessary adjustments continuously.Lack of Business Context: Understand the purpose of forecasting and factors impacting results; if necessary, conduct reverse engineering to clarify data and goals."}
{"title": "Leveraging Amazon Titan Text Premier for Agent-Based AI in Software Testing", "published_at": 1716731031, "tags": [], "user": "Adeline Makokha", "url": "https://dev.to/aws-builders/leveraging-amazon-titan-text-premier-for-agent-based-ai-in-software-testing-581g", "details": "IntroductionAmazon Titan Text Premier, now available through Amazon Bedrock, is a state-of-the-art generative AI model that can revolutionize various fields, including software testing. This article provides a detailed guide on how to implement Retrieval-Augmented Generation (RAG) and agent-based generative AI applications to enhance software testing processes, optimizing outcomes with these advanced technologies.Understanding RAG and Agent-Based Generative AIRetrieval-Augmented Generation (RAG)RAG combines retrieval-based techniques with generative models to create systems capable of fetching relevant information from extensive data sets and using this context to generate high-quality responses. This is particularly useful for tasks requiring detailed and contextually accurate outputs, such as creating comprehensive test cases or documentation.Agent-Based Generative AIAgent-based generative AI employs autonomous agents powered by generative models to perform tasks like test case creation, scenario simulation, and software interaction. These agents can learn and adapt from their interactions, making software testing more efficient and effective.How to Implement RAG and Agent-Based Generative AI in Software TestingStep 1: Setting Up the Environment1.1 Accessing Amazon BedrockLog into your AWS account and go to the Amazon Bedrock service.Ensure you have the necessary permissions to use the Amazon Titan Text Premier model.1.2 Provisioning the Titan Text Premier ModelFollow the AWS documentation to set up the Titan Text Premier model in your AWS environment.Configure the model to meet your specific software testing needs.Step 2: Creating a RAG System for Test Case Generation2.1 Preparing the DataCollect a comprehensive set of documents, including user manuals, past test cases, and bug reports.Use a retrieval system like Elasticsearch or Amazon Kendra to index this data for efficient searching.2.2 Implementing the RAG FrameworkDevelop a retrieval component that queries the indexed data based on test requirements.Integrate the Titan Text Premier model to generate test cases using the retrieved information.2.3 Automating Test Case GenerationCreate automation scripts to streamline the process of retrieving and generating test cases.Use these generated test cases to enhance your existing test suite for broader and more thorough testing.Step 3: Deploying Agent-Based Generative AI for Dynamic Testing3.1 Defining Agent Roles and ScenariosIdentify the types of agents needed, such as UI testers, API testers, and performance testers.Define scenarios for these agents to cover, including edge cases and common user interactions.3.2 Developing Agent LogicUse the Titan Text Premier model to enable agents to dynamically generate and execute test scripts.Implement logic for agents to adapt and learn from test results, improving their effectiveness over time.3.3 Integrating with CI/CD PipelinesConnect the agent-based testing system to your Continuous Integration/Continuous Deployment (CI/CD) pipeline.Ensure agents can autonomously start tests, analyze results, and report issues, supporting continuous testing.Benefits of Using Amazon Titan Text Premier in Software TestingComprehensive Test CoverageRAG and generative AI allow for the creation of a wide range of test scenarios, including those that might be overlooked by human testers, ensuring thorough test coverage.Enhanced EfficiencyAutomating test case generation and execution reduces manual effort and speeds up the testing process, enabling testers to focus on more complex issues.Continuous ImprovementGenerative AI models learn from test results, continuously improving the accuracy and relevance of generated test cases and scenarios.ScalabilityAgent-based systems can easily scale to handle large test suites and extensive applications, providing robust testing capabilities without significant additional resources.ConclusionIntegrating Amazon Titan Text Premier into your software testing framework with RAG and agent-based generative AI greatly enhances testing efficiency and effectiveness. By automating and optimizing test processes, organizations can achieve higher-quality software products with faster release cycles. Amazon Bedrock's advanced infrastructure and capabilities make it feasible and highly beneficial to implement these innovative AI techniques.Embrace the future of software testing with Amazon Titan Text Premier and transform your testing strategies for superior results.REFERENCEBuild generative AI applicationsAWS AI Service Cards"}
{"title": "DC Bat Cowls with Amplify Gen 2 Fullstack Typescript", "published_at": 1716713999, "tags": ["devchallenge", "awschallenge", "amplify", "fullstack"], "user": "Mark Ramrattan ", "url": "https://dev.to/aws-builders/dc-bat-cowls-with-amplify-gen-2-fullstack-typescript-43ac", "details": "This is a submission for theThe AWS Amplify Fullstack TypeScript ChallengeWhat I BuiltI built an application that helps users find the DC Bat Cowls \ud83e\udd87 trait rarity usingAmplify Gen 2 with Typescript. What a Bat Cowl is can be foundhere. The marketplace for them ishere. Summary on Bat Cowls is a really cool project that has enabled me to create our own DC Comic... and created our own super villain within the DC Universe.DemoYou can view a Live Demo of the application here:https://www.dcbatcowls.comGit Repository:https://github.com/markramrattan/dcbatcowlsJourneyMy process (journey) involved spending the Amplify Gen 2 release week soaking up all the information each day. That was wild!\ud83e\udd2a (plus the 4 hour AWS Twitch stream) and an incredible insight into the BIG steps forward the AWS Amplify team is making. I could probably write 5 blogs... on all the AWS AMPLIFY updates \ud83d\ude02 (Yes it's capitalised for a reason aka it's AMPLIFIED...) though if you want to read what's new check out these two blogs:https://aws.amazon.com/blogs/mobile/team-workflows-amplify/https://aws.amazon.com/blogs/mobile/new-in-aws-amplify-integrate-with-sql-databases-oidc-saml-providers-and-the-aws-cdk/First thing I did was do the Quickstart during the Amplify Gen 2 release week. That template gives you a fast and quick (hands-on) insight on the new features. If you want to check it out, it'shere. I used the NextJS App  Router version.What's weird or interesting \ud83e\uddd0 is my own development journey has switched to NextJS and Typescript. Then this pops up... with AWS Amplify Gen 2 using a TypeScript-based, code-first developer experience (DX) for defining backends.The Gen 2 DX offers a unified Amplify developer experience with hosting, backend, and UI-building capabilities and a code-first approach.I think that means... I am a *Full Stack developer... *I remember back in the days of putting Front-end Developer on my C.V. \ud83d\ude06 Those days are LONG gone\ud83d\udca8 I am enjoying this Full Stack Developer experience. A great understanding of how Amplify works is the concepts section (probably a great place to start). The page has a great overview on how to reframe your mind on what it currently is now:https://docs.amplify.aws/nextjs/how-amplify-works/concepts/This diagram from the page above is great to visually understand the constructs:Ok, enough theory! \ud83e\udde0 We did the reading, let's build and participate in this Amplify Gen 2 challenge. The initial Base of the project is the quickstart template. Sometimes it's better to not start from a blank canvas. Gutted it out and started working on my data models...How was I going to store the data?What would it look like?Would users find it beneficial being in that format?Reflecting, I probably did it the wrong way around. I should of spent more time with users and work backwards. Though... this project is still in development and I'll continue to work on it after the challenge is done.I really enjoyed learning about constructing the data model. I am used to Amplify Studio and doing it the visual way and this is a big change for me. I found it a better experience (code first)... which is weird (as I am a more visual person).After spending time working out the model. I used the data manager to input the data needed. I wish there was a way to add the data in bulk (i.e.) upload this csv, reformat it and populate it in the tables (there probably is... if you know how, drop a comment or tag me). Though saying that, when the Bat Cowls get re-minted on the blockchain, i'll probably use the API to populate the data.Connecting up the model to the Front UI was fun. I used tailwind and reverted to what I know (not always best), though I was able to quickly design it and push out something cool! \ud83d\ude0eYes the different traits are clickable and it takes you to more details (rarity).For me, this looks Cool! \ud83d\ude0e and built SO FAST\ud83d\udca8 Normally it takes me at least a month to get it to this level. Instead it's increased my speed of development. I used Amazon Q with Amplify integration! That shizzle is SOO GOOD! I would say it's a 10x improvement in development speed (for me). Though lots still to learn! and excited about integrating more user benefits.I also added a Web3 wallet fromThirdWebso users in the future will be able to connect their Bat Cowls to the application. Great integration and has the ability to specify wallets and restrict usage to particular blockchains.<ConnectButtonclient={client}wallets={wallets}chain={defineChain(11297108109)}></ConnectButton>Enter fullscreen modeExit fullscreen modeI added the Data component for this project. I could easily add the other three (Authentication / Serverless Function / File Storage). However, I tried to make this about self learning and improvement aligned to building something of benefit to the user.Going forward (continuously learning), I'll definitely be integrating more components when needed for user benefit.Thank you for reading my blog. Feel free to check out what I've created. All feedback is welcome \ud83d\ude0eEditor: Dr Shivani KochharUpdated Data for DC Bat Cowls byJake&NFTink3r"}
{"title": "How important is having great soft skills as a Software Engineer for personal growth and career success", "published_at": 1716696984, "tags": [], "user": "Ige Adetokunbo Temitayo", "url": "https://dev.to/aws-builders/how-important-is-having-great-soft-skills-as-a-software-engineer-for-personal-growth-and-career-success-46o1", "details": "As an Engineer, I would like to understand what I need to do to ensure that my soft skills are equally as good as my technical skills. Technical skills are the fundamental requirements for engineers; however, soft skills also play a vital role in career success, advancement, and effectiveness in a working environment.In the modern workspace having adequate soft skills is becoming very vital for career success and personal growth. Soft skills are now becoming a distinguishing factor among different great Engineers. Soft skills such as ability to engage in effective and clear communication among colleagues, teamwork, problem-solving, and adaptability.I will be walking you through some practical tips and suggestions on how you can enhance your soft skills as a Software engineer. These are suggestions from my experience and also having checked success stories of some great Software Engineers.Step 1: Active ListeningIt is very important to cultivate the habit of active listening and being fully present in a conversation. In addition, also avoid interrupting and focus on understanding what message the speaker is trying to pass across. One use case will be to fully understand the client's requirements before engaging in executing the assignment.Additionally, it is not only listening to the speaker but also confirming you understand the message the speaker is trying to pass across by summarizing what the speaker has said and asking clarifying questions where necessary.Step 2: Effective CommunicationWhen delivering your message, ensure your message is clear and concise by avoiding any jargon, being direct, or any irrelevant message. It is also very important to pay attention to body language and ensure you engage in eye contact with your audience.Also, practice engaging in conversation with non-technical stakeholders and getting feedback to ensure that your stakeholders can understand your message.Step 3: Teamwork and CollaborationIt is very essential to understand your team\u2019s goal and objective; this will help you to understand how you can add value and be an effective member of your team. Develop the habit of having a shared purpose and objective within your team and also carrying every member of the team along when you are working on an important feature. Ensure you regularly showcase your work and contribute positively towards the growth and development of the team. Additionally, during conflict resolution; it is very vital to use empathy to understand different options and find an agreed common resolution which is signed off by every member of the team.Step 4: Developing Problem-Solving and Critical Thinking SkillsCultivate the skills of breaking down problems into manageable and smaller parts or deliverables; this will help you to achieve your tasks faster and boost your confidence toward working on more complex tasks. Use frameworks like SWOT analysis to evaluate your situation.Critical thinking is a crucial soft skill; you should regularly engage in brainstorming sessions and be open to unconventional solutions to solving problems.You should also be open to continuous learning and developing of changing and new technologies and methodologies.Step 5: Adaptability and FlexibilityAlways be on the lookout for an opportunity to embrace change any time it presents itself. See a change as an opportunity for growth and development. As an Engineer always embrace an opportunity to learn new technologies and subscribe to learning platforms such as Coursera, Udemy, Udacity, and Linkedln Learning to mention but a few to learn and acquire new technical and soft skills.Step 6: Leadership and Management SkillsTaking ownership and successful completion of a project is a very critical soft skill. Taking ownership shows that you are responsible and reliable. Whenever you have an opportunity to lead projects and teams ensure you work on this effectively and see it as an opportunity for you to showcase your skills and be at the top of your gameConclusionEngineers need to invest time and effort in developing their soft skills to achieve a successful career and be ahead of their pairs. Improving your soft skills will require conscious, intentional efforts and continuous practice. By focusing on active learning, effective communication, teamwork, and collaboration, developing problem-solving and critical thinking skills, adaptability and flexibility, and leadership and management skills you can significantly enhance your effectiveness and succeed at your workplace. Embrace the challenge by investing in your personal growth and see how your soft skills will pave the way for new opportunities and achievements."}
{"title": "Dive into Linux Working", "published_at": 1716678360, "tags": [], "user": "Yashvi Kothari", "url": "https://dev.to/aws-builders/dive-into-linux-working-474p", "details": "Let's dive into Linux WorkingUnderstanding the Linux Operating System: A Deep Dive1. Kernel InitializationThe kernel is the heart of the Linux operating system. During boot, several critical steps occur:BIOS/UEFI and Bootloader:The BIOS or UEFI firmware initializes hardware components.The bootloader (e.g., GRUB) loads the Linux kernel (vmlinuz).Kernel Initialization:The kernel initializes hardware devices (CPU, memory, storage, etc.).It sets up essential data structures (e.g., process tables, page tables).The root filesystem is identified and mounted.2. Init ProcessThe kernel starts the init process (traditionallyinit, but modern systems usesystemd).Init is responsible for:Starting system services and daemons.Managing user sessions and spawning user-space processes.3. User Space InitializationInit spawns the first user-space process (usuallyinitorsystemd).User sessions (e.g., graphical desktop environments) start from here.4. Filesystem HierarchyLinux follows a standard directory structure:/: Root directory./bin,/sbin: Essential system binaries./etc: Configuration files./home: User home directories./var: Variable data (logs, caches)./tmp: Temporary files./usr: User programs and libraries./opt: Optional software./dev: Device files./proc: Virtual filesystem for process information.5. Processes and SchedulingThe scheduler (e.g., Completely Fair Scheduler) manages process execution.Processes are created usingfork()andexec()system calls.Priorities, nice values, and CPU affinity affect scheduling.6. Memory ManagementVirtual memory management:Page tables map virtual addresses to physical memory.TLB (Translation Lookaside Buffer) caches page table entries.Demand paging and swapping optimize memory usage.Memory allocation:malloc(),free()manage dynamic memory.Kernel memory management handles system memory.7. File I/O and System CallsFile descriptors (stdin, stdout, stderr) facilitate I/O.System calls (e.g.,open(),read(),write(),close()) interact with files.8. NetworkingNetwork stack:IP, TCP, UDP protocols.Socket API for network communication.Network configuration:ifconfig,ipcommands.Routing tables determine packet forwarding.9. Security and PermissionsUsers, groups, and permissions control access.sudoallows privilege escalation.File integrity checks (e.g.,md5sum,sha256sum) verify file integrity.10. Device DriversKernel modules manage hardware devices.Examples:usb-storage,e1000(Ethernet),i915(graphics).11. Logging and Debuggingsysloganddmesgprovide system logs.Debugging tools (e.g.,strace,gdb) help diagnose issues.12. Shutdown and RebootInit sends signals to processes.Filesystems are unmounted.The system halts or reboots.\ud83c\udf1f Cheatsheet Linux\ud83d\udc27Linux Basics:Commands:ls: List files and directories.cd: Change directory.pwd: Print working directory.cp: Copy files or directories.mv: Move or rename files.rm: Remove files or directories.File Permissions:chmod: Modify file permissions.chown: Change file ownership.chgrp: Change group ownership.Processes:ps: List running processes.top: Monitor system processes.kill: Terminate processes.nice: Adjust process priority.Package Management:yum(RPM-based systems): Install, update, and manage packages.apt-get(Debian-based systems): Similar functionality.File System Hierarchy:Understand the directory structure:/: Root directory./bin: Essential system binaries./etc: Configuration files./home: User home directories./var: Variable data (logs, caches)./tmp: Temporary files./usr: User programs and libraries./opt: Optional software./dev: Device files./proc: Virtual filesystem for process information.Networking:ifconfigorip: Network configuration.ping: Check network connectivity.netstat: Network statistics.ssh: Secure shell for remote access.iptables: Firewall rules.Shell Scripting:Create and execute shell scripts:Variables.Loops (for, while).Conditionals (if, else).Functions.Input/output redirection.System Administration:User management:useradd,userdel,passwd.Disk management:df,du,mount.Cron jobs:crontab.Security:sudo: Execute commands with superuser privileges.File integrity checks:md5sum,sha256sum.Firewalls and SELinux.Sharing working overview, and there's much more to explore!Feel free to ask questions and share with beginners starting with AWS DevOps. Happy learning! \ud83c\udf1f\ud83d\udc27"}
{"title": "The Neighborhood Domain will Quickly Improve your Modeling Skills", "published_at": 1716652613, "tags": ["aws", "serverless", "architecture", "programming"], "user": "Benjamen Pyle", "url": "https://dev.to/aws-builders/the-neighborhood-domain-will-quickly-improve-your-modeling-skills-4d9f", "details": "Microservices are about domain decomposition.  This decomposition allows code to be deployed that is more isolated, durable, resilient, and launched in a compute form factor designed for the requirements.  In addition, the isolation allows for teams to move on their deployment schedules and cadences.  These reasons are attractive, but they do come with domain complexity that often manifests itself as architectural complexity.  And while I can't remove the architectural complexity, I do want to offer an approach to domain modeling that will feel more familiar.  Introducing the domain neighborhood.Domain ModelingModeling a business domain is one of those tasks that everyone agrees is important but often gets overlooked when it comes time to implement a new software solution.  Speaking from experience though, skipping this step will create confusion and miscommunications that can become both costly in time as requirements are written and also costly in rework when things aren't implemented in congruence with what the domain is.  Software systems operate better when they look and sound like the businesses they model.When doing this modeling work, an order of business that a domain modeler or architect must do is look at the macro picture of the business solution and begin to carve out boundaries.  Boundaries in the domain world are often referred to as bounded contexts.  A bounded context can be defined as a grouping of related microservices that operate to perform a set of functions on a user's behalf.  Bounded contexts are made up of multiple aggregate roots that form a collective bond which solidifies the ubiquitous language that a group of people operate within.That's a lot of words that can often be difficult to wrap your head around.  When modeled, it often looks like this.A more Neighborhood Domain WayThe above diagram shows two bounded contexts representing a single solution.  The example is basic, but the concepts apply to more complex examples as well.An Orders bounded context deals with a user adding items to an order and submitting a payment.  These boxes represent individual microservices that perform their operations and work in concert to deliver the intended functionality.In a neighborhood domain, that same user also exists in the Users bounded context. The operations and behaviors in this domain deal with preferences, previous orders, and notifications that the user must manage.  It's still the same user as in the orders domain, but just in a different context.  It will also be a separate microservice that is deployed and built from a different codebase than the one that is in orders.Introducing the Neighborhood DomainTake what I've written so far and the two bounded contexts described and let's think of them more as neighborhoods.  A neighborhood domain will contain the same thing as a bounded context with multiple aggregate roots but let's describe it with concrete things we can all agree upon.NeighborhoodLet's get a little bit more concrete as I introduce this.  A neighborhood is a collection of microservices that are related to performing a set of user-expected operations.  They work together to accomplish shared tasks.   It's very much like a community.  It's a boundary that all of the homes collectively reside in.To make this a little bit more concrete, a neighborhood in AWS is managed by one of the following:Application Load BalancerNetwork Load BalancerAPI GatewayRoute 53 Hosted ZoneThe neighborhood is the top-level piece of a neighborhood domain.House as a MicroserviceIf a neighborhood has no houses, is it a neighborhood?  In the neighborhood domain, houses are the microservices.  A house has entities that live in it and those entities perform operations and tasks.What makes a house though a part of the community is to be associated with a neighborhood.  The house could be implemented as a container hosted in EC2, ECS, or EKS.  Or the house could be a series of Lambda Functions or even something like LightSail.   If we carry this even further, houses need roads to connect them to other neighborhood properties.  Those relate to API Gateway routes or Load Balancer rules.Some LagniappeI've been talking so far about another way to think about bounded context and aggregate roots, but I've also been mixing in concrete items like AWS services to drive the points home.  I want to give you a little something extra or \"Lagniappe\" that will further extend the harmony that is a multi-domain business solution.In the USA, Home Owners Associations (HOA) have become common governing bodies to make sure that homes and their owners stay within a certain set of approved bylaws.  These bylaws enforce the way neighbors keep their homes in the community up to standards.I know what you are thinking right now ... where in the world is this going?  I'll tell you.Services often need to talk over APIs to other services.  What's to prevent a service from being a \"noisy neighbor\" or a neighbor that doesn't honor the rules of the community?Enter the service mesh.  A service mesh operates as an HOA in a neighborhood to set some common guidelines about how services will talk to each other and protect the neighborhood from a house that's not operating under the bylaws.Bringing it All TogetherThe neighborhood domain is just another way to think about a 20 + year-old concept that was introduced by Eric Evans in his bookDomain Driven Design.When looking at complex domains as neighborhoods, this is what the original diagram would look like.Neighborhoods, houses, and HOAs are constructs that you can use to model the bounded contexts and aggregate roots that exist in your world.Wrapping UpThis was a much different article than what I normally write about.  The genesis though was some work I was doing with a client recently and when trying to help them break up a large domain, the concept of neighborhoods showed up.  Bounded contexts and aggregate roots are tough to explain, but neighborhoods, well everyone knows what those are.  Getting people to rally around communities of homes operating together to serve a user's purpose makes the work of building a ubiquitous language that much simpler. And as an architect, building that language is much more important than many give it credit for.I hope that this new way of thinking about domain modeling will give you another tool to build amazing software that delights your customers.  Because remember, software needs to be either fun or useful.  If neither, then why do it?  And most importantly, software is ateam sport.  And working as a team makes everything go better, faster and more efficient.Thanks for reading and happy building!"}
{"title": "AWS Storage Gateway for Home Backups", "published_at": 1716627663, "tags": ["storagegateway", "backup", "aws"], "user": "Piotr Pabis", "url": "https://dev.to/aws-builders/aws-storage-gateway-for-home-backups-5e0o", "details": "World Backup Day is long behind us but it's never too late. I was thinking about methods to backup my data at home. I have some USB disks, old computers that still contain some data - maybe valuable, maybe not.Usual cloud solutions might be of decent price - both MEGA and iCloud cost 10\u20ac per month for 2TB. This is comparable to S3 Glacier Flexible retrieval. However, if I decided to create rules for moving to Deep Archive, the price goes down quickly - less than 3\u20ac per month for 2TB. What is more I don't have full insight into how is the data handled by user-friendly cloud providers. With AWS I know that I can configure cross-region replication and the data is kept in multiple AZs (except One Zone IA). Nevertheless, in this case I am still limited to a single provider. There are still other problems like privacy, data sovereignty and jurisdiction.One of the issues with S3 is finding a decent client to interface with it. There are some open-source and commercial providers out there: S3 Explorer, S3 Pro, Cyberduck, Filezilla to name a few. But AWS has its own offerings as well, so why not use one directly from the source? There's clunky S3 Console in the browser. There's AWS CLI. But the most comfortable to use is AWS Storage Gateway. With this solution, S3 buckets are visible as standard NFS or SMB shares.Today, I will guide you through setting up AWS Storage Gateway and exposing it on the local network - with two prerequisites: a switch/router and a computer with Ethernet port.My old PCsSo I have my old Dell laptop and an Intel NUC. Both of them were not touched since a long time. But recently, I booted my laptop. The screen only got worse - both bubbles under the touchscreen and internal LCD layers separating or getting moisture.Not only that but sometimes the laptop just plain glitched. It happened mostly during boot time. After this glitch the laptop would not boot - it only blinked the HDD LED. I had to unscrew it and pop RAM sticks and SSD out and put it in again. But even after that it started complaining about \"Memory misconfiguration\". Eventually I got back to Linux.I tried to create the AWS Storage Gateway VM on this laptop. It wasn't as easy as it sounds but eventually after debugging with Google, I got it working in a strict setup.Starting AWS Storage GatewayAs I am just a home Linux user, I don't have any VMWare or Hyper-V setup. Fortunately, AWS offers also a KVM image. To obtain the image, go to AWS Storage Gateway in your AWS Console. Click\"Create new gateway\". You don't need to fill out anything for now. At the bottom of the page you have a list of supported platforms. Download the image for your hypervisor.I downloaded it and tried to run it with the XML configuration provided forvirsh, even after tweaking the memory, CPU and image location, I couldn't get to the console. But hey, it's aqcow2, so it's just a standard Qemu image.Booting it with VirtualBox didn't work, so I used standardqemucommand to run it and viola! I see the login screen in my GTK window. However, this this is not everything. We need to configure networking so that our VM is accessible from the local network. Also, keep in mind that the disk provided by AWS has up to 80GB by default and will grow eventually. So either you have to resize it or make a lot of free space on your host.$sudoaptinstallqemu-system qemu-system-x86 qemu-utils# Ubuntu 20.04 LTS$qemu-system-x86_64-m4096-hdaprod-us-east-1.qcow2-accelkvm# Run the VMEnter fullscreen modeExit fullscreen modeConfiguring the networkJust booting into the machine might not be enough. It uses NAT on the host by default and I want to be able to access the NFS from my other machines on the local network. To do this, I had to add a bridge interface. I want the easiest setup possible so I will usebridge-utilspackage.$sudoaptinstallbridge-utils$sudobrctl addbr aws0# Creates a bridge interface aws0$sudobrctl addif aws0 wlp2s0# Adds the physical interface to the bridgecan't add wlp2s0 to bridge aws0: Operation not supportedEnter fullscreen modeExit fullscreen modeOh no! I can't add my WLAN interface as a bridge. Luckily, my laptop has RJ-45, so I can just connect it with a cable. Bridging over WLAN is more work than forwarding ports over NAT, so I will just stick to the cable \ud83d\ude05. I followedthis guideto configure the bridge for QEMU. It would be very useful to make the steps below automated, otherwise you will need to repeat them with every system restart.$# Create a bridge interface$sudobrctl addbr aws0$sudobrctl addif aws0 eno1# Change to your own Ethernet interface$# Bring up the interfaces$sudoifconfig eno1 up$sudoifconfig aws0 up$# Allow bridged connections traffic$sudoiptables-IFORWARD-mphysdev--physdev-is-bridged-jACCEPT$sudodhclient aws0# I will use my home network's DHCP for the bridge$curl https://api.ipify.org# Check if we have public connectivity stillEnter fullscreen modeExit fullscreen modePreparing cache storageIn order to use the File Gateway, you need at least one extra virtual disk that will be used for cache. We will useqemu-imgcommand to create one and then format it to XFS. I don't know if it's necessary, maybe Storage Gateway supports unformatted disks. But it's still good to go through the process to learn more about QEMU \ud83d\ude06.I will create first 160GB raw image. Next I will \"mount\" it usingqemu-nbdwith network block device kernel module on my host - it is needed to create disk devices for QEMU images on the host. It's a lower level step than mounting a disk such as ISO images on a loop device. For convenience, I will usegpartedto format the disk.$qemu-img create-fqcow2 aws-cache.qcow2 160G$sudomodprobe nbd# Load nbd driver on the host$sudoqemu-nbd-c/dev/nbd0 aws-cache.qcow2# Mount$sudogparted /dev/nbd0Enter fullscreen modeExit fullscreen modeFirst create partition table on the disk: SelectDevice->Create partition table. I created classic MBR partition. I don't know if it will work with GPT. Next, create a new partition usingPartitionmenu and selectxfsas the format. Apply all changes.After finished work in GParted, remember to unmount any disks that could be automatically mounted and delete thenbddevice.$sudoqemu-nbd-d/dev/nbd0Enter fullscreen modeExit fullscreen modeRunning the GatewayNext we can start our Storage Gateway VM with QEMU. I gave my VM 8GB of RAM as 4-16 is recommended. The MAC address can be almost anything unique. MACs starting52:54:00are locally administered devices, similar to private IP addresses in layer 3. What is also useful is to set KVM acceleration for QEMU and set at least 2 vCPUs. Even with these settings it might take a while for the VM to be ready.$qemu-system-x86_64-m8192\\-hdaprod-us-east-1.qcow2\\-hdbaws-cache.qcow2\\-netnic,model=virtio,macaddr=52:54:00:88:99:11\\-netbridge,br=aws0\\-smp2\\-accelkvmEnter fullscreen modeExit fullscreen modeOnce your VM is started you have to log in. Useadmin,passwordcredentials. You should be greeted with the AWS Storage Gateway configuration wizard. I let myself compare the IP that is seen by the VM and what is registered in my router. The bridge connection is working!You see multiple options in the configuration screen. You can test the connectivity first by selecting the appropriate option. To start the real work, press0to get the activation key. Fill the values which are needed for your gateway. For most use cases it will be just1each time, except the region (for me it iseu-west-1). You should get the activation key.Now, go back to AWS. Create a new gateway. Fill all the information for your gateway - name, timezone and select File Gateway. On the second page, in connection options, selectActivation keyand type the key you got from the VM. It might take time to activate the gateway.We should be greeted with a success message that our gateway is activated. We can proceed to some more configuration required for the file gateway. Also there might be some updates to install. My image was very outdated so just let it install using \"Apply immediately\" before proceeding.Configuring cacheAs we did in some previous section, we created another virtual disk that is supposed to be used as gateway's cache. Now we need to specify it in AWS so that it can start using it. Go toActions->Configure cache storage. Select what is available.Creating a bucket and a shareLet's go to our S3 console and create a bucket. Leave all the settings as default but choose some unique name. Keep the region the same as your Storage Gateway is located. In this bucket all the files we share via Storage Gateway will be reflected.Next go to Storage Gateway console andFile shareson the left. Create a new NFS share with the specified gateway and bucket. I then edited the share access settings and set the allowed clients to my home network subnet, that is192.168.86.0/24.After the share is created, you can look into its details. At the bottom of the page there are example commands for mounting the share from the local network. As of May 2024, the Mac command has a redundant space in options-onearnolockso remember to remove it \ud83d\ude05. And change below IP to the one of your gateway.$# On Linux$sudo mkdir/mnt/share$sudomount-tnfs\\-onolock,hard\\192.168.86.40:/my-home-gw-123456\\/mnt/share$# Or on Mac$mkdir~/mnt/share$sudomount-tnfs\\-overs=3,rsize=1048576,wsize=1048576,hard,nolock\\-v192.168.86.40:/my-home-gw-12345\\~/mnt/shareEnter fullscreen modeExit fullscreen modeIf you can't connect, its useful to userpcinfofor troubleshooting. Use it to get information about the RPC services (NFS being one of them). You should see something running on port 2049. If it's not visible, it can be a problem with your gateway configuration. If you see it and still can't mount orrpcinfodoes not respond, check firewall.$rpcinfo-p192.168.86.40    program vers proto   port     100000    4   tcp    111  rpcbind     ...     100003    3   udp   2049  nfs     100003    3   tcp   2049  nfsEnter fullscreen modeExit fullscreen modeOnce the filesystem is mounted I can see the files that are in the bucket in Finder. I can copy new files with ease and synchronize between different devices and the cloud. This gives me a way to not only backup but also process the files on my Mac with low latency as they are cached on the local network on the gateway.The uploads indeed have some latency. Once I copied the file from Finder, it took a good 10 seconds to eventually appear in S3.Gateway maintenanceAll the maintenance of the gateway is done through AWS console, CLI or API. There's not much you are able to do using the gateway's terminal (unless you mounth the disk on the side). You can perform updates remotely and monitor the metrics that are available in the gateway'sMonitoringtab in AWS console."}
{"title": "Reducing AWS Lambda Cold Starts", "published_at": 1716580907, "tags": ["coldstart", "python", "aws", "serverless"], "user": "Rey Abolofia", "url": "https://dev.to/aws-builders/reducing-aws-lambda-cold-starts-3eea", "details": "IntroductionAWS Lambda functions with large dependencies can suffer from significant cold start times due to the time it takes to import these dependencies. In this post, we'll explore a simple yet effective way to reduce Python cold start times without changing a line of code by precompiling dependencies into bytecode. We'll cover the following topics:Understanding Python bytecode and*.pycfilesPrecompiling dependencies to reduce init durationUsing Python optimization levels for further improvementsReducing memory overhead withPYTHONNODEBUGRANGES=1Understanding Python Bytecode and*.pycFilesWhen Python loads source code for the first time, itcompiles it to bytecodeand saves it to*.pycfiles in__pycache__directories. On subsequent loads, Python will use these precompiled*.pycfiles instead of recompiling the source code, saving time.By precompiling dependencies and removing the original*.pyfiles, we can bypass the bytecode compilation step during function initialization. This can significantly reduce init duration. For example, a simple handler withimport numpy as npcan see an init duration reduction of approximately 20%.Removing*.pyfiles affects the detail in tracebacks when exceptions occur. With*.pyfiles, tracebacks include the relevant source code lines. Without*.pyfiles, tracebacks only display line numbers, requiring you to refer to your version-controlled source code for debugging. For custom code not in version control, consider keeping the relevant*.pyfiles to aid in debugging. For third-party packages, removing*.pyfiles can improve cold start times at the cost of slightly less detailed tracebacks.BenefitsFaster imports during runtimeReduced package size by removing*.pyfilesSame*.pycfiles work on any OSHow toprecompile dependencies and remove*.pyfiles:$python-mcompileall-b.$find.-name\"*.py\"-deleteEnter fullscreen modeExit fullscreen modeCautionAlways test your code after precompilation, as some packages do rely on the presence of*.pyfiles.Using Python Optimization LevelsPython offersoptimization levelsthat can further improve init and runtime duration by removing debug statements and docstrings.BenefitsOptimization LevelEffect-ORemoves assert statements and code blocks that rely on__debug__-OORemoves assert statements,__debug__code blocks, and docstringsHow toprecompile with optimization level 2:$python-mcompileall-o2-b.$find.-name\"*.py\"-deleteEnter fullscreen modeExit fullscreen modeCautionTest your code thoroughly, as optimization levels may introduce subtle bugs if your business logic relies on assert statements or docstrings.Reducing Memory Overhead withPYTHONNODEBUGRANGES=1In Python 3.11+, you can use thePYTHONNODEBUGRANGES=1environment variable to disable the inclusion of column numbers in tracebacks. This reduces memory overhead but sacrifices the ability to pinpoint the exact location of exceptions on a given line of code.Example traceback with debug ranges:Traceback (most recent call last):   File \"hello.py\", line 1, in <module>     print(f\"Hello world! {1/0}\")                           ~^~ ZeroDivisionError: division by zeroEnter fullscreen modeExit fullscreen modeExample traceback without debug ranges:Traceback (most recent call last):   File \"hello.py\", line 1, in <module>     print(f\"Hello world! {1/0}\") ZeroDivisionError: division by zeroEnter fullscreen modeExit fullscreen modeBenefitsReduced memory overheadReduced package sizesHow toprecompile with optimization level 2 and no debug ranges:$ PYTHONNODEBUGRANGES=1 python-mcompileall-o2-b.$find.-name\"*.py\"-deleteEnter fullscreen modeExit fullscreen modeSummaryBy precompiling dependencies, using optimization levels, and disabling debug ranges, you can significantly reduce cold start times in your AWS Lambda Python functions. These techniques can lead to over 20% faster startup times, allowing your functions to respond more quickly to events. Try these optimizations in your own functions and see the performance improvements for yourself!"}
{"title": "AWS SnapStart - Part 21 Measuring cold starts and deployment time with Java 17 using different compilation options", "published_at": 1716562664, "tags": ["aws", "java", "serverless", "coldstart"], "user": "Vadym Kazulkin", "url": "https://dev.to/aws-builders/aws-snapstart-part-21-measuring-cold-starts-and-deployment-time-with-java-17-using-different-compilation-options-o14", "details": "IntroductionIn the previous parts we've done many measurements with AWS Lambda using Java 17 runtime with and without using AWS SnapStart and additionally using SnapStart and priming DynamoDB invocation :cold starts usingdifferent deployment artifact sizescold starts and deployment time usingdifferent Lambda memory settingswarm startsusing different Lambda memory settingsWe've done all those measurements using the following JAVA_TOOL_OPTIONS: \"-XX:+TieredCompilation -XX:TieredStopAtLevel=1\" defined in the AWS SAM template.yaml. This means that client compilation (c1) without profiling will be applied. It was considered the best choice due to the articleOptimizing AWS Lambda function performance for Javaby Mark Sailes. But all these measurements have been done for Java 11 and before Lambda SnapStart has been released. So now it's time to revisit this topic and measure cold and warm start times with different Java compilation options without SnapStart enabled, with SnapStart enabled (and additionally with priming). In this article we'll do it for Java 17 runtime and will compare it with the same measurements for Java 21 already performed in the articleMeasuring cold and warm starts with Java 21 using different compilation optionsMeaning of Java compilation optionsThis picture shows Java compilation available.If you don't specify any options, the default one applied for will be tiered compilation. You can read more about it in this articleTiered Compilation in JVMor generally about client (C1) and server (C2) compilation in the articleClient, Server, and Tiered Compilation. There are also many other settings so you can apply to each of the compilation options. You can read more about them in this articleJVM c1, c2 compiler thread \u2013 high CPU consumption?Measuring cold starts and deployment time with Java 17 using different compilation optionsIn our experiment we'll re-use the application introduced inpart 8for this. Here is the code for thesample application. There are basically 2 Lambda functions which both respond to the API Gateway requests and retrieve product by id received from the API Gateway from DynamoDB. One Lambda function GetProductByIdWithPureJava17Lambda can be used with and without SnapStart and the second one GetProductByIdWithPureJava17LambdaAndPriming uses SnapStart and DynamoDB request invocation priming.The results of the experiment below were based on reproducing more than 100 cold and approximately 100.000 warm starts. For it (and experiments from my previous article) I used the load test toolhey, but you can use whatever tool you want, likeServerless-artilleryorPostman. I ran all these experiments with 5 different compilation options defined in thetemplate.yaml. This happens in the Globals section where variable named \"JAVA_TOOL_OPTIONS\" is defined in the Environment section of the Lambda function:Globals:   Function:     CodeUri: target/aws-pure-lambda-snap-start-17-1.0.0-SNAPSHOT.jar     Runtime: java21     ....     Environment:       Variables:         JAVA_TOOL_OPTIONS: \"-XX:+TieredCompilation -XX:TieredStopAtLevel=1\"Enter fullscreen modeExit fullscreen modeno options (tiered compilation will take place)JAVA_TOOL_OPTIONS: \"-XX:+TieredCompilation -XX:TieredStopAtLevel=1\"  (client/C1 compilation without profiling)JAVA_TOOL_OPTIONS: \"-XX:+TieredCompilation -XX:TieredStopAtLevel=2\"  (client/C1 compilation with basic profiling)JAVA_TOOL_OPTIONS: \"-XX:+TieredCompilation -XX:TieredStopAtLevel=3\"  (client/C1 compilation with full profiling)JAVA_TOOL_OPTIONS: \"-XX:+TieredCompilation -XX:TieredStopAtLevel=4\" (server/C2 compilation)For their meaning see our explanations above. We will refer to those compilation options in the table column \"Compilation Option\" by their number in the tables below, for example number 5 stays for JAVA_TOOL_OPTIONS: \"-XX:+TieredCompilation -XX:TieredStopAtLevel=4\". Abbreviationcis for the cold start andwis for the warm start.Cold (c) and warm (w) start times without SnapStart in ms:Compilation Optionc p50c p75c p90c p99c p99.9c maxw p50w p75w p90w p99w p99.9w max12831.332924.852950.123120.343257.033386.675.736.507.8820.4949.621355.0822880.532918.792974.453337.293515.863651.656.117.058.9423.5462.991272.9632906.392950.593016.83283.313409.653593.655.736.617.8721.0753.741548.9543247.93348.823481.413673.513798.973904.136.727.759.3824.6972.671494.9854146.664231.94377.424557.214699.034780.636.117.2710.1529.87103.032062.84Cold (c) and warm (w) start times with SnapStart without Priming in ms:Compilation Optionc p50c p75c p90c p99c p99.9c maxw p50w p75w p90w p99w p99.9w max11506.201577.061845.012010.622280.4622815.826.728.3922.81798.461377.5421521.331578.641918.352113.652115.772117.426.017.058.9423.92101.411077.4531463.161532.001886.031990.622020.692021.395.926.728.0022.0995.171179.1341657.881755.072057.372158.492169.302170.656.417.278.8024.3096.691374.4352269.102340.502581.362762.912807.452808.896.417.7511.3432.861506.601941.26Cold (c) and warm (w) start times with SnapStart and with DynamoDB invocation Priming in ms:Compilation Optionc p50c p75c p90c p99c p99.9c maxw p50w p75w p90w p99w p99.9w max1708.90790.50960.611041.611148.801149.915.646.618.3821.07141.53373.372692.79758.001003.801204.061216.151216.886.217.279.3825.09103.03256.653670.98720.331007.821072.251200.451200.645.386.117.2719.1599.81303.524732.99828.881030.071271.241350.411390.036.307.058.5223.17103.03469.455937.841056.291227.141422.781445.721447.096.307.7511.1632.86122.69381.03ConclusionsFor all measurements for Java 17 we discovered that setting compilation options -XX:+TieredCompilation -XX:TieredStopAtLevel= 3 or 4 produced much worse cold and warm starts as the tiered compilation or -XX:TieredStopAtLevel=1 and 2 (client compilation without or with basic profling). With Java 21 we observed much worse cold and warm starts also starting with -XX:TieredStopAtLevel=2 which is not the case for Java 17.For the Lambda function with Java 17 without SnapStart enabled  tiered compilation (default one) is a better option for having lower cold and warm starts for nearly all percentiles for our use caseFor the Lambda function with Java 21 without SnapStart  enabled it\u2019s different see my articleMeasuring cold and warm starts with Java 21 using different compilation options: client compilation without profiling (-XX:+TieredCompilation -XX:TieredStopAtLevel=1 ) is the better option for having lower cold and warm starts for nearly all percentiles for our use case.For the Lambda function with Java 17 with SnapStart enabled (with priming of DynamoDB invocation or without priming) the tiered compilation or -XX:TieredStopAtLevel=1 and 2 produced very close cold and warm starts which vary a bit depending on the compilation option and percentile.For the Lambda function with Java 21 with SnapStart enabled (with priming of DynamoDB invocation or without priming) it was different: tiered compilation (the default one) outperformed the client compilation without profiling (-XX:+TieredCompilation -XX:TieredStopAtLevel=1 ) in terms of the lower cold start time and also for the warm start time for nearly all percentiles for our use case.So please review/re-measure the cold and warm start times for your use case if you use Java 17, as tiered compilation can be a better choice if you enable SnapStart for your Lambda function(s). In our case we didn't use any framework like Spring Boot, Micronaut or Quarkus which may also impact the measurements."}
{"title": "My personal AWS account setup - IAM Identity Center, temporary credentials and sandbox account", "published_at": 1716499589, "tags": ["aws", "security"], "user": "Julian Michel", "url": "https://dev.to/aws-builders/my-personal-aws-account-setup-iam-identity-center-temporary-credentials-and-sandbox-account-39mc", "details": "I work on AWS projects every day and have access to internal and customer AWS accounts. But I also manage some personal AWS accounts.  They are useful for several reasons:I can run personal AWS applications, such as the smart home application I built.I can test new features, e.g. it was very helpful to learn about the CDK pipeline when it was introduced. I was able to create the accounts as suggested in the documentation. Later I was able to apply my knowledge to the customer project.If I have some permission issues, I can try to replicate the problem in my personal AWS accounts without any restrictions. I can try new services that are only available in organizational root accounts, such as AWS Identity Center.Of course, I want to run the AWS account in a secure and convenient way. So I decided to manage the accounts as described in this article.Account structure and AWS OrganizationsLike a business, I created anAWS organizationto manage my accounts. I'm using these accounts:Root account, which owns the organizationDev, Test, and Prod accounts to develop, test, and run my applicationPipeline account, which contains an AWS CodePipeline to deploy to Dev, Int, ProdSandbox account for testing new stuff (gets nuked after testing)User management and authentication using AWS Identity CenterI'm sure AWS IAM user management was great when it first launched. Now,AWS IAM Identity Centerhas more features and is easier to use. For example, it provides an easy-to-use interface to access all accounts and roles. Or it has improvedMFA capabilities, such as support for Apple's TouchID. That is why I chose AWS Identity Center. It is set up in the AWS Organizations root account and connects to all accounts in the organization.When you open AWS IAM Identity Center, you can see the accounts and roles you can assume:Temporary credentials and LeappAWS IAM Identity Center provides temporary credentials by default, which is a good security choice. It supportsSSO integrationwith AWS CLI or manual download of access key, secret access key, and session token.Personally, I prefer to useLeapp, a tool that supports secure cloud access in multi-account environments. Recently, the company behind Leapp announced the end of the commercial version. Theopen source versionstill exists and can be used.Leapp displays all AWS accounts/roles that are configured in AWS Identity Center. If you select an account, the credentials can be used in the CLI. You can override the default profile so you don't have to pass a--profileparameter. Or, you can configure a name profile.Nuking the sandbox accountWhen I'm learning a new AWS service or testing a complex scenario, I'm not always using infrastructure as code. In this case, it takes a long time to manually clean up an AWS account and delete all (expensive) resources.To improve this process, I useaws-nuke, which automatically deletes all AWS resources in an AWS account.Be careful with this tool. The first time I used it, it also deleted the configurations required for IAM Identity Center. So I couldn't login anymore.aws-nuke supports filters to exclude resources that should not deleted. I created filters to exclude the IAM Identity Center configuration and resources created during AWS CDK bootstrapping. I ended up with these filters:presets:common:filters:IAMRole:-\"OrganizationAccountAccessRole\"-type:globvalue:\"cdk-hnb659fds-*\"-type:globvalue:\"AWSReservedSSO_AdministratorAccess_*\"IAMRolePolicyAttachment:-\"OrganizationAccountAccessRole->AdministratorAccess\"-type:globvalue:\"AWSReservedSSO_AdministratorAccess_*->AdministratorAccess\"-type:globvalue:\"cdk-hnb659fds-*\"IAMRolePolicy:-type:globvalue:\"cdk-hnb659fds-*\"IAMSAMLProvider:-type:globvalue:\"arn:aws:iam::*:saml-provider/AWSSSO_*_DO_NOT_DELETE\"S3Bucket:-type:globvalue:\"s3://cdk-hnb659fds-assets*\"SSMParameter:-\"/cdk-bootstrap/hnb659fds/version\"CloudFormationStack:-CDKToolkitECRRepository:-type:globvalue:\"Repository:cdk-hnb659fds-container-assets-*\"Enter fullscreen modeExit fullscreen modeSummaryThis setup works very well for me. AWS IAM Identity Center is great for managing users and logging into AWS accounts. Leapp is very helpful when using CLI credentials. However, it's not the only option as AWS IAM Identity Center offers other options as well. I regularly use the sandbox account when testing or learning about AWS services. By deleting everything with aws-nuke, I can easily start with an empty AWS account - and of course, I don't have to pay for unused resources."}
{"title": "AWS CI CD SETUP", "published_at": 1716498100, "tags": ["awsdevops", "cloudsecurity", "cicd"], "user": "Yashvi Kothari", "url": "https://dev.to/aws-builders/aws-ci-cd-setup-35li", "details": "Yashvi Kothari , a DevOps, Cloud Infrastructure Security Architect and DevSecOps Engineer, and cool intern, Mishi Final Year Graduate started working on assigned projects.(Take Mishi as example name \ud83d\ude05 )\ud83c\udf1fBuilding a CI/CD Pipeline with AWS ServicesLeading e-commerce company, was facingchallengesin managing their software development process.They had multiple developers working on different parts of the application, and the integration of these changes was a manual and time-consuming process.This led to frequent integration issues, delays in releases, and a lack of transparency in the development process.Now Yashvi had to take ownership and  decided to streamline the software delivery process for client.Solution:To address these challenges, we decided to implement an AWS CI/CD pipeline.This pipeline would automate the integration of code changes from multiple developers, ensuring a standardized and controlled approach to code releases.We chose AWS CodePipeline as the core component of our pipeline, as it provides a robust and scalable infrastructure for automating the software development process.Mishi has already read about CI/CD were essential for faster, safer, and more reliable software releases.She is super excited. \ud83d\ude0c \ud83e\udd29Basic Flow would be as :-1.User Pushes Code Changes:Developer pushes the code changes to a CodeCommit repository.CodeCommit RepositoryYashvi started by creating a CodeCommit repository to host her team\u2019s application code.Whenever a team member pushed changes to this repository, it would trigger the CI/CD pipeline.2.CodeCommit Triggers CodePipeline: Upon receiving the push event, CodeCommit triggers the CodePipeline.CodeCommit sent a signal to CodePipeline\u2014a powerful orchestration service.Mike (eCommerce) said, It's all magical \ud83e\ude84.3.CodePipeline initiates a build process in CodeBuild..\ud83d\udd17 CodeBuild Constructs the Application \ud83d\udd28CodeBuild fetched the latest code from CodeCommit, compiled it, ran tests, and produced a app artifact.Yashvi smiled; her application was taking shape.4.\ud83d\udce6 Artifacts in S3 BucketsCodeBuild builds the application and can optionally upload artifacts to S3.Now it optionally uploaded the artifact to an S3 bucket.Yashvi knew that these artifacts held the promise of her application\u2019s success.\ud83d\udcca CodeBuild Reports BackCodeBuild reports the build status (success or failure) back to CodePipeline, just like a loyal messenger.6.If the build is successful, CodePipeline triggers a deployment in CodeDeploy.CodeDeploy downloads build artifacts from S3 (if uploaded) and deploys the application to an EC2 instance (or another deployment target).CodeDeploy was the magician who could transform artifacts into live applications. It downloaded the build artifacts from S3.CodePipeline notifies the user of the overall pipeline status (success or failure).Now imagine application coming to life deployed to target EC2 ECS, serving users with every click.\ud83d\udd17  ECommerce *Platform Scaling*Yashvi\u2019s team used this CI/CD pipeline to deploy updates to their e-commerce platform.With each successful deployment, they improved user experience, fixed bugs, and added new features. Mishi, learned in/out and contributed to the team\u2019s success.Now AWS CI/CD pipeline in place, we able to automate the integration of code changes, reducing the likelihood of integration issues and another day of collaborative development environment.Additional Setup:-Monitor application performance using AWS CloudWatch.AWS #DevOps #CI/CD #CloudMagic_Disclaimer: _It is example inspired by multiple projects implementation and real-world practices as professional.Any resemblance to actual events or persons is purely coincidental."}
{"title": "Issue 45 of AWS Cloud Security Weekly", "published_at": 1716495291, "tags": [], "user": "AJ", "url": "https://dev.to/aws-builders/issue-45-of-aws-cloud-security-weekly-e42", "details": "(This is just the summary of Issue 45 of AWS Cloud Security weekly @https://aws-cloudsec.com/p/issue-45<< Subscribe to receive the full version in your inbox weekly).What happened in AWS CloudSecurity & CyberSecurity last week May 13-May 22, 2024?Amazon QuickSight now allows connectivity to Redshift data sources using an IAM role through GetClusterCredentialswithIAM. This enhancement builds on the previously introduced Redshift RunasRole feature by making the Database user/Database Group parameters optional, effectively linking the temporary user identity directly to the IAM credentials.Amazon Detective adds support for EKS audit logs in Security Lake integration.AWS Security Hub now supports version 3.0 of the Center for Internet Security (CIS) AWS Foundations Benchmark which includes 37 security controls, with 7 new controls that are unique to version 3.0. Security Hub has met the criteria for the CIS Security Software Certification and has been certified for levels 1 and 2 of the CIS AWS Foundations Benchmark version 3.0.Trending on the news & advisories (Subscribe to the newsletter for details):Amazon has a new CEO.Oracle goes vegan: Dumps Terraform for OpenTofu.Microsoft will require MFA for all Azure users.SEC: Financial orgs have 30 days to send data breach notifications.FedRAMP board launched to support safe, secure use of cloud services in government.Prison for cybersecurity expert selling private videos from inside 400,000 homes.Employee Personal GitHub Repos Expose Internal Azure and Red Hat Secrets.CISA and ONCD Award the Winners of the Fifth Annual President\u2019s Cup Cybersecurity Competition.Linguistic Lumberjack: Attacking Cloud Services via Logging Endpoints (Fluent Bit - CVE-2024-4323).Amazon S3 will no longer charge for several HTTP error codes.Microsoft will require MFA for all Azure users."}
{"title": "From Good to Great: Using AWS Well-Architected Tool for Cloud Excellence", "published_at": 1716488628, "tags": ["aws", "wellarchitected", "cloud", "workload"], "user": "Ahmed Srebrenica", "url": "https://dev.to/aws-builders/from-good-to-great-using-aws-well-architected-tool-for-cloud-excellence-1b68", "details": "Operation excellence, Security, Reliability, Performance Efficiency, Cost Optimisation, SustainabilityIntroductionA few years ago, AWS provided us with the AWS Well-Architected Framework on their website, which describes key concepts, design principles, and architectural best practices for designing and running workloads in the cloud. The page features six pillars:Operational Excellence, Security, Reliability, Performance Efficiency, Cost Optimization, and Sustainability. For each pillar, there is an option to read more about it, as well as a lab option. After some time, AWS enabled us to define workloads through the service AWS Well-Architected Tool. Today, I will write more about how excellent this service is and how it can help our workload function better than before.Downloaded from:https://afdclinics.com/the-six-pillars-of-health/PrerequisitesCreateAWS Account. I already have an AWS Account, so I won\u2019t be creating a new one.Define a workloadYou need to do a few steps to define your workload:Go to your account on AWS Console and type in the search barAWS Well-Architected Tool.2. Click theDefine workloadorange button.3. You need to define a few parameters for your workload, such as N*ame, Description, Review owner, Environment, and Region. Every other parameter isoptional. When you are finished, press theNext*\u00a0button4. If you want, you can create a profile or search for it. I won\u2019t do that.5. By default theAWS Well-Architected Frameworkis checked. Click theDefine workloadbutton.Workload overview part INow that we have our workload, let\u2019s see what options we have available. In theoverviewsection, we have basic information such as:Last updatedOverall questions answeredOverall risksWorkload notesTheMilestones sectionis very important. In it, we store upgrades, for example, we receive results after the first review, and then based on the results we change certain things related to our application and want to perform a new upgrade. The new upgrade will be displayed in the Milestones section, just like the old one, which will remain there.TheProperties sectionhas properties of our workload that we set before.In theShares section, we can share this with the other Principals.Start reviewingIt\u2019s time to answer the questions for each lens, just click theStart reviewingbutton.We havesix pillarson the left side, and each pillar has several questions.In the middle, we have the main question and several sub-questions.On the right side, we have explanations for every sub-question.Our task now is to go through each pillar, each question, and each sub-question, and after we finish, it\u2019s necessary to press theSave and exitbutton andSave milestonebutton.Name your milestone and click theSavebutton.Workload overview part IIWhat did you get after you answered the questions?I got 16 high-risk and 4 medium-riskand I have to do everything to minimize the risk.How can you do the same?Scroll down and you will seeLenses. Click the High risks number (16).You will see under theimprovement planwhat you need to improve.For example, the question isHow do you securely operate your workload?Click therecommended improvement itemsand you will see something like this:In my case, I will go to my app and I do as recommended.Don\u2019t forget to click every recommendation, it will guide you to AWS documentation.Continue reviewingIf you have fixed what was risky, you can go toContinue Reviewingand check off what you have fixed. Go to your workload under the AWS Well-Architected tool and click theContinue reviewingbutton.After you have checked off what you have fixed, you need to click theSave Milestonebutton. I will name itbasketball-scoreboard-1.1.Under theMilestones section, you will see your versions of the AWS Well-Architected tool.ConclusionThe AWS Well-Architected Tool is essential if you want your workload to operate according to the best AWS practices. Speaking from my experience, the AWS Well-Architected Tool has helped me a lot, especially in terms of security, so I highly recommend using it. As for pricing, there is no additional charge for the AWS Well-Architected Tool. You pay only for your underlying AWS resources.www.ahmedsrebrenica.com"}
{"title": "\"\ud83d\ude80 Streamlining Kubernetes Deployment: Setting Up EKSCTL, Kubectl, and AWS CLI on Amazon Linux 2 \ud83d\udee0\ufe0f\"", "published_at": 1716481847, "tags": ["aws", "kubernetes", "devops", "beginners"], "user": "Sarvar Nadaf", "url": "https://dev.to/aws-builders/-streamlining-kubernetes-deployment-setting-up-eksctl-kubectl-and-aws-cli-on-amazon-linux-2--1j16", "details": "Hey! It's Sarvar Nadaf again, a senior developer at Luxoft. I worked on several technologies like Cloud Ops (Azure and AWS), Data Ops, Serverless Analytics, and Dev Ops for various clients across the globe.Today, we'll look into AWS EKS's fundamentals. This post will assist you in setting up the various command-line interfaces that will enable you to communicate with the AWS EKS cluster from a laptop or other Linux-based device.Today, we will configure ESKCTL, Kubectl, and the AWS CLI. With each Linux distribution, there are a few minimal requirements and differences. I'm setting up all of these command-line tools on an Amazon Linux 2 machine today. Simply, I'll obtain a free tier Amazon Linux 2 server and let's begin configuring command-line interfaces.AWS CLI\u00a0-The open-source AWS CLI is a really powerful resource. This enables our interaction with the entire AWS service. Just by doing a few simple steps, we may operate our AWS account from anywhere.Prerequisite -AWS AccountIAM UserAccess key ID and Secret access key1.Use the following command to become the root user. so no extra sudo command for each step.[ec2-user ~]$ sudo su -Enter fullscreen modeExit fullscreen mode2.Become a Root\u00a0User Use the following command to carry out a fast software update to make sure your instance's software packages are up-to-date:[root ~]$ sudo yum update \u2013yEnter fullscreen modeExit fullscreen mode3.Updating the\u00a0server. Currently, we are downloading the zip file containing the AWS CLI package. To download the AWS CLI package from the URL, we are using the curl command. As you can see below, we are converting the package into awscli.zip using the -o option.[root ~]$ curl \"https://awscli.amazonaws.com/awscli-exe-linux-x86_64.zip\" -o \"awscliv.zip\"Enter fullscreen modeExit fullscreen mode4.Downloading AWS CLI\u00a0Package Once the command has been executed successfully, we can use the ls command to view the awscli.zip file. We are now unzipping the zip file that is on our server.[root ~]$ unzip -q awscli.zipEnter fullscreen modeExit fullscreen mode5.Checking package using ls\u00a0command As soon as the zip file is unzipped, the entire awscli package is installed. To run and execute the awscli package, we are using the following command.[root ~]$\u00a0./aws/installEnter fullscreen modeExit fullscreen mode6.Installing AWS\u00a0CLI After the AWS CLI has been successfully installed, we are just running the command below to see if it is functioning as expected or not. You have successfully configured the AWS CLI on your server if you can see the output similar to the one below.[root ~]$ aws --versionEnter fullscreen modeExit fullscreen mode7.Checking the AWS\u00a0Version At this time, your IAM user information is being configured in the AWS CLI. To obtain programmatic access to all AWS services, we must submit the access key and secret access key of the IAM user. With the AWS Configure command, we must supply all the necessary information, including the region and region code for the connection you want to make. The output format must always be JSON, so we will receive the output in JSON format. We will be able to access the service that the IAM user has granted us access to.[root ~]$ aws configureEnter fullscreen modeExit fullscreen mode8.Setting up AWS IAM\u00a0User If you can see the result of the following command after setting up the AWS CLI, your server has been successfully set up for use with the AWS CLI.[root ~]$ aws s3 lsEnter fullscreen modeExit fullscreen modeKubectl -The Kubernetes API server can be reached using the command line program kubectl. When we access our AWS EKS cluster from our system, Kubectl is a big help. We can use our machine to access all of Kubernete's resources. Many operating system package managers contain the kubectl binary. It's frequently simpler to use a package manager for installation than to download and manually install everything.1.Here, we're setting up a Kubernetes repository. One file must be created in order to build a repository. The file's repo extension contains information about the gpgkey and the kubenetes baseurl of kubectl. This file is located in the directory /etc/yum.repos.d.[root ~]$ cat <<EOF | sudo tee /etc/yum.repos.d/kubernetes.repo [kubernetes] name=Kubernetes baseurl=https://packages.cloud.google.com/yum/repos/kubernetes-el7-\\$basearch enabled=1 gpgcheck=1 gpgkey=https://packages.cloud.google.com/yum/doc/rpm-package-key.gpg EOFEnter fullscreen modeExit fullscreen mode2.Updating Repo Using the cat command, we are checking that everything appears to be as expected.[root ~]$ cat /etc/yum.repos.d/kubernetes.repoEnter fullscreen modeExit fullscreen mode3.Checking Repo Using the command below, we are installing the kubectl CLI right now.[root ~]$ sudo yum install -y kubectlEnter fullscreen modeExit fullscreen mode4.Installing Kubectl After the kubectl CLI has been installed successfully, we use the version command to determine whether kubectl has been installed correctly or not.[root ~]$ kubectl versionEnter fullscreen modeExit fullscreen modeEKSCTL -The AWS EKS cluster can be interacted with using the tool EKSCTL. We'll carry out the operations necessary to create, update, manage, and delete the AWS EKS cluster. This command line interface for communicating with the AWS EKS Cluster is highly useful.1.Here, we're using the curl command to download the most recent version of the eksctl package into a tar file, which we then untar under the tmp directory with the following command.[root ~]$ curl --silent --location \"https://github.com/weaveworks/eksctl/releases/latest/download/eksctl_$(uname -s)_amd64.tar.gz\" | tar xz -C /tmpEnter fullscreen modeExit fullscreen mode2.Installing and untar the\u00a0eksctl Data from the untar eksctl package is being transferred to the /usr/local/bin directory.[root ~]$ sudo mv /tmp/eksctl /usr/local/binEnter fullscreen modeExit fullscreen mode3.Moving from tmp directory to bin directory After the eksctl CLI has been installed successfully, we use the version command to determine whether kubectl has been installed correctly or not.[root ~]$ eksctl versionEnter fullscreen modeExit fullscreen modeCongratulations! Kubectl, eksctl, and the AWS CLI you all have been installed successfully. We have witnessed the step-by-step installation process for all three CLIs. This CLI will undoubtedly be useful while using the AWS EKS cluster or learning about the AWS EKS. This personally helped me a lot since once you learn how to use the CLI, you won't need a console user interface to see things; just a few commands will give you access to all the information in your Linux terminal. If you have any questions, please post a comment below.\u2014 \u2014 \u2014 \u2014 \u2014 \u2014 \u2014 \u2014Here is the End!Thank you for taking the time to read my article. I hope you found this article informative and helpful. As I continue to explore the latest developments in technology, I look forward to sharing my insights with you. Stay tuned for more articles like this one that break down complex concepts and make them easier to understand.Remember, learning is a lifelong journey, and it\u2019s important to keep up with the latest trends and developments to stay ahead of the curve. Thank you again for reading, and I hope to see you in the next article!Happy Learning!"}
{"title": "Your containerized application with IAC on AWS \u2014 Pt.3", "published_at": 1716470664, "tags": [], "user": "Paloma Lataliza", "url": "https://dev.to/aws-builders/your-containerized-application-with-iac-on-aws-pt3-56ec", "details": "Hi Folks! This will be the final post in our series on infrastructure and containers. We will utilize Terragrunt and our infrastructure in this section, and at the conclusion, we will have our application operating on Fargate on AWS.The docker image I\u2019ll be using in this lesson comes from Sonic, an old game that many people associate with their early years.You may use this image or find it on my dockerhub, whichever you would like.DIRECTORIESAgain, I\u2019ll leave our directory structure here so you can guide yourself:app modules     \u251c\u2500\u2500 amazon_vpc     \u251c\u2500\u2500 aws_loadbalancer     \u251c\u2500\u2500 aws_fargate     \u251c\u2500\u2500 aws_roles     \u251c\u2500\u2500 aws_ecs_cluster     \u2514\u2500\u2500 aws_targetgroup     \u2514\u2500\u2500 aws_certificate_manager  terragrunt     \u2514\u2500\u2500 dev         \u2514\u2500\u2500 us-east-1             \u251c\u2500\u2500 aws_ecs             \u2502   \u251c\u2500\u2500 cluster             \u2502   \u2514\u2500\u2500 service             \u251c\u2500\u2500 aws_loadbalancer             \u251c\u2500\u2500 amazon_vpc             \u251c\u2500\u2500 aws_targetgroup             \u251c\u2500\u2500 aws_roles             \u251c\u2500\u2500 aws_certificate_manager             \u2514\u2500\u2500 terragrunt.hclEnter fullscreen modeExit fullscreen modeTERRAGRUNTFirst, let\u2019s look at our terragrunt.hcl, located in us-east-1. It will be used for all common variables in our code, as well as for creating our backend settings and the lock in the dynamodb database.Typical variables are going to be region, project_name, domain_name, env, host_headers and container_port.terragrunt.hclremote_state {   backend = \"s3\"   generate = {     path      = \"backend.tf\"     if_exists = \"overwrite\"   }   config = {     bucket           = \"sonic-iac-series\"     key              = \"dev/${path_relative_to_include()}/terraform.tfstate\"     region           = \"us-east-1\"     encrypt          = true     dynamodb_table   = \"terraform-state-lock\"   } }  inputs = {    region            = \"us-east-1\"    project_name      = \"sonic-iac\"    env               = \"dev\"    domain_name       = \"your domain\"    host_headers      = \"sonic.your domain\"    container_port    = \"8080\"    tags = {      ambiente        = \"dev\"      projeto         = \"sonic-iac\"      plataforma      = \"aws\"      gerenciado      = \"terraform/terragrunt\"    } }  generate \"provider\" {     path      = \"provider.tf\"     if_exists = \"overwrite\"     contents = <<EOF provider \"aws\" {   profile   = \"default\"   region    = \"us-east-1\" } EOF }Enter fullscreen modeExit fullscreen modeVPCThe first resource to be created will be the VPC, as it will be needed for most of our resources.terragrunt     \u2514\u2500\u2500 dev         \u2514\u2500\u2500 us-east-1              \u2514\u2500\u2500 amazon_vpc                  \u2514\u2500\u2500 terragrunt.hclEnter fullscreen modeExit fullscreen modeWe will use a range of /25, starting with IP 172.35.0.221, to construct our VPC. Four subnets \u2014 two public and two private \u2014 will be created inside it.VPC: 172.35.0.128/25Public Subnet 1: 172.35.0.128/27Public Subnet 2: 172.35.0.160/27Private Subnet 1: 172.35.0.192/27Private Subnet 2: 172.35.0.224/27 These code files will be created within:terragrunt.hclinclude {   path = find_in_parent_folders() }  inputs = {     vpc_cidr_block              = \"172.35.0.128/25\"     public_subnet1_cidr_block   = \"172.35.0.128/27\"     public_subnet2_cidr_block   = \"172.35.0.160/27\"     private_subnet1_cidr_block  = \"172.35.0.192/27\"     private_subnet2_cidr_block  = \"172.35.0.224/27\"     availability_zone1 = \"us-east-1a\"     availability_zone2 = \"us-east-1b\" } terraform {   source = \"../../../../modules/amazon_vpc\"   extra_arguments \"custom_vars\" {     commands = [         \"apply\",         \"plan\",         \"import\",         \"push\",         \"refresh\"     ]   } }Enter fullscreen modeExit fullscreen modeIAM PERMISSIONSThe next thing to be created will be permissions for our resources.terragrunt     \u2514\u2500\u2500 dev         \u2514\u2500\u2500 us-east-1              \u2514\u2500\u2500 aws_roles                  \u2514\u2500\u2500 terragrunt.hclEnter fullscreen modeExit fullscreen modeterragrunt.hclinclude {   path = find_in_parent_folders() }  terraform {   source = \"../../../../modules/aws_roles\"   extra_arguments \"custom_vars\" {     commands = [         \"apply\",         \"plan\",         \"import\",         \"push\",         \"refresh\"     ]   } }Enter fullscreen modeExit fullscreen modeAWS CERTIFICATE MANAGERThese are the configurations for applying our certificate; we will generate the certificate and use our domain to validate it.terragrunt     \u2514\u2500\u2500 dev         \u2514\u2500\u2500 us-east-1              \u2514\u2500\u2500 aws_certificate_manager                  \u2514\u2500\u2500 terragrunt.hclEnter fullscreen modeExit fullscreen modeterragrunt.hclinclude {   path = find_in_parent_folders() }  terraform {   source = \"../../../../modules/aws_certificate_manager\"   extra_arguments \"custom_vars\" {     commands = [         \"apply\",         \"plan\",         \"import\",         \"push\",         \"refresh\"     ]   } }Enter fullscreen modeExit fullscreen modeAWS LOAD BALANCERLet\u2019s set up our loadbalancer using Terragrunt now. This will help distribute our traffic and guarantee that our application is highly available.terragrunt     \u2514\u2500\u2500 dev         \u2514\u2500\u2500 us-east-1              \u2514\u2500\u2500 aws_loadbalancer                  \u2514\u2500\u2500 terragrunt.hclEnter fullscreen modeExit fullscreen modeOur Terragrunt setup looks like this. It\u2019s important to note that in order to increase everything\u2019s dynamic nature, we use dependencies between modules.terragrunt.hclinclude {   path = find_in_parent_folders() }  dependency \"vpc\" {   config_path = \"../amazon_vpc\" }  dependency \"acm\" {   config_path = \"../aws_certificate_manager\" }  inputs = {   vpc_id       = dependency.vpc.outputs.vpc_id   subnet_id_1  = dependency.vpc.outputs.public_subnet1_id   subnet_id_2  = dependency.vpc.outputs.public_subnet2_id   alb_internal = false   certificate_arn = dependency.acm.outputs.acm_arn   priority_listener_rule  = \"1\" } terraform {   source = \"../../../../modules/aws_loadbalancer\"   extra_arguments \"custom_vars\" {     commands = [       \"apply\",       \"plan\",       \"import\",       \"push\",       \"refresh\"     ]   } }Enter fullscreen modeExit fullscreen modeAWS TARGET GROUPHere we will configure our Target Group with Terragrunt, it is super essential for directing traffic to the correct servers for our application.terragrunt     \u2514\u2500\u2500 dev         \u2514\u2500\u2500 us-east-1              \u2514\u2500\u2500 aws_targetgroup                  \u2514\u2500\u2500 terragrunt.hclEnter fullscreen modeExit fullscreen modeterragrunt.hclinclude {   path = find_in_parent_folders() } dependency \"loadbalancer\" {   config_path = \"../aws_loadbalancer\" }     dependency \"vpc\" {   config_path = \"../amazon_vpc\" }  dependency \"acm\" {   config_path = \"../aws_certificate_manager\" }  inputs = {   vpc_id                  = dependency.vpc.outputs.vpc_id   subnet_id_1             = dependency.vpc.outputs.public_subnet1_id   subnet_id_2             = dependency.vpc.outputs.public_subnet2_id   certificate_arn         = dependency.acm.outputs.acm_arn   listener_ssl_arn        = dependency.loadbalancer.outputs.listener_ssl_arn   priority_listener_rule  = \"2\"   health_check_path       = \"/\" }  terraform {   source = \"../../../../modules/aws_targetgroup\"   extra_arguments \"custom_vars\" {     commands = [       \"apply\",       \"plan\",       \"import\",       \"push\",       \"refresh\"     ]   } }Enter fullscreen modeExit fullscreen modeECS CLUSTERIn this step we will create our ECS cluster that will host our application.terragrunt     \u2514\u2500\u2500 dev         \u2514\u2500\u2500 us-east-1              \u2514\u2500\u2500 aws_ecs                  \u2514\u2500\u2500 cluster                        \u2514\u2500\u2500 terragrunt.hclEnter fullscreen modeExit fullscreen modeterragrunt.hclinclude {   path = find_in_parent_folders() }  terraform {   source = \"../../../../../modules/aws_ecs_cluster\"   extra_arguments \"custom_vars\" {     commands = [         \"apply\",         \"plan\",         \"import\",         \"push\",         \"refresh\"     ]   } }Enter fullscreen modeExit fullscreen modeFARGATE AND ECRWe will construct our fargate service, the repository in the ECR, and a record on our domain as the final configuration file.terragrunt     \u2514\u2500\u2500 dev         \u2514\u2500\u2500 us-east-1              \u2514\u2500\u2500 aws_ecs                  \u2514\u2500\u2500 service                        \u2514\u2500\u2500 terragrunt.hclEnter fullscreen modeExit fullscreen modeterragrunt.hclinclude {   path = find_in_parent_folders() } dependency \"loadbalancer\" {   config_path = \"../../aws_loadbalancer\" }     dependency \"vpc\" {   config_path = \"../../amazon_vpc\" } dependency \"role\" {   config_path = \"../../aws_roles\" }  dependency \"targetgroup\" {   config_path = \"../../aws_targetgroup\" }  dependency \"cluster\" {   config_path = \"../cluster\" }  inputs = {   vpc_id                = dependency.vpc.outputs.vpc_id   subnet_id_1           = dependency.vpc.outputs.private_subnet1_id   subnet_id_2           = dependency.vpc.outputs.private_subnet2_id   alb_dns_name          = dependency.loadbalancer.outputs.alb_dns_name   sg_alb                = dependency.loadbalancer.outputs.alb_secgrp_id   target_group_arn      = dependency.targetgroup.outputs.tg_alb_arn   cluster_arn           = dependency.cluster.outputs.cluster_arn   ecs_role_arn          = dependency.role.outputs.ecs_role_arn   instance_count        = \"1\"   container_vcpu        = \"512\"   container_memory      = \"1024\"   aws_account_id        = \"your account number\" }  terraform {   source = \"../../../../../modules/aws_fargate\"   extra_arguments \"custom_vars\" {     commands = [         \"apply\",         \"plan\",         \"import\",         \"push\",         \"refresh\"     ]   } }Enter fullscreen modeExit fullscreen modeAPPLYAfter the entire structure has been created, you must apply terragrunt to all directories that contain terragrunt.hcl in the following order.terragrunt/dev/us-east-1terragrunt/dev/us-east-1/amazon_vpcterragrunt/dev/us-east-1/aws_rolesterragrunt/dev/us-east-1/aws_certificate_managerterragrunt/dev/us-east-1/aws_loadbalancerterragrunt/dev/us-east-1/aws_targetgroupterragrunt/dev/us-east-1/aws_ecs/clusterterragrunt/dev/us-east-1/aws_ecs/fargateUse this command on terminal to apply. You need use in each directoryterragrunt applyor in the root folder use:terragrunt run-all applyECRNow we have applied all our infrastructure and our ECR repository has been created, we must upload our image for use in our container.The image must be downloaded from Docker Hub as an initial step. You can use another image if you prefer or your own from your application.use this command to download my sonic image:docker pull shescloud/sonic-the-hedgehogTESTINGI used a domain I had and our application was temporarily hosted at sonic.shescloud.tech.DESTROYIf you are using it for study, or as a way to complete a test, don\u2019t forget to destroy all resources at the end to avoid unnecessary costs. To delete everything, we will do a process similar to apply, but in the opposite way.Before deleting everything via terragrunt, you need to access your AWS account, go to the ECR service and delete the image from the repository. After completing this step, you can proceed with destroying each of the repositories.Now, you must destroy to all directories that contain terragrunt.hcl in the following order.terragrunt/dev/us-east-1/aws_ecs/fargateterragrunt/dev/us-east-1/aws_ecs/clusterterragrunt/dev/us-east-1/aws_targetgroupterragrunt/dev/us-east-1/aws_loadbalancerterragrunt/dev/us-east-1/aws_rolesterragrunt/dev/us-east-1/aws_certificate_managerterragrunt/dev/us-east-1/amazon_vpcterragrunt/dev/us-east-1Use this command on terminal to destroy. You need use in each directoryterragrunt destroyor in the root folder use:`terragrunt run-all destroy`GITHUBYou can check the repository with the code on my github:https://github.com/shescloud/terraform-terragrunt-fargateAnd that\u2019s it folks! I hope you enjoyed it and get a lot out of this code. See u soon!"}
{"title": "Red Hat Enterprise Linux on AWS Cloud", "published_at": 1716463583, "tags": ["aws", "rhel", "security", "beginners"], "user": "Jakub Wo\u0142ynko", "url": "https://dev.to/aws-builders/red-hat-enterprise-linux-on-aws-cloud-56gb", "details": "WelcomeLet\u2019s talk about basic IT operations included in the everyday tasks range. For example, accessing VMs. As you may realize (or not) - not everyone is using immutable infrastructure. Especially when their core business isn\u2019t IT, and they are a bit bigger than 2-pizza team. That is why today we will talk about accessing the console of Red Hat Enterprise Linux 9.3 in AWS Cloud. I will show you the 3 most useful methods - in my opinion; there are no statistics.Initial noteDue to the fact, that I appreciate AWS CDK, all examples today will be written in TypeScript.Additionally, I decided to use shared stacks and store VPC config separately,as well as default instance properties.The network is very simple, one public subnet,one private subnet, and one NAT(default option when usingsubnetType: ec2.SubnetType.PRIVATE_WITH_EGRESS).import*ascdkfrom'aws-cdk-lib';import*asec2from'aws-cdk-lib/aws-ec2';exportclassSharedNetworkStackextendscdk.Stack{publicreadonlyvpc:ec2.Vpc;constructor(scope:cdk.App,id:string,props?:cdk.StackProps){super(scope,id,props);cdk.Tags.of(this).add(\"description\",\"Shared Network\");cdk.Tags.of(this).add(\"organization\",\"3sky.dev\");cdk.Tags.of(this).add(\"owner\",\"3sky\");this.vpc=newec2.Vpc(this,'TheVPC',{ipAddresses:ec2.IpAddresses.cidr(\"10.192.0.0/20\"),maxAzs:1,enableDnsHostnames:true,enableDnsSupport:true,restrictDefaultSecurityGroup:true,subnetConfiguration:[{cidrMask:28,name:\"public\",subnetType:ec2.SubnetType.PUBLIC,},{cidrMask:28,name:\"private\",subnetType:ec2.SubnetType.PRIVATE_WITH_EGRESS,},],});}}Enter fullscreen modeExit fullscreen modeAs a base AMI for all instances, I will be using the publicly available latest RH build:amazon/RHEL-9.3.0_HVM-20240117-x86_64-49-Hourly2-GP3. With the smallest possible instance size. Everything is sorted in file calledbin/rhel.ts#!/usr/bin/env nodeimport'source-map-support/register';import*ascdkfrom'aws-cdk-lib';import*asec2from'aws-cdk-lib/aws-ec2';import{SSHStack}from'../lib/ssh-stack';import{ICStack}from'../lib/ic-stack';import{SSMStack}from'../lib/ssm-stack';import{SharedNetworkStack}from'../lib/shared-network';constapp=newcdk.App();constnetwork=newSharedNetworkStack(app,'SharedNetworkStack');constdefaultInstanceProps={vpc:network.vpc,machineImage:ec2.MachineImage.genericLinux({// amazon/RHEL-9.3.0_HVM-20240117-x86_64-49-Hourly2-GP3\"eu-central-1\":\"ami-0134dde2b68fe1b07\",}),instanceType:ec2.InstanceType.of(ec2.InstanceClass.BURSTABLE2,ec2.InstanceSize.MICRO,),};newSSHStack(app,'SSHStack',{instanceProps:defaultInstanceProps,vpc:network.vpc,});newICStack(app,'ICStack',{instanceProps:defaultInstanceProps,vpc:network.vpc,});newSSMStack(app,'SSMStack',{instanceProps:defaultInstanceProps,});Enter fullscreen modeExit fullscreen modeSSHLet\u2019s start with the basics. Regular SSH, what do we need to make this possible?SSH Key Pairssh-server and ssh-client installedconnection to configured port(default: 22)Bastion Host, as we\u2019re simulating enterprisesetupThe initial assumption is, that we already have a key pair if not, please generate it with the following command:ssh-keygen\\-ted25519\\-C\"aws@local-testing\"\\-f~/.ssh/id_ed25519_local_testingEnter fullscreen modeExit fullscreen modeLet\u2019s back to code, we\u2019re using a much too open Security Group for the Bastion host, regular in-VPC SG, and two hosts with the same ssh-key configured. That is why the created stack definition is rather simple:import*ascdkfrom'aws-cdk-lib';import{Construct}from'constructs';import*asec2from'aws-cdk-lib/aws-ec2';exportinterfaceSSHStackPropsextendscdk.StackProps{vpc:ec2.Vpc;instanceProps:any;}exportclassSSHStackextendscdk.Stack{constructor(scope:Construct,id:string,props:SSHStackProps){super(scope,id,props);consttheVPC=props.vpc;consttheProps=props.instanceProps;// WARNING: change key material to your ownconstawsKeyPair=newec2.CfnKeyPair(this,\"localkeypair\",{publicKeyMaterial:\"ssh-ed25519 AAAAC3NzaC1lZDI1NTE5AAAAIJxYZEBNRLXmuign6ZgNbmaSK7cnQAgFpx8cCscoqVed local\",keyName:\"localawesomekey\",});constmyKeyPair=ec2.KeyPair.fromKeyPairAttributes(this,\"mykey\",{keyPairName:awsKeyPair.keyName,});consttooopenSG=newec2.SecurityGroup(this,\"tooopenSG\",{securityGroupName:\"Allow all SSH traffic\",vpc:theVPC,allowAllOutbound:false});tooopenSG.addIngressRule(// NOTE: we should use a more specific network range with// ec2.Peer.ipv4(\"x.x.x.x/24\")ec2.Peer.anyIpv4(),ec2.Port.tcp(22),\"Allow SSH\",false,);constdefaultSG=newec2.SecurityGroup(this,\"regularSG\",{securityGroupName:\"Regular in-VPC SG\",vpc:theVPC,allowAllOutbound:false});defaultSG.addIngressRule(ec2.Peer.ipv4(theVPC.vpcCidrBlock),ec2.Port.tcp(22),\"Allow SSH inside VPC only\");constbastion=newec2.Instance(this,'bastion-host',{instanceName:'bastion-host',vpcSubnets:{subnetType:ec2.SubnetType.PUBLIC,},securityGroup:tooopenSG,keyPair:myKeyPair,...theProps,});constinstance=newec2.Instance(this,'host',{instanceName:'host',vpcSubnets:{subnetType:ec2.SubnetType.PRIVATE_WITH_EGRESS,},securityGroup:defaultSG,keyPair:myKeyPair,...theProps,});newcdk.CfnOutput(this,\"bastionIP\",{value:bastion.instancePublicIp,description:\"Public IP address of the bastion host\",});newcdk.CfnOutput(this,\"instnaceIP\",{value:instance.instancePrivateIp,description:\"Private IP address of thsh host\",});}}Enter fullscreen modeExit fullscreen modeThen after executingcdk deploy SSHStack, and waiting around 200s, we should be able to see:\u2705  SSHStack  \u2728  Deploymenttime: 200.17s  Outputs: SSHStack.bastionIP=3.121.228.159 SSHStack.instnaceIP=10.192.0.24Enter fullscreen modeExit fullscreen modeGreat, now we can use our ssh key andec2-user, for connection with our instance.$ssh ec2-user@3.121.228.159-i~/.ssh/id_ed25519_local  Register this system with Red Hat Insights: insights-client--registerCreate an account or view all your systems at https://red.ht/insights-dashboard[ec2-user@ip-10-192-0-6 ~]$clear[ec2-user@ip-10-192-0-6 ~]$logoutConnection to 3.121.228.159 closed.Enter fullscreen modeExit fullscreen modeOk, for accessing our regular target host I strongly recommend using a regular~/.ssh/configfile, with the following content:Host aws-bastion   PreferredAuthentications publickeyIdentitiesOnly=yesIdentityFile ~/.ssh/id_ed25519_local   User ec2-user   Hostname 3.76.116.53  Host aws-host   PreferredAuthentications publickeyIdentitiesOnly=yesProxyJump jump   IdentityFile ~/.ssh/id_ed25519_local   User ec2-user   Hostname 10.192.0.24Enter fullscreen modeExit fullscreen modeThe good thing about it is that it\u2019s very easy to configure Ansible with it. Our inventory file will be just simple:[bastion] aws-bastion[instance] aws-host[aws:children] aws-bastion aws-hostEnter fullscreen modeExit fullscreen modeHowever, in the case of a real-world system, I would recommend using Ansible dynamic inventory, based on proper tagging.propseasy-to-implement solutionstandard SSH, so we can use Ansible just after deploymentno additional costs(besides Bastion host)consexposing VMs to the public internet isn\u2019t the most secure solutionwe need to manage SSH key pairwe need to manage userswe need to manage accesses manually, from the OS levelno dedicated logging solution, besides SyslogSSM Session ManagerSetting SSM based ondocumentationcould be a bit more challenging, as we need to:install SSM agentconfigure role and instance profilesetupimport*ascdkfrom'aws-cdk-lib';import{Construct}from'constructs';import*asec2from'aws-cdk-lib/aws-ec2';import*asiamfrom'aws-cdk-lib/aws-iam'exportinterfaceSSMStackPropsextendscdk.StackProps{instanceProps:any;}exportclassSSMStackextendscdk.Stack{constructor(scope:Construct,id:string,props:SSMStackProps){super(scope,id,props);consttheProps=props.instanceProps;constssmRole=newiam.Role(this,\"SSMRole\",{assumedBy:newiam.ServicePrincipal(\"ec2.amazonaws.com\"),managedPolicies:[iam.ManagedPolicy.fromAwsManagedPolicyName(\"AmazonSSMManagedInstanceCore\")],roleName:\"SSMRole\"});newiam.InstanceProfile(this,\"SSMInstanceProfile\",{role:ssmRole,instanceProfileName:\"SSMInstanceProfile\"});constuserData=ec2.UserData.forLinux();userData.addCommands('set -o xtrace','sudo dnf install -y https://s3.amazonaws.com/ec2-downloads-windows/SSMAgent/latest/linux_amd64/amazon-ssm-agent.rpm','sudo systemctl enable amazon-ssm-agent','sudo systemctl start amazon-ssm-agent');constinstnace=newec2.Instance(this,'instance-with-ssm',{instanceName:'instance-with-ssm',vpcSubnets:{subnetType:ec2.SubnetType.PRIVATE_WITH_EGRESS,},role:ssmRole,allowAllOutbound:true,detailedMonitoring:true,userData:userData,...theProps,});newcdk.CfnOutput(this,\"HostID\",{value:instnace.instanceId,description:\"ID of the regular host\",});newcdk.CfnOutput(this,\"hostDNS\",{value:instnace.instancePrivateDnsName,description:\"Hostname of the regular host\",});}}Enter fullscreen modeExit fullscreen modeAs you can see, we need to specify the role and instance profile(which can\u2019t be displayed in GUI), and then we place the instance in a private subnet with specific user data and role. After a while we should be able to connect via  CLI:$aws ssm start-session--targeti-0110d03f4713d475c   Starting session with SessionId: kuba@3sky.dev-6n5goh43iisz45cmvpht54ize4 sh-5.1$sudosu[root@ip-10-192-0-26 bin]#Enter fullscreen modeExit fullscreen modeOr via GUI:bonus: cockpitAs we\u2019re using the Red Hat system(however also available on Ubuntu etc), and SSM supports port forwarding we can utilize the power of the cockpit. For example, manage subscriptions, check security recommendations, and connect withRed Hat Insights. How to do it? Login to our instance, create an admin user with a password, and then start the port forwarding session.# login to host$aws ssm start-session--targeti-0113d03f4713d412b# become a root and create the needed usersh-5.1$sudosu[root@ip-10-192-0-24:~]$sudouseradd kuba[root@ip-10-192-0-24:~]$sudopasswd kuba[root@ip-10-192-0-24:~]$sudousermod-aGwheel kuba[root@ip-10-192-0-24:~]$exitsh-5.1$exit## start port-forwarding from the workstation$aws ssm start-session\\--targeti-0113d03f4713d412b--document-nameAWS-StartPortForwardingSessionToRemoteHost\\--parameters'{\"portNumber\":[\"9090\"],\"localPortNumber\":[\"9090\"],\"host\":[\"ip-10-192-0-24\"]}'Enter fullscreen modeExit fullscreen modepropsstraightforward setupallow the user to access the instance from GUI and CLIdoes not require using or storing ssh-keyshas built in monitoring with CloudWatchprovides the ability to restrict access based on IAMsupport port forwarding (useful for DB access)consusing Ansible will be challenging(however supported)requires more AWS-specific knowledge than Bastion hostin case of failure, push us to annoying debuggingEC2 instance connectOur EC2 Instance Connect setup and configuration will be based onofficial documentation.Here are the main prerequisites we have:installed ec2-instance-connect packages on our host, which are not included in the default Red Hat buildEC2 instance connect endpoint placed in the corresponding networkopen network connection to instance security group on TCP/22open network connection from EC2 instance to connect security group to instances on TCP/22setupBased on these prerequisites content of our file is as:import*ascdkfrom'aws-cdk-lib';import{Construct}from'constructs';import*asec2from'aws-cdk-lib/aws-ec2';exportinterfaceICStackPropsextendscdk.StackProps{vpc:ec2.Vpc;instanceProps:any;}exportclassICStackextendscdk.Stack{constructor(scope:Construct,id:string,props:ICStackProps){super(scope,id,props);consttheVPC=props.vpc;consttheProps=props.instanceProps;consticeSG=newec2.SecurityGroup(this,\"iceSG\",{securityGroupName:\"Instance Connect SG\",vpc:theVPC,allowAllOutbound:false});iceSG.addEgressRule(ec2.Peer.ipv4(theVPC.vpcCidrBlock),ec2.Port.tcp(22),\"Allow outbound traffic from SG\",);// WARNING: We need outbound for package installationconsticeSGtoVM=newec2.SecurityGroup(this,\"iceSGtoVM\",{securityGroupName:\"Allow access over instance connect\",vpc:theVPC,});iceSGtoVM.addIngressRule(iceSG,ec2.Port.tcp(22),\"Allow SSH traffic from iceSG\",);newec2.CfnInstanceConnectEndpoint(this,\"myInstanceConnectEndpoint\",{securityGroupIds:[iceSG.securityGroupId],subnetId:theVPC.privateSubnets[0].subnetId});constuserData=ec2.UserData.forLinux();userData.addCommands('set -o xtrace','mkdir /tmp/ec2-instance-connect','curl https://amazon-ec2-instance-connect-us-west-2.s3.us-west-2.amazonaws.com/latest/linux_amd64/ec2-instance-connect.rpm -o /tmp/ec2-instance-connect/ec2-instance-connect.rpm','curl https://amazon-ec2-instance-connect-us-west-2.s3.us-west-2.amazonaws.com/latest/linux_amd64/ec2-instance-connect-selinux.noarch.rpm -o /tmp/ec2-instance-connect/ec2-instance-connect-selinux.rpm','sudo yum install -y /tmp/ec2-instance-connect/ec2-instance-connect.rpm /tmp/ec2-instance-connect/ec2-instance-connect-selinux.rpm');constinstnace=newec2.Instance(this,'instance-with-ic',{instanceName:'instance-with-ic',vpcSubnets:{subnetType:ec2.SubnetType.PRIVATE_WITH_EGRESS,},securityGroup:iceSGtoVM,allowAllOutbound:true,detailedMonitoring:true,userData:userData,...theProps,});newcdk.CfnOutput(this,\"HostIP\",{value:instnace.instanceId,description:\"Public IP address of the regular host\",});}}Enter fullscreen modeExit fullscreen modeAs you can see, the longest part is just user-data for downloading and installing needed packages. What is important in my opinion, we always should test it before deploying it on production. In case of errors with the installation process, debugging will be hard and will require adding a bastion host with keys, and instance recreation.After deploying a stack(a bit longer this time), we should be able to access our instance with CLI:$aws ec2-instance-connect ssh--instance-idi-08385338c2614df28  The authenticity of host'10.192.0.26 (<no hostip for proxy command>)'can't be established. ED25519 key fingerprint is SHA256:BAxtwbZYKsK6hTJbvqOGgulGYftNQHZHMSpBkIGRTeY. This key is not known by any other names. Are you sure you want to continue connecting (yes/no/[fingerprint])? yes Warning: Permanently added '10.192.0.26' (ED25519) to the list of known hosts. Register this system with Red Hat Insights: insights-client --register Create an account or view all your systems at https://red.ht/insights-dashboard [ec2-user@ip-10-192-0-26 ~]$Enter fullscreen modeExit fullscreen modeor via GUI:propsstraightforward setupallow the user to access the instance from GUI and CLIdoes not require using or storing ssh-keyshas built in monitoring with CloudWatchprovides the ability to restrict access based on IAMconsusing Ansible will be very challenging(no official support or plugin)requires more AWS-specific knowledge than Bastion hostin case of failure, push us to annoying debuggingFinal notesnetwork setup was simplified; one AZ; 2 subnetssecurity groups were to open, especially outbound rules(due to package downloading)SSM could be used over endpoints as well:com.amazonaws.:aws-region:.ssmcom.amazonaws.:aws-region:.ssmmessagescom.amazonaws.:aws-region:.ssmmessageswe should use already pre-built AMI to avoid this issuerepo as always can be findhereSummaryAs you can see setting up RHEL in AWS isn\u2019t such hard, for sure it\u2019s more expensive and requires ec2-connect-instance or SSM agent installation(Amazon Linux does not), but if we\u2019re going to use RHEL in the cloud, probably we have good reason to do so. For example, great support, Insights, or Cloud Console, but my job was to show possibilities and do it with style."}
{"title": "Update multiple Kubernetes objects/configmaps in one go!", "published_at": 1716382465, "tags": ["kubernetes", "linux", "devops", "beginners"], "user": "Sunny Bhambhani", "url": "https://dev.to/aws-builders/update-multiple-kubernetes-objectsconfigmaps-in-one-go-5417", "details": "There may be cases wherein we just need to update a Kubernetes configmap or any other Kubernetes object based on our requirements.And let's say it's just one or two configmaps(here we will be talking about configmaps just for simplicity but its applicable for other Kubernetes objects as well) then that's fine, right? We can simply do the following:kubectl edit -n NAMESPACE_NAME configmap CONFIGMAP_NAMEUpdate it based on our requirements.Save it.But think about a scenario, wherein we need to update +100 configmaps with the same values how to do it?We cannot update or edit each and every object since that would be a bit tedious and will be error-prone as well. Plus there might be chances we might skip some of them.So, the best way to do it is by usingpatchcommand in an automated way.Refer:https://kubernetes.io/docs/reference/kubectl/generated/kubectl_patch/for more details about it.Let's see this in action:I have a very minimal configmap which contains some DATABASE details.apiVersion: v1 data:   DATABASE_HOST: \"api01.ybdb.io\"   DATABASE_NAME: \"api01\"   DATABASE_PORT: \"5433\" kind: ConfigMap metadata:   name: api01-configmap   namespace: defaultEnter fullscreen modeExit fullscreen modeNow, let's assume there is an update instead of port number 5433. Our database now listens on port 5444.We can easily accomplish this usingkubectl editbut let's seepatchin action:$ kubectl patch configmap api01-configmap -n default -p '{\"data\":{\"DATABASE_PORT\":\"5444\"}}' configmap/api01-configmap patched $ kubectl get cm -n default -o yaml api01-configmap apiVersion: v1 data:   DATABASE_HOST: api01.ybdb.io   DATABASE_NAME: api01   DATABASE_PORT: \"5444\" kind: ConfigMap metadata:   creationTimestamp: \"2024-05-22T10:06:39Z\"   name: api01-configmap   namespace: default   resourceVersion: \"67651\"   uid: 2d49a0ea-6910-420c-b2bf-00e42e6c08d7Enter fullscreen modeExit fullscreen modeThe command is pretty straight forwardkubectl patch configmap api01-configmap -n default -p '{\"data\":{\"DATABASE_PORT\":\"5444\"}}'-n is to specify the namespace name where the configmap resides.-p is to specify what patch/update we want in the Kubernetes object.Here, underdata:we are updating a key calledDATABASE_PORT:to5444.Note: If in caseDATABASE_PORT:key doesn't exist, it will create that key for us.Assume you want to see the output as well in the same command then add -o or --output flag with the type [json,name,yaml,etc].$ kubectl patch configmap api01-configmap -n default -p '{\"data\":{\"DATABASE_PORT\":\"5444\"}}' -o yaml apiVersion: v1 data:   DATABASE_HOST: api01.ybdb.io   DATABASE_NAME: api01   DATABASE_PORT: \"5444\" kind: ConfigMap metadata:   creationTimestamp: \"2024-05-22T10:47:41Z\"   name: api01-configmap   namespace: default   resourceVersion: \"69416\"   uid: a303bd0e-7f61-49c0-b671-06649ecdf999Enter fullscreen modeExit fullscreen modeBut there is another catch, you can also not run the patch command +100 times, I mean definitely you can but again that might be error-prone too.So, in that case we can write a simple reusable bash script that can do the job for us and can help us to update +100 configmaps in one go.It can be in any language but I have chosenbash, since it's pretty common.Quick rundown of the script:Here I have three variables, NAMESPACE (where the object resides), CONFIGMAP_FILE (input file which contains a list of all the configmaps that need to be updated), DATA (a set of key:value pair which needs to be replaced/added).Here I am creating log files as well (update_cm.log - contains generic readable information, patch.log - contains output of patch command).As of now, I just have added action as add but we can have other actions as well like replace or remove (based on our requirements).#!/bin/bash  NAMESPACE=\"default\" CONFIGMAP_FILE=\"configmaps\" DATA='{\"data\":{\"DATABASE_PORT\":\"5444\"}}'  rm patch.log update_cm.log  if [ ! -f \"$CONFIGMAP_FILE\" ]; then   echo \"File $CONFIGMAP_FILE not found.\"   exit 1 fi  echo `date` >> update_cm.log echo \"----------------------------\" >> update_cm.log  action=$1  if [ \"$action\" == \"add\" ]; then   echo \"Action: $action\"   echo \"Action: $action\" >> update_cm.log   while IFS= read -r cm; do     echo \"Updating ConfigMap: $cm\"     echo \"Updating ConfigMap: $cm\" >> update_cm.log     kubectl patch configmap \"$cm\" -n \"$NAMESPACE\" -p $DATA >> patch.log 2>&1   done < \"$CONFIGMAP_FILE\"  else   echo \"Invalid action, please specify add\".   exit 1 fi  echo \"----------------------------\" >> update_cm.logEnter fullscreen modeExit fullscreen modeFeel free to use it, tweak it, based on your requirements. This script is for configmaps but similar logic we can write for other objects as well if required.Happy learning :)References:Google.Kubernetes docs (https://kubernetes.io/docs/home/).kubectl explainDisclaimer:We should not manually update/patch the Kubernetes objects, this should be done when absolutely necessary or needed on the fly.Ideally these all values should come from the version control system wherein the values files are maintained."}
{"title": "Integrate Kafka with a Serverless application", "published_at": 1716351993, "tags": ["serverless", "kafka", "eventdriven", "aws"], "user": "Daniele Frasca", "url": "https://dev.to/aws-builders/integrate-kafka-with-a-serverless-application-2c9n", "details": "We are all working with the microservices pattern, or at least in a context with multiple teams where information exchange is necessary.IntroductionIn an event-driven-application setup, I usually see 3 main components to exchange messages:Apache Kafkais an open-source, high-performance, fault-tolerant, and scalable platform for building real-time streaming data pipelines and applications. Streaming data is continuously generated by thousands of data sources, which simultaneously send the data records in.Amazon EventBridgeis a serverless service that uses events to connect application components, making it easier to build scalable event-driven applications. Event-driven architecture builds loosely coupled software systems that work together by emitting and responding to events.Amazon Kinesisis an AWS version similar to Kafka, a high-performance, fault-tolerant, and scalable platform for building real-time streaming data pipelines and applications. Streaming data is continuously generated by thousands of data sources, which send the data records simultaneously.While Apache Kafka and Amazon Kinesis share many similarities in terms of functionality, they are not identical. Their choice often depends on factors such as use cases and infrastructure.In theory, streaming platforms like Kafka and Kinesis Data Streams and event routers like EventBridge serve different purposes and have different characteristics:Streaming Platforms are designed to handle high-volume real-time data streams. They provide capabilities for storing, processing, and analysing streams of records in real-time. They are often used in cases where large amounts of live data need to be processed and analysed, such as log monitoring, real-time analytics, and IoT data processing.Event Routers are designed to route events from various sources to different destinations. Event-driven architectures often use them to decouple microservices, orchestrate workflows, or integrate with third-party services.Both types of systems deal with events or messages, and the difference is subtle. I have often found a reality where those services are used for the same reasons.In the company where I work, we useConfluent Apache Kafkaas a common bus for all teams. Regardless of the architecture, whether it's Cluster or Serverless, they all subscribe to or publish to Kafka.The direct integration to Kafka requires code for:FilteringTransforming the messageMoving to some other downstream serviceHandling the errorThe problem with writing code is that you must maintain it, which is never the same over time. When the architecture grows, sometimes it gets wild, and there is no longer a standard.For example, Kafka has the concept of Consumer Groups. A Kafka Consumer Group is a feature that allows a pool of consumers (each with a unique ID) to divide up the processing of data over a collection of Kafka topics.In simple words, it means this:Kafka is configured with 3 consumer groups for a TopicI have 3 workers that subscribe to that TopicWhen a worker (consumer) fails to process a message and throws an error, Kafka's default behaviour is to retry delivering the message. If the worker continues to throw an error for the same message, this can lead to repeated retries, blocking the consumer group and creating a lag.The solution could be for the worker to catch the error and either handle it or ignore it. It can then manually commit the offset for the message, telling Kafka it has been processed. This will prevent Kafka from trying to redeliver the same message or trying to move it to a DLQ.Even if it is logically easy, I can guarantee that with time, there will be services that do it, some will swallow the error, and so on, creating a mess.I usually try to mitigate issues I can have while working with distributed systems, so I tend to migrate to or use the AWS Serverless offering.Integration typesI know I sound like a broken record, but I'll repeat it: the AWS Serverless offering is 2 things:AWS takes care of scalability and availability and patching of their servicesI integrate their managed services and focus on what truly matters: writing code and delivering business value.When you are working with Apache Kafka in AWS setup, there are 2 types of integrations:Amazon EventBridge open-source connector for Apache Kafka ConnectAmazon EventBridge PipesThisKafka sink connector for Amazon EventBridgeallows you to send events (records) from one or multiple Kafka topic(s) to the specified event bus, including useful features such as:offloading large events to S3configurable Topic to event detail-type name mapping custom IAM profiles per connectorIAM role-based authenticationsupport for dead-letter queuesschema registry support for Avro and Protocol Buffers (Protobuf).The connector is currently in versionv1.3.0and has great features compared to Pipes. These are offloading large events to S3 and automatic dead-letter queues, which is a different topic where you can send the failed message without writing code to handle the errors in the consumer logic.On the other hand, without access to the cluster or relying on a different team for this setup, the best choice to move forward and apply standards to my services is to use Amazon EventBridge Pipes.EventBridge Pipes is an incredible service that offers point-to-point connections between a source and a target. This feature can be very powerful, especially when we use EB as a target. Pipes handle all point-to-point connections efficiently, allowing us to transform the message and ensure that the downstream consumer can understand it.There are so many otherconcepts, but the most exciting part of this service is that it enables me to enrich and modify the message before passing it to the downstream consumer.If we compare the connector and pipe at a glance in the context of Kafka, the most significant differentiators are DLQ/S3 offloading (until available in Pipes) and Avro/Protobuf on the connector side vs. super simple setup/operations and enrichment with Pipes.Kafka messages can go up to 32MB, while serverless services like EventBridge/SQS/SNS can handle only a maximum of 256KB while Lambda is up to 6MBWhile it seems unrealistic to have such events, having events bigger than 256KB is not unrealistic.What is the problem with such messages?This is known as a poisoned message. The Amazon EventBridge open-source connector for Apache Kafka Connect can handle this case because it moves the message to a different Kafka Topic or is configured to offload large payloads to S3 and send the \u201ctrimmed-down\u201d event to EventBridge, while with Amazon EventBridge Pipes and other types like Lambda/Cluster, it will block the processing of your messages from the Kafka consumer group until the Pipes retries are exhausted. Because the message cannot be moved to a DLQ (SQS only supported), it will be lost forever without you knowing about it.A limited solution with EventBridge Pipes is to leverage the enrichment concept. With theClaim Check pattern, we can circumvent this limitation. However, this works only if the message in input is not over the 6MB Lambda payload limit.It is particularly useful when dealing with large data sets or messages. The pattern works by storing the large message in a data store and then passing a reference to the data to subsequent components. These components can then use the claim check to retrieve the data.Publishing to KafkaUntil now, I wrote only about how to configure Kafka to subscribe to the message, but I also need to publish, and we have 2 options:Writing code using the SDKKafka proxy viaAPI destinationsWriting code is simple once downloaded from the official SDK library. In this example, I will useKafkaJSconst producer = kafka.producer()  await producer.connect() await producer.send({     topic: 'topic-name',     messages: [         { key: 'key1', value: 'hello world' },         { key: 'key2', value: 'hey hey!' }     ], })Enter fullscreen modeExit fullscreen modeAn alternative is to useConfluent REST Proxy for Apache Kafka, and I can do this without writing any code but only leveraging the Serverless offering using EventBridge API destinations.######################################################################### #  FROM EB TO KAKFKA ##########################################################################   ApiDestinationsTargetRole:     Type: AWS::IAM::Role     Properties:       AssumeRolePolicyDocument:         Version: '2012-10-17'         Statement:           - Effect: Allow             Principal:               Service: events.amazonaws.com             Action: sts:AssumeRole       Path: /service-role/       Policies:         - PolicyName: secretmanager           PolicyDocument:             Version: '2012-10-17'             Statement:               - Effect: Allow                 Action:                   - \"secretsmanager:DescribeSecret\"                   - \"secretsmanager:GetSecretValue\"                 Resource: \"*\"         - PolicyName: destinationinvoke           PolicyDocument:             Version: '2012-10-17'             Statement:               - Effect: Allow                 Action:                   - events:InvokeApiDestination                 Resource:                   - !GetAtt KafkaProxyApi.Arn    MyApiDestinationFailedDeliveryDLQ:     Type: AWS::SQS::Queue    MyApiDestinationFailedDeliveryDLQPolicy:     Type: AWS::SQS::QueuePolicy     Properties:       Queues:         - !Ref MyApiDestinationFailedDeliveryDLQ       PolicyDocument:         Statement:         - Effect: Allow           Principal:             Service: events.amazonaws.com           Action: SQS:SendMessage           Resource: !GetAtt MyApiDestinationFailedDeliveryDLQ.Arn    ApiConnection:     Type: AWS::Events::Connection     Properties:       AuthorizationType: BASIC       AuthParameters:         BasicAuthParameters:           Password: '{{resolve:secretsmanager:confluent_cloud/service_account/myservice:SecretString:password}}'           Username: '{{resolve:secretsmanager:confluent_cloud/service_account/myservice:SecretString:username}}'    KafkaProxyApi:     Type: AWS::Events::ApiDestination     Properties:       ConnectionArn: !GetAtt ApiConnection.Arn       HttpMethod: POST       InvocationEndpoint:         Fn::Sub:           - '${RegistryUrl}/kafka/v3/clusters/${ClusterId}/topics/${Topic}/records'           - RegistryUrl: '{{resolve:secretsmanager:confluent_cloud/service_account/myservice:SecretString:schema_registry_url}}'             ClusterId: '{{resolve:secretsmanager:confluent_cloud/service_account/myservice:SecretString:cluster_id}}'             Topic:               Fn::FindInMap:                 - KafkaTopic                 - Ref: StageName                 - MyApiDestinationModified       InvocationRateLimitPerSecond: !FindInMap [Api, !Ref StageName, 'invocationRateLimitPerSecond']    MyApiDestinationAddedRule:     Type: AWS::Events::Rule     Properties:       EventBusName: !Ref LocalBusName       EventPattern:         source:           - \"mysource\"         detail-type:           - \"SOMETHING_CREATED\"       State: ENABLED       Targets:         - Id: MyApiDestination-added           Arn: !GetAtt KafkaProxyApi.Arn           RoleArn: !GetAtt ApiDestinationsTargetRole.Arn           InputTransformer:             InputPathsMap:               prop1: $.detail.data.prop1               prop2: $.detail.data.prop2               createdAt: $.detail.data.createdAt             InputTemplate: |-               {                 \"key\": {                   \"type\": \"JSON\",                   \"data\": \"<prop2>\"                 },                 \"value\": {                   \"type\": \"JSON\",                   \"data\": {                     \"prop1\":  \"<prop1>\",                     \"prop2\":  \"<prop2>\",                     \"_timestamp\": <createdAt>,                     \"action\": \"CREATED\"                   }                 }               }           HttpParameters:             HeaderParameters:               Content-Type: \"application/json\"           DeadLetterConfig:             Arn: !GetAtt MyApiDestinationFailedDeliveryDLQ.Arn           RetryPolicy:             MaximumRetryAttempts: 3             MaximumEventAgeInSeconds: 60Enter fullscreen modeExit fullscreen modeConclusionIn this article, I discussed two types of integration with Apache Kafka and how we can integrate into the AWS ecosystem without giving up the convenience of using managed services. At the time of writing, Kafka EventBridge Connector and Amazon EventBridge Pipes have some differences, which can significantly impact architectural decisions. With time, they will become more equal, but until then, it is crucial to understand the specific use case and the tradeoffs each configuration can bring. Ultimately, I presented two ways to publish messages into Kafka, and again, it is crucial to understand the limitations. For example, using the HTTP API Destination can lead to quota limitations at both the AWS and Confluent Kafka levels. While these limitations are sometimes flexible, writing simple code in highly concurrency scenarios may be easier."}
{"title": "Knowledge Base Support for the Generic Bedrock Agent Test UI", "published_at": 1716320529, "tags": [], "user": "Anthony Wat", "url": "https://dev.to/aws-builders/knowledge-base-support-for-the-generic-bedrock-agent-test-ui-4gfg", "details": "IntroductionIn the blog postDeveloping a Generic Streamlit UI to Test Amazon Bedrock Agents, I shared the design andsource codeof a basic yet functional UI for testing Bedrock agents. Since then, I've explored Knowledge Bases for Amazon Bedrock and shared my insights in another blog post,Adding an Amazon Bedrock Knowledge Base to the Forex Rate Assistant. If you haven't checked it out yet, I highly recommend doing so.The Bedrock console offers additional features for testing agents that are integrated with knowledge bases, including citations in the responses and trace information about the retrieved results from knowledge bases:With a bit of work, I have added similar support to the generic test UI and I am happy to share the updates in theGitHub repository.Design overviewWith the latest update, citations are now added to the response in a manner similar to how they are displayed in the Bedrock console:TheAgent for Bedrock Runtime APIprovides, for each citation, the start and end index of the text that references the source as well as the document location in S3. Through string manipulation, citation numbers are incorporated into the response text, and references are appended to the end of the text. Spacing is a bit finicky due to the use ofmarkdown, so some HTML markups are used.Another feature is the inclusion of citation details in theTracesection of the left pane:Each citation block provides the raw output from the API response and includes the information mentioned earlier, as well as the raw results retrieved by the knowledge base.Additionally, the trace blocks from the original test UI can now provide more detailed information for knowledge base invocations. You will find all references retrieved from the knowledge base, which form a superset of the citations in the final response after the model processes them.SummaryWith the improvements to the generic test UI outlined in this post, you should now be able to test any Bedrock agents with attached knowledge bases. I hope you find this update helpful and look forward to further enhancements as I continue on my Amazon Bedrock journey.Be sure to explore other posts on theAvangards Blogto learn more about generative AI in AWS, Terraform, and other technical topics. Have a great day!"}
{"title": "More secure Python Docker images with Amazon Linux \ud83d\udd10", "published_at": 1716302079, "tags": ["aws", "docker", "cloud", "devops"], "user": "Adrien Mornet", "url": "https://dev.to/aws-builders/more-secure-python-docker-images-with-amazon-linux-46f9", "details": "Amazon Linux is a Linux distribution provided by AWS specifically optimized for running workloads on AWS Cloud. This distribution, entirely managed by Amazon teams, offers very high standards in terms of security. In this article I\u2019ll explain why I prefer a Docker image based on Amazon Linux rather than Debian to run a python workload in the cloud.Debian based Python Docker imageLet\u2019s say you want to create a Docker image to run Python code in version3.9. Your first choice will probably be to start with a Docker image coming from Docker Hubpython:3.9right? I would have done the same, it\u2019s the easiest way.Most of the images on the Docker Hub are based on Debian and this is the case for python.Your Dockerfile would probably look like this :FROMpython:3.9WORKDIR/usr/src/appCOPYrequirements.txt ./RUNpipinstall\\--no-cache-dir-rrequirements.txtCOPY. .CMD\\[ \"python\", \"./your-daemon-or-script.py\" \\]Enter fullscreen modeExit fullscreen modeLet\u2019s build and push this Docker image to an ECR registry with \u201cScan on push\u201d enabled. The \u201cScan on push\u201d feature will run a security scan on your Docker image and see if there are any CVEs in it.Here is the result of the security scan :That\u2019s a lot for a fresh image containing only Debian libraries and a bit of python code, isn\u2019t it?Amazon Linux 2023 based Docker ImageLet\u2019s create a python Docker image now based onamazonlinux:2023:FROMamazonlinux:2023ENVPYTHON\\_VERSION\\=3.9RUN\\--mount=type=cache,target=/var/cache/dnf\\\\dnf \\-y update && \\\\       dnf install \\-y python${PYTHON\\_VERSION} python${PYTHON\\_VERSION}\\-pip shadow\\-utils git\\-all findutils awscli tar && \\\\       update\\-alternatives \\--install /usr/bin/python python /usr/bin/python${PYTHON\\_VERSION} 20 && \\\\       update\\-alternatives \\--set python /usr/bin/python${PYTHON\\_VERSION} && \\\\       dnf clean allWORKDIR/usr/src/appCOPYrequirements.txt ./RUNpipinstall\\--no-cache-dir-rrequirements.txtCOPY. .CMD\\[ \"python\", \"./your-daemon-or-script.py\" \\]Enter fullscreen modeExit fullscreen modeHere is the result of it\u2019s Security Scan :Much better!Why is there less CVEs in the Amazon Linux image?The differences in how Debian and Amazon Linux are developed and maintained contribute to the feeling that Debian-based Docker images are less frequently patched and therefore have more unpatched CVEs.Debian is a community-driven distribution. Security updates for Debian are generally reliable, but the frequency and speed at which they are released can vary because it relies heavily on volunteering contributions.Python image on Docker Hub is also maintained by community which means that two different communities will have to patch a CVE: the Debian community and the python docker image communityAmazon Linux is a distribution maintained by AWS. Amazon has a dedicated team that prioritizes security updates and patches, often releasing them quickly to ensure that their customers\u2019 systems remain secure.Amazon\u2019s centralized model and commercially driven approach to maintaining Amazon Linux ensures more consistent and rapid security updates which is why I think it\u2019s better to use docker images based from Amazon Linux instead of Debian.If you liked this post, you can find more on my bloghttps://adrien-mornet.tech/\ud83d\ude80"}
{"title": "AppSync CloudFormation Scaling Revisited", "published_at": 1716300300, "tags": ["aws", "serverless", "graphql"], "user": "Seth Orell", "url": "https://dev.to/aws-builders/appsync-cloudformation-scaling-revisited-9oo", "details": "Some years back, I was building an AppSync API with a few other engineers. None of us had GQL experience and we learned as we went. At the time, I ran across an article from Yan Cui titled \"How I scaled an AppSync project to 200+ resolvers\"1and thought \"That's interesting, but we won't need that.\"I was wrong.We quickly started bumping up against CloudFormation's stack limit and had to implement every one of Yan's good ideas in his article. This got us out of a jam and I am grateful that he took the time to write and share with the rest of us (Cheers, Yan! \ud83e\udd42).Yan's article closes with \"If you can think of a better way to do this, then please let me know!\" and, until recently, I didn't know of any other options. That changed recently when I began work on a new AppSync API for another client. I realized that I could take advantage of a new AppSync feature called \"Merged APIs\" to distribute my stack resources in a different way than the one Yan described, and I want to share it with you here.The ProblemBefore I get into any solutions, let's try to fully understand the problem.CloudFormation (CFN) is AWS's Infrastructure as Code (IaC) provisioning tool. It lets you define the AWS components you want to spin up and then handles the creation/update of those components (or, in CFN terms, \"Resources\") in a declarative way. You communicate to CFN via templates (JSON or YML) that you can store in source control and CFN \"builds the specified services in [a] safe, repeatable manner.\"2The number of Resources any one CloudFormation service (or \"Stack) is capped. At the time of Yan's article, that cap was 200. It has since moved to 500, but there still is a cap and you cannot increase it. Yan's article describes a way to split your one service into multiple CFN Stacks such that you can distribute your resources between them and stay under the Resource cap.Splitting stacks is complicated. Even using help from tools like serverless-plugin-split-stacks, you may have to manage how the tool splits up the stack. This \"how\" became the basis for Yan's article.Another ApproachIn May of 2023, AWS released Merged APIs for AppSync. This lets you create separate, discrete AppSync APIs and then present them as one unified API. The use case for this is often centered around having multiple teams work on domain-focused AppSync APIs, like so:source: Introducing Merged APIs on AWS AppSyncIn this arrangement, each independent AppSync has its own CloudFormation Stack. This distributes the number of total Resources among each of these domain-bounded Stacks. In the image, above, our overall resource cap went from 500 (for a single CFN stack) to 2000.These two methods (Merged APIs and Split Stacks) are not mutually exclusive. For example, the Books Service may have so many resources that you need to split it. Assuming you couldn't further split at the domain level, you could apply serverless-plugin-split-stacks to just this one service to alleviate its CFN Resource cap pressure.I have previously written about how to implementAppSync Merged API as Code, so refer to that article if you need specifics on how to wire it up.Some CaveatsLike Yan stated in his article, I am also a one-man team for this backend. With my approach, you will end up with one serverless.yml for each separate AppSync. For example, given the domains listed in the image above, you would have five serverless manifests (one for each domain + merged):serverless.users.yml,serverless.authors.yml,serverless.books.yml,serverless.reviews.yml, andserverless.merged.yml3. You are now splitting your schema definitions and implementation instructions by domain, but I like this.AWS initially advertised Merged APIs in AppSync as a way to address \"challenges related to multiple teams\"4. While I think this is a great solution for multiple teams, I don't have that problem (Yan didn't, either). Irrespective, the domain separation is something that I would do anyway; the separate deploys seems like a natural extension of that domain separation. I still have everything in a single repository, but I could easily separate it should I need to assign ownership to other teams.SummaryNow you have another tool to help avoid the CloudFormation resource cap. If you haven't yet worked with Merged APIs, this may be the thing that lets you get your hands dirty. I find splitting up my stacks this way (by domain) to be much easier to manage than with astacks-map.jsfile and thesplit-stacks plugin. Give it a try, and have fun!Further ReadingYan Cui:How I scaled an AppSync project to 200+ resolversOwnership Matters: [AppSync Merged API as Codehttps://sethorell.substack.com/p/appsync-merged-api-as-code)AWS Docs:What is AWS CloudFormation?AWS Whitepaper:Introduction to DevOps on AWS: CloudFormationGitHub:serverless-plugin-split-stacksAWS Blog:Introducing Merged APIs on AWS AppSynchttps://theburningmonk.com/2020/07/how-i-scaled-an-appsync-project-to-200-resolvers/\u21a9AWS Whitepaper \"Introduction to DevOps on AWS\"\u21a9This last serverless manifest is a thin wrapper over the CloudFormation that sets up the Merged API. See my2023 articleon how to structure this.\u21a9Introducing Merged APIs on AWS AppSync\u21a9"}
{"title": "Amazon EKS: Analyze control plane and CloudTrail logs for better detective controls", "published_at": 1716266310, "tags": ["eks", "audit", "cloudtrail", "threatdetection"], "user": "saifeddine Rajhi", "url": "https://dev.to/aws-builders/amazon-eks-analyze-control-plane-and-cloudtrail-logs-for-better-detective-controls-3b3a", "details": "Get visibility into EKS Activities and AWS API calls for better security monitoringIntroduction:In this blog post, we'll explore how to use EKS control plane logs and AWS CloudTrail logs to gain visibility into your cluster's activities, detect potential security threats, and investigate incidents.EKS Control Plane\u00a0Logs:Amazon EKS control plane logging sends audit and diagnostic logs directly to CloudWatch Logs in your account, aiding in cluster security and management. Log types can be tailored to your needs and are organized into log streams for each Amazon EKS cluster in CloudWatch.Pricing: Standard Amazon EKS pricing applies for cluster usage, along with CloudWatch Logs data ingestion and storage costs.Available log types:Kubernetes API server (api): Exposes the Kubernetes API.Audit (audit): Records users, administrators, or system components impacting the cluster.Authenticator (authenticator): Handles Kubernetes RBAC authentication via IAM credentials.Controller manager (controllerManager): Manages core control loops.Scheduler (scheduler): Determines pod scheduling.You can enable or disable each log type on a per-cluster basis using the AWS Management Console, AWS CLI, or through the Amazon EKS API.CloudTrail Logs:CloudTrail is enabled by default in your AWS account, recording activity including Amazon EKS events. Events can be viewed, searched, and downloaded in your AWS account.CloudTrail also documents interactions with AWS APIs by pods using IAM Roles for Service Accounts (IRSA), aiding in security auditing and compliance.Setup EKS Control Plane\u00a0logs:By default, cluster control plane logs aren't streamed to CloudWatch Logs. To enable logging for your cluster, you need to enable each log type individually.Note that CloudWatch Logs ingestion, archive storage, and data scanning rates apply to enabled control plane logs.To check the status of EKS Control Plane Logs, run the following command:aws eks describe-cluster --name eks-audit-logs-demo --query 'cluster.logging'Enter fullscreen modeExit fullscreen modeYou'll receive output similar to this:{     \"clusterLogging\": [         {             \"types\": [                 \"api\",                 \"audit\",                 \"authenticator\",                 \"controllerManager\",                 \"scheduler\"             ],             \"enabled\": true         }     ] }Enter fullscreen modeExit fullscreen modeTo update EKS Control Plane Logs, run this command:aws eks update-cluster-config --name eks-audit-logs-demo--logging '{\"clusterLogging\":[{\"types\":[\"api\",\"audit\",\"authenticator\",\"controllerManager\",\"scheduler\"],\"enabled\":true}]}'Enter fullscreen modeExit fullscreen modeAfter you have enabled any of the control plane log types for your Amazon EKS cluster, you can view them on the CloudWatch console.To learn more about viewing, analyzing, and managing logs in CloudWatch, seethe Amazon CloudWatch Logs User Guide.Run the following command to view log-streams.aws logs describe-log-streams --log-group-name /aws/eks/eks-audit-logs-demo/cluster --max-items 10 --order-by LastEventTime --query 'logStreams[].logStreamName'Enter fullscreen modeExit fullscreen modeThe output contained 10 log streams for log group /aws/eks/eks-audit-logs-demo/cluster.[     \"kube-controller-manager-9ac68952bbfa494eb1625e2ff3f07bf7\",     \"kube-controller-manager-96624a2fbc193d5ccd64f9f1ddbebbe3\",     \"kube-apiserver-96624a2fbc193d5ccd64f9f1ddbebbe3\",     \"kube-scheduler-96624a2fbc193d5ccd64f9f1ddbebbe3\",     \"authenticator-9ac68952bbfa494eb1625e2ff3f07bf7\",     \"cloud-controller-manager-9ac68952bbfa494eb1625e2ff3f07bf7\",     \"kube-scheduler-9ac68952bbfa494eb1625e2ff3f07bf7\",     \"authenticator-96624a2fbc193d5ccd64f9f1ddbebbe3\",     \"kube-apiserver-audit-9ac68952bbfa494eb1625e2ff3f07bf7\",     \"cloud-controller-manager-96624a2fbc193d5ccd64f9f1ddbebbe3\" ]Enter fullscreen modeExit fullscreen modeQuery EKS Control Plane logs using CloudWatch Logs Insights:CloudWatch Logs Insights enables you to query and analyze your EKS Control Plane logs interactively within CloudWatch Logs. This functionality enables you to swiftly and effectively address operational challenges.By executing queries, you can pinpoint potential causes of issues and verify the efficacy of implemented solutions. CloudWatch Logs Insights boasts a purpose-built query language equipped with straightforward yet robust commands, enhancing your ability to extract actionable insights from your log data.The following covers multiple scenarios and their queries. To run these queries in AWS Console,First, navigate to CloudWatch Logs Insights in the console:https://console.aws.amazon.com/cloudwatch/home#logsV2:logs-insightsThen select the Log Group/aws/eks/eks-audit-logs-demo/clusterList create, update, and delete operations to RoleBindings:Replace the query with the following and click \"Run Query\"fields @timestamp, @message | sort @timestamp desc | limit 100 | filter objectRef.resource=\"rolebindings\" and verb in [\"create\", \"update\", \"patch\", \"delete\"]Enter fullscreen modeExit fullscreen modeFind HTTP 5xx server errors related to Kubernetes API server requests.In Kubernetes, 5xx errors refer to a category of HTTP status codes that are returned by the Kubernetes API server to indicate server-side errors. These errors occur when the API server encounters an issue or encounters an unexpected condition that prevents it from fulfilling a client's request successfully.500\u00a0: Internal Server Error502\u00a0: Bad Gateway503\u00a0: Service Unavailable504\u00a0: Gateway Timeout509\u00a0: Bandwidth Limit ExceededReplace the query with the following and click \"Run Query\"fields @logStream, @timestamp, responseStatus.code, @message | filter @logStream like /^kube-apiserver-audit/ | filter responseStatus.code >= 500 | limit 50Enter fullscreen modeExit fullscreen modeAudit EKS using CloudTrail Insights:Use CloudTrail Insights to conduct an audit of your CloudTrail logs effectively. CloudTrail Insights automates the analysis of management events captured by your CloudTrail trails. It establishes a baseline for normal behavior and triggers Insights events when detecting unusual patterns.Upon detecting abnormal activity, CloudTrail Insights raises events via dashboard views in the CloudTrail console, delivers them to your Amazon S3 bucket, and forwards them to Amazon CloudWatch Events.You can also enable Insights on a trail from the AWS CLI by using the put-insight-selectors command:aws cloudtrail put-insight-selectors --trail-name eks-cloudtrail --insight-selectors '[{\"InsightType\": \"ApiCallRateInsight\"},{\"InsightType\": \"ApiErrorRateInsight\"}]'Enter fullscreen modeExit fullscreen modeAWS APIs called by pods that are using IAM Roles for Service Accounts (IRSA) are automatically logged to CloudTrail along with the name of the service account. If the name of a service account that wasn't explicitly authorized to call an API appears in the log, it may be an indication that the IAM role's trust policy was misconfigured.Generally speaking, Cloudtrail is a great way to ascribe AWS API calls to specific IAM principals.Use CloudTrail Insights to unearth suspicious activity:CloudTrail insights automatically analyzes write management events from CloudTrail trails and alerts you of unusual activity. This can help you identify when there's an increase in call volume on write APIs in your AWS account, including from pods that use IRSA to assume an IAM role. SeeAnnouncing CloudTrail Insights: Identify and Response to Unusual API Activityfor further information.Additional resources:As the volume of logs increases, parsing and filtering them with Log Insights or another log analysis tool may become ineffective. As an alternative, you might want to consider running Sysdig Falco and ekscloudwatch. Falco analyzes audit logs and flags anomalies or abuse over an extended period of time.The ekscloudwatch project forwards audit log events from CloudWatch to Falco for analysis.Falco provides a set of default audit rules along with the ability to add your own.Yet another option might be to store the audit logs in S3 and use the SageMaker Random Cut Forest algorithm to anomalous behaviors that warrant further investigation.Tools and resources:The following commercial and open source projects can be used to assess your cluster's alignment with established best practices:Amazon EKS Security Immersion Workshop\u200a-\u200aDetective Controlskubeauditkube-scan Assigns a risk score to the workloads running in your cluster in accordance with the Kubernetes Common Configuration Scoring System frameworkkubesec.iopolarisStarboardSnykKubescape Kubescape is an open source kubernetes security tool that scans clusters, YAML files, and Helm charts. It detects misconfigurations according to multiple frameworks (including NSA-CISA and MITRE ATT&CK\u00ae.)Conclusion:In conclusion, using Amazon EKS control plane and CloudTrail logs is essential for enhancing your security posture. By monitoring these logs, you gain invaluable insights into EKS activities and AWS API calls, enabling proactive detection and quick response to potential security threats. Implement these detective controls to enforce your AWS environment and maintain robust security standards."}
{"title": "Tactical Cloud Audit Log Analysis with DuckDB - AWS CloudTrail", "published_at": 1716243263, "tags": ["aws", "security", "cloudsecurity", "duckdb"], "user": "Dakota Riley", "url": "https://dev.to/aws-builders/tactical-cloud-audit-log-analysis-with-duckdb-aws-cloudtrail-2amk", "details": "Using DuckDB to query Cloud Provider audit logs when you don't have a SIEM available.\u26a0\ufe0f Just want the code? Check out my gisthereMore than once, I have been in a situation where I needed to query CloudTrail logs but was working in a customer environment where they weren\u2019t aggregated to a search interface. Another similar situation is when CloudTrail data events are disabled for cost reasons but need to be temporarily turned on for troubleshooting/audit purposes. While the CloudTrail console offers some (very) limited lookups (for management events only), and Athena is an option, what aboutDuckDB?DuckDB offers both the ability to retrieve directly from S3, as well as parse JSON files into queryable tables. This blog is my documentation of working through that process! This blog assumes you already have DuckDB installed, if not, starthere.SetupStart a DuckDB session. DuckDB can operate either fully in memory or utilize disk space to process datasets larger than your available memory. For Cloudtrail in a single account over a day, in memory should be fine, but we can use persistent storage mode to make sure our tables don't disappear when we exit:duckdb cloudtrail-analysisEnter fullscreen modeExit fullscreen modeNext, load theAWS extension:INSTALLAWS;LOADAWS;Enter fullscreen modeExit fullscreen modeThis lets you load AWS credentials from your CLI profiles a bit easier than the default workflow. We can load whatever credentials we have configured in our environment or AWS CLI profile using it:CALLload_aws_credentials();Enter fullscreen modeExit fullscreen modeBefore we go down the SQL Rabbit-hole, lets consider the structure of CloudTrail as it gets exported to S3:#cloudtrail_file.json{\"Records\":[{\"eventVersion\":\"1.09\",\"userIdentity\":{\"type\":\"IAMUser\",\"principalId\":\"EXAMPLE6E4XEGITWATV6R\",\"arn\":\"arn:aws:iam::123456789012:user/Mary_Major\",\"accountId\":\"123456789012\",}...}...,]}Enter fullscreen modeExit fullscreen modeTakeaway here is: the file is structured as a single JSON object, with a top level key ofRecordsthat is an array containing our CloudTrail entries that we are after. We will need to explode the records out of that array into a table to make them useful.DuckDB\u2019sread_jsonfunction by default will attempt to determine the schema of JSON files, and adapt the column data types accordingly. CloudTrail entries have a few common top level fields but tend to be very dynamic when it comes to specific fields for that event (eg RequestParameters). We can use themaximum_depthparameter on our read_json call to override this functionality.To avoid redownloading the files from S3 over and over again, we can use the CREATE TABLE \u2026 AS statement (aka CTAS in the SQL world) to create a table from our read_json query:CREATETABLEct_rawASSELECT*FROMread_json('s3://org-cloudtrail-111122223333/AWSLogs/o-123456043/111122223333/CloudTrail/us-east-1/2024/05/19/*.gz',maximum_depth=2);Enter fullscreen modeExit fullscreen modeThis gets us a table with a single column: Records with a data type of an array of JSON objects. Next, we can explode the list usingunnestto access the individual events:CREATETABLEctASSELECTunnest(Records)ASEventFROMct_raw;Enter fullscreen modeExit fullscreen modeThe JSON datatype allows us to to access the nested values using dot notation, which looks like this:event.userIdentity.arn. While this can offer us some limited querying, when we want to utilize our columns in the WHERE statement, the JSON datatype isn't ideal. To finish, we can extract the keys we care about into separate columns usingjson_extract_string:CREATETABLEcloudtrail_eventsASSELECTjson_extract_string(event,'$.eventVersion')ASeventVersion,json_extract_string(event,'$.userIdentity.type')ASuserType,json_extract_string(event,'$.userIdentity.principalId')ASprincipalId,json_extract_string(event,'$.userIdentity.arn')ASuserArn,json_extract_string(event,'$.userIdentity.accountId')ASaccountId,json_extract_string(event,'$.userIdentity.accessKeyId')ASaccessKeyId,json_extract_string(event,'$.userIdentity.userName')ASuserName,CAST(json_extract_string(event,'$.eventTime')ASTIMESTAMP)ASeventTime,json_extract_string(event,'$.eventSource')ASeventSource,json_extract_string(event,'$.eventName')ASeventName,json_extract_string(event,'$.awsRegion')ASawsRegion,json_extract_string(event,'$.sourceIPAddress')ASsourceIPAddress,json_extract_string(event,'$.userAgent')ASuserAgent,json_extract_string(event,'$.errorCode')ASerrorCode,json_extract_string(event,'$.errorMessage')ASerrorMessage,json_extract(event,'$.requestParameters')ASrequestParameters,json_extract(event,'$.responseElements')asresponseElements,json_extract(event,'$.resources')ASresources,FROMctEnter fullscreen modeExit fullscreen modeQuery time!Some sample queries:All actions taken by a particular IAM Principal:selecteventName,eventTime,userAgentfromcloudtrail_eventswherearn='REPLACE_ME';Enter fullscreen modeExit fullscreen modeAll the unique error messages:selectdistincterrorCodefromcloudtrail_events;Enter fullscreen modeExit fullscreen modeGet all events in the past 24 hours:select*fromcloudtrail_eventswhereeventtime>=(now()-INTERVAL'1 day');Enter fullscreen modeExit fullscreen modeHappy querying!"}
{"title": "DNS as code", "published_at": 1716213660, "tags": ["terraform", "dns", "homelab"], "user": "Jakub Wo\u0142ynko", "url": "https://dev.to/aws-builders/dns-as-code-40ic", "details": "WelcomeLooks like I have 10 days to create 2 blog posts. To be open, it\u2019s another attempt to become a creator, who makes at least 12 posts per year. Also, I decided, to make my write-ups a bit shorter and focused on one particular topic.  That is why, today I will show you how to migrate existingCloudflareconfiguration into OpenTofu manifests and make it even more cool withScalr.For context, I\u2019m the owner of 3 domains, where one is used for my home lab. What does that it mean? I like to access my services without VPN, from anywhere in the world. Hopefully, Cloudflare provides a solution calledtunnels. It could be used for exposing private endpoints/services directly on the internet, without setting up static IP(for example my ISP requires an additional fee for that), also we can easily add multiple \u201cdata centers\u201d, for example, NUC under our desk, home NAS, or regular server in your parent's basement.Import our resourcesFor importing our DNS settings I was using a tool calledcf-terraforming.As the first step, we need to become more familiar with the Cloudflare name convention., but don\u2019t panic. Essential variables are:--zone string Target the provided zone ID for the command--account string Target the provided account ID for the commandThe zone is an ID of your domain zone. Let\u2019s assume that if you're managingexample.comandhello.comdomains, every one of them has a dedicated zone ID.Account ID is the same for all zones and represents your account identity.Great, now we need to generate tokens for our process.Login to your accountGo tothispage and generate the token.Try to grant the lowest possible permission, ideally one token per zone, per needed resource.Now with the token stored in buffer, we can try to generate our first DNS entry.$cf-terraforming generate-eyour_email\\-tyour_token\\-zyour_zone_id\\--resource-typecloudflare_record FATA[0003] failed to detect provider installationEnter fullscreen modeExit fullscreen modeWait, what? Provider?Yes, for this operation we already need to have an initialised Tofu project.Let\u2019s then take a quick break, and talk about Tofu and Scalr configuration.OpenTofu and ScalrNow all of you know, thatIBM acquired Hashicorp, unfortunately when I performing my migration, I have no idea about Hashicorp feature. Also, Terraform Cloud is ugly and unfriendly to use IMHO. That I why I decided to use OpenTofu as a drop-in replacement and try Scalr as one of many Terraform Cloud alternatives.Ok, now it\u2019s a good moment to talk about the final project structure. Based on my research and experience managing resources in the following structure will be very flexible and easy to use based on my scale and needs.\u276f tree..\u251c\u2500\u2500 account_a \u2502\u00a0\u00a0 \u251c\u2500\u2500 zone_1 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 dns \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 dns.tf \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 dns.tf.bck \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 providers.tf \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2514\u2500\u2500 vars.tf \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2514\u2500\u2500 tunnels \u2502\u00a0\u00a0 \u2502\u00a0\u00a0     \u251c\u2500\u2500 outputs.tf \u2502\u00a0\u00a0 \u2502\u00a0\u00a0     \u251c\u2500\u2500 providers.tf \u2502\u00a0\u00a0 \u2502\u00a0\u00a0     \u251c\u2500\u2500 tunnels.tf \u2502\u00a0\u00a0 \u2502\u00a0\u00a0     \u2514\u2500\u2500 vars.tf \u2502\u00a0\u00a0 \u251c\u2500\u2500 zone_2 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2514\u2500\u2500 dns \u2502\u00a0\u00a0 \u2502\u00a0\u00a0     \u251c\u2500\u2500 dns.tf \u2502\u00a0\u00a0 \u2502\u00a0\u00a0     \u251c\u2500\u2500 providers.tf \u2502\u00a0\u00a0 \u2502\u00a0\u00a0     \u2514\u2500\u2500 vars.tf \u2502\u00a0\u00a0 \u2514\u2500\u2500 zone_3 \u2502\u00a0\u00a0     \u2514\u2500\u2500 dns \u2502\u00a0\u00a0         \u251c\u2500\u2500 dns.tf \u2502\u00a0\u00a0         \u251c\u2500\u2500 providers.tf \u2502\u00a0\u00a0         \u2514\u2500\u2500 vars.tf \u251c\u2500\u2500 LICENSE \u251c\u2500\u2500 README.md \u251c\u2500\u2500 main.tf \u2514\u2500\u2500 vars.tf  9 directories, 18 filesEnter fullscreen modeExit fullscreen modeAs you can see, I have dedicated vars and providers per resource in the zone, but it gives me full control over my critical at the end part of the infrastructure. Especially in the case of using \u201ca bit like beta\u201d type of resources, for example, tunnels or zero trust modules.  However, let\u2019s start from scratch.First ourmain.tfis very simple:terraform{cloud{hostname=\"<hostname>\"organization=\"<org>\"workspaces{name=\"<ws>\"}}}module\"zone_1_dns\"{source=\"./account_a/zone_1/dns\"api_token=var.zone_1_token}Enter fullscreen modeExit fullscreen modeIn the beginning, we have Scalr config, which is very easy and similar to any other service provider. Also, I\u2019m using Scalr as Terraform Backend for my service. Here you can find a dedicated posthow to configure it. I will do it with the usage of fewer details, with the usage of pictures.Go to Workspaces and click \u2018Create Workspaces\u2019Fill in the name, and add GitHub provider according to your preferences.Remember to use OpenTofuIaC Platform.Then create the workspace.Now that we have our account configured we can fill ourmain.tfaccording to description:The hostname argument refers to our instance of Scalr, the organization refers to the environment within our instance, and a workspace is just a workspace!Ok, now we need to add our first module.$mkdir-pv./account_a/zone_1/dnsEnter fullscreen modeExit fullscreen modeThen let\u2019s add./account_a/zone_1/dns/providers.tffile:terraform{required_version=\"~> 1.5\"required_providers{cloudflare={source=\"cloudflare/cloudflare\"version=\"4.28.0\"}}}provider\"cloudflare\"{api_token=var.api_token}Enter fullscreen modeExit fullscreen mode./account_a/zone_1/dns/vars.tffile:variable\"api_token\"{type=stringdescription=\"Cloudflare API token\"sensitive=true}Enter fullscreen modeExit fullscreen mode./vars.tffile:variable\"zone_a_token\"{type=stringdescription=\"Cloudflare API token for zone A\"sensitive=true}Enter fullscreen modeExit fullscreen modeAnd to avoid unnecessary typing.auto.tfvarszone_1_token=\"secret-token\"Enter fullscreen modeExit fullscreen modeAfter that, we can try to init our project (and plan to make sure that Scalr connection is fine)$tofu init[...]$tofu plantofu plan [...] No changes. Your infrastructure matches the configuration.  OpenTofu has compared your real infrastructure against your configuration and found no differences, so no changes are needed. [...]Enter fullscreen modeExit fullscreen modeGreat, we\u2019re finally ready to generate our DNS entries.$cf-terraforming generate\\-eyour_email\\-tyour_token\\-zyour_zone_id\\--resource-typecloudflare_record FATA[0003] failed toreadprovider schemaexit status 1  Error: Failed to load plugin schemas  Errorwhileloading schemasforplugin components: Failed to obtain provider schema: Could not load the schemaforthe provider registry.terraform.io/cloudflare/cloudflare: failed to instantiate provider\"registry.terraform.io/cloudflare/cloudflare\"to obtain schema: unavailable provider\"registry.terraform.io/cloudflare/cloudflare\"..Enter fullscreen modeExit fullscreen modeAh, drop-in replacement right? Yes, but actually no, if terraform is required by another tool. But hey! It\u2019s only a home project.DNS generationSmall fix, and bum!$rm-rf.terraform .terraform.lock.hcl$terraform version Terraform v1.5.7 on darwin_arm64[...]$terraform init[...]$cf-terraforming generate\\-eyour_email\\-tyour_token\\-zyour_zone_id\\--resource-typecloudflare_record  resource\"cloudflare_record\"\"terraform_managed_resource_16e2c78bf8ed9b72215f56cc7fd\"{name=\"example.com\"proxied=truettl=1type=\"A\"value=\"23.21.20.120\"zone_id=\"your_zone_id\"}resource\"cloudflare_record\"\"terraform_managed_resource_74824bf72d4607c154097887d66\"{name=\"example.com\"proxied=truettl=1type=\"A\"value=\"23.21.20.19\"zone_id=\"your_zone_id\"}resource\"cloudflare_record\"\"terraform_managed_resource_06a882256f1ed1b393253633d6c1\"{name=\"www\"proxied=truettl=1type=\"CNAME\"value=\"example.com\"zone_id=\"your_zone_id\"}Enter fullscreen modeExit fullscreen modeIn general, we can even redirect command output to file and populate./account_a/zone_1/dns/dns.tf$cf-terraforming generate\\-eyour_email\\-tyour_token\\-zyour_zone_id\\--resource-typecloudflare_record>./account_a/zone_1/dns/dns.tfEnter fullscreen modeExit fullscreen modeFastterraform plan, and please notice to things.Scalr is still using OpenTofu, even if you usedterraform plancommand on your workstation.We\u2019re going to create 3 records, even if they already exist\u2026 and that\u2019s the problem we need to resolve.$terraform planRunning plan in Terraform Cloud. Output will stream here. Pressing Ctrl-C will stop streaming the logs, but will not stop the plan from running remotely.  Preparing the remote plan...  To view this run in a browser, visit: https://example.scalr.io/app/prod/my-awesome-cloudflare-config/runs/run-v0oce2teqmu10eo0k  Waiting for the plan to start...  OpenTofu v1.7.1 [...] Plan: 3 to add, 0 to change, 0 to destroy. [...]Enter fullscreen modeExit fullscreen modeImporting configThankfully we can useimportflag of the cf-terraforming tool. Let\u2019s use it then:cf-terraforming import\\-eyour_email\\-tyour_token\\-zyour_zone_id\\--resource-typecloudflare_record\\--modern-import-block>>main.tfEnter fullscreen modeExit fullscreen modeNow we should be able to import our resources right? Not yet. Cf-terraforming does not respect file structure, so our imports have the following format:import{to=cloudflare_record.terraform_managed_resource_16e2c78bf8ed9b72215f56cc7fdid=\"1fca8e4a1d16a9cdccc3f9f30ebb6317/06a882256f1ed1b39322f7453633d6c1\"}Enter fullscreen modeExit fullscreen modeUnfortunately, this will produce an error:terraform plan[...]------------------------------------------------------------------------\u2577 \u2502 Error: Configurationforimport target does not exist \u2502 \u2502   on main.tf line 16: \u2502   16: import{\u2502 \u2502 The configurationforthe given import \u2502 cloudflare_record.terraform_managed_resource_16e2c78bf8ed9b72215f56cc7fd \u2502 does not exist. All target instances must have an associated configuration \u2502 to be imported.Enter fullscreen modeExit fullscreen modeAnd that is very easy to solve, in ourmain.tffile just add a module. prefix. (It could be painful in case of big import).import{to=module.zone_1_dns.cloudflare_record.terraform_managed_resource_16e2c78bf8ed9b72215f56cc7fdid=\"1fca8e4a1d16a9cdccc3f9f30ebb6317/16e2c78bfd13568ed9b72215f56cc7fd\"}Enter fullscreen modeExit fullscreen modeNow we can test our manifests by executingterraform plan:$terraform plan[...] Plan: 3 to import, 0 to add, 0 to change, 0 to destroy.[...]Enter fullscreen modeExit fullscreen modeNow stop! Do not apply by hand. We have Scalr for it, right? The only thing you need to do is push your code to the repository(and add a token as a variable in the Scalr console). After push you should see something like this in your web console:Final notesAs we planned first, the pushing code does nothing, and does not populate the state, but further declaration will do as well as polish the state.You can migrate to OpenTofu again, as soon as you migrate all Cloudflare blocks from Cloudflare\u2019s API.Not all objects are supported bycf-terraformingtool, for example, tunnels. You can check supported resourceshereYou can visit this repo template onGitHub.SummaryUff, that was an interesting adventure, right?As you can see, Scalr is just fine and provides all the features I can expect from my \u201cremote backend\u201d service.OpenTofu in case of regular projects can act as a drop-in Terraform replacement, at least for today.Cloudflare is great, even if you\u2019re on the free tier, and you\u2019re the product."}
{"title": "GenAI: Using Amazon Bedrock, API Gateway, Lambda and S3", "published_at": 1716212553, "tags": ["aws", "serverless", "ai", "bedrock"], "user": "Jhon Robert Quintero Hurtado", "url": "https://dev.to/aws-builders/genai-using-amazon-bedrock-api-gateway-lambda-and-s3-j6e", "details": "About Me:Jhon Robert Quintero H.IntroductionI was thrilled to be a speaker at theXIII Meeting SOFTHARD 2024, hosted by theFaculty of Engineeringof theInstituci\u00f3n Universitaria Antonio Jos\u00e9 Camacho. Here's a snapshot of the key takeaways from my presentation on Generative AI, focusing on Amazon Bedrock, API Gateway, Lambda, and S3. Buckle up for an exciting ride!Amazon BedrockPicture this: Amazon Bedrock is your magic wand for building and scaling generative AI applications with foundation models. It's like having a fully managed AI factory at your fingertips!Fully Managed Service: Say goodbye to the nitty-gritty of managing infrastructure.Choose Your Model: It\u2019s like a buffet of the best AI models from AI21 Labs, Anthropic, Cohere, Meta, and Stability. Just pick what you need!Customize with Your Data: Make your model truly yours.For instance, Anthropic\u2019s Claude model is the hotshot for tasks like summarization and complex reasoning, while Stability AI\u2019s Stable Diffusion model is your go-to for generating unique images, art, and designs. Need copywriting help? Cohere Command's got your back!Anthropic\u2019s Claude 3 ModelMeet Claude 3, the genius in the room! It's smarter, faster, and cheaper than GPT 3.5T, and it\u2019s got vision, literally!WithClaude 3, you can do:Dialogue and role-playSummarization and Q&ATranslationDatabase querying and retrievalCoding-related tasksClassification, metadata extraction, and analysisText and content generationContent moderationAmazon\u2019s Titan ModelTheTitanmodel is another powerhouse. Here\u2019s what it can do:Text Embeddings: Converts text into numerical form for searching, retrieving, and clustering. Think of it as translating words into a secret numerical language.Image Generator: Create stunning images with just a few words. It\u2019s like having a personal artist at your service!Customizing Amazon Titan modelsYou can nowcustomize foundation models(FMs) with your own data inAmazon Bedrockto build applications that are specific to your domain, organization, and use case. With custom models, you can create unique user experiences that reflect your company\u2019s style, voice, and services.Withfine-tuning, you can increase model accuracy by providing your own task-specific labeled training dataset and further specialize your FMs. You can train models using your own data in a secure environment with customer-managed keys.Continued pre-traininghelps models become more specific to your domain.Knowledge Bases for Amazon BedrockIt lets you put text documents, like articles or reports, into a knowledge base. It also automatically creates vector representations of text documents, which are called embeddings. These embeddings can be used for retrieval-augmented generation. This is a key feature of the Amazon Bedrock service. It lets you use your own data to enhance foundation models.Stores embeddings in your vector database (Amazon OpenSearch). Retrieves embeddings and augments prompts.Solution Design: Text SummarizationLet\u2019s dive into the cool stuff! Here\u2019s how I designed a text summarization solution using Bedrock, API Gateway, Lambda, and S3.Bedrock: Our star player.API Gateway: Acts as the doorman, directing client requests to the right Lambda function.Lambda Function: Processes the text and sends it to Bedrock for summarization.S3 Bucket: Stores the summarized text.Check out the source code below. It\u2019s like a recipe for your favorite dish \u2013 just follow the steps and enjoy the result!Python Source Codeimport boto3 import botocore.config import json import base64 from datetime import datetime from email import message_from_bytes   def extract_text_from_multipart(data):     msg = message_from_bytes(data)      text_content = ''      if msg.is_multipart():         for part in msg.walk():             if part.get_content_type() == \"text/plain\":                 text_content += part.get_payload(decode=True).decode('utf-8') + \"\\n\"      else:         if msg.get_content_type() == \"text/plain\":             text_content = msg.get_payload(decode=True).decode('utf-8')      return text_content.strip() if text_content else None   def generate_summary_from_bedrock(content:str) ->str:     prompt = f\"\"\"Summarize the following meeting notes: {content} \"\"\"      body = json.dumps({\"inputText\": prompt,                         \"textGenerationConfig\":{                            \"maxTokenCount\":4096,                            \"stopSequences\":[],                            \"temperature\":0,                            \"topP\":1                        },                       })       modelId = 'amazon.titan-tg1-large' # change this to use a different version from the model provider     accept = 'application/json'     contentType = 'application/json'              try:         boto3_bedrock = boto3.client('bedrock-runtime')         response = boto3_bedrock.invoke_model(body=body, modelId=modelId, accept=accept, contentType=contentType)         response_body = json.loads(response.get('body').read())              summary = response_body.get('results')[0].get('outputText')         return summary      except Exception as e:         print(f\"Error generating the summary: {e}\")         return \"\"  def save_summary_to_s3_bucket(summary, s3_bucket, s3_key):      s3 = boto3.client('s3')      try:         s3.put_object(Bucket = s3_bucket, Key = s3_key, Body = summary)         print(\"Summary saved to s3\")      except Exception as e:         print(\"Error when saving the summary to s3\")   def lambda_handler(event,context):      decoded_body = base64.b64decode(event['body'])      text_content = extract_text_from_multipart(decoded_body)      if not text_content:         return {             'statusCode':400,             'body':json.dumps(\"Failed to extract content\")         }       summary = generate_summary_from_bedrock(text_content)      if summary:         current_time = datetime.now().strftime('%H%M%S') #UTC TIME, NOT NECCESSARILY YOUR TIMEZONE         s3_key = f'summary-output/{current_time}.txt'         s3_bucket = 'bedrock-analisys-co'          save_summary_to_s3_bucket(summary, s3_bucket, s3_key)      else:         print(\"No summary was generated\")       return {         'statusCode':200,         'body':json.dumps(\"Summary generation finished\")     }Text Generation ConfigurationUnderstanding thetextGenerationConfigparameters is key for tweaking the model's behavior:maxTokenCount:This sets the maximum number of tokens (words or subwords) that the text generation model can produce. It helps control the length of the generated text.Think of this as setting the maximum length for a speech. Just like a speechwriter might say, \"Your speech should be no longer than 500 words,\" the maxTokenCount controls the length of the generated text, ensuring it doesn\u2019t run on indefinitely.stopSequences:This is a list of sequences (e.g., newline characters) that, if encountered during text generation, will cause the generation to stop.Imagine you\u2019re listening to a song that has a specific note that signals the end, like a grand finale. Similarly, stopSequences act like these finishing notes; they are predefined sequences that, when detected, tell the model to stop generating more text.temperature:This parameter controls the \"creativity\" or \"randomness\" of the text generation. A lower temperature (e.g., 0) will result in more conservative, predictable text, while a higher temperature (e.g., 1) will produce more diverse and potentially more creative text.Picture a painter with a palette of colors. A lower temperature is like having only a few colors to choose from, resulting in a more predictable and uniform painting. A higher temperature is like having a wide variety of colors, leading to a more vibrant and unexpected artwork.topP:This is a technique called \"nucleus sampling\" that limits the text generation to the most likely tokens, based on the model's probability distribution. A value of 1 means no filtering, while a lower value (e.g., 0.9) will restrict the generation to the top 90% of the most likely tokens.Imagine a buffet where you want to sample the most popular dishes. A topP value of 1 means you can choose from the entire spread. A lower topP value, like 0.9, means you\u2019re only selecting from the top 90% of the most popular dishes, skipping the least likely choices to ensure a satisfying and high-quality meal.These parameters let you fine-tune the model\u2019s behavior to suit your needs. Feel free to experiment and find the perfect balance!Video DemoStay tuned for the video demo where I'll walk you through the entire process.Seeing is believing, right?Related Post:Designing a Highly Scalable System on AWS (analisys.co)"}
{"title": "Minikube en Cloud9: Crea un ambiente de pruebas para Kubernetes", "published_at": 1716210211, "tags": ["kubernetes", "aws", "espa\u00f1ol", "contenedores"], "user": "Max Zeballos", "url": "https://dev.to/aws-builders/minikube-en-cloud9-crea-un-ambiente-de-pruebas-para-kubernetes-3ej8", "details": "Introducci\u00f3nCuando empiezas a aprender una nueva tecnolog\u00eda, es de mucha ayuda poder configurar ambientes de pruebas r\u00e1pidamente, esto te permite realizar tus pruebas de concepto de manera \u00e1gil.Kubernetes es la herramienta mas adoptada en la orquestaci\u00f3n de contenedores. Por otro lado Cloud9 es el IDE en linea de AWS. Nosotros podemos realizar r\u00e1pidamente nuestras pruebas de concepto de Kubernetes en cloud9 con Minikube.En este art\u00edculo te ense\u00f1are como levantar un ambiente de pruebas con Minikube y Cloud9 para poder desplegar tu primer pod en Kubernetes.Configuraci\u00f3n del ambiente1 Creaci\u00f3n del ambiente en Cloud9Vamos a ingresar a nuestra cuenta de AWS y nos vamos a dirigir al servicio de Cloud9 y le daremos click en \"Create Environment\" como se muestra en la siguiente imagenColocaremos la siguiente informaci\u00f3nName: Minikube-environmentEnvironment-type: New EC2 instanceInstance type: t3.smallPlatform: Ubuntu Server 22.04 LTSA continuaci\u00f3n le daremos click en \"Create\"En la lista de ambientes aparecera el ambiente creado por nosotros.Le damos click en open y se nos abrir\u00e1 nuestro ambiente en una nueva pesta\u00f1a en el navegador.2 Incrementar las capacidades de disco de nuestro ambienteVamos a ubicar la instancia EC2 donde esta nuestro ambiente de Cloud9, para ello buscamos la instancia que contiene el nombre de nuestro ambiente.Le damos check a la instancia, nos dirigimos a la pesta\u00f1a \"storage\" y le damos click donde en el id del volumenLuego, le damos click en \"Modify volume\"El valor actual debe ser 10 y lo cambiaremos por 30, luego le damos click en \"Modify\"Ahora regresamos a nuestro ambiente de Cloud9 y vamos a incrementar el tama\u00f1o del sistema de archivos con los siguientes comandossudo growpart /dev/nvme0n1 1 sudo resize2fs /dev/nvme0n1p1Enter fullscreen modeExit fullscreen modeAhora nuestro sistema de archivos pasara de 10G a 30G.3 Instalaci\u00f3n de las herramientas necesariasLanzaremos los siguientes comandos para instalar kubectlcurl -LO \"https://dl.k8s.io/release/$(curl -L -s https://dl.k8s.io/release/stable.txt)/bin/linux/amd64/kubectl\" sudo install -o root -g root -m 0755 kubectl /usr/local/bin/kubectl kubectl version --clientEnter fullscreen modeExit fullscreen modeLanzaremos los siguientes comandos para instalar Minikubecurl -LO https://storage.googleapis.com/minikube/releases/latest/minikube-linux-amd64 sudo install minikube-linux-amd64 /usr/local/bin/minikubeEnter fullscreen modeExit fullscreen modeLanzando nuestro primer podVamos a iniciar nuestro ambiente en Minikube con el siguiente comandominikube startEnter fullscreen modeExit fullscreen modeLanzaremos nuestro primer pod en minikubekubectl run my-first-pod --image=nginxEnter fullscreen modeExit fullscreen modeVamos a verificar que nuestro pod se encuentre \"running\"kubectl get podsEnter fullscreen modeExit fullscreen modeFinalmente podemos detener la ejecuci\u00f3n de nuestro ambiente en minikubeminikube stopEnter fullscreen modeExit fullscreen modeEliminando nuestro ambiente de cloud9Para eliminar el ambiente vamos al servicio de cloud9, seleccionamos nuestro ambiente y le damos click en \"Delete\"AclaracionesEl tama\u00f1o de instancia m\u00ednimo para correr minikube es t3.small, de colocar una instancia con menor cantidad de vCPUs tendr\u00e1s el siguiente errorEl tama\u00f1o de disco con el que se crea un ambiente de Cloud9 son 10 Gigas, si no incrementas el tama\u00f1o del disco tendr\u00e1s el siguiente errorLa instancia t3.small no pertenece a la capa gratuita de AWS. El precio en Oregon (us-west-2) es de $0.0208 la hora, es decir en 8 horas de pruebas habremos gastado la suma de $0.1664 ($0.0208 x 8h). Es un precio bastante bajo y razonable.En caso necesites hacer pruebas con muchos contenedores ejecut\u00e1ndose a la vez, te recomiendo lanzar una instancia con una mayor cantidad de vCPUs, que cuente con los recursos suficientes para lanzar todos los contenedores que necesites"}
{"title": "Amazon DevOps Guru for the Serverless applications - Part 10 Anomaly detection on Aurora Serverless v2", "published_at": 1716207468, "tags": ["aws", "serverless", "devops", "aiops"], "user": "Vadym Kazulkin", "url": "https://dev.to/aws-builders/amazon-devops-guru-for-the-serverless-applications-part-10-anomaly-detection-on-aurora-serverless-v2-3am2", "details": "IntroductionIn the course of the series about Amazon DevOps Guru for the Serverless applications we explored many different anomalies recognized by the DevOps Guru. For my other article series aboutData API for Amazon Aurora Serverless v2I experimented a lot with Aurora Serverless v2 and of course asked myself a question whether anomalies with Aurora (Serverless v2) PostgreSQL database will be recognized by Amazon DevOps Guru service. So, let's explore this. In this article we'll use AWS SDK for Java and focus on the \"standard\" way to connect Lambda function to the Aurora (Serverless v2) PostgreSQL which is JDBC (Java Database Connectivity). We won't use Amazon RDS Proxy in our example. The same will be true for other programming languages supported by Lambda as they all offer their only functionality for that. We'll explore the anomaly detection on Aurora Serverless v2 with Data API in one of the next articles.Anomaly detection on Aurora Serverless v2Let's look into oursample applicationand useSAM templateto create infrastructure and deploy the application which architecture is described on the following picture :The application creates products stored in the Aurora Serverless v2 PostgreSQL database and retrieves them by id using JDBC. The relevant Lambda function which we'll use to retrieve product by its id is GetProductByIdViaAuroraServerlessV2WithoutDataApi and its handler implementation isGetProductByIdViaAuroraServerlessV2WithoutDataApiHandler.DevOps Guru suggests enablingRDS Performance Insightsto gain additional insights about the anomaly which we did for AuroraServerlessV2Instance in theSAM template.AuroraServerlessV2Instance:     Type: 'AWS::RDS::DBInstance'     Properties:       Engine: aurora-postgresql       DBInstanceClass: db.serverless       DBClusterIdentifier: !Ref AuroraServerlessV2Cluster       ....       EnablePerformanceInsights: true       PerformanceInsightsRetentionPeriod: 7Enter fullscreen modeExit fullscreen modeAs in the previous article we usehey toolto perform the load test like thishey -z 15m -c 300 -H \"X-API-Key: XXXa6XXXX\" https://XXX.execute-api.eu-central-1.amazonaws.com/prod/productsWithoutDataApi/1Enter fullscreen modeExit fullscreen modeIn this example we invoke the API Gateway endpoint with 300 concurrent containers for 15 minutes. Behind the prod/productsWithoutDataApi endpoint Lambda function GetProductByIdViaAuroraServerlessV2WithoutDataApi will be invoked which retrieves the product by id 1 from the Aurora Serverless v2 PostgreSQL database.We configured in ourSAM templateAurora database cluster to scale from minimal capacity 0.5 to maximal capacity 1 ACU (which is very small database size) in case of the increased load for the cost saving purpose only.AuroraServerlessV2Cluster:     Type: 'AWS::RDS::DBCluster' ...       ServerlessV2ScalingConfiguration:         MinCapacity: 0.5         MaxCapacity: 1Enter fullscreen modeExit fullscreen modeAurora (Serverless v2) database manages the maximal number of the database connections available proportionally to the database size (in our case the ACU setting). For more information please read the documentation aboutMaximum connections for Aurora Serverless v2. So with the increased number of invocations we expect to reach the maximal number of the database connections available and high database (CPU) load soon, so that database won't be able to respond to the new Lambda function requests to retrieve product by id.  With that we will provoke the anomaly and would like to figure out whether DevOps Guru will be able to detect it. And it will! The following insight was generated:And the following aggregated anomalous metrics have been identified:For the Aurora Serverless v2 database these are: DatabaseConnection Sum, DBLoad Average and DBLoadCPU Average.Graphed anomalies look like this:And the RDS Performance Insights metrics for database load sliced by the SQL statement look like this:So we see that the SQL statements :select id, name, price from tbl_product where id=?Enter fullscreen modeExit fullscreen modewith colors purple and orange caused very high database load.ConclusionIn this article learned that DevOps Guru could successfully detect anomalies with Aurora (Serverless v2) PostgreSQL database in case of Lambda function with Java 21 managed runtime connected to it via JDBC. We created a very high load on the database by invoking Lambda function to retrieve product by id several hundred times concurrently for multiple minutes. By doing this Aurora (Serverless v2) PostgreSQL database auto-scaled from 0.5 to 1 ACU which was not enough to sustain such a load. We saw that DevOps Guru correctly pointed to the increased sum of database connections and constantly high database (CPU) load operational anomalies."}
{"title": "From Scratch: OIDC Providers", "published_at": 1716206400, "tags": ["beginners", "aws", "devops", "terraform"], "user": "Joseph", "url": "https://dev.to/aws-builders/from-scratch-oidc-providers-252d", "details": "It's time to divert our attention to what will soon become our infrastructure deployment process. This will feel a lot like application deployments, but there will be some stark differences we'll be going over. Our applications will incorporate infrastructure in the future, but not everything we do with our IaC will be related to an application.Github ActionsI'll be deploying my infrastructure through Github Actions because it's where I keep my repositories anyway, and keeping the deployment scripts there is really convenient and makes tracking things really easy too.The Github and AWS DanceIf we're going to be deploying infra from within github actions, it stands to reason we'll have to give our workflows permission to deploy infrastructure in our AWS account. That's where OIDC comes in. AWS allows you to build a trust relationship with an OpenID connected provider (in our case github), and then define a permissions policy to a role in your account. Once the role is defined, you then have complete control over what permissions you let that role have within your AWS account.Github has somedocumentationon how the whole process works.Creating the ProviderWe'll start off by essentially followingAWS guidance on connecting with Github. In order to do that, we're going to need to grab a certificate from github to put into our provider definition.CertificateGrab the certificate from github by usingopenssl s_client -servername token.actions.githubusercontent.com -showcerts -connect token.actions.githubusercontent.com:443The cert you want is the last one you see in the terminal. You want to create a file namedcertificate.crtand put all the cert information from the terminal which includes the-----BEGIN CERTIFICATE-----and-----END CERTIFICATE-----parts too.ThumbprintNow with the certificate, we'll need to run through a process tograb the thumbprint. Open up a terminal in whatever path you made put the cert, and run the following command:openssl x509 -in certificate.crt -fingerprint -sha1 -nooutThe output should look something like this:SHA1 Fingerprint=99:0F:41:93:97:2F:2B:EC:F1:2D:DE:DA:52:37:F9:C9:52:F2:0D:9EYou only care about the alphanumerics in the fingerprint itself, so remove the colons and you're left with something like this:990F4193972F2BECF12DDEDA5237F9C952F20D9EDepending on where your provider is, you're fingerprint will be different.Provider DefinitionNow we're finally ready to script the provider itself.constoidcProvider=newIamOpenidConnectProvider(scope,`${name}-provider`,{url:\"https://token.actions.githubusercontent.com\",clientIdList:[\"sts.amazonaws.com\"],thumbprintList:[\"1b511abead59c6ce207077c0bf0e0043b1382612\"],});Enter fullscreen modeExit fullscreen modeIn the thumbprint list you'll put the fingerprint you got from earlier. You need to make sure your thumbprint is in all lowercase when you do this, because if you don't then terraform will always detect drift from the AWS provider that is provisioned since AWS will tell it it's in all lowercase.Trust RelationshipAfter we've defined the provider we have to describe the trust relationship between it and our AWS Account. This basicaly boils down to defining a policy that allows role assumption with a web identity.The policy looks like this:constassumePolicy=newDataAwsIamPolicyDocument(scope,`${name}-assume-policy`,{statement:[{actions:[\"sts:AssumeRoleWithWebIdentity\"],principals:[{type:\"Federated\",identifiers:[oidcProvider.arn],},],condition:[{test:\"StringEquals\",variable:\"token.actions.githubusercontent.com:aud\",values:[\"sts.amazonaws.com\"],},{test:\"StringLike\",variable:\"token.actions.githubusercontent.com:sub\",values:[`${config.githubOrg}/*`],},],},],});Enter fullscreen modeExit fullscreen modeThe provider ARN is the provider we made earlier, and the value intoken.actions.githubusercontent.com:subis your github org you want to allow to provision in your AWS account. You can limit this in various ways, but in my case I just want to give the whole org the same set of permissions. Alternatively you can limit this down to specific repos, or github envs, etc etc.PermissionsNow that we have the provider scripted and the trust relationship defined we can turn our attention to what permissions we want to give the role. To keep things simple, I'm going to give the deployment processPowerUserAccess. This is a managed policy from Amazon, and I have no security concerns around the permissions it allows. If you work in a space where the security concerns need to be more limiting, this is where you could make a more restrictive policy yourself, which is also fine.For this situation, I just need to grab the power user policy like this:constpowerUserPolicy=newDataAwsIamPolicy(scope,`${name}-trust-policy`,{name:\"PowerUserAccess\",});Enter fullscreen modeExit fullscreen modeRoleNone of this matters if we don't create a role that is going to use all this stuff:constrole=newIamRole(scope,`${name}-trust-role`,{namePrefix:`${name}-trust-role`,assumeRolePolicy:assumePolicy.json,});Enter fullscreen modeExit fullscreen modeThe assume role policy here is the trust relationship we defined. Now we need to give the role power user permissions.newIamRolePolicyAttachment(scope,`${name}-rpa`,{role:role.name,policyArn:powerUserPolicy.arn,});Enter fullscreen modeExit fullscreen modeTry is OutAfter we'vecdktf deploy '*'the stack we should try this out and see if it's working. I made a really simple workflow that just configures the aws cli and then prints out all the roles in the account to make sure it has the permissions we gave it earlier and everything is wired up properly.The workflow file looks like this:name:Deploy Infrastructureon:pushpermissions:id-token:writecontents:readjobs:Deploy:runs-on:ubuntu-lateststeps:-name:Git clone the repositoryuses:actions/checkout@v4-name:configure aws credentialsuses:aws-actions/configure-aws-credentials@v4with:role-to-assume:[arn:aws:iam::therolearn-i-just-made]role-session-name:githubsessionaws-region:us-east-2-name:Proof of Liferun:|aws iam list-rolesEnter fullscreen modeExit fullscreen modeAfter that it's just a matter of cleaning things up a little bit and push main up to github, which triggers the workflow to run, and I'm able to see the github action produce a list of the roles in my account.Next StepsWe're now ready to start letting Github Actions handle our infrastructure deployments. That's going to involve changing a few things we already did and then making some more workflows in our repo, but that's for next time."}
{"title": "Automating Snowflake Resource Deployment using Terraform and GitHub Actions", "published_at": 1716037071, "tags": ["snowflake", "terraform", "githubactions", "devops"], "user": "Chandrashekar Y M", "url": "https://dev.to/aws-builders/automating-snowflake-resource-deployment-using-terraform-and-github-actions-2blj", "details": "Lately at work, I have been using Terraform for our Infrastructure as Code (IaC) requirements for AWS workloads. As part of this learning journey, I also acquiredTerraform Associate certification.I wanted to explore Terraform for non-AWS use cases. At work, we are building a unified data platform for our data needs using Snowflake. So, I thought I will try to automate Snowflake resource deployments using Terraform.Snowflakeis defined as a cloud native, data platform offered as a SaaS. Lately, in the ever-evolving world of data platforms, Snowflake has emerged as a leading cloud-based data warehousing solution.Manual provisioning of Snowflake resources like Databases, Schemas, Tables, Grants, Warehouses etc, is time consuming and prone to errors. This is where Infrastructure as Code (IaC) tools like Terraform and CI/CD pipelines using GitHub Actions makes life easier.Terraformis an open-source, cloud agnostic IaC tool that allows us to define and provision cloud infrastructure using a very high level configuration language. Terraform implements this using plugins calledproviders.GitHub Actionsenables us to create efficient CI/CD pipelines based on code in GitHub repository.This blog post demonstrates a step-by-step guide on how to deploy resources to Snowflake using Terraform and GitHub Actions, leveraging our repositorycicd-with-terraform-for-snowflake. We will deploy a Database, a Schema, Grants and a Table onto two different environments (DEV and PROD) on the a Snowflake instance. We will use release based deployment pipelines to deploy to PROD environment.Some pre-requisites and assumptions:An AWS Account with an S3 bucket and DynamoDB table already provisioned - we will be using these for Terraform remote backend and State locking.AWS credentials (Access Key and Secret Access Key) for the above AWS account configured as GitHub repository secrets.A Snowflake instance, a user withACCOUNTADMINpermissions and related Key-pair authentication setup. Related private key configured as GitHub repository secret.A Snowflake role TF_READER pre-created in the Snowflake instance. We will be deploying grants for this role using Terraform resources.Setting up repository:Clone the repository to your local machine:git clone https://github.com/shekar-ym/cicd-with-terraform-for-snowflake.git cd cicd-with-terraform-for-snowflakeEnter fullscreen modeExit fullscreen modeRepository Structure:.github/workflows/: Contains the GitHub Actions workflow files that automate the deployment process.devandprodfolders contain the Terraform files fordevelopmentandproductionenvironment respectively.modulefolder contains the Terraform module definition which will be used for provisioning Snowflake resources like Database, Schema and Tables.Terrform Modules:Modules are containers for multiple resources that are used together. Modules are used to package and reuse resource configurations with Terraform.In our case, we will be using modules to define Snowflake database, schema, grants, table and warehouse resources configuration. This module will be reused to create resource for development and production environments.For example, below is the module resource configuration for database and schema:resource \"snowflake_database\" \"tf_database\" {   name                        = var.database   comment                     = \"Database for ${var.env_name}\"   data_retention_time_in_days = var.time_travel_in_days  }  resource \"snowflake_schema\" \"tf_schema\" {   name     = var.schema   database = snowflake_database.tf_database.name   comment  = \"Schema for ${var.env_name}\" }Enter fullscreen modeExit fullscreen modeRefer to the Githubrepositoryfor other module resource configurations.GitHub Actions Workflow:The workflow will trigger a deployment toDEVenvironment when you merge any code changes tomainbranch using a pull request.The workflow also includes a step for infrastructure code scan to scan Terraform code. This usesCheckovaction against infrastructure-as-code, open source packages, container images, and CI/CD configurations to identify misconfigurations, vulnerabilities, and license compliance issues.security-scan-terraform-code:         name: Security scan (terraform code)         runs-on: ubuntu-latest         steps:           - name: Checkout repo             uses: actions/checkout@v4            - name: Run Checkov action             id: checkov             uses: bridgecrewio/checkov-action@master             with:               directory: .               soft_fail: true               download_external_modules: true               framework: terraformEnter fullscreen modeExit fullscreen modeThere is a preview step, when you create a pull request - this preview step performs aterraform planto give you an overview what resources will be deployed or changed.When you create arelease/*branch from main branch, this triggers a deployment toPRODenvironment.Deploying the resources to DEV:Let us make some changes to the Terraform code, push the changes to GitHub repo and create a pull request (PR). Below is how the deployment pipeline looks:And below are the steps performed as part of preview:Let us merge our pull request tomainbranch.Here is the output ofterraform applystep:Let us verify the resources on Snowflake. As you can see, the deployment pipeline created a database(TASTY_BYTES_DEV) and schema(RAW_POS) and a table (MENU)A new warehouse was also provisioned.Deploying the resources to PROD:Let us create a release branch from the main branch. This will trigger a deployment toPRODenvironment.As mentioned earlier, there will be an preview step which performs aterraform planto give you an overview what resources will be deployed or changed.Since, I have configured environment protection rules, the pipeline stops for a manual approval, before triggering a deploy to PROD.Approving this will trigger a deploy to PROD.Here is the output of terraform apply step (for PROD):Completed pipeline:Let us verify the resources on Snowflake forPRODenvironment. As you can see, the deployment pipeline created a database(TASTY_BYTES_PROD) and schema(RAW_POS) and a table (MENU)A new warehouse forPRODwas also provisioned.Conclusion:Automating the deployment of Snowflake resources using Terraform and GitHub Actions streamlines the process, reduces the potential for errors, and ensures that infrastructure is managed consistently. This setup not only saves time but also enhances the reliability and reproducibility of deployments. By following the steps outlined in this guide, you can leverage the power of IaC and CI/CD to manage your Snowflake infrastructure efficiently.Thanks for reading. Please let me know your feedback in comments section."}
{"title": "How to build a single-page application deployment using AWS CDK", "published_at": 1716032160, "tags": ["webdev", "aws", "devops", "javascript"], "user": "Peter McAree", "url": "https://dev.to/aws-builders/how-to-build-a-single-page-application-deployment-using-aws-cdk-3fcn", "details": "All modern applications will require some form of static assets to actually power the functionality for their users, whether it\u2019s media content, supporting scripts or the application content itself.A super common pattern that I always call upon when deploying single-page applications (SPA) is deploying my static assets into S3 and serving them through CloudFront.This is incredibly useful when working not only with single-page applications, but also for static assets that need to be served to ensure your applications work as expected.Let\u2019s take a look at how we can define the infrastructure resources to facilitate serving these assets, using my old favourite AWS CDK.\ud83d\udd0d ArchitectureTo give you an idea of what we\u2019re wishing to achieve with this, here is a high-level architecture diagram of what we want to build:\u270d\ufe0f Let's write some CDKCrafting the CDK for this pattern is extremely straightforward. There are only the two components:S3 BucketCloudFront DistributionTo provision the S3 bucket, you can use the L2 construct that already exists within CDK:consts3Bucket=newcdk.aws_s3.Bucket(this,\"s3-bucket-content\",{blockPublicAccess:cdk.aws_s3.BlockPublicAccess.BLOCK_ALL,encryption:cdk.aws_s3.BucketEncryption.S3_MANAGED,enforceSSL:true,removalPolicy:cdk.RemovalPolicy.RETAIN,bucketName:\"cdk-spa-assets-pattern\",});Enter fullscreen modeExit fullscreen modeWith the above, we're just creating a S3 bucket that has public access denied and encryption at rest enabled.We're then going to create our CloudFront distribution and update the bucket to allow the distribution to read from it:constoriginAccessIdentity=newcdk.aws_cloudfront.OriginAccessIdentity(this,\"cloudfront-origin-access-identity\");s3Bucket.grantRead(originAccessIdentity);constcloudfrontS3Origin=newcdk.aws_cloudfront_origins.S3Origin(s3Bucket,{originAccessIdentity:originAccessIdentity,});constdistribution=newcdk.aws_cloudfront.Distribution(this,\"cloudfront-content-distribution\",{defaultBehavior:{origin:cloudfrontS3Origin,},defaultRootObject:\"index.html\",priceClass:cdk.aws_cloudfront.PriceClass.PRICE_CLASS_100,});Enter fullscreen modeExit fullscreen modeFirstly, we're creating theOriginAccessIdentityconstruct, this is then used to grant read access to the S3 bucket before assigning it to the CloudFront distribution itself.We're specifically using theDistributionconstruct here as it's the newer implementation (replacing theCloudFrontWebDistributionconstruct). As part of that, the API has changed slightly and we need to define a CloudFront origin itself - we are using S3 as our origin and can pass our origin identity permission into it to allow the distribution to read from the bucket.We assign that as our default behaviour (as a CloudFront distribution can have multiple behaviours), and finally define our price class.And that's it! Deploy those and you'll have a S3 bucket that you can push your build assets into. By default, the entry point for the CloudFront distribution isindex.html- so whenever you build your SPA assets, you simply push them into the bucket and you will be good to go!\ud83d\udcb0 Cost efficiencyLow cost because all of the assets are stored in S3 and served via CloudFront, so your costs will scale with the requests and the traffic coming inbound. Since CloudFront heavily caches at edge, the requests downstream to your S3 bucket will be fairly low - unless you explicitly invalidate the cache if you have new objects.ConclusionStatic assets can be pushed into S3 bucket and served via the global CloudFront CDN network.Extremely cost-effective for working with a lot of network requests.Can work for SPA static websites or other general static content that might be required to power your applications."}
{"title": "Synthetic Monitoring with Grafana Cloud", "published_at": 1716022083, "tags": ["aws", "monitoring", "http", "javascript"], "user": "Shakir", "url": "https://dev.to/aws-builders/synthetic-monitoring-with-grafana-cloud-1n5b", "details": "\ud83d\udd50Time for some synthetic monitoring fun with Grafana...\ud83d\udcca We'd deploy a simple function on AWS Lambda that generates some data. We then expose this function with a public URL which would then be probed with a synthetic test from Grafana cloud servers.AWS LambdaCreate a new AWS lambda function with runtime as nodejs and a name such assynthetic-monitoring-demo. Put the following code inindex.js, which returns data similar to what's returned by this Grafana providedendpoint. We just create a lambda function here so that we could test as required and delete it when done.const randomInt = (min, max) => Math.floor(Math.random() * (max - min + 1)) + min const randomChoice = (arr) => arr[randomInt(0, arr.length - 1)]  const generateCrocodileData = () => {     const names = [         \"Smiley\", \"Snappy\", \"Bitey\", \"Chompy\", \"Jaws\", \"Scaly\", \"Cruncher\",          \"Munch\", \"Gator\", \"Nibbles\", \"Swampy\", \"Toothless\", \"Grin\", \"Sly\",          \"Snapper\", \"Croc\", \"Dagger\", \"Ripjaw\", \"Spike\", \"Thrasher\"     ]     const species = [         \"Saltwater Crocodile\", \"Nile Crocodile\", \"Mugger Crocodile\",          \"American Crocodile\", \"Freshwater Crocodile\", \"Morelet's Crocodile\",          \"Cuban Crocodile\", \"Siamese Crocodile\", \"Philippine Crocodile\",          \"Orinoco Crocodile\", \"Dwarf Crocodile\", \"Slender-snouted Crocodile\"     ]      return {         name: randomChoice(names),         species: randomChoice(species),         age: randomInt(1, 70),         sex: randomChoice([\"M\", \"F\"]),     } }  export const handler = async () => {     const numRecords = 8     const crocodiles = Array.from({ length: numRecords }, generateCrocodileData)     return {         statusCode: 200,         body: JSON.stringify(crocodiles),     } }Enter fullscreen modeExit fullscreen modeThe crocodile species names were taken via generative AI, so not quite sure if it's correct\ud83d\ude09.Deploy the function, and then create a function URL in the configuration section with auth type as none, so that it could be accessed publicly\ud83c\udf0d.Try accessing the function URL.K6The synthetic monitoring in Grafana is based on k6. So we shall try some k6 basics on the local machine first.Installk6.brew install k6Enter fullscreen modeExit fullscreen modeThe commandk6 new test.jsshould create a file with nametest.jswith some references in it. However, for our case we can add a filetest.jsfrom scratch. Let's first import the functions/modules we need.import { check } from 'k6' import http from 'k6/http'Enter fullscreen modeExit fullscreen modeWe just need to create a function that's exported by default in this code inside which we can try calling the api with http modules's get() function. Let's use the function URL we generated as endpoint for this purpose and try printing the status, response body as an array and length of the returned response array.export default function main() {   const res = http.get('https://ieaivrjwpcdbuxh4tws5autdoa0sflqi.lambda-url.us-east-1.on.aws/')   console.log('The response status is', res.status)   console.log('The response array is', res.json())   console.log('The length of the response array is', res.json().length) }Enter fullscreen modeExit fullscreen modeWe could now this run this code with k6 and see the output as well as some test results.$ k6 run test.js            /\\      |\u203e\u203e| /\u203e\u203e/   /\u203e\u203e/         /\\  /  \\     |  |/  /   /  /         /  \\/    \\    |     (   /   \u203e\u203e\\      /          \\   |  |\\  \\ |  (\u203e)  |    / __________ \\  |__| \\__\\ \\_____/ .io       execution: local         script: test.js         output: -       scenarios: (100.00%) 1 scenario, 1 max VUs, 10m30s max duration (incl. graceful stop):               * default: 1 iterations for each of 1 VUs (maxDuration: 10m0s, gracefulStop: 30s)  INFO[0001] The response status is 200                    source=console INFO[0001] The response array is [{\"name\":\"Chompy\",\"species\":\"Mugger Crocodile\",\"age\":41,\"sex\":\"M\"},{\"name\":\"Snappy\",\"species\":\"Orinoco Crocodile\",\"age\":24,\"sex\":\"M\"},{\"name\":\"Spike\",\"species\":\"Mugger Crocodile\",\"age\":64,\"sex\":\"M\"},{\"species\":\"Siamese Crocodile\",\"age\":40,\"sex\":\"M\",\"name\":\"Gator\"},{\"species\":\"Freshwater Crocodile\",\"age\":44,\"sex\":\"F\",\"name\":\"Snapper\"},{\"sex\":\"F\",\"name\":\"Sly\",\"species\":\"Slender-snouted Crocodile\",\"age\":11},{\"age\":24,\"sex\":\"M\",\"name\":\"Ripjaw\",\"species\":\"Mugger Crocodile\"},{\"name\":\"Thrasher\",\"species\":\"Orinoco Crocodile\",\"age\":67,\"sex\":\"M\"}]  source=console INFO[0001] The length of the response array is 8         source=console       data_received..................: 6.5 kB 7.3 kB/s      data_sent......................: 542 B  606 B/s      http_req_blocked...............: avg=533.74ms min=533.74ms med=533.74ms max=533.74ms p(90)=533.74ms p(95)=533.74ms      http_req_connecting............: avg=246.67ms min=246.67ms med=246.67ms max=246.67ms p(90)=246.67ms p(95)=246.67ms      http_req_duration..............: avg=356.86ms min=356.86ms med=356.86ms max=356.86ms p(90)=356.86ms p(95)=356.86ms        { expected_response:true }...: avg=356.86ms min=356.86ms med=356.86ms max=356.86ms p(90)=356.86ms p(95)=356.86ms      http_req_failed................: 0.00%  \u2713 0        \u2717 1      http_req_receiving.............: avg=357\u00b5s    min=357\u00b5s    med=357\u00b5s    max=357\u00b5s    p(90)=357\u00b5s    p(95)=357\u00b5s         http_req_sending...............: avg=297\u00b5s    min=297\u00b5s    med=297\u00b5s    max=297\u00b5s    p(90)=297\u00b5s    p(95)=297\u00b5s         http_req_tls_handshaking.......: avg=285.27ms min=285.27ms med=285.27ms max=285.27ms p(90)=285.27ms p(95)=285.27ms      http_req_waiting...............: avg=356.21ms min=356.21ms med=356.21ms max=356.21ms p(90)=356.21ms p(95)=356.21ms      http_reqs......................: 1      1.117563/s      iteration_duration.............: avg=894.17ms min=894.17ms med=894.17ms max=894.17ms p(90)=894.17ms p(95)=894.17ms      iterations.....................: 1      1.117563/s   running (00m00.9s), 0/1 VUs, 1 complete and 0 interrupted iterations default \u2713 [======================================] 1 VUs  00m00.9s/10m0s  1/1 iters, 1 per VUEnter fullscreen modeExit fullscreen modeSo based on the output above, the api is returning 8 crocodiles with it's name, gender and other details. The test results give us some metrics such as data recd., sent and other http metrics. Note that we haven't added any checks so far, though we have imported the check function.For this usecase let's do a few checks such as:Check the response status to be 200Is it sending back 8 crocodile detailsIs it giving back the details of 4 male and 4 crocodiles, meaning is the response distrubuted equally based on genderThere should be atleast one crocodile under 15 years of ageThe code for these checks should look like below:check(res, {     'is status 200': (r) => r.status === 200,     'is response length 8': (r) => r.json().length === 8,     'is response distributed equally based on gender': (r) => {       let maleCount = 0       let femaleCount = 0       r.json().forEach(crocodile => {         if (crocodile.sex === 'M') {           maleCount++         } else if (crocodile.sex === 'F') {           femaleCount++         }       })       return (maleCount === femaleCount)     },     'is any crocodile under 15 years of age': (r) => {       const isAnyUnder15 = r.json().some(crocodile => crocodile.age < 15)       return isAnyUnder15     }   })Enter fullscreen modeExit fullscreen modePlease keep the checks block inside the main function after which we can run the test again.% k6 run test.js ---TRUNCATED---      \u2713 is status 200      \u2713 is response length 8      \u2717 is response distributed equally based on gender       \u21b3  0% \u2014 \u2713 0 / \u2717 1      \u2717 is any crocodile under 15 years of age       \u21b3  0% \u2014 \u2713 0 / \u2717 1 ---TRUNCATED---Enter fullscreen modeExit fullscreen modeSo, couple of our checks are failing.All good\ud83d\udc4d, so we got some grip on how to run k6 and add some checks. We can try doing the same stuff on grafana.GrafanaGo toGrafana Cloud > Home > Testing & synthetics > Add a new check > Scripted check. I have defined the following options in the form.Job name: test-job Instance: test-instance Probe locations: Seoul(I have chosen only one)Enter fullscreen modeExit fullscreen modeIn the script from, just put the same script we used before, and test/save it.Note that by default the logs and metrics related to our test would go to the loki and mimir endpoints of Grafana cloud.The out of box dashboard for our check should appear in sometime. The assertions panel which gives a stat and timeseries graph(expanded view) for the four checks.As a bonus let's go toHome > Dashbooards > New Dashboardand try creating a custom dashboard panel with mimir(prometheus type datasource).Here is the panel json for this.{   \"datasource\": {     \"uid\": \"grafanacloud-prom\",     \"type\": \"prometheus\"   },   \"type\": \"stat\",   \"title\": \"Current probe checks status\",   \"gridPos\": {     \"x\": 0,     \"y\": 8,     \"w\": 12,     \"h\": 8   },   \"id\": 1,   \"targets\": [     {       \"datasource\": {         \"type\": \"prometheus\",         \"uid\": \"grafanacloud-prom\"       },       \"refId\": \"A\",       \"expr\": \"sum(probe_checks_total{job=\\\"test-job\\\"}) by(result)\",       \"range\": false,       \"instant\": true,       \"editorMode\": \"code\",       \"legendFormat\": \"__auto\",       \"useBackend\": false,       \"disableTextWrap\": false,       \"fullMetaSearch\": false,       \"includeNullMetadata\": true,       \"format\": \"time_series\",       \"exemplar\": false     }   ],   \"options\": {     \"reduceOptions\": {       \"values\": true,       \"calcs\": [         \"lastNotNull\"       ],       \"fields\": \"\"     },     \"orientation\": \"auto\",     \"textMode\": \"auto\",     \"wideLayout\": true,     \"colorMode\": \"value\",     \"graphMode\": \"area\",     \"justifyMode\": \"auto\",     \"showPercentChange\": false   },   \"transformations\": [     {       \"id\": \"calculateField\",       \"options\": {         \"mode\": \"binary\",         \"reduce\": {           \"reducer\": \"sum\"         },         \"binary\": {           \"left\": \"fail\",           \"right\": \"pass\"         },         \"alias\": \"total\"       }     },     {       \"id\": \"calculateField\",       \"options\": {         \"mode\": \"binary\",         \"binary\": {           \"left\": \"pass\",           \"operator\": \"/\",           \"right\": \"total\"         },         \"alias\": \"success ratio\"       }     }   ],   \"fieldConfig\": {     \"defaults\": {       \"mappings\": [],       \"thresholds\": {         \"mode\": \"absolute\",         \"steps\": [           {             \"value\": null,             \"color\": \"green\"           },           {             \"value\": 80,             \"color\": \"red\"           }         ]       },       \"color\": {         \"mode\": \"thresholds\"       }     },     \"overrides\": [       {         \"matcher\": {           \"id\": \"byName\",           \"options\": \"fail\"         },         \"properties\": [           {             \"id\": \"thresholds\",             \"value\": {               \"mode\": \"absolute\",               \"steps\": [                 {                   \"color\": \"green\",                   \"value\": null                 },                 {                   \"color\": \"red\",                   \"value\": 1                 }               ]             }           }         ]       },       {         \"matcher\": {           \"id\": \"byName\",           \"options\": \"success ratio\"         },         \"properties\": [           {             \"id\": \"unit\",             \"value\": \"percentunit\"           },           {             \"id\": \"thresholds\",             \"value\": {               \"mode\": \"absolute\",               \"steps\": [                 {                   \"color\": \"red\",                   \"value\": null                 },                 {                   \"color\": \"yellow\",                   \"value\": 0.8                 },                 {                   \"value\": 0.9,                   \"color\": \"green\"                 }               ]             }           }         ]       },       {         \"matcher\": {           \"id\": \"byName\",           \"options\": \"total\"         },         \"properties\": [           {             \"id\": \"color\",             \"value\": {               \"mode\": \"shades\",               \"fixedColor\": \"purple\"             }           }         ]       }     ]   },   \"pluginVersion\": \"11.1.0-70724\" }Enter fullscreen modeExit fullscreen modeAnd one with loki for logs.Cool, we came to the end of this post\ud83d\udc4f, here we tried synthetic testing first with k6 and used the same script with Grafana cloud and visualized the data with couple of panels. As a housekeeping reminder, you may delete the check that's created in Grafana, so that it stops polling the endpoint, and then may delete the function in AWS Lambda. Thanks for reading!!!"}
{"title": "Streamline SSO Access to AWS Redshift Query Editor with Okta and Terraform", "published_at": 1716021686, "tags": ["aws", "queryeditor", "okta", "redshift"], "user": "Thulasiraj Komminar", "url": "https://dev.to/aws-builders/streamline-sso-access-to-aws-redshift-query-editor-with-okta-and-terraform-5fj7", "details": "IntroductionAWS Redshift Query Editor v2is a web-based tool that allows analysts to securely explore, share, and collaborate on data using SQL within a common notebook interface. It simplifies querying data with SQL and visualizing results with charts and graphs in just a few clicks.Integrating Redshift Query Editor v2 with your identity provider (IdP) automatically redirects users to the Query Editor v2 console instead of the Amazon Redshift console. This setup enables seamless access to Amazon Redshift clusters via federated credentials, eliminating the need to manage individual database users and passwords.In this blog post, we\u2019ll focus on usingOktaas the IdP and guide you through configuring your Okta application and AWS IAM permissions. We\u2019ll also demonstrate how to restrict user access to only the Query Editor v2, preventing them from performing administrative functions on the AWS Management Console.PrerequisitesBefore you begin, ensure you have the following prerequisites in place:An Okta Account: You need an active Okta account to serve as your identity provider.A Redshift Cluster: Ensure you have an existing Amazon Redshift cluster set up.Configured Query Editor v2: Make sure Redshift Query Editor v2 is configured. For more details, refer toConfiguring your AWS account.Create IAM Roles and PermissionsTo enable Okta to access Amazon Redshift Query Editor v2, you need to create two IAM roles. The first role will be used by Okta to access the Redshift Query Editor, and the second role will establish a trust relationship between the IdP (Okta) and AWS. We\u2019ll start by creating the role that Okta uses to access Amazon Redshift Query Editor v2. After setting up the Okta application, we\u2019ll create the trust relationship role using the metadata from the Okta app.data\"aws_caller_identity\"\"current\"{}data\"aws_region\"\"current\"{}data\"aws_iam_policy_document\"\"assume_policy\"{statement{effect=\"Allow\"actions=[\"sts:AssumeRoleWithSAML\",\"sts:TagSession\",]principals{type=\"Federated\"identifiers=[\"arn:aws:iam::${data.aws_caller_identity.current.account_id}:saml-provider/OktaRedshiftFederation\"]}condition{test=\"StringEquals\"variable=\"SAML:aud\"values=[\"https://signin.aws.amazon.com/saml\"]}}}data\"aws_iam_policy_document\"\"default\"{statement{actions=[\"redshift:GetClusterCredentials\",\"redshift:CreateClusterUser\",\"redshift:JoinGroup\",]resources=[\"arn:aws:redshift:${data.aws_region.current.name}:${data.aws_caller_identity.current.account_id}:cluster:${local.redshift_cluster_id}\",\"arn:aws:redshift:${data.aws_region.current.name}:${data.aws_caller_identity.current.account_id}:dbuser:${local.redshift_cluster_id}/*\",\"arn:aws:redshift:${data.aws_region.current.name}:${data.aws_caller_identity.current.account_id}:dbgroup:${local.redshift_cluster_id}/admin\",\"arn:aws:redshift:${data.aws_region.current.name}:${data.aws_caller_identity.current.account_id}:dbgroup:${local.redshift_cluster_id}/datascientist\",\"arn:aws:redshift:${data.aws_region.current.name}:${data.aws_caller_identity.current.account_id}:dbname:${local.redshift_cluster_id}/*\",]}}resource\"aws_iam_role\"\"default\"{name=\"RedshiftQueryEditorFederation\"assume_role_policy=data.aws_iam_policy_document.assume_policy.jsontags={environment=\"dev\"stack=\"redshift\"}}resource\"aws_iam_role_policy\"\"default\"{name=\"RedshiftQueryEditorFederation\"role=aws_iam_role.default.idpolicy=data.aws_iam_policy_document.default.json}resource\"aws_iam_role_policy_attachment\"\"default\"{role=aws_iam_role.default.namepolicy_arn=\"arn:aws:iam::aws:policy/AmazonRedshiftQueryEditorV2ReadSharing\"}Enter fullscreen modeExit fullscreen modeSet Up Okta ApplicationWith the IAM role for Okta access created, configure the Okta SAML application and assign it to the necessary Okta groups.data\"okta_group\"\"admin\"{name=\"admin\"}data\"okta_group\"\"datascientist\"{name=\"datascientist\"}resource\"okta_app_saml\"\"redshift\"{label=\"Redshift Query Editor v2\"logo=\"${path.module}/images/redshift.png\"preconfigured_app=\"amazon_aws\"saml_version=\"2.0\"default_relay_state=\"https://${data.aws_region.current.name}.console.aws.amazon.com/sqlworkbench/home\"app_settings_json=jsonencode({\"awsEnvironmentType\":\"aws.amazon\",\"appFilter\":\"okta\",\"groupFilter\":\"(?{{role}}[a-zA-Z0-9+=,.@\\\\-_]+)\",\"joinAllRoles\":false,\"loginURL\":\"https://console.aws.amazon.com/ec2/home\",\"roleValuePattern\":\"arn:aws:iam::${data.aws_caller_identity.current.account_id}:saml-provider/OktaRedshiftFederation,${aws_iam_role.default.arn}\",\"sessionDuration\":3600,\"useGroupMapping\":true,})attribute_statements{type=\"EXPRESSION\"name=\"https://aws.amazon.com/SAML/Attributes/PrincipalTag:RedshiftDbUser\"namespace=\"urn:oasis:names:tc:SAML:2.0:attrname-format:unspecified\"values=[\"user.username\"]}attribute_statements{type=\"EXPRESSION\"name=\"https://aws.amazon.com/SAML/Attributes/PrincipalTag:RedshiftDbGroups\"namespace=\"urn:oasis:names:tc:SAML:2.0:attrname-format:unspecified\"values=[\"String.join(\\\":\\\", isMemberOfGroupName(\\\"datascientist\\\") ? 'datascientist' : '',isMemberOfGroupName(\\\"admin\\\") ? 'admin' : '')\"]}}resource\"okta_app_group_assignments\"\"redshift\"{app_id=okta_app_saml.redshift.idgroup{id=data.okta_group.datascientist.id}group{id=data.okta_group.admin.id}}Enter fullscreen modeExit fullscreen modeCreate IAM SAML Provider RoleAfter setting up the Okta application, create the IAM SAML provider to establish a trust relationship between Okta and AWS using the Okta metadata.resource\"aws_iam_saml_provider\"\"redshift_okta\"{name=\"OktaRedshiftFederation\"saml_metadata_document=okta_app_saml.redshift.metadatatags={environment=\"dev\"stack=\"redshift\"}}Enter fullscreen modeExit fullscreen modeConclusionIn this blog post, we demonstrated how to securely federate SSO access to AWS Redshift Query Editor v2 using Okta as the identity provider, leveraging Terraform for seamless infrastructure management. By creating the necessary IAM roles and configuring the Okta SAML application, we established a robust trust relationship between Okta and AWS. This setup not only simplifies user access to Redshift Query Editor v2 but also enhances security by eliminating the need to share database users credentials. With this integration, your teams can efficiently explore, share, and collaborate on data, driving insightful decisions and streamlined operations."}
{"title": "Enabling Cross-Account Access for AWS Lake Formation with Data Filters Using Terraform", "published_at": 1715978066, "tags": ["awslakeformation", "terraform", "aws", "awscrossaccount"], "user": "Thulasiraj Komminar", "url": "https://dev.to/aws-builders/enabling-cross-account-access-for-lake-formation-with-data-filters-using-terraform-38d1", "details": "IntroductionIn myprevious blog, we explored enabling cross-account data sharing using AWS Lake Formation with Terraform. In this post, we\u2019ll dive deeper into enhancing that setup with data filters. Lake Formation data filtering allows for column-level, row-level, and cell-level security. This blog will focus specifically on implementing cell-level security to fine-tune data access controls.Designed by FreepikData filter levelsColumn-levelGranting permissions on a Data Catalog table with column-level filtering allows users to view only specific columns and nested columns they have access to. This enables defining security policies that grant access to partial sub-structures of nested columns.Row-levelGranting permissions on a Data Catalog table with row-level filtering allows users to view only specific rows of data they have access to. Filtering is based on the values of one or more columns, and nested column structures can be included in row-filter expressions.Cell-levelCell-level security combines both row and column filtering, providing a highly flexible permissions model.Creating a Data Filter in the Source AccountAssuming you have already followed the Lake Formation setup in Source Account as detailed in myprevious blog, we can now proceed with creating the data filter. Let\u2019s use an example involving IIoT measurements. Suppose you have equipment spread across multiple sites and need to grant specific IAM roles access to particular sites and columns. Here\u2019s how to achieve this using Terraform:In this example:Define Local Configuration: Thefilter_configvariable lists the sites, columns, and IAM roles in the target account that need access.Retrieve AWS Account ID: Theaws_caller_identitydata source fetches the current AWS account ID.Create Data Cell Filters: Theaws_lakeformation_data_cells_filterresource iterates over thefilter_configto create the necessary filters for each IAM role.This setup ensures that specific IAM roles have access only to the defined sites and columns, enhancing security and data management.Share Catalog with Target AccountNow that we\u2019ve created the data filter, let\u2019s utilize it while sharing the catalog. In the code snippet below, we\u2019ll share the database and table with the target account. Note that when sharing the table, we\u2019ll include the data filter created in the previous step.In this snippet:Share Database Permissions: Theaws_lakeformation_permissionsresource shares theIIoTDataLakedatabase with the target account and grants theDESCRIBEpermission.Share Table Permissions: Similarly, the resource shares themeasurementstable with the target account, granting theSELECTpermission. It also includes the data filter created earlier, ensuring that the target account only accesses the filtered data according to the defined criteria.With this setup, you can securely share specific data from your catalog with the target account, ensuring compliance and data integrity.Creating Resource Link in Target Account for AccessAfter sharing the catalog and table with a data filter to the target account, let\u2019s proceed to the target account to establish a resource link for accessing the shared catalog data.In this setup:Create Resource Link: Theaws_glue_catalog_databaseresource establishes a database resource link namedIIoTDataLake-Targetin the target account. It links to theIIoTDataLakedatabase in the source account, enabling access to the shared catalog data.By creating this resource link, you enable seamless access to the shared data catalog from the target account, facilitating data utilization and analysis across accounts while maintaining security and compliance measures.Granting Permissions for IAM RolesNow that we\u2019ve created the resource link, we can grant access to the resource link and the shared catalog. After this step, the IAM roles will have access to the filtered data shared from the source account.In this configuration:Grant Database Permissions: Theaws_lakeformation_permissionsresource grants the DESCRIBE permission to the IAM roles for theIIoTDataLake-Targetdatabase in the target account. This allows the roles to describe the database structure and metadata.Grant Table Permissions: Similarly, the resource grants theSELECTpermission to the IAM roles for themeasurementstable in the shared catalog. This enables the roles to select and read data from the table.With these permissions granted, the IAM roles now have access to the filtered data shared from the source account, allowing for seamless data analysis and utilization within the target account.ConclusionIn this blog, we\u2019ve delved into the intricacies of cross-account data sharing using AWS Lake Formation and Terraform. By implementing data filters and establishing resource links, we\u2019ve ensured secure access to shared data while maintaining granular control over permissions. This streamlined approach facilitates collaborative data analysis across accounts, empowering teams to derive insights effectively while upholding data security and compliance standards.ReferencesData filtering and cell-level security in Lake Formation\u200a\u2014\u200aAWS Lake Formation (amazon.com)"}
{"title": "Adding an Amazon Bedrock Knowledge Base to the Forex Rate Assistant", "published_at": 1715924289, "tags": ["aws", "ai"], "user": "Anthony Wat", "url": "https://dev.to/aws-builders/adding-an-amazon-bedrock-knowledge-base-to-the-forex-rate-assistant-488f", "details": "IntroductionIn our journey of experimenting with Amazon Bedrock up to this point, we have built abasic forex assistantas the basis for further enhancements to evaluate various Bedrock features and generative AI (gen AI) techniques. Our next step is to integrate a knowledge base to the agent, so that it can provide information about foreign currency exchange in general.In this blog post, we will define a representative use case for a RAG scenario for the forex rate agent, build a forex knowledge base, and attach it to the agent. Accuracy and performance of a gen AI application is also essential, so we'll conduct some tests and discuss challenges associated with RAG workflows.About Knowledge Bases for Amazon BedrockKnowledge Bases for Amazon Bedrockis a service that provides managed capability for implementingRetrieval Augmented Generation (RAG)workflows. Knowledge bases can be integrated with Bedrock agents to seamlessly enable RAG functionality, or be used as a component in custom AI-enabled applications using API.Knowledge Bases for Amazon Bedrock automates the ingestion of source documents, by generating embeddings with a foundation model, such asAmazon TitanorCohere Embed, and storing them in a supported vector store as depicted in the following diagram:To keep things simple, the service provides a quick start option that provisions on your behalf anAmazon OpenSearch Serverlessvector database for its use.Aside fromnative integration into a Bedrock agent, theAgents for Amazon Bedrock Runtime APIoffers both the ability to perform raw text and semantic search, and the ability to retrieve and generate a response with a foundation model, on knowledge bases. The latter allows the community to provide tighter integration in frameworks such asLangChainandLlamaIndexto simplify RAG scenarios. The runtime flow is shown in this diagram:Enhancing the forex rate assistant use caseIn the blog postBuilding a Basic Forex Rate Assistant Using Agents for Amazon Bedrock, we created a basic forex assistant that helps users look up the latest forex rates. It would be helpful if the assistant can also answer other questions on the broader topic.While information about the history of forex would be useful, the Claude models already possess such knowledge so it would not make a great use case for knowledge bases. As I searched for more specific subtopics, I found theFX Global Code, a set of common guidelines developed by theGlobal Foreign Exchange Committee (GFXC)which establishes universal principles to uphold integrity and ensure the effective operation of the wholesale FX market. The FX Global Code is conveniently available inPDF format, which is perfect for ingestion by the knowledge base.Requesting model access and creating the S3 bucket for document ingestionLet's start with theprerequisitesbyrequesting for model access. For the forex knowledge base, we will be using theTitan Embeddings G1 - Textmodel. You can review the model pricing informationhere.Next, we need to create the S3 bucket from which the knowledge base will ingest source documents. We can quickly do so in the S3 Console with the following settings:Bucket name:forex-kb-<region>-<account_id>(such asforex-kb-use1-123456789012)Block all public access:Checked (by default)Bucket versioning:EnableDefault encryption:SSE-S3 (by default)Once the S3 bucket is created, download theFX Global Code PDF fileand upload it to the bucket:This is sufficient for our purpose. For more information on other supported document formats and adding metadata for the filtering feature, refer to theAmazon Bedrock user guide.Creating a knowledge base along with a vector databaseNext, we can create the knowledge base in the Amazon Bedrock console following the steps below:SelectKnowledge basesin the left menu.On theKnowledge basespage, clickCreate knowledge base.InStep 1of theCreate knowledge basewizard, enter the following information and clickNext:Knowledge base name:ForexKBKnowledge base description:A knowledge base with information on foreign currency exchange.InStep 2of the wizard, enter the following information and clickNext:Data source name:ForexKBDataSourceS3 URI:Browse and select the S3 bucket that we created earlierInStep 3of the wizard, enter the following information and clickNext:Embeddings model:Titan Embeddings G1 - Text v1.2Vector database:Quick create a new vector storeInStep 4of the wizard, clickCreate knowledge base.The knowledge base and the vector database, which is an Amazon OpenSearch Server collection, will take a few minutes to create. When they are ready, you'll be directed to the knowledge base page, where you will be prompted to synchronize sync the data source. To do so, scroll down to theData source section, select the radio button beside the data source name, and clickSync:It will take less than a minute to complete, since we only have a single moderately-sized PDF document. Now the knowledge base is ready for some validation.Testing the knowledge baseA knowledge base must provide accurate results, so we need to validate it against information we know exists out of the source documents. This can be done using the integrated test pane. To simulate an end-to-end test for the RAG scenario, configure the test environment in the Bedrock console as follows:Enable theGenerate responseoption (which should already be enabled by default).ClickSelect model. In the new dialog, select theClaude 3 Haikumodel and clickApply.Click on the button with the three sliders, which opens theConfigurationpage. This should expand the test pane so you have more screen real estate to work with.I've prepared a couple of questions after skimming through the FX Global Code PDF file. Let's start by asking a basic question:What is the FX Global Code?The knowledge base responded with an answer that's consistent with the text on page 3 of the document.To see the underlying search results which the model used to generate the response, clickShow source details. Similar to agent trace, we can view the source chunks that are related to our question, and the associated raw text and metadata (which is mainly the citation information). Some source chunks refer to the table of contents, which some refers to the same passage from page 3 of the document.Next, let's ask something more specific, namely to give me information on a specific principle such as principle 15 which is on page 33 of the PDF file:What is principle 15 in the FX Global Code?Interestingly, the knowledge base doesn't seem to know the answer:If I force the knowledge base to usehybrid search, which combines both sematic and text search for better response, some source chucks were fetched but it does not seem to include one with the text from page 33.Since there are exactly five results, I figured that it might be limited by the maximum number. After increasing it to an arbitrary 20, the knowledge base finally returned a good response with the default search option:This goes to show that just like agents, knowledge bases must be tested and fine-tuned extensively to improve accuracy. The embedding model as well as the underlying vector store may also play a part in the overall behavior of the knowledge base.In any case, you've successfully created a knowledge base using Knowledge Bases for Amazon Bedrock, which can be integrated with your gen AI applications. To complete our experimentation, let's now integrate it with our forex rate assistant.Integrating the knowledge base to the forex rate assistantIf you haven't already done so, please follow the blog postBuilding a Basic Forex Rate Assistant Using Agents for Amazon Bedrockto create the forex rate assistant manually, or use the Terraform configuration from the blog postHow To Manage an Amazon Bedrock Agent Using Terraformto deploy it.Once the agent is ready, we canassociate the knowledge baseto it using the steps below in the Bedrock Console:SelectAgentsin the left menu.On theAgentspage, clickForexAssistantto open it.On the agent page, clickEdit in Agent Builder.On theAgent builderpage, scroll down to theKnowledge basessection and clickAdd.On theAdd knowledge basepage, enter the following information and clickAdd:Select knowledge base:ForexKBKnowledge base instructions for Agent:Use this knowledge base to retrieve information on foreign currency exchange, such as the FX Global Code.ClickSave and exit.Once the knowledge base is added, prepare the agent and ask the same first question as before to validate the integration:What is the FX Global Code?The agent responded with a decent answer. In the trace, we can see that the agent invoked the knowledge base as part of its toolset and retrieved the results for its use.We also want to ask the agent to fetch some exchange rate to ensure that the functionality is still working:What is the exchange rate from EUR to CAD?The agent responded with the rate fetched from theForexAPIaction group, which is what we expected.However, we run into issues when asking the second question from before:What is principle 15 in the FX Global Code?The agent responded with the inferior answer since we did not adjust the maximum number of retrieval results for the knowledge base.Unfortunately, our issue now is that there is no way that I know of to provide the knowledge base configuration in the agent, so we are stuck. At this point, there's nothing we can do other than opening an AWS support case to inquire about the lack of support... That being said, another angle to look at the problem is that the quality of the source could also affect the knowledge base's search accuracy, which brings us to the topic of common RAG challenges.Common RAG challengesLet's examine the source chunk from the correct answer for the \"principle 15\" question from our knowledge base test:This is also the text that was extracted from the PDF for embedding. Comparing it to the corresponding page in the PDF file, notice the following:The chunk text includes information such as headers and \"breadcrumbs\" that are not related to the main content.The text does not capture the context of the elements in the passage, such as the principle title in the red box and the principle summary in italic.It's fair to think that undesirable artifacts and lack of structural context would impact search accuracy, performance, and ultimately cost. Consequently, it makes sense to perform some data pre-processing before passing the source documents to the RAG workflow. Third-party APIs and tools, such asLlamaParseandLayoutPDFReader, can help with pre-processing PDF data, however keep in mind that source documents may take any forms and there is no one-size-fits-all solution. You may have to resort to developing custom processes for pre-processing and search your unique data.There are otherchallenges in building RAG-based LLM applicationsand proposed solutions which you should be aware of. However, some of them cannot be implemented in a managed solution such as Knowledge Bases for Amazon Bedrock, in which case you may need to build a custom solution yourself if you have a genuine need to address them. Such is the eternal quest to balance between effort and quality.Don't forget to delete the OpenSearch Serverless collectionBe aware that Knowledge Bases for Amazon Bedrock does not delete the vector database for you. Since the OpenSearch Serverless collection consumes at least one OpenSearch Compute Unit (OCU) which ischarged by the hour, you will incur a running cost for as long as the collection exists. Consequently, ensure that you manuallydelete the collectionafter you have deleted the knowledge base and other associated artifacts.SummaryIn this blog post, we created a knowledge base using Knowledge Bases for Amazon Bedrock which we integrate into theforex rate assistantto allow it to answer questions about the FX Global Code. Through some testing of the solution, we experienced some common challenges for RAG solutions and potential mitigation strategies. Although some of them are not applicable to Bedrock knowledge bases since it abstracts the implementation details, thus highlighting the potential need for a custom solution for more demanding scenarios.My next step is to enhance theTerraform configuration for the forex rate assistantto provision and integrate the knowledge base, and to enhance theStreamlit test appto display citations from knowledge base searches. Be sure to follow theAvangards Blogas I continue my journey on building gen AI applications using Amazon Bedrock and other AWS services. Thanks for reading and stay curious!"}
{"title": "Effortlessly Export AWS Health Organizational View to CSV with This CLI Tool", "published_at": 1715874599, "tags": ["aws", "awshealth", "go", "cli"], "user": "hayao-k", "url": "https://dev.to/aws-builders/effortlessly-export-aws-health-organizational-view-to-csv-with-this-cli-tool-29ae", "details": "\ud83d\udca1 The tools discussed in this article leverage the AWS Health API, which requires a Business or higher-level AWS Support plan.IntroductionFor all AWS Organizations administrators, how do you handle the events notified by AWS Health? I imagine you receive numerous notifications daily when utilizing AWS services.These events need to be properly managed as they can have a significant impact on the availability and reliability of your systems. Recently, there have been significant events scheduled, such as the Amazon RDS certificate update in August and the end of support for the AWS Lambda Python 3.8 runtime in October.If you're managing multiple accounts within your organization, manually checking events and gathering relevant information can take time and effort.I have developed a CLI tool called AWS Health Exporter to address this challenge.hayao-k/aws-health-exporterAWS Health Exporter is a command-line tool designed to describe AWS Health events for your organization.AWS Health ExporterHealth Exporter is a command-line tool designed to describe AWS Health events for your organization. It allows you to filter events by service name and status, and export the details to a CSV file. Optionally, you can echo the CSV content to standard output.FeaturesEvent Filtering: Filter events by service name, event status, and other criteria to get precisely the data you need.Entity Filtering: Filter affected entities by status code (IMPAIRED, UNIMPAIRED, UNKNOWN, PENDING, or RESOLVED).AWS Organizations Support: Works seamlessly with AWS Organizations, allowing you to get a health overview of all accounts.CSV Export: Automatically formats and exports the data into a CSV file, making it simple to store, share, and analyze.PrerequisitesAWS credentials with appropriate permissions to access AWS Health and AWS Organizations servicesYou must have a Business, Enterprise On-Ramp, or Enterprise Support plan from AWS Support to use the\u2026View on GitHubKey FeaturesAWS Health Exporter is a command-line tool for retrieving event information from the organizational view of AWS Health. It allows you to filter events by service name, status, and more and export details of the relevant accounts and resource IDs to a CSV file.AWS Organizations Support:Retrieves information from the organizational view of AWS Health. It cannot be used with standalone accounts, but there is an option to output data for a single account only.CSV Export:Data is formatted and exported in CSV format, making it easy to save, share, and analyze.Event Filtering:Filters events by conditions such as service name and status, making it easier to find the events you're looking for.Resource Filtering:Only retrieves resources matching specific status codes (IMPAIRED, UNIMPAIRED, UNKNOWN, PENDING, or RESOLVED).About AWS Health Organizational ViewEnabling the organizational view allows you to aggregate AWS Health events for all accounts within the organization. Data is retained for 90 days, and users/roles of the organization's management or delegated administrator accounts can access the information.You can set it up and refer to it from \"Your organization health\" in the AWS Health dashboard.In the organizational view, you can check information for each event, such as:Affected accountsNumber of affected resources and breakdown of their statusesResources affected within each accountThis tool can export all this information to a CSV file!Prerequisites for using the toolThe organizational view of AWS Health is enabled.AWS authentication credentials to access AWS Health and AWS OrganizationsAuthentication credentials for the management or delegated administrator accounts are required to use the organizational view.A business plan or higher-level AWS support contractRequired to use the AWS Health APIHow to UseDownload the latest binary suitable for your environment from the GitHub repository's releases page.https://github.com/hayao-k/aws-health-exporter/releaseswget https://github.com/hayao-k/aws-health-exporter/releases/download/v0.8.1/aws-health-exporter_0.8.1_linux_amd64.tar.gztarxvf aws-health-exporter_0.8.1_linux_amd64.tar.gzEnter fullscreen modeExit fullscreen modeTo use AWS Health Exporter, run the binary with the desired flags. Below are the available flags:--event-filter,--filter,-f: Filter events by service name, event status, and other criteria.--status-code,-c: Filter entity by status code. Possible values are IMPAIRED, UNIMPAIRED, UNKNOWN, PENDING and RESOLVED--echo,-e: Echo CSV content to standard output.--profile,-p: Specify the AWS credential profile to use.--account-id,-i: Specify a single account ID to process (optional).--output-file,--file-name,o: Specify the output CSV file name.Details of the event filtering optionThe--event-filteroption allows you to specify complex filtering criteria. Below is a table of the available fields that can be included in the filter criteria:FieldDescriptionPossible ValuesserviceFilter events by AWS service name.e.g.,LAMBDA,RDS,EKSstatusFilter events by status.open,closed,upcomingcategoryFilter events by category.issue,accountNotification,scheduledChange,investigationregionFilter events by region.AWS region codes, e.g.,us-east-1startTimeFilter events by start time.ISO 8601 date formatendTimeFilter events by end time.ISO 8601 date formatlastUpdatedTimeFilter events by last updated time.ISO 8601 date formatForstartTime,endTime,andlastUpdatedTime, you can specify a time range usingfromandtoin ISO 8601 date format. Here is the structure for determining the time range:{from:YYYY-MM-DDTHH:MM:SSZ,to:YYYY-MM-DDTHH:MM:SSZ}Example Commands# Describe RDS events with open status and export to CSV./health-exporter--event-filterservice=RDS,status=open# Describe upcoming LAMBDA events and echo the output to STDOUT./health-exporter--event-filterservice=LAMBDA,status=upcoming--echo# Describe only events in the Tokyo region and specify their last updated time../health-exporter----event-filter\"lastUpdatedTime={from=2024-03-01T00:00:00Z,to=2024-05-02T23:59:59Z},region=ap-northeast-1\"# Get entities with pending status only and specify a custom file name./health-exporter--status-codePENDING--output-filemy_event_details.csv# Get events using the specified profile./health-exporter--profilemy-profile# Process only a single account./health-exporter--account-id123456789012Enter fullscreen modeExit fullscreen modeExecution ExampleWhen you execute the command, an interactive prompt will be displayed. In the following example, the--event-filterflag extracts only the upcoming status events related to AWS Lambda.$health-exporter--event-filterservice=LAMBDA,status=upcoming--status-codePENDING Use the arrow keys to navigate: \u2193 \u2191 \u2192 \u2190  ? Select an event:    \u25b8 LAMBDA - AWS_LAMBDA_PLANNED_LIFECYCLE_EVENT(us-east-1, 2024-10-14 07:00:00)LAMBDA - AWS_LAMBDA_PLANNED_LIFECYCLE_EVENT(ap-northeast-1, 2024-10-14 07:00:00)LAMBDA - AWS_LAMBDA_PLANNED_LIFECYCLE_EVENT(ap-northeast-1, 2024-06-12 07:00:00)LAMBDA - AWS_LAMBDA_PLANNED_LIFECYCLE_EVENT(ap-southeast-2, 2024-10-14 07:00:00)\u2193   LAMBDA - AWS_LAMBDA_PLANNED_LIFECYCLE_EVENT(us-east-1, 2024-06-12 07:00:00)Enter fullscreen modeExit fullscreen modeFrom the prompt, select the event you want to output. After selection, the tool will gather related account and entity information and output it to a CSV file.\u2714 LAMBDA - AWS_LAMBDA_PLANNED_LIFECYCLE_EVENT(us-east-1, 2024-10-14 07:00:00)Event details have been written to AWS_LAMBDA_PLANNED_LIFECYCLE_EVENT_2024-10-14_07-00-00_us-east-1_PENDING.csv.Enter fullscreen modeExit fullscreen modeThe output CSV will contain information such as Account ID, Account Name, Region, Identifier, Status, and Last Updated. In this example, since--status-code PENDINGwas specified during command execution, only resources with PENDING status are output.Account ID, Account Name, Region, Identifier, Status, Last Updated 000000000000,account-0000,us-east-1,arn:aws:lambda:us-east-1:000000000000:function:Old_Runtime_Lambda_Function-1PBKPZPFSJ058,PENDING,2024-04-21 20:11:29 111111111111,account-1111,us-east-1,arn:aws:lambda:us-east-1:111111111111:function:Old_Runtime_Lambda_Function-uuTi2u7DbooD,PENDING,2024-04-21 20:11:29 111111111111,account-1111,us-east-1,arn:aws:lambda:us-east-1:111111111111:function:Old_Runtime_Lambda_Function-omdieC8Umobo,PENDING,2024-04-21 20:11:29 222222222222,account-2222,us-east-1,arn:aws:lambda:us-east-1:222222222222:function:Old_Runtime_Lambda_Function-ULZ27BYSQ0MN,PENDING,2024-04-21 20:11:29 222222222222,account-2222,us-east-1,arn:aws:lambda:us-east-1:222222222222:function:Old_Runtime_Lambda_Function-10YNGBMU46VP9,PENDING,2024-04-21 20:11:29 222222222222,account-2222,us-east-1,arn:aws:lambda:us-east-1:222222222222:function:Old_Runtime_Lambda_Function-CEgHAu41udFy,PENDING,2024-04-21 20:11:29 333333333333,account-3333,us-east-1,arn:aws:lambda:us-east-1:333333333333:function:Old_Runtime_Lambda_Function-zNKRpLWP0pXB,PENDING,2024-04-21 20:11:29 333333333333,account-3333,us-east-1,arn:aws:lambda:us-east-1:333333333333:function:Old_Runtime_Lambda_Function-24ES8MRQJ9R6,PENDING,2024-04-21 20:11:29 444444444444,account-4444,us-east-1,arn:aws:lambda:us-east-1:444444444444:function:Old_Runtime_Lambda_Function-134QIS8IYF84K,PENDING,2024-04-21 20:11:29 444444444444,account-4444,us-east-1,arn:aws:lambda:us-east-1:444444444444:function:Old_Runtime_Lambda_Function-B97VeyrZNXIy,PENDING,2024-04-21 20:11:29Enter fullscreen modeExit fullscreen modeMechanismPrimarily uses 3 AWS Health APIs.DescribeEventsForOrganization APIDescribeAffectedAccountsForOrganization APIDescribeAffectedEntitiesForOrganization APIDescribeEventsForOrganization APICalls the DescribeEventsForOrganization API to retrieve relevant events based on the filter conditions specified on the command line. This API returns only an overview of the events, so information about affected accounts or resources is not included.DescribeAffectedAccountsForOrganization APIThis API retrieves a list of accounts within the organization affected by the selected event.DescribeAffectedEntitiesForOrganization APIThis API returns a list of entities affected by one or more events in one or more accounts within the organization.When the user selects an event through the interactive prompt, information obtained from these APIs is formatted and output as a CSV file.I hope this helps you."}
{"title": "Switching Identity Providers in the IAM Identity Center", "published_at": 1715851476, "tags": ["aws", "identitycenter", "sso"], "user": "Maurice Borgmeier", "url": "https://dev.to/aws-builders/switching-identity-providers-in-the-iam-identity-center-5mo", "details": "Mergers and acquisitions are usually decided at the top of the corporate hierarchy and eventually trickle down to regular technologists who need to implement changes. Today, I'm sharing one of these stories.One of our customers was acquired by another company a while ago, and they are in the middle of integrating their identity management solutions. In a nutshell, that means all users and groups that our customer has in their Entra ID (formerly known as Azure AD) are recreated in the Entra ID Directory of the parent company. This is relevant to us because access to AWS accounts is granted based on a connection to the old Entra ID.The customer uses the IAM Identity Center (formerly AWS SSO) to authenticate users in their multi-account structure. They map users and groups from their Entra ID to IAM roles in their organization via Permission Sets. If you're not familiar with the terminology, I recommend you check outthis blog post introducing the Identity Center.Introduction to SSO with the IAM Identity Center and Entra IDMaurice Borgmeier for AWS Community Builders  \u30fb Apr 23#aws#cloud#identity#ssoThe challenge we faced was migrating from their existing Entra ID (old) to the parent companies' Entra ID (new) as an identity provider in the IAM Identity Center while minimizing disruptions to DevOps teams operating their services and retaining the current permission structure. Minimizing disruptions is vital because this solution manages access to all AWS accounts. If that's disrupted, service teams won't be able to access their environments to solve problems.The actual services within these accounts should be fine as the Identity Center manages humans accessing AWS accounts and not the communication of the systems within. Nevertheless, we want to be cautious.ConstraintsThere are a few constraints that you need to know about when approaching something like this:IAM Identity Center only supports a single active identity provider at any point in timeYou can't update the external IDs and issuers on users and groups in the Identity Center identity storeThese constraints are important because the first means you can't completely avoid disruptions. There will be a short downtime when you switch the identity providers in the IAM Identity Center. Arguably, the second is more problematic because it means you'll have to migrate your permission structure, too, since new users and groups will be created, and there's no direct way to map them to the old users and groups.Our ApproachAside from the technical implementation, lots of communication is required in advance of and during the migration to ensure everyone is aware of and prepared to deal with the changes. While this is critical work, it's not the focus of this document. We'll focus on the technical aspects, which start with preparing for the changes.PreparationIn preparation for the cutover, we create files that map old users to new users and old groups to new groups based on their Entra ID object identifiers. This can be done without impacting the existing system, and having them available during cutover is essential. Additionally, we create an IAM user with permissions to manage the identity center because once we switch over, our Entra ID-based users won't work immediately, and we need to have access to the environment. Furthermore, we export the existing permission structure, i.e., the permission sets and their assignments to accounts, users, and groups. Lastly, we will create a new Enterprise app in the new Entra ID and assign new users and groups to it.CutoverWith all these preparations done, we can start the cutover during a prearranged time window. During the cutover, we replace the existing IDP in the Identity Center and set up SAML authentication as well as SCIM for user provisioning with the new Enterprise app. Next we have to wait a few minutes for SCIM to provision all users and groups in the Identity Center identity store. Once that's done, we use the same script as we did during the preparation stage to export the permission structure with permission sets, users, groups, and all the assignments.The next step happens offline. We merge the old export with the new export using the aforementioned old user/group to new user/group mappings and update the identifiers in all assignments to point to the new users/groups instead of the old ones. We specifically chose to do this offline because this is a critical step, and we want to avoid an inconsistent state in AWS in case anything breaks during this procedure. Having this all file-based means it's easier to reason about what's happening and write test cases to ensure the logic is sound.This merged permission structure is now our target state for the real environment. Next, we use another script to basicallydiffthe export of the current environment and the target state. This creates a list of changes needed to get from the current environment state to the target environment state. Essentially, this is a list of assignments to add or remove. Our changeset is stored as a file as well to enable easy verification and for the same consistency considerations.Subsequently, it's time to apply these changes to the real AWS environment, which another script does. It's written so that it can be stopped and restarted at any point, i.e., it can handle changes that have already been applied. We can restart the process if something goes wrong and we somehow lose connection.After the changes have been applied, our system should be in the desired state. To double-check, we create another export anddiffthe target state with the export - there shouldn't be any changes. Now, users should be able to log in using their new identities from the new Entra ID and have all the same permissions that they used to have before.Clean upSwitching the Identity Provider in the Identity Center leaves us with a long list of orphaned users and groups, as the SCIM-provided users and groups from the old Entra ID aren't deleted automatically. To avoid confusion, we're using a script to delete the users and groups that deletes users and groups whose external issuer (a reference to Entra ID) points to the old Entra ID.There is another manual step here, in our case, that wasn't time-sensitive, though. Some tools used identifiers from the Identity Center to grant and restrict access to services like Quicksight. Since the underlying identifiers had to change, the policies had to be manually adjusted. This only affected a handful of users and wasn't worth automating.Dealing with Operational ProblemsWe tested the approach on a separate Identity Center environment first using the real Entra IDs involved in the live migration and designed the tools to be able to restart in case of problems. This means we were confident that the process would work, but nevertheless, we considered how things could go wrong.Once we sever the connection to the old Entra ID IDP, and switch over to the new one, we basically hit a point of no return. Switching back to the old one is possible, but we have no guarantee that all identifiers remain identical, and if we were forced to do that, we'd be in a position where our tested tools had failed in a way that we hadn't foreseen and couldn't recover from. In that scenario, it's unlikely that they'd help us recreate the mapping.This means we're at a point where we first focus on damage control, which means ensuring Ops personnel is able to access the production accounts to deal with any problems that may arise. In that scenario, we'd switch our Identity Center over to the internal Identity Store, create users for the Ops personnel, and manually assign them to the correct permission set.While not an ideal situation, it would allow us to fix any issues with the scripts, APIs, or permissions and finish the migration.Non-obvious things that are importantWhen applying the changes, we need to ensure that new assignments are created before we delete any assignment. Otherwise, we could end up in a situation where an AWS account no longer has any assignment to a given permission set and the underlying IAM role is deleted. Later, it would be recreated but with a different identifier, which may break policies.SummaryThis approach reduces both the downtime and risk of things going wrong while achieving the goal of migrating the identity providers. Additionally it allows us to verify the crucial mapping operations using automated tests.If you're in a similar situation,get in touch, we're happy to support you in your migration.Photo byJulia CraiceonUnsplash(Migrating Spoonbills)"}
{"title": "From Scratch: Permissions", "published_at": 1715817312, "tags": ["aws", "beginners", "devops", "terraform"], "user": "Joseph", "url": "https://dev.to/aws-builders/from-scratch-permissions-4c94", "details": "Once we have our groups and users set up from our external identity provider we can move on to defining what permissions they should have. This is mostly straightforward, but we'll have to take a few detours along the way because of how groups work with the Google Workspace IDP.Permission SetsThe first thing we have to do is script our permission sets. We will use these to connect IAM policies to our IDP groups. This will all be done in CDKTF.constset=newSsoadminPermissionSet(scope,`${name}-admin`,{name:`${name}-admin`,instanceArn:arnId,});Enter fullscreen modeExit fullscreen modeThere's not a lot to this. The ARN we are using here for theinstanceArnactually comes from the sameDataAwsSsoadminInstanceswe used to get thestoreIdbefore.PoliciesNow that we have our admin permission set, we have to actually give it an admin policy.newSsoadminManagedPolicyAttachment(scope,`${name}-admin-policy`,{instanceArn:arnId,managedPolicyArn:\"arn:aws:iam::aws:policy/AdministratorAccess\",permissionSetArn:set.arn,});Enter fullscreen modeExit fullscreen modeThe policy here is just the AWS managed policy that comes with every account. We're ready to deploy at this point, so we need to go back into our AWS console, and reactivate the root user's access key. Remember, the last time we used it we deactivated it as soon as we were done, and we're going to do the same thing here.With the access key active again, we can go ahead andcdktf deployour stack, and we should see two more resources get created. Once it's done we go back into the console and turn off the access keys again.Group AssignmentWith these new permissions we should be all set now to assign our groups those permissions and be on our way, but this is where we hit our first snag. We need to go into the identity center console as root and assign the group to our AWS account along with the admin permission set we just made, which we can do, but there's a problem. Because of how we had to make the groups in identity center, we aren't able to assign users to the group in the console. It has to be scripted. The root user shouldn't do this, though, because managing permissions at this level would mean people would have to be logging into root all the time, which is a big problem. Instead, what I'm going to do instead is assign myself to the AWS account. My user was brought over automatically through the identity provider, and I can set all that up in the console easily.Switching UsersWith that set up, I now have my user assigned as an admin in the AWS account. Now it's time to configure AWS CLIwith my sso. With that done, we're going to make a new stack that uses the admin profile to allocate infrastructure.Account AssignmentNow we're ready to assign the group to the AWS account, and add any users we want along the way. Since we'll be doing this as an admin user, we'll need to make a new CDKTF stack, and deploy it using the admin profile after logging in usingaws sso login. In my case, I made anadminprofile and configured the aws cli with it.The first thing we'll add to the new stack is going to be the account assignment that adds the group to the AWS account with admin permissions.newSsoadminAccountAssignment(scope,`${name}-ga-admin`,{provider:config.provider,principalType:\"GROUP\",targetType:\"AWS_ACCOUNT\",targetId:config.caller.accountId,permissionSetArn:config.identityCenter.permissions.admin.arn,instanceArn:config.storeArn,principalId:config.identityCenter.groups.admin.groupId,});Enter fullscreen modeExit fullscreen modeAt this point the group is set up and ready to use. We can now go ahead and add some users to it.constjuser=newDataAwsIdentitystoreUser(scope,`${name}-iu-joseph`,{identityStoreId:config.storeId,alternateIdentifier:{uniqueAttribute:{attributePath:\"UserName\",attributeValue:\"joseph@awsdemo.com\",},},});newIdentitystoreGroupMembership(scope,`${name}-gm-joseph`,{identityStoreId:config.storeId,groupId:config.identityCenter.groups.admin.groupId,memberId:juser.userId,});Enter fullscreen modeExit fullscreen modeRemember the users are automatically syncronized from our external identity provider, so we won't be making a user. Instead, we just need to grab the one we already have. In my case I found my own account by username. There are other options too, but this one is pretty straightforward. From this point it's just a matter of creating a membership for the user to belong to the group.Separate StacksWe started off by building some resources using root and then made a separate stack for things an admin can then do. However, the interesting thing about this approach is that once you have an admin that can log into the system, they also have enough permissions to do everything in the first stack we made. We used the root user to get us just far enough in our IaC build out that our admin users could take over. From this point on, we won't be needing the root user anymore, so we can go ahead and delete the access key it had.As always, if you are interested in how everything ties together, you can take a look at thehow to cloudrepository."}
{"title": "[Lab] AWS Lambda LLRT vs Node.js", "published_at": 1715800739, "tags": [], "user": "Amador Criado", "url": "https://dev.to/aws-builders/lab-aws-lambda-llrt-vs-nodejs-44g4", "details": "IntroductionAWS Lambda is a serverless compute service that runs code in response to events and automatically manages the underlying resources for you. You can use AWS Lambda to extend other AWS services with custom logic, or create your own backend services that operate at AWS scale, security and the main feature considered for this lab, performance.One of the biggest challenges that is common among AWS Lambda and serverless computing is the Performance. Unlike traditional server-based environments where infrastructure resources can be provisioned and fine-tuned to meet specific performance requirements, serverless platforms like Lambda abstract away much of the underlying infrastructure management, leaving less control over performance optimization in the hands of developers. Additionally, as serverless architectures rely heavily on event-driven processing and pay-per-execution pricing models, optimizing performance becomes crucial for minimizing costs, ensuring responsive user experiences, and meeting stringent latency requirements.What is Lambda LLRT?WarningLLRT is anexperimentalpackage. It is subject to change and intended only for evaluation purposes.AWS has open-sourced its JavaScript runtime, calledLLRT (Low Latency Runtime), an experimental, lightweight JavaScript runtime designed to address the growing demand for fast and efficient Serverless applications.LLRT is designed to address the growing demand for fast and efficient Serverless applications. LLRT offers up to over10xfaster startup and up to2xoverall lower cost compared to other JavaScript runtimes running onAWS Lambda.It's built in Rust, utilizing QuickJS as JavaScript engine, ensuring efficient memory usage and swift startup.Real-Time processing or data transformation are the main use cases for adopting LLRT for future projects. LLRT Enables a focus on critical tasks like event-driven processing or streaming data, while seamless integration with other AWS services ensures rapid response times.Key features of LLRTFaster Starup Times:Over 10x faster startupcompared to other JavaScript runtimes running on AWS Lambda.Cost optimization: Up to2x overall lower costcompared to other runtimes. By optimizing memory usage and reducing startup time, Lambda LLRT helps minimize the cost of running serverless workloads.Built on Rust: Improves performance, reduced cold start times, memory efficiency, enhanced concurrency and safety.QuickJS Engine: Lightweight and efficient JavaScript engine written in C. Ideal for fast execution, efficient memory usage, and seamless integration for embedding JavaScript in AWS Lambda.No JIT compiler:unlike NodeJs, Bun & Deno, LLRT not incorporates JIT compiler. That contributes to reduce system complexity and runtime size. Without the JIT overhead, CPU and memory resources can be more efficiently allocated.OverviewThe goal of this lab is to conduct a comparative analysis between AWS Lambda's Low Latency Runtime (LLRT) and traditional Node.js runtime, focusing on their performance and efficiency in real-world serverless applications. By deploying identical functions with LLRT and Node.js, with the aim to evaluate their respective startup times, execution speeds, and resource consumption.The previous results of performance profiling by AWS Labs, available on GitHub, offer valuable benchmarks and insights:LLRT- DynamoDB Put, ARM, 128MB:Node.js 20- DynamoDB Put, ARM, 128MB:LabThe lab is composed by 2 Lambda functions \"llrt-put-item\" and \"nodejs-put-item\", both designed to put any received event into a DynamoDB table named \"llrt-sam-table\".Implemented using AWS SAM, the project leveragesAWS Lambda Power Tuningfor performance profiling and benchmarking. While resembling the default benchmark by AWS Labs, all code developed is accessible on my personal GitHub repository, facilitating easy replication and further exploration of the lab's results:https://github.com/acriado-otc/example-aws-lambda-llrtBenchmarking: LLRT vs Node.jsTo benchmark the lab, AWS Lambda Power Tuning has been chosen as a open-source tool developed by AWS Labs. AWS Lambda Power Tuning is a state machine powered by AWS Step Functions that will helps us to get results for both lab's Lambda functions and get results for cost and/or performance in a data-driven way.While you can manually run tests on functions by selecting different memory allocations and measuring the time taken to complete, the AWS Lambda Power Tuning tool allows us to automate the process.The state machine is designed to be easy to deploy and fast to execute. Also, it's language agnostic.DeploymentTo deploy AWS Lambda Power Tuning, clone the official repository and choose the preferred deployment method. In my case I used SAM cli for simplicity, but I highly recommend to use AWS CDK for IaC deployments:https://github.com/alexcasalboni/aws-lambda-power-tuningExecuting the State MachineIndependently of how you've deployed the state machine,\u00a0you can execute it in a few different ways.\u00a0Programmatically,\u00a0using the AWS CLI,\u00a0or AWS SDK.\u00a0In the case of our lab, we are going to execute it manually,\u00a0using the AWS Step Functions web console.Once properly deployed, You will find the new state machine in theStep Functions Consolein the AWS account and region defined for the lab:Find it and click\u00a0\"Start execution\".A descriptivenameandinputshould be provided.We are going to executellrt-put-item first,that's the input used**:**{\"lambdaARN\":\"arn:aws:lambda:eu-west-1:XXXXXXXXXXX:function:llrt-sam-LlrtPutItemFunction-42aAa2of5wqn\",\"powerValues\":[128,256,512,1024],\"num\":500,\"payload\":{\"message\":\"hello llrt lambda\"},\"parallelInvocation\":true,\"strategy\":\"speed\"}Enter fullscreen modeExit fullscreen modeAll other fields should remain by default. Finally, find it and click\u00a0\"Start execution\".After some seconds/minutes the Execution should have the status \"Succeded\"Repeat the same process for thenodejs-put-itemlambda function. The unique field to change from the input should be the function's ARN and optionally the payload.While running each execution, we can see the status/progress of the step function execution with the graph view:Results and AnalysisIn order to get accurate results several executions have been made for each Lambda function, with 'speed' strategy. Next we are presenting the results for each Lambda separately, and a final comparison:LLRT Lambda function:{\"power\":1024,\"cost\":9.519999999999999e-8,\"duration\":6.25,\"stateMachine\":{\"executionCost\":0.00025,\"lambdaCost\":0.0001901263,\"visualization\":\"https://lambda-power-tuning.show/#gAAAAQACAAQ=;REREQWbm5kIREdFAAADIQA==;ata9Muy90zTBcEwzwXDMMw==\"}}Enter fullscreen modeExit fullscreen modeNodejs{\"power\":1024,\"cost\":1.512e-7,\"duration\":8.666666666666666,\"stateMachine\":{\"executionCost\":0.00025,\"lambdaCost\":0.00025384590000000003,\"visualization\":\"https://lambda-power-tuning.show/#gAAAAQACAAQ=;d7fdQ7y7iEKamatBq6oKQQ==;Ckp6Nc+VmzRwbUY0ilkiNA==\"}}Enter fullscreen modeExit fullscreen modeComparisonAfter several executions of the AWS Power Tuning step function. It's clear that both lambda functions has a similar performance for 1024MB and 512MB of memory allocation. The huge difference appears in the interval from 128MB to 256MB, where LLRT is more suitable option and should be considered for small functions.In terms of cost, for LLRT 128MB is the cheaper average execution, and 256MB the wrost, meanwhile for NodeJs function the wrost cost 128MB, just the opposite of LLRT. However, it's important to remark that for this laboratoy, the criteria applied for the benchmark has been 'speed'. To get more accurate cost results should be executed with 'cost' strategy accordingly.ConclusionLLRT clearly can add advantages for some projects, such as faster startup times and potential cost savings, but it currently lacks the stability, support, and real-world testing required to be considered for production use.For developers working with smaller serverless functions that prioritize rapid response times and efficient resource utilization, LLRT is an alternative to traditional Lambda runtimes. However, it's essential to evaluate carefully LLRT and consider the specific requirements of your application before.As LLRT continues to evolve and mature, it's adoption will increase for sure, and progressively a refinement of this runtime for latency-sensitive use cases will be provided by AWS. In the meantime, let's monitor LLRT's development and wait for news, specially for production environments.References and resourceshttps://github.com/awslabs/llrthttps://aws.amazon.com/lambda/features/https://bellard.org/quickjs/https://docs.aws.amazon.com/lambda/latest/operatorguide/profile-functions.htmlhttps://serverlessrepo.aws.amazon.com/applications/arn:aws:serverlessrepo:us-east-1:451282441545:applications~aws-lambda-power-tuning"}
{"title": "Gatekeep CodeCatalyst Workflow using Approval", "published_at": 1715793973, "tags": ["aws", "codecatalyst", "cicd", "terraform"], "user": "Luthfi Anandra", "url": "https://dev.to/aws-builders/gatekeep-codecatalyst-workflow-using-approval-1945", "details": "In some use cases, our CI/CD pipeline or workflow may need to be validated before code can be deployed.Refer to this announcement \"Workflow approvals for Amazon CodeCatalyst\", Amazon CodeCatalyst, now has released feature called approval gates which can be utilized so that workflow can be paused until user with privilege can validate the workflow and whether workflow is approved and can be processed or not.More details regarding Approval in CodeCatalyst can be read on this documentation:Approval Gate.Table of ContentsScenarioPrerequisitesConfiguration StepsDirectory StructureCodeCatalyst Workflow File ConfigurationsExample of Approval Process in CodeCatalyst WorkflowSummaryScenarioThis blog is related with my previous blog \"CI/CD Pipeline for Terraform Workflow Using Amazon CodeCatalyst\". In my previous blog, in workflow/pipeline, when terraform plan has finished and will continue to terraform apply, there is a process in between where pipeline will wait for 1 minute to check the terraform plan output and if the plan is not expected, we need to cancel the workflow manually. The difficulty that faced back then is because there was no out of the box solution provided by CodeCatalyst to restrict whether shifting between one action to other action can be cancelled if there is unexpected result.In this blog, i will make an update which between terraform plan and terraform apply action, there will be an approval action. The goal if the plan is not expected, we can cancel the workflow and terraform apply will not be processed.Here is the diagram that used in this blog:PrerequisitesHere is the prerequisite that needed to follow configurations discussed in this blog:CodeCatalyst space and project that has been connected with AWS account and source repository. In this blog, I use GitHub as source repository. For configuration reference, please check my other blog \u201cBuild and Release Container Image to Amazon Elastic Container Registry (ECR) via Amazon CodeCatalyst\u201dConfiguration StepsDirectory StructureBelow is directory structure used in this blog:. \u251c\u2500\u2500 .codecatalyst/ \u2502   \u2514\u2500\u2500 workflows/ \u2502       \u2514\u2500\u2500 tf-sandbox-sbx0-vpc.yml \u2514\u2500\u2500 sandbox/     \u2514\u2500\u2500 sbx0/         \u2514\u2500\u2500 vpc/             \u251c\u2500\u2500 backend.tf               \u251c\u2500\u2500 main.tf             \u251c\u2500\u2500 terraform.tfvars             \u2514\u2500\u2500 variables.tfEnter fullscreen modeExit fullscreen modeCodeCatalyst Workflow File ConfigurationsHere is the configuration of CodeCatalyst workflow file that written in file./codecatalyst/workflows/tf-sandbox-sbx0-vpc.ymlName:tf-sandbox-sbx0-vpcSchemaVersion:\"1.0\"Triggers:-Type:PUSHBranches:-masterFilesChanged:-sandbox\\/sbx0\\/vpc\\/.*Actions:terraform-plan:Identifier:aws/build@v1Inputs:Sources:-WorkflowSourceEnvironment:Name:sandboxConnections:-Name:lanandra-sandboxRole:codecatalyst-adminConfiguration:Container:Registry:DockerHubImage:hashicorp/terraform:1.8.2Steps:-Run:cd sandbox/sbx0/vpc-Run:terraform fmt -check -no-color-Run:terraform init -no-color-Run:terraform validate -no-color-Run:terraform plan -no-color -input=falseCompute:Type:EC2wait-for-approval:Identifier:aws/approval@v1DependsOn:-terraform-planConfiguration:ApprovalsRequired:1terraform-apply:DependsOn:-wait-for-approvalIdentifier:aws/build@v1Inputs:Sources:-WorkflowSourceEnvironment:Name:sandboxConnections:-Name:lanandra-sandboxRole:codecatalyst-adminConfiguration:Container:Registry:DockerHubImage:hashicorp/terraform:1.8.2Steps:-Run:cd sandbox/sbx0/vpc-Run:terraform init -no-color-Run:terraform apply -auto-approve -no-color -input=falseCompute:Type:EC2Enter fullscreen modeExit fullscreen modeNext, I will explain more detail each sections that defined on workflow file above:Define workflow name. Then define how workflow will be ran. Workflow will be ran automatically if there is a push to repository, specifically to directorysandbox/sbx0/vpcName:tf-sandbox-sbx0-vpcSchemaVersion:\"1.0\"Triggers:-Type:PUSHBranches:-masterFilesChanged:-sandbox\\/sbx0\\/vpc\\/.*Define action. First action will run workflowterraform plan. In this section, action uses identifieraws/build@v1. Then action will be ran on sandbox environment that has been connected with AWS account and also has IAM role associated with that account. InConfigurationsection, has been defined that action will be ran on top of container using specific version of terraform public image that pulled from DockerHub. Also has defined working directory and several terraform commands which includeterraform plan. Lastly, define compute type which isEC2Actions:terraform-plan:Identifier:aws/build@v1Inputs:Sources:-WorkflowSourceEnvironment:Name:sandboxConnections:-Name:lanandra-sandboxRole:codecatalyst-adminConfiguration:Container:Registry:DockerHubImage:hashicorp/terraform:1.8.2Steps:-Run:cd sandbox/sbx0/vpc-Run:terraform fmt -check -no-color-Run:terraform init -no-color-Run:terraform validate -no-color-Run:terraform plan -no-color -input=falseCompute:Type:EC2Next action iswait-for-approval. Goal of this action is to verify workflow can be ran to next action if there is an approval from privileged user(s). This action is using identifieraws/approval@v1and depends on previous action which isterraform-plan. This action need at least 1 privileged user approval so that next action can be ran.wait-for-approval:Identifier:aws/approval@v1DependsOn:-terraform-planConfiguration:ApprovalsRequired:1Next action that defined is action to runterraform apply. This action more or less similar with actionterraform-plan, the difference is just terraform command that declared on configuration. In this section, action will run commands that needed forterraform apply. This action is depends on previous action which iswait-for-approvalterraform-apply:DependsOn:-wait-for-approvalIdentifier:aws/build@v1Inputs:Sources:-WorkflowSourceEnvironment:Name:sandboxConnections:-Name:lanandra-sandboxRole:codecatalyst-adminConfiguration:Container:Registry:DockerHubImage:hashicorp/terraform:1.8.2Steps:-Run:cd sandbox/sbx0/vpc-Run:terraform init -no-color-Run:terraform apply -auto-approve -no-color -input=falseCompute:Type:EC2Example of Approval Process in CodeCatalyst WorkflowI have defined Terraform codes that will be used as example in this blog. Files can be found in this GitHub path:https://github.com/lanandra/mock-aws-infrastructure/tree/master/sandbox/sbx0/vpcExistingmain.tfconfiguration:########VPC########data\"aws_availability_zones\"\"available\"{}locals{azs=slice(data.aws_availability_zones.available.names,0,3)tags={\"environment\"=\"${var.env_name}\"\"managedBy\"=\"terraform\"}}module\"sbx0_apse1\"{source=\"terraform-aws-modules/vpc/aws\"version=\"5.8.1\"name=var.env_namecidr=var.vpc_cidrazs=local.azsprivate_subnets=[fork,vinlocal.azs:cidrsubnet(var.vpc_cidr,8,k)]public_subnets=[fork,vinlocal.azs:cidrsubnet(var.vpc_cidr,8,k+3)]enable_nat_gateway=falsesingle_nat_gateway=truetags=local.tags}Enter fullscreen modeExit fullscreen modeHere are the configuration steps:Change Terraform configuration. Enable NAT gateway in VPCenable_nat_gateway=trueAs defined in workflow file, all changes inside directorysandbox/sbx0/vpcwill trigger workflow run. Go to CodeCatalyst web console to verify. Go to space and project where workflow ran. Go to CI/CD menu, then choose Workflows. Verify there is new runs. Go to that runsGo to run details by clicking name or run. Next, we will be redirected to overview page of that run. Click actionterraform-plan, on the right side will appear tray box that display all steps that configured on that action. If we want to see details ofterraform plan, go to tablog, then click stepterraform plan -no-color -input-false. Verify action is succeededNext, actionwait-for-approvalwill be ran and waiting input from privileged user, whether workflow run can be approved or notChoose approve then click submitIf pop-up approval appeared, clickConfirmto continue workflowWe also can see the status of approval, whether it has been approved or rejectedAfter approval has been approved, next action will be ran where in this blog example is terraform apply. Verify terraform apply is succeededSummaryWe have reached the last section of this blog. Here are some key takeaways that can be summarized:In some use cases, maybe we will find condition where we will need approval before deployment in CI/CD pipeline can be doneCodeCatalyst have approval feature that can gatekeep deployment in CI/CDPlease comment if you have any suggestions, critiques, or thoughts.Hope this article will benefit you. Thank you!"}
{"title": "Configura\u00e7\u00e3o do Elastic Beanstalk para melhorar a toler\u00e2ncia a falhas e o dimensionamento autom\u00e1tico", "published_at": 1715738589, "tags": ["aws", "beanstalk", "cloudcomputing", "cloudoperations"], "user": "Carlos Filho", "url": "https://dev.to/aws-builders/configuracao-do-elastic-beanstalk-para-melhorar-a-tolerancia-a-falhas-e-o-dimensionamento-automatico-225l", "details": "Ol\u00e1 a todos, o artigo de hoje, como voc\u00ea viu no t\u00edtulo, trata da configura\u00e7\u00e3o do Elastic Beanstalk, especificamente do balanceador da AWS e do n\u00famero de inst\u00e2ncias do TagretGroup, dependendo das condi\u00e7\u00f5es de que precisamos.Introdu\u00e7\u00e3o ao Elastic BeanstalkO Elastic Beanstalk \u00e9 uma plataforma de servi\u00e7os de alto n\u00edvel que automatiza a implanta\u00e7\u00e3o de aplicativos na nuvem. O EB configura automaticamente a infraestrutura e os recursos necess\u00e1rios, como inst\u00e2ncias EC2 (servidores virtuais), grupos de seguran\u00e7a, objetos S3 para armazenar arquivos de vers\u00e3o de aplicativos, bancos de dados RDS, balanceadores de carga Elastic Load Balancing e muito mais para garantir que o aplicativo seja lan\u00e7ado com o m\u00ednimo de tempo e esfor\u00e7o por parte do desenvolvedor.Os principais componentes do Elastic Beanstalk s\u00e3o:Aplicativo \u00e9 a unidade b\u00e1sica. Primeiro, voc\u00ea cria um aplicativo que serve como um cont\u00eainer para suas vers\u00f5es e ambientes.Um ambiente \u00e9 uma vers\u00e3o do aplicativo que \u00e9 implantada em uma infraestrutura espec\u00edfica do AWS e serve a uma fun\u00e7\u00e3o espec\u00edfica, como um ambiente de teste ou de produ\u00e7\u00e3o.Recursos com os quais estaremos lidando:EC2 (Elastic Compute Cloud) s\u00e3o servidores virtuais que podem ser locados na nuvem da AWS para executar aplicativos.Grupo de destino (Target Group) \u00e9 um conceito usado na AWS para gerenciar o tr\u00e1fego no n\u00edvel do aplicativo. Os Target Groups permitem que voc\u00ea especifique exatamente para onde o tr\u00e1fego que chega ao Load Balancer deve ser direcionado. Cada Target Group est\u00e1 associado a um recurso espec\u00edfico, como EC2 ou fun\u00e7\u00f5es lambda.Os Auto Scaling Groups permitem que o n\u00famero de inst\u00e2ncias do EC2 seja dimensionado automaticamente em resposta a altera\u00e7\u00f5es na carga ou em outros fatores.Ap\u00f3s uma breve descri\u00e7\u00e3o do EB, vamos ao t\u00f3pico do artigo.Introdu\u00e7\u00e3o e desafiosImagem 1O aplicativo estava sendo executado em uma \u00fanica inst\u00e2ncia c5.xlarge e todas as verifica\u00e7\u00f5es de status do AWS mostravam \u201cSucesso\u201d, a m\u00e1quina estava no estado \u201cEm execu\u00e7\u00e3o\u201d, mas o aplicativo n\u00e3o estava respondendo. Depois de examinar os logs do servidor, percebemos que o problema era devido \u00e0 reinicializa\u00e7\u00e3o do IIS (esse \u00e9 um procedimento padr\u00e3o - geralmente acontece a cada 29 horas, mas voc\u00ea tamb\u00e9m pode definir o per\u00edodo individualmente).Elastic BeanstalkO Elastic Beanstalk n\u00e3o foi projetado para funcionar com uma \u00fanica m\u00e1quina, ent\u00e3o, criamos esse est\u00e1gio:2x c5.large (c5.large \u00e9 exatamente 2 vezes mais barato que c5.xlarge);Se em 5 minutos a carga da CPU for >80%, adicionaremos outra inst\u00e2ncia.Se a carga da CPU for <40% em 5 minutos, removeremos uma inst\u00e2ncia.Imagem 2Resources:   AWSEBAutoScalingGroup:     Type: \"AWS::AutoScaling::AutoScalingGroup\"     Properties:       MinSize: 2       MaxSize: 3    AWSEBCloudwatchAlarmHigh:     Type: \"AWS::CloudWatch::Alarm\"     Properties:       EvaluationPeriods: 1        MetricName: CPUUtilization       Namespace: AWS/EC2       Period: 300       Statistic: Average       ComparisonOperator: GreaterThanOrEqualToThreshold       Threshold: 80       Unit: Percent       AlarmActions:         - Ref: \"AWSEBAutoScalingScaleUpPolicy\"    AWSEBCloudwatchAlarmLow:     Type: \"AWS::CloudWatch::Alarm\"     Properties:       EvaluationPeriods: 1       MetricName: CPUUtilization       Namespace: AWS/EC2       Period: 300       Statistic: Average       ComparisonOperator: LessThanOrEqualToThreshold       Threshold: 40       Unit: Percent       AlarmActions:         - Ref: \"AWSEBAutoScalingScaleDownPolicy\"    AWSEBAutoScalingScaleUpPolicy:     Type: \"AWS::AutoScaling::ScalingPolicy\"     Properties:       AdjustmentType: ChangeInCapacity       AutoScalingGroupName:         Ref: \"AWSEBAutoScalingGroup\"       Cooldown: 360       ScalingAdjustment: 1Enter fullscreen modeExit fullscreen modeO principal problema com essa solu\u00e7\u00e3o \u00e9 que ela n\u00e3o funcionar\u00e1 como pretendido - por padr\u00e3o, o AWS presume que, se todas as verifica\u00e7\u00f5es de status em uma inst\u00e2ncia forem aprovadas, n\u00e3o h\u00e1 problema em permitir o tr\u00e1fego nela. O que \u00e9 errado para o nosso estudo de caso - precisamos adicionar nosso pr\u00f3prio HealthCheck.Vamos come\u00e7ar!Configura\u00e7\u00e3oPara a configura\u00e7\u00e3o, precisamos de um ID de ambiente e dessas guias na barra lateral do EC2, procure por Target Groups e posteriormente vamos em Auto Scaling Groups:Imagem 3Vamos seguir a ordem:Target Groups Aqui precisamos definir a configura\u00e7\u00e3o do Target Group de acordo com nossos crit\u00e9rios de tempo de atividade. Em nosso caso, consideramos suficiente detectar uma m\u00e1quina n\u00e3o saud\u00e1vel ap\u00f3s 1 minuto: 3 solicita\u00e7\u00f5es a cada 15 segundos + timeouts (5 segundos) = 1 minuto.Imagem 4Al\u00e9m disso, na guia Attributes (Atributos), h\u00e1 um par\u00e2metro importante: Deregistration delay (Atraso no cancelamento do registro). Quando um registro deixa de responder no /hc, ele deve ser substitu\u00eddo no tempo especificado nesse par\u00e2metro:Imagem 5Resources:   AWSEBV2LoadBalancerTargetGroup:     Type: \"AWS::ElasticLoadBalancingV2::TargetGroup\"     Properties:       HealthCheckIntervalSeconds: 15       HealthCheckPath: /hc       HealthCheckProtocol: HTTP       HealthCheckTimeoutSeconds: 5       HealthyThresholdCount: 5       UnhealthyThresholdCount: 3       TargetGroupAttributes:         - Key: deregistration_delay.timeout_seconds           Value: '20'Enter fullscreen modeExit fullscreen modeAuto Scaling GroupsHabilitar HealthChecks e definir per\u00edodo de car\u00eancia.O par\u00e2metro mais importante, sem o qual todas as configura\u00e7\u00f5es acima n\u00e3o ser\u00e3o aplicadas:Imagem 6Para adicionar verifica\u00e7\u00f5es de ELB, clique em \u201cAtivar verifica\u00e7\u00f5es de integridade do Elastic Load Balancing\u201d e defina o per\u00edodo de car\u00eancia da verifica\u00e7\u00e3o de integridade.O per\u00edodo de car\u00eancia da verifica\u00e7\u00e3o de integridade \u00e9 um per\u00edodo de espera durante o qual o AWS n\u00e3o tomar\u00e1 nenhuma medida para remover ou substituir inst\u00e2ncias rec\u00e9m-criadas (ou em execu\u00e7\u00e3o) em um Auto Scaling Group, mesmo que elas falhem na verifica\u00e7\u00e3o de integridade.Imagine que, acabamos de lan\u00e7ar um novo aplicativo e ele precisa de algum tempo para ser totalmente inicializado e come\u00e7ar a funcionar corretamente. Se o AWS come\u00e7asse a verificar imediatamente se ele est\u00e1 funcionando como deveria e ainda visse que algo est\u00e1 errado (porque o aplicativo ainda precisa de tempo para concluir todos os processos de inicializa\u00e7\u00e3o), o AWS o rotularia como n\u00e3o saud\u00e1vel e tentaria reinici\u00e1-lo ou criar uma nova inst\u00e2ncia.Imagem 7Selecionar o per\u00edodo de car\u00eancia ideal n\u00e3o \u00e9 a tarefa mais trivial. Simplesmente medimos quanto tempo leva, em m\u00e9dia, para que um novo aplicativo comece a enviar 200 solicita\u00e7\u00f5es. Para o nosso exemplo, esse tempo \u00e9 inferior a 8 minutos, portanto, definimos o valor de 500 segundos com uma reserva.Aten\u00e7\u00e3o:Voc\u00ea deve levar em conta diferentes fatores que podem aumentar o tempo de carregamento do aplicativo: migra\u00e7\u00f5es de banco de dados ou atualiza\u00e7\u00f5es importantes do aplicativo. Nesses casos, voc\u00ea precisar\u00e1 aumentar o per\u00edodo de car\u00eancia manualmente.Resources:   AWSEBAutoScalingGroup:     Type: \"AWS::AutoScaling::AutoScalingGroup\"     Properties:       HealthCheckType: ELB       HealthCheckGracePeriod: 500Enter fullscreen modeExit fullscreen modePol\u00edtica de manuten\u00e7\u00e3o de inst\u00e2nciasTamb\u00e9m precisamos permitir que o ELB exceda o limite de N inst\u00e2ncias caso uma delas n\u00e3o esteja respondendo, mas ainda n\u00e3o tenha sido removida do EB. Ou seja, queremos permitir que uma inst\u00e2ncia seja adicionada preventivamente enquanto a que n\u00e3o est\u00e1 funcionando estiver sendo exclu\u00edda. Como resultado, teremos 1 inst\u00e2ncia a mais do que o permitido no EB de uma s\u00f3 vez. Para fazer isso, selecione o par\u00e2metro \u201cInstance maintenance policy\u201d (Pol\u00edtica de manuten\u00e7\u00e3o de inst\u00e2ncia) no Auto Scaling Group e defina-o como Launch before terminating (Iniciar antes de encerrar):Imagem 8Experimento (hora da verdade)Vamos verificar como o balanceador se comportar\u00e1 se pararmos propositalmente nosso aplicativo em uma inst\u00e2ncia (por exemplo, com o comando Stop-Service)Vamos alterar ligeiramente nossa configura\u00e7\u00e3o para o experimento - o n\u00famero m\u00e1ximo de inst\u00e2ncias ser\u00e1 2, mantendo os mesmos par\u00e2metros de CPU:Imagem 9Vemos inst\u00e2ncias de trabalho:Imagem 10Provisionei uma EC2 Windows Server para teste:Imagem 11Em um minuto (de acordo com nossas configura\u00e7\u00f5es de TargetGroups), veremos a seguinte imagem:Imagem 12A inst\u00e2ncia falhou logicamente no HealthCheck e agora todo o tr\u00e1fego vai para a \u00fanica inst\u00e2ncia restante.E, ao mesmo tempo (como lembramos, em 20 segundos), uma nova inst\u00e2ncia \u00e9 iniciada, mesmo antes de a anterior ser exclu\u00edda! Como resultado, vemos 3 inst\u00e2ncias (\u00e9 claro, sempre haver\u00e1 apenas 2 inst\u00e2ncias funcionando):Imagem 13Depois de excluir uma inst\u00e2ncia que n\u00e3o est\u00e1 funcionando, nosso sistema retorna a um estado est\u00e1vel de duas m\u00e1quinas funcionando.Isso tamb\u00e9m pode ser visto nos Eventos de Envio do EB:Imagem 14Espero que voc\u00ea tenha aprendido mais sobre a configura\u00e7\u00e3o do Elastic Beanstalk no AWS."}
{"title": "Multi-Cluster com Karmada [Lab Session]", "published_at": 1715733965, "tags": ["kubernetes", "karmada", "community", "aws"], "user": "Paulo Ponciano", "url": "https://dev.to/aws-builders/multi-cluster-com-karmada-lab-session-2dgm", "details": "What is Karmada?Karmada (Kubernetes Armada) is a Kubernetes management system that enables you to run your cloud-native applications across multiple Kubernetes clusters and clouds, with no changes to your applications. By speaking Kubernetes-native APIs and providing advanced scheduling capabilities, Karmada enables truly open, multi-cloud Kubernetes.Karmada aims to provide turnkey automation for multi-cluster application management in multi-cloud and hybrid cloud scenarios, with key features such as centralized multi-cloud management, high availability, failure recovery, and traffic scheduling.ArchitectureSource:https://karmada.ioArquitetura do LabDeploy da infraCluster 1 oupegasus:git clone https://github.com/paulofponciano/EKS-Istio-Karpenter-ArgoCD.gitEnter fullscreen modeExit fullscreen mode[!NOTE]Altere os valores emvariables.tfvarsse for necess\u00e1rio.No arquivonlb.tf, estamos informando um certificado do ACM. Altere para o seu certificado ou remova comentando as linhas 38, 39, 40 e descomentando a linha 37.Com certificado:resource\"aws_lb_listener\"\"ingress_443\"{load_balancer_arn=aws_lb.istio_ingress.arnport=\"443\"#protocol          = \"TCP\"protocol=\"TLS\"certificate_arn=\"arn:aws:acm:us-east-2:ACCOUNTID:certificate/bfbfe3ce-d347-4c42-8986-f45e95e04ca1\"alpn_policy=\"HTTP2Preferred\"default_action{type=\"forward\"target_group_arn=aws_lb_target_group.https.arn}}Enter fullscreen modeExit fullscreen modeSem certificado:resource\"aws_lb_listener\"\"ingress_443\"{load_balancer_arn=aws_lb.istio_ingress.arnport=\"443\"protocol=\"TCP\"# protocol        = \"TLS\"# certificate_arn = \"arn:aws:acm:us-east-2:ACCOUNTID:certificate/bfbfe3ce-d347-4c42-8986-f45e95e04ca1\"# alpn_policy     = \"HTTP2Preferred\"default_action{type=\"forward\"target_group_arn=aws_lb_target_group.https.arn}}Enter fullscreen modeExit fullscreen modetofu initEnter fullscreen modeExit fullscreen modetofu plan--var-filevariables.tfvarsEnter fullscreen modeExit fullscreen modetofu apply--var-filevariables.tfvarsEnter fullscreen modeExit fullscreen modeCluster 2 oupegasus-2:git clone https://github.com/paulofponciano/EKS-Istio-Karpenter.gitEnter fullscreen modeExit fullscreen mode[!NOTE]Altere os valores emvariables.tfvarsse for necess\u00e1rio.tofu initEnter fullscreen modeExit fullscreen modetofu plan--var-filevariables.tfvarsEnter fullscreen modeExit fullscreen modetofu apply--var-filevariables.tfvarsEnter fullscreen modeExit fullscreen modeDeploy do karmadagit clone https://github.com/paulofponciano/karmada.gitEnter fullscreen modeExit fullscreen modecdkarmadaEnter fullscreen modeExit fullscreen modeEntrando no contexto do clusterpegasus:aws eks update-kubeconfig--regionus-east-2--namepegasusEnter fullscreen modeExit fullscreen mode[!NOTE]Os comandos a seguir ser\u00e3o executados no clusterpegasus(Cluster 1), onde temos o Argo rodando.helm repo add karmada-charts https://raw.githubusercontent.com/karmada-io/karmada/master/chartsEnter fullscreen modeExit fullscreen modehelm repo updateEnter fullscreen modeExit fullscreen modehelm--namespacekarmada-system upgrade-ikarmada karmada-charts/karmada--create-namespaceEnter fullscreen modeExit fullscreen modeVerificando o deployment:kubectl get pods-nkarmada-systemEnter fullscreen modeExit fullscreen modeAgora j\u00e1 temos o controlplane do karmada rodando no clusterpegasus.Com o deploy, \u00e9 gerado um secret com okubeconfignecess\u00e1rio para conectarmos no controlplane do karmada:kubectl get secrets-nkarmada-system |grepkarmada-kubeconfigEnter fullscreen modeExit fullscreen modekubectl get secret-nkarmada-system karmada-kubeconfig-ojsonpath='{.data.kubeconfig}'|base64--decodeEnter fullscreen modeExit fullscreen modeCriando IRSA (Iam Role for Service Account)[!NOTE]Altere os valores deACCOUNTIDnaiam-policy-irsa-karmada.jsone no comandoeksctlabaixo.Isso refletir\u00e1 em umaService Accountdentro do clusterpegasus, usaremos ela montando em um pod do ubuntu como ponto de acessotempor\u00e1rioao controlplane do karmada.aws iam create-policy--policy-nameubuntu-admin-karmada\\--policy-documentfile://iam-policy-irsa-karmada.jsonEnter fullscreen modeExit fullscreen modeeksctl create iamserviceaccount--nameubuntu-admin-karmada\\--namespacekarmada-system\\--clusterpegasus\\--attach-policy-arnarn:aws:iam::ACCOUNTID:policy/ubuntu-admin-karmada\\--regionus-east-2\\--profiledefault\\--approveEnter fullscreen modeExit fullscreen modeNo lado AWS, isso reflete em uma IAM role que podemos adicionar nos dois clusters EKS.EKS IAM - Console (pegasus-2):Fa\u00e7a o mesmo para o clusterpegasus, onde o karmada controlplane est\u00e1 rodando.Acessando Karmada API-ServerVamos subir agora aquele pod do ubuntu-admin no clusterpegasus. No manifesto j\u00e1 est\u00e1 tudo definido para utilizar aService Accountque criamos mais acima.kubectl apply-fhttps://raw.githubusercontent.com/paulofponciano/karmada/main/ubuntu-admin-karmada.yamlEnter fullscreen modeExit fullscreen modekubectl get pods-nkarmada-system |grepubuntuEnter fullscreen modeExit fullscreen modeNesse momento, vamos entrar no container do ubuntu, que est\u00e1 rodando no clusterpegasus:kubectlexec-itubuntu-admin-karmada-nkarmada-system--/bin/bashEnter fullscreen modeExit fullscreen modeInstalando o kubectl karmada:curl-shttps://raw.githubusercontent.com/karmada-io/karmada/master/hack/install-cli.sh | bash-skubectl-karmadaEnter fullscreen modeExit fullscreen modeEntrando no contexto do clusterpegasus-2:aws eks update-kubeconfig--regionus-west-2--namepegasus-2--kubeconfig$HOME/.kube/pegasus-2.configEnter fullscreen modeExit fullscreen modekubectl get nodes--kubeconfig$HOME/.kube/pegasus-2.configEnter fullscreen modeExit fullscreen modeVamos fazer o mesmo para o clusterpegasus:aws eks update-kubeconfig--regionus-east-2--namepegasus--kubeconfig$HOME/.kube/pegasus.configEnter fullscreen modeExit fullscreen modekubectl get nodes--kubeconfig$HOME/.kube/pegasus.configEnter fullscreen modeExit fullscreen modeChecando acesso ao karmada apiserver:kubectl get all-A--kubeconfig/etc/karmada-kubeconfig/kubeconfigEnter fullscreen modeExit fullscreen modeJoin dopegasus-2no karmada:kubectl karmada--kubeconfig/etc/karmada-kubeconfig/kubeconfigjoinpegasus-2--cluster-kubeconfig=$HOME/.kube/pegasus-2.configEnter fullscreen modeExit fullscreen modeJoin dopegasusno karmada:kubectl karmada--kubeconfig/etc/karmada-kubeconfig/kubeconfigjoinpegasus--cluster-kubeconfig=$HOME/.kube/pegasus.configEnter fullscreen modeExit fullscreen modeChecando status dos clusters adicionados:kubectl--kubeconfig/etc/karmada-kubeconfig/kubeconfig get clustersEnter fullscreen modeExit fullscreen modeInstalando CLI do ArgoCD:curl-sSL-oargocd-linux-amd64 https://github.com/argoproj/argo-cd/releases/latest/download/argocd-linux-amd64install-m555 argocd-linux-amd64 /usr/local/bin/argocdrmargocd-linux-amd64Enter fullscreen modeExit fullscreen modeRecuperando o secret / password do Argo server e fazendo login:kubectl-nargocd get secret argocd-initial-admin-secret-ojsonpath=\"{.data.password}\"|base64-d;echoEnter fullscreen modeExit fullscreen modeargocd login argocd-server.argocd.svc.cluster.local:80--usernameadminEnter fullscreen modeExit fullscreen modeAdicionando o karmada como cluster no ArgoCD:argocd cluster add karmada-apiserver--kubeconfig/etc/karmada-kubeconfig/kubeconfig--namekarmada-controlplaneEnter fullscreen modeExit fullscreen modeDeploy com ArgoCDSe acessarmos a UI do Argo, que est\u00e1 rodando no clusterpegasus, veremos que o karmada est\u00e1 registrado como um cluster onde \u00e9 poss\u00edvel o Argo fazer deploy:Podemos agora aplicar um manifesto que ir\u00e1 definir uma nova fonte para o Argo buscar por deploys. Nesse caso, essa fonte \u00e9 umreposit\u00f3riono GitHub:kubectl apply-fkarmada-argo-app.yamlEnter fullscreen modeExit fullscreen modeComo j\u00e1 temos manifestos nesse reposit\u00f3rio (no path/app-manifests), o Argo j\u00e1 faz osyncentregando essas aplica\u00e7\u00f5es no controlplane do karmada e o karmada por sua vez, entrega nos dois clusters de acordo com o que for definido nos manifestos dePropagationPolicy:No clusterpegasuspodemos ver:kubectl get pods-owide |grepredisEnter fullscreen modeExit fullscreen modeClusterpegasus-2:kubectl get pods-owide |grepnginxEnter fullscreen modeExit fullscreen modeNo caso do deploy do RabbitMQ, podemos ver que existem replicas rodando nos dois clusters, quando olharmos os aquivos dePropagationPolicypoderemos entender.kubectl get pods-owide--contextarn:aws:eks:us-east-2:ACCOUNTID:cluster/pegasus |greprabbitmqEnter fullscreen modeExit fullscreen modekubectl get pods-owide--contextarn:aws:eks:us-west-2:ACCOUNTID:cluster/pegasus-2 |greprabbitmqEnter fullscreen modeExit fullscreen modeKarmada OverridePolicy e PropagationPolicyNoreposit\u00f3rioque o Argo est\u00e1 monitorando, podemos ver os manifestos do karmada e tamb\u00e9m o manifestos de deployment que usamos como exemplo.Exemplo RedisRegras de override e selector do deployment onde ser\u00e1 aplicado, no caso 'redis':apiVersion:policy.karmada.io/v1alpha1kind:OverridePolicymetadata:name:redis-opspec:resourceSelectors:-apiVersion:apps/v1kind:Deploymentname:redisoverrideRules:-targetCluster:clusterNames:-pegasus-2overriders:labelsOverrider:-operator:addvalue:env:skoala-dev-operator:addvalue:env-stat:skoala-stage-operator:removevalue:for:for-operator:replacevalue:bar:test-targetCluster:clusterNames:-pegasusoverriders:annotationsOverrider:-operator:addvalue:env:skoala-stage-operator:removevalue:bom:bom-operator:replacevalue:emma:sophiaEnter fullscreen modeExit fullscreen modeRegras de propaga\u00e7\u00e3o, selector do deployment e target cluster. Nesse caso de failover, esse deployment deve ser migrado para o cluster pegasus-2 caso o cluster pegasus entre em falha:apiVersion:policy.karmada.io/v1alpha1kind:PropagationPolicymetadata:name:redis-propagationspec:propagateDeps:truefailover:application:decisionConditions:tolerationSeconds:120purgeMode:NeverresourceSelectors:-apiVersion:apps/v1kind:Deploymentname:redisplacement:clusterAffinity:clusterNames:-pegasus-pegasus-2spreadConstraints:-maxGroups:1minGroups:1spreadByField:clusterEnter fullscreen modeExit fullscreen modeExemplo NginxapiVersion:policy.karmada.io/v1alpha1kind:OverridePolicymetadata:name:nginx-opspec:resourceSelectors:-apiVersion:apps/v1kind:Deploymentname:nginxoverrideRules:-targetCluster:clusterNames:-pegasus-2overriders:labelsOverrider:-operator:addvalue:env:skoala-dev-operator:addvalue:env-stat:skoala-stage-operator:removevalue:for:for-operator:replacevalue:bar:testEnter fullscreen modeExit fullscreen modeNeste caso, apenas o clusterpegasus-2foi definido em 'targetCluster':apiVersion:policy.karmada.io/v1alpha1kind:PropagationPolicymetadata:name:nginx-propagationspec:resourceSelectors:-apiVersion:apps/v1kind:Deploymentname:nginxplacement:clusterAffinity:clusterNames:-pegasus-2replicaScheduling:replicaDivisionPreference:WeightedreplicaSchedulingType:DividedweightPreference:staticWeightList:-targetCluster:clusterNames:-pegasus-2weight:1Enter fullscreen modeExit fullscreen modeExemplo RabbitMQapiVersion:policy.karmada.io/v1alpha1kind:OverridePolicymetadata:name:rabbitmq-opspec:resourceSelectors:-apiVersion:apps/v1kind:Deploymentname:rabbitmqoverrideRules:-targetCluster:clusterNames:-pegasus-2overriders:labelsOverrider:-operator:addvalue:env:skoala-dev-operator:addvalue:env-stat:skoala-stage-operator:removevalue:for:for-operator:replacevalue:bar:test-targetCluster:clusterNames:-pegasusoverriders:annotationsOverrider:-operator:addvalue:env:skoala-stage-operator:removevalue:bom:bom-operator:replacevalue:emma:sophiaEnter fullscreen modeExit fullscreen modeAqui temos algo diferente, onde os dois clusters s\u00e3o definidos em 'targetCluster', por\u00e9m com pesos (weights) diferentes, fazendo com que o karmada entregue as r\u00e9plicas de acordo com o peso de cada cluster:apiVersion:policy.karmada.io/v1alpha1kind:PropagationPolicymetadata:name:rabbitmq-propagationspec:resourceSelectors:-apiVersion:apps/v1kind:Deploymentname:rabbitmqplacement:clusterAffinity:clusterNames:-pegasus-pegasus-2replicaScheduling:replicaDivisionPreference:WeightedreplicaSchedulingType:DividedweightPreference:staticWeightList:-targetCluster:clusterNames:-pegasusweight:2-targetCluster:clusterNames:-pegasus-2weight:1Enter fullscreen modeExit fullscreen modeRemover ubuntu-admin e IRSAPodemos deletar o pod ubuntu que usamos para setup do karmada, e tamb\u00e9m a IRSA:kubectl delete-fubuntu-admin-karmada.yamlEnter fullscreen modeExit fullscreen modeeksctl delete iamserviceaccount--nameubuntu-admin-karmada\\--namespacekarmada-system\\--clusterpegasus\\--regionus-east-2\\--profiledefaultEnter fullscreen modeExit fullscreen modePara a pr\u00f3xima, vamos buscar um cen\u00e1rio total de DR com o karmada e ver at\u00e9 onde chegamos.Keep shipping!"}
{"title": "Looking to beef up the security of your AWS EC2 instances?", "published_at": 1715733168, "tags": ["ec2", "security", "aws", "englishcontent"], "user": "Carlos Filho", "url": "https://dev.to/aws-builders/looking-to-beef-up-the-security-of-your-aws-ec2-instances-4252", "details": "Check out our latest video where I dive into the top practices you need to implement now to protect your cloud environment."}
{"title": "How to Easily Implement easily RAG (Retrieval Augmented Generation) with Bedrock!", "published_at": 1715709238, "tags": ["cloudcomputing", "ai", "aws", "python"], "user": "Faye Ellis", "url": "https://dev.to/aws-builders/how-to-easily-implement-easily-rag-retrieval-augmented-generation-with-bedrock-1mo2", "details": "\ud83d\ude80 Here\u2019s how to securely provide your own custom data to Large Language AI models in Amazon Bedrock, using a Bedrock Knowledge Base.This is an example of how you can easily implement RAG (Retrieval Augmented Generation) with Bedrock!It works with lots of different document formats, including CSVs, Microsoft Word, plain text, PDFs, and more!Seeherefor full details of how to do this!"}
{"title": "How I built a Multiplayer App in 3 days", "published_at": 1715672002, "tags": ["aws", "react", "observability", "multiplayer"], "user": "Alvin Johansson", "url": "https://dev.to/aws-builders/how-i-built-a-multiplayer-app-in-3-days-5dd2", "details": "I thought it was time to create a follow up of my previous postGet your idea deployed to prod already. In the post I detail how you can quickly bootstrap an app and get it deployed and ready in no-time. This is a loose retelling of how I followed my own advice and got an idea to prod.The ideaJust before I went to sleep last Sunday I thought it would be fun to create an app for my friends and I where we can rate the Eurovision Song Contest participants live together. The idea was to have everyones votes being tallied up live as we make adjustments to the ratings. This led me into looking atReplicacheas the solution. I've been interested in trying it out for a while now and this was the perfect project for it. I quickly threw together a sketch onExcalidrawso I would have something to go on for tomorrow.BuildingAs the Monday began I started reading documentation and looking into the tech I wanted to use. For most of the stack I went with things I know. But for the multiplayer part I had to read up and understand the core concepts behind Replicache. While reading I came across their hosted serviceReflect. This was a perfect fit as time was of the essence and I would not have to build my own Replicache backend! It's completely free for 1000 user hours every month. As this project is supposed to be used during the 5 hours of Eurovision every year we can calculate the maximum amount of users we can have online for the full duration by simple division.1000hu / 5h = 200u200 users? We're totally good.Boostrapping the projectFor the project bootstrapping I chose to base my project on Reflect's own React template. This decision came from the idea that I wanted to play around with it a bit first and get a proper understanding of it before I try to implement my idea. While playing around with the template I made new mutators and subscribers and got a feeling of how the service works. Great start!Skeleton structureThe next step was setting up basic React and HTML elements to be the skeleton of my idea. I got all the fields I wanted up and a general structure of the main feature. Key here is not to put too much time into styling. I mostly used the css styles already included in the scaffolded project and added just a little bit for the rounded borders and layout to be possible.Implementing the main featureNow with the basic ui elements in place we can start implementing the features we care about. For me it was getting live updates when someone changes a rating. At first I built it without taking into consideration what song was being rated just to get something up. and running. And when I got that working I updated the data structure to handle the mutations based on id's and boom the main part of the application was working! I could now switch between different songs and add individual ratings to them while also seeing the updates in real time from another browser!Adding the other bare necessitiesNow that the main feature had been implemented I had two things left to do in regards to the bare necessities. I wanted some sort of \"login\" page and a scoreboard.\"Login\" pageThe login page would serve as a way for the user to identify themselves with a name and then join a specific room ID. I purposefully made this super simple, letting users choose whatever room ID they want and Reflect would handle creating the room gracefully with the users not noticing anything at all. For another user to join the same room they simply had to type the same room ID. This is possible since Reflect is usingCloudFlares Durable Objectsin the background, serverlessly handling the client states for me and I don't have to be worried about any cost as I'm within the limits of Reflect's free plan.ScoreboardThe scoreboard should show the aggregate total of all the scores from all the users, ranked after either order in the show or the max average. This could surely be built in a more sophisticated way with a table where you could sort based on any attribute but I kept it simple. I render the list based on how many contributions have been rated and calculate the average for each then sort it.StylingAll the functionality I intended is in place now! Time to make it look at least a little better. As I am not the best css:er nor very imaginative I gave a lot of the stylistic choices to ChatGPT. In essence I gave it the layout of the page with the current styles I had in place and then asked it to \"pimp my ride\"-it in the style of Eurovision. Color wise that seems to mean \"add gradients\". This works well enough for me!DeployingI was happy with this! Everything seems to work well while developing and it seems ready enough. First I created a production environment for my Reflect service by runningnpx reflect publish --app lagom-euroand added the generated url to my .env. Then I ransst initto addsst ionto the project. I added a StaticSite resource and ran a deployment to my prod AWS accountsst deploy --stage production. A production environment was now up and running!Testing and fixingThe site was up and available via CloudFront. I started to show people and tested it with a few devices. Found some unintended behaviors and things that could be clarified. I added som help text and a button to show and hide the help.DomainFinally I bought a domain for it. I continue to name my projects after my favorite Swedish word \"Lagom\" which translates to \"Just right\". I attached the domain to my sst resource and boom the site was now available fromhttps://lagomeurovision.com/!ObservabilityThis is a bit late in the game I admit, but better late than never! I figured it could be cool to know how many visitors and rooms I have on this small little project. I went ahead and created an environment for this project on myBaselimeaccount. Baselime is a serverless observability service, recently acquired by CloudFlare! I've used Baselime in a few of my smaller projects now and it's super nice to use. For this project I installed their React SDK and added a custom event that emits when a room is joined.I then set up a dashboard where I'd have an overview of the page stats. During the night I had 8 unique rooms and people joined a room 29 times. Very reasonably the site was mostly used on phones with the iPhone hogging the spotlight at 21 counts and Android following on 9. Stats are cool.Final take-awaysThese are my personal key take-aways from this project:Bootstrap your project to get up and running quicklyIn my case it meant using a Reflect scaffold to have the project structured from the startRead the documentationIt's way easier to build things if you understand how the technology worksStart with the main featureFunctionality first, style laterAsk for help if you have a question or get stuckI asked for help in both the Replicache Discord and Baselime Slack servers. In the Replicache server I got help from a community member. In the Baselime server I got help from one of the developers (Thanks Thomas!)Leverage ChatGPT for things you don't necessarily care for (my case, a lot of css)I like building for my friendsThanks!Thanks for taking the time to read or at least scroll through all of this. This post is both meant as an example of me eating my own dog food regarding my previous post but also as an exercise in writing. As a newer content creator/tech blogger/aws influencer I've yet to really find my voice. I think I enjoy this type of content quite a bit. I know I like reading about people's own little side projects or how someone built something very cool. If there is something you liked/disagreed/have a question about please write to me! A comment on the post or a direct message on LinkedIn or X are both welcome.Here's the final site.Here's the repo if you want to check it out.If you enjoyed this post you could follow me on \ud835\udd4f at@Paliago. I mostly engage with the serverless community and post pictures of my pets."}
{"title": "Issue 44 of AWS Cloud Security Weekly", "published_at": 1715642940, "tags": ["aws", "security", "cybersecurity", "cloudsec"], "user": "AJ", "url": "https://dev.to/aws-builders/issue-44-of-aws-cloud-security-weekly-plb", "details": "(This is just the summary of Issue 44 of AWS Cloud Security weekly @https://aws-cloudsec.com/p/issue-44<< Subscribe to receive the full version in your inbox weekly).What happened in AWS CloudSecurity & CyberSecurity last week May 06-May 13, 2024?Amazon Connect has introduced AWS CloudTrail integration for flow management pages on the Connect admin website. This means that whenever you add, modify, or remove a flow from a flow management page, the corresponding activity is recorded in AWS CloudTrail. This enables enhanced visibility, reporting, and compliance.Amazon Elastic Container Registry (ECR) has expanded its compatibility by integrating GitLab Container Registry as an endorsed upstream registry for ECR's pull-through cache functionality. With this latest update, GitLab users can seamlessly synchronize images from the newly incorporated upstream registry to their private ECR repositories.Trending on the news & advisories (Subscribe to the newsletter for details):Apple platform security May 2024 report- PDF.Zscaler responds to post by a threat actor claiming to have potentially obtained unauthorized information from a cybersecurity company.Threat Actor Claims Sale of Dell Database Containing 49 Million Customer Records.Ohio Lottery- Data breach notification.CISA and Partners Release Advisory on Black Basta Ransomware.Dell: Threat actor claims he scraped 49M Dell customer addresses before the company found out.White house- 2024 report on the cybersecurity posture of the united states."}
{"title": "Mastering the AWS Security Specialty (SCS) Exam - A Quick Guide", "published_at": 1715640438, "tags": ["aws", "cybersecurity", "certification", "beginners"], "user": "Damien J. Burks", "url": "https://dev.to/aws-builders/mastering-the-aws-security-specialty-scs-exam-a-quick-guide-2go0", "details": "Table of ContentsIntroductionWhy It's Essential to Start with the AWS Certified Solutions Architect AssociateUnderstanding the AWS Security Specialty ExamKey Points About the ExamKey Resources for Preparing the AWS Security Specialty ExamStephane Maarek's Ultimate AWS Certified Security Specialty CourseAWS Security Specialty Focus Labs by WhizlabsTutorialsDojo\u2019s Practice Exams and Cheat SheetsLeveraging Insights from Becky Weiss\u2019s TalkKey AWS Services to Focus OnConclusionIntroductionIn this article, I will share my journey on how to successfully pass the AWS Certified Security Specialty (SCS) exam. From my experience, this ranks as one of the toughest exams right after the AWS Certified DevOps Engineeer Professional (DOP) Certification. However, with the right approach and resources that I'll discuss today, you'll be well on your way to succeeding just like I did on my first attempt.Why It's Essential to Start with the AWS Certified Solutions Architect AssociateFirst things first, if you\u2019re aiming for the AWS Security Specialty certification, I highly recommend having the AWS Certified Solutions Architect Associate (SAA) under your belt. The foundational knowledge you gain from the Solutions Architect Associate is crucial. It not only prepares you with the basic principles of AWS architecture, but it also makes the uphill climb of the Security Specialty exam smoother.Understanding the AWS Security Specialty ExamAccording to AWS, the Security Specialty exam validates your ability to design and implement security solutions on AWS. It checks your understanding in specialized data classifications, data protections, and the architectures for implementing security controls. I've highlighted some of the key points about the exam below.Key Points About the ExamType of Questions:65 questions, both multiple choice and multiple response.Duration:170 minutes, nearly three hours to clear the hurdles.Cost:$300 USD.Testing Options:You can either take it online from the comfort of your home, or you can head to a testing center.For more details, you can check out theAWS official page.Also, be sure to review the exam guide as well. It is incredibly important for you to have a firm understanding of what you'll be tested over and what services will be omitted from the exam. You can find the exam guide here:Latest SCS Exam GuideNOTE: The exam details and guide can change at any time, so always make sure you defer to the official page for more information.Key Resources for Preparing the AWS Security Specialty ExamLet\u2019s dive into some top resources that helped me ace this exam. There are a total of four key resources that I highly recommend you all use to study for this exam(aside from the recommended whitepapers and such by AWS).NOTE: Click the images within each section to be redirected to the source as you continue to scroll down.Stephane Maarek's Ultimate AWS Certified Security Specialty CourseThis is one of the highest-rated courses out there for the Security Specialty exam. Stephane Maarek, known for his ability to demystify complex AWS concepts succinctly, has prepared around 16 hours of on-demand videos which are hosted on Udemy. The course also includes hands-on labs, essential for practical understanding. However, there is no sandbox environment provided, so you'll need to make sure you request your own AWS account. Using some AWS services during the course in your own account might cost a bit, but it\u2019s a worthwhile investment for your preparation.AWS Security Specialty Focus Labs by WhizlabsHands-on experience iscrucial, and that\u2019s why I also recommend the AWS Security Specialty Focus Labs offered by Whizlabs. With around 54 labs tailored to various exam domains like Threat Detection and Identity Access Management (IAM), these labs areSUPER invaluable. Unfortunately, these labs are not free. These labs are priced at $65.95, so be sure to keep an eye out for sales to grab a good deal!TutorialsDojo\u2019s Practice Exams and Cheat SheetsTutorialsDojo is another excellent resource that I constantly use for AWS exams. Their practice exams are known to mirror the actual exam\u2019s difficulty. In most cases, they are even harder than the actual exam. Prior to sitting for the exam, I highly recommend you are consistently scoring about an 80%. Consistently scoring above 80% on these can boost your confidence significantly prior to sitting for the exam, and the odds of you passing it on the first try are pretty high.Furthermore, their cheat sheets also provide detailed notes on services that you'll come across in the exam, which I highly recommend you leverage as much as possible.Leveraging Insights from Becky Weiss\u2019s TalkBecky Weiss\u2019s talk, The Fundamentals of AWS Cloud Security, is a treasure trove of information covering basic network security, access management policies, and data encryption. Her ability to simplify complex topics into digestible bits is what makes her session a must-watch.With that being stated, I implore you to watch this all the way through. I can guarantee you will have a much better understanding of how Cloud Security works within AWS and how to best leverage AWS services to protect your assets.Key AWS Services to Focus OnBefore you step into the examination room, youmustensure you\u2019re well-versed in several critical AWS services. You'll want to payveryclose attention to:Identity Access and Management (IAM)Key Management Service (KMS)CloudWatchCloudTrailGuardDutyInspectorOrganizationsMacieWAFSecurityHubDetectiveVirtual Private Cloud (VPC)ConfigSystems Manager (Parameter Store)I am almost certain that you're going to see all of these on the exam, so make sure youpay close attentionto each of these, and also, get some lab time in as well.ConclusionThank you for following along in this guide. I hope it not only prepares you well for the AWS Security Specialty exam, but it also inspires you to leverage the power of AWS in securing applications and data. Until next time, happy studying, keep securing those networks, and stay curious!Disclaimer:This blog post reflects my personal experiences and opinions. This blogs original content is based off of the followingYouTube Video:All images located in the blog post have been sourced from different places. Click on the image to get redirected to the original source."}
{"title": "Spring Boot 3 application on AWS Lambda - Part 5 Introduction to AWS Lambda Web Adapter", "published_at": 1715612477, "tags": ["aws", "java", "serverless", "springboot"], "user": "Vadym Kazulkin", "url": "https://dev.to/aws-builders/spring-boot-3-application-on-aws-lambda-part-5-introduction-to-aws-lambda-web-adapter-m21", "details": "IntroductionDuring the parts2,3and4we introduced the concepts behind the AWS Serverless Java Container and especially its variant for the Spring Boot (3) application, learned how to develop, deploy, run and optimize Spring Boot 3 Application on AWS Lambda using AWS Serverless Java Container. In this part of the series we'll introduce another alternative, the tool calledAWS Lambda Web Adapterand the concepts behind it.AWS Lambda Web AdapterAWS Lambda Web Adapter is a tool written in Rust programming language (which is very fast) to run web applications on AWS Lambda. It allows developers to build web apps (HTTP API) with familiar frameworks (e.g. Express.js, Next.js, Flask,Spring Boot, ASP.NET and Laravel, anything that speaks HTTP 1.1/1.0) and run it on AWS Lambda. The same Docker image can run on AWS Lambda, Amazon EC2, AWS Fargate, and local computers *.To its most important features belong :-Run web applications on AWS Lambda-Supports Amazon API Gateway Rest API and Http API endpoints, Lambda Function URLs, and Application Load Balancer-Supports Lambda managed runtimes, custom runtimes and docker OCI images-Supports any web frameworks and languages, no new code dependency to include-Automatic encode binary response-Enables graceful shutdown-Supports response payload compression-Supports response streaming as Lambda Web Adapter invoke mode. The default mode is \"buffered\". When configured as \"response_stream\", Lambda Web Adapter will stream response to Lambda service. This makes sense to use to reduce the time to first byte when a big file or video will be streamed to the client. You can read a deep dive to this featurehereandhere. Using response streaming we can bypass AWS Lambda service quota for the maximum size of an incoming synchronous invocation request or outgoing response which is 6 MB.-Supports non-http event triggers. It supports all non-HTTP event triggers, such as SQS, SNS, S3, DynamoDB, Kinesis, Kafka, EventBridge, and Bedrock Agents.-Supports readiness check port/path and traffic port can be configured using environment variables. When a new Lambda Execution Environment starts up, Lambda Web Adapter will boot up as a Lambda Extension, followed by the web application. Lambda Web Adapter will retry this request every 10 milliseconds until the web application returns an HTTP response (status code >= 100 and < 500) or the function times out. After passing readiness check, Lambda Web Adapter will start Lambda Runtime and forward the invokes to the web application.-Supports local debugging. Lambda Web Adapter allows developers to develop web applications locally with familiar tools and debuggers: just run the web app locally and test it. If we want to simulate Lambda Runtime environment locally, we can use AWS SAM CLI withsam localcommand.You can find a very good introduction video to the AWS Lambda Web Adapter which Harold Sun gave at theRe:Invent 2023.AWS Lambda Web Adapter can be deployed in several ways:-Lambda functions packaged as Docker Images-Lambda functions packaged as OCI Images-Lambda functions packaged as Zip package forAWS managed runtimesand attached as Lambda layer to your Lambda function. This is how we'll deploy AWS Lambda Web Adapter for our sample Spring Boot 3 application in the next part of the series.ConclusionIn this part of the series, we introduced AWS Lambda Web Adapter and concepts behind it. In next part we'll learn how to develop deploy and run the Serverless application (on AWS Lambda) using this tool.*SourceAWS Lambda Web Adapter"}
{"title": "How to Add Paid Features to Your SaaS Apps", "published_at": 1715605200, "tags": ["saas", "aws"], "user": "Jason Wadsworth", "url": "https://dev.to/aws-builders/how-to-add-paid-features-to-your-saas-apps-1fjb", "details": "Welcome! This post focuses on implementing feature tiers in SaaS applications, rather than payment processing tools like Stripe or Square. If you're interested in learning about tiers and managing features for different customer levels, read on!If you are still here, great! Let's get into what I am going to talk about.Why should you consider tiers in your SaaS application?How can you manage which customers have what tiers/features?How do you add it to your code?How do you account for the \u201dnoisy neighbor\u201d problem?How do you make sure you\u2019re not losing money?Let's get startedWhat is the point of tiers?When talking about a SaaS application you'll often hear about tiers or levels. The idea is simply that you have different features that each tier has access to in your application. Usually, the tiers are progressive, so someone in the second tier would get everything included in the first tier plus something more. Why might you want to include multiple tiers?Increase AdoptionIf you have an application for which you are currently charging money then adding a lower tier, whether free or just cheaper, you can enable increased adoption of your app. That increased adoption enables some of the next points. If nothing else, more people using your app means more people being aware of your app, which is a good thing.Upsell OpportunitiesWhether you're adding a lower tier to increase the number of users or adding a paid, or higher cost, tier, the point of doing it is to upsell. You have users who are currently using your app; hopefully getting value out of it at whatever tier they are in. By adding another tier you have the chance to convert that user into a paid, or higher-paying, user.LearningOne thing that is often overlooked when considering a free tier is the value of what you can learn by having more users using your app. If you've ever worked in a startup, or just have worked on an app with limited users, you know how challenging it can be to get feedback, and then to understand what value to put on the feedback you get. With a small set of users, you aren't sure if the request from one or two users is really valuable or just valuable to them. As you increase the number of users in your app you have a chance to hear from a larger audience. That means a bigger sample size and more meaningful data.Keep this one in mind if you are deciding whether or not to hold on to the lower tier(s) of your app.It's also important to always understand your users. Users at different tiers sometimes have different needs. Be sure to weigh your feedback in light of who you are getting the feedback from and test any hypothesis against all of your user personas.It's also important to always understand your users. Users at different tiers sometimes have different needs. Be sure to weigh your feedback in light of who you are getting the feedback from and test any hypothesis against all of your user personas.EnablementSpeaking of understanding user needs, one last point on the \"why\" of offering tiers is that it can enable you to do things that you can't do for free, or can't do at the price of your current app. Imagine you want to add a cool AI feature to your app. It sounds like a great idea, but you quickly realize that giving it away for free is too expensive. By adding a new tier you can charge a fee that makes adding that feature an option. This can be true even if you aren't looking to make money; you can price it in a way that at least covers the cost.Tier ManagementNow that we understand some of the reasons why you might want to add tiers, how do you manage them?I'll start with some things you shouldn't do...Don't Build Your Own ToolsAs engineers, we always think we can do that. We'll look at some problem and decide that it's not a hard problem to solve and we'll go off and solve it. Don't fall into this trap. Engineering resources are precious; spend them on things that add value to your app, your users.Don't Make it ComplicatedI'm always telling people to stop making things more complicated than it needs to be. Solve the problem that is in front of you, not one that you might have later. This is true with tier management. In its simplest form it's just a true/false; does this tenant have this tier. It can become more complex later, as you learn, but start simple...always.There are, of course, some things you can/should do as well...Do Use 3rd Party ToolsThis kind of goes without saying since it's the opposite of the first don't I listed, but it's worth restating and giving some examples. Using tools from third parties means taking advantage of what they have done so you don't have to do that work. This means you are free to build things that make your app special. I like to use feature flag tools for this. Some examples areLaunchDarkly,Split, andAWS App Config. I can't say I've used App Config, but the principles behind all of these are about the same; you pass in some bit of information and it tells you if the thing is on/off. It's a simple way to get tier management without a lot of work. Plus, if you aren't using feature flags in your app already you really should consider them. That's probably worth a blog post of its own.Start With Options That Don't ScaleThis is something you'll hear a lot in the startup space, but it's true everywhere. When you are doing something new you don't know how successful it will be. Don't spend time building things to make your life easier when it does before knowing if it will. It's okay if the first implementation of your tier management is someone going into your feature flag tool and manually changing values. You can track your billing in Excel when you're just getting started. Sure, those options are going to be painful if you are successful, but that's the point when you should increase the automation; not before.Build As You GrowEventually, those things that don't scale will be pain points, for you and possibly for your users. As that happens start to build. Add something to automatically set values in the feature flag tool using their APIs. Send out invoices automatically with some sort of billing software. Even an internal UI that makes tier management a little easier can be a quick win that improves things just long enough to get to the next level of scale.How Do You Make It Work?At some point, you have to start putting something in the code. Here are some things to keep in mind when you do so.Focus On Features, Not TiersWe've been talking about tiers a lot, but what tiers really are is a collection of features. When you are adding features to your code you should mostly be thinking in terms of those features, not the tiers themselves. We've all seen pages that look something like the following:Each tier shows you what features are included. Imagine if you want to move a feature from the basic tier to the free tier. If you focus on tiers then you have to go into the code and make that change. That doesn't sound so bad, after all, it's just one feature, so just one place in the code. What if you decided to add a whole new tier? Now you have to find every feature throughout the code and make sure the new tier is included in all the right spots. This makes changing tiers difficult and limits your sales and marketing options.In addition to allowing you to change tiers, taking a feature-based approach allows you to grandfather in users when you make changes, and even do ala-cart sales where individual features are added for particular customers.Don't Confuse Permissions and FeaturesThere is some crossover between permissions and features, so it's easy to think they can be seen as the same. Both may result in a 403 - forbidden response from an HTTP call, after all. While they do have things in common, they are different.Permissions will often go beyond the high-level \"can you access this feature?\" and into object-level permissions. A user may be allowed to access the files feature, but may only be able to see certain files.Features, on the other hand, will often go into application flow. You may have permission to access the search feature, for example, but a feature flag may determine whether you use the standard search or the AI-based search. There isn't a permission issue, it's just a different path within the code.Use Feature FlagsI already touched on this a bit; feature flags are a great way to determine what features a user/tenant can access. The code snippet below shows a quick example of what it might look like to evaluate whether a user can access a particular API. In this example, we have a Lambda function that is being called by AppSync. Our AppSync is using a custom authorizer where we are adding thetenantIdanduserIdto theresolverContext. All the code does is make a call to the feature flag service to determine the flag value for the given context. We use theuserIdas the key, but the important data point here is thetenantId. That's the value that we'll have rules for to determine whether the value is true or false. If it's false we'll simply throw an error and we're done. If it's true then it does what it would normally do. Again, keep it simple to start with. Many of the feature flag tools have capabilities beyond simple true/false evaluation, but that's all you need to get started.Make Sure the UI is AwareWhen you're building a UI that has different levels of permissions it's a good practice to hide things from users that they can't do. This limits confusion and generally creates a better user experience. How many of you have clicked a button in the AWS console only to be given an error message saying you aren't allowed. Not a great experience.When dealing with tiers and features you want to take a different approach. As we've already said, permissions and features are not the same. When you have a feature that is available to the user at a different tier you want the UI to show that to them. That doesn't mean it should look like you can do something and it will give you an error when you try it. Let's avoid rebuilding the AWS console experience. But you can grey out a button and show a message when the user hovers over it.Making sure your UI is aware is how you upsell. You have users in the app who may not realize what additional capabilities your app offers at higher pricing tiers. Tell them!Dealing With Noisy NeighborsWhen you build a SaaS application you're most often hoping to get some cost benefits from having your different tenants share resources. This creates the opportunity for what is referred to as the noisy neighbor problem. It happens when one tenant is impacted by or is impacting other tenants. In a multi-tier application, this can happen in several ways, and its impact can be made worse when paid customers feel like the free-tier tenants are causing the system to slow down. This is particularly noticeable if you add a free tier and suddenly everything is worse.There are things you can do to help.Rate LimitingThere is a reason why every API, every service, in AWS has limits. The main reason is that AWS is IaaS/PaaS. Those share the same noisy neighbor problem as a SaaS app. Rate limiting allows you to limit how much any one user or tenant can use your system. By controlling the rate at which tenants can use your application you can avoid becoming overwhelmed by a single tenant.In AWS one way to achieve this is by utilizingusage plansin API Gateway. With a usage plan, you can set the maximum rate for calls with the same API key. The nice thing is that you can have different usage plans so you can have different limits for different tiers. You might want your paid customers to be able to hit your APIs more frequently than the free ones, and usage plans make that easy to do.If you aren't using API Gateway (REST API to be specific) your options are a bit more limited. You can get some benefit fromWAF, though it's not really designed to be tenant-based. Still, it can help. Beyond that, you're mostly on your own. Keep in mind that anything you implement in your code is already sharing some amount of resources. Let's just hope AWS decides to add it to other places, like AppSync, in the near future.Segmented QueuesIf you've ever gone to an amusement park you've seen how queues work. You stand in line and wait for your turn. In a SaaS application, you may want to have your paid customers going through a different queue than your free customers. Think of this like the fast pass at an amusement park. The fast pass is still a queue. It goes to the same ride. It just has fewer people in it, so you get your turn faster. You can do the same thing by sending your data to different queues, one for the free tier and one for the paid. The great thing about this is that the same Lambda function is used by both. And because you can set the concurrency at the integration you can have each queue processing messages at different speeds. You can even go so far as to set up a queue for a single tenant.Tenant PartitioningAs your application grows you may want to have a multi-account strategy in place. You may start by thinking you can put all your free tier customers in one AWS account and your paid customers in another. While this strategy does make sure your free tier tenants aren't impacting your paid tier tenants it does create a couple of issues.First, it doesn't necessarily balance the workload. You may have some paid tenants who aren't doing much and some free tenants who do a lot. You really want a balance so that each account is doing a similar amount of work. That allows you to keep the account settings the same, making it easier to manage.Second, tenants are, hopefully, moving between tiers (hopefully going from free to paid). If you isolate the tenants by tier you need to have a migration plan in place for when a tenant changes tiers. That can be expensive and complicated, especially if you want the tenant to keep working while you migrate them.Instead of separating by tier, I like to do a weighted assignment approach. As a new tenant is added to the application the system determines where that tenant should go based on the usage of the current tenants. You can't know how the new tenant is going to behave but you can at least understand how the existing ones do and use that information to decide where to put a new tenant.MonitoringNo matter how you decide to manage the noisy neighbor problem the one you you really should do is have monitoring in place. It's important to understand how your system is behaving at all times so you can make adjustments before your customers start to complain. Monitor things like latency, iterator age, message age, concurrency, and anything else that can impact the performance of your application. Use AWS tools at a minimum.Understanding Your CostEven if you aren't adding multiple tiers to your app you really should try to understand the costs of each tenant/user. When you're considering adding tiers, either up or downstream, this information is crucial in understanding how much to charge and what features belong in what tier.Custom MetricsYou can use the AWS metrics to get a lot of information, but it does have its limits. When you can't get what you need from the out-of-the-box metrics create your own. Add it anywhere you have a feature that you want to understand better. If nothing else just record that the feature was used. Make sure you are recording metrics for anything that you might want to give away. You don't want to be surprised by the cost of something you gave away for free.Include Tier and TenantWhen you are adding your custom metrics be sure to include both the tier and tenant identifier to the metric as dimensions. This will allow you to look at the data aggregated by tier as well as to see how an individual tenant is using your app. It also can allow you to exclude a tenant if you believe their behavior is an anomaly.Gather Real Use DataSome APIs, like DynamoDB, return the actual usage with each call. For S3 you can see the size of the objects via EventBridge. Record that information, with tier and tenant added, so that you can see how much things will actually cost. While you can get this information from the generic AWS metrics you won't be able to see it by tier or tenant.RecapThere are so many things to think about as you make decisions about adding tiers to your app. We talked about a few key points today:Tiers allow you to upsell your appFeature flags are a great way to manage tiers/featuresFocus on features, not tiers to allow for greater flexibilityDon\u2019t forget about your neighborsAlways be aware of costIf you want to hear some more thoughts on this topic check out thepresentationI recently did on this subject with the#BelieveInServerlessgroup. There was a lot of great discussion that followed the presentation."}
{"title": "Deep Dive on MYSQL Aurora Database with Global Database Region Availability", "published_at": 1715462418, "tags": ["amazonrds", "cloudwatch", "cloudwatchlogs", "globaldatabase"], "user": "GargeeBhatnagar", "url": "https://dev.to/aws-builders/deep-dive-on-mysql-aurora-database-with-global-database-region-availability-27l7", "details": "\u201c I have checked the documents of AWS to get into a deep dive on the mysql aurora database with global database region availability feature. In terms of cost, need to pay for amazon rds and cloudwatch.\u201dAmazon Aurora is a fully managed relational database engine that's compatible with Mysql and Postgresql. As already know how mysql and postgresql combine the speed and reliability of high end commercial databases with the simplicity and cost effectiveness of open source databases. The code, tools and applications you use today with your existing mysql and postgresql databases can be used with aurora. With some workloads, aurora can deliver upto 5 times the throughput of mysql and upto 3 times the throughput of postgresql without requiring changes to most of your existing applications.In this post, you will experience how to deep dive on mysql aurora database with global database region availability. Here I have created an amazon rds mysql aurora database with a global database.Architecture OverviewThe architecture diagram shows the overall deployment architecture with data flow, amazon rds mysql aurora, cloudwatch.Solution overviewThe blog post consists of the following phases:Create of Amazon RDS Mysql Aurora with Multi-AZ ConfigurationAddition of AWS Region on Regional Cluster to get a Global Database with Primary and Secondary ClusterOutput of Global Database and Cloudwatch Log GroupPhase 1: Create of Amazon RDS Mysql Aurora with Multi-AZ ConfigurationOpen the console of Amazon RDS, create a database with mysql aurora compatible. Choose the required parameters as engine version, credentials, instance configuration, availability as multi-az, vpc, database authentication, monitoring configuration with database option. Database is created as a regional cluster with reader and writer instances.Phase 2: Addition of AWS Region on Regional Cluster to get a Global Database with Primary and Secondary ClusterSelect the regional cluster and click on add aws region option. Configure the global database settings with region, instance configuration, availability, connectivity and additional configurations.. Also enable the read replica write forwarding option.Phase 3: Output of Global Database and Cloudwatch Log GroupClean-upDelete of Amazon RDS and Cloudwatch Log Group.PricingI review the pricing and estimated cost of this example.Cost of Amazon Relational Database Service for Aurora MYSQL in US East (N. Virginia) = ($0.29 per RDS db.r5.large Single-AZ instance hour (or partial hour) running Aurora MySQL) x (1.458 Hrs) = $0.42Cost of Amazon Relational Database Service for Aurora MYSQL in US West (Oregon) = ($ 0.26 per RDS db.r6g.large Single-AZ instance hour (or partial hour) running Aurora MySQL) x (0.532 Hrs) = $0.14Cost of Cloudwatch = $0.0Total Cost = $0.56SummaryIn this post, I showed \u201chow to deep dive on mysql aurora database with global database region availability\u201d.For more details on Amazon RDS, Checkout Get started Amazon RDS, open theAmazon RDS console. To learn more, read theAmazon RDS documentation.Thanks for reading!Connect with me:Linkedin"}
{"title": "Secure Pattern for Deploying WASM on S3", "published_at": 1715445715, "tags": ["aws", "serverless", "rust", "webdev"], "user": "Benjamen Pyle", "url": "https://dev.to/aws-builders/secure-pattern-for-deploying-wasm-on-s3-25mg", "details": "Picking up where I left off from the last article, I'd built a simple WASM project with Rust and walked through how to generate a publishable distribution.  In this edition, which is probably the penultimate in the series, I need to get a path towards CloudFront and S3.  I want to stay true to the Serverless objective and those two services are perfect for shipping web-delivered code.  So let's dive into Deploying WASM on S3.Series ArticlesThis is as I mentioned the second article in a series about Serverless WASM with Rust.  If you missed the first, below is the link to jump in and read that first.  Don't worry, this will still be here.Getting started with Serverless Web Assembly (WASM) with RustLet's take a look at the architecture I will be building for the rest of this piece.ArchitectureThe main stars for deploying WASM onS3areCloudFrontand of course S3.  Those two services will do the heavy lifting with our compiled WASM distribution.What's cool about using WASM is that it's just some HTML, JavaScript, and an executable WASM file.  That means that it's just like running normal HTML, CSS, and JavaScript which makes S3 the perfect storage vehicle for this code.  And using CloudFront with it is amatch made in heavenDeploying WASM on S3Output of TrunkGoing back to building the WASM package, I used a tool calledTrunkto build and bundle the Rust code.  When I run the commandtrunk buildI'm presented with the following images.  The first is what the build looks like from the console and the second is the contents of thedistdirectory that is created and populated.With adistdirectory ready, I need to figure out a way to get that up into S3.  Let's explore how to make that happen.S3 for Static WebsiteMy default these days is to useCDKto build infrastructure and that's what I'm going to use here.  Specifically, CDK with TypeScript.To start deploying WASM on S3, I need to set up a bucket that is geared towards being a static website.  What this does for me is restrict access and set some other sensible and secure defaults.The code to accomplish that looks like this:constbucket=newBucket(this,'Bucket',{accessControl:BucketAccessControl.PRIVATE,});newBucketDeployment(this,'BucketDeployment',{destinationBucket:bucket,sources:[Source.asset('./dist')]})Enter fullscreen modeExit fullscreen modeWhat's going on above is that I'm creating a new bucket by \"newing\" a Bucket construct.  And then from that bucket, I'm creating another construct called BucketDeployment and sending two things in.The bucket I just created.The directory that holds the output of mytrunk buildcommand.With the S3 deployment part created in my deploying WASM on S3, it's now time to move to CloudFront.Establishing the CloudFront DistributionThere's no magic in any of this. Sure CDK makes it easy to build and package infrastructure but sometimes, things just are right in front of me.Creating a CloudFront distribution in front of my S3 bucket gives me the ability to ship my./distoutput to all of the edge locations that AWS provides and when a user requests access, it'll grab from that edge cache first before reaching out to the S3 origin.  Using this technique when deploying WASM on S3 works just like any other static website.constoriginAccessIdentity=newOriginAccessIdentity(this,'OriginAccessIdentity');bucket.grantRead(originAccessIdentity);newDistribution(this,'Distribution',{defaultRootObject:'index.html',defaultBehavior:{origin:newS3Origin(bucket,{originAccessIdentity}),},})Enter fullscreen modeExit fullscreen modeHere's what is happening in this code:Create an origin identity.Give the newly created bucket read access to the identity.Create a new distribution and assignindex.htmlas the default root object.Putting it TogetherRunningcdk deployin the working directory will push the code and complete the last step in deploying WASM to S3.All put together:trunk build cdk deployEnter fullscreen modeExit fullscreen modeThe S3 bucket will then show the HTML, JS, and WASM files.If I then browse to Cloudfront, I can pick up the URL for the distribution so that I can see if the WASM renders in the browser.Final CheckNow that we are coming to the end of this article on deploying WASM on S3, we can take a look at the browser to see where we are..It's nothing fancy but it's a start for where I'm going to go next with it.Wrapping UpTwo articles into this now-planned 3 article series I've shown you how to build a simple WASM application with Rust and then demonstrated a solution for deploying WASM on S3.  Moving into the finale, I'll put together the following finishing touches.More styled UIAPI build in RustConnect the WASM to the Rust API.Once these pieces are in place, I'll have a Serverless WASM implementation with Rust.I'm still not 100% sure about the use cases here, but I believe by exploring the topics above and building out more useful functionality, I'll be able to assess whether this is something worth exploring more.  WASM isn't just for the web, it can also run on Lambda and other compute options which might be worth checking out as well.And as always, here is thesource code that I'm working from on GitHub.Thanks for reading and happy building!"}
{"title": "Logging demo with OTEL Collector, CloudWatch and Grafana", "published_at": 1715437107, "tags": ["aws", "monitoring", "kubernetes", "cloudnative"], "user": "Shakir", "url": "https://dev.to/aws-builders/logging-demo-with-otel-collector-cloudwatch-and-grafana-53l4", "details": "Hello \ud83d\udc4b, in this post, we would be using the OTEL Demo App with OpenTelemetry Collector and export the logs from the microservices in the app to AWS CloudWatch.Let's get started!!! We can create a separate namespace in the kubernetes cluster withkubectl create ns otelcol-logs-demo.And then create a secret that holds the AWS credentials.kubectl create secret generic aws-credentials \\     --from-literal=AWS_ACCESS_KEY_ID=<access-key-id> \\     --from-literal=AWS_SECRET_ACCESS_KEY=<aws-secret-access-key> \\     -n otelcol-logs-demoEnter fullscreen modeExit fullscreen modeWe would be deploying open telemetry demo app, otel collector as well as Grafana with a single helmchart. The chart comes with other components too, we could disable those that we do not need for this lab. Our helm values would like below.$ cat values.yaml  opentelemetry-collector:   config:     exporters:       awscloudwatchlogs:         log_group_name: otel-logs-demo         log_stream_name: otel-demo-app         region: ap-south-2     service:       pipelines:         logs:           exporters:             - awscloudwatchlogs         metrics: {}         traces: {}   extraEnvsFrom:     - secretRef:         name: aws-credentials prometheus:   enabled: false jaeger:   enabled: false opensearch:   enabled: falseEnter fullscreen modeExit fullscreen modeIn the values above, we have the cloudwatchlogs exporter section where we specify the region, log group and stream names. We are only setting up logs in the pipeline as we are only dealing with that for this demo. We are then setting the AWS credentials via env vars so that the collector could authenticate and send logs to cloud watch. Then, finally we are disabling certain applications which we do not need for this exercise.Let's install the chart.helm repo add open-telemetry https://open-telemetry.github.io/opentelemetry-helm-charts  helm install my-otel-demo open-telemetry/opentelemetry-demo -n otelcol-logs-demo -f values.yamlEnter fullscreen modeExit fullscreen modeHead over to CloudWatch logs in the mentioned region and you should be able to see the log group. And, inside the log group we should have the stream.We should be able to view the logs, for ex. from cartservice as shown in the screenshot below.We can now go to Grafana cloud and add the cloud watch datasource with access credentials.And should now be able to see the logs in Grafana for our log group, on the explore tab.As a bonus let's try adding a panel that shows the no. of messages per severity.Add a panel with query as shown in the screenshot below.Go to the transform data tab and apply a transformation to parse the severity_text from the json message.And then do a group by transformation to calculate the no. of messages per severity.Can choose a visualization type such as pie chart like below.The options I have set in the panel are as follows:Title: Logs by severity Value options > Show > All values Legend > Legend values: Value, PercentEnter fullscreen modeExit fullscreen modeOk that's it for this post, we saw how to get logs from the otel demo app via open collector to AWS CloudWatch Logs and explored it on Grafana with a sample visualization. We just need to delete the namespace withkubectl delete ns otelcol-logs-demoso that logs are not sent to AWS cloudwatch. Thanks for reading !!!"}
{"title": "Harnessing Managed GitHub Action Runners on AWS CodeBuild for Efficient DevOps Workflows", "published_at": 1715344668, "tags": ["aws", "devops", "github", "codebuild"], "user": "Chandrashekar Y M", "url": "https://dev.to/aws-builders/harnessing-managed-github-action-runners-on-aws-codebuild-for-efficient-devops-workflows-57ma", "details": "Few weeks back, AWS announced anewfeature involving AWS CodeBuild, that allows you to configure self-hosted GitHub action runners in CodeBuild containers to process GitHub Action workflow jobs. This feature allows CodeBuild projects to receive GitHub Actions workflow job events and run them on CodeBuild ephemeral hosts.What is AWS CodeBuild?AWS CodeBuild is a robust, managed continuous integration service that automates code compilation, testing, and artifact production without requiring the management of underlying servers.Traditionally, our approach involved using EC2 Spot Instances with custom AMIs in a scheduled auto-scaling setup to accommodate the fluctuating demands of GitHub Action runners. This method, while effective, often led to bottlenecks due to time-zone variances across our DevOps teams, resulting in delayed CI/CD pipelines.The introduction of managed GitHub Action runners by AWS offers a promising alternative, integrating seamlessly with AWS services like IAM, Secrets Manager, CloudTrail, and VPC for enhanced security and operational efficiency.Let us explore this feature step by step, by connecting one of my GitHubrepositoriesto a CodeBuild project, see how it can pick up the queued workflow job and performs the GitHub actions configured in the workflow job. The workflow is quite simple one - it builds a container image based on a Docker file, tags it and pushes this to an existing Amazon Elastic Container Registry (ECR).Step 1- CodeBuild project:Let us navigate to CodeBuild console and create a project with namegithub-action-runnersSelect the source provider as GitHub and Connect using OAuth. We can also us PAC (personal access tokens) to connect to GitHub. To keep things simple, let us stick to OAuth:Authorize aws-codesuite to access your GitHub repositories:.Once the connection is successful, you should be able to select your repository from the list:ForWebhook, let us selectRebuild every time a code change is pushed to this repositoryandEvent typeasWORKFLOW_JOB_QUEUEDThere are following event types available - PUSH, PULL_REQUEST_CREATED, PULL_REQUEST_UPDATED, PULL_REQUEST_REOPENED, PULL_REQUEST_MERGED, PULL_REQUEST_CLOSED, WORKFLOW_JOB_QUEUED,RELEASED, PRERELEASED.You can also add additional conditions for \"Start build\" and \"Don't start build\", if needed.Next, let us choose the Compute Environment for the CodeBuild. CodeBuild offers On-demand and Reserved Capacity options to choose from. For image options you can choose CodeBuild managed images or a custom Docker image. In case of custom image option, you can choose an image from your Amazon ECR (in your account or from another account). Or you can choose an image hosted in an external Docker registry.Let us stick to default values for our use case.Note that, we can always override these options by using a label in our GitHub Actions workflow file. More on this in later sections.CodeBuild can create a service role with required permissions for you or you can create and choose your own custom role for the project.Next is Buildspec yaml file. In our case, Buildspec will be ignored when we use CodeBuild to run GitHub Actions workflow jobs. Instead, CodeBuild will override it to use commands that will setup the self-hosted runner.Let us go-ahead and create the CodeBuild project. Once the creation is successful, you can see that a webhook has been created on your GitHub repository. Navigate to Settings --> Webhooks section to see this:Step 2: GitHub Actions Workflow ConfigurationBelow is the workflow file:name: HelloWorld app on:   repository_dispatch:     types: [webhook_triggered]   pull_request:     branches:       - main   push:     branches:       - main env:   AWS_REGION: 'us-east-1'  jobs:   build:     name: Build Docker Image     runs-on: codebuild-github-action-runner-${{ github.run_id }}-${{ github.run_attempt }}-al2-5.0-small     steps:       - name: Checkout         uses: actions/checkout@v4        - name: Set outputs         id: vars         run: echo \"short_sha=$(git rev-parse --short HEAD)\" >> $GITHUB_OUTPUT        - name: Setup AWS ECR Details         uses: aws-actions/configure-aws-credentials@v4         with:           aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}           aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}           aws-region: ${{env.AWS_REGION}}         - name: Login to Amazon ECR         id: login-ecr         uses: aws-actions/amazon-ecr-login@v2        - name: Build, tag, and push image to Amazon ECR         id: build         env:           ECR_REGISTRY: ${{ steps.login-ecr.outputs.registry }}           ECR_REPOSITORY: ${{ secrets.AWS_ECR_REPO }}           IMAGE_TAG: v1.0.0.${{ steps.vars.outputs.short_sha }}         run: |           docker build -t $ECR_REGISTRY/$ECR_REPOSITORY:$IMAGE_TAG .           docker push $ECR_REGISTRY/$ECR_REPOSITORY:$IMAGE_TAG           echo \"image_tag=$ECR_REGISTRY/$ECR_REPOSITORY:$IMAGE_TAG\" >> $GITHUB_OUTPUT           echo \"${{ github.event.action }}\"     outputs:       image_tag: ${{ steps.build.outputs.image_tag }}Enter fullscreen modeExit fullscreen modeFrom above, you can see that I have AWS credentials and ECR private repository name, configured as repository secrets and referred in the workflow. Refer to my repositoryherefor Docker file and other scripts.Note thatruns-onlabel has the value in formatcodebuild-<project-name>-${{ github.run_id }}-${{ github.run_attempt }}-<image>-<image-version>-<instance-size>Enter fullscreen modeExit fullscreen modewhereproject-name= Name of the CodeBuild project we created in step 1 above.image-image-version-instance-size= al2-5.0-small, which indicates I am overriding the values configured in Environment section of the CodeBuild project.Refer to theSupported compute imagestable in thispagefor the list of compute images that CodeBuild provides.When you push any changes or create a PR to themainbranch, workflow will be triggered.The webhook associated with this repository will notify the CodeBuild project, which is now our GitHub actions runner picks up this job, as shown below:If you navigate to CodeBuild project on AWS console, you can see that CodeBuild run is in progress, executing the GitHub actions:Also, on GitHub Settings page, you can see anephemeralGitHub actions runner powered by CodeBuild is in works.Below is snapshot of the container image that was built and pushed to an existing ECR repository, as part of the GitHub workflow.Looking AheadThis integration not only simplifies our operations but also potentially reduces costs and improves performance compared to our previous setups. I plan to further explore and optimize this feature, considering even custom runner images to align with our security standards.I'm excited to see how this feature evolves and look forward to sharing more insights, including a detailed cost comparison among various runner options.Please let me know what you think by adding your comments."}
{"title": "Coding with a Cyborg: The Rise of the Amazon Q", "published_at": 1715327185, "tags": ["aws", "serverless", "amazonq", "genai"], "user": "Kasun de Silva", "url": "https://dev.to/aws-builders/coding-with-a-cyborg-the-rise-of-the-amazon-q-11a2", "details": "Introducing the Amazon Q VS Code Extension: Your AI CopilotImagine having an AI assistant whispering coding insights in your ear as you write. That\u2019s the magic of Amazon Q! It has a VS Code extension seamlessly integrates with your development environment, offering real-time code completion, error detection, and even the ability to generate code snippets based on your natural language descriptions.Building A Lambda function with Amazon Q\u2019s HelpLet\u2019s use Amazon Q to streamline building our Lambda function. Here\u2019s a glimpse of the workflow:Fire Up VS Code and Install the Amazon Q Extension: This is a breeze \u2014 just search for \u201cAmazon Q\u201d in theVS Code extensionmarketplace and hit install.Project structure - Let's say we are going create a lambda function and deploy it via terraform. we can use the following basic project structure.. \u251c\u2500\u2500 main.tf \u2514\u2500\u2500 send-email-lambda     \u2514\u2500\u2500 send-email.pyEnter fullscreen modeExit fullscreen modeCraft Your Natural Language Request: Describe what you want the function to do, I\u2019m going to ask:Write a lambda function that can be used to send notifications when certain events occur. For example, a function could be triggered to send an email when a new order is placed on an e-commerce websiteWitness the Power of AI: Amazon Q analyses your request and generates the basic Lambda function code, including boilerplate and event handling.Copy the lambda code and put that in the send-email.py file.Adding SMS Functionality with Amazon Q\u2019s GuidanceNow, let\u2019s extend the Lambda to send SMS notifications as well. Here\u2019s where Amazon Q shines again:New Feature: Add a comment # Send SMS after the send email section of your code and press enter. Amazon Q will start suggesting you code blocks as you go in to new lines.AI to the Rescue: Amazon Q taps into its knowledge base and suggests relevant code snippets or documentation links to achieve your goal.Terraform Takes the Wheel for DeploymentWith the Lambda function primed, it\u2019s time to deploy it using Terraform\u2019s infrastructure as code (IaC). Terraform lets you define your infrastructure in code, ensuring consistent and automated deployments. You can use Amazon Q within VS Code to help you write the Terraform code for creating the Lambda function, IAM roles, and SNS topics needed for email and SMS notifications.Craft Your Natural Language Request: Describe what you want the function to do, and this time I\u2019m going to ask:Write a terraform code that creates a zip file of this \u201csend-email-lambda/send-email.py\u201d python code and deploy it to my aws accountThe Final Frontier: Deployment and Beyond!Once the Terraform code is ready, simply runterraform applyto deploy your serverless notification machine to the cloud. This is just a taste of the possibilities! With Amazon Q as your AI partner, you can craft complex applications with unprecedented ease.Remember, this is just the beginning; exploreAmazon Q\u2019s capabilities and see how it can transform your coding experience!Happy Coding with Amazon Q"}
{"title": "How To Manage AWS Security Hub in AWS Organizations Using Terraform", "published_at": 1715318898, "tags": ["aws", "terraform", "security"], "user": "Anthony Wat", "url": "https://dev.to/aws-builders/how-to-manage-aws-security-hub-in-aws-organizations-using-terraform-5gl4", "details": "IntroductionEarlier I've published the blog postHow To Manage Amazon GuardDuty in AWS Organizations Using Terraformwhich is essential in establishing threat detection as part of a security baseline, such as the AWS Security Baseline which I covered extensively in the blog seriesHow to implement the AWS Startup Security Baseline (SSB) using Terraform.Similarly, a good security baseline must include the means to manage the security posture, achieved using Security Hub in AWS. In this blog post, I will walk you through the steps to configuring Security Hub with central configuration in Terraform.About the use caseAWS Security Hubis a security service that helps you manage security posture by collecting security data from AWS and third-party sources, and enabling analysis and remediation of security issues that are found.Late last year,AWS introduced new central configuration capabilities in AWS Security Hubin the form of Security Hub configuration policies (SHCPs). With SHCPs, we can customize many aspects of the Security Hub configuration which can be consistently applied to all members of the organization. This addresses many challenges with managing Security Hub across an organization which I experienced first hand last year. It was practically futile to build Security Hub enablement intoAWS Control Tower Account Factory for Terraform (AFT)! As this is the new best practice, we'll be using this feature.Since it is increasingly common to establish an AWS landing zone usingAWS Control Tower, we will use thestandard account structurein a Control Tower landing zone to demonstrate how to configure Security Hub in Terraform:The relevant accounts for our use case in the landing zone are:TheManagementaccount for the organization where AWS Organizations is configured. For details, refer toIntegrating Security Hub with AWS Organizations.TheAuditaccount where security and compliance services are typically centralized in a Control Tower landing zone.The objective is to delegate Security Hub administrative duties from theManagementaccount to theAuditaccount, after which all organization configurations are managed in theAuditaccount. With that said, let's see how we can achieve this using Terraform!Designating a Security Hub administrator accountSecurity Hub delegated administrator is configured in theManagementaccount, so we need a provider associated with it in Terraform. To keep things simple, we will take a multi-provider approach by defining two providers, one for theManagementaccount and another for theAuditaccount, using AWS CLI profiles as follows:provider\"aws\"{alias=\"management\"# Use \"aws configure\" to create the \"management\" profile with the Management account credentialsprofile=\"management\"}provider\"aws\"{alias=\"audit\"# Use \"aws configure\" to create the \"audit\" profile with the Audit account credentialsprofile=\"audit\"}Enter fullscreen modeExit fullscreen modeWe can then use theaws_securityhub_organization_admin_accountresourceto set the delegated administrator. However, I noticed the following in theAuditaccount:After this resource is created, Security Hub will be enabled with the default standards (AWS Foundational Security Best Practices v1.0.0 and CIS AWS Foundations Benchmark v1.2.0).When the resource is deleted, Security Hub remains enabled.These side effects are undesirable since ideally, we want full control over the lifecycle and configuration of Security Hub in Terraform. To address this issue, we will preemptively enable Security Hub in theAuditaccount using theaws_securityhub_accountresource. Later we will also apply the same configuration policy that will be associated to the organization.data\"aws_caller_identity\"\"audit\"{provider=aws.audit}resource\"aws_securityhub_account\"\"audit\"{provider=aws.auditenable_default_standards=false}resource\"aws_securityhub_organization_admin_account\"\"this\"{provider=aws.managementadmin_account_id=data.aws_caller_identity.audit.account_iddepends_on=[aws_securityhub_account.audit]}Enter fullscreen modeExit fullscreen modeWith theAuditaccount designated as the Security Hub administrator, we can now manage the organization configuration.Configuring cross-region aggregationSecurity Hub provides across-region aggregationfeature that centralizes findings, finding updates, insights, control compliance statuses, and security scores from multiple regions into a single region. Being able to review all findings in one place is incredibly useful for security analysts. We can enable this feature for all regions using theaws_securityhub_finding_aggregatorresourcein Terraform as follows:resource\"aws_securityhub_finding_aggregator\"\"this\"{provider=aws.auditlinking_mode=\"ALL_REGIONS\"depends_on=[aws_securityhub_account.audit]}Enter fullscreen modeExit fullscreen modeEnabling central configurationFirst, we need to apply the organization configuration to enable central configuration. Since the settings are defined in an configuration policy, we need to disable all settings that are related to local configuration. We will achieve this using theaws_securityhub_organization_configurationresource:resource\"aws_securityhub_organization_configuration\"\"this\"{provider=aws.auditauto_enable=falseauto_enable_standards=\"NONE\"organization_configuration{configuration_type=\"CENTRAL\"}depends_on=[aws_securityhub_organization_admin_account.this,aws_securityhub_finding_aggregator.this]}Enter fullscreen modeExit fullscreen mode\u26a0 If you have enabled delegated administrator at some point prior toNovember 2023 when the central configuration feature was released, you may encounter aDataUnavailableExceptionindicating that the organization data is still syncing when you create the organization configuration. To resolve this error, open an AWS support case to have them fix the data in the backend.Creating and associating a configuration policyWith the organization configuration primed, we can now create and associate a configuration policy. This can be done with theaws_securityhub_configuration_policyresourceand theaws_securityhub_configuration_policy_associationresource.For illustration, let's assume that we want to enable only theCIS AWS Foundations Benchmark v1.4.0standard across the organization. We also want to disable the control[IAM.6] Hardware MFA should be enabled for the root user.The configuration policy can be defined in Terraform as follows:data\"aws_region\"\"audit\"{provider=aws.audit}data\"aws_partition\"\"audit\"{provider=aws.audit}resource\"aws_securityhub_configuration_policy\"\"this\"{provider=aws.auditname=\"ExamplePolicy\"description=\"This is an example SHCP.\"configuration_policy{service_enabled=trueenabled_standard_arns=[\"arn:${data.aws_partition.audit.partition}:securityhub:${data.aws_region.audit.name}::standards/cis-aws-foundations-benchmark/v/1.4.0\"]security_controls_configuration{disabled_control_identifiers=[\"IAM.6\"]}}depends_on=[aws_securityhub_organization_configuration.this]}Enter fullscreen modeExit fullscreen mode\ud83d\udca1 You can find the ARN format for the Security Hub standardshere. Note that all standards are regional except for CIS AWS Foundations Benchmark v1.2.0.Lastly, we will associate this configuration policy to the entire organization:data\"aws_organizations_organization\"\"this\"{provider=aws.management}resource\"aws_securityhub_configuration_policy_association\"\"org\"{provider=aws.audittarget_id=data.aws_organizations_organization.this.roots[0].idpolicy_id=aws_securityhub_configuration_policy.this.id}Enter fullscreen modeExit fullscreen modeBefore you apply the Terraform configuration, there is one issue which I found while cleaning up my environment that should be addressed in the Terraform configuration.Addressing a state-related issue which causes policy deletion to failWhile cleaning up my environment, I encountered the following state-related error when attempting to destroy theaws_securityhub_configuration_policyresource:aws_securityhub_configuration_policy_association.org: Destroying...[id=r-lzgl] aws_securityhub_configuration_policy_association.org: Destructioncompleteafter 2s aws_securityhub_configuration_policy.this: Destroying...[id=f7bf343f-af38-4b1d-9116-73f43cfb5d61] \u2577 \u2502 Error: deleting Security Hub Configuration Policy(f7bf343f-af38-4b1d-9116-73f43cfb5d61): operation error SecurityHub: DeleteConfigurationPolicy, https response error StatusCode: 409, RequestID: 06f4448f-4133-412a-b89b-bda896f7fa08, ResourceConflictException: Policy f7bf343f-af38-4b1d-9116-73f43cfb5d61 is associated with one or more accounts or organizational units. You must disassociate the policy before you can delete it.Enter fullscreen modeExit fullscreen modeHowever, you can see in the first two lines in the output that the configuration policy association is already destroyed before the attempt to destroy the policy.After examining the Terraform resource code and the AWS API contract, I found that theStartConfigurationPolicyDisassociationAPI actiondoes not report the disassociation status, nor is there another API action that can query the status. So this is not a Terraform AWS Provider bug per se and having the issue addressed upstream seems unlikely.As a workaround, I turned to thetime_sleepresourcethat can add a wait time for resource destruction. Through trial and error, I learned that 10 seconds is sufficient for the state to be updated. So we can update the Terraform configuration as follows:# Some wait time is needed to account for state changes after the configuration policy is disassociatedresource\"time_sleep\"\"aws_securityhub_configuration_policy_this\"{destroy_duration=\"10s\"depends_on=[aws_securityhub_configuration_policy.this]}resource\"aws_securityhub_configuration_policy_association\"\"org\"{provider=aws.audittarget_id=data.aws_organizations_organization.this.roots[0].idpolicy_id=aws_securityhub_configuration_policy.this.iddepends_on=[time_sleep.aws_securityhub_configuration_policy_this]}Enter fullscreen modeExit fullscreen modeWith this change, the full Terraform configuration can be destroyed successfully.\u2705 You can find the complete Terraform in theGitHub repositorythat accompanies this blog post.With the complete Terraform configuration, you can now apply it to establish theAuditaccount as the delegated administrator and apply the SHCP to all accounts and all regions (as per the finding aggregator settings).Caveats about disabling Security Hub in member accountsDue to the design of the Security Hub API and the Terraform resources, Security Hub will not be disabled in the member accounts when you runterraform destroy. Normally this wouldn't be a problem for a production landing zone. However, if you are only testing, this could lead to unexpected costs especially when left running in all accounts and all regions.Since it would be difficult to disable Security Hub in individual account, a smarter way would be to disable Security Hub using the SHCP. This can be done by changing theaws_securityhub_configuration_policy.thisresource definition to the following:resource\"aws_securityhub_configuration_policy\"\"this\"{provider=aws.auditname=\"ExamplePolicy\"description=\"This is an example SHCP.\"configuration_policy{service_enabled=false}depends_on=[aws_securityhub_organization_configuration.this]}Enter fullscreen modeExit fullscreen modeAfter you re-apply the Terraform configuration, Security Hub should be disabled in all accounts and all regions. Then you can safely runterraform destroyto remove the remaining Security Hub resources and configuration.SummaryIn this blog post, you learned how to implement central configuration to manage AWS Security Hub in AWS Organizations using Terraform. By consolidating all management work of all accounts in an organization and all regions into a delegated administrator account, you now have a single pane of glass to review and manage your cloud security posture.For more tips and walkthroughs on AWS, Terraform, and more, please check out theAvangards Blog. Thanks for reading!"}
{"title": "From Scratch: IaC", "published_at": 1715304803, "tags": ["aws", "terraform", "devops", "beginners"], "user": "Joseph", "url": "https://dev.to/aws-builders/from-scratch-iac-2f2m", "details": "It occurred to me recently that it might not be obvious when someone should start using IaC. This can be daunting because there are things you can't do, and some things you shouldn't do with IaC. There are also situations where you could use it, but it doesn't lead to any benefit.Sign up for AWS Cloud AccountThere will some some things you have to do that are probably common sense you can't leverage IaC for.Signing up for your AWS cloud accountis one of them. In my case I'm completely starting from scratch, so let's go through everythign involved in that. During this series, when we actually use IaC, I will be very explicit about it, so you'll know the difference between something I had to do by hand / manually vs something I scripted.Root AccessSo you've signed up for your account, and you have your root account. For the majority of the work you'll be doing, you willnotbe using your root account. It's dangerous to rely too heavily on your root account, but there are a couple things you have to use it for, especially at the beginning.MFA your Root AccountI've had the privelege of working with some really talented security engineers around cloud, likeChris Farrisfor example. When it comes to cloud governance you really can't do wrong by justdoing what they recommend. So it should come to no surprise the next few steps I'll be doing are going to essentially be doing that.Your Root Account will be the only account that can be logged in outside your identity provider, and the account that can do the most damage if compromised, so it's critical you keep it as secure as possible.Setup External Identity ProviderYoureallyneed to connect your organization with an identity provider. Before you can do this you have toenable Identity Center. While you go through this process AWS is going to ask you to enable Organizations as well, which you'll want to do. In my case I'm going to be doing this with google workspaces, so I just followedthis documentationand got up and running.Customize the access portal urlThis isn't necessary but it's a nice to have. Once you have your identity provider active, the portal to sign in can be a little obscure, but you canchange itto use a subdomain of your liking, assuming it's available.GroupsWhen working with Google Workspaces, one of the limitations with the IDP integration is that workspace groups don't automatically come over like users do. To make matters even more complicated, you can't add groups using the AWS console, you have to add them through the CLI or the API. That being said, it's worth going through the trouble to make some groups anyway, because they will be useful later when we want to start giving permissions to our teams.Enter IaCNow, most of the time folks will just make the groups using the CLI and call it a day, but I think this is actually a good starting point to creating your IaC footprint. This does come with somecaveatsthough, because this isn't going to follow the normal flow of scripting we will get into later after we're done using the root user.Scripting against the Root AccountWe are going to be creating identity groups using terraform, but in order to do that we need some access keys, and they have to come from root (because we don't have any other users yet). This is something that is usually recommendedneverto do, and while I largely do agree with this practice, in this situation I argue it's actually a better practice to do it this way than to do it manually. Why? Scripting something makes it repeatable, which is less prone to error. Additionally, there are some safeguards we'll be putting in place so that the root user doesn't get compromised. I can't tell you how many times I've set something up without IaC, and then a month later I forgot what I did, which caused me or my team alotof headaches. It also creates a less secure situation because you now have to circle back on what you did and rethink everything, which can introduce errors and misteps.So we're going to make some access keys, but we're going to immediatelydeactivatethem. Why? Because we haven't written our scripts yet, and we're not going to leave anything to chance when it comes to the root account.Show me the codeI'm going to be building a solution around this usingCDKTFbut you could solve this with other scripting tools as well.To remedy this group situation is actually pretty easy and doesn't require a lot of code. We're going to define a construct to hold onto all our org's groups. We'll deploy this once and shouldn't need to revisit this again, which will be a common pattern for anything we script using the root user.exportclassIdentityCenterGroupsextendsConstruct{constructor(scope:Construct,name:string,config:{provider:AwsProvider;org:string;}){super(scope,name);conststores=newDataAwsSsoadminInstances(scope,`${name}-stores`,config);conststoreId=Fn.element(stores.identityStoreIds,0);newIdentitystoreGroup(scope,`${name}-admins`,{identityStoreId:storeId,displayName:`${config.org}-admins`,provider:config.provider,});}}Enter fullscreen modeExit fullscreen modeHere I'm just making an admins group, but you would also add others likedevelopersorsecuritybased on whatever needs you have. This is important because later we can give these groups specific permissions.Finally we need to add this construct to our stack and deploy it.classRootStackextendsTerraformStack{constructor(scope:Construct,id:string){super(scope,id);constproviderAsRoot=newAwsProvider(this,`${id}-provider`,{profile:\"root\",});newIdentityCenterGroups(this,`${id}-identity-center`,{provider:providerAsRoot,org:organization,});}}constapp=newApp();newRootStack(app,`${organization}`);app.synth();Enter fullscreen modeExit fullscreen modeBefore we can deploy it, we need to go back into our AWS console and activate the access keys for our root account. Now we can deploy this withcdktfy deployand we should immediately see the groups show up in Identity Center. After we're verified the group was created, immediately go back to the access keys for root and deactivate them.What's NextWe have our AWS account created and connected to our identity provider of choice. That Identity Provider isn't configured to provide access into our AWS account yet, however, because we haven't configured permissions yet. That's what we'll get into next. It's also worth pointing out that, even though we've been doing everything using the root user up until this point, our goal is to doas little as possible with this user, so once we've finished the bare minimum with root, we'll be switching over to an identity user for everything else. We're just not to that point yet.Also one last thing for reference because I get asked this a lot: it took me about 3 hours to do everything in this articleandwrite the blog article. If you are wondering how long it takes to do IaC properly, it really doesn't take that long. If you're curious exactly what I built for this set up, you can check it out in myhow to cloudrepository in the aws section."}
{"title": "Terraform Module Gotchas - Inline block example S3 AWS", "published_at": 1715291170, "tags": ["terraform", "aws", "s3", "cloud"], "user": "Jorge Tovar", "url": "https://dev.to/aws-builders/terraform-module-gotchas-inline-block-example-s3-aws-1ib2", "details": "Software is all about composition.ModulesModules are the key ingredient for writing reusable, maintainable code. When planning to deploy AWS infrastructure, it's better to leverage this functionality and be aware of the common gotchas to avoid conflicts with other Terraform configurations.Modules are essentially terraform files with resource definitions that you can use elsewhere.TerraformTerraform and Infrastructure as Code are no exceptions when we talk about software composition. We should use modules to create and compose our building blocks,just as we use functions to compose and create our business logic.There are two gotchas we need to be aware of when using modules in Terraform:File pathsInline blocksGitHub RepositoryInline blocksThe configuration of some Terraform resources can be done through inline blocks or independent resources.Inline blocks exampleThese are properties of the resource that you can define within the same resource. For example,ingressandegressare inline blocks, and there are plenty of them in Terraform, such asversioningin an S3 bucket and many more.resource\"aws_security_group\"\"alb\"{name=\"${var.cluster_name}-alb\"ingress{from_port=local.http_portto_port=local.http_portprotocol=local.tcp_protocolcidr_blocks=local.all_ips}egress{from_port=local.any_portto_port=local.any_portprotocol=local.any_protocolcidr_blocks=local.all_ips}}Enter fullscreen modeExit fullscreen modeSeparate resource customization exampleThese are properties of the resource that you define as independent Terraform resources. In this case, we create our bucket using a module, but we modify some properties of this resource using new resources. In the example, we have added versioning to the bucket.module\"s3_bucket\"{source=\"../modules/separate-resource\"name=\"jorgetovar\"}resource\"aws_s3_bucket_versioning\"\"versioning\"{bucket=module.s3_bucket.bucket_nameversioning_configuration{status=\"Enabled\"}}Enter fullscreen modeExit fullscreen modeInline blocks vs separate resources is a trade-off, and we must be cautious when using inline properties because we lose some flexibility in defining and customizing resource properties from the client's perspective.There is also the possibility of conflicts between inline blocks and client resources, which can lead to unexpected results. Finally, inline blocks have been deprecated in Terraform, perhaps for these reasons.Additionally, it's important to remember that defining Terraform configuration within both the module itself and the file that uses the module can lead to several issues.ComplexityWe should organize our modules in a way that we can easily interact with a part of the infrastructure without having tounderstand the whole system.Our job is to remove the accidental complexity and make the system easier to understand. When we use inline blocks, weare adding complexity to the system (properties and thing that we may not need to know about).AbstractionsBut we can also leverage abstractions and hide some details that are unnecessary, if all the buckets that we create haveversioning, maybe it makes more sense to move that resource to the module.File PathsBy default, Terraform uses the path relative to the current working directory, in our case, the localhost root module.Therefore, the module should be able to read files from its folder instead of the one where we are currently executing the commands.We can leverage the path variables of terraform to read the files that are in the module folder.The path variables are:Path.module: The path to the module folderPath.root: The path to the root module folderPath.cwd: The path to the current working directoryresource\"local_file\"\"mad_libs\"{count=var.num_filesfilename=\"build/madlibs-${count.index}.txt\"content=templatefile(\"${path.module}/${element(local.templates, count.index)}\",{nouns=random_shuffle.random_nouns[count.index].resultadjectives=random_shuffle.random_adjectives[count.index].resultverbs=random_shuffle.random_verbs[count.index].resultadverbs=random_shuffle.random_adverbs[count.index].resultnumbers=random_shuffle.random_numbers[count.index].resultname=\"Jorge Tovar\"})}Enter fullscreen modeExit fullscreen modeConclusionTerraform modules may be the best way to deploy our resources in AWS, but we must be aware of the challenges that come with this useful feature.Modules allow us to create abstractions and reduce the complexity of our infrastructure.LinkedInTwitterGitHubIf you enjoyed the articles, visit my blogjorgetovar.dev"}
{"title": "How I Crushed My AWS Certification Renewals Back-to-Back (and Why It Was a Bad Idea)", "published_at": 1715220707, "tags": ["aws", "certification"], "user": "Justin Wheeler", "url": "https://dev.to/aws-builders/how-i-crushed-my-aws-certification-renewals-back-to-back-and-why-it-was-a-bad-idea-56fh", "details": "I renewed all 12 of my active AWS certifications in 22 days! Most of the exams were back-to-back (based on testing center availability), and I even took two exams in a single day. The entire experience was insanely stressful, yet incredible.Why?In January, I was compiling a list of my goals this year, determined to be more accomplished than I was last year. I set some extremely ambitious goals for myself, to say the least. As if they are too ambitious, I guess we will wait and see. Since I had some AWS certifications expiring this year and some next year, I wanted to get them all renewed so I could focus on other priorities next year (Kubernetes, here I come)! The obsessive tendencies in me thought it would be nice if I could align the dates in some logical way rather than the random mess they were previously in. Although I had originally planned on finishing my AWS certifications by the end of Q1, plans changed. Aside from everyday life, there were other complications.Schedule4/08 Data Analytics Specialty4/22 DevOps Engineer Professional4/23 Solutions Architect Professional4/24 Advanced Networking Specialty4/25 Machine Learning Specialty4/27 Security Specialty4/29 Database Specialty4/29 SAP on AWS SpecialtyIf you didn't already know that professional-level exams automatically renew some lower-level certifications, now you do.DevOps Engineer Professional renewsDeveloper AssociateSysOps Administrator AssociateCloud PractitionerSolutions Architect Professional renewsSolutions Architect AssociateCloud PractitionerRetiring ExamsAWS had announced they were retiring three of their exams, which I already wrote about inNavigating AWS Certifications in 2024.Data Analytics Specialty (retiring April 8th)Database Specialty (retiring April 29th)SAP on AWS Specialty (retiring April 29th)Obviously, I contemplated skipping these exams; after all, they are retiring. Why didn't I? What's the point? Well, like I already said inthis LinkedIn postin regards to the Data Analytics Specialty exam, \"I don't agree with Amazon Web Services decision to retire this exam\".Big Datais more important now than ever before. Perhaps the Data Engineer Associate is meant to replace this exam. Although I think we may need a Data Engineer Professional if we expect the same level of depth.I feel that the Database Specialty is equally valuable. With all of thepurpose-built databasesthat AWS has to offer, it is invaluable to build a strong understanding to ensure the best database for the job can be chosen.My feelings for the SAP on AWS Specialty are not the same; I'm not terribly sad to see it go, yet I still wanted to renew it. Why?I had previously planned on taking this exam before I learned about the retirement. If I had skipped it, part of me would have always wondered if I could've actually passed it again.I wanted to test my SAP knowledge, as I am particularly weak in this domain. Even simple SAP configurations on AWS can easily costthousands of dollars per month, which makes it impossible to experiment on my own.How?I've talked about my study methodsbeforeand this time was pretty similar, to be honest. Except this time, I only had to focus on the new stuff since I've already passed these tests before.These are the websites that I used the most:A Cloud Guru (Paid)Cloud Academy (Paid)ML Exam (Free)Pluralsight (Paid)Practice TestsI started with practice tests to gauge my knowledge before hitting the books. This helped me focus on the newer services and concepts I wasn't familiar with.Did you know that API Gateway can directly call some other AWS services without Lambda? It's calledDirect integrations, and it's definitely something I learned through this experience.The scores varied wildly between A Cloud Guru and CloudAcademy, which was interesting. How can I get a 50% on CloudAcademy and a 90% on A Cloud Guru for the same exam? \ud83e\udd14Flash CardsI usedQuizletto create online flashcards. The AI functionality that has been added to Quizlet recently is really great. I'll highlight two examples.Magic Notesuses AI to generate flash cards automatically from a large source of text. I used this a couple of times to paste AWS FAQs or documentation.Q-Chatuses AI to interact with your own flash cards using natural language. With this, I don't have to worry about exactly how my flash cards are typed anymore.In the middle of this AI storm, it seems like Quizlet is doing AI the right way!Dedicated Study TimeFor a while now, I have always dedicated time every day to studying something. This year, I just focused the allocated time strictly on AWS. Nearing my exams, I would often trade time I would've spent on entertainment like movies or shows for studying instead to ensure I was as prepared as I could be. Do you know how hard it is to study for eight exams at the same time? In my mind, it was similar to \"finals week\" in college.Dedicated Testing TimeTypically, when I take a single exam, I will take a half day and be back to work. Although, this was much more taxing, I used a week of PTO so I could focus on my exams exclusively. I prefer testing in person, which resulted in commuting to testing centers that were about 35 miles away. Still, not all of my tests were in person. I did three exams using the online option calledOnVUE.ExpensesThe costs can be high for these types of exams. I had access to a bunch of benefits through my company and my engagement in AWS communities.First,AWS Community BuildersandAWS User Group Leadersboth provided me with a 100% voucher.Next, I was able to use multiple 50% vouchers that I had stockpiled from earning all of the certifications previously.AWS providesbenefitsto certified individuals.Finally,Bravo LTreimbursed me for every exam I passed.Words of Caution \ud83d\udea7I will never do this again. If you are thinking of attempting something similar, I hope I can change your mind. Overall, I felt like everything turned out okay, but it could've been far worse.My mental health suffered from this endeavor in a couple ways. I experienced anxiety, stress, andImposter Syndrome. I lost enjoyment in my life for a time because I was sacrificing all of my spare time to studying.My physical health was impacted as well. I found myself skipping meals to finish practice tests or additional lessons. Furthermore, my quality of sleep was affected.When planning, I want to ask that you don't overwhelm yourself. Give yourself plenty of time to accomplish your goals without rushing.As always, if you liked this content, maybe you would like toBuy Me a Coffeeor connect with me onLinkedIn."}
{"title": "Complex Event Filtering with AWS EventBridge Pipes, Rules and No Custom Code.", "published_at": 1715217269, "tags": ["eventdriven", "aws", "serverless", "lambda"], "user": "James Matson", "url": "https://dev.to/aws-builders/complex-event-filtering-with-aws-eventbridge-pipes-rules-and-no-custom-code-2b11", "details": "It can be a guilty pleasure introducing AWS Lambda functions into your solution. Lambda functions are many things, but perhaps none more so than the swiss army knife of cloud development.There\u2019s almost nothing they can\u2019t do, owing mostly to the fact that a Lambda is just an uber-connected vassal for your awesome code. One of the most common uses cases for a Lambda function is as the glue between one or more AWS services, if you\u2019ve worked in the serverless space I\u2019m telling you an all-too-familiar tale. You\u2019ve set up a DynamoDb table to record some event data, and now you want to take that event data and send it to an API after performing some logic or manipulation on the data. You might end up with a design that looks similar to the below:A glorious serverless Lambda function creates the glue between the exported stream events and the downstream REST API. Magic! Or, maybe you\u2019re the provider of the API, and you want to receive events from the outside world, perform all kinds of logic and business rules on the data, then submit different messages to different downstream services as a result. You might end up with:Again, Lambda functions serve to provide the glue and business logic between upstream services and downstream services. Using whatever language you\u2019re most comfortable in, you can fill the lambda with whatever code you need to get a virtually limitless variation of logic.It\u2019s awesome! I love Lambda.Really, I do! If I saw Lambda walking on the opposite side of a busy street on a rainy day, I\u2019d zig-zag through traffic to get to the other side, pick Lambda up right off the ground and hug it to pieces as the raindrops fell around us and the city seemed to freeze in place.Then I\u2019d place Lambda down, kiss its nose, and proceed to walk hand-in-hand with it through the rainy streets, talking about nothing and everything at the same time.Wow. That got really romantic. I\u2019m not crying, you\u2019re crying!This is me, hugging AWS Lambda. If this doesn\u2019t melt your heart, you\u2019re dead. You\u2019re literally dead.But there are downsides that come with filling your environment with Lambda functions.For one thing, Lambda functions are filled with code and libraries, so irrespective of whether you\u2019re using C#, Python, NodeJS or Go you have to face the reality of security scans, package management, code maintenance and just the overall technical debt that comes with maintaining a code base. (Even if that codebase is distributed among lots of little Lambda functions).Even if everything is well maintained, as runtimes age out of support from AWS, you\u2019ll be faced with the task of updating functions to the next supported runtime which \u2014 depending on how big your environment is \u2014 can range from a minor annoyance to a major headache.AWS however, have made some great moves across their landscape to lessen the reliance builders of today have on needing Lambda to form the \u2018glue\u2019 between services. There\u2019s no replacing Lambda for its flexibility and power to encapsulate business logic for your applications, but using Lambda functions simply as a way to stitch AWS native service A to AWS native service B is a pain and let\u2019s be honest \u2014 after the eleventy billionth time you\u2019ve done it \u2014 not much fun either.Now as a software developer from way back in the olden times of Visual Basic and ASP classic, I know that a dev\u2019s first instinct is to solve the problem with code. I get it. Code is beautiful, and in the right hands can solve all the problems, but the reality is that to think in a \u2018cloud native\u2019 manner actually involves in many cases thinking about code less as a solution to all problems and instead a solution to the problems only code can solve.Cloud services \u2014 AWS included \u2014 have come a long way to allowing you to get a lot of work done by leveraging the built in functionality of components like S3, EventBridge, CloudWatch, Step Functions and more. This means that when you do turn to application code, it\u2019s for the right reasons, because it\u2019s the right tool.Less is MoreTo demonstrate how taking the \u2018less Lambda/code is more\u2019 approach can work, I\u2019m going to take you through a real world use case, showing you one reasonable way of approaching the solution using AWS Lambda, and then an alternative that uses zero (that\u2019s right, zero \u2014 what sorcery is this?) Lambda functions to achieve the same ends.Alright, so what\u2019s our real world use case? (Bearing in mind this is a very real world use case. As in, it happened \u2014 to me \u2014 in the real world, a place I'm rumoured to visit from time to time).I\u2019ve been tasked with building a service as a part of a larger platform that will help provide stock on hand information to a retail website. In retail, accurate stock isn\u2019t a nice to have, it\u2019s essential. If you don\u2019t provide correct stock for your customers to view, you\u2019re either losing sales when you\u2019re not reporting stock that is there, or creating frustrated customers showing products in stock that actually aren\u2019t.In order to provide useful information quickly to the website, we have a third party search index product that holds our products as well as information about whether the products are in or out of stock. The stock on hand figures are going to be held in a DynamoDb table, and my job is to take the ever changing stock on hand figures from that database, work out whether the change is resulting in the productMoving from in stock to out of stockMoving from out of stock to in stockand send the resulting information to the third party search index via an API call. To keep things simple, we don\u2019t actually need to send the stock figure (e.g. 5 units in stock) itself to the search index, we just need to send the product \u2018sku\u2019 (Stock Keeping Unit) and whether it\u2019s in or out of stock.Lambda\u2019s Lambda\u2019s EverywhereLet\u2019s have a look at how we might approach this with our code and Lambda first approach:Not too shabby. Putting aside the complexities of error handling, retries and what have you, we have a pretty simple robust solution. Our stock table has DynamoDb streams enabled and configured to send NEW_AND_OLD_IMAGES. This means that when a value changes, the table will export some JSON data that tells us what the old value was as well as what the new value is. This is important for us to determine if the product is moving into/out of stock.We then have a Lambda function set up to be triggered by the DynamoDb stream event. This is called the \u2018Filtering Service\u2019. It\u2019s job is to examine the data and determine if it\u2019s something we should be sending to our search index. Remember, we don\u2019t care about movements of units up or down unless it results in the product moving in stock or out of stock. Here\u2019s a good visual reference below:If our filtering service says \u2018yup, this looks good \u2014 send it on\u2019 it\u2019s going to send that data to an SQS Queue. Why not directly to the next Lambda? Well, it\u2019s a bit frowned upon to directly invoke Lambda from Lambda and it doesn\u2019t hurt to put a little decoupling between Lambda A and B.The Queue will have a trigger setup to invoke the IndexService Lambda. It\u2019s job will be to obtain the required credentials to call the third party search index API, and send along a payload to that API that looks a bit like this:{     \"sku\": \"111837\",     \"in_stock\": false }Enter fullscreen modeExit fullscreen modeNice! Easy and done. Except you\u2019ve just introduced another 2 Lambda functions to your landscape. Functions that come with all the baggage that we talked about earlier.So, is there another way? A \u2014 and here\u2019s a new term I\u2019ve coined just for you \u2014 Lambdaless way?Of course.This is The WaySo how are we going to tackle this problem without any custom code or Lambda functions? We\u2019ll let\u2019s look at the design visually first, then we can walk through it (including \u2014 as always \u2014 an actual repository you can play around with).Whoah. Hang on a second.This looks more complicated than the other diagram, which if I put my architect hat on seems counterintuitive \u2014 right? You should seek to simplify not complicate an architecture?Well yes, that\u2019s absolutely right. But try to remember that in our other diagram there are 2 Lambda functions. Inside those functions is a whole bunch of code. That code includes branching logic and different commands/methods, none of which is actually shown on the diagram.We need to replicate that logic somewhere, so we\u2019re using native AWS services and components to do it. Hence, the diagram may look a little more busy, but in reality it\u2019s pretty elegant.Because we don\u2019t need any custom code packages, all of our architecture in the image will be delivered by way of a SAM (Serverless Application Model) template, a great infrastructure-as-code solution for AWS projects. You can see the full template in the repository here:RepoBut we\u2019ll be breaking it down piece by piece below.First, let\u2019s have a quick look at our DynamoDb table setup:Resources:DynamoDBStockOnHandTable:Type: AWS::DynamoDB::TableProperties:TableName: StockOnHandAttributeDefinitions:- AttributeName: skuAttributeType: SKeySchema:- AttributeName: skuKeyType: HASHProvisionedThroughput:ReadCapacityUnits: 5WriteCapacityUnits: 5StreamSpecification:StreamViewType: NEW_AND_OLD_IMAGESWe\u2019ve defined a simple table, with \u2018sku\u2019 as the hash or partition key. We\u2019ve set up a small amount of provisioned throughput (how much read and write \u2018load\u2019 our database can handle) and finally \u2014 most importantly \u2014 we\u2019ve enabled \u2018Streams\u2019 with the type of NEW_AND_OLD_IMAGES which we discussed in our Lambda solution.The idea is when an upstream system inserts a new record with a SKU and a stock on hand figure, the data will be streamed out of the database to trigger downstream events.Our DynamoDb table and DynamoDb stream remain the same as our Lambda based solution, but after that it\u2019s AWS EventBridge to the rescue to pretty much take care of everything else we could possibly need.EventBridge has become \u2014 in my humble opinion \u2014 the darling of event-driven serverless architecture in AWS. In our team, we are consistently using it for any solution where we need event-driven solutions at scale and with decoupling and fine control built into the solution from the start.So wer\u2019e sending our DynamoDb stream to an EventBridge Pipe.EventBridge Pipes are a great way to take data from a range of AWS sources, and then filter, enrich, transform and target a downstream source, all without any custom code.In our case though, we\u2019re just using the Pipe itself as a way to get our DynamoDb stream from DynamoDb into EventBridge itself, because at the time of writing at least there\u2019s no way to directly target an EventBridge bus with a DynamoDb stream. Some AWS services, like Lambda or API Gateway let you integrate directly to an EventBridge bus, but DynamoDb streams isn\u2019t one of them.Using a Pipe however, gives us the ability to get where we need to get. Let\u2019s have a look at the components that we\u2019ve set up to allow our \u2018Stream to EventBridge\u2019 connection:StockOnHandEventBus:     Type: AWS::Events::EventBus     Properties:       Name: StockEventBus    Pipe:     Type: AWS::Pipes::Pipe     Properties:       Name: ddb-to-eventbridge       Description: \"Pipe to connect DDB stream to EventBridge event bus\"       RoleArn: !GetAtt PipeRole.Arn       Source: !GetAtt DynamoDBStockOnHandTable.StreamArn       SourceParameters:         DynamoDBStreamParameters:           StartingPosition: LATEST           BatchSize: 10           DeadLetterConfig:             Arn: !GetAtt PipeDLQueue.Arn       Target: !GetAtt StockOnHandEventBus.Arn       TargetParameters:         EventBridgeEventBusParameters:           DetailType: \"StockEvent\"           Source: \"soh.event\"    PipeRole:     Type: AWS::IAM::Role     Properties:       AssumeRolePolicyDocument:         Version: 2012-10-17         Statement:           - Effect: Allow             Principal:               Service:                 - pipes.amazonaws.com             Action:               - sts:AssumeRole       Policies:         - PolicyName: SourcePolicy           PolicyDocument:             Version: 2012-10-17             Statement:               - Effect: Allow                 Action:                   - \"dynamodb:DescribeStream\"                   - \"dynamodb:GetRecords\"                   - \"dynamodb:GetShardIterator\"                   - \"dynamodb:ListStreams\"                   - \"sqs:SendMessage\"                 Resource:                    - !GetAtt DynamoDBStockOnHandTable.StreamArn                   - !GetAtt PipeDLQueue.Arn         - PolicyName: TargetPolicy           PolicyDocument:             Version: 2012-10-17             Statement:               - Effect: Allow                 Action:                   - 'events:PutEvents'                 Resource: !GetAtt StockOnHandEventBus.Arn    PipeDLQueue:      Type: AWS::SQS::Queue        Properties:        QueueName: DLQ-StockEvents     PipeDLQPolicy:     Type: AWS::SQS::QueuePolicy     Properties:       Queues:         - !Ref PipeDLQueue       PolicyDocument:         Version: \"2012-10-17\"         Statement:           - Effect: Allow             Principal:               Service: \"events.amazonaws.com\"             Action: \"sqs:SendMessage\"             Resource: !GetAtt PipeDLQueue.Arn             Condition:               ArnEquals:                 \"aws:SourceArn\": !Sub \"arn:aws:events:${AWS::Region}:${AWS::AccountId}:rule/StockEventBus/*\"Enter fullscreen modeExit fullscreen modeThere\u2019s quite a bit going on here. First, we\u2019ve created our custom EventBridge bus. This is just a way to seperate our particular sets of events so that we don\u2019t need to recognise them apart from other events that might come into the default EventBridge bus. It\u2019s our own private channel for our stock on hand service.Next we\u2019re defining our Pipe. The source for the Pipes events is our DynamoDb stream and the target is our EventBridge bus. We\u2019re sending our event from the pipe to EventBridge with the following parameters:DetailType: \u201cStockEvent\u201dSource: \u201csoh.event\u201dThe detail type and source are critical to allow EventBridge to properly filter and route the message where it needs to go next.You can see we\u2019re also referencing an IAM (Identity and Access Management) role. The role specifies that the AWS service \u2018pipes.amazonaws.com\u2019 can assume it, and the policies allow the role to accept the DynamoDb stream and target the EventBridge bus, as well as send any failures (messages that for some reason don\u2019t get to EventBridge) to our SQS DLQ (Dead Letter Queue).So now we\u2019re getting our DynamoDb event streamed out of the database and into EventBridge via our Pipe. What does the event look like? Let\u2019s take a look:{     \"version\": \"0\",     \"id\": \"REDACTED-ID\",     \"detail-type\": \"StockEvent\",     \"source\": \"soh.event\",     \"account\": \"REDACTED-ACCOUNT\",     \"time\": \"2024-05-02T07:04:50Z\",     \"region\": \"ap-southeast-2\",     \"resources\": [],     \"detail\": {         \"eventID\": \"REDACTED-EVENT-ID\",         \"eventName\": \"MODIFY\",         \"eventVersion\": \"1.1\",         \"eventSource\": \"aws:dynamodb\",         \"awsRegion\": \"ap-southeast-2\",         \"dynamodb\": {             \"ApproximateCreationDateTime\": 1714633489,             \"Keys\": {                 \"sku\": {                     \"S\": \"111837\"                 }             },             \"NewImage\": {                 \"sku\": {                     \"S\": \"111837\"                 },                 \"soh\": {                     \"N\": \"4\"                 }             },             \"OldImage\": {                 \"sku\": {                     \"S\": \"111837\"                 },                 \"soh\": {                     \"N\": \"11\"                 }             },             \"SequenceNumber\": \"REDACTED-SEQUENCE-NUMBER\",             \"SizeBytes\": 37,             \"StreamViewType\": \"NEW_AND_OLD_IMAGES\"         },         \"eventSourceARN\": \"REDACTED-ARN\"     } }Enter fullscreen modeExit fullscreen modeAs you can see, we\u2019ve got a standard DynamoDb stream event here, but pay particular attention to two areas that will be important further on. Firstly, the detail type and source. Those will be the same for every message, and will help with routing/filtering by rules.Then we have our old and new SOH figures in the OldImage and NewImage sections respectively. If you look at this specific example, you can see that based on our requirements this message shouldn\u2019t get sent to our 3rd party search index, because it\u2019s not a move from out of stock to in stock or vice versa.So with our event in EventBridge, what\u2019s next? That\u2019s where our EventBridge rules come in. EventBridge rules are a set of conditions tied to an event bus that tell EventBridge what data should be in the event to make it valid to trigger as well as defining one or more targets to send data to when that data matches the rule.Unsurprisingly, we have two rules set up in our SAM template. An \u2018in stock\u2019 rule and an \u2018out of stock\u2019 rule. Let\u2019s take a look at our in stock rule carefully, because a lot of the magic that lets us replace Lambda code is contained in these rules.InStockRule:     Type: AWS::Events::Rule     Properties:       Name: InStockRule       EventBusName: !Ref StockOnHandEventBus       EventPattern:         source:           - \"soh.event\"         \"detail-type\":           - \"StockEvent\"         detail:           eventSource:             - \"aws:dynamodb\"           eventName:             - \"MODIFY\"           dynamodb:             NewImage:               soh:                 N:                   - \"anything-but\": \"0\"             OldImage:               soh:                 N:                   - \"0\"       State: ENABLED       Targets:         - Arn: !GetAtt EventApiDestination.Arn           RoleArn: !GetAtt EventBridgeTargetRole.Arn           Id: \"StockOnHandApi\"           DeadLetterConfig:             Arn: !GetAtt PipeDLQueue.Arn           InputTransformer:             InputPathsMap:               sku: \"$.detail.dynamodb.NewImage.sku.S\"             InputTemplate: |               {                 \"sku\": <sku>,                 \"in_stock\": true               }Enter fullscreen modeExit fullscreen modeOur event rule uses an EventPattern to determine when to trigger. If you look at our pattern you can see it closely matches the structure of our DynamoDb stream event. The rule looks for the detail type and source of our event, and then interrogates the detail of the event. But here things get a little interesting. Rather than just look for constant values, we have some logic in our rule:NewImage:   soh:     N:       - \"anything-but\": \"0\" OldImage:   soh:     N:       - \"0\"Enter fullscreen modeExit fullscreen modeUsing \u2018content filtering\u2019 in the rule, we\u2019re able to express that we only want to trigger the rule when the event has a soh value in the NewImage section (the new figure) that is anything but 0, and an OldImage (the past figure) soh value that is exactly 0.That\u2019s how we ensure that the rule is triggered when something is \u2018in stock\u2019.We then define an EventApiDestination, that\u2019s basically telling the rule that our target will be an API that exists outside of AWS (more on that later) as well as the same DLQ (Dead Letter Queue) we mentioned before, for any failures / rejections from that API.Great! But, we have a problem. If you remember, our 3rd party API expects the format of data as:{     \"sku\": \"111837\",     \"in_stock\": false }Enter fullscreen modeExit fullscreen modeBut the DynamoDb stream data looks nothing like this? If we were using custom code, the transformation would be trivial, but what do we do without that option? Transformation to the rescue! EventBridge rules allow you to manipulate data and reshape it before sending it to the target.InputTransformer:   InputPathsMap:     sku: \"$.detail.dynamodb.NewImage.sku.S\"   InputTemplate: |     {       \"sku\": <sku>,       \"in_stock\": true     }Enter fullscreen modeExit fullscreen modeThis final part of the rule definition is essentially saying \u2018pick out the value from the JSON path detail.dynamodb.NewImage.sku.s, add it to the variable \u2018sku\u2019 then create a new JSON object that uses the variable, and provides an in_stock value of true.Huzzah! We\u2019re getting so close now.We won\u2019t go through the out of stock rule in detail because it\u2019s essentially the exact same rule with the exact same target, only our content filter is the exact inverse, and our transformation creatse a JSON object with in_stock: false.So let\u2019s recap. Our databse has streamed our event out, our pipe has gotten the event to our EventBridge bus, and our rules have ensured that a) we only get the events we want and b) those events are shaped as we require.Now we just need to send the event to the third party API, and that\u2019s where our rules \u2018target\u2019 comes in. A target can be a variety of AWS services (Lambda, SQS, Step Functions etc) but it can also be a standard API destination, even one that sits outside of AWS. To define that in our template, we use the following:EventApiConnection:     Type: AWS::Events::Connection     Properties:       Name: StockOnHandApiConnection       AuthorizationType: API_KEY       AuthParameters:         ApiKeyAuthParameters:           ApiKeyName: \"x-api-key\"           ApiKeyValue: \"xxx\"       Description: \"Connection to API Gateway\"    EventApiDestination:     Type: AWS::Events::ApiDestination     Properties:       Name: StockOnHandApiDestination       InvocationRateLimitPerSecond: 10       HttpMethod: POST       ConnectionArn: !GetAtt EventApiConnection.Arn       InvocationEndpoint: !Ref ApiDestinationEnter fullscreen modeExit fullscreen modeWe define a connection, which holds the authentication mechanism for our API (though in our case we\u2019re just passing some dummy values as we\u2019re using a special \u2018mock\u2019 API Gateway our team uses for integration tests) and then the API destination itself, which is where we describe the request as a POST request, to a specific URL, and set a rate limit to ensure we don\u2019t flood the API.And that\u2019s it! We are done. A complete solution without any custom code. So how about we deploy it and see if it works?Our Solution in ActionBecause we\u2019ve opted to use SAM to express our IaC (Infrastructure-as-Code) in a template, we get access to all the wonders and magic of the SAM CLI. This includes \u2018sam sync\u2019. This command allows us not only to deploy our template to AWS, but when combined with the \u2018watch\u2019 parameter, means if we make any changes to our template locally, the change will be automatically synced to the cloud without us even needing to think about deploying.Awesome, let\u2019s give it a shot.PS C:\\Users\\JMatson\\source\\repos\\SohWithEventBridge\\SohWithEventBridge> sam sync --watch --stack-name SohWithEventBridge --template serverless.yaml --parameter-overrides ApiDestination=https://get-your-own.comYou\u2019ll notice I\u2019m passing in the URL of the \u2018third party API\u2019 when deploying the template. This is because the endpoint parameter in the template has no useful value, so if you decide to grab the repo and have a go yourself, you\u2019ll need to supply an API that can accept the post request.By passing in a parameter override, we\u2019re populating the below parameter:Parameters:   ApiDestination:     Type: String     Default: \"<Your API here>\"Enter fullscreen modeExit fullscreen modeAfter a few minutes, our entire solution should be deployed to AWSA quick spot check of our resources:DynamoDb table with stream enabled? CheckOur Pipe set up with the right source and target? CheckOur Event bus and rules?Let\u2019s check one of the rules to make sure it has the right filtering, target and transformation set up:Looking good, so it\u2019s time to test this thing out.Because I\u2019m a nice person and I\u2019m all about the developer happiness these days, I\u2019ve included a nifty little python script in the repository that we\u2019re going to use to do some tests.You can grab it from:test scriptIt\u2019s not elegant, but it\u2019ll do. It\u2019s job will be to simulate activity by inserting a few items into our stock on hand table with stock figures, then updating them \u2014 possibly more than once \u2014 to validate different scenarios (in and out of stock as well as neither).The updates are:# Insert items put_ddb_item('188273', 0) put_ddb_item('723663', 20) put_ddb_item('111837', 50)  # Update items update_ddb_item('188273', 5) update_ddb_item('723663', 15) update_ddb_item('111837', 10)  # Additional update update_ddb_item('111837', 0) update_ddb_item('111837', 11) update_ddb_item('111837', 4)Enter fullscreen modeExit fullscreen modeLet\u2019s run it, and check our results:(.venv) PS C:\\Users\\JMatson\\source\\repos\\SohWithEventBridge\\SohWithEventBridge\\TestScripts> py test_script.py INFO:botocore.credentials:Found credentials in shared credentials file: ~/.aws/credentials INFO:root:Put item 188273 into DynamoDB table INFO:root:Consumed capacity: {'TableName': 'StockOnHand', 'CapacityUnits': 1.0} INFO:root:Put item 723663 into DynamoDB table INFO:root:Consumed capacity: {'TableName': 'StockOnHand', 'CapacityUnits': 1.0} INFO:root:Put item 111837 into DynamoDB table INFO:root:Consumed capacity: {'TableName': 'StockOnHand', 'CapacityUnits': 1.0} INFO:root:Update item 188273 in DynamoDB table INFO:root:Consumed capacity: {'TableName': 'StockOnHand', 'CapacityUnits': 1.0} INFO:root:Update item 723663 in DynamoDB table INFO:root:Consumed capacity: {'TableName': 'StockOnHand', 'CapacityUnits': 1.0} INFO:root:Update item 111837 in DynamoDB table INFO:root:Consumed capacity: {'TableName': 'StockOnHand', 'CapacityUnits': 1.0} INFO:root:Update item 111837 in DynamoDB table INFO:root:Consumed capacity: {'TableName': 'StockOnHand', 'CapacityUnits': 1.0} INFO:root:Update item 111837 in DynamoDB table INFO:root:Consumed capacity: {'TableName': 'StockOnHand', 'CapacityUnits': 1.0} INFO:root:Update item 111837 in DynamoDB table INFO:root:Consumed capacity: {'TableName': 'StockOnHand', 'CapacityUnits': 1.0} (.venv) PS C:\\Users\\JMatson\\source\\repos\\SohWithEventBridge\\SohWithEventBridge\\TestScripts>Enter fullscreen modeExit fullscreen modeOkay, so the script has run, inserted some products and updated them. Now if all is working and we focus on product sku 111837, we should expect the following:# Insert items put_ddb_item('111837', 50) <- This wont count as its an INSERT and we filter for MODIFY only.  # Update items update_ddb_item('111837', 10) <- 50 to 10 shouldnt filter through  # Additional update update_ddb_item('111837', 0) <- 10 to 0 we should see update_ddb_item('111837', 11) <- 0 to 11 we should see update_ddb_item('111837', 4) <- 11 to 4 shouldnt filter through So we should get 2 of the 5 events through.Enter fullscreen modeExit fullscreen modeNow you can check the monitoring at various points through the event-driven journey by checking invocation triggers for our Pipe or Rules, but we\u2019re going straight to our final target \u2014 our API \u2014 to check its logs using a simple CloudWatch Insights query.With all the transformation and filtering we\u2019ve done previously, we should only see a nice, neat JSON payload that tells us something is in stock, or out of stock:**CloudWatch Logs Insights**     region: ap-southeast-2     log-group-names: API-Gateway-Execution-Logs_t23ilm51j6/dev     start-time: -300s     end-time: 0s     query-string:Enter fullscreen modeExit fullscreen modefields @timestamp,@message, @logStream,@log| sort @timestamp desc| filter@messagelike '111837'--- | @timestamp | @message | @logStream | @log | | --- | --- | --- | --- | | 2024-05-02 11:57:27.388 | {   \"sku\": \"111837\",   \"in_stock\": true } | 26267e5fba9c96a4989c9b712553f791 | 712510509017:API-Gateway-Execution-Logs_t23ilm51j6/dev | | 2024-05-02 11:57:25.908 | {   \"sku\": \"111837\",   \"in_stock\": false } | 527de3f583aa2547d2819f2328657427 | 712510509017:API-Gateway-Execution-Logs_t23ilm51j6/dev | ---Enter fullscreen modeExit fullscreen modeHuzzah! Success. We get SKU 111837 having posted both an in stock and out of stock event to our third party API, thus concluding the journey of our Lambdaless and codeless event-driven solution.Not too shabby, eh? Well if you\u2019re not impressed that\u2019s okay, I\u2019m impressed enough for the both of us. While I\u2019m a software engineer at heart and I\u2019ll always enjoy the act of writing code, there\u2019s no denying the power and flexibility that can come from being able to combine native services through configuration alone to deliver on real world use cases.What are your thoughts about leaning into native services and cutting back on the use of code itself to solve problems? Let me know in the comments and \u2014 as always \u2014 if you enjoyed the article feel free to leave a like, claps or whatever you feel is appropriate.As a reminder \u2014 you can grab the complete repo for this guide below:https://github.com/kknd4eva/SohWithEventBridge"}
{"title": "How to Speedup Your CDK Test Feedback", "published_at": 1715190955, "tags": ["aws", "cicd", "cloud", "technology"], "user": "Joris Conijn", "url": "https://dev.to/aws-builders/how-to-speedup-your-cdk-test-feedback-19kk", "details": "When you deploy lambda functions using CDK and a test-driven approach, you might have noticed that the test feedback takes longer each time you add a new function. That\u2019s because each function will be bundled with its own set of dependencies. These dependencies need to be downloaded before CDK can bundle them into a single package.CDK SynthBefore you can deploy your infrastructure you need to synthesize your code. This will transform your code into a CloudFormation template plus some assets. When you have a lambda function the content of this function will be bundled into one of these assets.This all makes sense when you deploy your code. You need to ship the actual lambda code to AWS in the correct format with the correct dependencies. Otherwise, your solution will not work.But the synth step is also used when you test your infrastructure code. The process is like the deploy step. Instead of deploying your templates, you will inspect the outcome. For example, you are expecting a Lambda function. This function will use a certain runtime. And, for production environments, you want to confirm that the backup tags are applied.Skip the bundling when you run the tests!It might be an obvious answer.Skip the bundling when you run the tests!Assume we use the following code to create a Lambda function:const sampleFunction = new lambda.Function(this, 'SampleFunction', {   code: lambda.Code.fromAsset('./sample_function/', {     bundling: {       command: ['bash', '-c', 'pip install --target /asset-output -r ./requirements.txt && cp -au . /asset-output'],       image: lambda.Runtime.PYTHON_3_12.bundlingImage,     },   }),   runtime: lambda.Runtime.PYTHON_3_12,   handler: 'index.handler',   timeout: cdk.Duration.seconds(60),   memorySize: 1024,  });Enter fullscreen modeExit fullscreen modeYou can see that we are loading the code from a folder calledsample_function. Then we pass the options on how CDK needs to bundle the function:First, the dependencies will be downloaded and installed.Next, the content from thesample_functionis copied.The content of theasset_outputfolder will be the bundled function.Now, these steps will become part of the test. You can skip the bundling by defining the test as followed:import * as cdk from 'aws-cdk-lib'; import { Template } from 'aws-cdk-lib/assertions'; import { AppStack } from './AppStack';  test('Sample unit test on how to skip bundling', () => {  // Setup the application  const app = new cdk.App({    context: {      'aws:cdk:bundling-stacks': [],    },  });   // Setup the CDK stack  const stack = new AppStack(app, 'MyTestStack', {    backups: true, // For production we enable backups  });   // Synth the code  const template = Template.fromStack(stack);   // Confirm that the Lambda function uses python 3.12 runtime  template.hasResourceProperties('AWS::Lambda::Function', {    Runtime: 'python3.12',  });   // Confirm that the DynamoDB table has the Backup tag  template.hasResourceProperties('AWS::DynamoDB::Table', {    Tags: [      {        Key: 'Backup',        Value: 'YES',      },    ],  }); });Enter fullscreen modeExit fullscreen modeA few things are happening in this example:When the app object is being created, some specific context is supplied. This context will overwrite theaws:cdk:bundling-stacksoption. This means that the bundling options are overwritten with nothing.Then the stack object is created and a template is synthesized.We will confirm that we have a lambda function with the Python 3.12 runtime.We will also confirm that the DynamoDB table has the Backup tag with the value YES.Because we skipped the bundling the test will execute immediately. Giving you instant feedback on the code that you have been writing. You can now run the test after each change. This enables you to create a nice development cycle:Adding a test implementation.Adding the business logic.Running the tests to confirm the implementation.RepeatWhy is it ok to skip the bundling?When you write tests you should confirmyourbusiness logic. The bundling logic has already been tested by the team that is maintaining CDK. So you don\u2019t need to test it and it\u2019s safe to skip the bundling.You could argue that the options in the bundling need validation too. And yes, that is now skipped. Yet the bundling of these functions is usually always the same for each runtime. And when you deploy you will catch any bundling mistakes anyway. Next to that, you have tests for your lambda code as well. These tests are not executed against the bundled version of your code. These are executed against the source version of your code. So there is no need to bundle them from a testing perspective.ConclusionWhen it takes too long to run tests, you will run them less. This has a negative impact on your quality and efficiency. You can solve this by excluding heavy operations. Operations like downloading dependencies and bundling your lambda functions. This also helps with focussing on the tests that matter. The tests that actually confirm your business value.Follow me if you want to learn more about improving your efficiency and code quality.Photo byPixabayThe postHow to Speedup Your CDK Test Feedbackappeared first onXebia."}
{"title": "AWS SnapStart - Part 20 Measuring warm starts with Java 17 using different Lambda memory settings", "published_at": 1715178920, "tags": ["aws", "java", "serverless", "coldstart"], "user": "Vadym Kazulkin", "url": "https://dev.to/aws-builders/aws-snapstart-part-20-measuring-warm-starts-with-java-17-using-different-lambda-memory-settings-1p7j", "details": "IntroductionIn thepart 13of our series we measured the warm starts of the Lambda function with Corretto Java 21 runtime without SnapStart enabled, with SnapStart enabled and also applied DynamoDB invocation priming optimization with different memory settings.In this article we'll also provide measurements for the warm execution times for this use case for Java 17 (and compare them with Java 21), as using different Lambda memory settings not only affect the cold start times but also the warm execution times and Lambda costs. I'll also refer to and build on the measurements in my previous articleMeasuring cold starts and deployment time with Java 17 using different Lambda memory settings.Measuring warm starts with Java 17 with and without SnapStart enabled using different Lambda memory settingsIn our experiment we'll re-use the application introduced inpart 8for this. Here is the code for thesample application. There are basically 2 Lambda functions which both respond to the API Gateway requests and retrieve product by id received from the API Gateway from DynamoDB. One Lambda function GetProductByIdWithPureJava17Lambda can be used with and without SnapStart and the second one GetProductByIdWithPureJava17LambdaAndPriming uses SnapStart and DynamoDB request invocation priming. We'll measure cold and warm starts using the following memory settings in MBs : 256, 512, 768, 1024, 1536 and 2048.I also put the cold starts measured in thepart 19into the tables to see both cold and warm starts in one place. The results of the experiment below were based on reproducing more than 100 cold and approximately 100.000 warm starts for the duration of our experiment which ran for approximately 1 hour. Here is the code for thesample application. For it (and experiments from my previous article) I used the load test toolhey, but you can use whatever tool you want, likeServerless-artilleryorPostman.Abbreviationcis for the cold start andwis for the warm start.Cold (c) and warm (w) start times without SnapStart  in ms:RAMc p50c p75c p90c p99c p99.9c maxw p50w p75w p90w p99w p99.9w max256 MB7309.667432.67575.517662.517782.257962.017.1615.5827.1673.825364.095914.32512 MB4213.374256.074325.174496.154661.234786.716.016.939.9933.9281.202731.29768 MB3310.083414.93551.824271.484421.094594.426.217.279.3826.311530.711833.731024 MB2880.532918.792974.453337.293515.863651.656.117.058.9423.5462.991272.961536 MB2390.152434.522464.462668.952812.152987.046.016.838.3920.0951.24839.922048 MB2198.022272.52397.972757.062892.653005.316.617.879.9924.69600.02895.15Cold (c) and warm (w) start times with SnapStart without Priming  in ms:RAMc p50c p75c p90c p99c p99.9c maxw p50w p75w p90w p99w p99.9w max256 MB4972.835227.665754.127551.767559.317562.56.4013.0921.7572.66242.794973.05512 MB2550.592604.692765.672942.483108.763110.695.826.839.6833.9285.162052.3768 MB1801.281887.922251.032604.692681.292681.346.016.939.0827.1693.671550.81024 MB1521.331578.641918.352113.652115.772117.426.017.058.9423.92101.411077.451536 MB1204.061325.321507.701817.561821.191821.65.926.838.3922.45609.62896.532048 MB1129.451286.171583.381819.371998.602000.176.307.519.3825.09472.89863.68Cold (c) and warm (w) start times with SnapStart and with DynamoDB invocation Priming in ms:RAMc p50c p75c p90c p99c p99.9c maxw p50w p75w p90w p99w p99.9w max256 MB1126.071183.771397.421608.901621.821622.236.7213.7224.3071.52511.95753.45512 MB831.84881.491136.241348.031386.291387.525.826.7210.1532.86254.63444.51768 MB819.46891.231141.931243.191808.501809.345.736.618.5224.3083.822073.351024 MB692.79758.001003.801204.061216.151216.886.217.279.3825.09103.03256.651536 MB713.17773.31995.801124.941372.501372.736.017.058.9423.92103.03262.892048 MB797.64858.871080.861296.491376.621377.056.417.519.3825.89115.14241.28ConclusionsIn this article we measured the warm start time of the Lambda function without SnapStart with SnapStart and for the latter with additional priming of DynamoDB invocation using different Lambda memory settings.In case of not enabling SnapStart for the Lambda function we observed that increasing memory reduces the warm execution time for our use case especially for p>90. As adding more memory to the Lambda function is also a cost factor, the sweet spot between cold and warm start time and cost is around 1024 MB memory setting for the Lambda function for our use case. You can useAWS Lambda Power Tuningfor very nice visualisations.We made similar observations in the case of enabling SnapStart for the Lambda function without and with additionally using priming of DynamoDB invocation request.In case of applying priming of the DynamoDB invocation it's still the same observation and there is only little to no improvement of the warm execution times using more than 1024 MB memory even for p99, p99.9 and maximal values.Comparing the measurements with the same measurement done inpart 13but for Java 21 we observe that the overallwarm start timesare lower with Java 21 compared to Java 17 for nearly all percentiles but especially visible for the higher ones."}
{"title": "CDK CodeChecker v2: Breaking Free from Third-Party Dependencies", "published_at": 1715171971, "tags": ["cdk", "pullrequests", "aws", "codechecker"], "user": "Yvo van Zee", "url": "https://dev.to/aws-builders/cdk-codechecker-v2-breaking-free-from-third-party-dependencies-3ob5", "details": "BackgroundHave you ever relied on a tool only to have it unexpectedly fail? I recently experienced this with CodeChecker, a tool that automatically checks Pull Requests. Despite no code changes, it suddenly stopped working. Upon investigation, it became clear that the underlying software had become deprecated.As you might have seen in my previous blogs, I like to use the Cloud Development Kit (CDK) for my daily Infrastructure as Code (IaC) work with AWS. During my current assignment, creating a data analytics platform for an enterprise in the financial sector, we are using theCDK CodeCheckera lot. It helps us streamline the development process, keep code quality high, make peer reviews a standard, and test changes in an automated manner.Real World ScenarioIn my current assignment, we encountered a problem with the CodeChecker. The problem is in the 3rd party cloud components construct we are using. It seems that the construct is not maintained by the creator/community. So now it raises the following warning:[WARNING] aws-cdk-lib.CustomResourceProviderRuntime#NODEJS_14_X is deprecated.Enter fullscreen modeExit fullscreen modeThe warning basically means that AWS has NodeJS 14 deprecated and you should use higher versions of node. When you fix this with for example an aspect that bumbs your Lambda functions versions used by the custom resource, you will enter in new errors like:10:22:02 AM | CREATE_FAILED        | Custom::ApprovalRuleTemplate                      | xxx-xxxx-xxx...omResource/Default Received response status[FAILED] from custom resource. Message returned: Error: Cannot find module'aws-sdk'Require stack: - /var/task/index.js - /var/task/__entrypoint__.js - /var/runtime/index.mjs at Module._resolveFilename(node:internal/modules/cjs/loader:1134:15)at Module._load(node:internal/modules/cjs/loader:975:27)at Module.require(node:internal/modules/cjs/loader:1225:19)at require(node:internal/modules/helpers:177:18)at Object.<anonymous>(/var/task/index.js:92:18)at __webpack_require__(/var/task/index.js:21:30)at Object.<anonymous>(/var/task/index.js:102:19)at __webpack_require__(/var/task/index.js:21:30)at /var/task/index.js:85:18 at Object.<anonymous>(/var/task/index.js:88:10)(RequestId: df6317ec-19e3-4aec-ab93-e7da70596c82)Enter fullscreen modeExit fullscreen modeLong story short, it is broken. And, since this is a high-level L3 CDK Construct, a lot has been taken care of and abstracted away. Another problem we had with this construct was that it was not compliant according to the security framework used by the enterprise. So it felt a bit like ahoutje-touwtje(Band-Aid) solution. So as the current problems made the CodeChecker failing completely it was time to make an own version.SolutionThere are three solutions to the problem.Contributing to the Cloud Componentsgithub repository. Unfortunately, this option was doomed for failure as the project isn't maintained and a lot of open pull requests from other developers are still pending. Also solutions to the current problem were already submitted but not merged at all.Creating a fork of the Cloud Components repository and start maintaining ourselves. The problem here is that the Cloud Components repository consisted of more software than only the CodeChecker, which is basically a combination of two projects already. So this would require maintaining a lot more code than anticipated.Simplify the CodeChecker using Standard CDK libraries. Eventually, this felt best and required the least amount of maintainability!In the meantime AWS alsoreleased a blog on this. We can use this as a reference. Let's optimize the CDK CodeChecker to version 2.0.Go BuildWe want to get rid of the cloudcomponents construct. As a starting point I am using theCodeChecker codefrom my previous blog. The main responsibility for the cloudcomponents construct is to create an approval template, assign it to a repository and create a codebuild project. For the last there is CDK/CloudFormation intergration, but for the first two there isn't, it's basically 1 API call for creation of an approval template, and 1 for the assign to the repository. So how can we tackle this. Well CDK has a custom resource framework especially for single API calls, called theAwsCustomResource.Looking at thedocumentationof how to create an approval template, you can use a json template. The template needs to have a 'DestinationReferences', which describes to which branch the template looks; a 'Type', the amount of approvals needed called; 'NumberOfApprovalsNeeded', and whom is allowed to approve; the 'ApprovalPoolMembers'. So let's create the json template:importjsontemplate={'Version':'2018-11-08','DestinationReferences':[f'refs/heads/{branch}'],'Statements':[{'Type':'Approvers','NumberOfApprovalsNeeded':required_approvals,'ApprovalPoolMembers':[f'arn:aws:sts::{Stack.of(self).account}:assumed-role/developer/*',f'arn:aws:sts::{Stack.of(self).account}:assumed-role/{self.codechecker_role.role_name}/*',]}]}json_string=json.dumps(template)Enter fullscreen modeExit fullscreen modeAbove you see the template which we can use in our CDK python code. As you can see it allows the developer role and the CodeChecker itself as approvers. The branch and required_approvals are variables so we can create a loop for multiple branches. On the last line we create a json_string, as the API call is expecting a string instead of an object.Approval templateNow create the approval template with the AwsCustomResource:fromaws_cdkimport(custom_resources,aws_ec2)create_approval_template=custom_resources.AwsCustomResource(self,f'CreateApprovalTemplateFor{branch}On{repository_name}',on_create=custom_resources.AwsSdkCall(service='CodeCommit',action='createApprovalRuleTemplate',physical_resource_id=custom_resources.PhysicalResourceId.of(f'{str(required_approvals)}-approval-for-{self._repository.repository_name}-{branch}'),parameters={'approvalRuleTemplateName':f'{str(required_approvals)}-approval-for-{self._repository.repository_name}-{branch}','approvalRuleTemplateDescription':f'Requires{required_approvals}approvals from the team to approve the pull request','approvalRuleTemplateContent':json_string,}),on_update=custom_resources.AwsSdkCall(service='CodeCommit',action='updateApprovalRuleTemplateContent',parameters={'approvalRuleTemplateName':f'{str(required_approvals)}-approval-for-{self._repository.repository_name}-{branch}','newRuleContent':json_string,}),on_delete=custom_resources.AwsSdkCall(service='CodeCommit',action='deleteApprovalRuleTemplate',parameters={'approvalRuleTemplateName':f'{str(required_approvals)}-approval-for-{self._repository.repository_name}-{branch}'}),policy=custom_resources.AwsCustomResourcePolicy.from_sdk_calls(resources=custom_resources.AwsCustomResourcePolicy.ANY_RESOURCE),vpc=vpc,vpc_subnets=aws_ec2.SubnetSelection(subnet_type=aws_ec2.SubnetType.PRIVATE_ISOLATED),)Enter fullscreen modeExit fullscreen modeSo what happens here, well we are using the AwsCustomResource provider to address single API calls. The provider always uses a on_create, on_update and on_delete action. This is needed to handle the same CloudFormation actions when a stack/resource is created, updated or deleted. You can do without the on_update or on_delete, but that will result in an orphan resource when you clean up your stack, as it will never be deleted.on_create:The API call we need iscreateApprovalRuleTemplatefor the service CodeCommit. The physical_resource_id is needed to keep track of updates and deletes. At the parameter section we provide input for the API call. Here you can see that we use our template for the parameter: approvalRuleTemplateContent.on_update:The on_update part is using a different API call, updateApprovalRuleTemplateContent, to update the template itself. So for example when we want more people or roles to allow the approval, we need to update the json template and do the API call to update the template in CodeCommit.on_delete:The on_delete is basically reverting back the on_create API call. So using the deleteApprovalRuleTemplate call we can remove the template.I've added VPC configuration here, to make sure that the lambda's created by the provider framework will run in a VPC, this is mandatory at my current client.Template associationNow we have code for our template creation, we also need to associate the template with a repository. Again like the creation, there is no CloudFormation support for this, but there areAPI calls. The resource looks the same in essence, but using different API calls.associate_approval_template=custom_resources.AwsCustomResource(self,f'AssociateApprovalTemplateFor{branch}On{repository_name}',on_create=custom_resources.AwsSdkCall(service='CodeCommit',action='associateApprovalRuleTemplateWithRepository',physical_resource_id=custom_resources.PhysicalResourceId.of(f'{str(required_approvals)}-approval-for-{self._repository.repository_name}-{branch}-association'),parameters={'approvalRuleTemplateName':f'{str(required_approvals)}-approval-for-{self._repository.repository_name}-{branch}','repositoryName':self._repository.repository_name}),on_delete=custom_resources.AwsSdkCall(service='CodeCommit',action='disassociateApprovalRuleTemplateFromRepository',parameters={'approvalRuleTemplateName':f'{str(required_approvals)}-approval-for-{self._repository.repository_name}-{branch}','repositoryName':self._repository.repository_name}),policy=custom_resources.AwsCustomResourcePolicy.from_sdk_calls(resources=custom_resources.AwsCustomResourcePolicy.ANY_RESOURCE),vpc=vpc,vpc_subnets=aws_ec2.SubnetSelection(subnet_type=aws_ec2.SubnetType.PRIVATE_ISOLATED),)associate_approval_template.node.add_dependency(create_approval_template)Enter fullscreen modeExit fullscreen modeFor the linking of the template with a repository there are only two API calls unfortunately. The associateApprovalRuleTemplateWithRepository and disassociateApprovalRuleTemplateFromRepository API call. Therefore we skip the \"on_update\" call. As we cannotdo two API calls in 1 on_update, like disassociate and then associate again, we lack the update function here. But that is fine for this use case.Last we add a dependency between the associate and creation, as the associate can only happen when the template is created.CodeChecker CodeBuild projectWith the approval templates in place we also need the CodeChecker project itself. In my previous blog we used the cloudcomponents PullRequestCheck resource for that. Let's recreate that with native AWS resources. Starting with the CodeBuild project:from aws_cdk import aws_codebuild  pullrequest_project = aws_codebuild.Project(     self,     f\"PullRequestCheckFor{branch}\",     project_name=f\"{repository_name.lower()}-codechecker-{branch}\",     source=aws_codebuild.Source.code_commit(repository=self._repository),     role=self.codechecker_role,     environment=aws_codebuild.BuildEnvironment(build_image=aws_codebuild.LinuxBuildImage.STANDARD_6_0),     build_spec=aws_codebuild.BuildSpec.from_object_to_yaml(         {             \"version\": \"0.2\",             \"env\": {\"git-credential-helper\": \"yes\"},             \"phases\": {                 \"install\": {                     \"commands\": [                         \"npm install -g aws-cdk\",                         \"pip install -r requirements.txt\",                     ]                 },                 \"build\": {                     \"commands\": [                         \"cdk synth\",                     ]                 },                 \"post_build\": {                     \"commands\": [                         \"pytest --junitxml=reports/codechecker-pytest.xml > pytest-output.txt\",                         'if grep -i \"passed\" pytest-output.txt; then PYTEST_RESULT=\"PASSED\"; else PYTEST_RESULT=\"FAILED\"; fi',                         'if [ $PYTEST_RESULT != \"PASSED\" ]; then PR_STATUS=\"REVOKE\"; else PR_STATUS=\"APPROVE\"; fi',                         \"echo $PR_STATUS\",                         \"REVISION_ID=$(aws codecommit get-pull-request --pull-request-id $PULL_REQUEST_ID | jq -r '.pullRequest.revisionId')\",                         \"aws codecommit update-pull-request-approval-state --pull-request-id $PULL_REQUEST_ID --revision-id $REVISION_ID --approval-state $PR_STATUS --region $AWS_REGION\"                     ]                 },             },             \"reports\": {                 \"pytest_reports\": {                     \"files\": [\"codechecker-pytest.xml\"],                     \"base-directory\": \"reports\",                     \"file-format\": \"JUNITXML\"                 }             }         }     ),     vpc=vpc,     subnet_selection=aws_ec2.SubnetSelection(         subnet_type=aws_ec2.SubnetType.PRIVATE_ISOLATED     ), )Enter fullscreen modeExit fullscreen modeThis is a CodeBuild project which uses the repository as a source. The project installs CDK plus the requirement.txt file with pip in the install phase. As a pre-test in the build stage, CodeBuild tries to do a CDK synth action, to check if it can generate CloudFormation templates. In the post_build phase CodeBuild runs the pytest. It also creates a junitxml file as a report, which you can see in CodeBuild in the console as evidence, see also the reports part. The output is saved in a text file. This is because the if statement checks if the pytest passes or not. If the pytest was successful, it stores \"APPROVE\" value in the PR_STATUS variable. This variable then is used to add an approval to the Pull Request. In all other cases it will revoke the Pull Request. We are using some variables needed to get the revision id and update the approval state.Catching the eventWith the CodeChecker project created, we need something to trigger the project when a PR is created. Before the cloudcomponents construct was responsible for that, but we will use EventBridge instead:fromaws_cdkimportaws_eventspull_request_rule=aws_events.Rule(self,f\"OnPullRequest{branch}EventRule\",event_pattern=aws_events.EventPattern(source=[\"aws.codecommit\"],resources=[self.repository.repository_arn],detail={\"event\":[\"pullRequestCreated\",\"pullRequestSourceBranchUpdated\",]},),)pull_request_rule.add_target(target=aws_events_targets.CodeBuildProject(pullrequest_project,event=aws_events.RuleTargetInput.from_object({\"sourceVersion\":aws_events.EventField.from_path(\"$.detail.sourceCommit\"),\"environmentVariablesOverride\":[{\"name\":\"DESTINATION_COMMIT_ID\",\"type\":\"PLAINTEXT\",\"value\":aws_events.EventField.from_path(\"$.detail.destinationCommit\"),},{\"name\":\"PULL_REQUEST_ID\",\"type\":\"PLAINTEXT\",\"value\":aws_events.EventField.from_path(\"$.detail.pullRequestId\"),},{\"name\":\"SOURCE_COMMIT_ID\",\"type\":\"PLAINTEXT\",\"value\":aws_events.EventField.from_path(\"$.detail.sourceCommit\")},{\"name\":\"REPOSITORY_NAME\",\"type\":\"PLAINTEXT\",\"value\":aws_events.EventField.from_path(\"$.detail.repositoryNames[0]\"),}]}),))Enter fullscreen modeExit fullscreen modeFirst we create a rule to check on two type of events, the pullRequestCreated and pullRequestSourceBranchUpdated event. Basically this catches when a Pull Request has been created or the existing Pull Request has been updated.The CodeBuild project of CodeChecer will be configured as a target. Important here is the environmentVariablesOverride, this makes it possible to use the DESTINATION_COMMIT_ID, PULL_REQUEST_ID, SOURCE_COMMIT_ID and REPOSITORY_NAME inside the CodeBuild project as environment variables. We needed these for getting the REVISION_ID and updating the Pull Request approval state.SummaryBy replacing the cloudcomponents constructs with AWS native resources, we make ourselves less dependent on third party constructs. We only need four blocks of code plus the approval template definition to completely replace the outdated cloudcomponents constructs. The resources are \"maintained\" by the CDK project and thus can follow your normal update cycle of CDK.By replacing the cloudcomponents constructs with AWS native resources, we make ourselves less dependant on third-party constructs. Only four blocks of code plus the approval template definition are needed to completely replace the outdated cloudcomponents constructs. These resources are maintained by the CDK project and can follow your normal update cycle of CDK.Try yourselfReady to simplify your CodeChecker setup and reduce dependencies on third-party constructs? Dive into the world of AWS native resources and optimize your CDK CodeChecker with the provided guidance. Check out the code and additional resources onGitHubto get started today!"}
{"title": "Top 3 Cloud and DevOps Projects To Supercharge Your Resume", "published_at": 1715145894, "tags": ["aws", "devops", "cloud", "kubernetes"], "user": "piyushsachdeva", "url": "https://dev.to/aws-builders/top-5-cloud-and-devops-projects-to-supercharge-your-resume-62f", "details": "IntroductionIn this blog, we'll be discussing three amazing DevOps and Cloud projects that you should have in your resume, and when I say have in your resume, it doesn't mean you add in your resume. It means that you should Implement those projects by yourself, get the learning out of it, create some useful artifacts like blogs or GitHub repositories, and then you should add them to your resume so that the recruiters/hiring managers will know that these projects you have implemented by yourself.If you prefer watching a 10-minute video with all the details, you can check the below link; else, you can continue with the blog:What can you expect from this blog?In addition to sharing project ideas and steps, I will provide a complete end-to-end solution through blog posts, GitHub repositories, and YouTube videos. Anyone interested in implementing the solution can learn how to do so using these resources as guidance.Project 1: Static website hosting and CICDDifficulty Level:BeginnerFocus Area:Cloud Storage, CDN and CICDHost a static website on the cloud of your choice, either AWS, Azure, or GCP, and Implement cicd on that.Technologies Covered \ud83d\udcda ( Use either of the below options)AWS: Amazon S3, CloudFront, and Route 53Azure: Azure Storage, Azure CDN, and DNS ManagementGoogle Cloud: Cloud Storage, Load Balancing, and Content Delivery Networks (CDN)Get Started \ud83d\ude80Review the project requirements. \u2714\ufe0fDive into AWS, Azure, or GCP documentation to familiarize yourself with the services mentioned. \ud83d\udcd6Start building your architecture diagram. \ud83c\udfd7\ufe0fDocument your progress and implementation steps in a blog or GitHub Readme. \ud83d\udcddChallenges Faced: Discuss any challenges you encountered and how you overcame them. \ud83e\udd14Key Takeaways: Share what you learned from this project. \ud83e\uddd0Resources: List any helpful resources or references you used. \ud83d\udcdaReference resources: \u2705\ud83d\udca1 If you are an absolute beginner to the cloud and CICD, get yourself. Familiarize with the concepts, you can refer to the below documentation and study material:For AWShttps://docs.aws.amazon.com/AmazonS3/latest/userguide/HostingWebsiteOnS3Setup.htmlhttps://docs.aws.amazon.com/AmazonS3/latest/userguide/website-hosting-cloudfront-walkthrough.htmlFor Azurehttps://learn.microsoft.com/en-us/azure/storage/blobs/storage-blob-static-website-how-to?tabs=azure-portalhttps://learn.microsoft.com/en-us/azure/storage/blobs/storage-custom-domain-name?tabs=azure-portalFor GCPhttps://cloud.google.com/storage/docs/hosting-static-websitehttps://www.cloudskillsboost.google/focuses/1251?parent=catalogIf you are a visual learner, feel free to check out the video solutionArchitecture Diagram for AWS by AnkitBlog containing step-by-step instructions for AWSBlog by AnkitArchitecture Diagram for Azure by NishantBlog By Nishant containing the step-by-step instructions for AzureBlog by NishantProject #2: 3-tier architectureDifficulty Level:IntermediateFocus Area: Cloud Infrastructure, Networking, 3-tier applicationFor this project, your objective is to design a 3-tier architecture in a cloud platform, such as AWS, Azure, or GCP, with a focus on the following key considerations:High Availability: Ensure your architecture is highly available, capable of withstanding failures, and can provide uninterrupted service.Scalability: Design your architecture to be highly scalable, allowing for easy and efficient resource scaling as your application's demands increase.Fault Tolerance: Implement fault-tolerant mechanisms to minimize downtime and service interruptions during failures.Custom VPC/VNET: Consider using a custom Virtual Private Cloud (VPC) for AWS or GCP, a Virtual Network (VNET) for Azure, or a similar network customization instead of relying on the default configurations.Security: Prioritize security by adhering to best practices for Identity and Access Management (IAM) and implementing robust security measures.Your design should consider these considerations, resulting in a well-structured, efficient, and secure 3-tier architecture.You can use your chosen cloud platform to implement this architecture effectively.Reference resources: \u2705\ud83d\udca1 To ensure you're ready to take on this challenge, it's essential to have a solid understanding of networking concepts. Check out the following resources for guidance:\ud83d\udc49 AWS:Click Here\ud83d\udc49 Azure:Click Here\ud83d\udc49 GCP:Click Here\ud83d\ude80IP address calculation/CIDR and Subnet Masks:Click hereIf you are a video person, feel free to check out the below video for end to end solution:\ud83d\udc47Architecture Diagram for AWSImage SourceArchitecture Diagram for AzureImage SourceDetailed workshop for AWS with step-by-step instructions:Workshop linkDetailed blog for Azure for step-by-step instructionsBlog by Nishant SinghProject 3: Implement a 2-tier architecture in AWS, Azure, or GCP using Terraform \ud83d\ude80Difficulty Level:IntermediateFocus Area:Iac using Terraform, Custom modules, Infrastructure.This project aims to leverage the best practices of infrastructure as code (IaC) to create a reusable and shareable infrastructure setup. Our focus is on promoting modularity, flexibility, and maintainability.Key Guidelines1. Leverage Custom ModulesBuild custom modules to break your infrastructure code into reusable and shareable components. This approach organizes your code and allows other team members to incorporate and adapt the components easily for their specific needs.2. Use Variables and Data SourcesPlease implement variables and data sources in your IaC code to enhance flexibility and maintainability. Variables make it easier to adapt and modify configuration settings, while data sources allow you to retrieve information from external sources to inform your infrastructure.3. Remote State FileYou can store your state file remotely. This practice enhances collaboration, security, and version control of your IaC code. Consider using your infrastructure's remote state storage service, such as Terraform Cloud or AWS S3.4. Security FirstKeep security in mind throughout your IaC development. Ensure your infrastructure is configured with appropriate security measures and adhere to best practices for secure and compliant deployments.If you are a visual learner, feel free to check out the video solutionRepository containing the code for AWS Terraform \ud83d\udc49Click here\ud83d\udc48Architecture Diagram for AWS by Mahesh UpretiBlog containing step-by-step instructions for AWSBlog by Mahesh UpretiArchitecture Diagram for Azure by JoelBlog containing the step-by-step instructions for AzureBlog by Joel"}
{"title": "Exploring the \"Requester Pays\" Feature for AWS S3 Buckets. Use Cases and Cost Analysis", "published_at": 1715133330, "tags": ["aws", "cloudcomputing", "cloudcost", "s3"], "user": "n\u24d0t\u24d0Li\u24d4", "url": "https://dev.to/aws-builders/exploring-the-requester-pays-feature-for-aws-s3-buckets-use-cases-and-cost-analysis-3jk4", "details": "As infrastructure architects, we need to manage cloud resources with cost in mind, so understanding features likeRequester Paysfor AWS S3 is useful for cost optimization.TheRequester Paysfeature of AWS S3 buckets is a good option to explore in \"some cases\" because it can potentially help with optimizing the cost of AWS S3 buckets.Note: Please keep in mind that I used the term \"some cases\" because different use cases will likely require different resources and configurations.Examples of such practical cases include:Large Data Sets: Companies provide large datasets used for training machine learning (ML) models.Benefitfrom enabling Requester Pays because they can reduce the high costs associated with accessing training datasets for AI and ML.Commercial Distribution: Platforms that offer video editing directly in the cloud, where users stream large video files during the editing process (an example of such an app mostly likely already installed on your phone :) )Benefit: Users will pay for the data they consume, and the platform will provide high-performance editing tools, saving on costs with usage for scalable service offerings.Cross-Account Access: This is when users from other AWS accounts frequently access S3 objects.Benefit: Users will pay the cross-account data transfer costsHow to enable the Requester Pays featureUsing Terraform, we can create theaws_s3_bucket_request_payment_configurationresource for an AWS S3 bucket. For the payer attribute (described later in this blog), we can selectBucketOwner or Requester.BucketOwneris the default setting for all new S3 buckets if no specific request payment configuration is applied.Simple terraform code snippet ( of course, notproduction-friendly):provider\"aws\"{region=\"us-east-1\"}variable\"request_payer\"{description=<<-EOD(Optional) Specifies who should bear the cost of Amazon S3 data transfer.      It can be either BucketOwner or Requester. By default, the owner of the S3      bucket would incur the costs of any data transfer. See Requester Pays      Buckets developer guide for more information.EODtype=stringdefault=\"Requester\"}resource\"aws_s3_bucket\"\"this\"{bucket=\"in-n-out-editme\"force_destroy=falsetags={foo=\"bar\"}}resource\"aws_s3_bucket_request_payment_configuration\"\"this\"{bucket=aws_s3_bucket.this.idpayer=var.request_payer}output\"bucket_name\"{value=aws_s3_bucket.this.bucket}output\"requester_pays_status\"{value=aws_s3_bucket_request_payment_configuration.this.payer}Enter fullscreen modeExit fullscreen modeNote:aws_s3_bucket_request_payment_configurationresource cannot be used with S3 directory buckets.View in the AWS Console UIThe result of the above code (Enabledfeature) can be found in the AWS console under the S3 Bucket overview and navigating to theRequester Pays UIview (refer to the screenshot below)Understanding the Cost breakdownCharges are handled differently in AWS S3 when using the Requester Pays feature compared to the standard S3 pricing model.Normally, thebucket ownerpays for all data transfer and request charges associated with the bucket. However, in the Requester Pays model, these costs are shifted to the person or service accessing the data (requester).Bucket Owner Costs:Storage: the data stored in the S3 bucket regardless of access patterns.Requester Costs:Data Transfer Out: When data is transferred from the S3 bucket to the internet or another AWS region, the requester pays for the data transfer costs.S3 Requests: for initiated operations, such as PUT, GET, POST, and LIST.Including when requestauthenticationfails and therequest is anonymous, both resulting in an HTTP 403 error.How are Requesters billed?When accessing theRequester PaysS3 buckets,requestersmust include billing details:x-amz-request-payer: requesterin the request header. This indicates that they agree to pay for the data transfer and request costs. AWS utilizes this header to ensure that costs are billed to therequester.This setup prevents unauthorizedrequestersfrom being charged without their consent.Requestersalso need IAM permissions to ensure that only authorized users can access the data.How to View Payment Split in AWS ConsoleHere are the steps to check how the payment is split forRequester PaysS3 buckets:*Billing and Cost Management Dashboard*Navigate to the Billing and Cost Management Dashboard.Use theCost Explorerto get detailed information about S3.Go toCost Explorer: filter your data by service ( which in this case would be Amazon S3)Detailed Billing Reports:EnableDetailed Billing Reportswith Resources and Tags to see detailed information about your S3 usage. If you haveRequester Paysenabled, these reports will include information on requester-initiated transactions.Reports can be set to show which bucket incurred which costs so you can see the split between what you pay as thebucket owner(for storage and owner-initiated data transfers) and whatrequesterspay (for their data transfers).Tagging:Add cost allocation tags on your S3 buckets. Once tagged, activate these tags in theCost Allocation Tagsection of the Billing Dashboard to include them in your reports.Visualizing with AWS QuickSight Integration:It is possible to downloadDetailed Billing Reportsreports from the S3 bucket specified and analyze them using tools like Amazon QuickSight.SummaryWhen implementing theRequester Paysmodel on an S3 bucket,it's important to ensure that potential data users are aware that they will incur charges for their data access.It is important to Monitor Access to your S3 buckets at all times. Use S3 access logs or AWS CloudTrail tomonitor who is accessing your S3 bucketsto manage data transfer and associated costs (maybe one day I will write a blog about this setup)Understanding theRequester Paysmodel allows all of us, AWS users, to:deploy cost-effective data-sharing solutions in AWSmanage costs associated with data stored in AWS S3help to build cost-effective services aligned with organizational or project goals.Official documentation link:https://docs.aws.amazon.com/AmazonS3/latest/userguide/RequesterPaysBuckets.html?icmpid=docs_amazons3_console"}
{"title": "How to Cloud: Virtual Machines", "published_at": 1715130507, "tags": ["beginners", "devops", "aws"], "user": "Joseph", "url": "https://dev.to/aws-builders/how-to-cloud-virtual-machines-4h0", "details": "Ok at this point we are now ready to get into deploying your app into some kind of compute solution in your cloud provider. In order to make the best decisions when it comes to choosing said compute solution, we need to start with the foundations of almost all of them: virtual machines.Cornerstone of ComputeVirtual machines will make up the cornerstone of all compute solutions you will build on top of, whether you have control over them or not. In fact, that\u2019s one of the decision criteria you should consider when you get to choosing one, but that\u2019s for another post. For now, what\u2019s important is understanding the ins and outs of how your cloud provider allocates and operates their VMs, and how those aspects of your cloud provider effect the higher level compute solutions they all offer.Bare MetalVirtual Machines (VMs) are probably the closest to the metal kind of solution that you can leverage with a cloud provider. AWS calls this offering AWS EC2, Azure just calls them Azure Virtual Machines, and GCP calls them Compute Engine. They all end up doing the same thing in the end. You request for a certain configuration of machine, and after some time (usually a few minutes), you can have a machine spun up that closely mimics an on prem hypervisor. You can almost, for all intents and purposes, do the same thing to this machine that you would do with one mounted in a data center. That means you have the highest degree of control on this machine that you can possibly have in your cloud provider. You can mount disks to them for storage, you can configure them with GPU, SSD state storage, basically set up any kind of compute or data storage solution you want.This also means, however, that you have the least amount of operational efficiency by leveraging a virtual machine. You have to keep the OS up to date from a security perspective. You have to maintain the overall health and state of the machine. You have to connect that virtual machine to all the other offerings of your cloud provider to create an overall solution. There is a lot of extra work you have to put in to make that virtual machine do what you want it to do. The cloud provider is really only giving you one thing you didn\u2019t have before, and that is rapid availability.Cost ModelThis is where things might get a little tricky. On paper, VMs look to be the cheapest of all compute solutions. However, if you are using VMs as the only element in your compute solution, you are going to have significant labor costs. In fact, if you don\u2019t add other aspects to your compute solution, like serverless functions, or some type of orchestration plane, you will end up operating the most expensive solution you could have built once you consider the total all in cost. VMs are still crucial to understand, however, because they feed into all the other compute solutions you can use. This will ultimately boil down to trades off between labor costs or operational efficiency, and operational flexibility or control. We can get into those specifics more in other posts when we talk about the specific solutions that are offered and how they effect those trade offs.To keep things simple for now, let\u2019s just do a basic run down of the cost structure for VMs by themselves. It will serve as a fraction of the overall cost to build your solution, and in a lot of ways it makes up the foundation of your cost model.ExamplesAWSAmazon offers a variety of ways to rent their VMs. The most straight forward (and expensive) one is just using their On Demand allocation. Prices vary depending on your configuration of CPU, Memory, GPU support, storage, etc, and I won\u2019t be going into the details of how they effect cost, but it\u2019s important to note that On Demand pricing effectively influences all other pricing.There is also Reserved pricing, which allows you to actually reserve a certain level of allocation in 1 or 3 year agreements. That may sound confusing, but it\u2019s important to note in this pricing model that you aren\u2019t renting a specific VM for a specified time. It\u2019s more like you are renting the time for that configuration, and that agreement nets you some cost savings. Having said that, there are some limitations. There are two ways to purchase a Reserved instance, Standard and Convertible. Standard rental allows you to modify certain aspects of the reservation like availability zone, networking type, instance size, but you can\u2019t change other things such as the instance type. You can, however, sell that rental license on the AWS marketplace. Then there is convertible. Convertible agreements allow you to exchange one instance for another. You can do this as many times as you like but the value of the overall exchange has to be equal or greater than the agreement you put in place to begin with. Convertible rentals can not be sold on the AWS marketplace. If you choose either of these styles of pricing, you can get upwards of a 75% reduction in price from AWS for the equivalent machine with on demand pricing.Finally we have Spot pricing. Spot pricing doesn\u2019t require a termed agreement like reserved pricing, but it can still afford you similar savings. It achieves this through one major stipulation: AWS may choose at any time to terminate your instance with only a two minute warning. This can sound devastating if your cloud solution relies solely on VMs, but, as we will discuss in later posts, layering an orchestration platform on top of your rented VMs can make this an extremely appealing option. With this type of set up you can achieve savings up to 90% of what you would have spent on an on demand instance.GCPYou\u2019re going to start to see a lot of repetition amongst cloud providers. What GCP offers is extremely similar to AWS, and largely the only differences are in names.GCP\u2019s main offering is largely referred as on demand or \u201cpay as you go\u201d pricing. It\u2019s akin to AWS\u2019 on demand pricing, and the structure of offering has a lot of similarities. From there, GCP offers \u201ccommitment\u201d pricing that is similar in principle to AWS\u2019 reserved pricing. In addition to this offering, GCP will also give discounts for what they call \u201csustained use\u201d discounts (or SUDs). SUDs can give you a 30% discount if you haven\u2019t received some other form of discount and your compute has been running for at least 25% of the billable month. GCP also offers Spot or Preemptible VMs, and as you\u2019ve probably guessed they function in a very similar way to what AWS Spot instances offer. With GCP spot instances you can achieve savings upwards of 90%.AzureWe\u2019re going to do the big three and then we\u2019re going to stop. I wanted to get all three laid out so you can see the pattern. This will effect your decision process later when choosing your solutions, and it\u2019s important to understand that all cloud providers should (most do) offer you the same cost savings models.Azure\u2019s VMs can be rented under a Pay as you Go model, a Savings Plan, Reserved Instances, and finally Spot models. Pay as you Go is the same thing as On Demand. Reserved is the same as before. Spot as well. Savings Plans are worth pointing out. They allow you to lock in an agreed to hourly price for a variety of services Azure offers for a period ranging from 1 to 3 years. In order to get those reduced prices, however, you have to agree to a certain level of consumption during that time period.Get ContainerizedDeveloping the right compute solution is a series of related decisions. It starts with understanding how to choose the right VM structure. We\u2019ve already talked aboutcontainerization, so the next thing we\u2019ll go over will be how to tie together containerized applications with the foundation of virtual machines we just went over."}
{"title": "Secure API Gateway with Amazon Cognito using SAM", "published_at": 1715126400, "tags": ["aws", "security", "serverless", "sam"], "user": "Andres Moreno", "url": "https://dev.to/aws-builders/secure-api-gateway-with-amazon-cognito-using-sam-57k6", "details": "BackstoryI create a lot of APIs, these are for blog posts, for playing around with new functionality or tools that I've created for myself. All of these have been created without authentication in place. Not securing APIs can create data exposures for you, but it can also pose a financial risk to your accounts if a malicious user gets your endpoints. This is why I want to secure any API I create but I want this to be with minimal setup so that it's simple to replicate many times.We first need to understand a few concepts around what we're setting up.What is Cognito?Amazon Cognito is a service provided by AWS that allows you to add authentication to your applications or services. It integrates natively with API Gateway to secure each endpoint.Cognito has multiple layers where you can apply different types of configurations, this gives us the flexibility to get things setup for different use cases.User Pool-A Cognito user pool is the backbone to everything in Cognito. This is anOpenID Connect identity providerwhich contains the user directory to authenticate and authorize users.User Pool Domain-The user pool domain is used to give the authentication url a better name for users and applications to authenticate with.Resource Server-An OAuth 2.0 API serverthat validates that an access token contains the scopes that authorize the requested endpoint in the API.User Pool Client-A user pool client is a configuration within a user pool that directly interacts with your application that will be authenticating using Cognito.Authentication FlowsThere are several authentication flows that you can use for your applications.In this post from Auth0you can get a better understanding about which flow is better for your use case.Since I am setting up very basic authentication to be able to test my API using Postman I will use theClient Credentials Flow (CCF)to allow us to authenticate our requests by getting anaccess tokenby  sending aclient idand aclient secretto Cognito. This token is aJSON Web Token (JWT). The CCF is recommended when working with Machine-to-Machine (M2M) communication like CLIs, APIs, etc.Setting it all up with SAMI don't want to have a Cognito User Pool per API created, to simplify this I will have a single Auth Stack that will contain theUser Pooland theUser Pool Domainresources. We will then share the data from this stack to our API stacks to be able to create theResource ServerandUser Pool Clientsseparately. Below is a picture that shows this structure.Now let's see how each of these pieces are set up using SAM.Auth StackIn this stack we are going to define the resources that will be consumed by other APIs to authenticate.You can find my full setup for this stackin this GitHub repository1. User PoolThe User Pool doesn't require a lot of configuration when doing CCF. You are able to add more restrictions and configuration but as mentioned before, we are trying to keep it simple for now.CognitoUserPool:Type:AWS::Cognito::UserPoolEnter fullscreen modeExit fullscreen mode2. User Pool DomainIn my setup I'm usingandmoredevas my domain which makes our authentication base url look likehttps://andmoredev.auth.us-east-1.amazoncognito.com. You can setup up your own custom domain by setting up theCustomDomainConfig.CognitoUserPoolDomain:Type:AWS::Cognito::UserPoolDomainProperties:UserPoolId:!RefCognitoUserPoolDomain:andmoredevEnter fullscreen modeExit fullscreen modeWe need to make sure other stacks can get access to the User Pool Id and ARN to be able to create the resource server and user pool client. To do this we are going to use SSM Parameters.CognitoUserPoolIdParameter:Type:AWS::SSM::ParameterProperties:Name:!Sub/${AWS::StackName}/CognitoUserPoolIdType:StringValue:!RefCognitoUserPoolCognitoUserPoolArnParameter:Type:AWS::SSM::ParameterProperties:Name:!Sub/${AWS::StackName}/CognitoUserPoolArnType:StringValue:!GetAttCognitoUserPool.ArnEnter fullscreen modeExit fullscreen modeNow we can go and add authentication to our API stack using this user pool.API StackI will be adding authentication to an existing APIin this GitHub repository. To look at the specific changes I made to get authentication working, you can look at thePR where I did this.1. Consuming Auth Stack resourcesWe first need to get the user pool Id and ARN from SSM by adding them into theParameterssection of our template.Parameters:CognitoUserPoolId:Type:'AWS::SSM::Parameter::Value<String>'Default:'/andmoredev-auth/CognitoUserPoolId'CognitoUserPoolArn:Type:'AWS::SSM::Parameter::Value<String>'Default:'/andmoredev-auth/CognitoUserPoolArn'Enter fullscreen modeExit fullscreen mode2. Resource ServerWe are creating a resource server with one scope that will be used to give access to all endpoints in our API. I will not go into more details on more advanced scope design in this post.LayerlessESBuildResourceServer:Type:AWS::Cognito::UserPoolResourceServerProperties:UserPoolId:!RefCognitoUserPoolIdIdentifier:layerless-esbuildName:Layerless ESBuildScopes:-ScopeName:apiScopeDescription:Allow api accessEnter fullscreen modeExit fullscreen mode3. User Pool ClientBelow is the definition for our user pool client.CognitoTestAutomationClient:Type:AWS::Cognito::UserPoolClientDependsOn:-PostmanResourceServerProperties:UserPoolId:!RefCognitoUserPoolIdGenerateSecret:trueAllowedOAuthFlows:-client_credentialsAllowedOAuthScopes:-layerless-esbuild/apiAllowedOAuthFlowsUserPoolClient:trueEnter fullscreen modeExit fullscreen modeLet's talk about the properties we've here.UserPoolId - reference to the id of the user pool created in our Auth Stack.GenerateSecret - this is necessary for us to use with client credentials flow.AllowedOAuthFlows - we are telling Cognito that this client will only allow CCFAllowedOAuthScopes - We need to make sure this array contains scopes that have been defined in our resource server.AllowedOAuthFlowsUserPoolClient - This is what enables us to use standard OAuth functionality with our user pool client.I've seen deployment scenarios where the resource server gets deployed after the client and we get an error saying the scope does not exist, this is the reason I am explicitly adding theDependsOnfor this resource.4. Hooking it all up to API GatewayTo tell our API Gateway to authenticate using our new cognito user pool we need to add theAuthproperty, it will look something like this.Auth:DefaultAuthorizer:ClientCognitoAuthorizerAuthorizers:ClientCognitoAuthorizer:UserPoolArn:!RefCognitoUserPoolArnAuthorizationScopes:-layerless-esbuild/echoEnter fullscreen modeExit fullscreen modeWe are consuming the user pool ARN from the Auth Stack and allowing the scope that we've created in our resource server.THAT IS ALL! Now we can deploy this stack to be able to test it.Testing it with Postman1. Grab authentication dataTo test this we need to go into the AWS console and grab the Client Id and Client Secret that were generated. These are located in the Cognito service by selecting the new user pool and going in theApp integrationsection. In the bottom you will see your new app client, once you open it you will see something like the image below. You can copy the Client ID and Client secret from here, we will use these values in the next steps.Please handle these values with care, if compromised someone could gain access to your APIs and do malicious things.2. Test un-authenticated requestTo verify our API is secure we will first run an unauthenticated request. To do this we will call our endpoint without setting anything for authentication, when we send this request we should get a 401 - Unauthorized response as shown below.3. Set authentication data in PostmanWith the values we will now use a newPostman feature calledVaultsthat allows us to securely store sensitive data. To do this we will go into the Vault section in the bottom of the window and add our secrets.4. Setup request authenticationNow back in the Postman request we can set the Authorization configuration. There are several parameters to set here.Grant type - This will have a value ofClient Credentialsspecifying that we will be using the client credentials flow.Access Token URL - The value for your specific user pool will vary depending on your configured user pool domain. It will look something like thishttps://[your-domain].auth.us-east-1.amazoncognito.com/oauth2/tokenClient ID - We will grab this from the vault by setting a value of{{vault:clientId}}Client Secret - Also from the vault with a value of{{value:clientSecert}}Scope - This will be based on what was set on the resource server. From our example it islayerless-esbuild/echo.5. Get the JWTWe can now get a new access token by going to the bottom of the Authorization section and pressing theGet New Access Tokenbutton. If successful you will receive a prompt where you can click a button that saysUse Token. By pressing that button you will get the value presented in the Access Token and it will be used in theAuthorizationheader of your request.If we send the request now we will get a successful response.Wrap UpSadly security is often left as an afterthought, just like it happened to me with all the APIs I've created. In this post we were able to understand a few of the concepts relating to authentication and the resources necessary to set this up in AWS with Amazon Cognito. We also tested that our API is now secure and how we can get a token to authenticate against it.I hope this allows people to add a basic layer of security to their APIs so that we make malicious users work a little bit more."}
{"title": "Terragrunt for Multi-Region/Multi-Account Deployments", "published_at": 1715121504, "tags": ["devops", "terraform", "terragrunt", "aws"], "user": "Ervin Szilagyi", "url": "https://dev.to/aws-builders/terragrunt-for-multi-regionmulti-account-deployments-1o1", "details": "Since a few years ago I've been working for a company whose products\u00a0are used\u00a0by millions. It feels somewhat refreshing to know that my contributions\u00a0have an impact on\u00a0the daily lives of so many people. On the other hand, this work also comes with a lot of anxiety in cases when you have to make decisions, even though these decisions\u00a0are usually made together\u00a0with a team of other highly experienced individuals.Such a decision was the introduction of Terragrunt in our workflow. Why did we need Terragrunt, you may ask?\u00a0This\u00a0is what\u00a0I will try to answer in the following lines of this article.As a disclaimer, this article is subjective, based on my own experience in finding a solution to\u00a0the problems we had. Usually, there is more than one way to tackle a challenge\u00a0and\u00a0in many cases, there are no perfect solutions. Knowing this, I think it is\u00a0important\u00a0to address the weaknesses and limitations of your solution, which this article will do later.What is Terragrunt?Before going into the \"whys\",\u00a0I think\u00a0it is\u00a0important\u00a0to know what\u00a0is Terragrunt.\u00a0Now\u00a0I\u00a0won't give you their marketing points or any definition copied from sites like Wikipedia, I\u00a0will try to explain\u00a0what is\u00a0the purpose of this tool from my point of view.Essentially,\u00a0Terragrunt is a wrapper around Terraform, acting as an orchestrator over multiple Terraform modules.\u00a0As developers, we\u00a0have to\u00a0organize our Terraform code inmodules. Under\u00a0to hood, each module gets embedded inside a Terraform project\u00a0and\u00a0it\u00a0is\u00a0deployed\u00a0individually.\u00a0Modules communicate between themselves with outputs anddependencyblocks, while the dependency tree and rollout order are determined by Terragrunt.Why Terragrunt?To understand why Terragrunt\u00a0was chosen, it would make sense to go through a timeline of challenges we encountered.Let's assume we have an application spanning through a few micro-services with an SQL database, some static assets, and a few ETL jobs bringing in some data from external providers. We decide we want to migrate everything to the AWS. Our users are from\u00a0all over\u00a0the globe, but our main focus is Europe and the US. We have to offer different data to the EU users and the US users, moreover, we would like to reduce the latency of the responses. So\u00a0it\u00a0makes sense to deploy the whole stack on\u00a02\u00a0different regions. We also want\u00a0to have\u00a0the application deployed separately for development and testing, for which we can use\u00a0different\u00a0AWS accounts.For\u00a0IaC\u00a0we decided to use Terraform because we have the most experience with that compared to other options.\u00a0Having these in mind, the following events happened afterward:We started writing our Terraform code. We put everything in a single Terraform project. We relied ontfvarsfiles to have inputs for different environments and regions.We shortly ran into a scaling problem: attempting to do anapplywent from a few minutes to a few tens of minutes.\u00a0Moreover, we run into\u00a0so\u00a0communication and deployment issues\u00a0in terms of\u00a0certain\u00a0changes being deployed to production before we wanted them.Our Terraform project got bigger and bigger. We decide to slice it somehow into smaller pieces. We introduced the internal concept of \"stacks\" (this was well before the introduction of Terraform Cloud Stacks). From our point of view, a \"stack\" essentially was a Terraform project deploying a well-defined part of our infrastructure. Each stack could use resources deployed by other stacks by relying on Terraform outputs andterraform_remote_stateor just by simply using data sources.With the introduction of\u00a0stacks\u00a0we had different projects for networking, databases, ETL (we used mainly AWS Batch), storage (S3 buckets), and so on.\u00a0This\u00a0worked for a while until we ran into another problem. At first, it was easy to follow which stack depends on which other stack, but shortly\u00a0we\u00a0ran into the issue of circular dependencies. Stack A could create resources used by stack\u00a0B,\u00a0while\u00a0also relying on resources\u00a0created\u00a0by stack B.\u00a0Obviously,\u00a0this is bad, and at this point, there is no entity to check and police our dependencies.Moreover, we run into another problem.\u00a0Certain\u00a0resources are needed only for\u00a0certain\u00a0environments. For example, we needed a read replica only for prod, for\u00a0testing and development we could get by with only the\u00a0main\u00a0database.\u00a0In the beginning, we could solve this by having conditions on whether we want to deploy the resource in the current environment\u00a0or not.\u00a0At a certain point, we notice that we have to put these conditions in many places, adding a lot of complexity baggage to our infrastructure code.So we decided to introduce Terragrunt.To answer the initial question, we chose Terragrunt because:It solved the dependency hell we encountered.\u00a0With\u00a0Terragrunt\u00a0we have to be explicit in defining\u00a0on who does\u00a0our current module depends.It fits the multi-region/multi-account approach.\u00a0In case\u00a0we\u00a0dour\u00a0our modules wisely, we use only the necessary modules for each region/environment.\u00a0The catch\u00a0here\u00a0is that we have to modularize our code\u00a0and\u00a0we\u00a0do it adequately, which might\u00a0be not\u00a0as easy as we would expect.By introducing versioning for our modules, we could evolve different environments at their own pace.Now, all of these come with a price: refactoring. Terragrunt relies on Terraform modules. Our initial code was not as modular as we might\u00a0expected. So we had to do a lot of refactoring, which also came with an even\u00a0bigger\u00a0challenge: state management and transferring resources between states.How does Terragrunt Work?To use Terragrunt,\u00a0first, we have to\u00a0be comfortable withTerraform modules. The concept of modularization is simple:Terraform provides building blocks such as resources and data sources;Some of these resources are often used together (for\u00a0example:\u00a0a database and Route 53 record for its hostname)It would make sense to group these resources in a reusable container. These reusable containers are called modules.Modules can communicate between themselves with inputs and outputs. Terragrunt requires that all of our Terraform resources be part of modules.Setting Up a Terragrunt ProjectIn the official Terragrunt\u00a0documentation\u00a0there isa good articleabout how to set up a Terragrunt project and where to place modules.\u00a0In fact,\u00a0there is also arepositoryon GitHub providing an example project on how the creators recommend setting up Terragrunt. I certainly recommend going through that\u00a0repository,\u00a0because it is a good reference for a starting point. Having that said, I like to structure mine a little bit differently. My recommendation is to have different AWS accounts for each environment. Getting a new account\u00a0usually\u00a0is\u00a0relatively easy to accomplish\u00a0even\u00a0if we are working in a corporate environment (your workplace most likely is using AWS Organizations to manage accounts). The existence of multiple accounts does not require additional costs, we\u00a0only pay for what we use.In theterragrunt-infrastructure-live-examplethe split for the environments is done\u00a0byprodandnon-prodaccounts.\u00a0Each of these is further split by region. Thenon-prodaccount is also used forqaandstageenvironments. This setup can be perfectly acceptable, the\u00a0one\u00a0downside being that we will have to think about a naming convention for our\u00a0resources,\u00a0since innon-prodwe\u00a0will have the same cloud resources for bothqaandstage. While this is not that big of a deal, I prefer to have one environment per account. My proposal for a Terragrunt project setup would look like this (GitHub repository for this example project can\u00a0be found\u00a0here:https://github.com/Ernyoke/tg-multi-account):tg-multi-account \u2502   .gitignore \u2502   global.hcl \u2502   terragrunt.hcl \u2502 \u251c\u2500\u2500\u2500dev \u2502   \u2502   account.hcl \u2502   \u2502 \u2502   \u2514\u2500\u2500\u2500us-east-1 \u2502       \u2502   region.hcl \u2502       \u2502 \u2502       \u251c\u2500\u2500\u2500alb \u2502       \u2502       terragrunt.hcl \u2502       \u2502 \u2502       \u251c\u2500\u2500\u2500ecs-cluster \u2502       \u2502\u00a0 \u00a0 \u00a0 \u00a0terragrunt.hcl \u2502       \u2502 \u2502       \u251c\u2500\u2500\u2500ecs-services \u2502       \u2502   \u2514\u2500\u2500\u2500frontend \u2502       \u2502\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0terragrunt.hcl \u2502       \u2502 \u2502       \u2514\u2500\u2500\u2500vpc \u2502\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0terragrunt.hcl \u2502 \u251c\u2500\u2500\u2500prod \u2502   \u2502   account.hcl \u2502   \u2502 \u2502   \u251c\u2500\u2500\u2500eu-west-1 \u2502   \u2502   \u2502   region.hcl \u2502   \u2502   \u2502 \u2502   \u2502   \u251c\u2500\u2500\u2500alb \u2502   \u2502   \u2502       terragrunt.hcl \u2502   \u2502   \u2502 \u2502   \u2502   \u251c\u2500\u2500\u2500ecs-cluster \u2502   \u2502   \u2502\u00a0 \u00a0 \u00a0 \u00a0terragrunt.hcl \u2502   \u2502   \u2502 \u2502   \u2502   \u251c\u2500\u2500\u2500ecs-services \u2502   \u2502   \u2502   \u2514\u2500\u2500\u2500frontend \u2502   \u2502   \u2502\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0terragrunt.hcl \u2502   \u2502   \u2502 \u2502   \u2502   \u2514\u2500\u2500\u2500vpc \u2502   \u2502\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0terragrunt.hcl \u2502   \u2502 \u2502   \u2514\u2500\u2500\u2500us-east-1 \u2502       \u2502   region.hcl \u2502       \u2502 \u2502       \u251c\u2500\u2500\u2500alb \u2502       \u2502       terragrunt.hcl \u2502       \u2502 \u2502       \u251c\u2500\u2500\u2500ecs-cluster \u2502       \u2502\u00a0 \u00a0 \u00a0 \u00a0terragrunt.hcl \u2502       \u2502 \u2502       \u251c\u2500\u2500\u2500ecs-services \u2502       \u2502   \u2514\u2500\u2500\u2500frontend \u2502       \u2502\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0terragrunt.hcl \u2502       \u2502 \u2502       \u2514\u2500\u2500\u2500vpc \u2502\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0terragrunt.hcl \u2502 \u251c\u2500\u2500\u2500qa \u2502   \u2502   account.hcl \u2502   \u2502 \u2502   \u251c\u2500\u2500\u2500eu-west-1 \u2502   \u2502   \u2502   region.hcl \u2502   \u2502   \u2502 \u2502   \u2502   \u251c\u2500\u2500\u2500alb \u2502   \u2502   \u2502       terragrunt.hcl \u2502   \u2502   \u2502 \u2502   \u2502   \u251c\u2500\u2500\u2500ecs-cluster \u2502   \u2502   \u2502\u00a0 \u00a0 \u00a0 \u00a0terragrunt.hcl \u2502   \u2502   \u2502 \u2502   \u2502   \u251c\u2500\u2500\u2500ecs-services \u2502   \u2502   \u2502   \u2514\u2500\u2500\u2500frontend \u2502   \u2502   \u2502\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0terragrunt.hcl \u2502   \u2502   \u2502 \u2502   \u2502   \u2514\u2500\u2500\u2500vpc \u2502   \u2502\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0terragrunt.hcl \u2502   \u2502 \u2502   \u2514\u2500\u2500\u2500us-east-1 \u2502       \u2502   region.hcl \u2502       \u2502 \u2502       \u251c\u2500\u2500\u2500alb \u2502       \u2502       terragrunt.hcl \u2502       \u2502 \u2502       \u251c\u2500\u2500\u2500ecs-cluster \u2502       \u2502\u00a0 \u00a0 \u00a0 \u00a0terragrunt.hcl \u2502       \u2502 \u2502       \u251c\u2500\u2500\u2500ecs-services \u2502       \u2502   \u2514\u2500\u2500\u2500frontend \u2502       \u2502\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0terragrunt.hcl \u2502       \u2502 \u2502       \u2514\u2500\u2500\u2500vpc \u2502\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0terragrunt.hcl \u2502 \u2514\u2500\u2500\u2500_env \u00a0 \u00a0 \u00a0 \u00a0\u00a0frontend.hcl \u00a0 \u00a0 \u00a0 \u00a0\u00a0vpc.hclEnter fullscreen modeExit fullscreen modeHere\u00a0we\u00a0have\u00a03\u00a0environments:dev,qa, andprod. Each environment should be living in a single AWS account. The\u00a0root of the project\u00a0contains configuration (locals) shared by every\u00a0environment. If we go inside a directory acting as an environment/account, we have the account-specific properties (account.hcl).\u00a0We also create directories here for each region\u00a0in which\u00a0we would want to\u00a0provision\u00a0things.\u00a0Navigating one step\u00a0deeper\u00a0we find the region-specific configuration (region.hcl) and all the modules we would like to have in that region.Now\u00a0let's\u00a0focus on the Terragrunt modules. If we open a configuration, for\u00a0example\u00a0for the VPC, a possible implementation would be the following:terraform{source=\"tfr:///terraform-aws-modules/vpc/aws//.?version=5.8.1\"}include\"root\"{path=find_in_parent_folders()}locals{global_vars=read_terragrunt_config(find_in_parent_folders(\"global.hcl\"))project_name=local.global_vars.locals.project_name}inputs={name=\"${local.project_name}-vpc\"cidr=\"10.0.0.0/16\"azs=[\"us-east-1a\",\"us-east-1b\",\"us-east-1c\"]private_subnets=[\"10.0.1.0/24\",\"10.0.2.0/24\",\"10.0.3.0/24\"]public_subnets=[\"10.0.101.0/24\",\"10.0.102.0/24\",\"10.0.103.0/24\"]enable_nat_gateway=trueenable_vpn_gateway=false}Enter fullscreen modeExit fullscreen modeTo give a short explanation of what we have here:In theterraformblock\u00a0we have to\u00a0specify a path to a Terraform module.\u00a0For example, in this case, we use the VPC module from theterraform-aws-modulesopen source project for which we either could use the URL of the repository, or we could use the URL provided by the Terragrunt registry. We don't necessarily need to rely on other people's code, we can use modules maintained by ourselves by providing a link to our remote Git repository, or we can even have it point to a local path on our drive.Theincludeblock is optional. It\u00a0is used\u00a0for \"inheritance\". By inheritance, we can think of it as if the parent configuration file is\u00a0copy/pasted in the current configuration file.\u00a0This\u00a0can be useful because we can share\u00a0common\u00a0inputs with other environments/regions. Including them in the current configuration, Terragrunt will automatically provide them to the Terraform module. Also, we\u00a0have the ability to\u00a0append/override\u00a0certain\u00a0inputs as we wish.Thelocalsblock essentially acts the same as Terraformlocals.\u00a0These are local \"variables\" used in the current configuration. We can also read locals from other configuration files with Terragrunt functions (read_terragrunt_config).Theinputsare values we provide to the Terraform module. If we use inheritance, the includes provided by the parent configuration are automatically merged with current includes, making our configurationDRY, arguably less readable, more\u00a0on this later.Taking the \"DRY\" -ness a step further, we\u00a0can\u00a0notice that modules such asvpcare used in each environment/region with little configurational difference.\u00a0What we can do is\u00a0extract this configuration into a top-level folder, such as_env,\u00a0and rely on the inheritance feature discussed before.The extracted file will look like this:# ./_env/vpc.hclterraform{source=\"tfr:///terraform-aws-modules/vpc/aws//.?version=5.8.1\"}locals{global_vars=read_terragrunt_config(find_in_parent_folders(\"global.hcl\"))project_name=local.global_vars.locals.project_name}inputs={name=\"${local.project_name}-vpc\"cidr=\"10.0.0.0/16\"azs=[\"us-east-1a\",\"us-east-1b\",\"us-east-1c\"]private_subnets=[\"10.0.1.0/24\",\"10.0.2.0/24\",\"10.0.3.0/24\"]public_subnets=[\"10.0.101.0/24\",\"10.0.102.0/24\",\"10.0.103.0/24\"]enable_nat_gateway=trueenable_vpn_gateway=false}Enter fullscreen modeExit fullscreen modeIn case our region is not from us-east-1, we can override the availability zones with sensible ones:# ./qa/eu-west-1/vpc/terragrunt.hclinclude\"root\"{path=find_in_parent_folders()}include\"env\"{path=\"${get_terragrunt_dir()}/../../_env/vpc.hcl\"}inputs={azs=[\"eu-west-1a\",\"eu-west-1b\",\"eu-west-1c\"]}Enter fullscreen modeExit fullscreen modeMore or\u00a0less\u00a0this is what we need to know to be able to write Terragrunt code. Now\u00a0let's\u00a0discuss deployments.DeploymentsThe deployment of a Terragrunt project can\u00a0be accomplished\u00a0with the following command:terragrunt\u00a0run-all applyEnter fullscreen modeExit fullscreen modeIf we run this command from\u00a0the root\u00a0our our\u00a0project, Terragrunt will attempt to deploy all the resources in all the accounts.\u00a0This\u00a0assumes we have the necessary rights to deploy to each account\u00a0and\u00a0we also made sure that Terragrunt knows about the IAM role it can\u00a0assume\u00a0to do the provisioning (see:iam_role).LATER UPDATE: It seems like it is not possible to executerun-all applyfrom the root of the project in this current example. Terragrunt will fail to find eitherglobals.hcloraccount.hcl. It appears strange to be unable to findglobals.hclsince this file is in the same directory together with theterragrunt.hclconfiguration file from where it is referenced. It might be understandable to not be able to locateaccount.hcl. This file is not in the current root folder, it is in a child folder relative to the root, sofind_in_parent_folderswill fail to locate it. This was an oversight on my part while building the example project for this article and I want to apologize for that. Deploying from an environment should work as it is presented in the upcoming lines.In case\u00a0we don't want to deploy everything everywhere at once, we can\u00a0simply\u00a0navigate into the folder of the environment/region where we want to provision resources, and we can execute the command there.In case\u00a0we want to deploy\u00a0certain\u00a0modules only in a\u00a0certain\u00a0region, we can navigate into the folder for that module and execute the command there. An important thing to note here, if we runterragrunt\u00a0run-all applyfor a module,\u00a0all the dependencies of that module will also be deployed.\u00a0This\u00a0might be time-consuming in cases where we need to execute it frequently (in case we do development\u00a0for\u00a0example). As a workaround, we can runterragrunt\u00a0applyinstead (without therun-allcommand).\u00a0This\u00a0will omit to roll out all the dependencies. It will rely on previous outputs cached\u00a0with previous\u00a0deployments.\u00a0If\u00a0there was no previous deployment for a dependency, the command will fail.Terragrunt commands are similar to what we've been accustomed to while using Terraform. We can see the plan by executing theplancommand (with certain limitations, more about this below), we can import resources with theimportcommand,\u00a0we\u00a0canforce-unlockthe state of a module in case it got stuck with an unsuccessful apply, and so on.Downsides and LimitationsLike every other tool, Terragrunt has its limitations, especially if we are coming from a Terraform setup.\u00a0While I consider Terragrunt\u00a0to be\u00a0a valuable and\u00a0useful\u00a0tool, I think\u00a0is\u00a0very important to know its limitations in case we consider adopting it.The following is a list of challenges\u00a0that I've\u00a0encountered during its adoption and day-to-day usage. I imagine there are plenty of others faced by other people, this\u00a0is not an exhaustive list.1.Steep learning curve and complexity of usage: if we are new to Terragrunt, we may get easily overwhelmed by all the new concepts we\u00a0are introduced\u00a0to. As we get more familiar with it, we\u00a0are faced\u00a0with\u00a0other challenges, such as configuration inheritance.\u00a0Having to adhere\u00a0to practices that make our configuration\u00a0DRY,\u00a0can also make it more challenging to understand and follow.2.Thereplancommand is broken (at least in\u00a0certain\u00a0scenarios): this is stated even in the documentation for therun-allcommand[WARNING] Using run-all with\u00a0plan\u00a0is currently broken for\u00a0certain\u00a0use cases. If you have a stack of Terragrunt modules with dependencies between them\u2014either via dependency blocks orterraform_remote_statedata sources\u2014and you\u2019ve never deployed them, thenrun-all\u00a0planwill fail as it will not be possible to resolve the dependency blocks orterraform_remote_statedata sources!This\u00a0might seem a non-issue at first, but if we consider also\u00a0to following\u00a0note for theapplycommand...[NOTE] Usingrun-allwith apply or destroy silently adds the-auto-approveflag to the command line arguments passed to Terraform due to issues with shared stdin\u00a0making\u00a0individual approvals impossible....we can probably guess why it might be dangerous\u00a0to\u00a0simply\u00a0do deployments.\u00a0Although,\u00a0it might not be such drastic of a situation. My recommendation is that if we have doubts, we should restrict roll-outs to individual modules.\u00a0In the case of\u00a0modules, we can executeplanorapplywithout the silent auto-approve flag.Also, Terragrunt\u00a0is meant\u00a0to\u00a0be used\u00a0with multiple environments in mind.\u00a0Having a\u00a0successful rollout in a non-prod environment should make us confident and prepared for the production rollout.3.There is no easy way to import Terraform resources (at least I'm not aware of any): in case we have a Terraform project\u00a0and\u00a0we decide to transform it to Terragrunt, we most likely will have\u00a0to manually import all the resources into the new state.\u00a0This\u00a0might be a non-issue if we could destroy our Terraform stack and re-provision everything with Terragrunt, but this might not be possible in the case of a production environment where availability is\u00a0important.4.Deployment Speed: Terragrunt is running Terraform under the hood. It will invoke Terraform independently for each module, an\u00a0action that takes time. A mitigation for this is to keep deployments scoped to and apply\u00a0only\u00a0what we need.\u00a0In cases where we need a plan for the whole project (for a security assessment\u00a0for\u00a0example),\u00a0most\u00a0likely\u00a0we will still\u00a0have to wait a lot to get that.Alternatives to TerragruntAt the beginning of this article, we have seen why Terragrunt made sense for us\u00a0for\u00a0a business-critical solution. In the previous section, we\u00a0were also faced\u00a0with the limitations of this tool.\u00a0Having in mind\u00a0all of these, we could want to keep an eye on what other alternatives exist.Here are a few examples that\u00a0can be considered\u00a0instead of Terragrunt.Terramate: It seems like a good alternative, and I think it could have been a better choice for\u00a0certain\u00a0issues we had. With\u00a0Terramate\u00a0the transition from Terraform might have been easier since\u00a0Terraform projects can be imported\u00a0seamlessly.\u00a0Furthermore, we don't have to think\u00a0right away\u00a0about how to modularize everything.\u00a0The reason\u00a0it was not\u00a0chosen,\u00a0is that my team was mainly familiar with Terragrunt. We had no experience with Terramate, so\u00a0we decided to play it safely.Terraform Stacks: at the point of writing this post, it is still not generally available. It was not even considered by us back\u00a0then,\u00a0since it was in private preview and nobody had access to it. It might be a good choice in the future, but\u00a0for now, it is not something we can use.Terraform Workspaces: they are a similar approach to having different\u00a0tfvars\u00a0files per environment/region, a solution we were extensively using. We found that it is not the best choice since it scales poorly if the infrastructure gets bigger and bigger. However, If you start a project, I still recommend sticking to workspaces at the beginning and moving to something\u00a0afterward,\u00a0when\u00a0it is\u00a0needed.Insert any other tool here:\u00a0understandably\u00a0there are many\u00a0other\u00a0options\u00a0out there.\u00a0\u00a0When\u00a0making a decision\u00a0for something that will have to be maintained by multiple people for a living,\u00a0usually we\u00a0go with the one tool that has\u00a0to most\u00a0support on the internet, it\u00a0is known\u00a0by most of the people from the team and generally has a good reputation.Final ThoughtsIn conclusion, Terragrunt is a powerful tool with many functionalities. It is an opinionated way of working with infrastructure. It might not be the best choice for everyone.Should I use it for my next project?It depends. If you did not encounter some of the issues that\u00a0are aimed\u00a0to be solved by it, then you probably may not want to use it. It will add considerable maintenance baggage. To quote the Terragrunt author here[source]:If\u00a0you're working on a small project (e.g., a solo project or hobby), none of this matters, and you probably don't need Terragrunt.\u00a0But if you're working at a company that is using Terraform to manage infrastructure for multiple teams and multiple environments, the items above make it hard to create code that is maintainable and understandable, and that's where Terragrunt can be a great option.References:Terraform ModulesTerragrunt Dependency BlocsThe terraform_remote_state Data SourceKeep your Terraform code DRYiam_rolerun-all"}
{"title": "How to secure API Gateway using JWT and Lambda Authorizers with Clerk", "published_at": 1715112909, "tags": ["clerk", "aws", "serverless", "security"], "user": "Brian Morrison II", "url": "https://dev.to/aws-builders/how-to-secure-api-gateway-using-jwt-and-lambda-authorizers-with-clerk-3g7l", "details": "One of the common ways to access AWS services over HTTP is through API Gateway.API Gateway acts as a centralized entry point for many of the services offered through AWS. For example, you can configure serverless Lambda functions that are capable of accepting HTTP events from API Gateway for processing, enabling you to build a completely serverless backend for your application or service. Allowing any traffic into an AWS account should be done securely, otherwise, you run the risk of services being taken advantage and causing issues such as data exfiltration or a surprise AWS bill. Luckily, API Gateway offers a feature calledauthorizersthat can be used to secure your endpoints before traffic ever reaches the service on the other side.In this article, you'll learn what API Gateway authorizers are, how they work, andhow to use them with Clerk.What are API Gateway Authorizers?API Gateway authorizers are a feature of API Gateway that allows you to lock down your API endpoints so that only authorized requests are permitted.API Gateway is compatible with a wide array of AWS services, allowing you to mix and match multiple services behind a single domain to precisely craft the service that your users need. While services such as Lambda or EC2 can have built-in logic to verify the request, something like DynamoDB does not have that capability. When an authorizer is attached to an endpoint, API Gateway will first use the authorizer to verify that the request being sent in is by an authorized party before passing it through to the service, or denying the request if it's unauthorized.To better explain how an authorizer works, I'll use the example of the serverless API in the introduction of this article.How Authorizers WorkLet's assume you have a simple serverless API that combines API Gateway and a series of Lambda functions.When a request comes into the API, the request will be proxied to one of the associated Lambda functions for processing before sending a response back to the caller. Without authorizers, the request is sent directly to the Lambda function, regardless of who sent it or where it comes from. This means that the code for each Lambda would need to individually verify that the request is valid.Now let's look at the same example with an authorizer attached to each endpoint. When a request comes into an endpoint that is protected with an authorizer, API Gateway will first send the request to the authorizer to verify and deny the request depending on if the it passes the checks defined by the authorizer.Authorizers enable you to centralize your authorization logic, protect services that would otherwise be difficult to protect, and utilize caching to reduce your AWS bill.Understanding JWT and Lambda AuthorizersThere are several types of authorizers available depending on the type of API Gateway configuration you have, but we're going to focus on the two that are compatible with Clerk: JWT and Lambda authorizersJWT AuthorizersA JWT authorizer uses anOpenID Connect Discoveryendpoint to validate tokens based on the included JSON Web Key Set (JWKS).The OpenID Connect Discovery endpoint contains information about the identity provider (also known as the IdP) that can be used when configuring services to support authenticating with the IdP. This endpoint contains information such as who is issuing the JWT (or token), what authorization scopes are supported, what information is included in the tokens, etc.One of the things that is often included in the Discovery endpoint is the URI of the JWKS, a collection of public keys that can be used to verify the signature on a token. If the signature of a token is valid, the information about the user that is included within it can be considered trustworthy. When a request comes in that is protected by a JWT authorizer, AWS will use the publically available JWKS, along with an audience (aud) value in the claims, to validate the token and pass the request on if it's valid.JWT Authorizers are a simple way of verifying requests using the JWKS but are only available on the HTTP-type of API Gateway instances.Lambda AuthorizersLambda authorizers allow you to create a custom Lambda function using the language of your choice to validate inbound requests.They offer the most flexibility but are also relatively complex when compared to JWT authorizers. When a Lambda authorizer is executed, the configured authorization header is passed along to a Lambda function in the event parameter, but it's up to you to write the code that validates the event and responds with an IAM policy. However, because you are essentially writing code, you can even parse the claims and conditionally permit the request based on more than just an audience value, something we'll explore later in this article.Lambda authorizers are compatible with both REST and HTTP API Gateway types.Using Clerk with API Gateway AuthorizersClerk's use ofJWTsmakes our service compatible with API Gateway authorizers when configured as explained in this article.When a user signs into an application with Clerk, a short-lived token is created and stored in the browser cookies. Clerk's libraries allow you to easily extract this token and use it however you need, including adding it to the authorization header of an HTTP request:'use client'import{useSession}from'@clerk/nextjs'exportdefaultfunctionHome(){const{session}=useSession()asyncfunctioncallApi(){consttoken=awaitsession?.getToken()awaitfetch(url,{headers:{Authorization:`Bearer${token}`,},})}// Code removed for brevity...}Enter fullscreen modeExit fullscreen modeNow let's take a look at how to configure both a JWT and Lambda authorizer to work with Clerk.PrerequisitesTo follow along with this portion of the article, you'll need the following configured:An AWS account, and familiarity with Lambda and API GatewayAClerk accountNode and NPM installed on your computerWhile everything discussed attempts to be covered as part of the AWS free tier, be aware that we will be creating resources that may cost real money.Using Clerk with JWT AuthorizersAs mentioned earlier, JWT Authorizers require you to know the OpenID Connect Discovery endpoint, as well as anaudvalue in the claims of the token being checked, so let's start by gathering this info.In the Clerk Dashboard, select\"API Keys\"from the navigation, then click\"Show API URLs\". This will show you URLs for the Frontend and Backend API. Take note of the value value in theFrontend API URLfield, this will be used as the OpenID Connect Discovery endpoint as Clerk automatically sets up this endpoint for each application created in your account.Next, you'll need to configure theaudclaim value since Clerk tokens do not contain this by default. The value can be any arbitrary string, it just needs to match what's specified in the authorizer configuration, which we'll do in the next step. This example uses \"ClerkJwtAuthorizer\" as the value, but you're free to use something else.To add a static value to all tokens, select\"Sessions\"from the navigation, then the\"Edit\"button underCustomize session token. In the modal that appears, modify theClaimsto include anaudvalue. If you don't have any other custom claims defined, it should look like this:{\"aud\":\"ClerkJwtAuthorizer\"}Enter fullscreen modeExit fullscreen modeIn an HTTP-type API Gateway instance in AWS, you can create an authorizer by selecting\"Authorization\"in the left navigation, then the\"Manage authorizers\", and finally the\"Create\"button.JWT Authorizer is selected by default, but you'll need to populate the following values:Name- A friendly name for the authorizer.Identity source- Where the token should be referenced in the request. This can be left with the default value of \u201c$request.header.Authorization\u201d which will use theAuthorizationheader of the inbound request.Issuer URL- This should be set to the Frontend API URL value from earlier.Audience- The value set earlier in theaudfield of the session claims (you may have to click\"Add Audience\"for this input to appear).Once you click\"Create\", you'll be returned to the previous screen. From here, select the\"Attach authorizers to routes\"tab. This will display a list of the routes configured in your API. Choose a route from the list, then use the\"Select existing authorizer dropdown\"to select the authorizer you created earlier, then\"Attach authorizer\".Repeat this process for every route you want to protect, and any request that are executed against these routes will automatically be protected using the user's Clerk session token.One caveat to using JWT authorizers is that they are incompatible with theANYmethod available in AWS if the API will be called from the browser. This is due to the fact that CORS preflight requests will not include the Authorization header, which will cause the authorizer to deny the preflight request.Using a Lambda Authorizer with ClerkWhile Lambda authorizers are compatible with HTTP-type API Gateways, they are more common in REST types, so this section of the guide will move over to a REST API. Since Lambda authorizers are limited to a short execution window, we'll be usingClerk networkless verificationto make sure the request is authorized. Essentially we'll be embedding the public key of the key set into the code to eliminate unnecessary network requests, making the code as efficient as possible.Start in the Clerk dashboard and navigate back to the\"API Keys\"section. Now select\"Show JWT public key.\"From the modal, copy the block of text underPEM Public Keyfor later use.Now let's create a Lambda function in AWS that will serve as the authorizer. As stated earlier, you can use any supported language, but I'll be using JavaScript for this demo. Start a terminal session on your workstation and run the following commands to initialize a new Node project and install thejsonwebtokenlibrary.# Initialize a new NPM projectnpm init-y# Install the `jsonwebtoken` dependencynpminstalljsonwebtokenEnter fullscreen modeExit fullscreen modeIn the root of the project, create anindex.jsfile and populate it with the following code, replacing the contents of thepublicKeyvariable with thePEM Public Keystring from earlier (it should be a multiline string in the code). Notice how we're also checking the user's metadata included in the claims to make sure they have the role of \u201cadmin\u201d, something that is not possible with JWT authorizers.importjwtfrom'jsonwebtoken'constpublicKey=`{PASTE YOUR PEM KEY HERE}`exportasyncfunctionhandler(event,context,callback){// Extract the token from the Authorization headerconsttoken=event.authorizationToken.split('')[1]// Verifies and decodes the JWTconstclaims=jwt.verify(token,publicKey)// Check if the user is an adminif(claims.metadata.role==='admin'){callback(null,generatePolicy(claims.metadata,claims.sub,'Allow',event.methodArn))}else{callback(null,generatePolicy(null,claims.sub,'Deny',event.methodArn))}}functiongeneratePolicy(metadata,principalId,effect,resource){constauthResponse={principalId:principalId,policyDocument:{Version:'2012-10-17',Statement:[{Action:'execute-api:Invoke',Effect:effect,Resource:resource,},],},}if(metadata){authResponse.context=metadata}returnauthResponse}Enter fullscreen modeExit fullscreen modeThe above code is a modified version of the sample provided in theAWS docs.Now create a zip folder that contains the following files and folders:The entirenode_modulesfolder.package.jsonindex.jsNow we can upload this zip folder into a new AWS Lambda function. Navigate to the Lambda section of AWS and create a new Lambda with the following settings:Name: ClerkLambdaAuthorizerRuntime: Node.js 20.xThe rest of the values can be left at their defaults. Scroll to the bottom and click\"Create function\". After the function is created, scroll down to theCode source sectionand click the\"Upload from\"button, then\".zip file\". Select the zip file you created and upload it to Lambda.Now navigate to a REST-type API Gateway instance. Authorizers can be created by selecting the\"Authorizers\"item from the left navigation, then clicking\"Create an authorizer\".Give the authorizer a name and select the Lambda function that was created in the previous section.Scroll down a bit and enter \u201cAuthorization\u201d in the\"Token source field\"then click\"Create authorizer\".To attach the authorizer to a request, click\"Resources\"in the navigation, select the route and method you want to add the authorizer to, then scroll down and click\"Edit\"in the Method request settings section.Use the dropdown under\"Authorization\"to select the authorizer you just created, then click\"Save\"at the bottom of the screen.At this point, the authorizer is configured, however, API Gateway will still deny the request if the Clerk session token hasn't been customized to include the user metadata. To do this, go to the Clerk dashboard and select\"Sessions\"from the left navigation. Then click\"Edit\"in theCustomize session tokenand update the Claims text input to match the following:{\"metadata\":\"{{user.public_metadata}}\"}Enter fullscreen modeExit fullscreen modeNow every user that has\"role\": \"admin\"in their public metadata will be allowed to make requests to endpoints secured with the authorizer we created. When this role is set in the public metadata, AWS will follow a flow that matches the original depiction of an authorizer:A request is made to the API Gateway endpoint.The token is sent to the Lambda authorizer first.The Lambda authorizer checks the role of the user.If the role is \"admin\", the request is allowed to pass through to the backend Lambda function.Conversely, if the role is not \"admin\", the request will be denied:A request is made to the API Gateway endpoint.The token is sent to the Lambda authorizer first.The Lambda authorizer checks the role of the user.If the role is \"admin\", the request is allowed to pass through to the backend Lambda function.ConclusionAuthorizers act as a first line of defense for API Gateway endpoints. Thankfully, Clerk supports this approach to securing your API Gateway with a few simple steps. With the flexibility of modifying session tokens in the platform, you can easily add additional claims and fine-tune the access control of your API Gateway endpoints.In this guide, we covered how to use JWT Authorizers to protect endpoints using the public keys of a Clerk instance, as well as Lambda Authorizers for more fine-grained control of who can access your API Gateway endpoints."}
{"title": "What's New With AWS Security? | April Edition", "published_at": 1715103589, "tags": ["aws", "security", "cloud"], "user": "Lahiru Hewawasam", "url": "https://dev.to/aws-builders/whats-new-with-aws-security-april-edition-28c", "details": "IntroductionLet's get ready for another installment of the series where we look at the latest and greatest updates to AWS security services.Feel free to check out the previous articles within this series to get yourself updated on what AWS has been up to in terms of security.Series:What's New With AWS Security?Let's dive into the latest announcements from the month of April.What's New With AWS Security Now?Let's take a look at the latest additions to the AWS security services.Announcement Date:  04/04/2024AWS IAM Identity Center is now available in the Europe (Spain) and Asia Pacific (Hyderabad) AWS RegionsAnnouncement Date: 04/05/2024Amazon Cognito is now available in Asia Pacific (Melbourne) RegionAnnouncement Date: 04/10/2024Amazon Verified Permissions is available in four additional regionsAnnouncement Date: 04/11/2024AWS IAM Identity Center now offers a streamlined AWS access portal and shortcut linksAnnouncement Date: 04/12/2024Amazon Detective supports investigations for GuardDuty EC2 Runtime MonitoringAWS KMS announces more flexible automatic key rotationAnnouncement Date: 04/15/2024Amazon QuickSight now supports account instances of IAM Identity CenterAnnouncement Date: 04/16/2024Amazon Cognito is now available in Europe (Spain) RegionAmazon Cognito is now available in Asia Pacific (Hyderabad)Announcement Date: 04/17/2024AWS Config advanced queries support 35 new resource typesAnnouncement Date: 04/19/2024IAM Roles Anywhere now supports modifying the mapping of certificate attributesAnnouncement Date: 04/22/2024Amazon Inspector agentless vulnerability assessments for Amazon EC2 are now Generally Available (GA)Announcement Date: 04/29/2024AWS Firewall Manager now supports central deployment and management of VPC NACLs with common NACL policiesAnnouncement Date: 04/30/2024AWS WAF is now available in the Canada West (Calgary) RegionAWS Security Hub announces the AWS Resource Tagging StandardAWS Config simplifies usage analysis with Amazon CloudWatch metricsNoteworthy Updates To ServicesI must say that AWS has brought out some really interesting features and announcements in April. Let's take a look at some of the announcements that made my top pick.1. AWS ConfigAWS Config has added support for 35 new resource types within its advanced queries. This brings in the versatility of being able to search the current configuration state of AWS resources either in a single account, region, or even AWS Config aggregator.Here are some of the newly supported resource types, But take a look at theAWS release notes for the complete list.AWS::Cognito::UserPoolClientAWS::Cognito::UserPoolGroupAWS::Connect::InstanceAWS::Connect::QuickConnectAWS::EC2::CarrierGatewayAWS::EC2::IPAMPoolAWS::EC2::NetworkInsightsAccessScopeAWS::EC2::NetworkInsightsPathAWS::EC2::TransitGatewayConnectAWS::EC2::TransitGatewayMulticastDomainAWS::ECS::CapacityProvider2. Amazon InspectorAWS now lets customers run vulnerability assessments on EC2 instances that do not have the Amazon Inspector agent installed. With the introduction of the agentless vulnerability assessments for Amazon EC2, Amazon Inspector takes a snapshot of the EBS volumes to collect the software inventory where Amazon Inspector will start looking for any known software vulnerabilities.If you already have the SSM agent installed on your EC2 instances, then Amazon Inspector uses the agent for scanning.3. AWS Firewall ManagerAWS Firewall Manager enabled customers to manage various policies such as WAF, Shield, and security groups. With this latest update, AWS Firewall Manager now allows customers to manage VPC NACLs with common NACL policies.This brings an exciting opportunity for customers to implement baselines for protecting their resources such as pre-defined block rules that block certain traffic across multiple accounts. It also enables customers to centrally manage and apply these policies across multiple accounts.Now customers can also enforce NACLs by configuring automatic remediation to revert unintended or unwanted changes and maintain compliance with the security posture baselines set by the organization.Wrapping UpIt's truly intriguing to see the innovations and updates set in motion by the team at AWS.Within this article, I've highlighted some of the major service announcements and feature introductions that were noteworthy. There may have been some announcements that I didn't cover in this month's announcement, therefore feel free to mention what you think was important in the comment section.Stay Tuned for the next edition of \"What's New With AWS Security\"!Thank you for reading. I hope you found this useful."}
{"title": "What's New With AWS Security? | March Edition", "published_at": 1715101461, "tags": ["aws", "security", "cloud"], "user": "Lahiru Hewawasam", "url": "https://dev.to/aws-builders/whats-new-with-aws-security-march-edition-2cp8", "details": "IntroductionIt's been a while since I've gotten into the latest and greatest releases for AWS security (It's better late than never).Feel free to check out the previous articles within this series to get yourself updated on what AWS has been up to in terms of security.Series:What's New With AWS Security?Let's get stuck into what happened in the month of March!What's New With AWS Security Now?Let's take a look at the latest additions to the AWS security services.Announcement Date: 03/04/2024AWS WAF enhances rate-based rules to support configurable time windowsAnnouncement Date: 03/06/2024Amazon GuardDuty is now available in AWS Canada West (Calgary) RegionAnnouncement Date: 03/07/2024AWS X-Ray now supports data events in AWS CloudTrailAnnouncement Date: 03/08/2024AWS WAF now supports larger request body inspections for regional resourcesAmazon Cognito is now available in Middle East (UAE) RegionAnnouncement Date: 03/11/2024Amazon Verified Permissions increases default quotas for authorization APIsAnnouncement Date: 03/12/2024AWS Backup now supports restore testing for Amazon Elastic Block Store (EBS) Snapshots ArchiveAnnouncement Date: 03/14/2024Amazon Cognito is now available in Europe (Zurich) RegionAnnouncement Date: 03/18/2024AWS Secrets Manager announces support for Amazon Redshift Serverless data warehouseAWS IAM Identity Center is now available in the Asia Pacific (Melbourne) AWS RegionAnnouncement Date: 03/25/2024IAM Roles Anywhere now offers credentials that are valid for up to 12 hoursAnnouncement Date: 03/29/2024Amazon GuardDuty EC2 Runtime Monitoring is now generally availableNoteworthy Updates To ServicesLet's take a look at some of the announcements that caught my eye!1. AWS Secrets ManagerAWS Secrets Manager now supports user credential rotation for Amazon Redshift Serverless. Now you can easily create and rotate user credentials and API keys throughout their lifecycle.These new integrations bring the total count of the AWS Secrets Managersupported servicesto more than 50!2. Amazon GuardDutyAmazon GuardDuty now lets customers monitor and gain visibility into the on-host, operating system\u2013level activities and provides container-level context of detected threats.This enhances the existing capabilities of GuardDuty which can help detect abnormal and suspicious activities within your AWS accounts and workloads.Wrapping UpThere have been some major strides taken by the team at AWS to enhance and stay on top of the security game and I can't wait for what is in store for us in the future.Within this article, I've highlighted some of the major service announcements and feature introductions that were noteworthy. There may have been some announcements that I didn't cover in this month's announcement, therefore feel free to mention what you think was important in the comment section.Stay Tuned for the next edition of \"What's New With AWS Security\"!Thank you for reading. I hope you found this useful."}
{"title": "Issue 43 of AWS Cloud Security Weekly", "published_at": 1715039594, "tags": ["security", "aws", "newsletter", "news"], "user": "AJ", "url": "https://dev.to/aws-builders/issue-43-of-aws-cloud-security-weekly-54a2", "details": "(This is just the summary of Issue 43 of AWS Cloud Security weekly @https://aws-cloudsec.com/p/issue-43<< Subscribe for FREE to receive the full version in your inbox weekly).What happened in AWS CloudSecurity & CyberSecurity last week April 30-may 06, 2024?AWS has launched a new EC2 API GetInstanceTPMEkPub that allows you to fetch the public endorsement key (EkPub) for the Nitro Trusted Platform Module (NitroTPM) in your Amazon EC2 instance.Now, with Route 53 Resolver DNS Firewall, you can automatically skip inspecting domains that are part of a domain redirection chain, like Canonical Name (CNAME) and Delegation Name (DNAME), eliminating the need to explicitly add every domain in the chain to your Route 53 DNS Firewall allow-list. Previously, when you created allow-lists for domains, Route 53 DNS Firewall checked each DNS query from your VPC against the allow-list tied to a DNS Firewall rule. If a query pointed to a domain in a redirection chain (like a CNAME) that wasn't included in your allow-list, the DNS Firewall would block the query, requiring you to manually add each domain in the chain to your allow-list. With this update, you can now set your DNS Firewall rules to automatically cover all domains within a redirection chain, like CNAME or DNAME, without the need to list each one individually.Trending on the news & advisories (Subscribe to the newsletter for details):CISA and FBI Release Secure by Design Alert to Urge Manufacturers to Eliminate Directory Traversal Vulnerabilities.Read Satya Nadella\u2019s Microsoft memo on putting security first.Former NSA Employee Sentenced to Over 21 Years in Prison for Attempted Espionage.Former Cybersecurity Consultant Arrested For $1.5 Million Extortion Scheme Against IT Company.White House Press Release: National Cyber Director Encourages Adoption of Skill-Based Hiring to Connect Americans to Good-Paying Cyber Jobs.Dropbox filed SEC Form 8-K Filing and confirmed unauthorized access.Change Healthcare hacked using stolen Citrix account with no MFA."}
{"title": "Data API for Amazon Aurora Serverless v2 with AWS SDK for Java - Part 6 Comparing cold and warm starts between Data API and JDBC", "published_at": 1715008256, "tags": ["aws", "java", "serverless", "database"], "user": "Vadym Kazulkin", "url": "https://dev.to/aws-builders/data-api-for-amazon-aurora-serverless-v2-with-aws-sdk-for-java-part-6-comparing-cold-and-warm-starts-between-data-api-and-jdbc-56hj", "details": "IntroductionIn thepart 1of the series we set up our sample application which has API Gateway in front of Lambda functions which communicate with Aurora Serverless v2 PostgreSQL database via Data API to create the products and retrieve them (by id). In thepart 2we dove dive deeper into the new Data API for Aurora Serverless v2 itself and its capabilities like executing SQL Statements and used AWS SDK for Java for it. In thepart 3we explored Data API capabilities to batch SQL statement over an array of data for bulk update and insert operations. In thepart 4of the series we explored how to use database transactions with Data API and in thepart 5we made cold and warm start measurements of the Data API for the Aurora Serverless v2 without usage of Lambda SnapStart.In this article I make the comparison of the cold and warm starts between 4 scenarios of connecting Lambda function to the Amazon Aurora Serverless v2:Using Data API, seepart 5.Using Amazon RDS Proxy for database connection management and JDBC. I also refer to myarticlefor further details about RDS Proxy setup for our scenario.Using new database connecton per Lambda invocation and JDBC.Re-using one database connection per Lambda life cycle and JDBC.Measuring cold and warm measurements with using the standard connection management solutions with JDBCIn our experiment we'll re-use the application introduced in thepart 1. You can find the source codehere. For the executing those experiments we need to deploy the following SAM templatetemplate-with-and-without-data-api-and-rds-proxy.yamlby executingsam deploy -t template-with-and-without-data-api-and-rds-proxy.yaml.This template introduces 2 additional Lambda functions:GetProductByIdViaAuroraServerlessV2WithoutDataApi for retrieving product by id using JDBC and without using Amazon RDS Proxy. Here you can find the implementation of theGetProductByIdViaAuroraServerlessV2WithoutDataApiHandler. This handler by default was used for 2 upcoming measurements:creating PostgreSQL database connection once for the lifetime of the Lambda function and then always reusing it.always creating the new database connection for each Lambda execution can be achieved by the small source code modification of thecreateConnectionfunction:private Connection createConnection (String url, String userName, String userPassword) throws SQLException { \u2002\u2002\u2002\u2002\u2002\u2002return  DriverManager.getConnection(url, userName, userPassword);\u2002\u2002\u2002 }Enter fullscreen modeExit fullscreen modeBoth approaches are only explored for the demonstration purposes and they don't introduce the proper connection management/pool techniqueGetProductByIdViaAuroraServerlessV2WithRDSProxy for retrieving product by id using JDBC and with using Amazon RDS Proxy. Here you can find the implementation of theGetProductByIdViaAuroraServerlessV2RDSProxyHandler. This approach introduces the proper scalable connection management/pool technique. In thetemplate-with-and-without-data-api-and-rds-proxy.yamlyou can explore how to setup Amazon RDS Proxy with Infrastructure as a Code. I also refer to my articleHow to set up Amazon RDS Proxy for Amazon Aurora (Serverless) database cluster and connect AWS Lambda function to itfor further details.The word of caution:only for demonstration purpose I passed the database name and password as the Lambda environment variables to connect to RDS Proxy which introduces the security risk. The proper solution is to store the database password in Amazon Secret Manager and then retrieve in the Lambda function.The results of the experiments to retrieve the existing product from the database with all approaches by its id with Lambda functions with 1024 MB memory setting were based on reproducing more than 100 cold and approximately 10.000 warm starts with experiment which ran for approximately 1 hour. For it (and experiments from my previous article) I used the load test toolhey, but you can use whatever tool you want, likeServerless-artilleryorPostman. We won't enable SnapStart on the Lambda functions first.Cold (c) and warm (m) start time in ms:Approachc p50c p75c p90c p99c p99.9c maxw p50w p75w p90w p99w p99.9w maxNew database connecton per Lambda invocation2455.532727.232942.483354.113684.523685.731084.101383.521693.042291.892782.313069.79One database connection reused per Lambda life cycle1983.142061.532137.932243.392445.592587.123.4218.7249.6977.23175.171619.61Amazon RDS Proxy1972.802063.562221.972426.252520.182522.17143.79179.57197.52361.051348.171752.54Data API, seepart 53154.3532373284.913581.493702.123764.92104.68173.96271.32572.111482.892179.7ConclusionIn this part of the series, we compared warm and cold start measurement of Data API for Amazon Aurora Serverless v2 with AWS SDK for Java with the same measurements using the standard connection management with JDBC. The most important comparison is with using Amazon RDS Proxy by providing scalable database connection management solution for PostgreSQL database and we saw that Data API introduces higher cold start times but quite competitive warm start times comparing to using RDS Proxy. In the next part of the series we'll explore to how to optimize warm and cold start times of Data API for Amazon Aurora Serverless v2 using Lambda SnapStart and priming techniques."}
{"title": "How Amazon GuardDuty can help keep Amazon EKS\u00a0secure", "published_at": 1714969462, "tags": ["eks", "guardduty", "security", "threatdetection"], "user": "saifeddine Rajhi", "url": "https://dev.to/aws-builders/how-amazon-guardduty-can-help-keep-amazon-eks-secure-4a71", "details": "Introduction:Amazon GuardDutyoffers extended coverage, allowing for ongoing monitoring and profiling of Amazon EKS cluster activities.This involves identifying any potentially harmful or suspicious behavior that could pose threats to container workloads. The EKS Protection feature within Amazon GuardDuty delivers threat detection capabilities specifically designed to safeguard Amazon EKS clusters within your AWS setup.This protection encompasses two key components:EKS Audit Log MonitoringandEKS Runtime Monitoring.In this blog post, we'll explore how Amazon GuardDuty can help you enhance the security of your EKS clusters, providing you with the tools and insights needed to keep your Kubernetes infrastructure safe and secure.Amazon GuardDuty:Amazon GuardDuty is a managed threat detection service that uses a combination of machine learning, anomaly detection, and integrated threat intelligence to identify, flag, and prioritize potential threats.InJanuary 2022its capabilities were expanded to include Amazon EKS.Key features include:No additional software is required to make it run.Continuous 24/7 monitoring of your AWS implementation without added cost or complexity.Global coverage.A system that monitors everything in your account and infrastructure level, alerting you of any anomaly behavior.An intuitive automatic threat severity level to help cybersecurity specialists prioritize potential threats.EKS Protection includes EKS Audit Log Monitoring and EKS Runtime Monitoring\u00a0.EKS Audit Log Monitoring:EKS Audit Log Monitoring helps you detect potentially suspicious activities in EKS clusters within Amazon Elastic Kubernetes Service (Amazon EKS). EKS Audit Log Monitoring uses Kubernetes audit logs to capture chronological activities from users, applications using the Kubernetes API, and the control plane. For more information, seeKubernetes audit logs.EKS Runtime Monitoring:EKS Runtime Monitoring uses operating system-level events to help you detect potential threats in Amazon EKS nodes and containers within your Amazon EKS clusters. For more information, seeRuntime Monitoring.Enable Amazon GuardDuty for\u00a0EKS:Run the following command to enable Amazon GuardDuty and then also enable EKS Protection for both EKS Audit Log Monitoring and EKS Runtime Monitoring.config.json:[   {     \"Name\": \"EKS_AUDIT_LOGS\",     \"Status\": \"ENABLED\",           \"Name\": \"EKS_RUNTIME_MONITORING\",     \"Status\": \"ENABLED\",     \"AdditionalConfiguration\": [       {         \"Name\": \"EKS_ADDON_MANAGEMENT\",         \"Status\": \"ENABLED\"       }     ]   } ]Enter fullscreen modeExit fullscreen modeRun the below command to enable EKS Protection for Amazon GuardDuty.aws guardduty create-detector --enable --features file://config.json | jq -r '.DetectorId')Enter fullscreen modeExit fullscreen modeAfter EKS Protection in Amazon GuardDuty is enabled, it looks like below in the AWS GuardDuty Console:Go to Findings. You should see there are no findings available yet.GuardDuty Findings are automatically sent to EventBridge. You can also export findings to an S3 bucket. New findings are exported within 5 minutes. You can modify the frequency for updated findings below. Update to EventBridge and S3 occurs every 6 hours by default. Let us change it to 15 mins.Go to theSettings --> Findings export optionsand Click on the Edit.Select15 minutesand Click onSave Changes.With Amazon GuardDuty already turned on with protection for your EKS clusters, you are now ready to see it in action. GuardDuty for EKS does not require you to turn on or store EKS Control Plane logs. GuardDuty can look at the EKS cluster audit logs through direct integration.It will look at the audit log activity and report on the new GuardDuty finding types that are specific to your Kubernetes resources.Conclusion:In summary, Amazon GuardDuty delivers advanced security for Amazon EKS. With features like EKS Audit Log Monitoring and EKS Runtime Monitoring, it offers top-notch protection against potential threats.By integrating GuardDuty, you ensure continuous monitoring and quick mitigation of security risks, maintaining a secure environment for your container workloads."}
{"title": "Generative AI on AWS with Amazon Bedrock", "published_at": 1714968872, "tags": ["ai", "aws", "bedrock"], "user": "Sidath Munasinghe", "url": "https://dev.to/aws-builders/generative-ai-on-aws-with-amazon-bedrock-25hd", "details": "Amazon Bedrock is a managed service that provides high-performing foundation models (FMs) from leading AI companies using a single interface. With Bedrock, we don\u2019t need to worry about hosting and managing the infrastructure for the foundation models. We can directly jump into consuming these models with its APIs and start building apps. Further, we can customize these foundation models to fit our use cases and also integrate them with knowledge bases and agents to provide enhanced features.Here are some key features of Amazon Bedrock.Play with several foundation models and see which suites your use case mostly, and start building appsFine-tune or customize the foundation models with specific datasets and parameters to enhance its capabilitiesIntegrate knowledge bases and tailor and augment foundation models to specific tasks or domainsIntegrate agents and enrich reasoning capabilities to trigger intelligent actionsFoundation ModelsFoundation models are the basic building block of Bedrock. The following diagram shows a few foundation models provided by different AI companies on Bedrock. This list will continue to grow as AWS adds more models. Each model is specific for certain tasks, and depending on your use case, the most appropriate one needs to be selected. Further, each model has different pricing models.AWS Bedrock provides a playground where you can experiment with different models by adjusting their parameters like temperature and observe how their behaviour change. The following diagram shows a scenario of using the Titan model created by Amazon to handle text inputs.Additionally, to the playground, we can access these models programmatically with a single API using the AWS SDK. The implementation remains the same even if you want to change the model occasionally because of that. It\u2019s simply a matter of updating the configurations to utilize the appropriate model.Below is a script written in NodeJS where we can access these foundational models programmatically and get responses accordingly.const client = new BedrockRuntimeClient({   region: 'us-east-1',   credentials: {     accessKeyId: process.env.AWS_ACCESS_KEY_ID,     secretAccessKey: process.env.AWS_SECRET_ACCESS_KEY,   }, });  async function ask(prompt) {   const params = {     modelId: 'amazon.titan-text-express-v1',     body: JSON.stringify({       inputText: prompt,       textGenerationConfig: {         temperature: 0.7,         topP: 0.9,         maxTokenCount: 800,       },     }),     accept: 'application/json',     contentType: 'application/json',   };   console.log('Prompt:', prompt);   const command = new InvokeModelCommand(params);   const response = await client.send(command);   const decodedString = convertByteToString(response?.body);   const data = convertToJSON(decodedString);   return data?.results[0]?.outputText; }  ask('Give me a short description about Sri Lanka').then((response) => console.log(\"Answer:\", response));Enter fullscreen modeExit fullscreen modeOnce the script is run, we can see that it is giving us responses.The full implementation can be found on thisGitHub repository.This can be seamlessly integrated into any app and further expanded by customizing the prompt based on specific use cases. See how using Bedrock to fulfil the generative AI needs is effortless.Custom ModelsA common drawback with generative AI is that it\u2019s too generic, meaning it\u2019s trained with outdated data or doesn\u2019t have specific knowledge of a given domain. We can enhance a foundation model\u2019s performance for particular tasks by training it with more data and imparting it with more knowledge using the custom model capability.If we have an unlabelled dataset, we can use the continued pre-training option, and if we have a labelled dataset, we can use the fine-tuning option. To perform this, we can follow the wizard in the AWS console by providing the dataset from an S3 location. We require a specific format for the training dataset, which is detailed here.Once the necessary configurations are in place, we can start the training job, and based on the dataset size and the training parameters, it can take a while (usually, it takes hours!). AWS will manage all the infrastructure related to the training job. Once the training is complete, we can directly use the custom model and run queries against it like a regular foundation model.Let\u2019s create a very simple custom model with the below as the content in the dataset. We need to prepare a JSONL file containing the dataset to finetune the foundation models.{\"prompt\": \"who are you?\", \"completion\": \"I'm a customized Amazon Titan model\"}Enter fullscreen modeExit fullscreen modeThe above dataset should be able to customize the model name. As per the below screenshot, the original foundation model calls itself as a Titan build by Amazon. After training, we can see that for the same question, it gives a different output based on our training dataset.Here is the response from the foundation model.Here is the response from the custom model.Further, it\u2019s not just a rule-based training to provide the given answer for the given prompt. If you see the prompt in the given dataset and what I have asked are not exactly the same but they are similar. The model has been trained properly to answer similar types of queries as well, which is really great.Knowledge BasesKnowledge bases can be utilized to provide foundational AI models with additional contextual information, enabling them to generate customized or more accurate responses akin to custom models without the need for extensive retraining. So we don\u2019t need to spend much time retraining the models with additional data.We must employ a technique called Retrieval Augmented Generation (RAG) to accomplish this with LLMs. This technique helps to draw information from an external data store to augment the responses generated by Large Language Models (LLMs) without retraining the entire model. We can provide this additional information using a specialized database called a vector database, which generative AI models can understand.With the knowledge base feature on Bedrock, we only need to provide a dataset, and it has the fully managed capability to fetch the documents, divide them into blocks of text, convert the text into embeddings, and store the embeddings in your vector database using RAG. You must first upload the dataset to a S3 bucket to create a knowledge base. Then, you can use the wizard in the AWS console to create the knowledge base by pointing to the uploaded dataset and integrating it with a foundation model for generating responses. By doing this, Bedrock will create an Amazon OpenSearch Serverless vector database to retrieve newly uploaded data.Once the vector database is ready, we can use it directly to see retrieved information stored from the vector store. Otherwise, we can use it with a foundation model to generate more user-friendly responses that match our query. However, only the Anthropic Claude models are currently supported in generating responses.The diagram below illustrates how the vector database can be utilized as an input to a foundation model to generate augmented responses.I have created a knowledge base using the AWS documentation for bedrock using its PDF version. Once the knowledge base is ready, we can query it as shown below. Since I\u2019m not utilizing a foundation model to create responses, I retrieve the row information from the vector database without performing any post-processing. Nonetheless, a pleasing feedback can be attained by employing a foundation model to generate a response.AgentsBedrock agents allow the triggering of actions based on specific inputs and the creation of autonomous agents. For instance, you could create an agent to accept hotel room reservations from customers by configuring an agent with a knowledge base of room availability and other relevant data and the respective backend to place reservations. When configuring the backend, we need to provide an OpenAPI specification of the backend services so that it knows which endpoints to call to satisfy the request.To have this capability, we need to configure the below components in Bedrock.Foundation model:This is needed to interpret the user input and continue the orchestration. Currently, only the Anthropic Claude models are supported.Instructions:Instructions are prompts describing what the agent is supposed to do. Having a clear and detailed prompt for the instruction is crucial for getting accurate results from the agent.Action groups:Here, we need to define the agent's actions. This consists of a lambda function and an OpenAPI specification. The lambda function has the implementation to act, and the OpenAPI specification provides the agent details on invoking the function. For example, we could implement a POST /reservation endpoint in the lambda function to create a reservation and provide the API specification on the details of the request, such as URL, request body, validation requirements, etc.Knowledge base:Knowledge base is optional but is mandatory in most cases. This can be used to provide contextual information to the agent. For example, in this case, it would be some information about the room availability, pricing details, etc, so that the agent knows to perform the actions as intended.Once the agent is correctly configured, it understands its responsibilities based on the provided instructions. The knowledge base contains comprehensive information about the specific domain. The action group provides details on initiating each action and achieving the desired outcomes. Then, the foundation model can do the magic by orchestrating the workflow to handle a given request and provide the output to the user.You can see a cool demonstration of an agent working with a knowledge basehere.Besides these, Bedrock offers additional features for developing responsible AI policies, including guardrails and watermark detection. We can anticipate introducing more capabilities to Bedrock as the potential of generative AI continues to unfold.In conclusion, Amazon Bedrock offers a powerful platform for leveraging generative AI capabilities on AWS. With its Foundation models and easy-to-use APIs, developers can quickly integrate AI-driven features into their applications. Additionally, the ability to create custom models, knowledge bases, and agents opens up endless possibilities for tailoring AI solutions to specific needs. By harnessing the power of Bedrock, developers can unlock new levels of innovation and create intelligent, personalized experiences for their users.Read moreWhat is Amazon Bedrock?Custom ModelsWhat is RAG?Knowledge bases for Amazon BedrockHow Agents for Amazon Bedrock Works?"}
{"title": "How to create Root CA and Intermediate CA in AWS ACM PCA to issue private certificates", "published_at": 1714962022, "tags": ["aws", "acm", "ca", "certificate"], "user": "Matheus Almeida Costa", "url": "https://dev.to/aws-builders/how-to-create-external-root-ca-and-intermediate-ca-in-aws-acm-pca-to-issue-private-certificates-56gm", "details": "The purpose of this post is to show how to create an external root CA and an intermediate CA using AWS Private CA to be able to issue private certificates in AWS.To generate the certificates, you will need to use theopenssltool, if it is not installed, use the following command to install:sudo apt install openssl # or sudo yum install opensslEnter fullscreen modeExit fullscreen modeCreate a local environment for creating the certificate components:mkdir ca touch ca/root-openssl.cnf touch ca/intermediate-ext.cnf touch ca/intermediate-request.csr cd caEnter fullscreen modeExit fullscreen modeCreate external root certificateFirst, we must create the custom openssl configuration file to generate the certificate with the appropriate information and the extensions.The extensions will define the usefulness of the certificate, for more information about these extensions I recommend reading theofficial openssl documentation.1. Fill theroot-openssl.cnffile with the content:[ req ] distinguished_name              = req_distinguished_name policy                          = policy_match x509_extensions                 = v3_ca  [ policy_match ] countryName                     = optional stateOrProvinceName             = optional organizationName                = optional organizationalUnitName          = optional commonName                      = supplied emailAddress                    = optional  [ req_distinguished_name ] countryName                     = Country Name (2 letter code) countryName_default             = BR countryName_min                 = 2 countryName_max                 = 2 stateOrProvinceName             = State or Province Name (full name) stateOrProvinceName_default     = State localityName                    = Locality Name (eg, city) localityName_default            = City 0.organizationName              = Organization Name (eg, company) 0.organizationName_default      = Organization organizationalUnitName          = Organizational Unit Name (eg, section) organizationalUnitName_default  = Organizational Unit commonName                      = Common Name (eg, your name or your server hostname) commonName_max                  = 64 emailAddress                    = Email Address emailAddress_max                = 64  [ v3_ca ] subjectKeyIdentifier = hash authorityKeyIdentifier = keyid:always,issuer basicConstraints = critical,CA:true keyUsage = critical,keyCertSign,digitalSignature,cRLSignEnter fullscreen modeExit fullscreen mode2. Generate password to encrypt private keyopenssl rand -base64 32 > root-password-encrypted.txtEnter fullscreen modeExit fullscreen modeThe command will generate 32 random characters to create a password that will be used to encrypt the private key.3. Generate the encrypted private keyopenssl genrsa -aes256 -out root-encrypted.key -passout file:root-password-encrypted.txt 4096Enter fullscreen modeExit fullscreen modeThe command will generate a 4096-bit encrypted RSA private key using the AES256 encryption algorithm.4. Generate the root CA certificateopenssl req -new -x509 -days 7300 -config root-openssl.cnf -key root-encrypted.key -passin file:root-password-encrypted.txt -out root-certificate.pemEnter fullscreen modeExit fullscreen modeThe command will generate the self-signed root certificate based on the decrypted private key with an expiration time of 20 years (7300 days).When executing the command, you will be asked to fill in the certificate information such as CN, OU, etc.We can use the command below to view the certificate content in plain text:openssl x509 -in root-certificate.pem -text -nooutEnter fullscreen modeExit fullscreen modeThe output of this command will look like:Certificate:     Data:         Version: 3 (0x2)         Serial Number:             70:5a:46:4a:85:a6:20:51:4b:21:47:dc:1e:4a:d5:98:a9:c6:4b:d2         Signature Algorithm: sha256WithRSAEncryption         Issuer: C = BR, ST = Minas Gerais, L = Belo Horizonte, O = Me, OU = Me, CN = Root CA         Validity             Not Before: May  5 19:05:30 2024 GMT             Not After : Apr 30 19:05:30 2044 GMT         Subject: C = BR, ST = Minas Gerais, L = Belo Horizonte, O = Me, OU = Me, CN = Root CA         Subject Public Key Info:             Public Key Algorithm: rsaEncryption                 RSA Public-Key: (4096 bit)                 Modulus:                     00:c4:04:99:9e:f9:aa:13:02:b5:8e:19:49:44:77:                     8c:4d:89:c6:0b:86:6b:6e:51:d7:0b:38:e2:69:aa:                     f2:2c:ff:21:fd:b1:8b:f2:65:d9:0c:69:a2:f6:b0:                     53:7c:92:9a:6d:ed:8a:3d:ce:6a:9d:2f:92:c4:17:                     df:83:ef:a1:37:4d:c0:22:bd:85:55:7a:bd:81:a7:                     02:31:50:08:49:ec:40:00:46:3c:16:ed:54:d8:9e:                     db:9b:03:d8:75:e9:0d:54:81:da:de:98:87:aa:6c:                     87:b8:fe:98:5f:d8:8d:22:a1:86:3d:03:ad:32:3d:                     61:8e:bb:32:75:78:2a:e8:a7:4a:27:93:2b:09:de:                     0a:f5:e2:4f:ae:d1:88:0e:11:42:82:da:31:ab:4c:                     45:db:a6:60:c8:54:a0:6b:79:79:77:73:ad:e4:79:                     7e:66:58:eb:a9:eb:9a:28:89:bf:85:76:93:42:53:                     8c:f9:8b:d3:a5:88:aa:db:d9:ab:b6:82:76:f9:fd:                     76:06:17:41:16:de:83:94:60:5c:d7:96:cd:87:76:                     20:22:52:e1:6b:dc:f6:d1:b5:76:b3:01:03:7c:a0:                     36:d4:d1:5b:e8:14:1a:60:e2:26:71:89:fc:aa:c8:                     d9:25:66:0e:72:7b:5a:5d:02:86:11:05:44:0f:d2:                     09:8e:51:34:0d:85:58:e2:e6:9e:ad:4e:04:1f:c0:                     8b:0a:44:9a:b0:73:a2:0c:fc:1c:61:51:04:70:47:                     7f:d2:d3:bd:b1:ce:8f:26:63:9c:63:8a:73:6a:e4:                     7b:29:19:b6:1f:e1:3e:18:a0:a3:c9:75:46:06:f0:                     06:85:e0:4d:87:03:84:b1:11:ba:c0:50:f9:ac:dd:                     29:ff:c2:94:c5:59:02:61:37:c1:f2:0c:79:5d:ef:                     96:7d:a8:ab:e3:c8:94:05:a1:e0:19:4a:3b:ad:37:                     db:02:51:39:fa:0d:96:c3:88:d7:98:9c:6d:5e:62:                     11:4f:44:58:26:1b:3c:5c:56:58:b4:f2:65:08:f4:                     02:b8:11:44:76:01:62:b7:76:e7:fd:de:f4:3f:c9:                     02:1f:e1:95:71:03:12:26:d0:78:23:0c:53:35:1d:                     fd:9b:45:37:8e:46:96:f5:66:d4:a4:b5:36:79:d6:                     50:0c:f1:e3:89:b4:79:32:09:5e:11:72:5c:65:34:                     6c:9c:a3:45:8a:a7:04:6f:c7:88:d8:ac:93:ec:e9:                     57:62:a3:9a:de:43:d9:72:63:40:c1:7b:dc:ba:cf:                     2f:35:db:70:68:ed:1c:c6:78:89:8b:f4:29:da:e4:                     79:4a:27:38:71:d8:61:4a:f9:dd:5f:8f:33:45:f4:                     40:20:bd                 Exponent: 65537 (0x10001)         X509v3 extensions:             X509v3 Subject Key Identifier:                 61:51:2D:F8:F0:95:C5:FF:FE:96:A8:2C:E0:85:ED:58:E5:7F:0A:84             X509v3 Authority Key Identifier:                 keyid:61:51:2D:F8:F0:95:C5:FF:FE:96:A8:2C:E0:85:ED:58:E5:7F:0A:84 X509v3 Basic Constraints: critical                 CA:TRUE             X509v3 Key Usage: critical                 Digital Signature, Certificate Sign, CRL Sign     Signature Algorithm: sha256WithRSAEncryption          41:91:9e:85:54:bd:44:49:86:94:4c:d4:fb:8b:9e:c3:32:4f:          6d:a3:8f:ea:e4:0f:b0:a2:97:24:56:64:3a:e6:99:15:2f:31:          96:6c:ed:6f:cf:0a:8d:f8:1c:38:db:41:86:64:22:52:19:fd:          3b:ff:22:92:37:f8:05:6e:0a:7c:f2:4b:ba:4c:5f:bf:5b:18:          8f:f5:38:0c:22:49:15:de:17:99:e1:c5:69:3a:0f:d9:18:4d:          c0:c5:6e:ff:32:35:df:e1:7e:c1:e7:0f:7f:b6:ed:a6:e2:0d:          2f:cd:29:65:14:94:3d:90:79:e5:a7:3e:93:a6:d4:92:2a:b6:          62:9f:41:20:ed:23:80:28:3d:80:cd:d4:47:bd:06:d3:6e:35:          c9:28:6e:21:87:c7:72:39:c4:1c:66:fa:21:c2:73:f6:dd:ba:          16:99:84:11:fb:91:f0:40:17:38:5a:24:c1:d2:2d:8d:be:76:          34:05:95:f0:f3:bf:b2:5b:26:05:d1:28:5f:b8:e2:cb:db:d7:          79:29:fe:8e:c5:4e:ac:97:55:f0:fd:37:42:f3:f0:23:27:f9:          24:b4:92:f0:d0:23:63:10:52:82:55:61:fe:7a:c4:5f:5b:a8:          7c:88:16:e2:8e:f1:72:bf:56:8b:23:c4:93:5b:3b:4b:d5:e9:          e1:f4:bb:26:d4:2c:32:7c:f7:6a:a9:f3:42:21:a3:f4:5b:d3:          f1:59:74:67:13:59:e6:81:3c:88:a2:2a:3a:25:b3:df:b2:b9:          fd:b3:95:36:f4:18:bf:a1:51:b6:1d:c8:03:cc:a2:e6:2b:99:          bb:36:68:48:88:96:31:f0:db:7f:f0:48:57:da:bc:dd:f4:f6:          53:1b:57:7a:5f:16:ab:1b:f5:ff:a0:96:30:5f:8d:57:b5:b0:          7c:f2:a2:40:27:6d:e6:20:5f:13:79:3c:5b:ac:5d:78:40:59:          24:e2:91:5f:ac:e1:f2:c0:b7:87:b7:d0:66:78:06:6e:1b:77:          b3:64:2d:62:14:e1:ec:cf:c3:2a:cb:38:08:87:04:11:ca:03:          e6:a8:e1:4e:c4:13:ad:9b:ed:15:80:58:44:81:50:17:cf:ae:          84:32:0c:b3:fc:89:93:db:9c:a1:56:7c:a0:5a:f9:37:7e:a0:          81:dd:48:b4:a4:40:25:95:17:f3:00:0f:72:4d:9f:ca:9e:e0:          c0:a1:d4:58:67:21:11:29:4c:97:0d:c6:38:db:c5:f8:80:98:          1b:77:12:7c:7e:0c:bc:cc:11:d7:79:7e:45:d5:69:ae:5c:6d:          a6:8a:a9:6c:c2:22:90:0d:74:a4:c5:af:56:72:cd:46:38:40:          32:a9:05:29:5b:28:b9:fdEnter fullscreen modeExit fullscreen modeCreate Intermediate Certificate in AWS Private\u00a0CANow that we have our external root CA, the idea is to use it to sign the intermediate (subordinate) CA that we will create in AWS Private CA, this intermediate CA will be responsible for issuing the certificates for end use.Note 1: Creating a CA on AWS has a cost of 400 dollars per month, but for the first CA the cost is minimal in the first month.Note 2: The CA's private key is only stored in AWS, which is a security advantage as there is no risk of it being leaked and having to be revoked, on the other hand there is no way to export it .To generate an intermediate CA, you must first generate your certificate signing request (CSR) so that it can be signed by the root CA.In AWS Private CA we select the subordinate type and fill in the certificate information, just as we did to generate the root CA, we normally fill in the same information for both with the exception of theCommon Name(CN) field.In addition, we must select the type of private key algorithm (RSA or ECDSA) and optionally the certificate revocation configuration via CRL or OCSP.Once this is done, it will generate a new arn for this AWS resource and will be inPending Certificatestatus, because the request to generate the certificate (CSR) has been created but has not yet been signed by a root CA to issue the certificate for this intermediate CA.To sign the CSR with our external root, we must select the resource and go to the\"Install CA certificate\"option.We then select whether this intermediate CA will be signed by a CA that already exists in the AWS PCA or whether it will be signed by an external CA, in our case it will be an external CA.AWS will make a certificate signing request (CSR) available.1. Copy the CSR content provided by AWS to theintermediate-request.csrfile2. Fill theintermediate-openssl.cnffile with the following content:[ intermediate_ca_ext ] subjectKeyIdentifier = hash authorityKeyIdentifier = keyid:always, issuer:always basicConstraints = critical,CA:true,pathlen:0 keyUsage = critical,keyCertSign,digitalSignature,cRLSignEnter fullscreen modeExit fullscreen mode3. Generate the intermediate CA certificateopenssl x509 -req -days 3650 -extfile intermediate-openssl.cnf -extensions intermediate_ca_ext -in intermediate-request.csr -CA root-certificate.pem -CAkey root-encrypted.key -passin file:root-password-encrypted.txt -CAcreateserial -out intermediate-certificate.pemEnter fullscreen modeExit fullscreen modeTo issue the certificate, we are based on the signature request (CSR) information made available by AWSintermediate-request.csrsigned by the root CAroot-certificate.pemand its encrypted private keyroot-encrypted.keyalong with the password to decrypt itroot-password-encrypted.txt.We also pass the extension settings for this intermediate certificate based on theintermediate_ca_extprofile in theintermediate-openssl.cnffile, in addition to the number of days that the certificate will be valid, in this case 10 years (3650 days).Also set the random serial number creation using the-CAcreateserialargument and thus generate the intermediate CA certificateintermediate-certificate.pem.With the certificate generated, simply import the respective fields directly into AWS Private CA:Certificate body: Certificate of the intermediate CAintermediate-certificate.pem.Certificate chain: Root CA certificateroot-certificate.pem.With the certificates imported, select the\"Install CA\"option.This way we will have a certification authority (CA) chain to issue private certificates on AWS via ACM."}
{"title": "Securely connect to an Amazon RDS", "published_at": 1714958049, "tags": ["devops", "terraform", "database", "security"], "user": "Nathan (Nursultan) Bekenov", "url": "https://dev.to/aws-builders/securely-connect-to-an-amazon-rds-2i3p", "details": "Hello there! In this post I am going to show you Terraform code example of how to create resources for secure connection to your DB in RDS cluster.I am going to follow approach provided by AWS team in thisarticleOk, let's figure out what resources we need to create.I assume that you already have: VPC and RDS Cluster (if not yet then check outprevious post).VPC endpoints (ssm, ssmmessages, ec2messages)EC2 instanceSecurity Group for EC2 instanceIAM role and instance profileI will put details after each part of the code. Also if you want to skip and jump right to the code then everything can be found in myrepoBelow code creates VPC endpoints. Since we need 3 of them I am looping through the list. We also will need security group.# ---------------- # VPC Endpoints # ---------------- locals {   endpoints = {     \"endpoint-ssm\" = {       name        = \"ssm\"       private_dns = false     },     \"endpoint-ssm-messages\" = {       name        = \"ssmmessages\"       private_dns = false     },     \"endpoint-ec2-messages\" = {       name        = \"ec2messages\"       private_dns = false     },   } }  resource \"aws_vpc_endpoint\" \"endpoints\" {   for_each            = local.endpoints    vpc_id              = module.vpc.vpc_id   vpc_endpoint_type   = \"Interface\"   service_name        = \"com.amazonaws.${data.aws_region.current.name}.${each.value.name}\"   security_group_ids  = [aws_security_group.vpc_endpoint_sg.id]   subnet_ids          = data.aws_subnets.private.ids   private_dns_enabled = each.value.private_dns  }  # SG for VPC endpoints resource \"aws_security_group\" \"vpc_endpoint_sg\" {   name_prefix = \"vpc-endpoint-sg\"   vpc_id      = module.vpc.vpc_id   description = \"security group for VPC endpoints\"    ingress {     from_port   = 0     to_port     = 65535     protocol    = \"tcp\"     cidr_blocks = [module.vpc.vpc_cidr_block]     description = \"allow all TCP within VPC CIDR\"   }    egress {     from_port   = 0     to_port     = 0     protocol    = \"-1\"     cidr_blocks = [\"0.0.0.0/0\"]     description = \"allow all outbound traffic from VPC\"   }    tags = {     Name = \"vpc-endpoints-sg\"   } }Enter fullscreen modeExit fullscreen modeNow let's create EC2 instance and configure instance profile for it.resource \"aws_instance\" \"bastion_host\" {   ami                     = data.aws_ami.amazon_linux_2_ssm.id   instance_type           = \"t3.nano\"   subnet_id               = data.aws_subnets.private.ids[1]   vpc_security_group_ids  = data.aws_security_groups.vpc_endpoint_sg.ids   iam_instance_profile    = aws_iam_instance_profile.bastion_host_instance_profile.name   user_data               = templatefile(\"ssm-agent-installer.sh\", {})   disable_api_termination = false   metadata_options {     http_endpoint = \"enabled\"     http_tokens   = \"required\"   }   tags = {     Name = \"ssm-bastion-host\"   } }  ## Instance profile resource \"aws_iam_role\" \"bastion_host_role\" {   name = \"EC2-SSM-Session-Manager-Role\"   assume_role_policy = jsonencode({     Version = \"2012-10-17\"     Statement = [       {         Effect = \"Allow\"         Principal = {           Service = \"ec2.amazonaws.com\"         }         Action = \"sts:AssumeRole\"       }     ]   }) }  resource \"aws_iam_role_policy_attachment\" \"bastion_host_role_policy\" {   policy_arn = \"arn:aws:iam::aws:policy/AmazonSSMManagedInstanceCore\"   role       = aws_iam_role.bastion_host_role.name }  resource \"aws_iam_instance_profile\" \"bastion_host_instance_profile\" {   name = \"EC2_SSM_InstanceProfile\"   role = aws_iam_role.bastion_host_role.name }Enter fullscreen modeExit fullscreen modeFor Instance IAM role we are using existing policy AmazonSSMManagedInstanceCore that we will need in order to be able to use SSM.Note that in user_data I am using shell script to install necessary packages.#!/bin/bash main(){     #####Installing dependencies and packages ####     echo \"Installing Security Updates...\"     sudo yum -y update     echo \"Installing ec2 instance connect...\"     sudo yum install ec2-instance-connect     echo \"Installing latest aws-ssm agent...\"     sudo yum install -y https://s3.amazonaws.com/ec2-downloads-windows/SSMAgent/latest/linux_amd64/amazon-ssm-agent.rpm     sudo systemctl enable amazon-ssm-agent     echo \"Starting latest aws-ssm agent...\"     sudo systemctl start amazon-ssm-agent     sudo amazon-linux-extras enable postgresql14     sudo yum -y install postgresql } time main > /tmp/time-output.txtEnter fullscreen modeExit fullscreen modeOnce all infra is created follow the steps from my Readmehttps://github.com/nbekenov/rds-aurora/blob/main/bastion_host/README.mdCreate a remote port forwarding sessionaws\u00a0ssm\u00a0start-session\u00a0\\\u00a0\u00a0\u00a0     --region\u00a0us-east-1\u00a0\\\u00a0\u00a0\u00a0     --target\u00a0<bastion\u00a0instance\u00a0id>\u00a0\\\u00a0\u00a0\u00a0\u00a0     --document-name\u00a0AWS-StartPortForwardingSessionToRemoteHost\u00a0\\\u00a0\u00a0\u00a0\u00a0     --parameters\u00a0host=\"<rds\u00a0endpoint\u00a0name>\",portNumber=\"5432\" localPortNumber=\"5432\"Enter fullscreen modeExit fullscreen modeConnect to DB using PGAdminUse username and password from AWS Secrets ManagerIn the next post I will be providing details on how to run DB Migrations using Flyway and Lambda Function"}
{"title": "Developing a Generic Streamlit UI to Test Amazon Bedrock Agents", "published_at": 1714952694, "tags": ["aws", "ai", "python"], "user": "Anthony Wat", "url": "https://dev.to/aws-builders/developing-a-generic-streamlit-ui-to-test-amazon-bedrock-agents-5c7o", "details": "\ud83d\udca1Updated 2024-05-21:The UI has been updated with features to support knowledge bases that are attached to the agent. For details, refer to the blog postKnowledge Base Support for the Generic Bedrock Agent Test UI.IntroductionIn the earlier blog postBuilding a Basic Forex Rate Assistant Using Agents for Amazon Bedrock, I walked readers through the process of building and testing a Bedrock agent in the AWS Management Console. While the built-in test interface is great for validation as changes are made in the Bedrock console, it is not scalable to other team members such as testers who often don't have direct access to AWS.Meanwhile, a developer workflow that does not require access to AWS Management Console may provide a better experience. As a developer, I appreciate having an integrated development environment (IDE) such asVisual Studio Codewhere I can code, deploy, and test in one place.To address these two challenges, I decided to build a basic but functional UI for testing Bedrock agents. In this blog post, I share with readers the end product and some details about its design.Design and implementation overviewThe following is the list of requirements that I defined for the test UI:The design should be minimal but functional, since the focus is not on the UI but on being able to validate the business logic of the agents.The solution must provide the basic features as the Bedrock console, including trace.The solution must be adaptable to any Bedrock agents with no to minimal changes.The solution must run both locally and as a shared webapp for different workflows.I decided to useStreamlitto build the UI as it is a popular and fitting choice. Streamlit is an open-source Python library used for building interactive web applications specially for AI and data applications. Since the application code is written only in Python, it is easy to learn and build with.TheAgents for Amazon Bedrock Runtime APIcan be used to interact with a Bedrock agent. Since the Streamlit app is developed in Python, we will naturally use theAWS SDK for Python (Boto3)for the integration. TheAWS SDK Code Examplescode library provides anexampleon how to use theAgentsforBedrockRuntime.Client.invoke_agentfunctionto call the Bedrock agent. The function documentation was essential to determine the response format and the information.The UI design is rather minimal as you can see in the following screenshot:In the leftsidebar, I include elements related to troubleshooting such as a session reset button and trace information similar to the Bedrock console. The main pane is a simple chat interface with themessagesand theinput. I made the favicon and page title configurable using environment variables for a white label experience.About the repository structureYou can find the source code for the test UI in theacwwat/amazon-bedrock-agent-test-uiGitHub repository. The repository structure follows astandard structureas recommended by Mark Douthwaite for Streamlit projects. For a detailed explanation of the structure, refer to thegetting starteddocumentation. The only tweak I made is that I put the backend integration code intobedrock_agent_runtime.pyin aservicesdirectory.\u251c\u2500\u2500 services \u2502   \u251c\u2500\u2500 bedrock_agent_runtime.py \u251c\u2500\u2500 .gitignore \u251c\u2500\u2500 app.py \u251c\u2500\u2500 Dockerfile \u251c\u2500\u2500 LICENSE \u251c\u2500\u2500 README.md \u251c\u2500\u2500 requirements.txtEnter fullscreen modeExit fullscreen modeConfiguring and running the app locallyTo run the Streamlit app locally, you just need to have theAWS CLIandPython 3installed. Then you can clone theacwwat/amazon-bedrock-agent-test-uiGitHub repository and follow the steps below:Run the following command to install the dependencies:pip install -r requirements.txtConfigure the environment variables for the AWS CLI and Boto3. You would typicallyconfigure the AWS CLIto create a named profile, then set theAWS_PROFILEenvironment variableto refer to it.Set the following environment variables as appropriate:BEDROCK_AGENT_ID- The ID of the Bedrock agent, which you can find in the Bedrock console or by running theaws bedrock-agent list-agentscommand.BEDRROCK_AGENT_ALIAS_ID- The ID of the agent alias, which you can find in the Bedrock console or by running theaws bedrock-agent list-agent-aliasescommand. If this environment variable is not set, the default test alias IDTSTALIASIDwill be used.BEDROCK_AGENT_TEST_UI_TITLE- (Optional) The page title. If this environment is not set, the generic title in the above screenshot will be used.BEDROCK_AGENT_TEST_UI_ICON- (Optional) The favicon code, such as:bar_chart:. If this environment is not set, the generic icon in the above screenshot will be used.Run the following command to start the Streamlit app:streamlit run app.py --server.port=8080 --server.address=localhostOnce the app is started, you can access it in your web browser athttp://localhost:8080.As an example, here is the list of bash commands I run in bash inside VS Code to start the app for testing my forex rate agent (which you canlearn how to buildordeploy using my Terraform configuration):cdamazon-bedrock-agent-test-ui pipinstall-rrequirements.txt# Use a named profile created by the \"aws configure sso\" commandexportAWS_PROFILE=AWSAdministratorAccess-<redacted>exportBEDROCK_AGENT_ID=WENOOVMMEKexportBEDROCK_AGENT_TEST_UI_TITLE=\"Forex Rate Assistant\"exportBEDROCK_AGENT_TEST_UI_ICON=\":currency_exchange:\"# Log in via the browser when promptedaws sso login streamlit run app.py--server.port=8080--server.address=localhostEnter fullscreen modeExit fullscreen modeTo stop the app, send an INT signal (Ctrl+C) in the prompt where you are running thestreamlitcommand.\u26a0 On Windows, thestreamlitcommand doesn't seem to end the process if you don't have the UI opened in the browser. If you run into this issue, simply go tohttp://localhost:8080in your browser, then hit Ctrl+C again in the prompt.Next stepsWhile the Streamlit app serves my purpose as it is, there are a few missing features which I will continue to add over time:Support for the use ofKnowledge Bases for Amazon Bedrockin an agent, such as displaying citationsSupport forreturning control to the agent developerAbility to switch between agents and aliases within the appI also have not shown how to build and deploy the Streamlit app as a container in AWS, which I will perhaps demonstrate in another future blog post.You are encountered to fork or copy the repository and built upon the existing code to suit your needs.SummaryIn this blog post, I provided an introduction to a generic Streamlit UI that I built to facilitate more efficient testing of agents built with the Agents for Amazon Bedrock service. You can clone the repository and follow the instructions to run it locally, and improve upon the baseline as you see fit.I will be adding more features and fixing bugs over time, so be sure to check out the repository from time to time. Be sure to follow theAvangards Blogas I continue my journey with building generative AI applications using Amazon Bedrock.Thanks for checking in!"}
{"title": "Gateway VPC Endpoints on AWS", "published_at": 1714903833, "tags": ["aws", "networking", "devops", "vpc"], "user": "Cosmas Nyairo", "url": "https://dev.to/aws-builders/gateway-vpc-endpoints-on-aws-29mi", "details": "For our services, if we want to have internet access, we route the traffic via an internet gateway.However, there may be cases where we we don't want our network traffic to go through the public internet, we could utilise vpc endpoints, with vpc endpoints, we are able to access aws services via a private network.A gateway vpc endpoint targets ip routes in a prefix list that belong to an aws service. Supported services are: AWS S3 and DynamoDBBelow architecture diagram of a gateway vpc endpoint showcases the implementation:Gateway VPC Endpoints Example:Guide to creating a gateway vpc endpoint.On the vpc dashboard click on the endpoints tab, then create endpoint button on the right.Choose the name of the endpoint being created, For this demo, we use the aws services category,Use the filter:Type = Gatewayfor us to create a gateway vpc endpoint then, choose the service name type to be created then choose your private vpc and which route table for the prefix list entries to be added to.Choose the VPC endpoint policy to be used when accessing the resources and tag your resources for effective cost tracking.Our route table will have new entries added with the prefix list of the service we're connecting to:"}
{"title": "Cracking the AWS Certified Database - Specialty Exam: My Tips and Tricks", "published_at": 1714763862, "tags": [], "user": "Tetiana Mostova", "url": "https://dev.to/aws-builders/cracking-the-aws-certified-database-specialty-exam-my-tips-and-tricks-12b3", "details": "Hey everyone,I recently passed the AWS Certified Database - Specialty (DBS-C01) exam, and I wanted to share my experience and some tips that helped me along the way. If you're planning to take this exam, keep reading!Know the Exam Guide:First things first, make sure you read theAWS CertifiedDatabase - Specialty (DBS-C01) Exam Guide. It tells youexactly what services and topics you need to know beforetaking the exam. Trust me, it's a lifesaver!Use AWS FAQs and Whitepapers:AWS has a ton of FAQs andwhitepapersthat can help youprepare. Focus on the FAQs for the services mentioned in theexam guide. They often have information that can help youanswer exam questions. And don't forget to check out thewhitepapers for a deeper understanding of database servicesand best practices.Practice with Tutorial Dojo:Tutorial Dojois awesome for practice questions. What I loveabout them is that they explain why each answer is right orwrong. They also have cheat sheets and other helpfulresources. Definitely check them out!Use free exam topics questions:Head over toexamtopics.comand join the discussions. Eventhough they might not give you the exact answers, followingother people discussions who have taken the exam can besuper helpful.You can learn from their experiences and get a better ideaof what to focus on.Exam Day Tips:If you're taking the online exam from home, make sure you have a valid ID, a clean desk, and a quiet room. You can only use one screen during the exam.Be ready for a lot of questions onTimeStream DB. Make sure you know this topic well.Expect some tough questions on database migration and design, especially moving from on-premises to the cloud.Don't panic if you come across difficult questions. Remember, there are 15 questions that don't count towards your score, but you won't know which ones they are. Let's hope it's the hardest ones and do your best on each question.Preparing for the AWS Certified Database - Specialty exam takes time and effort, but with the right resources and practice, you can totally do it! Follow the tips I mentioned and use the study materials I suggested. Even if you feel unsure during the exam, trust yourself and keep going. When you pass, you'll feel so proud of yourself, and it will all be worth it. You've got this!Good luck on your exam!"}
{"title": "Your containerized application with IAC on AWS \u2014 Pt.2", "published_at": 1714759728, "tags": [], "user": "Paloma Lataliza", "url": "https://dev.to/aws-builders/your-containerized-application-with-iac-on-aws-pt2-3f5d", "details": "Hi everyone! We\u2019ll see how to create our terraform modules in this blog article. Next, we\u2019ll publish our application to AWS Fargate using the terraform modules we created here and also terragrunt.TERRAFORMWe will establish our directory structure and our terraform module scripts in this blog article. We will set everything up and utilize terraform in conjunction with Terragrunt in part 3.DIRECTORIESOur codes must be organized at the directory level in order to use terraform and terragrunt:app modules     \u251c\u2500\u2500 amazon_vpc     \u251c\u2500\u2500 aws_loadbalancer     \u251c\u2500\u2500 aws_fargate     \u251c\u2500\u2500 aws_roles     \u251c\u2500\u2500 aws_ecs_cluster     \u2514\u2500\u2500 aws_targetgroup     \u2514\u2500\u2500 aws_certificate_managerEnter fullscreen modeExit fullscreen modeterragrunt     \u2514\u2500\u2500 dev         \u2514\u2500\u2500 us-east-1             \u251c\u2500\u2500 aws_ecs             \u2502   \u251c\u2500\u2500 cluster             \u2502   \u2514\u2500\u2500 service             \u251c\u2500\u2500 aws_loadbalancer             \u251c\u2500\u2500 amazon_vpc             \u251c\u2500\u2500 aws_targetgroup             \u251c\u2500\u2500 aws_roles             \u251c\u2500\u2500 aws_certificate_manager             \u2514\u2500\u2500 terragrunt.hclEnter fullscreen modeExit fullscreen modeapp: This is our infrastructure\u2019s primary directory.modules: Each unique AWS resource or service has a subdirectory within this directory. The modules will be inserted here, arranged according to resources like VPC, load balancers, ECS, etc.Terraformsubdirectories: Module-specific Terraform files are located in subdirectories like amazon_vpc and aws_loadbalancer.Terragrunt: Terragrunt configurations are kept in this directory.dev: Stands for the configuration of the development environment.us-east-1: Configurations unique to the AWS region \u201cus-east-1\u201d.Terragrunt subdirectories: Environment- and region-specific options for individual services may be found in the aws_ecs, aws_loadbalancer, amazon_vpc, etc. folders.terragrunt.hcl: This is our Terragrunt configuration file, where we will include backend configurations as well as those that apply to all services in the \u201cus-east-1\u201d area of the development environment. -** Modules have three files**: variables.tf, main.tf, and _outputs.tf in each of the subdirectories. Roles will make use of a _data.tfmain.tf: The main.tf file, which defines and configures AWS resources, is the hub of the module.variables.tf: Allows for module customisation and reuse by defining variables that the module will use._outputs.tf: Indicates which module outputs \u2014 information \u2014 will be accessible to other modules or the Terraform project in its whole._data.tf: To consult and look up information on already-existing resources or services, we shall utilize data.RESOURCESThe following are the AWS resources that we will use:VPCSUBNETSROUTE TABLEINTERNET GATEWAYNAT GATEWAYELASTIC IPECRSECURITY GROUPAPPLICATION LOAD BALANCERFARGATEROUTE53ACMTERRAFORM MODULESVPCLet\u2019s get started with VPC module creation. It will be necessary for each and every one of our apps\u2019 network connections.modules  \u251c\u2500\u2500 amazon_vpcEnter fullscreen modeExit fullscreen modemain.tf// Creat VPC resource \"aws_vpc\" \"vpc\" {   cidr_block           = var.vpc_cidr_block   enable_dns_hostnames = true   enable_dns_support   = true    tags = merge(     {       \"Name\" = \"${var.env}-${var.project_name}-vpc\"     },     var.tags,   ) }   // Creat public subnet1 for VPC resource \"aws_subnet\" \"public_subnet1\" {   vpc_id            = aws_vpc.vpc.id   cidr_block        = var.public_subnet1_cidr_block   availability_zone = var.availability_zone1    tags = merge(     {       \"Name\" = \"${var.env}-${var.project_name}-public-subnet1\"     },     var.tags,   ) }   // Creat public subnet2 for VPC resource \"aws_subnet\" \"public_subnet2\" {   vpc_id            = aws_vpc.vpc.id   cidr_block        = var.public_subnet2_cidr_block   availability_zone = var.availability_zone2    tags = merge(     {       \"Name\" = \"${var.env}-${var.project_name}-public-subnet2\"     },     var.tags,   ) }   // Creat private subnet1 for VPC resource \"aws_subnet\" \"private_subnet1\" {   vpc_id            = aws_vpc.vpc.id   cidr_block        = var.private_subnet1_cidr_block   availability_zone = var.availability_zone1    tags = merge(     {       \"Name\" = \"${var.env}-${var.project_name}-private-subnet1\"     },     var.tags,   ) }   // Creat private subnet2 for VPC resource \"aws_subnet\" \"private_subnet2\" {   vpc_id            = aws_vpc.vpc.id   cidr_block        = var.private_subnet2_cidr_block   availability_zone = var.availability_zone2    tags = merge(     {       \"Name\" = \"${var.env}-${var.project_name}-private-subnet2\"     },     var.tags,   ) }   // Create Internet gateway resource \"aws_internet_gateway\" \"igw\" {   vpc_id = aws_vpc.vpc.id    tags = merge(     {       \"Name\" = \"${var.env}-${var.project_name}\"     },     var.tags,   ) }   // Creat route IGW VPC default rtb resource \"aws_default_route_table\" \"vpc_default_rtb\" {   default_route_table_id = aws_vpc.vpc.default_route_table_id    # Internet gtw route   route {     cidr_block = \"0.0.0.0/0\"     gateway_id = aws_internet_gateway.igw.id   }    tags = merge(     {       \"Name\" = \"${var.env}-${var.project_name}-vpc-default-rtb\"     },     var.tags,   ) }   // Associate a public subnet1 with VPC resource \"aws_route_table_association\" \"public_subnet1_rtb_association\" {   subnet_id      = aws_subnet.public_subnet1.id   route_table_id = aws_default_route_table.vpc_default_rtb.id }  # Associate public subnet2 with VPC resource \"aws_route_table_association\" \"public_subnet2_rtb_association\" {   subnet_id      = aws_subnet.public_subnet2.id   route_table_id = aws_default_route_table.vpc_default_rtb.id }  # Create custom private route table 1  resource \"aws_route_table\" \"private_rtb1\" {   vpc_id = aws_vpc.vpc.id    tags = merge(     {       \"Name\" = \"${var.env}-${var.project_name}-rtb1\"     },     var.tags,   ) }   // Creat custom private route table 2 resource \"aws_route_table\" \"private_rtb2\" {   vpc_id = aws_vpc.vpc.id    tags = merge(     {       \"Name\" = \"${var.env}-${var.project_name}-rtb2\"     },     var.tags,   ) }   // Creat EIP for nat1 resource \"aws_eip\" \"eip1\" {   domain = \"vpc\"    tags = merge(     {       \"Name\" = \"${var.env}-${var.project_name}-eip1\"     },     var.tags,   ) }   // Creat EIP for nat2 resource \"aws_eip\" \"eip2\" {   domain = \"vpc\"    tags = merge(     {       \"Name\" = \"${var.env}-${var.project_name}-eip2\"     },     var.tags,   ) }   // Creat NAT GTW1 resource \"aws_nat_gateway\" \"nat_gtw1\" {   allocation_id = aws_eip.eip1.id   subnet_id     = aws_subnet.public_subnet1.id    tags = merge(     {       \"Name\" = \"${var.env}-${var.project_name}-nat-gtw1\"     },     var.tags,   ) }   // Creat NAT GTW2 resource \"aws_nat_gateway\" \"nat_gtw2\" {   allocation_id = aws_eip.eip2.id   subnet_id     = aws_subnet.public_subnet2.id    tags = merge(     {       \"Name\" = \"${var.env}-${var.project_name}-nat-gtw2\"     },     var.tags,   ) }   // Configure natgtw route private route table 1 resource \"aws_route\" \"private_rtb1_nat_gtw1\" {   route_table_id         = aws_route_table.private_rtb1.id   destination_cidr_block = \"0.0.0.0/0\"   nat_gateway_id         = aws_nat_gateway.nat_gtw1.id }    // Configure nat gtw route private route table 2 resource \"aws_route\" \"private_rtb2_nat_gtw2\" {   route_table_id         = aws_route_table.private_rtb2.id   destination_cidr_block = \"0.0.0.0/0\"   nat_gateway_id         = aws_nat_gateway.nat_gtw2.id }    // Associate private subnet1 VPC resource \"aws_route_table_association\" \"private_subnet1_rtb_association\" {   subnet_id      = aws_subnet.private_subnet1.id   route_table_id = aws_route_table.private_rtb1.id }    // Associate private subnet2 VPC resource \"aws_route_table_association\" \"private_subnet2_rtb_association\" {   subnet_id      = aws_subnet.private_subnet2.id   route_table_id = aws_route_table.private_rtb2.id }    resource \"aws_security_group\" \"default\" {   name        = \"${var.env}-${var.project_name}-sg-vpc\"   description = \"Default security group to allow inbound/outbound from the VPC\"   vpc_id      = \"${aws_vpc.vpc.id}\"    ingress {     from_port = \"0\"     to_port   = \"0\"     protocol  = \"-1\"     self      = true   }    egress {     from_port = \"0\"     to_port   = \"0\"     protocol  = \"-1\"     self      = \"true\"   } }Enter fullscreen modeExit fullscreen modevariables.tfvariable \"vpc_cidr_block\" { }  variable \"public_subnet1_cidr_block\" { }  variable \"public_subnet2_cidr_block\" { }  variable \"private_subnet1_cidr_block\" { }  variable \"private_subnet2_cidr_block\" { }  variable \"availability_zone1\" { }  variable \"availability_zone2\" { }  variable \"project_name\" { }  variable \"env\" { }  variable \"tags\" {   type = map(string) }Enter fullscreen modeExit fullscreen mode_outputs.tfoutput \"vpc_arn\" {   value = aws_vpc.vpc.arn }  output \"vpc_id\" {   value = aws_vpc.vpc.id }  output \"vpc_main_rtb\" {   value = aws_vpc.vpc.main_route_table_id }  output \"vpc_cidr_block\" {   value = aws_vpc.vpc.cidr_block }   output \"public_subnet1_id\" {   value = aws_subnet.public_subnet1.id }  output \"public_subnet1_cidr_block\" {   value = aws_subnet.public_subnet1.cidr_block }  output \"public_subnet1_az\" {   value = aws_subnet.public_subnet1.availability_zone }  output \"public_subnet1_az_id\" {   value = aws_subnet.public_subnet1.availability_zone_id }   output \"public_subnet2_id\" {   value = aws_subnet.public_subnet2.id }  output \"public_subnet2_cidr_block\" {   value = aws_subnet.public_subnet2.cidr_block }  output \"public_subnet2\" {   value = aws_subnet.public_subnet2.availability_zone }  output \"public_subnet2_az_id\" {   value = aws_subnet.public_subnet2.availability_zone_id }  output \"private_subnet1_id\" {   value = aws_subnet.private_subnet1.id }  output \"private_subnet1_cidr_block\" {   value = aws_subnet.private_subnet1.cidr_block }  output \"private_subnet1_az\" {   value = aws_subnet.private_subnet1.availability_zone }  output \"private_subnet1_az_id\" {   value = aws_subnet.private_subnet1.availability_zone_id }  output \"private_subnet2_id\" {   value = aws_subnet.private_subnet2.id }  output \"private_subnet2_cidr_block\" {   value = aws_subnet.private_subnet2.cidr_block }  output \"private_subnet2_az\" {   value = aws_subnet.private_subnet2.availability_zone }  output \"private_subnet2_az_id\" {   value = aws_subnet.public_subnet2.availability_zone_id }  output \"igw_id\" {   value = aws_internet_gateway.igw.id }  output \"default_rtb_id\" {   value = aws_default_route_table.vpc_default_rtb.id }Enter fullscreen modeExit fullscreen modeIAM PERMISSIONSWe need to create permissions for our services.modules  \u251c\u2500\u2500 aws_rolesEnter fullscreen modeExit fullscreen mode_data.tfdata \"aws_iam_policy_document\" \"ecs_service_role\" {   statement {     actions = [       \"application-autoscaling:DeleteScalingPolicy\",       \"application-autoscaling:DeregisterScalableTarget\",       \"application-autoscaling:DescribeScalableTargets\",       \"application-autoscaling:DescribeScalingActivities\",       \"application-autoscaling:DescribeScalingPolicies\",       \"application-autoscaling:PutScalingPolicy\",       \"application-autoscaling:RegisterScalableTarget\",       \"autoscaling:UpdateAutoScalingGroup\",       \"autoscaling:CreateAutoScalingGroup\",       \"autoscaling:CreateLaunchConfiguration\",       \"autoscaling:DeleteAutoScalingGroup\",       \"autoscaling:DeleteLaunchConfiguration\",       \"autoscaling:Describe*\",       \"ec2:CreateNetworkInterface\",       \"ec2:DescribeDhcpOptions\",       \"ec2:DescribeNetworkInterfaces\",       \"ec2:DeleteNetworkInterface\",       \"ec2:DescribeSubnets\",       \"ec2:DescribeSecurityGroups\",       \"ec2:DescribeVpcs\",       \"ec2:AssociateRouteTable\",       \"ec2:AttachInternetGateway\",       \"ec2:AuthorizeSecurityGroupIngress\",       \"ec2:CancelSpotFleetRequests\",       \"ec2:CreateInternetGateway\",       \"ec2:CreateLaunchTemplate\",       \"ec2:CreateRoute\",       \"ec2:CreateRouteTable\",       \"ec2:CreateSecurityGroup\",       \"ec2:CreateSubnet\",       \"ec2:CreateVpc\",       \"ec2:DeleteLaunchTemplate\",       \"ec2:DeleteSubnet\",       \"ec2:DeleteVpc\",       \"ec2:Describe*\",       \"ec2:DetachInternetGateway\",       \"ec2:DisassociateRouteTable\",       \"ec2:ModifySubnetAttribute\",       \"ec2:ModifyVpcAttribute\",       \"ec2:RunInstances\",       \"ec2:RequestSpotFleet\",       \"codebuild:BatchGetBuilds\",       \"codebuild:StartBuild\",       \"s3:GetObject\",       \"s3:GetObjectVersion\",       \"s3:GetBucketVersioning\",       \"s3:PutObject\",       \"s3:PutObjectAcl\",       \"s3:ListBucket\",       \"es:ESHttpPost\",       \"ecr:*\",       \"ecs:*\",       \"ec2:*\",       \"sqs:*\",       \"cloudwatch:*\",       \"logs:*\",       \"iam:PassRole\",       \"elasticloadbalancing:Describe*\",       \"iam:AttachRolePolicy\",       \"iam:CreateRole\",       \"iam:GetPolicy\",       \"iam:GetPolicyVersion\",       \"iam:GetRole\",       \"iam:ListAttachedRolePolicies\",       \"iam:ListRoles\",       \"iam:ListGroups\",       \"iam:ListUsers\",       \"iam:ListInstanceProfiles\",       \"elasticfilesystem:*\",       \"secretsmanager:GetSecretValue\",       \"ssm:GetParameters\",       \"ssm:GetParameter\",       \"ssm:GetParametersByPath\",       \"kms:Decrypt\",       \"dynamodb:GetItem\",       \"dynamodb:PutItem\",       \"dynamodb:UpdateItem\",       \"dynamodb:DeleteItem\",       \"dynamodb:Query\",       \"dynamodb:Scan\",     ]      sid       = \"1\"     effect    = \"Allow\"     resources = [\"*\"]   } }Enter fullscreen modeExit fullscreen modemain.tf// Creat policy resource \"aws_iam_policy\" \"ecs_service_policy\" {   name   = \"${var.env}-${var.project_name}-policy\"   path   = \"/\"   policy = data.aws_iam_policy_document.ecs_service_role.json }  // Creat IAM Role resource \"aws_iam_role\" \"ecs_service_role\" {   name                  = \"${var.env}-${var.project_name}-role\"   force_detach_policies = \"true\"   assume_role_policy = <<EOF {   \"Version\": \"2012-10-17\",   \"Statement\": [     {       \"Sid\": \"1\",       \"Effect\": \"Allow\",       \"Action\": \"sts:AssumeRole\",       \"Principal\": {         \"Service\": [           \"ecs.amazonaws.com\",           \"ecs-tasks.amazonaws.com\",           \"codebuild.amazonaws.com\",           \"codepipeline.amazonaws.com\",           \"ecs.application-autoscaling.amazonaws.com\",           \"ec2.amazonaws.com\",           \"ecr.amazonaws.com\"         ]       }     }   ] } EOF    tags = merge(     {       \"Name\" = \"${var.env}-${var.project_name}\"     },     var.tags,   ) }  resource \"aws_iam_policy_attachment\" \"ecs_service_role_atachment_policy\" {   name       = \"${var.env}-${var.project_name}-policy-attachment\"   roles      = [aws_iam_role.ecs_service_role.name]   policy_arn = aws_iam_policy.ecs_service_policy.arn }Enter fullscreen modeExit fullscreen modevariables.tfvariable \"env\" { }  variable \"project_name\" { }   variable \"tags\" {     type        = map(string)   default     = {} }Enter fullscreen modeExit fullscreen mode_outputs.tfoutput ecs_role_arn {     value = aws_iam_role.ecs_service_role.arn }Enter fullscreen modeExit fullscreen modeAWS CERTIFICATE MANAGERmodules  \u251c\u2500\u2500 aws_certificate_managerEnter fullscreen modeExit fullscreen modeWe will also need a domain already configured in a zone hosted on AWS. With the domain created, we will create a valid TLS certificate within our account.main.tf// creat the certificate resource \"aws_acm_certificate\" \"cert\" {   domain_name       = \"*.${var.domain_name}\"   validation_method = \"DNS\"    tags = merge(     {       \"Name\" = \"${var.env}-${var.project_name}\"     },     var.tags,   )   lifecycle {     create_before_destroy = true   } }   // validation certificate resource \"aws_route53_record\" \"record_certificate_validation\" {   for_each = {     for dvo in aws_acm_certificate.cert.domain_validation_options : dvo.domain_name => {       name   = dvo.resource_record_name       record = dvo.resource_record_value       type   = dvo.resource_record_type     }   }    allow_overwrite = true   name            = each.value.name   records         = [each.value.record]   ttl             = 60   type            = each.value.type   zone_id         = \"Z08676461KWRT5RHNLSKS\" }Enter fullscreen modeExit fullscreen modevariables.tfvariable \"env\" { }  variable \"domain_name\" { }  variable \"project_name\" { }  variable \"tags\" {   type    = map(string)   default = {} }Enter fullscreen modeExit fullscreen mode_outputs.tfoutput \"acm_arn\" {   value = aws_acm_certificate.cert.arn }Enter fullscreen modeExit fullscreen modeAWS LOAD BALANCERHere, we will create an application load balancer that will handle the balancing of our applications.modules  \u251c\u2500\u2500 aws_loadbalancerEnter fullscreen modeExit fullscreen modemain.tf// Creat AWS ALB  resource \"aws_lb\" \"alb\" {   load_balancer_type         = \"application\"   internal                   = var.alb_internal   name                       = \"${var.env}-alb-${var.project_name}\"   subnets                    = [\"${var.subnet_id_1}\", \"${var.subnet_id_2}\"]   drop_invalid_header_fields = var.alb_drop_invalid_header_fields    security_groups = [     aws_security_group.alb.id,   ]    idle_timeout = 400    dynamic \"access_logs\" {     for_each = compact([var.lb_access_logs_bucket])      content {       bucket  = var.lb_access_logs_bucket       prefix  = var.lb_access_logs_prefix       enabled = true     }   }    tags = {     Name = \"${var.env}-alb-${var.project_name}\"   } }    //Creat SG ALB resource \"aws_security_group\" \"alb\" {   name        = \"${var.env}-sg-alb-${var.project_name}\"   description = \"SG for ECS ALB\"   vpc_id      = var.vpc_id    revoke_rules_on_delete = \"true\"    ingress {     description = \"TLS from VPC\"     from_port   = 443     to_port     = 443     protocol    = \"tcp\"     cidr_blocks = [\"0.0.0.0/0\"]    }    ingress {     description = \"HTTP from VPC\"     from_port   = 80     to_port     = 80     protocol    = \"tcp\"     cidr_blocks = [\"0.0.0.0/0\"]    }    egress {     from_port   = 0     to_port     = 0     protocol    = \"-1\"     cidr_blocks = [\"0.0.0.0/0\"]   }     tags = {     Name = \"${var.env}-alb-${var.project_name}\"   } }  //Creat default TG - ALB resource \"aws_alb_target_group\" \"target_group\" {   name        = \"${var.env}-tg-default-alb\"   port        = 80   protocol    = \"HTTP\"   target_type = \"ip\"   vpc_id      = var.vpc_id    lifecycle {     create_before_destroy = true   }    tags = merge(     {       \"Name\" = \"${var.env}-tg-${var.project_name}\"     },     var.tags,   ) }   // Creat HTTPS listener resource \"aws_alb_listener\" \"listener_ssl\" {   load_balancer_arn = aws_lb.alb.arn   port              = \"443\"   protocol          = \"HTTPS\"   ssl_policy        = \"ELBSecurityPolicy-2016-08\"   certificate_arn   = var.certificate_arn    default_action {     target_group_arn = aws_alb_target_group.target_group.arn     type             = \"forward\"   }   depends_on = [     aws_alb_target_group.target_group   ] }   resource \"aws_alb_listener_rule\" \"ssl_listener_rule\" {   action {     target_group_arn = aws_alb_target_group.target_group.arn     type             = \"forward\"   }    condition {     host_header {       values = [\"default.${var.domain_name}\"]     }   }    priority     = var.priority_listener_rule   listener_arn = aws_alb_listener.listener_ssl.arn    depends_on = [     aws_alb_listener.listener_ssl,     aws_alb_target_group.target_group   ] }   // Creat HTTP listener resource \"aws_lb_listener\" \"listener_http\" {   load_balancer_arn = aws_lb.alb.arn   port              = \"80\"   protocol          = \"HTTP\"    default_action {     type = \"redirect\"      redirect {       port        = \"443\"       protocol    = \"HTTPS\"       status_code = \"HTTP_301\"     }   }  }Enter fullscreen modeExit fullscreen modevariables.tfvariable \"alb\" {   default = true }  variable \"alb_http_listener\" {   default = true }  variable \"alb_sg_allow_test_listener\" {   default = true }  variable \"alb_sg_allow_egress_https_world\" {   default = true }  variable \"alb_only\" {   default = false }  variable \"alb_ssl_policy\" {   default = \"ELBSecurityPolicy-2016-08\"   type    = string }  variable \"alb_internal_ssl_policy\" {   default = \"ELBSecurityPolicy-TLS-1-2-Ext-2018-06\"   type    = string }  variable \"alb_drop_invalid_header_fields\" {   default = true   type    = bool }  variable \"lb_access_logs_bucket\" {   type    = string   default = \"\" }  variable \"lb_access_logs_prefix\" {   type    = string   default = \"\" }  variable \"vpc_id\" {   type    = string   default = \"\" }  variable \"subnet_id_1\" {   type    = string   default = \"\" }  variable \"subnet_id_2\" {   type    = string   default = \"\" }  variable \"project_name\" {   type    = string   default = \"\" }  variable \"env\" {   type    = string   default = \"\" }  variable \"alb_internal\" {   type    = bool   default = false }  variable \"certificate_arn\" {   type    = string   default = \"\" }  variable \"tags\" {   type    = map(string)   default = {} }  variable \"priority_listener_rule\" { }  variable \"domain_name\" { }Enter fullscreen modeExit fullscreen modeoutputs.tfoutput \"alb_arn\" {   value = aws_lb.alb.arn }  output \"alb_dns_name\" {   value = aws_lb.alb.dns_name }   output \"alb_secgrp_id\" {   value = aws_security_group.alb.id }   output \"alb_arn_suffix\" {   value = trimspace(regex(\".*loadbalancer/(.*)\", aws_lb.alb.arn)[0]) }  output \"listener_ssl_arn\" {   value = aws_alb_listener.listener_ssl.arn }Enter fullscreen modeExit fullscreen modeAWS TARGET GROUPMoving forward, let\u2019s look at the codes that will comprise our TG.modules  \u251c\u2500\u2500 aws_targetgroupEnter fullscreen modeExit fullscreen modemain.tf//Creat Target Group resource \"aws_alb_target_group\" \"target_group\" {   name        = \"${var.env}-tg-${var.project_name}\"   port        = 80   protocol    = \"HTTP\"   target_type = \"ip\"   vpc_id      = var.vpc_id    health_check {     matcher             = \"200-299\"     path                = var.health_check_path     port                = var.container_port     protocol            = \"HTTP\"     unhealthy_threshold = 8     timeout             = 10   }     lifecycle {     create_before_destroy = true   }    tags = merge(     {       \"Name\" = \"${var.env}-tg-${var.project_name}\"     },     var.tags,   ) }    // Creat HTTPS listener rule resource \"aws_alb_listener_rule\" \"ssl_listener_rule\" {   action {     target_group_arn = aws_alb_target_group.target_group.arn     type             = \"forward\"   }    condition {     host_header {       values = [\"${var.host_headers}\"]     }   }    priority     = var.priority_listener_rule   listener_arn = var.listener_ssl_arn  }Enter fullscreen modeExit fullscreen modevariables.tfvariable \"project_name\" { }  variable \"env\" { }  variable \"certificate_arn\" { }  variable \"tags\" {   description = \"Mapa de tags para serem aplicadas aos recursos.\"   type        = map(string)   default     = {} }  variable \"vpc_id\" { }  variable \"subnet_id_1\" { }  variable \"subnet_id_2\" { }  variable \"listener_ssl_arn\" { }  variable \"priority_listener_rule\" { }  variable \"host_headers\" { }  variable \"health_check_path\" { }  variable \"container_port\" { }Enter fullscreen modeExit fullscreen mode_outputs.tfoutput \"tg_alb_arn\" {   value = aws_alb_target_group.target_group.arn }  output \"tg_arn_suffix\" {   value = regex(\".*:(.*)\", aws_alb_target_group.target_group.arn)[0] }Enter fullscreen modeExit fullscreen modeECS and ECRAll of the container configurations will be made here. We will build an ECS cluster first, and then a fargate service with all the necessary components. To host our application image, we will construct a repository in ECR in addition to the cluster and service.ECS CLUSTER modules  \u251c\u2500\u2500 aws_clusterEnter fullscreen modeExit fullscreen modemain.tf// Creat ECS cluster ECS  resource \"aws_ecs_cluster\" \"ecs\" {   name = \"${var.env}-${var.project_name}\"    setting {     name  = \"containerInsights\"     value = var.container_insights ? \"enabled\" : \"disabled\"   }    lifecycle {     ignore_changes = [       tags     ]   }  }Enter fullscreen modeExit fullscreen modevariables.tfvariable \"project_name\" {   type        = string   default     = \"\" }  variable \"env\" {   type        = string   default     = \"\" }  variable \"container_insights\" {   type        = bool   default     = false }Enter fullscreen modeExit fullscreen mode_outputs.tfoutput \"cluster_name\" {   value = aws_ecs_cluster.ecs.name }  output \"cluster_arn\" {   value = aws_ecs_cluster.ecs.arn }Enter fullscreen modeExit fullscreen modeFARGATEmodules  \u251c\u2500\u2500 aws_fargateEnter fullscreen modeExit fullscreen modemain.tf//Creat ECR repositpry resource \"aws_ecr_repository\" \"ecs_cluster_ecr\" {   name = \"${var.env}-${var.project_name}\"    tags = merge(     {       \"Name\" = \"${var.env}-${var.project_name}\"     },     var.tags,   ) }  //Creat Route53 record  resource \"aws_route53_record\" \"record_sonic\" {   zone_id = \"Z08676461KWRT5RHNLSKS\"   name    = \"${var.host_headers}\"   type    = \"CNAME\"   ttl     = 300   records = [var.alb_dns_name] }  //Creat Task Definition resource \"aws_ecs_task_definition\" \"ecs_task_definition\" {   family = \"${var.env}-task-def-${var.project_name}\"    container_definitions = <<DEFINITION [   {     \"name\":  \"${var.env}-${var.project_name}\" ,     \"image\": \"${var.aws_account_id}.dkr.ecr.${var.region}.amazonaws.com/${var.env}-${var.project_name}:latest\",     \"essential\": true,     \"memoryReservation\": 64,     \"portMappings\": [{       \"containerPort\": ${var.container_port}     }],     \"environment\": [       {         \"name\": \"ENV_PORT\",         \"value\": \"${var.container_port}\"       },       {         \"name\": \"ENVIRONMENT\",         \"value\": \"${var.env}\"       }     ],     \"logConfiguration\": {       \"logDriver\": \"awslogs\",       \"options\": {         \"awslogs-group\": \"ecs-${var.env}-${var.project_name}\",         \"awslogs-region\": \"${var.region}\",         \"awslogs-create-group\": \"true\",         \"awslogs-stream-prefix\": \"${var.env}-${var.project_name}\"       }     }   } ]  DEFINITION     requires_compatibilities = [\"FARGATE\"]   network_mode             = \"awsvpc\"   task_role_arn            = var.ecs_role_arn   execution_role_arn       = var.ecs_role_arn   cpu                      = var.container_vcpu   memory                   = var.container_memory }   //Creat Fargate Service resource \"aws_ecs_service\" \"ecs_service\" {   name            = \"${var.env}-${var.project_name}-service\"   cluster         = \"${var.cluster_arn}\"   task_definition = aws_ecs_task_definition.ecs_task_definition.arn   desired_count   = var.instance_count   launch_type     = \"FARGATE\"    load_balancer {     target_group_arn = var.target_group_arn     container_name   = \"${var.env}-${var.project_name}\"     container_port   = var.container_port   }    network_configuration {     security_groups  = [aws_security_group.sg_ecs.id]     subnets          = [\"${var.subnet_id_1}\", \"${var.subnet_id_2}\"]     assign_public_ip = \"false\"   }    deployment_minimum_healthy_percent = 50   deployment_maximum_percent         = 400    tags = merge(     {       \"Name\" = \"${var.env}-${var.project_name}\"     },     var.tags,   ) }    ///Creat SG to ECS resource \"aws_security_group\" \"sg_ecs\" {   name                   = \"${var.env}-sg-ecs-${var.project_name}\"   description            = \"SG for ECS\"   vpc_id                 = var.vpc_id   revoke_rules_on_delete = \"true\"    egress {     from_port   = 0     to_port     = 0     protocol    = \"-1\"     cidr_blocks = [\"0.0.0.0/0\"]   }    tags = {     Name = \"${var.env}-sg-ecs-${var.project_name}\"   } }  // SG rule ALB resource \"aws_security_group_rule\" \"rule_ecs_alb\" {   description              = \"from ALB\"   type                     = \"ingress\"   from_port                = 0   to_port                  = 0   protocol                 = \"-1\"   security_group_id        = aws_security_group.sg_ecs.id   source_security_group_id = var.sg_alb }  // SG rule ECS resource \"aws_security_group_rule\" \"in_ecs_nodes\" {   description              = \"from ECS\"   type                     = \"ingress\"   from_port                = 0   to_port                  = 0   protocol                 = \"-1\"   security_group_id        = aws_security_group.sg_ecs.id   source_security_group_id = aws_security_group.sg_ecs.id }Enter fullscreen modeExit fullscreen modevariables.tfvariable \"env\" { }  variable \"region\" { }  variable \"project_name\" { }  variable \"container_port\" { }  variable \"instance_count\" { }  variable \"container_vcpu\" { }  variable \"container_memory\" { }  variable \"vpc_id\" { }  variable \"subnet_id_1\" { }  variable \"subnet_id_2\" { }  variable \"aws_account_id\" { }  variable \"tags\" {   type    = map(string)   default = {} }  variable \"ecs_role_arn\" { }  variable \"target_group_arn\" { }  variable \"sg_alb\" { }  variable \"cluster_arn\" { }   variable \"host_headers\" { }  variable \"alb_dns_name\" { }Enter fullscreen modeExit fullscreen mode_outputs.tfoutput \"sg_ecs\" {   value = aws_security_group.sg_ecs.id }  output \"service_name\" {   value = aws_ecs_service.ecs_service.name }Enter fullscreen modeExit fullscreen modeOur modules are ready, and in the next section, we will create the hcl for Terragrunt and also apply our code. See Ya!"}
{"title": "Your containerized application with IAC on AWS \u2014 Pt.1", "published_at": 1714756198, "tags": [], "user": "Paloma Lataliza", "url": "https://dev.to/aws-builders/your-containerized-application-with-iac-on-aws-pt1-npm", "details": "These days, automation and efficiency are critical in the realm of cloud computing. The administration of your containerized apps may be optimized with AWS Fargate and Infrastructure as Code (IaC). This blog article will walk you through the process of utilizing these potent technologies to increase your applications scalability and efficiency.FARGATEAmazon\u2019s container computing service, Fargate, allows you to use AWS containers without the need to manage servers.I have written a blog article regarding Fargate that you may read here.INTRODUCTION TO INFRASTRUCTURE AS CODE (IAC)Infra as code is super important when we think about a stable, scalable and prepared infrastructure for any accident. You may use software development methods like code review and continuous testing, as well as version, reuse and share your configuration by describing your infrastructure in configuration files.I\u2019ve written a blog article about Terraform and Terragrunt, which we\u2019ll be using in this instance.CONNECTING AWS TO YOUR COMPUTERWith the help of the AWS CLI, you can easily control AWS services from the terminal on your PC. It allows you to use scripts to automate operations and manage a variety of AWS services.INSTALLATION OF AWS CLIwindowsGo to theofficial AWS websiteand download the MSI installation.LinuxTo download the installation package, use curl. Apply the subsequent command:curl \"https://awscli.amazonaws.com/awscli-exe-linux-x86_64-2.0.30.zip\" -o \"awscliv2.zip\" unzip awscliv2.zip sudo ./aws/installEnter fullscreen modeExit fullscreen modeTo confirm if it was installed, run the command:aws --versionEnter fullscreen modeExit fullscreen modeMacOsRun the installer after downloading it from this link:https://awscli.amazonaws.com/AWSCLIV2-2.0.30.pkgwhich aws aws --versionEnter fullscreen modeExit fullscreen mode\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026INSTALLATION OF TERRAFORM AND TERRAGRUNTTERRAFORMLinuxFirst, open your terminal. We\u2019ll use wget or curl to download the latest version of Terraform:wget https://releases.hashicorp.com/terraform/1.1.0/terraform_1.1.0Enter fullscreen modeExit fullscreen modeNow, if unzip isn\u2019t already installed, let\u2019s install it before unzipping the file:sudo apt-get install unzip -y unzip terraform_1.1.0_linux_amd64.zipEnter fullscreen modeExit fullscreen modeIn this step, we will move the terraform executable to a directory included in your system PATH, such as /usr/local/bin:sudo mv terraform /usr/local/bin/Enter fullscreen modeExit fullscreen modeFinally, we\u2019ll check if the installation is legitimate:terraform -versionEnter fullscreen modeExit fullscreen modeWindows1.Visit the official Terraform websiteand get the Windows binary that is needed.Use WinZip, WinRAR or another unzip application, to extract the file.Put Terraform in the Path SystemClick \u201cThis Computer\u201d with a right-click, then choose \u201cProperties.\u201dSelect \u2018Environment Variables\u2019 after selecting \u2018Advanced System Settings\u2019.Locate the Path variable under \u2018System Variables\u2019 and modify it by adding the path of the directory where you unpacked Terraform.Use the Command Prompt to Confirm Installation:terraform -versionEnter fullscreen modeExit fullscreen modeMacOsWe are going to start installing Homebrew. If you do not yet have Homebrew, you may install it using this command on the terminal:/bin/bash -c \"$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/HEAD/install.sh)\"Enter fullscreen modeExit fullscreen modeHere, we will install the terraform using Brew. Without a terminal, run:brew tap hashicorp/tap brew install hashicorp/tap/terraformEnter fullscreen modeExit fullscreen modeFinally, we will check whether everything is well with our installation:terraform -versionEnter fullscreen modeExit fullscreen modeTERRGRUNTWindows:You can install Terragrunt on Windows by using Chocolatey :.choco install terragruntMacOS :You can install Terragrunt on Mac by using Homebrew :.brew install terragruntLinux :Most Linux users are able to use Homebrew : brew install terragrunt. Users of Arch Linux can utilize pacman -S terragruntto install it community-terragrunt.FreeBSD :You can install Terragrunt on FreeBSD by using Pkg :.pkg install terragrunt\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026Configure AWS CLIIn this blog article, we will create a new user and configure our credentials. This user here belongs to the adm group, but be careful when it is necessary to give permissions to other people who use your account or who are users of your organization. Adopt the concept of least privilege always.You may read more about IAM on my blog post here.We navigate to IAM -> Users -> Create User. Once the user is created, we will create the credentials. To do this, log in with your credentials and select Security Credentials -> Access Keys -> Create Access Key.Remember to copy the credentials exactly as they were created.After creating the credentials, lets open a terminal and set up the AWS CLI. To do this, type the command below and provide your credentials:aws configureEnter fullscreen modeExit fullscreen modeTo test, simply run a command for any service. Here we will try to list the EC2 instances of the account.aws ec2 describe-instancesEnter fullscreen modeExit fullscreen modeIf it returns anything like the code below, it means that it worked. If there is no instance, the code below will be returned; if there is an instance, the information from the GIF conformation will be returned.{     \"Reservations\": [] }Enter fullscreen modeExit fullscreen mode\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026Now that the requirements are all up and operational, we can use Terraform and Terragrunt to build our infrastructure.In part two of this series, let\u2019s see how. See Ya!"}
{"title": "AWS Amplify: A Game-Changer for Developers", "published_at": 1714755762, "tags": [], "user": "Paloma Lataliza", "url": "https://dev.to/aws-builders/aws-amplify-a-game-changer-for-developers-22l0", "details": "Hi Folks,Today we\u2019re going to talk a little about Amplify. An AWS tool that I\u2019ve been testing for a few days and that has helped me a lot. I\u2019m sure you\u2019ll love this if you build mobile or web apps or just love technology as much as I do. Together, let\u2019s explore the wonders of AWS Amplify and discover why developers are increasingly turning to it as their toolbox of choice.Anyhow, what is Amazon Amplify?Imagine having a toolbox full of everything you need to build, deploy, and maintain your applications. We were able to do this with Amplify and thanks to its seamless interaction with front-end and back-end technologies, it is more than just a platform; It\u2019s a really cool way for people who work as developers to simplify and have an easier time using features like user authentication, data storage, API wizard, and more.Why AWS Amplify RocksSimple Authentication: Are you sick of having to log in and out? With their really unique authentication solutions, which include clever social sign-ins, Amplify has you covered.Data Storage: Akin to Taking a Cloud Walk Data management and storage are a breeze with Amazon S3 and Amazon DynamoDB.Talk Nerdy to Me About API Integration: GraphQL and REST APIs without the headache? Sure, please!Hosting: The Cloud-Based Home of Your Website: Hosting that is scalable, effective, and all around fantastic for your online projects.Be Adaptable, Scale Up: Amplify expands with you, whether it\u2019s a tiny blog or the upcoming popular social media app.Feelings in the Community: Have a query? The community has solutions. It resembles a tech family!Final ThoughtsThere you have it, the full splendor of AWS Amplify. It is simultaneously a tech revolution, a game-changer, and a lifesaver. Regardless of your level of experience as a developer, Amplify is absolutely worth checking out.Although AWS Amplify is fantastic, there is a slight learning curve and you will find that you will need to study a little more to use the features 100%."}
{"title": "AWS Re:Invent 2022", "published_at": 1714755647, "tags": [], "user": "Paloma Lataliza", "url": "https://dev.to/aws-builders/aws-reinvent-2022-1kcc", "details": "O QUE \u00c9 O RE:INVENTTodo ano no final de novembro em Las Vegas, a AWS promove um evento incr\u00edvel, que re\u00fane pessoas do mundo inteiro para anunciar as novas tecnologias ou features AWS, trazer conte\u00fado t\u00e9cnico com v\u00e1rias palestras, workshops, exposi\u00e7\u00e3o de parceiros em tech e muitas sess\u00f5es onde voc\u00ea tem a oportunidade de discutir e aprender muito.As atividades s\u00e3o separadas por n\u00edveis da seguinte forma:level 100 \u2014 Foundational (iniciante)level 200 \u2014 Intermediate (intermedi\u00e1rio)level 300 \u2014 Advanced (Avan\u00e7ado)level 400 \u2014 Expert (Especialista)Voc\u00ea pode participar de duas formas, assistindo o evento de forma online e que \u00e9 gratuito ou comprar o ingresso para participar de forma presencial em Las Vegas. Voc\u00ea pode se inscrever para o desse anoclicando aqui.Geralmente o evento ocorre v\u00e1rios dos grandes hoteis cassinos em Las Vegas simultaneamente, como o The Venetian, Caesars Forum, MGM Grand, Wynn, Encore e Mandalay Bay.MINHA EXPERI\u00caNCIANo ano passado eu tive a oportunidade de estar presente e a experi\u00eancia foi incr\u00edvel. Muito mais do que as sess\u00f5es t\u00e9cnicas, tive a oportunidade de estar lado a lado de grandes pessoas da AWS para retirar d\u00favidas e expor minhas ideias, assim como conheci pessoas fora da curva que vieram de todo canto do mundo o que me proporcionou uma vis\u00e3o macro de como a tecnologia tem influ\u00eanciado os mais diversos pa\u00edses. Posso dizer que o networking \u00e9 um ponto muito forte do evento.O evento \u00e9 divido por sess\u00f5es em v\u00e1rios dos grandes hoteis cassino de Vegas, ent\u00e3o \u00e9 preciso se programar bem para conseguir ver as palestras que gostaria. Os t\u00f3picos que mais me chamaram aten\u00e7\u00e3o foram serverless e containers, e minha grade de talks, workshops, encontros foram voltadas para esse assunto.Al\u00e9m das pessoas, do conhecimento adquirido, tudo foi super divertido, participei de in\u00fameros games envolvendo tecnologia, descobri como funciona o serverless expresso (cafezim nosso de cada dia) e ganhei muitos brindes. Foram tantos brindes que precisei at\u00e9 comprar uma mala nova para trazer tanta coisa.COMUNIDADEMais um ponto forte de estar presente no re:Invent \u00e9 se conectar com as comunidades AWS de todo mundo, abrir portas e abranger ideias que podemos tamb\u00e9m implementar aqui no Brasil. Participei de jantares e encontros voltados para builders e l\u00edderes dos grupos de usu\u00e1rios dos mais diversos pa\u00edses e o intercambio de conhecimento foi sensacional.SE VOC\u00ca PRETENDENTE IR AO RE:INVENT ESSE ANO, SE LIGA NESSAS DICASLas Vegas fica em meio a um deserto, onde a noite faz muito frio, durante o dia tem um sol absurdo e muitas das vezes voc\u00ea vai colocar o casaco pelo frio e logo depois retira-lo pelo calor. Ent\u00e3o considere levar roupas de frio, mas tamb\u00e9m roupas leves.O protetor labial, soro fisiologico, hidratante facil e protetor solar v\u00e3o virar seus melhores amigos. Por causa do tempo seco eu tive muuuuitos problemas com nariz e o soro me ajudou muit, labios e pele extremamente ressacados e muito sol, ent\u00e3o leve esses produtinhos para te ajudar a ter uma experi\u00eancia melhor no evento.Ainda falando sobre conforto, considere seriamente em ter um tenis ou outro cal\u00e7ado super confortavel, pois por ser um evento que acontece em v\u00e1rios locais simultaneamente, acaba que andamos muitooooo e no final do dia, seus pezinho v\u00e3o agradecer muuuito.Considere muito se hospedar em um hotel perto do Venetian, assim voc\u00ea estar\u00e1 relativamente perto dos demais hoteis do evento e os keynotes acontecem nesse hotel, ent\u00e3o facilita muito.Uma dica super importante \u00e9 assim que receber acesso a plataforma para ver as sess\u00f5es do evento, j\u00e1 fa\u00e7a sua grade, pois como o volume de pessoas \u00e9 muito grande, os lugares esgotam muuuito r\u00e1pido.Usar o app Events da AWS para conferir as sess\u00f5es que voc\u00ea marcou, assim como o local delas \u00e9 muito importante, pois assim voc\u00ea consegue seguir seu planejamento sem muito estresse.Considere muito ter espa\u00e7os nas sua agenda para ir a expo do evento e tamb\u00e9m fazer networking, assim voc\u00ea vai ter uma experi\u00eancia mais completaOutra coisa super legal e ter sempre na m\u00e3o o QR do seu likendin ou redes sociais, assim a gente consegue se conectar de forma mais facil com as pessoas que conhecemos.Chegue sempre uns 5 minutos antes na sua sess\u00e3o, o pessoal \u00e9 super pontual e se voc\u00ea se atrasar eles v\u00e3o ceder sua vaga para quem est\u00e1 esperando na fila. Isso tamb\u00e9m \u00e9 uma dica para caso voc\u00ea queira ver uma sess\u00e3o que est\u00e1 com os lugares no app esgotados, chegue antes e fique na fila de espera, caso o pessoal n\u00e3o chegue na hora, voc\u00ea conseguir\u00e1 entrar.Para ir de um hotel ao outro a AWS fornecem os shuttles, que s\u00e3o onibus que nos levam para os hoteis desejados. \u00c9 importante checar qual \u00e9 o ponto de cada shuttle de acordo com o hotel que voc\u00ea pretende ir, as vezes o que tem destino para um hotel determinado fica em um ponto diferente de um outro hotel X.Curta bastante, interaja, n\u00e3o tenha medo de falar. Eu fiquei super t\u00edmida pelo meu ingl\u00eas e vejo que se tivesse me soltado mais nos primeiros dias, minha experi\u00eancia teria sido mais fantastica ainda. Ps: Ningu\u00e9m est\u00e1 ligando se voc\u00ea fala bem ou mal, se erra ou pronuncia errado, o que conta \u00e9 se comunicarNOVIDADES QUE ACHEI SUPER INTERESSANTESAWS Application ComposerO application composer \u00e9 um servi\u00e7o que foi lan\u00e7ado pra ajudar a gente a construir de maneira visual aplica\u00e7\u00f5es serverless. Ele funciona basicamente com a gente escolhendo os servi\u00e7os que nossa aplica\u00e7\u00e3o vai ter e conectando eles. No final ele gera um template do cloudformation de acordo com a arquitetura que criamos manualmente.Amazon CodeWhispererUma outra novidade que me deixou super encantada foi o CodeWhisperer, que \u00e9 um recurso que usa aprendizado de m\u00e1quina e grandes modelos de linguagem para gerar recomenda\u00e7\u00f5es ali no nosso editor de texto com base no c\u00f3digo que a gente j\u00e1 escreveu e tamb\u00e9m nos coment\u00e1rios em linguagem natural. O que \u00e9 super legal tamb\u00e9m \u00e9 que ele analisa nosso codigo j\u00e1 batido e nos fala quais s\u00e3o os servi\u00e7os de nuvem, bibliotecas e frameworks que s\u00e3o mais adequados para o nosso cen\u00e1rio ali.Ele suporta as linguagens Python, Java e JavaScript C# e TypeScript. Voc\u00ea pode usar ele no Visual Studio Code, JetBrains, Cloud9 e LambdaLambda SnapStartFalando sobre serverless, a AWS anunciou um recurso bem legal no lambda que \u00e9 basicamente a redu\u00e7\u00e3o do cold start para fun\u00e7\u00f5es em Java. O snapStart t\u00e1 dispon\u00edvel pro Corretto java 11 runtime e ajuda a reduzir at\u00e9 90% do cold start das fun\u00e7\u00f5es java. Para entender como ele funciona precisamos analisar o clico de vida do lambda que possui 03 est\u00e1gios, a inicializa\u00e7\u00e3o, que \u00e9 onde rola a treta do cold start, a invoca\u00e7\u00e3o da fun\u00e7\u00e3o em si e depois o shutdown.O que rola pra dimunuir o Cold start quando a gente ativa o snapStart, \u00e9 que o c\u00f3digo da fun\u00e7\u00e3o \u00e9 inicializado antes mesmo dela ser invocada, acontece um snapshot imut\u00e1vel e encriptado da mem\u00f3ria e do estado de disco do nosso ambiente de execu\u00e7\u00e3o inicializado e esse snapshot fica cacheado para as pr\u00f3ximas execu\u00e7\u00f5es daquela fun\u00e7\u00e3o. Ent\u00e3o, nas pr\u00f3ximas execu\u00e7\u00f5es daquela fun\u00e7\u00e3o, ela vai criar um ambiente de execu\u00e7\u00e3o novo e esse ambiente vai ser criado a partir do snapshot que t\u00e1 l\u00e1 em cache ao inv\u00e9s de inicializar ele totalmente do zero, que era o que rolava antes e com isso acontece a redu\u00e7\u00e3o dr\u00e1stica do cold start.Ter participado do AWS re: Invent 2022 foi uma experi\u00eancia inigual\u00e1vel. O vento foi fant\u00e1stico e trouxe comigo boas lembran\u00e7as, bastante conhecimento sobre tecnologia e tend\u00eancias da computa\u00e7\u00e3o em nuvem e amizades. E para fechar tudo com chave de ouro teve o re:Play, uma suuuuper festa com m\u00fasica ao vivo, luzes, comida e muita gente animada!"}
{"title": "Voc\u00ea j\u00e1 ouviu falar em Kubernetes?", "published_at": 1714755375, "tags": [], "user": "Paloma Lataliza", "url": "https://dev.to/aws-builders/voce-ja-ouviu-falar-em-kubernetes-df2", "details": "O QUE \u00c9 O KUBERNETESO kubernetes \u00e9 um projeto opensource que foi lan\u00e7ado l\u00e1 em 2014. Ele veio da inspira\u00e7\u00e3o de um sistema que o Google usava pra gerencia seus servi\u00e7os l\u00e1 na nuvem. O mais legal do kubernetes \u00e9 que ele gerencia nossos containers(voc\u00ea pode entender mais sobre containers nesse meu artigo aqui)de forma inteligente e eficiente.Quando a gente usa kubernetes conseguimos gerenciar v\u00e1rios containers ao mesmo tempo e tamb\u00e9m garantir que eles sempre fiquem dispon\u00edveis e escal\u00e1veis. Basicamente, d\u00e1 pra pensar no kubernetes como um sistema de orquestra\u00e7\u00e3o de containers onde ele coordena e automatiza a implanta\u00e7\u00e3o e tudo referente aos containers da nossa aplica\u00e7\u00e3o em um conjunto de m\u00e1quinas chamado cluster. Nessa gerencia, ele distribui os nossos containers pelo cluster de acordo com a demanda, os recursos dispon\u00edveis, monitora todo o funcionamento deles e quando acontece uma falha ali, ele se recuperar.VANTAGENSCorre\u00e7\u00e3o de falhas: O Kubernetes nos ajuda bastante na corre\u00e7ao de falhas de container porque quando ele ve indentifica que tem algum container que tem algum problema ou n\u00e3o t\u00e1 funcionando bem ou n\u00e3o responde a verifica\u00e7\u00e3o de inetgridade que configuramos ele reinicia esses containers, substitui os containers que apresentam problemas, apaga os containers que n\u00e3o est\u00e3o respondemdo a verifica\u00e7\u00e3o de integridade e n\u00e3o os deixam up at\u00e9 que eles fiquem funcionais.Escalabilidade:A escalabilidade \u00e9 bem importante e faz com que a gente consiga alterar automaticamente o numero de containers que est\u00e3o sendo executados com base no uso de CPU e outras metricas. Conseguimos tambem fazer essa escala de forma manual por meio de um comando no CLI e dessa forma, conseguimos alterar o nosso numero de containers que est\u00e3o sendo executados -** Automatiza\u00e7\u00e3o: **A automatiza\u00e7ao tambem \u00e9 bem legal no kubernetes, porque a gente fala como vamos querer que nossos containers sejam, estejam. Se quisermos mudar alguma coisa, ele ajuda a gente com isso e faz isso para de forma controlada. Ele pode, por exemplo, criar novos containers, apagar os antigos e transferir recursos dos antigos para os novos automaticamente.Sistema de armazenamento:O Kubernetes permite montar automaticamente um sistema de armazenamento, como armazenamentos locais, provedores de nuvem p\u00fablica, etc.Aloca\u00e7\u00e3o de containers:Ali no kubernetes, d\u00e1 pra ter um sistema de aloca\u00e7\u00e3o de containers em servidores ou nodes de maneira bem mais eficiente. O objetivo \u00e9 usar ao m\u00e1ximo os recursos dispon\u00edveis (como CPU e mem\u00f3ria) sem desperdi\u00e7ar espa\u00e7o. Por ex: se a gente tem v\u00e1rios containers ali e eles tem requisitos de recursos bem diferentes, o \u201cautomatic bin packing\u201d nos ajudar\u00e1 a garantir que esses containers sejam distribu\u00eddos nos servidores de uma forma que eles aproveite ao m\u00e1ximo os recursos, enquanto atende \u00e0s necessidades de cada containers.Em resumo, o Kubernetes \u00e9 uma ferramenta bem poderosa e vers\u00e1til que nos ajuda na gerencia dos nossos containers. Nos pr\u00f3ximos posts vamos apronfundar em como ele funciona e subir uma aplica\u00e7\u00e3o nele.continua \u2026"}
{"title": "AWS IAM \u2014 Identity and Access Management", "published_at": 1714755252, "tags": [], "user": "Paloma Lataliza", "url": "https://dev.to/aws-builders/aws-iam-identity-and-access-management-8ph", "details": "Quando a gente usa a AWS qualquer coisa que fazemos passa pelo IAM, seja quando criamos uma inst\u00e2ncia EC2, quando adicionamos um objeto no S3 ou quando invokamos um lambda.O IAM ou Identity and Access Management \u00e9 um conceito global de gest\u00e3o de acesso e n\u00e3o foi criado pela AWS, esse acr\u00f4nimo foi adotado pelo provedor de nuvem para um dos servi\u00e7os mais importantes, que a cada segundo recebe mais de 400 milh\u00f5es de chamadas.E o IAM serve para controlar autentica\u00e7\u00e3o e autoriza\u00e7\u00e3o. Autentica\u00e7\u00e3o \u00e9 o processo que a gente se autentica seja com um usu\u00e1rio e senha, com uma chave e logo depois vem a autoriza\u00e7\u00e3o, ent\u00e3o apesar de voc\u00ea estar autenticado, ser\u00e1 que voc\u00ea t\u00e1 autorizado, ou tem aquela determinada permiss\u00e3o?Ele \u00e9 composto por usu\u00e1rios, grupos, pol\u00edticas de permiss\u00f5es, roles, MFA, SSO, etc, sendo uma ferramenta bem completa, onde a AWS investe bastante para que a gente consiga fazer toda a gerencia necess\u00e1ria.Ent\u00e3o quando a gente fala de IAM a gente pode pensar nele como um firewall de gest\u00e3o de acesso, um firewall do presente e por que? Por ser uma ferramenta que ajuda no controle de acessos assim como faz o firewall de rede com determinadas conex\u00f5es.MODELO DE POL\u00cdTICAAs pol\u00edticas do IAM s\u00e3o compostas por alguns elementos e s\u00e3o escritas em Json. Esses elementos que a comp\u00f5em s\u00e3o o version, sid, statement, action, effect, resource, condition.O version \u00e9 uma anota\u00e7\u00e3o da vers\u00e3o que vai definir quais s\u00e3o os elementos que ir\u00e3o compor uma pol\u00edtica, at\u00e9 o momento isso \u00e9 imut\u00e1vel e a AWS n\u00e3o tem uma nova vers\u00e3o ainda, ent\u00e3o ainda sempre vai ser o mesmo.O SID \u00e9 um identificador da pol\u00edtica e a\u00ed voc\u00ea pode dar um nome, ou algo que ajude a entender o que ela faz.O statement \u00e9 um bloco e \u00e9 nele que a gente vai definir o que a nossa pol\u00edtica faz, adicionar as condi\u00e7\u00f5es, quais v\u00e3o ser nossas permiss\u00f5es, a\u00e7\u00f5es que a gente pode tomar e todas as regras que ir\u00e3o comp\u00f4-laO effect dentro de uma pol\u00edtica \u00e9 o switch que diz se voc\u00ea vai ou n\u00e3o permitir aquela determinada a\u00e7\u00e3o, ent\u00e3o ele vai ser deny ou allow e ser\u00e3o somente esses dois valores.O resource \u00e9 o ARN do seu servi\u00e7o AWS, o ARN \u00e9 como se fosse um ID daquele servi\u00e7o, ent\u00e3o por exemplo eu tenho um EC2 e pra eu saber qual \u00e9 um EC2 espec\u00edfico, eu tenho o ARN dele. Ent\u00e3o \u00e9 uma string que a AWS cria pra identificar ali os servi\u00e7os.O condition \u00e9 uma express\u00e3o l\u00f3gica, elementos que v\u00e3o te ajudar a definir o que voc\u00ea vai fazer com aquela permiss\u00e3o ou negativa, ela pode ser um IP, um usu\u00e1rio, uma TAG, uma condi\u00e7\u00e3o no sentido literal.E uma coisa que tamb\u00e9m \u00e9 importante falar \u00e9 que tamb\u00e9m temos a negativa desses elementos, como ao inv\u00e9s de ter um action, teremos um NOTaction, um NOTresource, NOTcondition, e sempre que n\u00e3o for aquele resource, condition ou action voc\u00ea faz o que o effect diz.Ent\u00e3o, sempre que a gente falar em pol\u00edtica do IAM, a gente fala de um documento Json com v\u00e1rios elementos que v\u00e3o negar ou permitir determinadas a\u00e7\u00f5es.E pode parecer intimidador no in\u00edcio e levar ao pensamento de que \u00e9 necess\u00e1rio decorar todos esses elementos, juntos com todas as formas de associar um recurso ou n\u00e3o, mas pelo contr\u00e1rio, voc\u00ea n\u00e3o precisa decorar e sim entender o que s\u00e3o esses elementos e a\u00ed que entra o pulo do gato, usar uma ferramenta da AWS que \u00e9 oAWS Policy Genque ir\u00e1 criar a pol\u00edtica para voc\u00ea.L\u00d3GICA DA AVALIA\u00c7\u00c3O DA POL\u00cdTICA DO IAMAs l\u00f3gicas de avalia\u00e7\u00e3o de pol\u00edtica servem para definir um fluxo que o IAM faz para decidir se voc\u00ea est\u00e1 autorizado ou n\u00e3o a realizar determinada a\u00e7\u00e3o. \u00c9 importante falar do implicit deny e saber que tudo que \u00e9 IAM j\u00e1 come\u00e7a com esse implicit deny, mas o que \u00e9 isso? \u00c9 basicamente falar que tudo no IAM \u00e9 proibido, exceto o que \u00e9 liberado. Para gente entender, vamos pensar no seguinte. Voc\u00ea acessa o console e tenta visualizar as inst\u00e2ncias EC2 que a conta possui. Nesse primeiro momento o AWS IAM assume que voc\u00ea n\u00e3o tem acesso a nada, mas a\u00ed ele corre l\u00e1 e analisa as politicas que o seu usu\u00e1rio possui e se alguma politica disser que voc\u00ea tem acesso a algum ou todos os EC2 da conta, ele permite o seu acesso e caso nenhuma que voc\u00ea possui fale nada sobre inst\u00e2ncias EC2, ele assume o implict deny e voc\u00ea n\u00e3o consegue acessar nada ali do painel de EC2.Ent\u00e3o a l\u00f3gica come\u00e7a assim, voc\u00ea autenticou e pediu acesso ao EC2, O IAM vai assumir um implict deny nesse primeiro momento e depois passa a analisar as politicas que voc\u00ea tem pra ver se alguma te permite acessar esse recurso. Mas a\u00ed, imagina a seguinte situa\u00e7\u00e3o, o IAM est\u00e1 analisando sua politica nessa sequ\u00eancia encontra um deny, na AWS al\u00e9m do implict deny, a gente tem o explicit deny e esse explicit deny \u00e9 quando escrevemos um deny em uma pol\u00edtica e ao contrario do implicit deny, o explicit sobrepoe um allow. Resumindo, primeiro come\u00e7a com tudo negado, se tiver um allow ele permite, mas se tiver um deny escrito, ele nega novamente a sua permiss\u00e3o.ROLESQuando a gente fala sobre roles ou fun\u00e7\u00f5es no IAM, podemos pensar que uma role \u00e9 como um entidade que tem certas permiss\u00f5es definidas e est\u00e3o ali para direcionar as autoriza\u00e7\u00f5es que dizem quais a\u00e7\u00f5es podem ser feitas ali com os recursos.O que \u00e9 bem interessante ao usar as roles \u00e9 que elas n\u00e3o est\u00e3o atribuidas a um usu\u00e1rio ou grupo, como geralmente acontece com as permiss\u00f5es. Ao contr\u00e1rio, um usu\u00e1rio (ou at\u00e9 mesmo um servi\u00e7o da AWS) pode \u201cassumir\u201dessa fun\u00e7\u00e3o quando precisar e depois que terminar o que precisava, \u201cdesassumir\u201d e assim as permiss\u00f5es s\u00e3o retiradas.O uso de roles pode ser v\u00e1lido em situ\u00e7oes assim:Quando um usu\u00e1rio precisa de permiss\u00f5es adicionais por um per\u00edodo limitado.Quando aplica\u00e7oes ou servi\u00e7os da AWS precisam se comunicar com outros recursos da AWS.Para permitir que usu\u00e1rios de contas da AWS que s\u00e3o externas possam acessar os recursos em sua pr\u00f3pria conta da AWS.Isso \u00e9 um jeito bastante seguro e eficaz de conceder permiss\u00f5es tempor\u00e1rios, porque voc\u00ea n\u00e3o precisa mexer nos permiss\u00f5es permanentes do usu\u00e1rio. Al\u00e9m disso, as credenciais que o usu\u00e1rio/recurso recebe quando \u201cassume\u201d uma fun\u00e7\u00e3o s\u00e3o tempor\u00e1rias e s\u00e3o criadas automaticamente pela AWS. Ent\u00e3o, voc\u00ea n\u00e3o precisa se preocupar em gerenci\u00e1-las.MODELO TRADICIONAL RBACO RBAC ou role-based access control \u00e9m\u00e9todo eficiente para determinar as permiss\u00f5es de cada indiv\u00edduo em um sistema. Ele funciona da seguinte forma, voc\u00ea dertemina roles, regras e atacha ao recurso, usu\u00e1rio, grupo que voc\u00ea precisa. Esse modelo \u00e9 mais tradicional, mais comum e mais utilizado no IAM.Em termos pr\u00e1ticos, ao inv\u00e9s de conceder a cada usu\u00e1rio permiss\u00f5es individuais para acessar determinados recursos, o RBAC agrega estas permiss\u00f5es em fun\u00e7\u00f5es. Dessa forma, os usu\u00e1rios s\u00e3o designados para uma ou mais fun\u00e7\u00f5es, e assim, adquirem as permiss\u00f5es vinculadas a essa fun\u00e7\u00e3o.\u00c9 importante que voc\u00ea aplique o conceito do privil\u00e9gio m\u00ednimo ao usar um RBAC para evitar acidentes.MODELO ESCAL\u00c1VEL ABACO modelo ABAC, atribuct-based access control, ou modelo escal\u00e1vel usa uma variedade de atributos, incluindo os atributos do usu\u00e1rio, recurso, ambiente, etc, para decidir quem pode acessar o que.No ABAC, os atributos s\u00e3o essencialmente qualidades ou caracter\u00edsticas que podem ser utilizadas para estabelecer regras de acesso. Por exemplo, os atributos do usu\u00e1rio podem incluir sua fun\u00e7\u00e3o, departamento ou localiza\u00e7\u00e3o geogr\u00e1fica. Os atributos do recurso podem compreender o tipo de recurso (como um arquivo, um equipamento de rede, etc.) e sua classifica\u00e7\u00e3o de seguran\u00e7a. Os atributos do ambiente podem englobar o hor\u00e1rio, a localiza\u00e7\u00e3o ou a seguran\u00e7a da rede.Para entender melhor, vamos usar o seguinte cen\u00e1rio. Imagina s\u00f3 que voc\u00ea tem v\u00e1rios lambdas ali e que voc\u00ea precisa dar permiss\u00e3o para alguns usu\u00e1rios. Se voc\u00ea for usar o modelo RBAC, a cada nova fun\u00e7\u00e3o lambda voc\u00ea precisaria adicionar o ARN na pol\u00edtica e no modelo ABAC, a gente s\u00f3 cria o lambda com uma TAG e ent\u00e3o aqueles usu\u00e1rios que tiverem aquela pol\u00edtica, v\u00e3o conseguir acessar tudo que tiver a TAG que voc\u00ea criou para esses lambdas.E a\u00ed voc\u00ea pode criar a personaliza\u00e7\u00e3o que voc\u00ea necessitar, como TAGS para localiza\u00e7\u00e3o geografica, grupos de usu\u00e1rios, recursos, ambientes, etc.O AWS IAM \u00e9 um servi\u00e7o global que voc\u00ea pode usar para gerenciar o acesso aos servi\u00e7os e recursos da AWS. O acesso pode ser concedido a usu\u00e1rios, grupos e fun\u00e7\u00f5es do IAM usando pol\u00edticas de permiss\u00e3o. Ele \u00e9 uma ferramenta complexa e que precisa de cuidados ao trabalhar, mas muito essencial e importante"}
{"title": "Como \u00e9 o AWS Community Builders", "published_at": 1714755030, "tags": [], "user": "Paloma Lataliza", "url": "https://dev.to/aws-builders/como-e-o-aws-community-builders-598e", "details": "O AWS Community Builders \u00e9 um programa de comunidade super colaborativo que re\u00fane pessoas apaixonadas por tecnologia, nuvem com foco em AWS. O programa incentiva bastante os membros a compartilhar conhecimento, se conectar \u00e0 comunidade; ajuda a melhorar as habilidades t\u00e9cnicas, fazer muito networking e nos d\u00e1 a acesso av\u00e1rios conte\u00fados t\u00e9cnicos de qualidade.O tempo do programa \u00e9 de 1 ano e caso deseje continuar participando, voc\u00ea tem que renovar sua inscri\u00e7\u00e3o ap\u00f3s o per\u00edodo.COMO APLICAR?As inscri\u00e7\u00f5es s\u00e3o abertas duas vezes por ano, os candidatos passam por uma sele\u00e7\u00e3o e depois s\u00e3o informados se foram ou n\u00e3o aprovados naquele semestre. Para se inscrever voc\u00ea pode acessar osite oficialdo programa preencher o formul\u00e1rio. Caso as inscri\u00e7\u00f5es j\u00e1 tenham sido encerradas, voc\u00ea pode adicionar seu nome \u00e0 lista de espera para o pr\u00f3ximo semestre e quando reabrir, voc\u00ea recebar\u00e1 um e-mail avisando a reabertura.O formul\u00e1rio de inscri\u00e7\u00e3o no programa consiste em perguntas como, o que voc\u00ea j\u00e1 fez para trazer mais conhecimento para as pessoas? Qual sua \u00e1rea de interesse em tecnologia \u00e1rea? Por que voc\u00ea deseja se inscrever? Quais s\u00e3o seus interesses na AWS e como voc\u00ea v\u00ea e espera estar daqui algum tempo. Tamb\u00e9m \u00e9 necess\u00e1rio que voc\u00ea envie alguns links de conte\u00fado que tenha criado, como videos no youtube, blogposts, eventos que organizou, palestras que realizou, projetos no github, conte\u00fado que tenha publicado nas m\u00eddias sociais, etc.QUEM PODE SE INSCREVER?Qualquer pessoa pode participar desde que seja maior de idade, seja entusiasta em alguma \u00e1rea de tecnologia , tenha ao menos algum conhecimento sobre cloud/AWS e seja uma pessoa que curte compartilhar conhecimento.CONTE\u00daDOOs n\u00edveis eleg\u00edveis s\u00e3o:N\u00edvel 100 (Iniciante)N\u00edvel 200 (Intermedi\u00e1rio)N\u00edvel 300 (Avan\u00e7ado)N\u00edvel 400 (Expert)Esses s\u00e3o os n\u00edveis do conte\u00fado que voc\u00ea pode criar. Voc\u00ea pode come\u00e7ar a sua jornada de cria\u00e7\u00e3o de conte\u00fado falando para um p\u00fablico mais iniciante (N\u00edvel 100) , como por exemplo: \u201cPrimeiros Passos Para Iniciar na Nuvem\u201d, \u201cO Que \u00e9 AWS\u201d e ir seguindo assim. Mas, a cada ano que se passa e voc\u00ea renova sua inscri\u00e7\u00e3o, \u00e9 esperado que os seus conte\u00fados v\u00e3o indo para um n\u00edvel t\u00e9cnico maior.Ent\u00e3o se voc\u00ea t\u00e1 no terceiro ou quarto ano de programa, n\u00e3o \u00e9 interessante que as suas publica\u00e7\u00f5es sejam do tipo: \u201cO que \u00e9 o S3\u201d ou \u201cO que \u00e9 AWS\u201d.O QUE EU PRECISO FAZER PARA SER ACEITO?N\u00e3o existem regras r\u00edgidas sobre como ser aceito no programa e o primeiro passo para uma inscri\u00e7\u00e3o aprovada \u00e9 responder com calma todas as perguntas do formul\u00e1rio, revisar as regras do programa e tentar deixar as suas respostas o mais completa e clara poss\u00edvel.Como explicado algumas vezes anteriormente, voc\u00ea precisa ter um esp\u00edrito de comunidade, ter um conhecimento t\u00e9cnico na \u00e1rea que desejar entrar e estar sempre criando conte\u00fado. Ent\u00e3o comece participando com mais frequencia das comuinidades da AWS, os AWS User Groups, ajude na organiza\u00e7\u00e3o de eventos, seja palestrante, participe dos foruns, m\u00eddias e tamb\u00e9m dos eventos promovidos pela AWS.Aposte tamb\u00e9m na cria\u00e7\u00e3o de materiais t\u00e9cnicos que possam ajudar as pessoas a entender, ingressar, resolver um problema ou ficar por dentro das tendencias de tecnologia e com foco em AWS. Posso dizer que uma das maiores chaves para ser aceito \u00e9 a dissemina\u00e7\u00e3o do seu material.CATEGORIAS DO AWS COMMUNITY BUILDERCada integrante do AWS CB \u00e9 vinculado a uma categoria atribuida a AWS/cloud computing. Elas s\u00e3o:Cloud OperationsContainersData (Databases, Analytics, BI)DevToolsFront-End Web and Mobile DevelopmentGame TechMachine LearningNetworking and Content DeliverySecurity and IdentityServerlessStorageNa sua inscri\u00e7\u00e3o voc\u00ea pode escolher a que voc\u00ea se sente mais atra\u00eddo/confort\u00e1vel ou que \u00e9 a sua \u00e1rea de estudo.FUI ACEITO, E AGORA?Agora \u00e9 s\u00f3 felicidade, mas brincadeiras a parte, \u00e9 bem interessante participar das call\u2019s promovidas pelos CB\u2019s. Por ser um programa global, voc\u00ea ter\u00e1 contato com pessoas do mundo inteiro, o que \u00e9 uma oportunidade incr\u00edvel de networking e discuss\u00e3o tecnol\u00f3gica.Ent\u00e3o aproveite bastente a intera\u00e7\u00e3o com a comunidade de forma global, fique de olho das possibilidades de mentoria, demonstre vontade de sempre aprender mais e n\u00e3o menos importante, continue criando o seu conteudo e se fortalecendo cada vez mais.**Ps: Por ser uma comunidade global, o idioma que \u00e9 falado \u00e9 o ingl\u00eas.QUAIS AS VANTAGENS DE SER UM AWS COMMUNITY BUILDER?Uma das maiores vantagens do programa \u00e9 ter acesso a pessoas da AWS na sua categoria, isso pode te ajudar muito no crescimento t\u00e9cnico. Acontecem tamb\u00e9m varias talks e eventos exclusivos onde n\u00e3o s\u00f3 os CB\u2019s mas tamb\u00e9m o pessoal da AWS compartilham seus conhecimentos e tamb\u00e9m ensinam como melhorar a sua cria\u00e7\u00e3o de conte\u00fado.Outra vantagem demaaaais \u00e9 que quando est\u00e3o lan\u00e7ando algum beta program a gente tem acesso antecipado, sencional, n\u00e9?A gente tamb\u00e9m ganha alguns brindes incr\u00edveis como, cr\u00e9dito de 500 d\u00f3lares para usar na conta AWS (A cada vez que voc\u00ea renova a sua inscri\u00e7\u00e3o, voc\u00ea ganha os cr\u00e9ditos novamente). Isso \u00e9 muito bom para estudar, testar POC\u2019s e novos servi\u00e7os.Para incentivar nossos estudos, ganhamos acesso \u00e0 CloudAcademy durante o per\u00edodo que estamos no programa e al\u00e9m disso uma certifica\u00e7\u00e3o AWS de qualquer n\u00edvel.O reconhecimento que temos \u00e9 absurdo, isso ajuda muito no meio profissional, al\u00e9m ter a possibilidade ser chamado para palestras em eventos da comunidade e tamb\u00e9m da AWS em si.E por \u00faltimo e n\u00e3o menos importante, temos um baita desconto no re:Invent que pode chegar at\u00e9 50%. E falando em re:Invent, existe a possibilidade (Caso se aplique ao programa All Builders Welcome Grant e seja contemplado) tamb\u00e9m de ganhar uma bolsa para comparecer ao evento em Las Vegas onde todos os custos s\u00e3o pagos pela AWS, passagem, hotel e ingresso.MINHA EXPERI\u00caNCIANa minha primeira inscri\u00e7\u00e3o selecionei como categoria containers e c\u00e1 estou no meu meu terceiro ano de programa com a mesma categoria, onde descobri uma paix\u00e3o. CONTAINERS \u00c9 VIDA! Posso dizer que ser uma AWS Community Builder foi um dos pilares para meu crescimento t\u00e9cnico. Conheci e tive contato com v\u00e1rias pessoas incr\u00edveis, oportunidades que me fizeram evoluir tanto no t\u00e9cnico, quanto em comunidade e tamb\u00e9m pessoal.Falando de mimo, uma coisa bem legal tamb\u00e9m \u00e9 que a cada renova\u00e7\u00e3o eles enviam um SWAG (brinde) adicional para os CB\u2019s, e esses SWAGS s\u00e3o bem maneiros. Se liga s\u00f3:Meu SWAG do primeiro ano:Meu SWAG do segundo ano:Voc\u00ea pode ver o diret\u00f3rio com todos osAWS Community Builders, se conectar com eles e trocar uma ideia bem legal. Meu cora\u00e7\u00e3o ficou quentinho quando vi meu nome pela primeira vez l\u00e1.Olha eu aqui.Se voc\u00ea pretende se aplicar e tiver d\u00favidas, pode me chamar no Twitter que posso te ajudar a revisar sua aplica\u00e7\u00e3o ou dar dicas sobre.\u00c9 isso! Boa sorte!"}
{"title": "Conceitos de Computa\u00e7\u00e3o: IAAS, SAAS, PAAS. Ques bichos s\u00e3o esses?", "published_at": 1714754773, "tags": [], "user": "Paloma Lataliza", "url": "https://dev.to/aws-builders/conceitos-de-computacao-iaas-saas-paas-ques-bichos-sao-esses-4efd", "details": "Docker, containers, conteineriza\u00e7\u00e3o s\u00e3o assuntos muito presentes no dia de quem trabalha com tecnologia atualmente. Nessa s\u00e9rie de blogposts vamos entender o que s\u00e3o essas tecnologias e como us\u00e1-las.O QUE \u00c9 UM CONTAINER?Quando a gente fala em container, sempre vem a imagem daqueles blocos de metal gigantes, que transportam as nossas comprinhas da china ou de outros pa\u00edses pra c\u00e1, mas olha, um container pode ser muito mais que isso.Ent\u00e3o o que \u00e9 um container? Antes de responder essa pergunta \u00e9 necess\u00e1rio entender que quando falamos de container, estamos falando de isolamento. Mas como assim isolamento? Quando subimos um container e esse container est\u00e1 rodando na nossa m\u00e1quina, n\u00e3o existe nenhuma camada de abstra\u00e7\u00e3o gigante como um hypervisor e desta forma \u00e9 como se isolassemos um peda\u00e7o ali da nossa m\u00e1quina e o atribu\u00edsse para nosso container.Com esse isolamento temos processos que s\u00e3o somente desse container, mount points, usu\u00e1rios, redes, etc, e outros container n\u00e3o conseguem enxergar nada que n\u00e3o seja nele pr\u00f3prio. Um ponto legal \u00e9 que conseguimos fazer isolamento de recurso para quando eu quero dedicar um determinado peda\u00e7o do meu cpu, da minha mem\u00f3ria, da taxa de I/O de input/output para um container.Ent\u00e3o um container \u00e9 um bloco isolado, que \u201cvirtualiza\u201d as nossas aplica\u00e7\u00f5es e compartilha o kernel do sistema operacional.E como funciona o container, pensando na nossa aplica\u00e7\u00e3o, seja ela qual for, caso seja necess\u00e1rio alguma bibliotecas, caracter\u00edsticas do sistema operacional, dependencias, o container encapsula tudo para nosso uso e caso a gente precise de mais alguma coisa, ele utiliza o kernel do nosso sistema operacional hospedeiro. Um container n\u00e3o possui um kernel pr\u00f3prio.CONTAINERS x VM\u2019sUma d\u00favida muito comum \u00e9 sobre m\u00e1quinas virtuais e containers. Mas ent\u00e3o, um container \u00e9 uma m\u00e1quina virtual pequena? A resposta \u00e9 n\u00e3o, n\u00e3o tem nada haver com VM, por mais que d\u00ea a entender ou seja um pouqinho semelhantes.A m\u00e1quina virtual emula todo um sistema operacional e muita das vezes com um hardware tamb\u00e9m, j\u00e1 o container n\u00e3o, podemos pensar nele como a nossa aplica\u00e7\u00e3oFERRAMENTAS E COMPONENTES FUNDAMENTAISPara entender mais sobre o funcionamento de containers, precisamos falar um pouco sobre algumas tecnologias, recursos, ferramentas fundamentais no seu uso. Elas s\u00e3o os namespaces, Cgroups e o Union File System.NAMESPACESPara falar sobre container, \u00e9 importante falar sobre namespaces e para entendermos isso melhor, vamos usar uma analogia. Vamos imaginar que voc\u00ea viva com sua fam\u00edlia na sua casa e cada integrante tem o seu pr\u00f3prio quarto. Os namespaces s\u00e3o como os quartos, que s\u00e3o espa\u00e7os separados/isolados para que cada integrante da sua familia possa ter e guardar seus objetos, viver de forma tranquila sem intereferir no espa\u00e7o dos demais. Ent\u00e3o assim, se voc\u00ea precisa de um travesseiro para dormir, voc\u00ea vai at\u00e9 o seu quarto (namespace) para pegar e n\u00e3o no quarto da sua irm\u00e3.Ent\u00e3o, podemos ententer que um namespace \u00e9 uma forma de isolar e separar v\u00e1rios recursos e processos no Linux. E com essa separa\u00e7\u00e3o, os processos em um namespace s\u00e3o conseguem afetar ou acessar diretamente recursos em outro namespace. Quando muitas aplica\u00e7\u00f5es, servi\u00e7os, programa est\u00e3o trabalhando ali ao mesmo tempo os namespaces s\u00e3o super uteis para garantir o isolamento e seguran\u00e7a.No Linux, existem v\u00e1rios tipos de namespace, cada o objetivo de isolar um componente espec\u00edfico:PID (Process ID): N\u00e3o deixa com que processos de um namespace vejam ou interajam com processos de outro namespace.Network: Aqui os recursos de rede como regras de firewall, tabelas de roteamento e interfaces s\u00e3o isolados tamb\u00e9m e assim cada namespace tem uma configura\u00e7\u00e3o de rede personalizada e independente.Mount: O isolamento tamb\u00e9m acontece nos pontos de montagem e assim as modifica\u00e7\u00f5es em um namespace n\u00e3o afetam outros namespaces.User: isola os IDs de usu\u00e1rio e grupo para que os processos dentro de um namespace possam ter seus pr\u00f3prios IDs de usu\u00e1rio e grupo sem influenciar os IDs dentro de outros namespaces.IPC (Inter-process Communication): Nos namespace de comunica\u00e7\u00e3o entre processos isolamento acontece na comunica\u00e7\u00e3o dos processos de namespace. Assim n\u00e3o d\u00e1 pra para que processos em v\u00e1rios namespaces se comuniquem diretamente, isolando os recursos de comunica\u00e7\u00e3o entre processos.UTS (UNIX Time-sharing System): Aqui cada namespace pode ter seu pr\u00f3prio nome de host e dom\u00ednio independente e exclusivo.Esses namespaces s\u00e3o fundamentais para a cria\u00e7\u00e3o de ambientes virtuais e cont\u00eaineres, como Docker e Kubernetes, onde cada cont\u00eainer pode ter seu pr\u00f3prio conjunto isolado de recursos e processos, garantindo que os aplicativos e servi\u00e7os executados em cont\u00eaineres diferentes n\u00e3o interfiram uns nos outros.Em resumo, um namespace no Linux \u00e9 como um \u201cquarto\u201d separado que isola e protege os recursos e processos do sistema, garantindo que cada aplicativo ou servi\u00e7o possa funcionar de forma independente e segura, sem interferir nos outros.CGROUPSOs Cgroups do Linux, uma ferramenta que o kernel tem a seu dispor, possibilitam a regula\u00e7\u00e3o e o controle do uso de recursos do sistema. Quando pensamos nos Cgroups, falamos de coisas como a CPU, a mem\u00f3ria e a entrada/sa\u00edda (I/O), todos vitais para a execu\u00e7\u00e3o de processos ou conjuntos de processos. Com isso, temos uma distribui\u00e7\u00e3o de recursos mais justa entre os processos do sistema operacional, evitando que um \u00fanico processo possa se apropriar de todos os recursos, o que prejudicaria o desempenho de outros programas.Para compreender melhor vamos usar uma analogia, onde consideremos uma situa\u00e7\u00e3o em que voc\u00ea est\u00e1 organizando um evento de tecnologia. Nesse evento, ocorrer\u00e3o diversas palestras sobre t\u00f3picos como computa\u00e7\u00e3o em nuvem, desenvolvimento de software e redes. Cada uma dessas palestras exige recursos espec\u00edficos \u2014 espa\u00e7os adequados, tempo de apresenta\u00e7\u00e3o e equipamentos. Os Cgroups atuam como as normas que voc\u00ea, enquanto organizador, implementa para assegurar que cada apresenta\u00e7\u00e3o obtenha os recursos que necessita sem comprometer as demais.A seguir, veja alguns exemplos de como os Cgroups s\u00e3o implementados:Restri\u00e7\u00e3o da CPU: Vamos usar uma analogia comum para entender isso. Imagine ter um editor de texto e um jogo pesado rodando ao mesmo tempo no seu computador. O jogo, por ser mais pesado, tende a consumir mais recursos, deixando pouco para o editor de texto, que pode ficar lento ou at\u00e9 travar. Como se fosse um monte de crian\u00e7as correndo para pegar o \u00faltimo doce, quem for mais r\u00e1pido leva! Mas usando os Cgroups, podemos equilibrar essa distribui\u00e7\u00e3o de processamento, garantindo que todos os aplicativos funcionem sem problemas.Gerenciamento de Mem\u00f3ria: Da mesma forma que controlamos o poder de processamento, tamb\u00e9m podemos administrar o uso da mem\u00f3ria usando os Cgroups. Aqui usamos Cgroups para garantir que cada aplicativo use apenas o que precisa, evitando que o sistema fique sobrecarregado.Controle de entrada/sa\u00edda (E/S): Cgroups tamb\u00e9m permite regular o uso de dispositivos de armazenamento, como SSDs e HDs e com os Cgroups, n\u00f3s limitamos a taxa de transmiss\u00e3o de dados para que nenhum aplicativo monopolize o acesso aos dispositivos, mantendo o sistema equilibrado e funcionando de maneira eficiente.Desta forma, os Cgroups podem ser resumidos como um recurso extremamente \u00fatil e adapt\u00e1vel, que habilita o controle e a restri\u00e7\u00e3o de uso dos recursos do sistema operacional por processos isolados ou conjuntos de processos. Com isso, se consegue um equil\u00edbrio no desempenho das aplica\u00e7\u00f5es, otimizando a estabilidade e efici\u00eancia do sistema em seu conjunto.UNION FILE SYSTEMO Union File System (UFS) representa uma variedade de sistema de arquivos, que deixa a gente combinar varias camadas de arquivos e diret\u00f3rios em uma visualiza\u00e7\u00e3o unificada. Ele \u00e9 majoritariamente empregado em tecnologias de cont\u00eaineres, como Docker, para a administra\u00e7\u00e3o eficiente e flex\u00edvel das camadas de arquivos de um cont\u00eainer.Ao lidar com cont\u00eaineres, o Union File System possibilita que m\u00faltiplas camadas sejam fundidas em uma \u00fanica imagem de cont\u00eainer. Cada camada pode carregar informa\u00e7\u00f5es espec\u00edficas, como arquivos de configura\u00e7\u00e3o ou bibliotecas de software, e pode ser compartilhado entre cont\u00eaineres distintos para economia de espa\u00e7o e tempo.Resumindo, o Union File System funciona como um manual de receitas multicamadas, onde cada camada equivale a um conjunto de arquivos e diret\u00f3rios. Ele permite a jun\u00e7\u00e3o dessas camadas de maneira eficaz e vers\u00e1til, fornecendo uma visualiza\u00e7\u00e3o unificada dos arquivos e facilitando a administra\u00e7\u00e3o de cont\u00eaineres.BENEFICIOS DE USAR CONTAINERSO uso de containers nas nossas aplica\u00e7\u00f5es no dia a dia deixa tudo mais perform\u00e1tico e com menos gasto de recurso.Um dos maiores pontos ben\u00e9ficos do uso de containers \u00e9 a portabilidade. Se eu uso um container em um servidor e migro ele para um outro com um outro S.O, ele ainda sim vai funcionar de maneira bem perform\u00e1tica. Por ser bem menor e mais simples, \u00e9 bem mais f\u00e1cil de movimentar do que uma VM."}
{"title": "IAC e Terraform. O que \u00e9 e porque s\u00e3o t\u00e3o importantes? Ep.2", "published_at": 1714754285, "tags": [], "user": "Paloma Lataliza", "url": "https://dev.to/aws-builders/iac-e-terraform-o-que-e-e-porque-sao-tao-importantes-ep2-34fj", "details": "Conforme vimos noartigo anterior, utilizar infraestrutura como c\u00f3digo \u00e9 fundamental, principalmente quando estamos trabalho com provedores de nuvem. O terraform veio pra ajudar em v\u00e1rios desafios como vimos anteriormente e sua uiliza\u00e7\u00e3o tem trazido mais beneficios do que ferramentas nativas dos cloud providers.Aqui continuaremos falando sobre o funcionamento e as vantagens do uso do terraform e tamb\u00e9m do terragrunt.TERRAFORMCONTROLE DE ESTADOSAntes a gente usava as GMUD\u2019s para controlar o estado do nosso ambiente e realizar as atualiza\u00e7\u00f5es, agora com o terraform conseguimos controlar os estados da infraestrutura garantindo o estado atual a cada itera\u00e7\u00e3o. Esses estados s\u00e3o basicamente a nossa ger\u00eancia de configura\u00e7\u00e3o.Desta maneira eu posso entender qual \u00e9 o estado do ambiente naquele momento e posteriormente at\u00e9 export\u00e1-lo.PLANEJAMENTONo terraform, antes de realizar qualquer altera\u00e7\u00e3o, a gente pode ter uma no\u00e7\u00e3o de tudo que ser\u00e1 criado em nosso ambiente. Podemos simular como aquele ambiente ficar\u00e1 ap\u00f3s a execu\u00e7\u00e3o, ver tudo que ser\u00e1 criado, o que ser\u00e1 alterado e removido antes de subir. E dessa maneira temos todo o controle de modifica\u00e7\u00f5es antes de escolher se vamos ou n\u00e3o aplicar esses updates.ABSTRA\u00c7\u00c3OCom o uso do terraform a gente consegue ter uma abstra\u00e7\u00e3o em alto n\u00edvel. Ao inv\u00e9s de ter um c\u00f3digo com valores est\u00e1tico, posso ter variaveis para alterar aquele valor de acordo com a minha necessidade e sem precisar de me preocupar em fazer essa modifica\u00e7\u00e3o em v\u00e1rios lugares.Por exemplo, a gerencia de v\u00e1rios ECS na AWS e neste cen\u00e1rio vamos supor que precisamos trocar o valor da mem\u00f3ria de duas tasks definitions. Aqui poder\u00edamos ter uma vari\u00e1vel chamadataskdef_memoriapara a quantidade de mem\u00f3ria das minhas tasks definitions e sempre que eu quero mudar esse valor, eu preciso alterar somente nessa vari\u00e1vel e n\u00e3o nos v\u00e1rios lugares em que esse recurso poderia estar. Assim o controle fica mais f\u00e1cilMODULARIZA\u00c7\u00c3OUm dos maiores benef\u00edcios do \u00e9 a modulariza\u00e7\u00e3o. A modulariza\u00e7\u00e3o \u00e9 muito importante pra evitar que a gente tenha uma repeti\u00e7\u00e3o de c\u00f3digo muito grande. Ou seja, eu posso criar um m\u00f3dulo generico para um recurso e utilizar esse m\u00f3dulo para criar esse recurso para diferentes ocasi\u00f5es. Por exemplo, pegando o caso da cria\u00e7\u00e3o de um EC2, eu posso usar esse m\u00f3dulo padr\u00e3o para subir v\u00e1rios EC2 diferentes, em contas diferentes, mudando somente as vari\u00e1veis que v\u00e3o personalizar essa inst\u00e2ncia. Vale lembrar que o terraform faz a organiza\u00e7\u00e3o de m\u00f3dulo a n\u00edvel de diret\u00f3rioE a ideia de modularizar, al\u00e9m de evitar a repeti\u00e7\u00e3o de c\u00f3digo, \u00e9 desacoplar os recursos e trabalhar com eles separadamente. Voltando ali no caso do EC2, para que esse servi\u00e7o funcione na AWS ele precisa tamb\u00e9m de outros recurso como rede e seguran\u00e7a e para criar a parte de rede eu posso ter um m\u00f3dulo de VPC (servi\u00e7o de rede da AWS) em um local diferente de onde est\u00e1 meu m\u00f3dulo de EC2.Dessa forma tudo fica mais simples e ajuda a n\u00e3o ter interfer\u00eancias quando precisamos ter atualiza\u00e7\u00f5es paralelas, porque se algu\u00e9m est\u00e1 modificando o m\u00f3dulo de VPC o de EC2 n\u00e3o ser\u00e1 afetado e isso garante tamb\u00e9m a confiabilidade.CRIA\u00c7\u00c3O DE NOVOS M\u00d3DULOS E PROVEDORESPor ser open source, d\u00e1 super para fazer gerenciamento por API o que faz com que tenhamos a oportunidade de criar um provedor novo ou um m\u00f3dulo novo. Os providers grandes de nuvem investem muito nisso e a gente consegue ver pessoas dessas nuvens trabalhando fortemente nesse desenvolvimento. Em 2021 mesmo, l\u00e1 no re:Invent a AWS anunciou um novo m\u00f3dulo do terraform, o AFT que permite provisionar e personalizar contas AWS por meio do Terraform usando um pipeline de implanta\u00e7\u00e3o.ORGANIZA\u00c7\u00c3OQuando usamos o terraform, podemos organizar nossos arquivos e c\u00f3digos em duas maneiras. A primeira \u00e9 tudo junto em um diret\u00f3rio s\u00f3 e a segunda \u00e9 em pastas separadas.Na primeira abordagem podemos criar umarquivo para cada recurso dentro uma pasta e nessa organiza\u00e7\u00e3o a declara\u00e7\u00e3o das vari\u00e1veis \u00e9 feita em um lugar s\u00f3, deixando a compreens\u00e3o mais f\u00e1cil e a repita\u00e7\u00e3o de c\u00f3digo menor, mas por outro lado quando quando precisarmos fazer uma atualiza\u00e7\u00e3o, toda a infraestrutura ser\u00e1 validada novamente. Ele vai checar se tem alguma atualiza\u00e7\u00e3o em todos os recursos dentro dessa pasta e se estiver alguma coisa n\u00e3o terminada pode quebrar o ambiente, assim como a aplica\u00e7\u00e3o da sua atualiza\u00e7\u00e3o pode ser muito demorada por precisar conferir recurso por recurso.A recomenda\u00e7\u00e3o para uso desse tipo de organiza\u00e7\u00e3o \u00e9 quando a equipe \u00e9 pequena, os recursos a serem criados n\u00e3o s\u00e3o muitos ou voc\u00ea ainda n\u00e3o tem a real no\u00e7\u00e3o da infra solicitada.Na segunda abordagem, a organiza\u00e7\u00e3o acontece a n\u00edvel de diret\u00f3rio e dessa forma conseguimos criar m\u00f3dulos e definir qual ser\u00e1 a ordem de execu\u00e7\u00e3o a partir de um depends_on. Aqui fica mais f\u00e1cil de trabalhar e n\u00e3o tem o problema citado anteriormente da atualiza\u00e7\u00e3o de todos os recursos, nos diferentes arquivos, de uma s\u00f3 vez.Nesse tipo de organiza\u00e7\u00e3o \u00e9 onde acontece a modulariza\u00e7\u00e3o, por mais que eu tenha que declarar variaveis em todos os modulos, o que pode causar um pouco de duplicidade caso existam variaveis dependentes, o reuso de c\u00f3digo \u00e9 bem poss\u00edvel. Eu posso chamar esse modulo em um lugar e alterar somente o valor das var\u00edaveis de acordo com a minha configura\u00e7\u00e3o.TERRAFORM WORKSPACESAgora que entendemos como funciona o terraform, precisamos pensar tamb\u00e9m em uma forma de trabalhar com nossos m\u00f3dulos, arquivos em v\u00e1rios ambientes, como desenvolvimento(DEV), homologa\u00e7\u00e3o(HML) e produ\u00e7\u00e3o(PRD).Quando temos v\u00e1rios ambientes para trabalhar e precisamos usar o mesmo c\u00f3digo, o terraform oferece um recurso chamado workspaces que basicamente cria ambientes separados, mas identicos e separa os states de cada um deles.O uso do workspaces come\u00e7a a ficar complicado quando temos um ambiente significativamente diferente dos demais, ent\u00e3o por exemplo, se meu ambiente de HML \u00e9 bem diferente de PRD e o pior, est\u00e3o em contas diferentes, isso come\u00e7a a dar mais trabalho e precisar\u00edamos come\u00e7ar a fazer algumas gambiarras para tentar contornar o problema, como criar v\u00e1rios arquivos de configura\u00e7\u00e3o backend dentro de um c\u00f3digo e ter que inicializar o terraform de acordo com o arquivo backend do ambiente a cada update.Al\u00e9m desse problema, caso tenha algum recurso compartilhado a complexidade de gerencia aumenta, pois os estados tamb\u00e9m v\u00e3o ter que ser compartilhados entre as workspaces.E para mim, o maior problema do workspace \u00e9 que n\u00e3o tem nada que avise em qual workspace voc\u00ea est\u00e1 e se por algum acaso voc\u00ea esquecer de trocar na hora na execu\u00e7\u00e3o, voc\u00ea vai subir o ambiente errado sem perceber.Desses problemas nasceu o terragrunt.TERRAGRUNTO terragrunt \u00e9 uma ferramenta usada para orquestrar a execu\u00e7\u00e3o do terraform. Ele trabalha a n\u00edvel de diret\u00f3rio tamb\u00e9m, ent\u00e3o voc\u00ea pode ter uma pasta para seu ambiente de DEV, um para HML e ou para PRD o que deixa mais facil a visualiza\u00e7\u00e3o e a evitar erros de aplica\u00e7\u00e3o por ambientes errados, como pode acontecer no workspaces.O legal do terragrunt \u00e9 a organiza\u00e7\u00e3o da execu\u00e7\u00e3o de acordo com as dependencias que temos, ent\u00e3o assim, se a gente tem in\u00fameros recursos que dependem um do outro, como um EC2 que depende de uma VPC, o terragrunt nos ajuda a gerenciar as dependencias pra fazer com que a VPC seja criada antes do EC2.Com o terragrunt \u00e9 poss\u00edvel fazer a comunica\u00e7\u00e3o de v\u00e1rios provedores de uma s\u00f3 vez. Ent\u00e3o se eu tenho uma aplica\u00e7\u00e3o que usa containers e um banco de dados, onde os servi\u00e7os de containers est\u00e3o na AWS e o banco de dados est\u00e3o na GCP, eu n\u00e3o preciso me preocupar em fazer a segrega\u00e7\u00e3o por providers, posso manter tudo no mesmo repositorio pois ele consegue comunicar com os dois ao mesmo tempo mantendo a hierarquia de pastas.Um outro ponto \u00e9 que ele pode trabalhar com diferentes vers\u00f5es dos nossos modulos terraform. Se criar uma vers\u00e3o nova tagueando-a, \u00e9 s\u00f3 informar na url de chamada do modulo essa vers\u00e3o e o terragunt vai trabalhar em cima dela.CONCLUS\u00c3OEnt\u00e3o o que usar e quando usar? Pra responder essa pergunta voc\u00ea precisa analisar o seu cen\u00e1rio. O uso do terraform \u00e9 importante para ajudar no controle e gerenciamento da sua infraestrutura, assim como para ajud\u00e1-lo a subir todo seu ambiente de forma r\u00e1pida cada aconte\u00e7a algum incidente.Se sua equipe for grande, com um projeto robusto o terraform com terragrunt \u00e9 uma boa op\u00e7\u00e3o. Caso seja somente para subir poucos recursos que n\u00e3o tenham criticidade alta, o terraform d\u00e1 conta do recado. E se a sua ideia for validar um ambiente antes de subir, criando uma copia testando e depois destruindo, o workspaces pode te ajudar.REFERENCIASEsse blogpost foi inspirado em uma talk da Camilla Gomes no DevOpsDays VIX 2022."}
{"title": "IAC e Terraform. O que \u00e9 e porque s\u00e3o t\u00e3o importantes? Ep.1", "published_at": 1714754125, "tags": [], "user": "Paloma Lataliza", "url": "https://dev.to/aws-builders/iac-e-terraform-o-que-e-e-porque-sao-tao-importantes-ep1-6cp", "details": "Para falar de IAC ou infraestrutura como c\u00f3digo, primeiramente precisamos abordar alguns assuntos e conceitos para entendermos a sua real necessidade. Vamos come\u00e7ar com a ger\u00eancia de configura\u00e7\u00e3o e o movimento DevOps.GER\u00caNCIA DE CONFIGURA\u00c7\u00c3O E MOVIMENTO DEVOPSExiste um conceito super importante que vem l\u00e1 do ITIL (Information Technology Infrastructure Library) que ajuda a gente entender a import\u00e2ncia da utiliza\u00e7\u00e3o de infra como c\u00f3digo, a ger\u00eancia de configura\u00e7\u00e3o. A ger\u00eancia de configura\u00e7\u00e3o pode ser resumida em um conjunto de atividades que viabiliza o controle das mudan\u00e7as que acontecem nas diferentes etapas do nosso software.E com essa pr\u00e1tica a gente consegue identificar e documentar as caracter\u00edsticas do nosso software, controlar as mudan\u00e7as que v\u00e3o ocorrer nele, registrar o estado em que ele se encontra, verificar a conformidade de acordo com os requisitos, e com isso a gente tem uma seguran\u00e7a no sentido da quebra do software.H\u00e1 um tempo atr\u00e1s era essencial o preenchimento de uma GMUD (Um documento de gest\u00e3o de mudan\u00e7as) e isso era simple quando t\u00ednhamos ali um data center e esse data center possuia poucos servidores. Neste cen\u00e1rio o controle era f\u00e1cil e as GMUD\u2019s eram mais efici\u00eantes, mas quando temos uma escala grande de m\u00e1quinas, com mais de 100 servidores, fica invi\u00e1vel, pois ao todo nesses servidores podemos ter uma quantidade de servi\u00e7os absurda e controle fica super burocr\u00e1tico.Pra ajudar a contornar essa burocracia o movimento DevOps veio para aproximar os precessos do pessoal de OPS com o pessoal de DEV e embora cada um tenha suas pr\u00e1ticas, a necessidadade dessa integra\u00e7\u00e3o trouxe atualiza\u00e7\u00f5es na maneira de trabalhar, com versionamento de c\u00f3digo, automa\u00e7\u00f5es e entrega cont\u00ednua. Com essas atualiza\u00e7\u00f5es e a melhora na comunica\u00e7\u00e3o das equipes, a nuvem trouxe fortemenete a id\u00e9ia da infraestrutura como c\u00f3digo.IACE o que \u00e9 IAC? IAC como dito anteriormente \u00e9 a pr\u00e1tica de usar c\u00f3digo para criar sua infraestrutura em cloud. Antes a gente abria o nosso console do provedor de nuvem e iamos clicando ou selecionando nos servi\u00e7os e preenchendo os par\u00e2metros para o funcionamento e agora, as boas pr\u00e1tica recomenda o uso de infra as code.Mas porque fazer um c\u00f3digo para criar um servi\u00e7o, sendo que usar o console \u00e9 mais f\u00e1cil?Porque tudo que \u00e9 manual muitas das vezes n\u00e3o \u00e9 poss\u00edvel replicar. E quando a equipe que trabalha com essa nuvem \u00e9 grande, gerenciar e controlar tudo o que foi criado fica mais complexo.E as vantagens de usar IAC n\u00e3o s\u00e3o somente o controle e gerenciamento, podemos contar com o reuso do c\u00f3digo, al\u00e9m da facilidade de criar e destruir ambientes o que nos ajuda quando apagamos algo por acidente ou quando ocorre um incidente como um ataque.FERRAMENTAS IACE quando a gente fala de IAC, pensamos logo em algumas ferramentas que podem nos auxiliar nesta jornada. Cada ferramenta possui suas particularidade e s\u00e3o designadas para cada caso de uso espec\u00edfico e podem ser utilizadas em conjunto. As mais conhecidas s\u00e3o o ansible, chef, terraform e puppet.ANSIBLEO Ansible \u00e9 uma solu\u00e7\u00e3o de gerenciamento de configura\u00e7\u00e3o e automa\u00e7\u00e3o. Com ele a gente pode projetar e gerenciar o estado que queremos de nosso sistema usando scripts YAML. A simplicidade do Ansible e a falta de necessidade de instalar software adicional no destino s\u00e3o dois de seus benef\u00edcios. Uma coisa bem legal do Ansible \u00e9 o modelo sem agente que depende s\u00f3 da comunica\u00e7\u00e3o SSH para os n\u00f3s gerenciados. Para usar a gente basicamente escreve um script que chamamos de playbook e nele definimos o que deve ser instalado e configurado.CHEFO Chef \u00e9 uma ferramenta que tem um proposito super parecido com o do Ansible, mas ao iv\u00e9ns de usar YAML, a gente tem o Domain Specific Language (DSL) escrita em Ruby. No Chef existe a necessidade da instala\u00e7\u00e3o de um agente no servidor de destino, o que n\u00e3o acontece no Ansible.PUPPETO Puppet \u00e9 uma ferramenta super similiar ao Chef, e as 03 possuem o mesmo prop\u00f3sito de gerenciamento de configura\u00e7\u00e3o e defini\u00e7\u00e3o do estado desejado do sistema. Assim como o Chef, o Puppet usa sua pr\u00f3pria linguagem de dom\u00ednio espec\u00edfico (DSL).CASO DE USO DESSAS FERRAMENTASVamos pegar um cen\u00e1rio onde temos 10 servidores e queremos instalar um editor de texto neles. Em vez de instalar esse editor de texto manualmente em cada servidor e seguir os procedimentos necess\u00e1rios para o funcionamento dele, voc\u00ea pode usar uma dessas ferramentas para fazer a instala\u00e7\u00e3o e configura\u00e7\u00e3o em todos os 10.TERRAFORMAntes da gente falar sobre o terraform, vamos a uma curiosidade. O termo terraform vem de terraformar no latim, onde \u00e9 terra \u00e9 a terra mesmo e formar \u00e9 moldar, criar. Geralmente \u00e9 usado na fic\u00e7\u00e3o cientifica pra quando cria-se um planeta e o mesmo \u00e9 habit\u00e1vel como a terra.E a ideia do terraform \u00e9 meio que isso tamb\u00e9m. Para usa-lo precisamos nos conectar a um provedor de nuvem e come\u00e7ar a criar os nossos recursos, a nossa infraestrutura (planetas).Ent\u00e3o, podemos falar que o terraform \u00e9 uma ferramenta de IAC usada para criar a nossa infraestrutura, ao contr\u00e1rio das ferramentas que citei anteriormente que s\u00e3o focadas mais no servidor pra dentro. O que atrai muita gente para usar o terraform \u00e9 sintaxe facil, uma comunidade bem ativa(open source) e a documenta\u00e7\u00e3o dele bem completa.Porque usar terraform e n\u00e3o uma ferramenta nativa?Muitas pessoas quando v\u00e3o come\u00e7ar usar IAC ficam na d\u00favida se come\u00e7am a usar o terraform ou a ferramenta nativa do provedor de nuvem, e a resposta pra essa d\u00favida pode ser esclarecida de acordo com o seu cen\u00e1rio. Mas olha, a utiliza\u00e7\u00e3o do terraform comparado ao uso do cloudformation da AWS pode ser bem mais ben\u00e9fica, visto que o terraform tem uma curva de aprendizado menor, suporta mais de 250 provedores atualmente. Com isso, conseguimos trabalhar com mais de um provedor usando a mesma ferramenta e acaba que n\u00e3o preciso ficar aprendendo uma ferramenta nova toda vez que vou mudar de cloud, preciso somente ver quais ser\u00e3o as particularidades de chamadas daquela outra cloud.Desta forma a gente ganha tempo, podendo nos aprofundar em uma ferramenta s\u00f3 e o gerenciamento fica bem mais f\u00e1cil.Continua \u2026"}
{"title": "Desmistificando o AWS LoadBalancer", "published_at": 1714753931, "tags": [], "user": "Paloma Lataliza", "url": "https://dev.to/aws-builders/desmistificando-o-aws-loadbalancer-k16", "details": "Voc\u00ea j\u00e1 quebrou a cabe\u00e7a para compreender a ideia do que \u00e9 balanceamento de carga da AWS? Se sim, relaxe! Voc\u00ea n\u00e3o est\u00e1 sozinho, isso aconteceu comigo no passado e agora vou te ajudar a entender o que \u00e9 o AWS LB.LOAD BALANCER: O QUE \u00c9 ISSO?Imagine que voc\u00ea foi em um supermercado fazer suas compras do m\u00eas e quando voc\u00ea chega para pagar, repara que tem v\u00e1rios caixas para fazer o seu pagamento. Quando as filas come\u00e7am a se formar, um funcion\u00e1rio do supermercado orienta os clientes at\u00e9 os caixas dispon\u00edveis para garantir um atendimento r\u00e1pido e eficaz a todos. O load balancer funciona da mesma forma que os checkouts e pagamentos de compras em um caixa de supermercado, ao inv\u00e9s de caixas e clientes a gente tem solicita\u00e7\u00f5es e servidores.Os clientes s\u00e3o como as requisi\u00e7\u00f5es que s\u00e3o enviadas, o supermercado \u00e9 como nossa aplica\u00e7\u00e3o e os caixas s\u00e3o como os servidores que processam as nossas requisi\u00e7\u00f5es. Se o fluxo de clientes (requisi\u00e7\u00f5es) n\u00e3o for bem distribu\u00eddo, alguns caixas podem ficar completamente sobrecarregados, enquanto outros podem ficar parados, o que deixaria v\u00e1rios clientes nervososos.De forma mais t\u00e9cnica, o balanceamento de carga (load balancer) distribui o tr\u00e1fego de rede entre v\u00e1rios servidores, garantindo uma distribui\u00e7\u00e3o eficaz de tarefas e evitando servidores sobrecarregados. O objetivo \u00e9 evitar gargalos, distribuir a carga de trabalho e, assim, aumentar a disponibilidade e confiabilidade do servi\u00e7o.AWS LOAD BALANCERVoltando ali na nossa hist\u00f3ria do supermercado, o AWS Load Balancer opera de forma semelhante a um gerente de supermercado, identifica o gargalo e atribui fun\u00e7\u00f5es aos caixas de acordo com a quantidade de produtos e clientes que consegue atender e seu n\u00edvel de disponibilidade.Agora que entendemos o que \u00e9 o LB, podemos citar alguns servi\u00e7os onde o mesmo \u00e9 respons\u00e1vel por toda essa distribui\u00e7\u00e3o de tr\u00e1fego. Vale lembrar que ele \u00e9 compat\u00edvel e trabalha com diversos servi\u00e7os e que vou falar somente de alguns aqui:EC2: Nas instancias do EC2, o AWS LB consegue distribuir o tr\u00e1fego que chega entre v\u00e1rias, uma ou mais AZ\u2019s.ECS: Quando a gente fala sobre o ECS, o load balancer tem um papel super importante na distribui\u00e7\u00e3o o tr\u00e1fego entre os containers desse servi\u00e7o, o que facilita o desenvolvimento e a execu\u00e7\u00e3o das nossas aplica\u00e7\u00f5es que s\u00e3o baseadas em micro-servi\u00e7os.EKS: Falando de EKS o AWS load balancer funciona de forma super similar ao que acontece no ECS, o que muda mesmo \u00e9 s\u00f3 o destino que s\u00e3o os containers do kubernetes.Dentro da nuvem AWS o load balancer \u00e9 divido entre 04 tipos e devemos nos atentar quando formos usar, pois cada um tem um uso e necessidades diferentes.Os tipos s\u00e3o:Application Load Balancer (ALB)Network Load Balancer (NLB)Classic Load Balancer (CLB)Gateway Load Balancer (GWLB)APPLICATION LOAD BALANCER (ALB)O Application Load Balancer \u00e9 um dos mais utilizados atualmente, pois trabalha ali no de aplica\u00e7\u00e3o, sendo exclusivo para tr\u00e1fego HTTP e HTTPS. Al\u00e9m de balancear a carga, ele examina o conte\u00fado das solicita\u00e7\u00f5es para encaminh\u00e1-las aos servidores corretos. Isso faz com que possamos ter o uso de roteamento baseado em conte\u00fado, permitindo a escolha de servidores com base na natureza da solicita\u00e7\u00e3o e outros fatores.Exemplo:Imagine que voc\u00ea tenha uma aplica\u00e7\u00e3o que oferece voc\u00ea tem diversos servi\u00e7os, como um chat para o pessoal conversar sobre seus posts, uma loja para vender seus produtos e um blog somente de videos. Usando nesse cen\u00e1rio o ALB, consiguimos fazer a distribui\u00e7\u00e3o da nossa carga de trabalho, otimizando-a e assim, consequentemente deixando a experi\u00eancia do usu\u00e1rio da aplica\u00e7\u00e3o melhor. O que acontece \u00e9 que quando algu\u00e9m entra em algum desses servi\u00e7os da sua aplica\u00e7\u00e3o, as requisi\u00e7\u00f5es s\u00e3o enviadas s\u00e3o direcionadas acordo com o servi\u00e7o acessado. Ent\u00e3o, desta forma o ALB envia as requisi\u00e7\u00f5es do blog para um conjunto de servidores X, as loja para um Y e do chat para um conjuntos de servidores Z.NETWORK LOAD BALANCER (NLB)Ao contr\u00e1rio de ALB, o NLB n\u00e3o lida com tr\u00e1fego que responde a protocolos de n\u00edveis mais alto como o HTTP e o HTTPS, ele trabalha ali a camada de conex\u00e3o de transporte do modelo OSI, tendo atua\u00e7\u00e3o nos protocolos TCP/UDP. Quando pensamos em um tr\u00e1fego mais alto a melhor op\u00e7\u00e3o seria esse tipo de balanceador de carga, pois consegue lidar com um fluxo alto e traz um lat\u00eancia extremamente baixa. Os maiores usos do NLB s\u00e3o aplica\u00e7\u00f5es que precisam de um alto desempenho e distribui\u00e7\u00e3o de tr\u00e1fego em tempo real.Exemplo:Vamos pensar no seguinte cen\u00e1rio, voc\u00ea quer muito ver um filme e percebe que pode ver esse filme em um determinado provedor de streaming. Esse provedor de streaming \u00e9 super famoso e recebe milhares de pessoas todos os dias para assistem ao filmes em seu cat\u00e1logo e tudo isso ao mesmo tempo. Para que voc\u00ea consiga ver seu conte\u00fado sem interrup\u00e7\u00f5es e de forma lisa, o NLB entra em cena e faz ali todo o balanceamento da carga entre v\u00e1rios servidores de streaming. Ele \u00e9 super importante nessa situa\u00e7\u00e3o, pois a demanda \u00e9 alta e afinal, tem v\u00e1rias pessoas vendo simultanemente.CLASSIC LOAD BALANCER (CLB)O Classic Load Balancer \u00e9 um balanceador mais antigo e em breve entrar\u00e1 em desuso pois ele funciona nos n\u00edveis de aplicativo e rede, mas n\u00e3o conta com recursos mais sofisticados como os encontrados nos ALBs e NLBs. Ele pode ser \u00fatil em situa\u00e7\u00f5es onde temos o tr\u00e1fego \u00e9 bem modero, mas ainda sim vale a pena considerar um dos dois balanceadores anteriores.Exemplo:Para ilustrarmos esse caso, podemos pensar em uma aplica\u00e7\u00e3o web que tenha um trafego baixo, n\u00e3o necessita de roteamento baseado em conte\u00fado ou de baixa lat\u00eancia. Da mesma forma que os anterioresm, ele distribui a carga entre os servidores dessa aplica\u00e7\u00e3o para que n\u00e3o tenhamos gargalos.GATEWAY LOAD BALANCER (GWLB)Esse tipo de load balancer \u00e9 bem novo e foi lan\u00e7ando em 2020. Conforme diz a AWS, ele basicamente combina um gateway de rede com um \u00fanico ponto de entrada e com a sa\u00edda para todo o tr\u00e1fego junto de um balanceador de carga. Essa balanceador distribui o tr\u00e1fego e dimensiona seus appliances virtuais de acordo com a demanda recebida.Esse LB \u00e9 tem a proposta de trabalho voltado para os appliances de rede, que s\u00e3o ferramentas ou softwares redes que podem ser usados para filtragem de dados, otimiza\u00e7\u00e3o de desempenho da rede, detec\u00e7\u00e3o e preven\u00e7\u00e3o de intrus\u00f5es, etc. Um exemplo s\u00e3o os firewalls, WAF\u2019s, IDS/IPS, entre outros.O Uso dele pode ser recomendado quando a gente tem um tr\u00e1fego bem alto chegando em nossa rede. Assim, o GWLB distribuir\u00e1 as solicita\u00e7\u00f5es entre as nossas diferentes ferramentas de rede, o que evita a sobrecarga em alguma delas. Outro ponto bem interessante \u00e9 que ele faz com que essas ferramentas abosrvam as solicita\u00e7\u00f5es da maneira que foram enviadas, o que pode ser bem importante quando falamos de seguran\u00e7a e otimiza\u00e7\u00e3o, onde os detalhes do tr\u00e1fego de rede precisam ser lidos para funcionar corretamente.Exemplo:Imagine que sua empresa pensa muito em seguran\u00e7a e configura ali firewalls de ponta e tem tamb\u00e9m um grande sistemas de preven\u00e7\u00e3o contra invas\u00f5es para analisar e filtrar o tr\u00e1fego de rede. Essas ferramentas precisam de um alto n\u00edvel de seguran\u00e7a e controle, o que pode ser dif\u00edcil controlar devido ao tamanho e a disponibilidade desses appliances de rede. \u00c9 aqui que o Gateway Load Balancer entra. Ele faz com que nossas ferramentas funcionem sem sobrecarga, pois distribui igualmente nossas solicita\u00e7\u00f5es de entrada entre os nossos appliances rede. Isso faz com a sobrecarga seja eliminada. Se caso algum appliance falhar ou estar sendo atualizado, o GWLB redireciona o tr\u00e1fego para as outras ferramentas e assim a nossa aplica\u00e7\u00e3o continua segura e healty.VANTAGENS DO USO DE LOAD BALANCERSAgora que entendemos o que \u00e9 e vimos os diferentes tipos de balanceadores de carga na AWS, vamos expor e relembrar as maiores vantagens de fazer o uso dessa ferramenta poderosa:Alta Disponibilidade:Uma das maiores vantagens de usar os balanceadores de carga \u00e9 o redirecionamento das requisi\u00e7\u00f5es quando um servidor falha ou fica indisponivel. Desta forma, ele manda o trafego para um que esteja saud\u00e1vel e assim sua aplica\u00e7\u00e3o n\u00e3o fica indispon\u00edvel.Escalabilidade:Quando a gente tem um fluxo muito alto ali na nossa aplica\u00e7\u00e3o o ELB consegue adicionar mais servidores para atender esse fluxo de acorodo com o que \u00e9 necess\u00e1rio e quando normalizar, esses servidores adicionais ser\u00e3o removidos. \u00c9 uma funcionalidade essencial para que a aplica\u00e7\u00e3o possa continuar dispon\u00edvel ap\u00f3s um grande tr\u00e1fego.Distribui\u00e7\u00e3o do Tr\u00e1fego Balanceada:Quando \u00e9 necess\u00e1rio realizar a distribui\u00e7\u00e3o de um determinado tr\u00e1fego, o load balancer faz isso de forma balanceada para que nenhum servidor fique sobrecarregado, o que traz um desempenho melhor pra nossa aplica\u00e7\u00e3o.Flexibilidade:Uma coisa bem interessante \u00e9 a possibiliade de distribuir o tr\u00e1fego de conforme a cria\u00e7\u00e3o de algumas regras, fazendo com que a gente possa direcionar determinados tipos de requisi\u00e7\u00f5es para servidores espec\u00edficos.Integra\u00e7\u00e3o com Servi\u00e7os AWS:O LB da AWS \u00e9 compat\u00edvel e bem f\u00e1cil de ser utilizado com outros servi\u00e7os, como o Auto Scaling, RDS, WAF, EC2, ECS, EKS, Lambda, etc.CUSTOSComo a gente sabe, na AWS a gente paga somente pelo o que usamos e a precifica\u00e7\u00e3o do Elastic Load Balancer depende do tempo, requisi\u00e7\u00f5es e tipo.No Application Load Balancer e no Network Load Balancer o pre\u00e7o \u00e9 definido por hora utilizada e uma m\u00e9trica composta por novas conex\u00f5es por segundo, conex\u00f5es ativas por minuto, bytes processados \u200b\u200bpor hora e avalia\u00e7\u00f5es das regras do seu listener por segundo.A cobran\u00e7a no Classic Load Balancer \u00e9 mais simples e composta pela quantidade de GB de dados transferidos por esse LB e tamb\u00e9m por hora utilizada.O Gateway Load Balancer possui um modelo de precifica\u00e7\u00e3o onde temos 02 tipos de cobran\u00e7as simult\u00e2neas. A primeira \u00e9 igual a que acontece no ALB e no NLB e a segunda \u00e9 pelo VPC Enpoint desse load balancer.Voc\u00ea pode simular os custos desses load balancers com a calculadora da AWSclicando aqui.CONCLUS\u00c3OFazer o uso de um load balancer com toda certeza \u00e9 importante quando falamos de disponibilidade, escalabilidade e desempenho das nossas aplica\u00e7\u00f5es nesse provedor. Quando a gente entende como funciona cada tipo, a implementa\u00e7\u00e3o fica mais f\u00e1cil e o resultado sempre \u00e9 recompensador, mas \u00e9 necess\u00e1rio que as configura\u00e7\u00f5es do mesmo sejam feitas de forma adequada para garantir que sua aplica\u00e7\u00e3o possa lidar com tr\u00e1fego esperado, ser consitente e confi\u00e1vel.\u00c9 importante lembrar de analisar o seu cen\u00e1rio e escolher do tipo load balancer que mais se adequa a sua aplica\u00e7\u00f5e e as suas necessidades."}
{"title": "Como me Preparei para a Certifica\u00e7\u00e3o AWS Cloud Practitioner (CLF-C01)", "published_at": 1714752345, "tags": [], "user": "Paloma Lataliza", "url": "https://dev.to/aws-builders/como-me-preparei-para-a-certificacao-aws-cloud-practitioner-clf-c01-5hmd", "details": "Como todo mundo sabe, o mercado de certifica\u00e7\u00f5es hoje est\u00e1 muito em alta. Cada vez mais as certifica\u00e7\u00f5es tem trazido visibilidade e a gente sabe que com o avan\u00e7o da tecnolgia e o crescimento de profissionais nas \u00e0reas de tech, as certifica\u00e7\u00f5es tem se tornado uma forma de apresentar um diferencial competitivo.O mais legal da jornada de se preparar para uma certifica\u00e7\u00e3o \u00e9 o conhecimento que a gente adquiri nesse tempo de estudo e prepara\u00e7\u00e3o. Quando eu resolvi fazer o AWS Cloud Practitioner eu j\u00e1 trabalha com nuvem, mas estudando para esse exame eu consegui rever v\u00e1rios conceitos e entender melhor alguns servi\u00e7os que eu n\u00e3o tinha tanto contato. Essa prova \u00e9 uma prova onde a gente precisa ter um compreens\u00e3o ou conhecimento b\u00e1sico sobre os servi\u00e7os da AWS, gerencimento de seguran\u00e7a, conformidade, casos de uso na AWS, suporte e faturamento. \u00c9 importante tamb\u00e9m revisar alguns conceitos de computa\u00e7\u00e3o em nuvem.SOBRE O EXAMENeste exame o tempo m\u00e1ximo para a execu\u00e7\u00e3o \u00e9 de 90 mintos e voc\u00ea pode escolher faz\u00ea-lo em um centro presencial ou online em sua casa.As minhas experi\u00eancias realizando as provas em casa foram mais negativas do que positivas, pois al\u00e9m dos poss\u00edveis problemas que podem ocorrer como falta de energia ou picos de queda na internet, a pessoa que aplicar\u00e1 sua prova pode ser muito r\u00edgida como foi nos meus casos e acabar pedindo para voc\u00ea realizar in\u00fameras verifica\u00e7\u00f5es. Caso n\u00e3o tenha um centro de treinamento pr\u00f3ximo ou voc\u00ea deseje mesmo fazer a prova em casa, no local deve estar somente voc\u00ea e com uma tela somente. Evite ter v\u00e1rios objetos ou perif\u00e9ricos em sua mesa e tenha seu documento de identifica\u00e7\u00e3o em m\u00e3os. \u00c9 importante que a porta esteja fechada, que em sua casa n\u00e3o tenha barulhos no momento, e que voc\u00ea n\u00e3o leia as quest\u00f5es falando, mesmo que seja em voz baixa, pois caso contr\u00e1rio o seu exame pode ser anulado. O valor atual da prova \u00e9 de 100 d\u00f3lares, a pontua\u00e7\u00e3o vai de 0 a 1000 e para a aprova\u00e7\u00e3o \u00e9 necess\u00e1rio ter 700pts.Essa certifica\u00e7\u00e3o pode ser realizada nos seguintes idiomas: Ingl\u00eas, franc\u00eas, alem\u00e3o, italiano, indon\u00e9sio, japon\u00eas, coreano, portugu\u00eas, chin\u00eas simplificado, espanhol e chin\u00eas tradicional.Falando sobre o conte\u00fado, temos presente 04 dom\u00ednios principais: conceitos de nuvem e tecnologia da AWS (26%), seguran\u00e7a e conformidade da AWS (25%), tecnologia de faturamento e precifica\u00e7\u00e3o da AWS (25%) e servi\u00e7os e recursos da AWS (24%).No meu exame eu vi bastante quest\u00f5es sobre EC2, ECS, S3, DynamoDb, RDS, Macie, XRay, CloudFront, Lambda, suporte e billing, al\u00e9m dos conceitos de computa\u00e7\u00e3o sendo relacionados com os servi\u00e7os da AWS, como por exemplo:\u201cQual solu\u00e7\u00e3o IAAS o cliente X poderia utilizar para um banco de dados em sua empresa?\u201dPREPARA\u00c7\u00c3O PARA O EXAMEQuando eu comecei a me preparar para essa prova, foquei muito nos conceitos de cloud e estudei bastante sobre os servi\u00e7os e sua utiliza\u00e7\u00e3o no dia a dia. Claro, tamb\u00e9m fiz v\u00e1rios simulados para conhecer o estilo da prova e posso dizer que em alguns, as quest\u00f5es eram beeeem semelhantes as do exame.Para o estudo \u00e9 importante ver os seguintes t\u00f3picos:1 . Conhecendo a AWS e Aprendendo Conceitos B\u00e1sicos da nuvemEstude sobre os conceitos b\u00e1sicos de nuvem, como IaaS, PaaS, SaaS, como a matriz responsabilidade compartilhada, capex e opex, tipos de nuvens, etcUse e abuse do free tier para entender os servi\u00e7os e produtos b\u00e1sicos da AWS, como IAM, EC2, S3, RDS, LAMBDA etc.Aprenda sobre os conceitos de regi\u00f5es e zonas de disponibilidade, edge locations.2. Seguran\u00e7a, Conformidade e RedeDesenvolva mais sobre o IAM, em toda parte de permiss\u00f5es, roles, polices, tipos de acessoEntenda um pouco mais sobre seguran\u00e7a com os servi\u00e7os como Shield, WAF, e AWS Inspector.Estude sobre redes com VPC, subnets, NACL\u2019s, ACL\u2019s, Transit Gateway, Load Balancer e tamb\u00e9m com os security groups3. Servi\u00e7osAprofunde seus conhecimentos nos servi\u00e7os b\u00e1sicos citados anteriormente e tamb\u00e9m nos servi\u00e7o a n\u00edvel de aplica\u00e7\u00e3o e CI/CD como Lambda, SQS, SNS, API Gateway, Code Pipeline, Code Build, Code Deploy e Code Commit.Aprenda tamb\u00e9m sobre servi\u00e7os de observabilidade e monitoramento como CloudWatch, X-ray, CloudTrail, AWS Config.4. Pre\u00e7o, Faturamento e SuporteAprenda sobre os modelos de cobran\u00e7a da AWS para os tipos de servi\u00e7os, faturamento, billing e tipos de precifica\u00e7\u00e3o.\u00c9 importante entender tamb\u00e9m o AWS Cost Explorer e o Budgets, assim como a diferen\u00e7a entre eles.Revise os planos de suporte da AWS, seus benef\u00edcios e a quem se aplica cada plano.Com todos esses passos entendidos, busque agora entender como fazer para melhorar os custos dos servi\u00e7os.CURSOS E SIMULADOSQuando eu estava estudando, eu fiz in\u00fameros simulados que encontrei na internet. O curso que me ajudou bastante foi o do Stephane Maarek, d\u00e1 pra ver todos esses t\u00f3picos e servi\u00e7os que citei. \u00c9 um curso bem completo e bem explicado. Esse curso \u00e9 pago e voc\u00ea pode encontr\u00e1-lo na Udemy com um valor entre R$27,90 a R$39,90, depende das promo\u00e7\u00f5es. Voc\u00ea pode compr\u00e1-loCLICANDO AQUI. Os simulados do Stephane Maarek s\u00e3o muito bons tamb\u00e9m e tendem a ser mais dif\u00edceis do que o pr\u00f3prio exame, o que \u00e9 bom pra nosso aprendizado. Os simulados dele tamb\u00e9m est\u00e3o na Udemy e tamb\u00e9m s\u00e3o pagos.Outro curso bem legal \u00e9 o AWS Cloud Practitioner Essentials do AWS Skill Builder. Ele \u00e9 gr\u00e1tis e voc\u00ea pode ver mais sobre eleCLICANDO AQUI. No AWS Skill Builder voc\u00ea tamb\u00e9m pode fazer simulados gr\u00e1tis, basta procurar dentro da plataforma.CONSIDERA\u00c7\u00d5ES FINAISEssa certifica\u00e7\u00e3o \u00e9 uma certifica\u00e7\u00e3o de entrada na AWS e \u00e9 super bem vista no mercado de trabalho. Vale super a pena tentar fazer. A AWS sempre tem campanhas bem legais de certifica\u00e7\u00f5es e as vezes com at\u00e9 vouchers de 100% de desconto. Para ficar de olho nessas campanhas, sigam a AWS e os AWS Technical Trainers nas redes sociais e tamb\u00e9m no linkedin.Um grande aliado para entender tudo \u00e9 praticar. Tente usar bastante o free tier para explorar o m\u00e1ximo poss\u00edvel dos servi\u00e7os. Fa\u00e7am bastaaante simulados e o mais importante, v\u00e1 no seu tempo, n\u00e3o se desgaste demais estudando como louco, estude com calma absorvendo o m\u00e1ximo de conte\u00fado poss\u00edvel.Espero ter te ajudo e que voc\u00ea obtenha sucesso em seu exame!"}
{"title": "PLAYING SUPER MARIO BROS ON FARGATE (CONTAINERS SERIES) \u2014 #EP 2", "published_at": 1714750646, "tags": [], "user": "Paloma Lataliza", "url": "https://dev.to/aws-builders/playing-super-mario-bros-on-fargate-containers-series-ep-2-3l0n", "details": "In the previous episodewe talked about some very important concepts, such as virtualization, containers and we ran our first local image. In this episode, we\u2019ll learn about the cloud we\u2019ll use to host our application and the services we\u2019ll use.CLOUD PROVIDER \u2014 AMAZON WEB SERVICESThe cloud that we are going to use is AWS, it is through it that we are going to host our application. You can read more about what AWS is inmy article.The AWS services we will be using are:ECRVPCSECURITY GROUPAPPLICATION LOAD BALANCERECS COM FARGATEWith these services we will be able to have our game in the cloud in a reliable and secure way. Now that we know what AWS is and what services we are going to use, it's time to understand these services, how they work and how to use them.ECRECR is an AWS service that lets you store, manage, and deploy Docker container images in the cloud. With it, share the images that are in the repository to deploy containers in services such as ECS and EKS.The coolest thing about ECR is that it brings security, scalability and high availability, ensuring that our images are ready to be deployed when needed.Now that we understand what ECR is, let\u2019s put it into practice. The first thing we\u2019re going to do is create a repository with ECR to host our image. Just access the ECR and create a repository for our image. Here I called it supermario, but you can name it whatever you want. Once we insert the name, just createThe next step is to download our Docker image to later use it. The image we will use in this practice is on Docker-hub and its author is pengbai. To download it, open your terminal and use the following command:docker pull pengbai/docker-supermarioEnter fullscreen modeExit fullscreen modeOnce this is done, we will have our image ready locally.Our next step is to upload the image we just downloaded to our repository and for that we will click on view push commands to see the necessary commands to upload our image and execute them. Step 2 of Push commands for supermario is not necessary, as we already have our image ready and for this reason we will not build it, skipping from step 1 to step 3.VPCShow, the first part of our journey has been completed, now let\u2019s get into the network concepts a little bit so we can connect our services, as well as make it work there on the internet.Amazon VPC (Virtual Private Cloud) is a service that allows you to create an isolated virtual network in the cloud.Simply put, a VPC is like a \u201cbubble\u201d in the cloud that lets you create an isolated and secure virtual private network where you can run your cloud computing resources along with your own subnets, firewall rules, internet gateways, and more. , all within a highly customizable and controlled environment.The advantage of using a VPC is that we can create a secure virtual private network to run our applications on our own managed network infrastructure.Now that we understand what VPC is and we have our image ready, let\u2019s create a network along with its respective subnets and all the dependencies we\u2019ll need to play our game.Let\u2019s access the VPC service, click on create VPC and fill in the settings for our VPC. Mine I called supermario-vpc.SECURITY GROUPThe Security Group is a very important security service that allows us to control access to our cloud services and environments.Basically, it\u2019s like a \u201cgateway\u201d that we configure to allow or block traffic to our cloud resources based on protocol, port, and IP. It works with rules, where we decide which IPs or resources can access our environment/resource and on which port this access will happen.To create the security group, we will access the page of this service and click on create security group. That done, let\u2019s enter the settings of our SG such as the name we are going to give it, description, the VPC it will use and the entry and exit rules. It is important to remember that in the entry rules we will leave all tcps and only for your IP and also for the security group itself (Add an all tcps rule, select custom in the source field and insert the ID of the SG itself).APPLICATION LOAD BALANCERThe next service to be created will be our Target Group and our Load Balancer that will do all our balancing.The Target Group, an AWS resource, is a group of resources that will receive traffic routed by the Load Balancer. It is through it that we manage where by determining our application\u2019s incoming traffic should be directed. With it we can direct traffic between various destinations such as EC2 instances, containers and Lambda functions.In the Load Balancer part, we will use the application type which is responsible for allowing applications to be highly available and scalable, distributing traffic to the healthiest Target Groups, improving fault tolerance and working with the HTTP and HTTPS protocolsFARGATEAt this moment we will host our application on ECS fargate. To understand more about fargate you can see myarticle here.TASK DEFINITIONBefore creating our cluster, It is the specification of an ECS task. In it, you can inform the configuration of containers, amount of memory/CPU, configuration of volumes, network mode, etc..We will need to open ECR -> REPOSITORIES -> SUPERMARIO and copy the URI of our image.Now let\u2019s click on task definition and create new task definition. That done, let\u2019s add all the necessary settings such as the name of the task definition, operating system family, task size and the execution role. Now, scrolling down the page a bit, in the Container definitions label, let\u2019s add an add container and choose a name for our container, insert the URI of the image we copied and the port it will run on. Here, it will run on port 8080.CLUSTERGo to ECS -> Clusters -> Create Cluster, select Networking Only, go to the next step, choose a name for our cluster and create.SERVICEOnce that\u2019s done, let\u2019s go to our last step, creating the service that will manage our task. The service will manage our task which contains our container and it is created based on our task definitionFor this, we will access our cluster and click on create service. At this point, we are going to add all the necessary configurations for our service, such as which type of service, which task definition we are going to use, how many tasks are going to be executed, which VPC, Load Balancer, Target Group, security group and autoscaling configurations will be part of the service.TASKThe task is the resource that contains all the configurations and information of our container and it runs and manages it.To make sure that everything went well, first we must check the status of our task, if it is RUNNING, it means that it is working. To test whether our application is operating the way we expect it to and that nothing is wrong, just copy the task\u2019s Public IP and add the container\u2019s port, for example IP:8080.Done, now you can enjoy your game :)"}
{"title": "Large Language Model Operations (LLMops) on AWS", "published_at": 1714737720, "tags": ["ai", "webdev", "aws"], "user": "Matteo Depascale", "url": "https://dev.to/aws-builders/large-language-model-operations-llmops-on-aws-4b86", "details": "\u26a0\ufe0f Hey! This blog post was initially published on my own blog. Check it out from the source:https://cloudnature.net/blog/large-language-model-operations-llmops-on-awsIntroductionLarge Language Models (LLMs) are becoming increasingly powerful, and their applications are widespread across many industries, from content creation to healthcare. Generative AI is becoming the de facto standard for multiple tasks due to its ability to generate human-like text, images, videos, and more.For LLMs, the evergreen MLops might not be sufficient anymore. We need practices and processes designed specifically for Large Language Models. These practices and processes are called Large Language Model Operations, or LLMops for short. In this article, we will discuss what these operations are and, more importantly, how we can leverage them with AWS.This article serves as both a high-level overview for everyone and a bit in-depth with technical details for those who want to dive deeper.What are LLMops?LLMops, short for Large Language Model Operations, refer to the practice of leveraging large language models (LLMs) like GPT-3, Anthropic Claude, Mistral AI, and others to automate various tasks and workflows. The core idea behind LLMops is to use the powerful generation capabilities of LLMs to create software applications, APIs, and tools that can understand and generate human-like text, images, video, and audio.Wild! I know, right? LLMs can even generate music with soundtrack and vocals too. I wonder what they will be capable of in a few years.The purpose of LLMops is to augment and automate a wide range of generation-related tasks that were previously labor-intensive or required significant domain expertise. So, LLMops encompass all operations needed to enable and enhance the usage of LLMs.Key Components of LLMopsWe introduced operations, let's look at what these operations actually are.\u26a0\ufe0f This is a series! I'll talk in depth about each one of these practices in separate articles.Data PreparationAre you familiar with the phrase: \"\ud83d\udca9 in -> \ud83d\udca9 out\"?The scope of data preparation is indeed to change that phrase into: \"\ud83d\udca9 in -> \u2728out\".In order to improve the quality of the data, there are a bunch of operations we can do:Text cleaning and normalization: You don't want to train your AI with your customers' credit card numbers, do you? That's essentially what you are doing here, you are cleaning the \"dirt\" in your data;Data deduplication: Removing duplicates is always a good thing, right? Your bill will thank you later on \ud83d\ude09;Data augmentation: Sometimes your data is not enough. You may need to add text classification or image description. Other times, you may need to generate generic synthetic data to include with your original data;Data filtering: When you don't need every piece of information in your raw data, you can filter out unnecessary data. This helps with unwanted data and unnecessary information that LLM may not need.Data IngestionYour data is cleaned, and it's now time to send the data out to begin the process. Data ingestion usually involves sending data to some kind of storage. Some common examples of data ingestion are:Database: When you don't need fancy words like semantic search, a classic SQL or NoSQL database is just fine, even for AI-related tasks;Vector database: When you actually need fancy words like semantic search, in that case, you need to use a vector database;Cold storage: After the data is processed, you may want to store your raw data in some kind of cold storage to pay less and always have your data in case something happens;Metadata database: It's always a good thing to track metadata in a database, like file locations or simply tagging these files, for instance.Model EvaluationWhatever you are doing with LLMs, you must employ some kind of evaluation mechanisms in order to at least get the feeling that the LLM answers properly to your prompts.There are soooo many ways to evaluate; the most common benchmarks are:GLUE (General Language Understanding Evaluation) BenchmarkSuperGLUE BenchmarkHellaSwagTruthfulQANot only that, of course, there are many frameworks, like Amazon Bedrock, that provide evaluation against your own prompts.Model Deployment and InferenceWhen your model is ready, it's time to deploy and serve your model, and this is exactly it, nothing too fancy, I'm sorry \ud83d\ude09.Once served, you (or your userbase) can start using your LLM.Model Fine-tuningUsing the foundation model (FM) may not be enough for you; in that case, you may want to consider improving (fine-tuning) your FM with your own dataset. Consequently, you have the power of a fully working LLM with your data.Retrieval-Augmented GenerationNot always do you need to train your LLMs with your own data. Sometimes Retrieval-Augmented Generation (RAG) is all you need(cit.)to add dynamic text into your prompt.And this is it, with RAG we can customize our prompt with our data. One of its downsides is the limit on the context window size, which means your prompt can't be higher than a particular amount of tokens.Model MonitoringMonitoring is essential in every aspect of the IT sector, and in this case, it is paramount (can I say this word or only AI is allowed to?\ud83d\ude1c).But what exactly is monitoring for Generative AI? Let's look at this list:System monitoring: Like every other system, you need to ensure your LLMs are up and running. So, if you deploy your LLM in your EKS cluster, then you need to ensure its scaling.Security: If someone is trying to hack your LLM, don't you want to know when it happens?Metrics to watch for: Choosing the right KPIs to monitor is essential. You may want to monitor your LLM's sentiment, its security, how many times it hallucinates, or even the quality of your LLM responses. There are so many frameworks and algorithms worth talking about when dealing with metrics to monitor, but you get the point of how this works.Model Governance and ReviewBy tracking, documenting, monitoring, versioning, and controlling access to LLMs, you can control model inputs and outputs to understand what might affect LLM outputs. Major companies are struggling with keeping their LLMs unbiased, and by introducing governance and human review, you can reduce the risks of biases in your LLMs.Granted that some models need governance more than others, but you don't want to destroy your company's reputation because your large language model said something it shouldn't have, do you? \ud83d\ude05Anyway, most of the time it's not about reputation; it's aboutresponsible AIand following its principles in order to create the safest version of your LLM.Cost OptimizationGenerative AI is expensive \ud83d\udcb8. Usually, enterprises do not have clarity in their minds about how much it costs for them. Is it the input or output costs? What if it's the hardware, or it may be the monthly commitment? Clearly, you need a written strategy so when dealing with costs, everyone can speak the same language.Apart from that, there are many strategies to mitigate costs in your LLMs, to mention a few of them:Prompt compressionCachingUse RAG instead of fine-tuningDeploy your own model instead of using a third-party onePrompt Engineering and ManagementPrompt engineering doesn't stop at that simple phrase you write to your LLM. Prompt engineering is a complex and large topic involving:Different prompt techniquesPrompt security defensesPrompt versioning and trackingPrompt optimization and tuningSecurity and ComplianceI don't need to say that Generative AI security should be the backbone of your Generative AI application design, right? We should always design solutions with security in mind.... a few minutes after I spared you a long version of my pep talk \ud83d\ude1cSecuring your large language model means protecting it from prompt injection, prompt leaks, DDoS attacks on your infrastructure, or even restricting the types of content it should receive/answer to.There are many tools you can employ to get the job done. For instance, in your application, you can useguardrailsto restrict input and output so your LLM doesn't answer with biased or hallucinatory text.Model Lifecycle ManagementLLMs have lifecycle management too, from model versioning to rolling back updates, or even archiving and retiring old versions of your LLM. In short, lifecycle management is essential and it includes almost all of the previous points.LLMops on AWSAWS is a leading public cloud provider and, as you can imagine, it offers every tool available to develop and build LLMops in the cloud. Throughout this article, you probably saw some reference architecture of how we can build that particular solution using AWS services.At its core, Amazon Bedrock is a fully managed service that provides access to foundation models (FMs) created by Amazon and third-party model providers through an API. Additionally, we can fine-tune it, run AI Agents, have its knowledge base, and even add guardrails and review your own LLM.Amazon Bedrock, along with a couple of other serverless services, can get us through each and every aspect of LLMops.\u26a0\ufe0f To know more about it, you can check out my blog post regarding Amazon Bedrock: \ud83d\udd17The Complete Guide to Amazon Bedrock for Generative AIConclusionAnd \"that\" is it, folks. Hopefully, I was able to make a good recap of what LLMops are and how we can do it on AWS. You probably noticed I didn't go into details regarding infrastructure, procedures, etc., why is that? Because I wanted to write about them in detail in my upcoming articles \ud83d\ude09Future articles will be listed somewhere \"here\" and they will talk about each LLM operation in depth, with reference architecture and projects to follow along.If you enjoyed this article, please let me know in the comment section or send me a DM. I'm always happy to chat! \u270c\ufe0fThank you so much for reading! \ud83d\ude4f Keep an eye out for more AWS related posts, and feel free to connect with me on LinkedIn \ud83d\udc49https://www.linkedin.com/in/matteo-depascale/.Disclaimer: opinions expressed are solely my own and do not express the views or opinions of my employer."}
{"title": "3GPP Insights: Expert Chatbot with Amazon Bedrock & RAG", "published_at": 1714729354, "tags": ["rag", "bedrock", "5g", "aws"], "user": "Marco Gonzalez", "url": "https://dev.to/aws-builders/3gpp-insights-expert-chatbot-with-amazon-bedrock-rag-l58", "details": "Being in the Telecom field for quite a few years now, I often hear the same questions: \"Can you show me in which part of the 3GPP standard this/that feature is mentioned? or Is this solution aligned to current 3GPP Standards?\". Whether it's a passionate new grad who just joined the company and wants to prove his/her value to the team, or a suspicious customer who loves to dive deep into every detail to make them look cooler with their boss\ud83d\ude09, the goal is the same: Get the desired data in a human-readable format.Thinking of the different ways to use GenAI for the Telecom Field, I came up with the following blog entry. What if you could just simply ask a GenAI model a 3GPP-feature compliance question and get the answer in seconds (or minutes depending on which LLM model you are testing)? Let's get started \ud83e\udd13Call-Flow:Below is the Architecture and call-flow of this 3GPP Chatbot, I will briefly explain each item in the following section:Call Flow Explanation:Data Integration Workflow:Load Data into Memory through PyPDFLoader, a library provided by Langchain, which allows us to insert all the data from the PDF into memory. This initial action should be performed by our good old Telco-guru.Break down the ingested data into single pages, paragraphs, lines, and then characters until we can create vector embeddings from smaller chunks of data. For that, I will use \"RecursiveCharacterTextSplitter,\" defining the desired chunk of characters.Create vector embeddings from the chunks of characters. In this demo, I will use Amazon Titan Text embedding.The last step is to store vector embeddings in a Vector store and create an index for easier search and retrieval.End-user Flow:A - The flow starts with our Telco Newgrad posting a question, which then goes to the Titan Text embedding model.B - Titan will create vector embeddings for that specific question.C - Once these vector embeddings are created, a similarity check will be in place in the vector store.D - If a match is found, a response or \"context\" will be retrieved and sent to the next step, which is the Foundation Model.E - The question and context will then be combined and sent to our Foundation Model, in this case, Llama3.F - A human-readable answer will be generated and prepared to be sent back to our Telco Newgrad.G - The final step is an accurate response sent back through the chatbox, solving our new grads' 3GPP-related questions and saving them minutes (or hours) in the process.ImplementationPre-requisites:The following tools must be installed before apply and test the code, so please check all items before moving on with the next steps.VSCode (Recommended one for its Anaconda Integration)PythonAWS CLIIAM role for VSCodeAnaconda Navigator -->Open VSCode from Anaconda NavigatorInstall Boto3pip install Boto3Install langchainpip install langchainInstall Streamlit for an easy FrontEnd optionpip install streamlitInstall Bedrockpip install BedrockInstall Flask-SQLalchemypip3 install flask-sqlalchemyInstall Pypdfpip install pypdfInstall faiss-gpupip install faiss-gpuorpip install faiss-cpu1. Data Load Operation:Our first piece of code will include the Data Load operation. We will create a new .py file and use the below code as referenceimport os from langchain.document_loaders import PyPDFLoader data_load=PyPDFLoader('https://www.etsi.org/deliver/etsi_ts/129500_129599/129510/16.04.00_60/ts_129510v160400p.pdf') data_test=data_load.load_and_split() print(len(data_test)) print(data_test[0]) ##You can test by replacing [0] to the page number you want to fetchEnter fullscreen modeExit fullscreen modeBelow code can then be omitted as its sole purpose is to help us understand how PyPDFLoader works :)print(len(data_test)) print(data_test[0]) ##You can test by replacing [0] to the page number you want to fetchEnter fullscreen modeExit fullscreen mode2. Data Transformation:For the Data transformation, we need to start by splitting the original text into smaller chunks.Refer to this link for official Langchain Documentation for Text SplitterLangchain-Text-SplitterName of this file: data_split_test.py#1. Import OS, Document Loader, Text Splitter, Bedrock Embeddings, Vector DB, VectorStoreIndex, Bedrock-LLM import os from langchain.document_loaders import PyPDFLoader from langchain.text_splitter import RecursiveCharacterTextSplitter  #2. Define the data source and load data with PDFLoader data_load=PyPDFLoader('https://www.etsi.org/deliver/etsi_ts/129500_129599/129510/16.04.00_60/ts_129510v160400p.pdf') #3. Split the Text based on Character, Tokens etc. - Recursively split by character - [\"\\n\\n\", \"\\n\", \" \", \"\"] data_split=RecursiveCharacterTextSplitter(separators=[\"\\n\\n\", \"\\n\", \" \", \"\"], chunk_size=100,chunk_overlap=10) data_sample = 'The mandatory standard HTTP headers as specified in clause 5.2.2.2 of 3GPP TS 29.500 [4] shall be supported.' data_split_test = data_split.split_text(data_sample) print(data_split_test)Enter fullscreen modeExit fullscreen mode3. Embedding, Vector Store & Index operationFor this step, we will invoke our Bedrock Titan model \"amazon.titan-embed-text-v1\" and create a vector store and Index.Name of this file: rag_backend.py#Import OS, Document Loader, Text Splitter, Bedrock Embeddings, Vector DB, VectorStoreIndex, Bedrock-LLM import os from langchain.document_loaders import PyPDFLoader from langchain.text_splitter import RecursiveCharacterTextSplitter from langchain.embeddings import BedrockEmbeddings from langchain.vectorstores import FAISS from langchain.indexes import VectorstoreIndexCreator from langchain.llms.bedrock import Bedrock  # Wrap within a function def 3gpp_index():     #2. Define the data source and load data with PDFLoader data_load=PyPDFLoader('https://www.etsi.org/deliver/etsi_ts/129500_129599/129510/16.04.00_60/ts_129510v160400p.pdf')     #3. Split the Text based on Character, Tokens etc. - Recursively split by character - [\"\\n\\n\", \"\\n\", \" \", \"\"]     data_split=RecursiveCharacterTextSplitter(separators=[\"\\n\\n\", \"\\n\", \" \", \"\"], chunk_size=100,chunk_overlap=10)     #4. Create Embeddings -- Client connection     data_embeddings=BedrockEmbeddings(     credentials_profile_name= 'default',     model_id='amazon.titan-embed-text-v1')     #5\u00e0 Create Vector DB, Store Embeddings and Index for Search - VectorstoreIndexCreator     data_index=VectorstoreIndexCreator(         text_splitter=data_split,         embedding=data_embeddings,         vectorstore_cls=FAISS)     #5b \u00a0Create index for 3GPP document     db_index=data_index.from_loaders([data_load])     return db_indexEnter fullscreen modeExit fullscreen mode4. LLM creation + ContextIt's time to create a Foundation Model that will process both the query and the generated Context. I have selected \"meta.llama3-8b-instruct-v1:0\" as it's an opensource solution and opensource == no hidden costs ;)Name of this file: rag_backend.py#Function to connect to Bedrock Foundation Model - Llama3 Foundation Model def 3gpp_llm():     llm=Bedrock(         credentials_profile_name='default',         model_id='meta.llama3-8b-instruct-v1:0',         model_kwargs={         \"max_tokens_to_sample\":3000,         \"temperature\": 0.1,         \"top_p\": 0.9})     return llm # The following function searches the user prompt and the best match from Vector DB and sends both to LLM. def 3gpp_rag_response(index,question):     rag_llm=3gpp_llm()     3gpp_rag_query=index.query(question=question,llm=rag_llm)     return 3gpp_rag_queryEnter fullscreen modeExit fullscreen mode5. FrontEnd and Final IntegrationThe below frontend code is provided by AWS and Streamlit. Below modifications were done to align with our lab:Name of this file: rag_frontend.pyimport streamlit as st  import rag_backend as 3gpp_demo ### replace rag_backend with your backend filename  st.set_page_config(page_title=\"3GPP Q and A with RAG\")   new_title = '<p style=\"font-family:sans-serif; color:Blue; font-size: 30px;\">3GPP Chatbot Guru with RAG \ud83e\udde9</p>' st.markdown(new_title, unsafe_allow_html=True)   if 'vector_index' not in st.session_state:      with st.spinner(\"\u23f3 Please wait for our minions to finish preparing your answer in the back\ud83d\udc7e\ud83d\udc7e\"):          st.session_state.vector_index = demo.3gpp_index() ### Your Index Function name from Backend File  input_text = st.text_area(\"Input text\", label_visibility=\"collapsed\")  go_button = st.button(\"\ud83d\udcccAnswer this Chatbot Gur\", type=\"primary\") ### Button Name  if go_button:       with st.spinner(\"\ud83d\udce2Minions are still working \ud83d\udc7e\ud83d\udc7e\"): ### Spinner message         response_content = demo.hr_rag_response(index=st.session_state.vector_index, question=input_text) ### replace with RAG Function from backend file         st.write(response_content)Enter fullscreen modeExit fullscreen modeOnce above code is prepared, you just need to compile it and run it using below command:streamlit run rag_frontend.pyWrap-upThank you for joining me on this journey through the exciting potential of generative AI in the telecommunications sector. As I've shown in this short blog entry, using large language models (LLMs) like Llama3 can revolutionize how we interact with complex 3GPP standards, providing rapid, precise answers that empower both technical professionals and business stakeholders.Whether you're a developer looking to integrate advanced AI capabilities into your applications, or a non-developer curious about leveraging AI to enhance operational efficiency, I encourage you to experiment with LLMs.Why wait? Start your LLM journey now and unleash the full potential of AI in your personal or professional projects.Happy Learning!"}
{"title": "DeepRacer-for-Cloud v5.2.2 now available with new real-time training metrics", "published_at": 1714725423, "tags": ["deepracer", "machinelearning", "ai", "observability"], "user": "Matt Camp", "url": "https://dev.to/aws-builders/deepracer-for-cloud-v522-now-available-with-new-real-time-training-metrics-7ki", "details": "DeepRacer-for-Cloudprovides a great way for developers to train DeepRacer models on EC2 (or other cloud compute instances, or even local servers) however many users have noticed that unlike the official AWS console it didn't provide the kind of friendly web UI showing the current state of training.While there are some fantasticlog analysis notebooksavailable these can be a little tricky to set up and often require re-loading vast amounts of log data to get a refreshed view of the metrics.Deepracer-for-Cloud v5.2.2 is now available and has added an exciting new feature which enables real-time metrics visualisation using Grafana.Under the hood this involves creating three new containers for Telegraf, InfluxDB, and Grafana.The Robomaker simulation workers send the training metrics to Telegraf, which aggregates and stores them in the InfluxDB time-series database. Grafana provides a presentation layer for interactive dashboards.Getting startedTo use this new feature you will needv5.2.2 of Deepracer-for-Cloud, and also the v5.2.2 Robomaker container image.Updating DeepRacer-for-CloudIf you're installing DRfC for the first time then it should already download the correct image and templates, but if you're upgrading an existing install then you'll need to do a few steps:If you installed DRfC the recommended way by cloning the GitHub repo then you should do agit pullon the master branch to fetch the latest updates.To enable real-time metrics you need to add two additional lines to yoursystem.envfile:DR_TELEGRAF_HOST=telegraf DR_TELEGRAF_PORT=8092Enter fullscreen modeExit fullscreen modeIn almost all cases you can paste these directly in without modifying the values, as the hostname will reference the telegraf container running inside Docker.If this is your first install then these lines will need to be uncommented.Updating the Robomaker container imageFirst pull the updated container image from DockerHub. Use the cpu or gpu tag as appropriate for your system.docker pull awsdeepracercommunity/deepracer-robomaker:5.2.2-cpu  or  docker pull awsdeepracercommunity/deepracer-robomaker:5.2.2-gpuEnter fullscreen modeExit fullscreen modeThen update theDR_ROBOMAKER_IMAGEline insystem.envto set to the new image you just pulled.DR_ROBOMAKER_IMAGE=5.2.1-cpuEnter fullscreen modeExit fullscreen modeStarting the metrics stackYou can then start the metrics containers usingdr-start-metrics. (You might need to relogin or reload your shell to pick up the new changes inbin/activate.sh)This will start the three new containers. If it's the first time starting the metrics stack then Grafana will need to run some database migrations that can take 30-60 seconds before the web UI is available.Collecting metricsAs long as the two Telegraf lines have been added to system.env and you have v5.2.2 of the robomaker container then all you have to do is start training normally and the metrics will be automatically generated.Using the dashboardsOnce the metrics stack is running you should be able to access the Grafana web UI on port 3000 (eg,http://localhost:3000if running locally)Grafana initially starts with an admin user provisioned (usernameadmin, passwordadmin). It will prompt you to choose a new password upon first connect, so you should do this right away.A template dashboard is provided to show how to access basic DeepRacer training metrics. You can use this dashboard as a base to build your own more customised dashboards.After connecting to the Grafana Web UI with a browser use the menu to browse to the Dashboards section.The template dashboard called DeepRacer Training template should be visible, showing graphs of reward, progress, and completed lap times.As this is an automatically provisioned dashboard you are not able to save changes to it, however you can copy it by clicking on the small cog icon to enter the dashboard settings page, and then clickingSave asto make an editable copy.Grafana dashboards are interactive - you can over over datapoints to see more details, and you can click and drag on a graph panel to zoom in.You can also change the time range using the selector box on the top right, and also select an auto-refresh period from the selector next to that.A full user guide on how to work the dashboards is available on theGrafana website.Currently we record metrics for training and evaluation sessions such as reward, progress, average and best lap times but in the future we'll be adding more even metrics and dashboards."}
{"title": "Securing Your Cloud: Proactive Strategies for AWS Security", "published_at": 1714664537, "tags": ["aws", "cloud", "security", "cloudcomputing"], "user": "Brandon Damue", "url": "https://dev.to/aws-builders/securing-your-cloud-proactive-strategies-for-aws-security-4040", "details": "As a professional working in any information and technology niche you learn quickly that securing every part of your IT infrastructure is arguably the most critical task. There is no room for debate \u2014 it is ESSENTIAL. Since the ability to tell stories and tell them well is an important skill in this modern economy, a little backstory if I may. Recently the IT infrastructure of the city of Hamilton, Ontario [where I live] was breached. I bet this set the city back in terms of finances (they needed to upgrade security measures and fortify their defenses) and potentially compromising resident safety (this is not an exaggeration). All this got me thinking: is a purely defensive security posture enough for businesses and IT professionals when push comes to shove? That question led me to write this article in which I explore how we can be more offensive in our security strategies within the AWS cloud environment. We are going to look at the various ways we can leverage various services to anticipate and mitigate potential security threats before they materialize. So Let\u2019s get to it!!Continuous Observation, Monitoring and AnalysisAs a self-proclaimed storyteller, here\u2019s another backstory for you. While at my first AWS summit last year in Toronto, I noticed that most of the companies with showcase booths at the summit were offering observability services. This is indicative of how important monitoring and observing your cloud infrastructure is. Monitoring and observability tools can help you detect suspicious activity, potential vulnerabilities, and even signs of an ongoing attack. For these, AWS services such as CloudWatch and CloudTrail are there to assist you. But how exactly do you make use of them, you might wonder. Without going into too much detail, here\u2019s how: By analyzing user activity in CloudTrail and centralizing logs in CloudWatch, you can proactively hunt for threats. You can set log alarms for anomalies and use CloudWatch Insights to investigate suspicious activity. CloudTrail data can even help you understand attacker behavior and prioritize security measures. Combining these tools with other strategies we are going to look at will help you shift from reactive defense to proactive threat hunting in your AWS cloud environment. To learn more about CloudWatch and CloudTrail, check out this article.Automating Security Tasks and ConfigurationsAutomating security tasks and configurations does not only save time spent correcting mistakes and ensuring consistency but also prevents costly errors that could disrupt operations. Here is how you can do this using services such as IAM, AWS config and AWS Lambda. You can use IAM to control user access (always remember to follow thePrinciple of Least Privilege), while using Config to continuously monitor your resources against pre-defined security rules. If Config detects a violation, it triggers an AWS Lambda function \u2014 a serverless compute service. You can write custom code in Lambda to automate remediation actions. For example, a Lambda function could automatically revert a non-compliant configuration change or send an alert to security personnel. By doing this, you automate security tasks and enforce compliance, freeing you to turn your attention on optimizing other aspects of your security posture.Incident Response PlanningI will not assume that everyone reading this knows what Incident Response Planning is from the jump. Before moving forward let me explain what it is in the first place. Incident Response planning is the process of developing a documented strategy on how your organization will detect, respond to and recover from security incidents. As important as Incident Response (IR) plans are, most companies have attested to the fact that their IR plans are informal or even nonexistent. Understanding that a threat to an organization\u2019s security is not only a technical issue but a threat to the organization\u2019s business continuity can go a long way to change how organizations take on IR planning. You don\u2019t necessarily have to build an IR plan from the ground up by yourself as there are many companies offering incident response services.Secure DevOps PracticesThe rise of DevOps practices in software development is attributed to a growing need for faster development cycles, improved collaboration, and better software quality. When and where does security come into play in this DevOps conversation? It is when the conversation changes from talking about DevOps to DevSecOps. To achieve DevSecOps on AWS, integrate security into every step of your development process. To integrate early security checks, leverage AWS security services like Inspector and CodeBuild for automated testing within your CodePipeline, enforce security best practices in your Infrastructure as Code (IaC) withCloudFormationand Config, automate patching withPatch Manager, and cultivate a security-aware DevOps team through training and incident response planning. This continuous approach embeds security within your AWS DevOps workflow for a more secure and efficient development process.Vulnerability ManagementThis is the the last offensive strategy we are going to look in this article but it is be no means the last security strategy you can leverage as there are many other robust strategies not included in this article. The whole point of vulnerability management is that regularly scanning your AWS environment for vulnerabilities is an essential security practice. By identifying potential weaknesses before attackers exploit them, you significantly reduce the risk of data breaches and downtime. This not only protects your sensitive data but also helps maintain compliance with industry regulations. Regular scans provide a clear picture of your overall security posture, allowing you to prioritize patching vulnerabilities and continuously strengthen your defenses. It\u2019s a proactive investment that pays off in a more secure and resilient AWS environment. You can use Amazon Inspector and even third-party vulnerability scanners to achieve this.Last wordsI hope after reading this article, you were able to take away at least one strategy that you are going to implement to improve the rigidity of the security posture of your AWS cloud environment. Remember, security is an ongoing journey, not a destination. As the threat landscape evolves, so should your security practices. By embracing a proactive and offensive approach, utilizing the powerful tools offered by AWS as well as other service providers, and continuously refining your strategies, you can build a robust and resilient cloud environment that is well-equipped to withstand even the most sophisticated attacks."}
{"title": "Your S3 objects could be public (even though the AWS Console doesn't say so)", "published_at": 1714660628, "tags": ["aws", "security", "data", "s3"], "user": "Paul SANTUS", "url": "https://dev.to/aws-builders/your-s3-objects-could-be-public-even-though-the-aws-console-doesnt-say-so-4dcl", "details": "S3 is an amazing storage service, able to durably store data at exabyte scale and present it with single-digit millisecond latency. Though its name stands for \"Simplestorage service\", its power comes with some risks, one of which is to find your private data has become public.In this blog post,I'll show a not-so-well known way your objects could mistakenly become public.How AWS protects your data in S3I'll start with an obvious statement: that S3, as aweb service, is publicly available (i.e. you can use the S3 API without setting up a VPN) doesn't mean that datahasto be public.In fact, S3 buckets have always been private by default. And since 2018, there has been some additional locks at both account and bucket-level that you can set to explicitly prevent objects from being public even if you mistakenly set a policy that could cause public access. And from April 2023, those are enabled by default at bucket-level.With great power comes great responsibility! AWS shared security model states that the user, who has the power to set policies to enable public access, is then responsible for its good implementation.Here are the standard ways that can be used to set permissions in S3:Through Access Control Lists. ACLs are not recommended anymore but can still be used to grant access to S3 resources (buckets or objects).Through Resource-based policies. Each bucket has a policy that can allow (or explicitly deny, which always takes precedence) access to objects. That's the recommended way to proceed, as it's easy to set granular permissions and also to grant access to other AWS accounts or AWS services.Through IAM Identity-based policies. Make sure not to useaction = s3:*andresource = *!Any of those permissions can be neutralizedwith the aforementioned \"Public access block\" settings.So, how could your objects still be public, then?Apart from the well-known (and voluntary) pattern to use CloudFront CDN's distributions to make S3 data publicly available, there are 2 ways that you could inadvertently make your S3 objects public.The reason why I wanted to make this blog post is that I recently found both those leaks in a client of mine. He had an S3 bucket which was shown as \"public access blocked\" in the AWS Console, but data was leaked by those two security holes.Leaked API Access key / secret keyThe first way data could leak was because my client distributed API access key and secret key in a frontend application. In his case the application was a mobile app, but that's still code that runs at the client side, can be decompiled / reverse engineered / memory dumped.The good thing is that AWS proactively scans the web (which, obviously, seems to include application stores vs. just scanning public repositories) for secrets and warned my client that this particular API Key was available.Cognito Identity Pool Unauthenticated Guest featureThe second way is more subtle.Cognito Identity Pools offer the ability to deliver short-term credentials in exchange for  an IDP-issued proof of authentication. That's very useful, for instance to let all people from the marketing department access files in the S3 bucket; or to let userJohnDoeaccess only bucket files that are prefixed byJohnDoe.And because that's sometimes needed (for instance, you may want customers to display an Amazon Location map even if they don't already have an account on your app), Cognito offers the ability to allow unauthenticated guest access, in which case user's are delivered short-term credentials associated to a role of your own choosing.If the role has s3:* access to the bucket, well... users can do pretty much what they want with your bucket and/or objects.Here is how this can be exploited by an attacker that knows just the identity pool id (which has to be distributed in the front-end application) and the AWS account id (which is quite easy to find if the bucket name is also in the front-end code)# Creating a guest identity from the pool % aws cognito-identity get-id \\  --account-id ACCOUNT-ID_HERE \\ --identity-pool-id \"REGION:IDENTITY_POOL_ID\" \\ --region REGION  # AWS API replies with a unique user ID {     \"IdentityId\": \"REGION:UNIQUE_USER_ID\" }  # Then we ask for short-term credentials attached to this identity % aws cognito-identity get-credentials-for-identity \\ --identity-id \"REGION:UNIQUE_USER_ID\" \\ --region REGION \\ --output json {     \"IdentityId\": \"REGION:UNIQUE_USER_ID\",     \"Credentials\": {         \"AccessKeyId\": \"ASIAY--EDITED-FOR-SECURITY-REASON--4FJ\",         \"SecretKey\": \"I4D2SZ4--EDITED-FOR-SECURITY-REASON--v1AwAp/\",         \"SessionToken\": \"IQoJb3JpZ2luX2VjEIz//////////wEaCWV1LXdlc3QtMyJHMEUCIQCgXefjo82cstPQSS1WcXALUfmq364unN+Y/v5sb--EDITED-FOR-SECURITY-REASON--mBbD+AzASKDK\",         \"Expiration\": \"2024-04-16T22:17:04+02:00\"     } } # In the next step you can actually make any API call that the `my-role-for-cognito-guests` is granted permissions for.Enter fullscreen modeExit fullscreen modeHow to safely grant (download/upload) access to specific S3 objects without exposing secrets and managing customer identity in AWS / Cognito?A simple way to deliver this use case is to usebackend-generatedS3 pre-signed URLs.With S3 pre-signed URLs, you can execute your own custom application authorisation logic in your backend code and then use an IAM user credentials known only by the backend app to generate a url that you distribute to the client.Using this URL, the client can perform only the selected operation on this specific object for a period of time you determine, effectively acting like short-terme credentials specific to this client.I'm the Security guy for Corporation X. How can I make sure none of my developers use Cognito Unauthenticated Guest feature?Like most compliance checks, you can either:Scan Cloudtrail logs and look for AllowUnauthenticatedIdentities of the CreateIdentityPool and UpdateIdentityPool API operationsUse AWS Config rules. At the time of writing, there is no AWS-managed rule that supports detecting Cognito Identity Pool Unauthenticated Guest Access (hi there, AWS Service team!) but you can always write your owncustom Config Rulerelying on Lambda!That's all folks! I hope you learnt something useful today! Don't hesitate to leave a comment below, I'd be happy to keep the discussion going. I'm also availableon LinkedInor viamy website."}
{"title": "Enhancing security for Lambda function URLs", "published_at": 1714640971, "tags": ["aws", "security", "serverless"], "user": "Arpad Toth", "url": "https://dev.to/aws-builders/enhancing-security-for-lambda-function-urls-55dc", "details": "AWS has introduced support for Origin Access Control on Lambda function URLs. This new feature ensures more secure and consistent content delivery for our function URLs.1. The scenarioAWS has recentlyannouncedthe support forCloudFrontOrigin Access Controlwith Lambda function URLorigins.I've experimented with this new feature and recapped my findings in this post.2. StepsInstead of digging into detailed instructions for creating each resource, I will outline the necessary steps here. You can find links to the relevant documentation pages at the end of this article.2.1. Lambda function with URLThe first step involves setting up a Lambda function with an enabled function URL using theAWS_IAMauth type. I previously discussed the different function URL authorization types inan article. To summarize, theAWS_IAMauth type secures the endpoint by only accepting signed requests, while theAuthType: NONEconfiguration opens up the URL to the public.2.2. CloudFront distributionNext, we create a CloudFront distribution using the Lambda function URL as the origin. We copy the URL and specify it as theOriginin the distribution.2.3. Origin Access Control (OAC)The next step is to create an Origin Access Control in CloudFront and specifyLambdaas the origin. We should then add the OAC to the distribution.In a previous post onOrigin Access Control, I discussed how OAC can limit direct access to an S3 bucket. We follow a similar principle here.2.4. Add the permissionsFinally, to allow the CloudFront distribution to invoke the function URL, we need to add the following permission to the Lambda function'sresource-based policy:{\"Effect\":\"Allow\",\"Principal\":{\"Service\":\"cloudfront.amazonaws.com\"},\"Action\":\"lambda:InvokeFunctionUrl\",\"Resource\":\"arn:aws:lambda:eu-central-1:ACCOUNT_ID:function:FUNCTION_NAME\",\"Condition\":{\"ArnLike\":{\"AWS:SourceArn\":\"arn:aws:cloudfront::ACCOUNT_ID:distribution/DISTRIBUTION_ID\"}}}Enter fullscreen modeExit fullscreen modeSince the AWS console doesn't provide a JSON editor for creating or editing Lambda resource policies, we either use the UI or add the policy via the CLI. Here's how we can add it using the CLI:aws lambda add-permission\\--statement-id\"AllowCloudFrontServicePrincipal\"\\--action\"lambda:InvokeFunctionUrl\"\\--principal\"cloudfront.amazonaws.com\"\\--source-arn\"arn:aws:cloudfront::ACCOUNT_ID:distribution/DISTRIBUTION_ID\"\\--function-nameFUNCTION_NAMEEnter fullscreen modeExit fullscreen mode3. ResultsIn summary, the setup involves a Lambda function URL protected byIAMand a Lambda resource policy allowing our CloudFront distribution to invoke this URL.Let's see the results of my experimentation.3.1. Calling the function URL directlyFirst, I tested the function URL directly by makingunsignedrequests, which, as expected, were all declined. Then, when Isignedthe requests using my credentials, they were successfully processed.This behaviour is expected since theAuthType: AWS_IAMconfiguration is designed to protect the endpoint. Only identities granted with thelambda:InvokeFunctionUrlpermission can successfully call the URL. Thus far, the results confirm the expected security behaviour.3.2. Calling the URL via CloudFrontNext, I tested invoking the function via the CloudFront domain, simulating an external user makingunsignedrequests. As expected, theGETrequest was successful, and I received a response from the Lambda function.I anticipated this outcome. While theAWS_IAMauth type secures the function URL by only serving signed requests, the Lambda resource policy allows access to the function through the CloudFront distribution. Hence, invoking the function through the CloudFront domain name returned a valid response.But when I attempted aPOSTrequest, it failed with the following error message:The request signature we calculated does not match the \\  signature you provided. Check your AWS Secret Access Key \\  and signing method. Consult the service documentation for details.Enter fullscreen modeExit fullscreen modeI didn't expect this result. When I reviewed the documentation, I found the following note:If you use PUT or POST methods with your Lambda function URL, \\ your user must provide a signed payload to CloudFront. Lambda \\ doesn't support unsigned payloads.Enter fullscreen modeExit fullscreen modeThe documentation indicates the payload itself requires signing with proper credentials forPOSTrequests. External users would not be able to perform this step.CloudFront enhances security with additional layers such asDDoS protectionorWAF, and also provides content delivery acceleration. We can also assign a custom domain to the distribution, providing a more user-friendly name than the complex function URL domain.However, when handlingPOSTrequests, the situation remains consistent whether the function URL is invoked directly or via CloudFront. All such requests must be signed so only authorized identities can executePOSToperations.So, while the new feature enables easier access forGETrequests,POSTrequests still require signed payloads. It restricts using function URLs with CloudFront distributions to those who can sign the requests with the necessary credentials.4. ConsiderationsCurrently, the benefits of using CloudFront with Lambda function URLs are limited toGETrequests if we aim to have the function URL protected. ForPOST,PATCH, andPUTmethods, signing the requests with AWS credentials (access key, secret access key, and session token for roles) is required. For instance, we can't have the CloudFront distribution's domain as a webhook URL because these endpoints typically utilizePOSTmethods.A practical enhancement would be enabling CloudFront to automatically sign the request payloads and forward them to the function URL origin. This way, we could use CloudFront in more use cases, like webhook implementations.Additionally, unlike S3, Lambda does not support locking function URL invocations to specific resources like a CloudFront distribution. We can't add aDenystatement to the function's resource policy!One way to restrict the function's business logic to invocations only from CloudFront is to add acustom headerto origin requests and verify the header's presence in the Lambda function. With this approach, the Lambda function executes each time a request is made, if only to check for the header, leading to an increased number of function invocations.The new feature is an enhancement in making function URLs a more attractive option for certain use cases. Now, we can makeGETrequests to the function URL via CloudFront without needing additional header validation code in the function's handler while maintaining the URL protection through IAM.However, the current limitations aroundPOSTandPUTrequests mean that we can use them with CloudFront and Origin Access Control (OAC) mostly in internal applications. In these scenarios, we can configure the resource that's making the call to sign the request payloads using its AWS credentials.UPDATE 2024-05-13In a recentX post, fellow Community Builder David Behroozi highlighted that the architecture described abovesupportsbothPOSTandPUTrequests. To make it work, we need to compute theSHA256hash of the body and include the hash in thex-amz-content-sha256header.Consider this example, where the request body is:{\"test\":\"message\"}Enter fullscreen modeExit fullscreen modeFor the above body, the SHA256 hash would be7acb899e28935d98d2a01e27212b5f130ca860f5899c1caabf18ec0988e5df62. This hash should then be included in the request headers as shown below:{\"headers\":{\"x-amz-content-sha256\":\"7acb899e28935d98d2a01e27212b5f130ca860f5899c1caabf18ec0988e5df62\"}}Enter fullscreen modeExit fullscreen modeBy following these steps, sendingPOSTrequests to the CloudFront endpoint should yield successful responses.Thanks to David for this great insight!5. SummaryThe integration of CloudFront Origin Access Control with Lambda function URLs introduces an additional layer of security, enabling us to keep function URLs private.With this new feature, external users can now initiateGETrequests via the CloudFront domain. This release makes Lambda function URLs a more viable and secure option for various use cases.6. Further readingCreating and managing Lambda function URLs- How to create Lambda function URLsCreating a distribution- How to create a CloudFront distributionCreating a new origin access control- OAC for S3 (the process is the same for Lambda origins)"}
{"title": "Cloud Security and Resilience: DevSecOps Tools and Practices", "published_at": 1714584165, "tags": ["devsecops", "devops", "aws", "security"], "user": "Ravindra Singh", "url": "https://dev.to/aws-builders/cloud-security-and-resilience-devsecops-tools-and-practices-2fgb", "details": "Introduction:DevSecOpsintegrates security practices into the software development life cycle, ensuring security measures are considered from the start\ud83d\udd10Infrastructure Code Scanning Tools1. Checkov:https://github.com/bridgecrewio/checkovCheckov is a static code analysis tool that helps developers prevent cloud misconfigurations during the development phase by scanning Terraform, CloudFormation, Kubernetes, and more.2. Terrascan:https://github.com/tenable/terrascanTerrascan detects security vulnerabilities and compliance violations across your IaC. Supports multiple cloud providers, ensuring that your infrastructure complies with security best practices.3. tfsec:https://github.com/aquasecurity/tfsectfsec uses a suite of security checks to scan your Terraform templates, helping to identify potential security issues before infrastructure is deployed.Application Code Scanning Tools1. Gitleaks:https://github.com/gitleaks/gitleaksGitleaks provides a way for developers to find and prevent security breaches by scanning Git repositories for secrets like passwords and API keys.2. SonarQube:https://github.com/SonarSource/sonarqubeSonarQube enhances code quality and security. It performs automatic reviews to detect bugs, vulnerabilities, and code smells in your code.3. Hadolint:https://github.com/hadolint/hadolintHadolint is a Dockerfile linter that helps you build best practice Docker images, reducing vulnerabilities in your container configurations.4. Trivy:https://github.com/aquasecurity/trivyTrivy is a versatile tool that scans for vulnerabilities in your containers, and also checks for vulnerabilities in your application dependencies.\u2638\ufe0fKubernetes Cluster Scanning ToolsOutline tools that are crucial for ensuring the security and compliance of Kubernetes clusters:1. Kubescape:https://github.com/kubescape/kubescapeKubescape is the first tool for testing if Kubernetes clusters are deployed securely as defined in Kubernetes Hardening Guidance by NSA and CISA.2. Kubebench:https://github.com/aquasecurity/kube-benchKubebench is an open-source tool that checks whether Kubernetes is deployed according to security best practices as defined in the CIS Kubernetes Benchmark.AWS Account Scanning ToolDiscuss how to maintain security posture across AWS accounts:1. Prowler:https://github.com/prowler-cloud/prowlerProwler provides security best practices assessments, audits, incident response readiness, and continuous monitoring for AWS environments.Chaos Engineering Service/ToolExplain chaos engineering and its relevance to ensuring system resilience and reliability:1. AWS Fault Injection Simulator (FIS):https://docs.aws.amazon.com/resilience-hub/latest/userguide/testing.htmlLearn more about AWS FIS, a service that helps you perform fault injection experiments on AWS to create and manage controlled disruptions.2. Gremlin:https://www.gremlin.com/chaos-engineeringGremlin is a tool that helps companies build resilient systems through controlled experimentation on software infrastructure.\ud83d\udcdaDemo Source Code:https://github.com/ravindrasinghh/Deploying-a-Bulletproof-Photo-Sharing-App-with-DevSecOps-Terraform-AWS-EKS-and-Chaos-Engineering"}
{"title": "Optimise and Secure AWS HTTP API Gateway by locking down direct access", "published_at": 1714575213, "tags": ["aws", "apigateway", "cloudfront", "security"], "user": "Vimal Paliwal", "url": "https://dev.to/aws-builders/optimise-and-secure-aws-http-api-gateway-by-locking-down-direct-access-2f3a", "details": "Photo byGrowtikaonUnsplashAWS HTTP API Gatewaylet\u2019s you deploy RESTful API quickly in the most affordable way without compromising on basic security, performance, scalability and observability but unlikeREST API Gatewaylacks manyadvanced featuressuch as WAF attachment, resource policy, API key management, caching, canary deployments, request body transformation, X-Ray tracing, etc.AWS WAFis a vital resource to secure publicly exposed endpoints from various types of attacks and because HTTP API Gateway does not support WAF association natively, we need to create aCloudFront distributionand use it as an entry point to the HTTP API Gateway.Even after implementing the above resources, we aren\u2019t fully secure because if a bad actor can discover our HTTP API Gateway URL, they can bypass WAF and our API becomes highly vulnerable to attacks. Hence, in this article we will learn how to lock down direct access to HTTP API Gateway URL. The implementation is very simple and straightforward so let\u2019s get started.HTTP API GatewayLet\u2019s navigate to API Gateway in AWS management console to launch a new HTTP API Gateway and click on the Build button available with the HTTP API container.Fig 1. API Gateway DashboardUpon clicking on the Build button we will be taken to a page that consists multiple steps before we can spin up an API gateway.Step 1 - Create API:Click onAdd IntegrationbuttonSelectHTTPas the integration type andGETas the method typeUnder URL endpoint, I am using a mock endpoint that I created usingbeeceptorwhich returns a fixed responseFinally, give your API gateway a name and click NextFig 2. HTTP API - Create APIStep 2 - Configure Routes:SelectGETas method and/as resource pathFrom the target dropdown, select the Integration that we created in the Step 1 and click NextFig 3. HTTP API - RoutesStep 3 - Define stagesWe can leave$defaultas the stage name and auto-deploy enabled and click NextFig 4. HTTP API - Define stagesIn the last step, review all the inputs and clickCreatebutton if everything looks good.Fig 5. HTTP API - ReviewOur API Gateway is now ready to accept incoming connections but we want to use CloudFront as the entry point to our API as it provides a global endpoint rather than a regional endpoint so let\u2019s create a CloudFront distribution.CloudFrontLet\u2019s navigate to CloudFront console and create a distribution for our API Gateway.Required values for each parameter is mentioned below per section. Optional fields can be left blank or filled as per your requirement.Section 1: OriginOrigin domain:API Gateway domainProtocol:HTTPSMinimum SSL protocol:TLSv1.2 (default)UnderAdd custom header, click on Add header button. We will add a custom header which CloudFront will forward to API Gateway for every request and we will validate the header for every request using Lambda Authorizer that we will create in the next step.Header name can bex-cf-api-gateway-tokenand value will be a long random string making it difficult for hackers to guess. It is not mandatory to choose the header name I suggested earlier so feel free to change it as per your convenience.Section 2: Default cache behaviourCompress objects automatically:YesViewer protocol policy:Redirect HTTP to HTTPSAllowed HTTP methods:GET, HEADRestrict viewer access:NoCache policy:CachingDisabledOrigin request policy:AllViewerExceptHostHeaderNote:We can skip theFunction associations,WAFandSettingssections completely and stick to the default values for these sections for this demo and create the distribution.Fig 6. CloudFront DistributionLet\u2019s also store the above created token in SSM Parameter store as a SecretString so that we can validate it later using Lambda Authorizer.Fig 7. SSM ParameterWe are almost close to achieving our target. Our last step is to create a Lambda Authorizer for our API Gateway that will validate the token received in the custom header for every request and decide whether to allow the request or not.Lambda AuthorizerBefore we create a Lambda function, we need to create an IAM role that will allow the function to read the SSM parameter that we created earlier.assume-role-policy-doc.json{\"Version\":\"2012-10-17\",\"Statement\":[{\"Effect\":\"Allow\",\"Principal\":{\"Service\":\"lambda.amazonaws.com\"},\"Action\":\"sts:AssumeRole\"}]}Enter fullscreen modeExit fullscreen modeaws iam create-role--role-nameapi-cf-demo-role--assume-role-policy-documentfile://assume-role-policy-doc.jsonEnter fullscreen modeExit fullscreen modeNow, let\u2019s create an inline policy that will allow the role to read value from SSM parameter.ssm-access-policy.json{\"Version\":\"2012-10-17\",\"Statement\":[{\"Effect\":\"Allow\",\"Action\":\"ssm:GetParameter\",\"Resource\":\"arn:aws:ssm:AWS_REGION:AWS_ACCOUNT_ID:parameter/SSM_PARAMETER_NAME\"}]}Enter fullscreen modeExit fullscreen modeaws iam put-role-policy--role-nameapi-cf-demo-role--policy-namessm-access--policy-documentfile://ssm-access-policy.jsonEnter fullscreen modeExit fullscreen modeNote:Make sure to replace the placeholders in the above policy with their actual values before creating the inline policy.Along with this inline policy, we also need to attach an AWS managed policy to this role so that lambda can write logs to CloudWatch logs.aws iam attach-role-policy--policy-arnarn:aws:iam::aws:policy/service-role/AWSLambdaBasicExecutionRole--role-nameapi-cf-demo-roleEnter fullscreen modeExit fullscreen modeFig 8. Lambda IAM RoleCool, we now have the IAM role that we need for our Lambda function so let\u2019s navigate to Lambda console and create the function.Feel free to give an appropriate name to the function, selectPython 3.11as the function runtime,x86_64for the architecture and forexecution roleselect the IAM role that we just created.Fig 9. Lambda FunctionOnce the function is created, replace the Lambda code with the below provided code. The below code will read token from the SSM parameter and validate it with the token received from the CloudFront. If both matches, it will return a JSON object that will instruct API Gateway to allow the request.Next, we need to add an environment variable to the Lambda function so go to Configuration > Environment variables and add a new variable with key as SSM_PARAMETER_NAME and value as the name of SSM Parameter that we created earlier to store the token.Fig 10. Lambda Environment VariablesGreat, our Lambda function is ready to be attached to the API Gateway as an authorizer so let\u2019s navigate back to API Gateway and do the same by following the below steps.On the API Gateway page, click onAuthorizationin the left panel and switch toManage authorizerstab.Click on Create authorizer and select Lambda as the type.Give the authorizer a name and select the region you created the lambda function and the lambda function from the dropdown. In case you don\u2019t see a dropdown, start typing the lambda function name and the dropdown should appear.ThePayload format versionandResponse modecan stay as it is but let\u2019s disableAuthorizer cachingand remove the pre-filledIdentity source.UnderInvoke permissionssection, make sure the switch is enabled so that API gateway can automatically create a resource policy and attach it to the lambda function. This policy will allow API gateway to invoke the lambda function.Fig 11. API Gateway Lambda AuthorizerNote:Sometimes API gateway might not add resource policy to the lambda function and because of this we won't get the desired result hence, before proceeding further go the lambda function and verify if the resource policy is attached to it. In case it isn't, run the following command to attach the required resource policy:aws lambda add-permission --function-name FUNCTION_NAME --source-arn arn:aws:execute-api:AWS_REGION:AWS_ACCOUNT_ID:API_GW_ID/authorizers/AUTHORIZER_ID --principal apigateway.amazonaws.com --statement-id AllowAPIGatewayAuthorizerAccess --action lambda:InvokeFunctionFig 12. Lambda Function Resource PolicyNext, let\u2019s attach the authorizer to the route that we create in the beginning so let\u2019s switch to theAttach authorizers to routestab, select the GET path within the route and attach the lambda authorizer that we just created.Fig 13. API Gateway Route AuthorizerThis completes our implementation of locking down direct access to API gateway and preventing users from bypassing WAF implemented on CloudFront. Let\u2019s test the implementation before saying adios.It\u2019s very simple to test the implementation. Simply visit both the API Gateway and the CloudFront URL and you will notice that API Gateway throws Forbidden error whereas CloudFront returns a message.Fig 14. API Gateway Test URLFig 15. CloudFront Test URLBravo! Mission accomplished!Covering the basicsHow do I protect my HTTP API gateway?There are various ways to protect AWS HTTP API Gateway such as attaching SSL certificates, using the latest TLS protocol version, applying API requests throttling, using authorizers,associating WAFand so on.How do I protect my AWS API Gateway from DDoS?AWS API Gateway by default includes basic DDoS protection which is provided viaAWS Shieldand you can further uplift the security of your APIs by attaching WAF and for highest level of protection you can subscribe to AWS Shield Advanced.Which authentication methods can I use to secure API Gateway?AWS HTTP Gateway supports three different ways for authenticating the requests. You can use IAM authorizer and manage access to the API for the users that can access to AWS account. For wider audience, you can create either a JWT authorizer or a lambda authorizer type for custom authentication logic."}
{"title": "Serverless Doesn't Stand Still", "published_at": 1714572000, "tags": ["serverless", "aws", "progress"], "user": "Seth Orell", "url": "https://dev.to/aws-builders/serverless-doesnt-stand-still-194", "details": "Last month, I read an article from Max Rohde titled\"Death by Cloud or How to Build Brittle Systems\". It contained some common complaints about the challenges of working with emerging, ever-changing technology, with a focus on serverless in particular.I don't want to spend a lot of time discussing his specific example, except to say that if you are worried about \"extensive configuration,\" and you deploy your serverless apps withTerraformor raw CloudFormation, you are going to have a bad time. If you use an IaC generator like Serverless Framework, you can have a serverless app running in production in an hour.However, the thing that most stuck with me from this article was its theme, its lamentation of change. Not any particular change, but change itself. I have a different perspective. I think change is good. All progress is change and I want more progress.Change is constant. Change is everywhere. Complaining about change is like shaking your fist at the sky. It's happening in your cloud provider, your customers, and your competitors; it's metaphysical1and you cannot escape it.Changes from Your Cloud ProviderAs an example, the author mentions his experience with AWS Lambda's sliding window of supported NodeJS runtimes. Node 16 was no longer to be supported by AWS Lambda (orby NodeJSfor that matter). Changing the version would require engineering effort that could be spent elsewhere, building new features, or otherwise growing the business. This is all true. However, what I found objectionable is the conclusion reached.The author then goes on to say that this kind of responding-to-change maintenance \"raises questions about sustainability and long-term viability in an environment where change is relentless and often forced upon us by service providers' decisions. [emphasis mine]\" But this is not limited to serverless, as the author initially implied. Every service AWS offers is changing every minute of every day as their teams make a constant stream of improvements2. Nothing stands still.In the name of \"stability,\" perhaps you think you ought to move away from the cloud and its managed services and control your own machines. But, isn't the data center itself changing? New hardware is coming in and old hardware is going out, like a modern-day Ship of Theseus3. Changes are being made to the electric grid. Other data centers/warehouses are being built in the area and are attaching to your Internet provider's backbone. What is staying the same?Can your cloud provider drive change you need to respond to? Absolutely. Will my private data center be immune from change? Not in the least. But, are these the only sources of change we need to contend with? Hardly.Change from CompetitorsLet's imagine for a minute that your cloud provider has frozen everything around your app so you never had to make a change to it. What's happening in your market? Are your competitors standing still? I guarantee (with one caveat below) that they are working hard to introduce a product that will render yours obsolete.Even with a strong customer base who love your product, those customers don't yet know the cool new technology that your competitors are working on. Those of us old enough can remember how loyal Blackberry users were in the early 2000s. The term \"crackberry\" was coined to describe the level of fanaticism the device had on its users. They couldn't live without it\u2026 until Apple introduced the iPhone. Apple didn't stand still for Research in Motion. Does anyone remember MySpace? Facebook does.There are endless examples of innovation (change!) disrupting industries. A few of my favorites are Standard Oil saving the whales4, Thomas Edison disrupting Standard Oil5, Microsoft putting a computer on everyone's desk, and AWS becoming everyone's operations team. Change doesn't stop for anyone.All progress is change and I want more progressAttempts to Stop ChangeThere is one place where change can be suppressed to the point it appears still, and that is where political force is applied in an attempt to halt change. In the United States, it doesn't matter if you have a groundbreaking new idea for 1st-class postal service, you are prevented--by law6--from implementing it. The US Postal Service is a true, government-enforced7, monopoly.Other industries are slightly less protected but are still tightly within the grasp of government control. Two of the most highly regulated industries in the U.S. are banking and air travel8. Let's say you have a highly efficient and effective method for screening your passengers for safety that doesn't involve long lines at the airport. I\u2019m sorry, but you cannot move forward without permission from the TSA and FAA. What if you have a tremendous new technology for people to manage and transfer money that could revolutionize banking? I'm afraid not, you don't have the approval of the OCC, FDIC, FRS, NCUA, and OTS. And that's just the federal level of bank regulators; there are many more at the state level.I have worked in FinTech and have seen firsthand how restrictive the environment is. Half your brain is tied behind your back before you even begin. Your new idea doesn't go to customers to determine success, it goes to regulators and your in-house compliance team. Anything that runs afoul of what the little tyrants of D.C. (or Brussels) have deemed acceptable is out. Change is not welcome9.Ironically (or not), the author cites--as examples of enduring technology--both banking and airlines as industries that still use mainframe computers. Isn't that cool? No. It's pathetic, and we all suffer from their third-rate technological offerings because innovation (change) is barred from entering.Impacts on OwnershipDoes change make ownership easier or harder? Let me sketch in some context first. I'm an advocate for engineering ownership. I think the developers should build it and run it. Is it easier for a developer to run a relatively static mainframe in a data center or to run a constantly evolving Lambda in AWS? There's no comparison.This is why I advocate for serverless. Serverless is currently the easiest way for engineers to build and run applications for their businesses. It's the easiest path to engineering ownership that I've found and I've had great results with teams who adopt this approach.Change HappensI consider myself pro-progress, and progress is change. Without progress, we stagnate, materially and intellectually (material progress is just a manifestation of intellectual progress). I'm excited that AWS is building better services that I can introduce to my clients to build their businesses.The bottom line is that everything great we have right now has displaced (changed!) something less great. There are amazing new things from great minds on their way to us if we let them. Will you let them? I will. Bring on the next great change!that which pertains to reality, to the nature of things, to existence.ref\u21a9AWS News Blog:Announcements\u21a9Wikipedia:Ship of Theseus\u21a9Foundation for Economic Education:How Capitalism Saved the Whales\u21a9Ibid\u21a9Wikipedia:Private Express Statutes\u21a9the only kind of monopoly\u21a9Mercatus Center at George Mason University:The McLaughlin-Sherouse List\u21a9I now refuse to professionally contribute to these highly regulated sectors and will not work for them.\u21a9"}
{"title": "How can you become an AWS Community Builder?", "published_at": 1714558600, "tags": ["aws", "community", "builder", "slack"], "user": "Ahmed Srebrenica", "url": "https://dev.to/aws-builders/how-can-you-become-an-aws-community-builder-29f4", "details": "IntroductionIn January of this year, AWS issued a call to fill out applications for the AWS Community Builders program \u2014 and I applied. I honestly hoped to be accepted because I know how much effort I put in to meet certain criteria. On March 4th, my happiness knew no bounds when I received an email from AWS stating that I was accepted into the AWS Community Builders program. To experience the same joy as me and to gain all the benefits that this program offers, I will give you some tips to increase your chances of being accepted into this prestigious program.What is the AWS Community Builders program?According to AWS, The AWS Community Builders program offers technical resources, education, and networking opportunities to AWS technical enthusiasts and emerging thought leaders who are passionate about sharing knowledge and connecting with the technical community.Interested AWS builders should apply to the program to build relationships with AWS product teams, AWS Heroes, and the AWS community.Throughout the program, AWS subject matter experts will provide informative webinars, share insights \u2014 including information about the latest services \u2014 as well as best practices for creating technical content, increasing reach, and sharing AWS knowledge across online and in-person communities. The program will accept a limited number of members per year. All AWS builders are welcome and encouraged to apply.How to increase your chances of being selected?I will share with you a few tips on how to be selected for this program. I will share exactly what I did:Engage in the community\u2014 I think this is the key thing you need to do. Engaging in the community can involve several things. My way of engaging in the community was mostly through writing articles for Medium and sharing those articles on various platforms. It\u2019s very satisfying when someone reaches out and tells you that your article helped them with a certain problem. My articles usually include some demos, but yours don\u2019t have to be the same as mine; they can be of any nature. Also, engaging in the community can involve attending various lectures, webinars, hands-on workshops, blogs, social media, and forums, and being active in everything I mentioned.Contribute to specific projects\u2014 I remember in the application I wrote for this program, there was a requirement for contribution. It wouldn\u2019t hurt to contribute to specific projects, even if it\u2019s just writing documentation or something simpler.Be active, be a leader, be an ambassador\u2014 AWS wants you to be active and help the community, what does that mean? Join some of the Slack or Discord channels where there are topics related to AWS and help others. For example, someone may have a problem creating an EC2 instance; if that\u2019s not a problem for you, reach out to that person, help them, and explain how to create an EC2 instance.Connect with existing members\u2014 Use social networks to connect with existing members who have been accepted into the AWS Community Builders program. But not only with them, but connect with prominent AWS professionals and people who contribute a lot to the community. Through social networks, share their posts related to AWS that can help the community.Make sure to write a good application\u2014 When writing your application, make sure it\u2019s as good as possible. In my application, I also wrote about things unrelated to AWS and IT in general. Since I was a basketball player, coach, and assistant at the university, I mentioned these experiences to let AWS know that I have experience working with communities.WARNING:Don\u2019t use AI tools in your texts that would write documentation instead of you. It is allowed to use them for code; however, a big NO is placed on everything else. Also, don\u2019t use AI in writing the application for this program, otherwise, you will be rejected.What are the benefits of this program?There are many benefits to this program. Depending on what you need most, that will be the greatest benefit for you. Some of the benefits include a $500 credit to your AWS Account, a SWAG package, becoming part of the AWS Community Builders community on Dev.to, joining the Slack community (with over 3000 users), daily communication in various ways with other AWS Community Builders, and the privilege of attending various lectures by different AWS experts.Personally, for me, the greatest benefit is being part of the Slack community. Why specifically that? I have the opportunity to learn a lot from other AWS Community Builders, both through the content they share and if I ask a question, someone will surely provide an answer for what I\u2019m looking for. I\u2019ve met new people, made new friends, and that is currently the biggest benefit for me. Certainly, the $500 credit is also helpful, but the community is what brings us together, and Slack is the best place for that.ConclusionYou don\u2019t need any special experience to be accepted into this program. It\u2019s enough to put in effort and work, and the right people will notice. Follow my advice and you\u2019ll likely be accepted. The next application cycle is in 2025, so start preparing now. Become part of this prestigious AWS community. PS. If you follow this text and pass the application next year, let me know the result, and we can celebrate with an online coffee.Add your name to the waitlistfor the January 2025 application cycle."}
{"title": "Deploy Rancher on Azure AKS using Azure Cli & Helm Charts", "published_at": 1714551610, "tags": ["azure", "kubernetes", "cloud", "containers"], "user": "Kunal Shah", "url": "https://dev.to/aws-builders/deploy-rancher-on-azure-aks-using-azure-cli-helm-charts-2825", "details": "Azure Cloud Hands on Lab Practice SeriesProject Overview \u2014This project revolves around Azure AKS where we deploy Rancher (platform for Kubernetes management). Rancher is a Kubernetes management tool to deploy and run clusters anywhere and on any provider. Azure AKS & Rancher is very vital combination when it comes to managing multi-cluster Kubernetes workloads from Single Dashboard.SOLUTIONS ARCHITECTURE OVERVIEW -First Let\u2019s understand the real world use case :Hybrid Cloud Management for a Global E-Commerce Platform:Imagine a large e-commerce platform that serves customers globally. The platform\u2019s infrastructure spans multiple regions to ensure low-latency access and high availability. The architecture includes microservices running on Kubernetes clusters managed by AWS EKS. Additionally, the company has on-premises data centers hosting legacy applications. Rancher provides a unified dashboard for managing EKS clusters spread across various regions.Finance: Multi-Tiered Application Deployment:A financial institution is migrating its legacy monolithic applications to microservices architecture on AWS EKS. The institution operates in multiple regions and requires secure and efficient deployment of multi-tiered applications. Rancher simplifies the deployment of microservices across EKS clusters in different regions.Retail: Seasonal Application Scaling:A retail chain experiences significant fluctuations in website traffic during holiday seasons. They need a solution to dynamically scale their applications on EKS to handle increased demand. Rancher enables automated scaling of applications based on predefined policies. Provides visibility into application performance, helping to optimize resource allocation during peak times.Manufacturing: Edge Computing for IoT Devices:A manufacturing company utilizes IoT devices across its facilities to monitor and optimize production processes. They need a solution to manage Kubernetes clusters at the edge for real-time data processing. Rancher supports the deployment of Kubernetes clusters at the edge, close to IoT devices. It also enables centralized management of edge clusters for easier monitoring and updates.Media and Entertainment: Content Delivery Optimization:A media streaming service operates globally and needs to optimize content delivery for users. They want to deploy and manage Kubernetes clusters efficiently to ensure low-latency streaming. It Integrates with AWS services like Amazon CloudFront for efficient content caching. Allows for easy scaling and updating of streaming applications across clusters. Facilitates the deployment of edge-native applications to process data locally.These diverse use cases demonstrate the versatility of Rancher onAZURE AKSin addressing industry-specific challenges and enhancing the management of Kubernetes clusters in various contexts.Ranchersimplifies operations, enhances security, and provides a unified platform for managing the hybrid cloud environment.Prerequisite \u2014AZURE ACCOUNT with valid Subscription ID.Resource Group for creating Azure resources.AZURE CLI(on local machine)HELMKUBECTLAzure Services Usage \u2014Azure Resources GroupAzure AKSAzure VnetAzure LoadbalancerAzure Managed IdentityVirtual Machine Scale SetSTEP BY STEP GUIDE -STEP 1 : Setting environment variablesStart the new terminal on local machine (linux/ubuntu/wsl)Set following env variables :AZURE_LOCATION=Azure Region (example uaenorth)CERT_MANAGER_VERSION=v1.12.5 (Cert Manager Version)KUBERNETES_VERSION=v1.26.6 (K8s Version)NODE_COUNT=2 (Number of Nodes required)RANCHER_VERSION=2.7.6 (Rancher Manager Version)RESOURCE_PREFIX=aks-demo (Name of Resource Group in Azure)SUBSCRIPTION_ID=XXXXXXX-XXXXXX (Subscription ID of Azure)VM_SIZE=Standard_D2s_v3 (Azure VM Instance Type)EMAIL_ADDRESS=youremail@domain.com(Azure account email)STEP 2 : Add Helm Repositorieshelm repo add devprohttps://devpro.github.io/helm-chartsThis repository contains Helm charts to build clusters with all components running in containers.helm repo add jetstackhttps://charts.jetstack.iohelm repo add ingress-nginxhttps://kubernetes.github.io/ingress-nginxhelm repo add rancher-latesthttps://releases.rancher.com/server-charts/latesthelm repo updatehelm repo listSTEP 2 : Login to Azure through Azure CLI for AuthenticationRunaz loginon local terminal (It will open a window in your browser automatically.)If not then you will get a link in CLI output, copy that generated link & paste in your browser to authenticate.Set Subscription if you have multiple subscription. (optional if you have only one subscription)az account set \u2014subscription $SUBSCRIPTION_IDCreate the resource group. (optional if you have already created)az group create \u2014name $RESOURCE_PREFIX \u2014location ${AZURE_LOCATION}STEP 3 : Creating Kubernetes Cluster (AKS) in NEW VNETCASE 1: AKS in new VNETaz aks create \u2014resource-group $RESOURCE_PREFIX \u2014name $RESOURCE_PREFIX \u2014kubernetes-version $KUBERNETES_VERSION -node-count $NODE_COUNT \u2014node-vm-size $VM_SIZECASE 2: AKS in existing VNETaz aks create \u2014resource-group $RESOURCE_PREFIX \u2014name $RESOURCE_PREFIX \u2014kubernetes-version $KUBERNETES_VERSION -node-count $NODE_COUNT \u2014node-vm-size $VM_SIZE \u2014network-plugin azure -vnet-subnet-id/subscriptions/$SUBSCRIPTION_ID/resourceGroups/$RESOURCE_PREFIX/providers/Microsoft.Network/virtualNetworks/AZURE_VNET_NAME/subnets/AZURE_SUBNET_NAMEWe are demonstratingCASE 1\u2014 NEW VNETAfter running the command fromCASE 1\u2014 This will take some time to create AKS & store ssh keys at your/home/username/.sshThese keys are used to ssh into the worker nodes (VMs)Once the AKS is ready it will give you cluster information in the CLI output.STEP 4 : Access the AKS Cluster through kubectllet\u2019s add cluster credentials to local config of kubectl.az aks get-credentials \u2014 resource-group $RESOURCE_PREFIX -name $RESOURCE_PREFIXCheck the nodes & all resources using kubectl.kubectl get nodeskubectl get all -ATIP\u2014 If you face any permission issue then change permission for ~/.kube/configVanilla Cluster on AKS is ready with requested worker nodes.STEP 5 : Ngnix Ingress Installation (exposing to Internet)Run below mentioned commands from local machine.helm upgrade \u2014install ingress-nginx ingress-nginx/ingress-nginx \u2014namespace ingress-nginx \u2014create-namespace \u2014set controller.service.annotations.\u201dservice\\.beta\\.kubernetes\\.io/azure-load-balancer-health-probe-request-path\u201d=/healthzCheck the services:kubectl get services -n ingress-nginxLet\u2019s store value of Public IP which needs to be used at the time of Rancher Installation.PUBLIC_IP_NGINX=kubectl get service -n ingress-nginx ingress-nginx-controller \u2014output jsonpath=\u2019{.status.loadBalancer.ingress[0].ip}\u2019echo$PUBLIC_IP_NGINX(It should give Public IP)STEP 6 : Install Certificates manager.Run below mentioned commands from local machine.kubectl apply -fhttps://github.com/cert-manager/cert-manager/releases/download/$CERT_MANAGER_VERSION/cert-manager.crds.yamlhelm upgrade \u2014install cert-manager jetstack/cert-manager \u2014namespace cert-manager \u2014create-namespace \u2014version $CERT_MANAGER_VERSIONkubectl get pods \u2014 namespace cert-manager ( 3 pods should be in Running status)helm upgrade \u2014install letsencrypt devpro/letsencrypt \u2014set registration.emailAddress=$EMAIL_ADDRESS \u2014namespace cert-managerkubectl get clusterissuer -n cert-manager ( 2 Cluster issuers should be True)STEP 7 : Rancher Installation.Run below mentioned commands from local machine.kubectl create namespace cattle-systemhelm upgrade \u2014install rancher rancher-latest/rancher \u2014namespace cattle-system \u2014set hostname=rancher.$PUBLIC_IP_NGINX.sslip.io -set \u2018ingress.extraAnnotations.cert-manager\\.io/cluster-issuer=letsencrypt-prod\u2019 \u2014set ingress.ingressClassName=nginx \u2014set ingress.tls.source=secret \u2014set ingress.tls.secretName=tls-rancher \u2014set replicas=2 \u2014version $RANCHER_VERSIONCheck the status of installation & wait for it to complete.kubectl -n cattle-system rollout status deploy/rancherFetch the generated password & copy itkubectl get secret \u2014namespace cattle-system bootstrap-secret -o go-template=\u2019 .data.bootstrapPassword|base64decode\u201c\\n\u201d\u2019STEP 8 : Accessing the Rancher UI :echohttps://rancher.$PUBLIC_IP_NGINX.sslip.io/Copy URL & Paste on browser for Initial setup of Rancher.Check the logs\u2192 kubectl logs -n ingress-nginx -lapp.kubernetes.io/component=controllerYou will be asked for password ( copied earlier )Hit Log in with Local User.Define the admin password, check the box and click on \u201cContinue\u201d. (store it)Finally Rancher is running and you can explore \u201clocal\u201d cluster.STEP 10 : DecommissionRun below command to destroy all resources.kubectl delete ns cert-managerkubectl delete ns ingress-nginxDecommission the resources under the Azure Resources group.Delete the Resources Group at the end.Congrats ! We have successfully completed lab for Deploying Rancher on Azure AKS using Azure CLI & Helm Charts.I am Kunal Shah, AWS Certified Solutions Architect, helping clients to achieve optimal solutions on the Cloud. Cloud Enabler by choice, DevOps Practitioner having 9+ Years of overall experience in the IT industry.I love to talk about Cloud Technology, DevOps, Digital Transformation, Analytics, Infrastructure, Dev Tools, Operational efficiency, Serverless, Cost Optimization, Cloud Networking & Security.aws #community #builders #devops #azure #aks #managed #kubernetes #solution #rancher #solution #management #centralize #dashboard #easy #management #scalability #operational #efficiency #robust #infrastructure #highly #available #reliable #controlled #design #acloudguyYou can reach out to me @acloudguy.in"}
{"title": "Building a Mass Emailing System Using AWS Lambda and SES", "published_at": 1714546848, "tags": [], "user": "Arjun Menon", "url": "https://dev.to/aws-builders/building-a-mass-emailing-system-using-aws-lambda-and-ses-21eo", "details": "A mass emailing system allows businesses to reach out to customers efficiently, and building one using Amazon Web Services (AWS) can be both scalable and cost-effective. In this blog post, I'll guide you through setting up a mass emailing system using AWS Lambda, Simple Email Service (SES), and S3.Step 1: Create an S3 BucketAmazon S3 will store the CSV files containing the email addresses of the recipients.Navigate to S3:In the AWS Management Console, click on S3 under the Storage category.Create a Bucket:Click on Create bucket. Name your bucket (it must be unique across AWS), select a region close to you for better performance, and then create the bucket.Step 2: Set Up AWS Simple Email Service (SES)AWS SES will handle the email sending operations.Navigate to SESVerify Your Email Address:Before sending emails, SES requires verification of your ownership of the email address. On the left pane, go toIdentities,and then verify the email address you want to send from.Important Note: As we are using SES in sandbox mode, you must manually verify each email address that you wish to send emails to before they can receive emails from your application. This is a restriction in the sandbox environment intended to prevent spam and abuse.Add and verify all Recipient Email Addresses:You can use Temp Mail for testing.Step 3: Create an IAM Role for LambdaThis role will allow your Lambda function to access AWS resources securely.Navigate to IAM:Go to IAM under Security, Identity, & Compliance.Create a New Role:Click on Roles, then Create role. Choose Lambda for the service that will use this role, then proceed to permissions.Attach Policies:Add policies like AmazonSESFullAccess, AmazonS3FullAccess, and AWSLambdaExecute.Step 4: Create a Lambda FunctionThis function will process the CSV file and send emails.Navigate to Lambda:Go to Lambda under Compute.Create Function:Choose Author from scratch, name your function, select Python 3.x for the runtime.Set Permissions:Choose the IAM role you created earlier.Create Function:Finalize the creation of your function.Step 5: Write Lambda Function CodeThe function will read from S3, parse CSV, and send emails via SES.import boto3import csvdef lambda_handler(event, context): s3 = boto3.client('s3') ses = boto3.client('ses') bucket_name = event['Records'][0]['s3']['bucket']['name'] key = event['Records'][0]['s3']['object']['key'] response = s3.get_object(Bucket=bucket_name, Key=key) lines = response['Body'].read().decode('utf-8').split() for row in csv.DictReader(lines): ses.send_email( Source='your-verified-email@example.com', Destination={'ToAddresses': [row['email']]}, Message={ 'Subject': {'Data': 'Your Email Subject'}, 'Body': {'Text': {'Data': \"Your email content\"}} } )Enter fullscreen modeExit fullscreen modeStep 6: Set Up S3 Event TriggerTrigger your Lambda function when a CSV file is uploaded to S3.Select Your Function:In Lambda, open your functions configuration.Add Trigger:Choose S3 from the list. Configure the trigger for PUT events, and set the file prefix or suffix (like .csv).Save:Save your configuration.Step 7: Test Your SetupUpload a CSV file with an email column to your S3 bucket and check the Lambda execution logs. Make sure the emails are being sent out successfully.Email Recieved:Reminder: As we are using SES in sandbox mode, you must manually verify each email address that you wish to send emails to before they can receive emails from your application. This is a restriction in the sandbox environment intended to prevent spam and abuse.ConclusionYou've now successfully set up a basic mass emailing system using AWS. This system is not only scalable but also cost-effective, perfect for businesses looking to manage their email marketing efforts. Always ensure to comply with legal requirements and manage opt-ins and opt-outs to respect user privacy and avoid spam."}
{"title": "How To Manage an Amazon Bedrock Agent Using Terraform", "published_at": 1714537850, "tags": ["aws", "terraform", "ai"], "user": "Anthony Wat", "url": "https://dev.to/aws-builders/how-to-manage-an-amazon-bedrock-agent-using-terraform-1lag", "details": "IntroductionIn the previous blog postBuilding a Basic Forex Rate Assistant Using Agents for Amazon Bedrock, I demonstrated how to create a Bedrock agent in the AWS Management Console and outlined some ideas on improving the solution. Before further experimentation, it makes sense to automate the deployment of the solution to enable quicker updates as we go through trail and error in fine-tuning an agent.In this blog post, we will automate the deployment of the basic forex rate assistant in Terraform using the resources that were recently released inv5.47.0 of the Terraform AWS Provider. Let's start by looking at the AWS resources in the AWS Management Console.Taking inventory of the required resourcesBy examining the agent we previously built, we see that it is comprised of the following AWS resources:TheagentitselfTheagent resource rolewhich is an IAM service role that provides the agent with access to other AWS services and resourcesTheaction groupthat defines API actions that the agent can performTheLambda functionassociated with the action group, which itself requires anexecution roleand aresource policythatallows the agent to invoke the functionWith the list of resources we need to provision, we can begin creating the Terraform configuration starting with the resources that the agent depends on.Defining resources for the IAM and Lambda dependenciesFor the agent resource role, thedocumentationalready provides the trust policy and the permissions required. It also specifies that the prefixAmazonBedrockExecutionRoleForAgents_must be used for the role name.The permission requires the foundational model's ARN, so we need at least the model's ID, which in our case isanthropic.claude-3-haiku-20240307-v1:0for Claude 3 Haiku. For consistency, we will use theaws_bedrock_foundational_modeldata sourceto look up its ARN. Thus we can define the Terraform configuration for the agent resource role as follows using theaws_iam_roleresourceand theaws_iam_role_policyresource:# Use data sources to get common information about the environmentdata\"aws_caller_identity\"\"this\"{}data\"aws_partition\"\"this\"{}data\"aws_region\"\"this\"{}locals{account_id=data.aws_caller_identity.this.account_idpartition=data.aws_partition.this.partitionregion=data.aws_region.this.name}data\"aws_bedrock_foundation_model\"\"this\"{model_id=\"anthropic.claude-3-haiku-20240307-v1:0\"}# Agent resource roleresource\"aws_iam_role\"\"bedrock_agent_forex_asst\"{name=\"AmazonBedrockExecutionRoleForAgents_ForexAssistant\"assume_role_policy=jsonencode({Version=\"2012-10-17\"Statement=[{Action=\"sts:AssumeRole\"Effect=\"Allow\"Principal={Service=\"bedrock.amazonaws.com\"}Condition={StringEquals={\"aws:SourceAccount\"=local.account_id}ArnLike={\"aws:SourceArn\"=\"arn:${local.partition}:bedrock:${local.region}:${local.account_id}:agent/*\"}}}]})}resource\"aws_iam_role_policy\"\"bedrock_agent_forex_asst\"{name=\"AmazonBedrockAgentBedrockFoundationModelPolicy_ForexAssistant\"role=aws_iam_role.bedrock_agent_forex_asst.namepolicy=jsonencode({Version=\"2012-10-17\"Statement=[{Action=\"bedrock:InvokeModel\"Effect=\"Allow\"Resource=data.aws_bedrock_foundation_model.this.model_arn}]})}Enter fullscreen modeExit fullscreen modeNext, we will define the Lambda execution role which just needs the basic permissions to write logs to CloudWatch that the AWS-managed IAM policyAWSLambdaBasicExecutionRoleprovides. The Terraform configuration for this IAM role can be defined as follows:data\"aws_iam_policy\"\"lambda_basic_execution\"{name=\"AWSLambdaBasicExecutionRole\"}# Action group Lambda execution roleresource\"aws_iam_role\"\"lambda_forex_api\"{name=\"FunctionExecutionRoleForLambda_ForexAPI\"assume_role_policy=jsonencode({Version=\"2012-10-17\"Statement=[{Action=\"sts:AssumeRole\"Effect=\"Allow\"Principal={Service=\"lambda.amazonaws.com\"}Condition={StringEquals={\"aws:SourceAccount\"=\"${local.account_id}\"}}}]})managed_policy_arns=[data.aws_iam_policy.lambda_basic_execution.arn]}Enter fullscreen modeExit fullscreen modeWe will then define the Terraform configuration for the Lambda function and its resource policy. Here is the source code for the Forex API Lambda function from the previous blog post for reference:importjsonimporturllib.parse# urllib is available in Lambda runtime w/o needing a layerimporturllib.requestdeflambda_handler(event,context):agent=event['agent']actionGroup=event['actionGroup']apiPath=event['apiPath']httpMethod=event['httpMethod']parameters=event.get('parameters',[])requestBody=event.get('requestBody',{})# Read and process input parameterscode=Noneforparameterinparameters:if(parameter[\"name\"]==\"code\"):# Just in case, convert to lowercase as expected by the APIcode=parameter[\"value\"].lower()# Execute your business logic here. For more information, refer to: https://docs.aws.amazon.com/bedrock/latest/userguide/agents-lambda.htmlapiPathWithParam=apiPath# Replace URI path parametersifcodeisnotNone:apiPathWithParam=apiPathWithParam.replace(\"{code}\",urllib.parse.quote(code))# TODO: Use a environment variable or Parameter Store to set the URLurl=\"https://cdn.jsdelivr.net/npm/@fawazahmed0/currency-api@latest/v1{apiPathWithParam}.min.json\".format(apiPathWithParam=apiPathWithParam)# Call the currency exchange rates API based on the provided path and wrap the responseapiResponse=urllib.request.urlopen(urllib.request.Request(url=url,headers={\"Accept\":\"application/json\"},method=\"GET\"))responseBody={\"application/json\":{\"body\":apiResponse.read()}}action_response={'actionGroup':actionGroup,'apiPath':apiPath,'httpMethod':httpMethod,'httpStatusCode':200,'responseBody':responseBody}api_response={'response':action_response,'messageVersion':event['messageVersion']}print(\"Response: {}\".format(api_response))returnapi_responseEnter fullscreen modeExit fullscreen modeWe will save this source code into a file calledindex.pyin thelambda/forex_apidirectory in the same directory as the Terraform configuration, which will be packaged as a zip file using thearchive_filedata sourceto pass as an argument to theaws_lambda_functionresource.Here is the Terraform configuration for the Lambda function based on my battle-tested templates:# Action group Lambda functiondata\"archive_file\"\"forex_api_zip\"{type=\"zip\"source_file=\"${path.module}/lambda/forex_api/index.py\"output_path=\"${path.module}/tmp/forex_api.zip\"output_file_mode=\"0666\"}resource\"aws_lambda_function\"\"forex_api\"{function_name=\"ForexAPI\"role=aws_iam_role.lambda_forex_api.arndescription=\"A Lambda function for the forex API action group\"filename=data.archive_file.forex_api_zip.output_pathhandler=\"index.lambda_handler\"runtime=\"python3.12\"# source_code_hash is required to detect changes to Lambda code/zipsource_code_hash=data.archive_file.forex_api_zip.output_base64sha256}Enter fullscreen modeExit fullscreen modeLastly, we will set the Lambda resource policy using theaws_lambda_permissionresourceaccording to the specifications in the documentation:resource\"aws_lambda_permission\"\"forex_api\"{action=\"lambda:invokeFunction\"function_name=aws_lambda_function.forex_api.function_nameprincipal=\"bedrock.amazonaws.com\"source_account=local.account_idsource_arn=\"arn:aws:bedrock:${local.region}:${local.account_id}:agent/*\"}Enter fullscreen modeExit fullscreen modeDefining the agent and action group resourcesWith the dependencies out of the way, we can now define the Terraform resource for the agent with the newaws_bedrockagent_agentresource, which is rather straightforward:resource\"aws_bedrockagent_agent\"\"forex_asst\"{agent_name=\"ForexAssistant\"agent_resource_role_arn=aws_iam_role.bedrock_agent_forex_asst.arndescription=\"An assisant that provides forex rate information.\"foundation_model=data.aws_bedrock_foundation_model.this.model_idinstruction=\"You are an assistant that looks up today's currency exchange rates. A user may ask you what the currency exchange rate is for one currency to another. They may provide either the currency name or the three-letter currency code. If they give you a name, you may first need to first look up the currency code by its name.\"}Enter fullscreen modeExit fullscreen modeThe action group can be defined in the agent using theaws_bedrockagent_action_groupresource. We will need theOpenAPI schemaYAML file from the previous blog post, which is included below for reference:openapi:3.0.0info:title:Currency APIdescription:Provides information about different currencies.version:1.0.0servers:-url:https://cdn.jsdelivr.net/npm/@fawazahmed0/currency-api@latest/v1paths:/currencies:get:description:|List all available currenciesresponses:\"200\":description:Successful responsecontent:application/json:schema:type:objectdescription:|A map where the key refers to the lowercase three-letter currency code and the value to the currency name in English.additionalProperties:type:string/currencies/{code}:get:description:|List the exchange rates of all available currencies with the currency specified by the given currency code in the URL path parameter as the base currencyparameters:-in:pathname:coderequired:truedescription:The lowercase three-letter code of the base currency for which to fetch exchange ratesschema:type:stringresponses:\"200\":description:Successful responsecontent:application/json:schema:type:objectdescription:|A map where the key refers to the three-letter currency code of the target currency and the value to the exchange rate to the target currency.additionalProperties:type:numberformat:floatEnter fullscreen modeExit fullscreen modeWe will save the file asschema.yamlin thelambda/forex_apidirectory for the Lambda function, since they somewhat go together. Since we are providing the OpenAPI schema in-line, the Terraform resource can be defined as follows:resource\"aws_bedrockagent_agent_action_group\"\"forex_api\"{action_group_name=\"ForexAPI\"agent_id=aws_bedrockagent_agent.forex_asst.idagent_version=\"DRAFT\"description=\"The currency exchange rates API\"skip_resource_in_use_check=trueaction_group_executor{lambda=aws_lambda_function.forex_api.arn}api_schema{payload=file(\"${path.module}/lambda/forex_api/schema.yaml\")}}Enter fullscreen modeExit fullscreen modeTesting the configurationNow that the full Terraform configuration is developed, we can apply it and make sure that it is working correctly. For me it took less than a minute to complete - here is the output for reference:aws_iam_role.bedrock_agent_forex_asst: Creating... aws_iam_role.lambda_forex_api: Creating... aws_iam_role.bedrock_agent_forex_asst: Creationcompleteafter 0s[id=AmazonBedrockExecutionRoleForAgents_ForexAssistant] aws_iam_role_policy.bedrock_agent_forex_asst: Creating... aws_bedrockagent_agent.forex_asst: Creating... aws_iam_role.lambda_forex_api: Creationcompleteafter 1s[id=FunctionExecutionRoleForLambda_ForexAPI] aws_lambda_function.forex_api: Creating... aws_iam_role_policy.bedrock_agent_forex_asst: Creationcompleteafter 1s[id=AmazonBedrockExecutionRoleForAgents_ForexAssistant:AmazonBedrockAgentBedrockFoundationModelPolicy_ForexAssistant] aws_bedrockagent_agent.forex_asst: Creationcompleteafter 4s[id=LTR1P1OJUC] aws_lambda_function.forex_api: Still creating...[10s elapsed] aws_lambda_function.forex_api: Creationcompleteafter 14s[id=ForexAPI] aws_lambda_permission.forex_api: Creating... aws_bedrockagent_agent_action_group.forex_api: Creating... aws_lambda_permission.forex_api: Creationcompleteafter 0s[id=terraform-20240430193700768300000002] aws_bedrockagent_agent_action_group.forex_api: Creationcompleteafter 0s[id=W1PDUUCT8P,LTR1P1OJUC,DRAFT]  Applycomplete!Resources: 7 added, 0 changed, 0 destroyed.Enter fullscreen modeExit fullscreen modeIn the Bedrock console, we can see that the agentForexAssistantis ready for testing. Using the test chat interface, I asked:What is the exchange rate from US Dollar to Canadian Dollar?However, I got the following unexpected answer:I apologize, but I am unable to look up the current exchange rate between US Dollar and Canadian Dollar. There seems to be an issue with the function call format that I am unable to resolve. I cannot provide the exchange rate information you requested.Looking at thetrace, it seems that the agent was not given the tool list and it tried to make up random functions to call, leading to errors:On closer look, it seems that this is because there are pending changes in the agent which is requirespreparationas indicated in the Bedrock console:This tells me that Terraform is not performing the preparation. In any case, once I clickPrepareand ask the same question again in a new session, the agent responds with the currency exchange rate I asked for:The exchange rate from US Dollar (USD) to Canadian Dollar (CAD) is 1 USD = 1.36660199 CAD.This is also confirmed in the trace which I will not show for brevity. Now we are one step away from an end-to-end IaC solution for the forex rate assistant, so let's try to address the issue.Workaround for agent preparation using a null resource\ud83d\udca12024-05-23:As of Terraform AWS Providerv5.49.0, theaws_bedrockagent_agentresource has aprepare_agentargument (trueby default) that controls whether the agent is prepared after the agent is created or updated. The Terraform configuration in the GitHub repository has been updated to account for this enhancement. However, the null resource is still required for action groups sinceaws_bedrockagent_action_groupstill does not prepare the agent.Looking at the Terraform AWS Provider documentation, I couldn't find any resource that supports preparation. As well, theaws_bedrockagent_agentresourceand theaws_bedrockagent_action_groupresourcedon't seem to have any argument that controls the preparation behavior. To be fair, the action is implemented as a separate API action calledPrepareAgentin the Agents for Bedrock API, which does not directly fit into the resource concept in Terraform.While I opened anissuein thehashicorp/terraform-provider-aws GitHub repository, one quick workaround I can think of is to use anull resourcewith thelocal-exec provisionerto run the equivalent AWS CLI command for the PrepareAgent API, which is theaws bedrock-agent prepare-agentcommand.Our objective is to trigger this null resource to be rerun (technically replaced) every time there are changes to the agent, which also extends to the action group. It is inefficient to simply prepare every time you apply the Terraform configuration, and if anything it is just one more moving part that can break. With that in mind, I devised the following resource that serve the purpose well.resource\"null_resource\"\"forex_asst_prepare\"{triggers={forex_asst_state=sha256(jsonencode(aws_bedrockagent_agent.forex_asst))forex_api_state=sha256(jsonencode(aws_bedrockagent_agent_action_group.forex_api))}provisioner\"local-exec\"{command=\"aws bedrock-agent prepare-agent --agent-id${aws_bedrockagent_agent.forex_asst.id}\"}depends_on=[aws_bedrockagent_agent.forex_asst,aws_bedrockagent_agent_action_group.forex_api]}Enter fullscreen modeExit fullscreen modeAs you can see, I am using thetriggersargumentin the null resource to control when the resource should be replaced. We target the two main sources of change, which is the agent and the action group. Since trigger requires a string, a good candidate is to use the two resource's state somehow, as long as it doesn't contain any attributes that change every time Terraform is run. To keep the string short, we simply derive the SHA256 checksum from the resource state JSON as the triggers. The local-exec provisioner simply calls the AWS CLI command with the agent ID fromaws_bedrockagent_agent.forex_asst.With this change, we will runterraform destroyand thenterraform applyto ensure full validity of the re-test. After Terraform completes successfully, we first check the agent in the Bedrock console to ensure that thePreparebutton is no longer shown. As well, we ask our question to hopefully receive an expected result, which we did:So there you have it, a functional Terraform configuration to deploy a basic forex rate assistant implemented using Agents for Amazon Bedrock!\u2705 For reference, I've dressed up the Terraform solution with variables and such, and checked in the final artifacts to the1_basicdirectory inthis repository. Feel free to check it out and use it as the basis for your Bedrock experimentation.Current limitations (it's brand new after all)It is not unexpected that we encounter some issues with brand new features, such as what we encountered in this blog post with the Agents for Amazon Bedrock resources. I myself dove a bit deeper and found a few more issues which I reported. I encourage you toreport any issuesthat you see as you work more with the Terraform resources.Meanwhile, there are still a couple resources related toKnowledge bases for Amazon Bedrockstill under development. I plan to integrate knowledge bases to our forex rate assistant, so I will eagerly wait for the Terraform resources to be ready for my next step in my Bedrock journey.SummaryIn this blog post, we developed the Terraform configuration for the basic forex rate assistant that we created interactively in the blog postBuilding a Basic Forex Rate Assistant Using Agents for Amazon Bedrock. While we encountered some issues, we were able to work around it as the community continues to build out the features in the Terraform AWS Provider. For now, I will pivot to enhancing the forex rate agent to add new capabilities and to address some of its known shortcomings.If you like this blog post, please be sure to check out other helpful articles on AWS, Terraform, and other DevOps topics in theAvangards Blog."}
{"title": "Say Goodbye to Manual Deployments: Automate Your EC2 Autoscaling with CodeDeploy and GitHub Actions", "published_at": 1714456133, "tags": ["aws", "ec2", "github", "codedeploy"], "user": "Avinash Dalvi", "url": "https://dev.to/aws-builders/say-goodbye-to-manual-deployments-automate-your-ec2-autoscaling-with-codedeploy-and-github-actions-4ijp", "details": "Hello Developers,In this blog, I am going to share my learnings on how to deploy code changes to EC2 autoscaling instances using CodeDeploy and GitHub Actions. But first, let's understand the background behind this.One of my community friends reached out to me asking for help to automate Amazon EC2 deployment, which involves autoscaling. He explained that he was doing these steps manually by first pulling code changes to one of the EC2 instances and then creating an AMI (Amazon Machine Image). Later, he initialised that AMI to the Auto Scaling group to launch a new instance with the updated code changes. This process was consuming a lot of manual time.I started working on this problem and tried to search for solutions. I knew about ECS task processes but had never automated EC2 deployment before. It sounds interesting to me.Let's check out how to do this.Few concepts need to know before dive into actual solution. I am considering everyone must be aware about Amazon EC2.CodeDeploy- CodeDeploy is an Amazon deployment service that automates application deployments to Amazon EC2 instances, on-premises instances, serverless Lambda functions, or Amazon ECS services.Github Actions -GitHub Actions is a continuous integration tool that operates based on your GitHub code lifecycle, such as code push, commit, PR merge, branch creation, and tag creation. You can utilize this hook to trigger other actions like Jenkins jobs, AWS services, and more.I assume you already have an EC2 setup running under an autoscaling group with a load balancer. I will skip that section. If you need more details on how to create this, watch this video:https://www.youtube.com/watch?v=Ekgi2HfnJcw.To achieve solutions, let's ensure that your setup covers the following points.EC2 instance has an IAM profile attached. If not, edit it in the Autoscaling template and then reinitiate the EC2 instances. IAM profile should have access to EC2 service.The EC2 AMI image should be configured with the CodeDeploy agent. For instructions on how to install the CodeDeploy agent, refer to the following link:https://docs.aws.amazon.com/codedeploy/latest/userguide/codedeploy-agent-operations-install-linux.htmlSetting up CodeDeploy with GitHub ActionsCreate a CodeDeploy Application :Go to the AWS console -> CodeDeploy -> Create Application. Name your application something like \"TestApplication\" and choose \"EC2/on-premise\" for the Compute platform option.Open the application \"TestApplication\".Create a Deployment group :We need an IAM role for this service. First, go to IAM -> Create a role -> Let's name it \"CodeDeployTestApplication\". Attach policies \"AmazonEC2RoleforAWSCodeDeploy\" and \"AWSCodeDeployRole\".Click on \"Create deployment group\". Fill in the deployment group name. Next, you will have to choose the IAM role for this service that we created in step 3.Under \"Deployment Type,\" choose \"In place\" for rolling out to all instances. If you prefer \"Blue/Green Deployment,\" you can select that option. You can learn more about \"Blue/Green Deployment\"here.Under \"Environment configuration,\" select \"Amazon EC2 Auto Scaling groups\" because we need to deploy code to all instances initiated by Auto Scaling groups.For \"Deployment settings,\" choose the option that suits you best or create your own configuration. In production, you can use \"OneAtATime.\"Next, under \"Load balancer,\" you can ignore this if you already have a load balancer attached to your EC2 instances. The options under \"Advanced\" are optional, so we can skip those.Create a deployment :Click on the \"Deployment group\" created in the above step. Then, click on \"Create Deployment.\"You have to choose the \"Revision Type\" to decide how you want to deploy the code, either from an S3 bucket or from GitHub. This blog will demonstrate the process using GitHub.To choose this option, you need to create a GitHub token that can access your GitHub code repository (Read access). You can find instructions on how to create a GitHub Tokenhere. After creating the token, paste it under \"GitHub token name.\" You can either use the GitHub token or simply type your GitHub username in the search box, then the \"Connect to GitHub\" option will be enabled.If you select \"Connect to GitHub,\" authorize GitHub, and choose the repository you wish to deploy.Next, select the \"Commit ID\" that you want to push to EC2 instances.Decide on the behaviour you require under \"Additional deployment behavior settings.\" This is for file operations.You can keep the same configuration under \"Deployment group overrides\" or override it for a specific deployment.If you want to keep the \"Roll back configuration overrides,\" choose enabled; otherwise, skip this step. Then click on \"Create Deployment.\" Then you can track progress in status page. If would like to know details status click on \"Deployment ID\". Then you can view list of instances for which deployment is going.I used a single instance to save time, but if you have multiple instances running under an autoscaling group, they will be displayed here for you to track progress. Click on \"View Events\" to check for any errors if the deployment fails.For details error log you can run this command inside EC2 instance. It will help you to fix issue based on logs details.tail-f/var/log/aws/codedeploy-agent/codedeploy-agent.logEnter fullscreen modeExit fullscreen modeThis step is for manual deployment. However, we are here to learn about automation and avoid manual processes. GitHub Actions will assist in skipping this deployment step.Setup Github Actions :We require two files in your Github code repository to achieve automation.appspecs.ymldeploy.yml under \".github/workflows\" folder. If don't have folder create folder and file.Create a appspecs.yml :Create theappspecs.ymlfile in the root folder. Infiles->source, specify the folder from which you want to copy files; it can be the root folder or a subfolder.Infiles->destination, choose the folder where you want to move the files. In my case, since I installed an Apache server, the application folder is/var/www/html/, so I selected that. If you are running a Node server, a Go server, or any other server, choose the appropriate folder accordingly.Use theoverwriteflag if you wish to overwrite existing files.Inbranch_config, select the branch you would like to deploy whenever there is a code push. Keep the same branch that we are going to use indeploy.yml.Pass parameters likedeploymentGroupNameanddeploymentGroupConfig; you can get these from Code Deploy -> Application -> Select Deployment group -> copy the name and ARN.version:0.0os:linuxfiles:-source:webapp-sample/destination:/var/www/html/overwrite:truefile_exists_behavior:OVERWRITEbranch_config:main:deploymentGroupName:TestDeploymentGroupdeploymentGroupConfig:serviceRoleArn:arn:aws:iam::ACCOUND_ID:role/CodeDeployTestApplicationEnter fullscreen modeExit fullscreen modeCreate a deploy.yml:Give a name to your deployment.Choose which lifecycle hook you want to listen to. I select to trigger this GitHub action whenever a new commit is made to the main branch.You can find a list of GitHub Actions hooks hereUnderjobsname job name whichever you wish. I gavebuild-push-deployit is customised.runs-oncan select either self hosted or ubuntu-latest.Create a GitHub environment in your repository settings because you need to store your AWS secret key and access key. You can learn how to create GitHub environment variableshere.so that all variable which start fromsecrets.will read from that environment.Next is definedstepswhich you would like to setup as part of this deployment process. Checkout code is mandatory which checkout code in instance which we selected underruns-onSelect second step as \"Configure AWS credentials\". And pass variable values from secrets.Next step it use CodeDeploy utility. For that instead ofnameusingidbecause we need to grab outputs values from this steps like thissteps.deploy.outputs.deploymentId. Pass variable underwith->applicationyour CodeDeploy application name.name:Push code to EC2 imaageon:push:branches:mainjobs:build-push-deploy:runs-on:ubuntu-latestenvironment:AWS_Environmentsteps:-name:Checkout Codeuses:actions/checkout@v4-name:List directoryrun:ls -al-name:\"ConfigureAWSCredentials\"uses:aws-actions/configure-aws-credentials@v4.0.2with:aws-region:us-east-1aws-access-key-id:${{ secrets.AWS_ACCESS_KEY }}aws-secret-access-key:${{ secrets.AWS_SECRET_KEY }}-id:deployuses:webfactory/create-aws-codedeploy-deployment@v0.2.2with:application:TestApplication-name:Create Code Deployuses:peter-evans/commit-comment@v2with:application:TestApplicationtoken:${{ secrets.CODEDEPLOY_TOKEN }}body:|@${{ github.actor }} this was deployed as [${{ steps.deploy.outputs.deploymentId }}](https://console.aws.amazon.com/codesuite/codedeploy/deployments/${{ steps.deploy.outputs.deploymentId }}?region=us-east-1) to group `${{ steps.deploy.outputs.deploymentGroupName }}`.Enter fullscreen modeExit fullscreen modeTo test these changes, make adjustments in your code file and monitor progress under \"Actions\". You can also track the same progress in the CodeDeploy deployment.Here is a diagram for the workflow.Demo code link :https://github.com/AvinashDalvi89/aws-ec2-autoscaling-deployment-codedeploy-github-actionsThats all. We completed EC2 autoscaling deployment using CodeDeploy and GitHub Actions. Remember that this is a high-level overview, and you\u2019ll need to adapt it to your specific use case. Feel free to explore the provided resources and customize the setup according to your project requirements.Happy deploying! \ud83d\ude80I hope this blog helps you learn. Feel free to reach out to me on my Twitter handle@avinashdalvi_or leave a comment on the blog. Special thanks to Sandip Das for his video on\"AWS EC2 + Autoscaling + Load Balancer + CodeDeploy | Deploy Code At Scale | DevOps With AWS Ep 5\") for providing me with a hint for the solution.References :https://docs.aws.amazon.com/codedeploy/latest/userguide/welcome.htmlhttps://github.com/features/actionshttps://www.linkedin.com/pulse/deploy-code-scale-aws-ec2-autoscaling-load-balancer-codedeploy-das/https://docs.github.com/en/actions/using-workflows/workflow-syntax-for-github-actions"}
{"title": "Issue 42 of AWS Cloud Security Weekly", "published_at": 1714441297, "tags": ["aws", "security", "iam", "newsletter"], "user": "AJ", "url": "https://dev.to/aws-builders/issue-42-of-aws-cloud-security-weekly-3m8b", "details": "(This is just the summary of Issue 42 of AWS Cloud Security weekly @https://aws-cloudsec.com/p/issue-42<< Subscribe for FREE to receive the full version in your inbox weekly).What happened in AWS CloudSecurity & CyberSecurity last week April 22-April 29, 2024?Amazon Inspector now provides continuous monitoring of EC2 instances for software vulnerabilities without the need for installing an agent or additional software. The existing method uses the AWS Systems Manager (SSM) agent to check for vulnerabilities in third-party software. With this update, Inspector introduces two scanning modes for EC2 assessments: hybrid scan mode and agent-based scan mode. In hybrid scan mode, Inspector utilizes SSM agents to gather data from instances for vulnerability assessments. If an instance doesn't have an SSM agent installed or configured, Inspector automatically switches to agentless scanning. In agentless scanning, Inspector creates snapshots of EBS volumes to gather software inventory from the instances to assess vulnerabilities.With AWS CloudFormation StackSets, you can now set up and manage Amazon Data Lifecycle Manager default policies for the entire organization or specific organizational units (OUs). These default policies complement existing backup strategies by ensuring that EBS-backed AMIs and EBS Snapshots are only created for instances and volumes lacking recent backups. This approach helps administrators ensure that all member accounts have thorough backup coverage while avoiding duplicate backups, thereby reducing both management effort and costs.AWS AppFabric now supports SentinelOne Singularity Cloud as both a data source and a compatible security destination, allowing IT administrators and security analysts to use AppFabric to connect with 27 supported SaaS applications, consolidate enriched and standardized SaaS audit logs, and monitor end-user access across their SaaS apps.AWS CodeBuild now supports managed GitHub Actions self-hosted runners, enabling you to configure CodeBuild projects to receive GitHub Actions workflow job events and execute them on CodeBuild's temporary hosts. With this feature, GitHub Actions can integrate seamlessly with AWS, providing enhanced security and convenience through services like IAM, AWS Secrets Manager, AWS CloudTrail, and Amazon VPC. Customers can leverage all the compute platforms offered by CodeBuild, including Lambda, GPU-enabled, and Arm-based instances.Trending on the news & advisories (Subscribe to the newsletter for details):UnitedHealth Group Updates on Change Healthcare Cyberattack- it paid the attackers.ArcaneDoor - New espionage-focused campaign found targeting perimeter network devices.WordPress Automatic Plugin <= 3.92.0 is vulnerable to SQL Injection.IBM to Buy HashiCorp in $6.4 Billion Deal.How an empty S3 bucket can make your AWS bill explode by Maciej Pocwierz.The State of AWS's Block Public Access: Is It Secure By Default? by Jason Ko.An overview of CloudTrail events that are interesting from an Incident Response perspective.Amazon Science- 98 Amazon Research Awards recipients announced.Okta- How to Block Residential Proxies using Okta."}
{"title": "My Journey to Passing the AWS Certified Solutions Architect Associate Exam", "published_at": 1714430946, "tags": ["aws", "certification", "beginners", "community"], "user": "Damien J. Burks", "url": "https://dev.to/aws-builders/my-journey-to-passing-the-aws-certified-solutions-architect-associate-exam-de", "details": "Table of ContentsIntroductionPrerequisitesExam Overview and GuideExam DetailsExam StructureExam GuideStudy Materials1.Stephane Maarek's Ultimate AWS Certified Solutions Architect Associate Course2.Jon Bonso's Practice Exams3.Whizlabs AWS LabsStudy Schedule & Exam TipsKey AWS Services to Focus OnConclusionIntroductionIn this article, I will share my journey on how to successfully pass the AWS Certified Solutions Architect - Associate (SAA) exam. My hope is that by sharing my experience, resources, study plan, and some crucial tips, I can help you navigate your way to acing this challenging certification exam.PrerequisitesBefore we get into the specifics of the exam, I highly recommend considering the AWS Certified Cloud Practitioner exam first. This foundational certification helps you understand the basic concepts of various AWS services.Although it's not a mandatory requirement, familiarity with basic programming or coding concepts can be advantageous.Exam Overview and GuideExam DetailsThe AWS Certified Solutions Architect Associate exam showcases knowledge and skills in AWS technology across a wide range of services. The focus is on designing cost and performance-optimized solutions, demonstrating a strong understanding of the AWS Well-Architected Framework and other AWS documentation.This exam is ideal for those with:Experience in AWS technologyStrong on-premises IT experience, especially in mapping on-premises to cloudExperience with other cloud service providers (CSPs)The certification helps you create robust, fault-tolerant, and scalable solutions using various AWS services. You'll learn about the AWS Command Line Interface (CLI), the management console, networking, security services, and AWS's global infrastructure.Exam StructureDuration:130 minutes (2 hours and 10 minutes)Cost:$150 USDFormat:65 questions (multiple choice or multiple response)Testing Options:AWS-approved testing center or onlineExam GuideThe exam guide provides detailed information on what the exam will validate regarding your ability to complete tasks within AWS. It includes:Scoring:Passing score is 720.Content Outline:The exam is divided into four domains:Domain 1:Design Secure Architectures (30%)Domain 2:Design Resilient Architectures (26%)Domain 3:Design High-Performing Architectures (24%)Domain 4:Design Cost-Optimized Architectures (20%)The guide also lists in-scope and out-of-scope AWS services and features, providing a clear path for your study.Study MaterialsChoosing the right study materials can make a significant difference in your exam preparation. Here are the resources that I found invaluable:1.Stephane Maarek's Ultimate AWS Certified Solutions Architect Associate CoursePlatform:UdemyOverview:This comprehensive course includes over 340 lectures, totaling approximately 27 hours long. Stephane Maarek does an excellent job explaining each service in detail, providing clear examples that are easy to remember. He explicitly highlights which topics will appear on the exam and offers hands-on lectures to reinforce the material.NOTE:Some hands-on sessions may incur additional costs if you're using a free-tier account.2.Jon Bonso's Practice ExamsPlatform:TutorialsDojo Website - Practice ExamsOverview:These practice exams are tough but closely mimic the complexity and depth of the actual exam questions. Initially, I scored between 40s and 60s, which was incredibly demotivating. However, after thoroughly reviewing each question and consulting theTutorialsDojo AWS Cheat Sheets, my scores improved significantly.3.Whizlabs AWS LabsPlatform:WhizlabsOverview:For those who prefer a more simulated lab environment, Whizlabs offers a token-based system to access their labs. This can be a bit pricey, but it's a worthwhile investment for hands-on learning. If you're willing to invest in it, it's well worth it! You can find more information about their labs here:Whizlabs AWS Solutions Architect - AssociateStudy Schedule & Exam TipsConsistency was key in my preparation. I spent about 2 hours each weekday and between 2-4 hours on weekends studying. This schedule allowed me to complete Stephane's course in just under a month. Afterward, I moved on to doing labs created by Whizlabs and taking practice exams.The practice exams provided by Jon Bonso were instrumental in my success. Despite failing all his practice tests initially, I managed to pass the DVA with an 873 previously and felt confident tackling the SAA. I revisited and retook the practice exams, focusing on understanding each question and the associated concepts thoroughly. This strategy boosted my scores to between 70s and 80s on subsequent attempts.I decided to take the exam despite not consistently scoring above 85 on the practice tests. This risk paid off, though I would recommend others ensure they are scoring at least 85+ if they prefer a safer margin. So, if you are scoring less than an 85 on the practice exams and don't feel confident a week or so before you sit for the exam,I'dhighly recommendyou reschedule it for another 2-3 weeks.Key AWS Services to Focus OnBased on my experience, Ihighlyrecommend you pay close attention to these services, as you might see them on the exam:VPC:Virtual Private CloudS3:Simple Storage ServiceIAM:Identity and Access ManagementAWS Storage GatewayEC2:Elastic Compute CloudEC2 Auto ScalingCloudFormationAWS LambdaSQS:Simple Queue ServiceSNS:Simple Notification ServiceGuardDutyShieldKinesis Data StreamsKinesis FirehoseStep FunctionsCost ExplorerRDS:Relational Database ServiceDynamoDBConclusionPassing the AWS Certified Solutions Architect - Associate exam is a challenging but achievable goal. With the right resources, a consistent study plan, and thorough practice, you can also succeed. I hope my experience inspires and guides you through your own certification journey.Thank you for reading, and best of luck if you're pursuing this certification!Disclaimer:This blog post reflects my personal experiences and opinions. This blogs original content is based off of the following video:All images located in the blog post have been sourced from different places. Click on the image to get redirected to the original source."}
{"title": "Building a Basic Forex Rate Assistant Using Agents for Amazon Bedrock", "published_at": 1714410626, "tags": ["aws", "ai"], "user": "Anthony Wat", "url": "https://dev.to/aws-builders/building-a-basic-forex-rate-assistant-using-agents-for-amazon-bedrock-17fp", "details": "IntroductionWith the prevalence of generative AI (gen AI), I've been keeping abreast on AWS' AI offerings for the past while. My journey started withAmazon Q Business, a fully managed service for building gen AI assistants. While the idea is great, it seems to be too basic as it is today and lacks the advanced features to improve the user experience in practice.I then ventured into the more advanced use cases using Amazon Bedrock and went through many workshops such asBuilding with Amazon Bedrock and LangChain. The challenge I find is that these workshops still tend to be basic, and they don't answer my questions about complex use cases. I came to learn about agents while going through LangChain literatures, but developing a full workflow felt like a daunting task when my full-time job is DevOps, not software development. Things always seem too simple that it doesn't provide enough business value, or too complex that it becomes too costly.After attending a recent AWS PartnerCast webinar on building intelligent enterprise apps using gen AI on AWS, I learned about Agents for Amazon Bedrock and some recent new features added to the service. The service seems to be within the Goldilocks zone matching my current skillsets, so I decided to dive heads-first to learn all about it. I decided to build something realistic and figured that I should share my journey with folks in this blog post.About Agents for Amazon BedrockAgents for Amazon Bedrockis a service that enables gen AI applications to execute multi-step tasks across company systems and data sources. It is effectively a managed service for agents andretrieval-augmented generation (RAG), which are common patterns to extend the capabilities of large language models (LLMs).Agents for Amazon Bedrock assumes the complexity of orchestrating the interactions between different components in such workflows, which must otherwise be programmed into your gen AI application. While you can use frameworks such asLangChainorLlamaIndexto develop these workflows, Agents for Amazon Bedrock makes it much more efficient for common use cases. Agents can also integrate withknowledge basesto enable RAG, as shown in the following diagram from the AWS documentation:Coming up with a basic but representative use caseTo help with brainstorming ideas for an agent, I decided to on these principles:The idea must be practical and with real-life data.Follow theKISS principle.For inspirations on what type of agents I should build, I turned to thePublic APIsGitHub repository which has a curated lists of free APIs. I narrowed my search for an API that does not require sign-up or an API key and returns useful information. I ultimately decided to use theFree Currency Exchange Rates API, which seemed promising upon some basic testing.Naturally, the idea was steered towards a forex rate assistant which helps users look up rates from the API. The API supports lookup by dates, however to keep it simple I decided to limit the lookup to only the latest rates for now. This also leaves some room for enhancing the agent later.Requesting for model accessAgents for Amazon Bedrock is a relatively new feature, so it issupported only in limited regions with limited model support. At the time of writing this blog post, it is only supported in US East (N. Virginia) (us-east-1) and US West (Oregon) (us-west-2) and only supports Anthropic models. We will use theus-west-2region for our evaluation.You should also be aware of thepricingfor different Anthropic models. With the recent addition of theClaude 3 model family, Haiku emerges as highly competitive with great price-to-performance balance. Thus we will use Haiku as the model for our agent.When you first use Amazon Bedrock, you mustrequest for access to the models. This can be done in the Amazon Bedrock console using theModel accesspage which can be opened in the left menu. On that page, you will see the list of base models by vendor and their access status similar to the following:To request for access, do the following:Click on theManage model accessbutton.On theRequest model accesspage, scroll down to the Anthropic models in the list.If this is the first time you are request access to Anthropic models, you will be required tosubmit use case details. Click on theSubmit use case detailsbutton to open the form, then fill it in as appropriate and clickSubmit.Check the box next to the models to which you wish to request access. Since we might compare different Anthropic models, let's check the box next toAnthropicto request access to all of them. Lastly, clickRequest model accessat the end of the page.The access status should now show \"In progress\" and the request will only take a few minutes to be approved if all goes well. Once available, the access status should change to \"Access granted\".Creating the OpenAPI schema for the currency exchange APIIn our agent, we will be using anaction groupthat defines an action that the agent can help the user perform by calling APIs via a Lambda function. Consequently, the action group in our agent requires the following:AnOpenAPI schemathat provides the specifications of the APIA Lambda function to which the action group makes API requestsThat is also to say, the Lambda function is effectively a \"proxy\" API that calls the actual APIs, which in our case is the free currency exchange rates API. Based on theAPI documentation, we know the following:Since we will only support the latest exchange rate, the base URI for our API would behttps://cdn.jsdelivr.net/npm/@fawazahmed0/currency-api@latest/v1.We need to use the/currencies.min.jsonAPI, which gets the list of available currencies in minified JSON format. This helps minimize the number of tokens (and thus cost and limit) processed by the model.We also need to use the/currencies/{code}.min.jsonAPI, gets the currency exchange rates with{code}as the base currency.Since this API does not provide the OpenAPI schema, we need to create it ourselves. I figured that this might be a regular exercise if I start testing Bedrock agents with different APIs, so I started looking for a tool that can generate OpenAPI schema, such as those listed in inOpenAPI.Tools. One category of tools seems to use network traffic, often in theHAR format, to generate the OpenAPI schema. I tried theOpenAPI DevToolswhich is a Chrome extension, however it did not work for the currency exchange rates API.After wrestling with it for a bit and eventually giving up, I instead turned toChatGPTto see if it is smart enough for the task. With my free plan, I asked ChatGPT 3.5 the following:Can you generate the OpenAPI spec YAML from this API GET URL:https://cdn.jsdelivr.net/npm/@fawazahmed0/currency-api@latest/v1/currencies.min.jsonTo my surprise, it did generate a somewhat decent API spec:While it is not usable as-is because the URL is missing the/v1part and it is lacking some descriptions, it has almost everything that I need. However, it struck me as odd that the response has uppercase currency code which is NOT what the API returns. So I started a new ChatGPT session and ask the same question, only to get a very different spec:At this point, I was certain that ChatGPT is not calling the API to generate the spec but rely on what its knowledge to generate an answer. It is probably experiencinghallucination, but it is good enough as a starting point \ud83e\udd37I did the same for the other API and adjusted the spec using theSwagger Editor. Specifically, I added detailed descriptions that should help the agent understand the API usages. The resulting OpenAPI YAML file is as follows:openapi:3.0.0info:title:Currency APIdescription:Provides information about different currencies.version:1.0.0servers:-url:https://cdn.jsdelivr.net/npm/@fawazahmed0/currency-api@latest/v1paths:/currencies.min.json:get:description:|List all available currenciesresponses:'200':description:Successful responsecontent:application/json:schema:type:objectdescription:|A map where the key refers to the three-letter currency code and the value to the currency name in English.additionalProperties:type:string/currencies/{code}.min.json:get:description:|List the exchange rates of all available currencies with the currency specified by the given currency code in the URL path parameter as the base currencyparameters:-in:pathname:coderequired:truedescription:The three-letter code of the base currency for which to fetch exchange ratesschema:type:stringresponses:'200':description:Successful responsecontent:application/json:schema:type:objectdescription:|A map where the key refers to the three-letter currency code of the target currency and the value to the exchange rate to the target currency.additionalProperties:type:numberformat:floatEnter fullscreen modeExit fullscreen modeCreating the agentNow let'screate the agentin the Amazon Bedrock console following the steps below:SelectAgentsin the left menu.On theAgentspage, clickCreate Agent.In theCreate Agentdialog, enter the following information and clickCreate:Name:ForexAssistantDescription:An assistant that provides forex rate information.On theAgent builderpage, enter the following information and clickSave:Agent resource role:Create and use a new service roleSelect model:Anthropic, Claude 3 HaikuInstructions for the Agent:You are an assistant that looks up today's currency exchange rates. A user may ask you what the currency exchange rate is for one currency to another. They may provide either the currency name or the three-letter currency code. If they give you a name, you may first need to first look up the currency code by its name.Note that I try to provide concise instructions for the agent to help it reason up front. Depending on the test results, we might need to adjust it later with moreprompt engineering.Creating the action groupWhile still in the agent builder, we willcreate the action groupthat calls our APIs. Let's perform the following steps:In theAction groupssection, clickAdd.On theCreate Action grouppage, enter the following information and clickCreate:Enter Action group name:ForexAPIDescription:The currency exchange rates APIAction group type:Define with API schemasAction group invocation:Quick create a new Lambda functionAction group schema:Define via in-line schema editorIn-line OpenAPI schema:Copy and paste the OpenAPI YAML from previous sectionAfter 15 seconds or so, you should receive a success message and be returned to the agent builder page. A dummy Lambda function should have been created, so our next step would be to add the logic to call the actual currency exchange rates API.Updating the Lambda function to call the APILet's go back into the action group page by clicking on the name of the action group (i.e.ForexAPI) in the list. In the edit page, click on theViewbutton near theSelect Lambda functionfield, which should take you to the function page in the Lambda console.On the function page, you will see the code template that has been generated for you, which provides some basic processing of the input event and the response event.After examining theinput event format, we will recognize that the attributes that we need to use are:apiPath, which should provide the path to the API as defined in the OpenAPI YAML (namely/currencies.min.jsonor/currencies/{code}.min.json).httpMethod, which should always begetin our case. We thus won't make use of this attribute directly in our example.parameters, which we need to provide for the rate lookup API which expects thecodeURI path parameter to be a three-level currency code.I will spare you the gory details on writing the Lambda function, so here's is the code and some implementation details provided in comments:importjsonimporturllib.parse# urllib is available in Lambda runtime w/o needing a layerimporturllib.requestdeflambda_handler(event,context):agent=event['agent']actionGroup=event['actionGroup']apiPath=event['apiPath']httpMethod=event['httpMethod']parameters=event.get('parameters',[])requestBody=event.get('requestBody',{})# Read and process input parameterscode=Noneforparameterinparameters:if(parameter[\"name\"]==\"code\"):# Just in case, convert to lowercase as expected by the APIcode=parameter[\"value\"].lower()# Execute your business logic here. For more information, refer to: https://docs.aws.amazon.com/bedrock/latest/userguide/agents-lambda.htmlapiPathWithParam=apiPath# Replace URI path parametersifcodeisnotNone:apiPathWithParam=apiPathWithParam.replace(\"{code}\",urllib.parse.quote(code))# TODO: Use a environment variable or Parameter Store to set the URLurl=\"https://cdn.jsdelivr.net/npm/@fawazahmed0/currency-api@latest/v1{apiPathWithParam}\".format(apiPathWithParam=apiPathWithParam)# Call the currency exchange rates API based on the provided path and wrap the responseapiResponse=urllib.request.urlopen(urllib.request.Request(url=url,headers={\"Accept\":\"application/json\"},method=\"GET\"))responseBody={\"application/json\":{\"body\":apiResponse.read()}}action_response={'actionGroup':actionGroup,'apiPath':apiPath,'httpMethod':httpMethod,'httpStatusCode':200,'responseBody':responseBody}api_response={'response':action_response,'messageVersion':event['messageVersion']}print(\"Response: {}\".format(api_response))returnapi_responseEnter fullscreen modeExit fullscreen modeYou can copy and paste this code into the editor and clickDeployto update it. At this point, we should test the Lambda function before returning to the Amazon Bedrock console. To do this, you can use the following event template to test the/currencies.min.jsonAPI (note that some irrelevant fields are omitted):{\"messageVersion\":\"1.0\",\"agent\":{\"name\":\"TBD\",\"id\":\"TBD\",\"alias\":\"TBD\",\"version\":\"TBD\"},\"inputText\":\"TBD\",\"sessionId\":\"TBD\",\"actionGroup\":\"TBD\",\"apiPath\":\"/currencies.min.json\",\"httpMethod\":\"get\"}Enter fullscreen modeExit fullscreen modeYou should see the success response with the list of currencies:You can then use the following event template to test the/currencies/{code}.min.jsonAPI:{\"messageVersion\":\"1.0\",\"agent\":{\"name\":\"TBD\",\"id\":\"TBD\",\"alias\":\"TBD\",\"version\":\"TBD\"},\"inputText\":\"TBD\",\"sessionId\":\"TBD\",\"actionGroup\":\"TBD\",\"apiPath\":\"/currencies/{code}.min.json\",\"httpMethod\":\"get\",\"parameters\":[{\"name\":\"code\",\"type\":\"string\",\"value\":\"usd\"}]}Enter fullscreen modeExit fullscreen modeYou should see the success response with the list of exchange rates from US dollar to other currencies:With the Lambda function verified, we can close the Lambda console and return to the Bedrock console to test the agent.Testing the agentIt is imperative that we test the agent thoroughly to ensure that it provides accurate answers. Back to the agent builder, we need to click on thePreparebutton to prepare it, which is requiredwhenever the agent is changed. We can then test the agent using the built-in chat interface to the right of the console using the following prompt:What is the forex rate from US Dollar to Japanese Yen?Interestingly, I got the following response from the agent:Sorry, I do not have the capability to look up the current forex rate from US Dollar to Japanese Yen. I can only provide a list of available currencies, but cannot retrieve the specific exchange rate you requested.\u26a0 When I was validating the solution from scratch, the agent was able to return the correct answer. This could be caused by the model parameters that affects variability of responses among other things - the model is a bit of a black box after all! If you cannot reproduce this problem, try a few prompt sessions and ask the same question.This is seemingly implying that the agent only knows of one API but not the other. So we need to troubleshoot the problem, which is where the ever-importanttrace featurecome into play. The trace helps you follow the agent's reasoning that leads it to the response it gives at that point in the conversation.When we show the trace using the link below the agent response, we can see the traces for each orchestration steps. There are four traces under theOrchestration and knowledge basetab:Trace step 1 indicates the agent's rationale of first getting the currency code from the list then calling the/currencies/{code}.min.jsonAPI to get the rate, which seems correct. It is also able to call the/currencies.min.jsonAPI to get the list of currencies to look up the code. So far so good.Trace step 2 indicates that it was able to get the currency code for US Dollar asUSD, however we are not sure why it's in uppercase. It also indicates thatget::ForexAPI::/currencies/USD.min.jsonis not a valid function, which is not true. It is unclear about the logic behind the decision.Trace step 3 indicates that it is calling the/currencies.min.jsonAPI again for whatever reason. Lastly trace step 4 indicates that it cannot get the currency exchange rate and therefore gave up with the response we saw in the chat.Since LLM is for the most part a black box, unfortunately we likely won't be able to get to the root cause. The only wild guess I could make is that the.min.jsonpart is throwing it off because it doesn't resemble a normal RESTful API, so perhaps we can try to adjust the API specifications to remove that part.Adjusting the API specs and re-testingLet's make the adjustment in the OpenAPI YAML by stripping out the.min.jsonpart from both API URLs:openapi:3.0.0info:title:Currency APIdescription:Provides information about different currencies.version:1.0.0servers:-url:https://cdn.jsdelivr.net/npm/@fawazahmed0/currency-api@latest/v1paths:/currencies:get:description:|List all available currenciesresponses:'200':description:Successful responsecontent:application/json:schema:type:objectdescription:|A map where the key refers to the three-letter currency code and the value to the currency name in English.additionalProperties:type:string/currencies/{code}:get:description:|List the exchange rates of all available currencies with the currency specified by the given currency code in the URL path parameter as the base currencyparameters:-in:pathname:coderequired:truedescription:The three-letter code of the base currency for which to fetch exchange ratesschema:type:stringresponses:'200':description:Successful responsecontent:application/json:schema:type:objectdescription:|A map where the key refers to the three-letter currency code of the target currency and the value to the exchange rate to the target currency.additionalProperties:type:numberformat:floatEnter fullscreen modeExit fullscreen modeThis will cause the agent to pass the API URL without the.min.jsonpart to the Lambda function in the event, so we need to add it to the URL before calling the currency exchange rates API in line 27. The resulting Lambda code is thus:importjsonimporturllib.parse# urllib is available in Lambda runtime w/o needing a layerimporturllib.requestdeflambda_handler(event,context):agent=event['agent']actionGroup=event['actionGroup']apiPath=event['apiPath']httpMethod=event['httpMethod']parameters=event.get('parameters',[])requestBody=event.get('requestBody',{})# Read and process input parameterscode=Noneforparameterinparameters:if(parameter[\"name\"]==\"code\"):# Just in case, convert to lowercase as expected by the APIcode=parameter[\"value\"].lower()# Execute your business logic here. For more information, refer to: https://docs.aws.amazon.com/bedrock/latest/userguide/agents-lambda.htmlapiPathWithParam=apiPath# Replace URI path parametersifcodeisnotNone:apiPathWithParam=apiPathWithParam.replace(\"{code}\",urllib.parse.quote(code))# TODO: Use a environment variable or Parameter Store to set the URLurl=\"https://cdn.jsdelivr.net/npm/@fawazahmed0/currency-api@latest/v1{apiPathWithParam}.min.json\".format(apiPathWithParam=apiPathWithParam)# Call the currency exchange rates API based on the provided path and wrap the responseapiResponse=urllib.request.urlopen(urllib.request.Request(url=url,headers={\"Accept\":\"application/json\"},method=\"GET\"))responseBody={\"application/json\":{\"body\":apiResponse.read()}}action_response={'actionGroup':actionGroup,'apiPath':apiPath,'httpMethod':httpMethod,'httpStatusCode':200,'responseBody':responseBody}api_response={'response':action_response,'messageVersion':event['messageVersion']}print(\"Response: {}\".format(api_response))returnapi_responseEnter fullscreen modeExit fullscreen modeOnce you updated both, you can prepare and test the agent again. Interestingly, we now get a proper response:The current forex rate from US Dollar (USD) to Japanese Yen (JPY) is 1 USD = 158.3147925 JPY.To ensure that the number is from the API and not other sources, we can review the agent's rationale from the trace like before. In trace step 2, we can see the right rationale and the invocation of the/currencies/{code}API withUSDas the code parameter (again, not sure why it's in uppercase) as expected:Hurray, we have successfully build a basic forex rate assistant using Agents for Amazon Bedrock! Naturally, you should test the agent extensively since LLMs are sometimes unpredictable and may require adjustments.\ud83d\udca1 In a follow-up blog postHow To Manage an Amazon Bedrock Agent Using Terraform, I provide details on how to automate the deployment of this solution using Terraform. Please feel free to read it or refer directly to the configuration inthis repository.Testing reveals more limitationsFor sake of experimentation, let's see what happens when we ask the assistant to do the reverse conversion. We will continue with the conversation in the same chat session and enter the following prompt:What about the other way around?The agent responded with the following:The current forex rate from Japanese Yen (JPY) to US Dollar (USD) is 1 JPY = 0.0063163 USD.However, a quick check shows that the number is a bit off. The response fromhttps://cdn.jsdelivr.net/npm/@fawazahmed0/currency-api@latest/v1/currencies/jpy.json(at the time of writing) shows 0.0063165291 which is also what I got from the calculator for 1 / 158.3147925. Again, we will need to review the trace to see what the agent is up to. The trace revealed that the agent is doing an inverse calculation but the calculation is incorrect for some reason:My expectation is that the agent should do another lookup from the API to get the right number. If the API were developed for a business and has a spread between the two exchange rates for profit, the agent would have given the wrong information. Putting that aside, the calculation is simply wrong.After doing some reading online, it seems thatLLMs in general are bad at mathbecause their design is to predict words and not performing computations. So the exchange right 0.0063163 might just be a predication by Haiku based on the data that it was trained with.Additional thoughts and summaryWhile we have built a functional forex rate assistant using Agents for Amazon Bedrock, it is certainly not production grade since it is not super accurate and it is a bit slow. Improving its accuracy is where the bulk of the effort for gen AI lies. AWS recommends the following strategies which developers should sequentially employ to improve their gen AI application:For instance, my next iteration of improvement could start with adjusting the modelinference parametersandprompt engineering, perhaps to ensure that it always calls the API instead of trying to do calculations. We also ought to look at why the LLM provide uppercase currency codes. Prompt engineering is admittedly more of an art and will require many rounds of trial and error, so be prepared for that.I hope you learn something new from this blog post and has a better understanding of the features, potentials, and limitations of Agents for Amazon Bedrock. We are only scratching the surface here, so you are encouraged to use this forex agent as a start point for more improvements or develop your own agent. You would also need to expose the agent to end-users with a new frontend or an existing application. For me, the next step is to look intohow to manage Bedrock agents using Terraform with the hot-off-the-press resources.If you enjoyed this blog post, please be sure to check out other contents related to AWS and DevOps in theAvangards Blog. Thanks for your time and have fun with gen AI!"}
{"title": "Spring Boot 3 application on AWS Lambda - Part 4 Measuring cold and warm starts with AWS Serverless Java Container", "published_at": 1714403923, "tags": ["aws", "java", "serverless", "springboot"], "user": "Vadym Kazulkin", "url": "https://dev.to/aws-builders/spring-boot-3-application-on-aws-lambda-part-4-measuring-cold-and-warm-starts-with-aws-serverless-java-container-mb0", "details": "IntroductionIn thepart 2of the series we introduced AWS Serverless Java Container and in thepart 3we demonstrated how to write AWS Lambda with AWS Serverless Java Container using Java 21 and Spring Boot 3.2. In this article of the series, we'll measure the cold and warm start time including enabling SnapStart on the Lambda function but also applying various priming techniques like priming the DynamoDB invocation and priming the whole web request. We'll use Spring Boot 3.2sample applicationfor our measurements, and for all Lambda functions use JAVA_TOOL_OPTIONS: \"-XX:+TieredCompilation -XX:TieredStopAtLevel=1\" and give them all 1024 MB memory.Measuring cold starts and warm time with AWS Serverless Java Container and using Java 21 and Spring Boot 3.2First of all I'd like to explore Lambda SnapStart (as the only way to have competitive cold start times) and various priming techniques. For the introduction to SnapStart I refer to my articleInitial measuring of Java 11 Lambda cold starts. Enabling SnapStart on Lambda function is only a matter of configuration  like :SnapStart:   ApplyOn: PublishedVersionsEnter fullscreen modeExit fullscreen modeapplied in the Lambda function Properties or Globals Lambda function section of the SAM template, so I'd like to dive deeper to how to use various priming techniques on top of it for our use case. I explained the ideas behind various priming techniques in my articleAWS Lambda SnapStart - Part 5 Measuring priming, end to end latency and deployment time. So please read it first.1) The code for priming of DynamoDB request can be foundhere.This class additionally implements import org.crac.Resourceinterface of theCraC project.With this invocationCore.getGlobalContext().register(this);Enter fullscreen modeExit fullscreen modeStreamLambdaHandlerWithDynamoDBRequestPriming class registers itself as CRaC resource.We additionally prime the DynamoDB invocation by implementingbeforeCheckpointmethod from theCRaC API.@Override public void beforeCheckpoint(org.crac.Context<? extends Resource> context) throws Exception { \u2002\u2002 productDao.getProduct(\"0\"); }Enter fullscreen modeExit fullscreen modewhich we'll invoked during the deployment phase of the Lambda funtion and before Firecracker microVM snapshot will be taken.2) The code for priming of the whole web request can be foundhere.This class also additionally implements import org.crac.Resourceinterface as in the example above. We'll re-use a bit ugly technique, which I described in my articleAWS Lambda SnapStart - Part 6 Priming the request invocation for Java 11 and Micronaut, Quarkus and Spring Boot frameworks. I don't recommend using this technique in production, but it demonstrates the further potential to reduce the cold start using priming of the whole web request by pre-loading the mapping between Sping Boot Web annotation model and Lambda model performing also DynamoDB invocation priming.What will be doing in thebeforeCheckpointmethod is toconstruct and proxy the whole /products/{id} invocation by invoking directly Lambda function with payload (as JSON) which looks identical to the APIGateway proxy request event but without going over the network. Web Request construction of the /products/{id} with id equals to 0 API Gateway request using the com.amazonaws.serverless.proxy.model.AwsProxyRequestabstraction looks like this:private static AwsProxyRequest getAwsProxyRequest () {   final AwsProxyRequest awsProxyRequest = new AwsProxyRequest ();   awsProxyRequest.setHttpMethod(\"GET\");   awsProxyRequest.setPath(\"/products/0\");   awsProxyRequest.setResource(\"/products/{id}\");   awsProxyRequest.setPathParameters(Map.of(\"id\",\"0\"));   final AwsProxyRequestContext awsProxyRequestContext = new AwsProxyRequestContext();   final ApiGatewayRequestIdentity apiGatewayRequestIdentity= new ApiGatewayRequestIdentity();   apiGatewayRequestIdentity.setApiKey(\"blabla\");   awsProxyRequestContext.setIdentity(apiGatewayRequestIdentity);   awsProxyRequest.setRequestContext(awsProxyRequestContext);   return awsProxyRequest;\u2002\u2002\u2002\u2002\u2002\u2002\u2002 }Enter fullscreen modeExit fullscreen mode@Override \u2002public void beforeCheckpoint(org.crac.Context<? extends Resource> context) throws Exception { \u2002\u2002\u2002handler.proxy(getAwsProxyRequest(), new MockLambdaContext()); \u2002}Enter fullscreen modeExit fullscreen modeWith that the following method@RequestMapping(path = \"/products/{id}\", method = RequestMethod.GET, produces = MediaType.APPLICATION_JSON_VALUE) public Optional<Product> getProductById(@PathVariable(\"id\") String id) {   return productDao.getProduct(id); }Enter fullscreen modeExit fullscreen modeof theProductControllerwith id equals to 0 will be invoked during priming which also sebsequently primes DynamoDB invocation.The results of the experiment below were based on reproducing more than 100 cold and approximately 100.000 warm starts with Lambda function with 1024 MB memory setting for the duration of 1 hour. For it I used the load test toolhey, but you can use whatever tool you want, likeServerless-artilleryorPostman.I ran all these experiments on ourGetProductByIdWithSpringBoot32Lambda function with 4 different scenarios :1) No SnapStart enabledin template.yaml use the following configuration:Handler: software.amazonaws.example.product.handler.StreamLambdaHandler::handleRequest   #SnapStart:     #ApplyOn: PublishedVersionsEnter fullscreen modeExit fullscreen mode2) SnapStart enabled but no priming appliedin template.yaml use the following configuration:Handler: software.amazonaws.example.product.handler.StreamLambdaHandler::handleRequest   SnapStart:     ApplyOn: PublishedVersionsEnter fullscreen modeExit fullscreen mode3) SnapStart enabled with DynamoDB invocation primingin template.yaml use the following configuration:Handler: software.amazonaws.example.product.handler.StreamLambdaHandlerWithDynamoDBRequestPriming::handleRequest   SnapStart:     ApplyOn: PublishedVersionsEnter fullscreen modeExit fullscreen mode4) SnapStart enabled with web request invocation primingin template.yaml use the following configuration:Handler: software.amazonaws.example.product.handler.StreamLambdaHandlerWithWebRequestPriming::handleRequest   SnapStart:     ApplyOn: PublishedVersionsEnter fullscreen modeExit fullscreen modeSo let's provide the results of the measurement. Abbreviationcstays for the cold start andwis for the warm start.Cold (c) and warm (w) start time in ms:Scenario Numberc p50c p75c p90c p99c p99.9c maxw p50w p75w p90w p99w p99.9w maxNo SnapStart enabled6358.976461.486664.827417.117424.537425.657.888.8010.4924.611312.021956.15SnapStart enabled but no priming applied1949.282061.492522.702713.642995.892996.98.009.0810.9926.30267.051726.98SnapStart enabled with DynamoDB invocation priming952.961024.061403.011615.351644.671645.058.009.2411.4526.44143.19504.61SnapStart enabled with web request invocation priming741.52790.501168.491333.291384.901386.117.638.5310.1623.09123.07346.89ConclusionBy enabling SnapStart on the Lambda function alone, it reduces the cold start time of the Lambda function significantly. By additionally using DynamoDB invocation priming and especially web request invocation priming (which I don't recommend using this technique in production though) we were be able to achieve cold starts only slightly higher than cold starts described in my articleAWS SnapStart -Measuring cold and warm starts with Java 21 using different memory settingswhere we measured cold and warm starts for the pure Lambda function without the usage of any frameworks including 1024 MB memory setting like in our scenario.In the next part of the series I'll make the introduction to theAWS Lambda Web Adapterand explain how our Serverless Spring Boot application on AWS can make use of it."}
{"title": "Amazon Bedrock Blueprint: Architecting AI Projects with Amazon Bedrock", "published_at": 1714395449, "tags": ["bedrock", "genai", "aws", "llm"], "user": "NaDia", "url": "https://dev.to/aws-builders/amazon-bedrock-blueprint-architecting-ai-projects-with-amazon-bedrock-4686", "details": "Initial WordsIf you're actively involved in the AI field and utilise AWS Cloud services, chances are you've explored Amazon Bedrock to enhance your applications with AI capabilities. Even if you haven't directly worked with it, you've likely heard about the advanced Foundational Models that Amazon Bedrock offers. In this blog post, I'll provide a comprehensive introduction to Amazon Bedrock components and delve into common workflows for integrating Amazon Bedrock into Generative AI projects.Amazon Bedrock ComponentsExploring various articles on Amazon Bedrock will give you enough information about its nature. As you may be aware, Amazon Bedrock offers a quick serverless experience, granting access to an extensive array of Foundational Models. Its unified API is especially noteworthy, as it streamlines the integration of these diverse models into your system.However, the question remains: how does Amazon Bedrock achieve this? What components does it comprise that set it apart from other AI platforms or services? This is the exploration we aim to undertake in this section.Foundational ModelsYes, Amazon Bedrock offers a wide range of Foundational Models like AI21 Labs, Anthropic, Cohere, Meta, Mistral AI, Stability AI, and Amazon. But what's the advantage of using these models within Bedrock? You're not restricted to just one specific model. Bedrock's API makes it easy to integrate these models. If you decide to switch from Mistral to Cohere models, all you need to do is change the model ID within your \"InvokeModel\" API from Bedrock. Additionally, if your system needs to integrate with multiple models, Bedrock's API layer allows you to invoke as many models as you need in parallel but completely isolated from each other.Knowledge BaseUsing Foundational Models alone has limitations. However, there are effective approaches to overcome these limitations, which I'll discuss in the \"AI Workflows With Bedrock\" section. It's still important to understand these limitations such as:Outdated informationLack of knowledge on your data set.Lack of transparency on how they arrived at specific answers.HallucinationI'm sure you're aware that Foundational Models are trained on vast amounts of data, but there's no guarantee they're always up to date with the latest information. These models provide answers without providing the source links for the context they used. The answers they give are very general, and if you want them to be based on your company's specific data, you'll need to retrain them using that data. However, if your data is constantly changing, continuously retraining the models can be computationally intensive and expensive. Additionally, by the time you finish retraining the model, your company may have already generated new data, making the model's information outdated.To address issues such as providing source links or offering more specific domain-related answers, Amazon Bedrock offers the \"Knowledge Base\" component. This feature provides additional data access during runtime.Using the Knowledge Base, you can create a RAG (Retrieval Augmented Generation) application that utilizes the \"RetrieveAndGenerate API\" to fetch information from your Knowledge Base (KB) and generate responses. Alternatively, you can build a basic RAG Application with the \"Retrieve API\", which retrieves information from the knowledge base and presents it to the user along with the source link.Beyond answering user queries, a KB can augment prompts for Foundational Models by adding context to the prompt. This adds RAG capability to Agents for Amazon Bedrock.Agents For BedrockThe Amazon Bedrock Knowledge Base (KB) handles data ingestion, while agents manage the Retrieval Augmented Generation (RAG) workflow. Agents for Amazon Bedrock automates prompt engineering and the organization of user-requested tasks.Agents can perform various tasks, including:Take actions to fulfill the users request.Agents have predefined Action groups, which are tasks they can autonomously perform. Each Action Group comprises some Lambda functions and API Schema. A crucial aspect of Schema Definition is the description of each Endpoint in your Schema. These descriptions can act as prompts to your Agent, helping it understand when to use which API Endpoint.Break down complex user queries for Foundational Model.Agents assist Foundation Models in comprehending user requests. In the upcoming workflow explanations, I will delve deeper into how Agents employ ReAct strategies to analyse user requests and determine the actions that Foundation Models should take to fulfill those queries.Collect additional informationWhen you create an Agent for Amazon Bedrock, you can configure Agent to collect additional information from user through natural language conversation.Common AI Workflows With Amazon BedrockHaving explored all the components of Amazon Bedrock, let's now delve into the most common patterns for integrating Amazon Bedrock into our Generative API applications. Knowing the common blueprints will help us to identify when a specific service is a good addition to our architecture and which blueprint is a good candidate for our use-case.Beginning with standard workflow to only use Amazon Bedrock API that invokes different models using \"InnvokeModel\" API.This invocation can be initiated either by an event within you AWS account or through Application API.In an event driven workflow, the Model Invocation can occurs by S3 notifications when a file is uploaded to a specific S3 Bucket. This might be necessary when new files are uploaded to the bucket, and you want to summarise the document using Amazon Bedrock Foundational Models.You could also set up your Application API with an AWS Lambda Function that triggers a Foundational Model. For instance, it could generate text based on a user-provided topic or describe an image uploaded by the user.This approach may appear simplistic, but that's the essence of utilising Amazon Bedrock as an API abstraction layer in your Generative AI application. Despite its simplicity, this method can yield effective responses and enhance answer quality using common techniques like Prompt engineering.The next pattern I'd like to discuss involves creating RAG applications using Knowledge Base, which blends prompt engineering techniques with retrieving information from external sources.To set up a RAG workflow, begin by creating a Knowledge Base in Amazon Bedrock. This involves specifying the S3 Bucket containing your external resources, determining the document chunk size, selecting the Embedding model to generate vectors for the dataset, and selecting a Vector Database to store the indexes, like Amazon OpenSearch. Setting the chunk size is crucial as it leads to finer embeddings, enhancing retrieval accuracy, and prevents overloading the model's context window with large source documents.Similar to most of AI powered workflows, this one also starts with user input prompt. RAG uses the same embedding model to create a vector embedding representation of the input prompt. This embedding is then used to query the Knowledge Base for similar vector embeddings to return the most relevant text as the query result. The query result is then added to the prompt, and the augmented prompt is passed to the FM. The model uses the additional context in the prompt to generate the response to the user query.Like many AI-powered workflows, this one begins with a user input prompt. RAG uses an embedding model to create a vector representation of the input prompt. This vector is used to search the Knowledge Base for similar vectors, returning the most relevant text as the query result. The query result is then combined with the prompt and passed to the FM. The model uses the augmented prompt to generate a response to the user's query.Ever since I was young, I've saved the best for last. Let's talk about the Amazon Bedrock workflow with Agents. Here, you can surpass limitations by combining all Amazon Bedrock components, from the Knowledge Base to your company's APIs, to empower the Model to generate robust answers.In an earlier section, I mentioned Agents extend FMs to understand user requests by breaking down complex tasks into multiple steps. This process occurs during the Pre-Processing phase. When an Agent receives a user request, it first analyses the request using ReAct Technique (Reason, Action, Observation).During this phase, the Agent:Reasons on the user query to understand the task at hand, determining whether it needs to call an API or access a Knowledge Base for information.Takes action to fulfill the request by executing the necessary steps.Returns the observation or results after completing the actions. It then incorporates this information into the input prompt, providing the model with additional context (Augmented Prompt).Final WordsUnderstanding the structure of Amazon Bedrock and the typical architectures for integrating it into Gen AI applications can help us make more informed decisions about which components to use to achieve our goals. However, it can be challenging to determine the best workflow, especially for those new to the Gen AI field.For simpler tasks that involve historical data, a standard approach with strong Prompt Engineering techniques is often effective. In contrast, for more complex tasks or when responses need to be specific to your dataset, leveraging Fine Tuning within Amazon Bedrock can be beneficial.When the model requires external data resources to fulfill user requests, using a Knowledge Base or a combination of a Knowledge Base and an Agent can be helpful. A Knowledge Base workflow is suitable for relatively static data such as company documents or FAQs, while an Agent with a Knowledge Base is better for dynamic information like databases or APIs.There is no one-size-fits-all solution, but the flexibility of Amazon Bedrock allows for various approaches to achieve the same result. The key is to choose the right approach for the task to achieve optimized results at minimal cost.I hope you found this article useful. In the next part, I will demonstrate the most advanced workflow, where we will use an Agent with APIs and a Knowledge Base to create a Tourist and Travel Assistant using Amazon Bedrock providing all the code snippets and code repository for your reference."}
{"title": "AWS Community Day Kenya", "published_at": 1714389177, "tags": ["awscommunity", "awscommunitydayke", "aws", "innovation"], "user": "Adeline Makokha", "url": "https://dev.to/aws-builders/aws-community-day-kenya-89h", "details": "The AWS User-group Kenya recently organized the first-ever AWS Community Day Kenya 2024, a big step forward for Kenya's tech scene. Held at KCA University on April 20th, the event brought together over 400 cloud enthusiasts, professionals, and businesses not only from Kenya but also neighboring countries like Uganda, Burundi, and Nigeria. The goal was simple: to share knowledge, discuss the latest trends, and explore how Amazon Web Services (AWS) can drive innovation in the cloud.Big shout out to our amazing sponsors: Amazon Web Services, KCA University, Silicon Overdrive, QuCoon Limited, eMobilis Mobile Technology Institute, ELEVEN DEGREES CONSULTING LIMITED, Moringa School, Labmero Consulting, Dumela Corp, Whizlabs, YellowCard and Mesh.The event boasted an impressive lineup of speakers, including Jeff Barr, Vice President & Chief Evangelist of AWS, who highlighted the global significance of the event. Eng. John Tanui, from Kenya's Ministry of ICT, and Dr. Aminah Zawedde, from Uganda's Ministry of ICT, and Robin Njiru shared insights into the region's digital transformation journey.Eng. Tanui emphasized Kenya's commitment to digital advancement and its partnership with AWS to achieve ambitious goals outlined in Kenya Vision 2030. He highlighted how cloud technology can boost economic growth and create opportunities for Kenya's youth, citing the recent establishment of an AWS development center in Nairobi.Dr. Zawedde echoed this sentiment, stressing the importance of empowering African youth to lead the technological revolution. She emphasized the role of events like the AWS Community Day in nurturing talent and fostering innovation, calling for its expansion to benefit more young people across the continent.Robin Njiru's keynote shed light on the transformative impact of AWS technologies, emphasizing their potential to drive innovation, spur economic growth, and empower businesses across Sub-Saharan Africa. He spoke passionately about AWS's commitment to supporting the region's digital transformation journey, highlighting initiatives aimed at fostering talent development, enhancing technical skills, and accelerating cloud adoption.In particular, He emphasized the role of events like the AWS Community Day in nurturing a vibrant and engaged community of cloud enthusiasts, professionals, and businesses. He commended the organizers and participants for their dedication and enthusiasm, recognizing their pivotal role in driving innovation and advancing technical expertise in the region.WOMEN IN TECH PANEL DISCUSSIONThere was a special session called Women in Tech. This session brought together successful women working in different tech roles to talk about their experiences and challenges in the industry.One big topic they talked about was how hard it can be for women to balance work and personal life. They shared tips like being flexible with work hours, taking care of themselves, and getting support from family and colleagues.The discussion showed how important it is to have a supportive environment for women in tech to do well both at work and in their personal lives. By sharing their stories, the panelists inspired others and helped make the tech community more inclusive.ARTIFICIAL INTELLIGENCE & MACHINE LEARNING TRACKAttendees had a chance to explore different tracks, each focusing on a specific area of cloud computing and technology. One standout track was all about AI and ML, where people got to dive into the world of artificial intelligence and machine learning. Led by experts, sessions covered topics like deep learning, language processing, and computer vision, showing how AI and ML are changing industries like healthcare and finance. Attendees also learned practical ways to use AWS tools like SageMaker to implement AI solutions.DEVOPS TRACKAnother popular track was DevOps, which took a deep dive into how to streamline software development and operations. Participants learned about automation, continuous integration/delivery, and infrastructure as code. Through workshops and discussions, they discovered how AWS services like CodePipeline, CodeDeploy, and CloudFormation can make development faster and more efficient.CYBERSECURITY TRACKIn the Cybersecurity track, attendees got a comprehensive look at how to keep cloud environments safe from cyber threats. Led by experts, sessions covered topics like identity management, data encryption, and threat detection. Attendees learned practical ways to use AWS services like IAM, KMS, and WAF to beef up security.SUSTAINABILITY AND ENVIRONMENT TRACKLastly, the Sustainability track focused on how AWS is making a positive impact on the environment through its cloud infrastructure. Sessions explored AWS's efforts in renewable energy and reducing carbon footprint. Attendees learned how organizations can use AWS services to build eco-friendly solutions and heard inspiring stories of businesses integrating sustainability into their cloud projects.AWS ONLINE DEEPRACER COMPETITIONAs the AWS Community Day Kenya 2024 was winding down, things heated up for the AWS DeepRacer competition, capping off weeks of buildup and excitement. This fun event brought together people from different backgrounds, all pumped up to show off their machine learning and racing skills.Leading up to the competition, participants spent more than a month getting ready, fine-tuning their DeepRacer models to perfection. With the help of mentors and AWS Solution Architect Sello Tseka, they tackled tricky algorithms and polished their racing strategies.When the big moment finally arrived, the tension was thick in the air as the three winning teams emerged victorious. Their hard work paid off big time, and the DeepRacer competition not only showed off their tech skills but also brought the AWS community closer together.This success story of the DeepRacer competition highlighted just how much innovation and creativity can come from machine learning and AI. As the day came to a close, participants left feeling proud and excited to keep exploring the endless possibilities of cloud computing and beyond.CONCLUSIONAs Kenya strides forward in its digital journey, events like the AWS Community Day play a crucial role in driving innovation and excellence in cloud computing. The AWS User-group Kenya has been instrumental in fostering a supportive community of AWS users and advocates, ultimately propelling Kenya towards a future powered by the endless possibilities of the cloud.REFERENCEAWS USERGROUP KENYAJEFF BARR KEYNOTEROBIN NJIRU KEYNOTEENG. JOHN TANUI KEYNOTEDR. AMINAH ZAWEDDE KEYNOTE"}
{"title": "AWS Detective", "published_at": 1714371214, "tags": ["aws", "detective", "security"], "user": "Manu Muraleedharan", "url": "https://dev.to/aws-builders/aws-detective-3p1p", "details": "AWS Detective service helps to analyze security issues.It automatically collects and analyzes security logs like VPC flow log, Cloudtrail and guard duty results and utilizes machine learning, graph theory and visualization to help in RCA.You can answer questions like1) How did this security incident happen?2) Where was the first intrusion3) How to prevent such incidents.Amazon Detective requires that you have Amazon GuardDuty enabled on your accounts for at least 48 hours before you enable Detective on those accounts. Findings are sent from GuardDuty to Detective every 6 hrs by default, this can be changed to be as fast as every 15 minutes. It takes 2 weeks of data to build a historical baseline.Finding GroupsIt groups findings from various services together based on incidents, so you can see the related findings in one place. It shows severity, entity affected, MITRE tactic used etc. The group is constructed as a graph that allows you to see the relation between various incidents that occurred. By default, the graph visualization is force-directed. You can manipulate the graph to get more details or different visualizations.For principals, EC2 instances, and EKS clusters you can see the most number of API calls and success, and failure counts.Powerful search functionality allows you to search the incidents in the environment through various options. This lets you see if the failures are consistent or if is there a suspicious pattern.Inside the investigation, you can see a visualization of how different incidents and entities are related to each other. You can manipulate this graph, and research the information that Detective gathers to gain insight into the security incident.InvestigationsFor findings in GuardDuty, you have the option to pivot to Detective and investigate the finding concerning the different entities involved (EC2 instance, IAM Role, Account, etc. )This runs an investigation of the findings in the data so far gathered and creates a report of all the related and relevant data. This can be used, then by the security analyst to find out details on how the attack occurred (eg: A day with many failed API calls with a bruteforce SSH), and what are remediation actions to be taken (isolate EC2, revoke sessions, rotate keys etc..)Analysts can find the details of the mappings to tactics, techniques, and procedures (TTP). All TTPs are classified according to their severity. The console shows the techniques and actions used. By selecting a specific TTP, they can see the details in the right pane.Once the analyst has enough information about the incident they can take remediation steps (isolate EC2, revoke sessions, rotate keys etc..)More information can be gained from this walkthrough of Detective (From AWS):https://www.youtube.com/watch?v=Rz8MvzPfTZA"}
{"title": "Enabling distributed tracing for containerized apps with AWS\u00a0X-Ray", "published_at": 1714367142, "tags": ["xray", "eks", "monitoring", "tracing"], "user": "saifeddine Rajhi", "url": "https://dev.to/aws-builders/enabling-distributed-tracing-for-containerized-apps-with-aws-x-ray-4o44", "details": "Introduction:AWS X-Rayis a distributed tracing service that provides deep insights into how containerized applications perform on AWS.By instrumenting your containers with the X-Ray SDK and deploying the X-Ray daemon, you can gain end-to-end visibility into requests flowing through your containerized services running on ECS or EKS.X-Ray creates detailed trace maps that show how requests are processed across multiple components, helping you identify performance bottlenecks, debug issues, and optimize your containerized applications.In this post, we'll explore how to enable X-Ray tracing for containers on ECS and EKS, the key concepts and components involved, and best practices for instrumenting and deploying your containerized workloads with X-Ray.Tracing with AWS\u00a0X-Ray:X-Raycreates detailed trace maps that show how requests are processed across multiple components, helping you identify performance bottlenecks, debug issues, and optimize your containerized applications. It integrates seamlessly with other AWS services like API Gateway, Lambda, EKS, ECS, and more to provide a comprehensive view of your distributed architecture.Instrumenting Containerized Applications with AWS X-Ray:Instrumenting your containerized application for X-Ray tracing involves sending trace data for incoming and outbound requests, along with metadata about each request.This is done by integrating the X-Ray SDK into your application code. Many instrumentation scenarios require only configuration changes, such as instrumenting all incoming HTTP requests and downstream calls to AWS services.There are several SDKs, agents, and tools that can be used to instrument your application for X-Ray tracing, depending on the language and framework you are using.Once instrumented, your application will send trace data to the X-Ray daemon, which collects and processes the data before sending it to the X-Ray service.Hands-On example:The first step is to install x-ray in our cluster. Let's use theokgolove/aws-xrayHelm chart.Add the okgolove Helm repository.$ helm repo add okgolove https://okgolove.github.io/helm-charts/Enter fullscreen modeExit fullscreen modeCreate axray-values.yamlfile, see the default values in values.yaml:serviceAccount:   annotations:      eks.amazonaws.com/role-arn: arn:aws:iam::*********:role/XRayAccessRole     name: aws-xray xray:   region: eu-west-1   loglevel: proEnter fullscreen modeExit fullscreen modeIn order for the X-Ray daemon to communicate with the service, we should to create a Kubernetes service account and attach an AWS Identity and Access Management (IAM) role and policy with necessary permissions.For this, we will use eksctleksctl create iamserviceaccount --name aws-xray \\ --namespace xray --role-name XRayAccessRole --cluster aws-xray \\  --attach-policy-arn arn:aws:iam::aws:policy/AWSXRayDaemonWriteAccess \\ --approve --override-existing-serviceaccountsEnter fullscreen modeExit fullscreen modeInstall the chart into the cluster, this will create a DaemonSet and a Service:$ helm -n xray install aws-xray okgolove/aws-xray -f xray-values.yamlEnter fullscreen modeExit fullscreen modeThis will deploy the X-Ray DaemonSet to the EKS cluster. The X-Ray daemon will be deployed to each worker node in the EKS cluster.For reference, see the example implementation used in this module.Checking X-Ray in\u00a0action:The AWS X-Ray SDKs are used to instrument your microservices. When using the DaemonSet in the example implementation, you need to configure it to point toaws-xray.xray:2000.The following showcases how to configure the X-Ray SDK for Go. This is merely an example and not a required step in the workshop.func init() {     xray.Configure(xray.Config{         DaemonAddr:     \"aws-xray.xray:2000\",         LogLevel:       \"info\",     }) }Enter fullscreen modeExit fullscreen modeWe now have the foundation in place to deploy microservices, which are instrumented with X-Ray SDKs, to the EKS cluster.In this step, we are going to deploy example front-end and back-end  microservices to the cluster.The example services are already instrumented using the X-Ray SDK for Go. Currently, X-Ray has SDKs for Go, Python, Node.js, Ruby,\u00a0.NET and Java.git clome https://github.com/seifrajhi/aws-xray-eks-containers-tracing.git cd aws-xray-eks-containers-tracing/ kubeclt apply -f backend/x-ray-sample-back-k8s.yml kubectl apply -f frontend/x-ray-sample-front-k8s.ymlEnter fullscreen modeExit fullscreen modeTo review the status of the deployments, you can run:kubectl describe deployments x-ray-sample-front-k8s x-ray-sample-back-k8sEnter fullscreen modeExit fullscreen modeFor the status of the services, run the following command:kubectl describe services x-ray-sample-front-k8s x-ray-sample-back-k8sEnter fullscreen modeExit fullscreen modeOnce the front-end service is deployed, run the following command to get the Elastic Load Balancer (ELB) endpoint and open it in a browser.kubectl get service x-ray-sample-front-k8s -o wideEnter fullscreen modeExit fullscreen modeAfter your ELB is deployed and available, open up the endpoint returned by the previous command in your browser and allow it to remain open. The front-end application makes a new request to the /api endpoint once per second, which in turn calls the back-end service.The JSON document displayed in the browser is the result of the request made to the back-end service.We now have the example microservices deployed, so we are going to investigate our Service Graph and Traces in X-Ray section of the AWS Management Console.The Service map in the console provides a visual representation of the steps identified by X-Ray for a particular trace. Each resource that sends data to X-Ray within the same context appears as a service in the graph.Next, go to the traces section in the AWS Management Console to view the execution times for the segments in the requests. At the top of the page, we can see the URL for the ELB endpoint and the corresponding traces below.If you click on the link on the left in the Trace list section you will see the overall execution time for the request (0.5ms for the x-ray-sample-front-k8s which wraps other segments and subsegments), as well as a breakdown of the individual segments in the request.In this visualization, you can see the front-end and back-end segments and a subsegment named x-ray-sample-back-k8s-gen In the back-end service source code, we instrumented a subsegment that surrounds a random number generator.In the Go example, the main segment is initialized in the xray.Handler helper, which in turn sets all necessary information in the http.Request context struct, so that it can be used when initializing the subsegment.Conclusion:AWS X-Ray provides valuable end-to-end visibility into containerized applications on ECS and EKS, enabling teams to optimize performance, identify issues, and deliver reliable services. By instrumenting containers with the X-Ray SDK and deploying the daemon, developers can easily trace requests across microservices and AWS resources. The intuitive X-Ray console allows quick analysis of trace data for comprehensive insights.As containerized architectures grow in complexity, X-Ray is a must-have tool for maintaining high-quality, performant applications on AWS."}
{"title": "Deploying a Bulletproof Photo Sharing App with DevSecOps Terraform, AWS, EKS and Chaos Engineering", "published_at": 1714366723, "tags": ["aws", "devsecops", "architecture", "terraform"], "user": "Ravindra Singh", "url": "https://dev.to/aws-builders/deploying-a-bulletproof-photo-sharing-app-with-devsecops-terraform-aws-eks-and-chaos-engineering-d25", "details": "\ud83d\udd12In today's digital world, security and reliability are paramount for any application, DevSecOps best practices to create a secure and reliable photo-sharing application. We'll use a combination of tools and services, including GitHub Actions, Checkov, Terraform, AWS EKS, and Prowler, to automate the software development and deployment process while ensuring security at every stage.\ud83d\udcdaSourceCode Link:https://github.com/ravindrasinghh/Deploying-a-Bulletproof-Photo-Sharing-App-with-DevSecOps-Terraform-AWS-EKS-and-Chaos-EngineeringWhat is DevSecOps?DevSecOps is a security-first approach to software development that emphasizes collaboration between development, security, and operations teams. By integrating security practices throughout the entire development lifecycle, DevSecOps helps to identify and fix vulnerabilities early on, reducing the risk of security breaches.How the Architecture WorksThe architecture we'll be using consists of several key components:Infrastructure Management with DevSecOps\ud83d\udee1\ufe0f1. Infrastructure as Code (IaC):We'll use Terraform to define our infrastructure as code. This means our infrastructure is created and managed using code, making it repeatable and manageable.2. Version Control and Automation:GitHub as your version control system to store and manage your IaC code.3. GitHub Actions:GitHub Actions automate our CI/CD pipeline,We'll use GitHub Actions to automate various tasks in our deployment pipeline, including security scanning.4. Security Scanning in the Pipeline:Integrate security scanning tools like TFSec and Checkov directly into your GitHub Actions workflows. Tools like TFSec and Checkov act as your quality inspectorsTFSec: Integrate TFSec within your GitHub Actions workflow to scan your Terraform configurations for security misconfigurations. This helps identify potential security issues within your infrastructure code early in the development process.Checkov: Similar to TFSec, Checkov reviews your infrastructure code (like Terraform) and additional configurations for things like Kubernetes. It helps ensure that everything is secure and set up correctly before anything is actually built or deployed.Code Commit and Build1. Code Commit:Developers push their code changes to a version control system like GitHub.2. Code Build Trigger:Upon a push event (or other configured triggers), GitHub Actions initiates the build process.3. Code Build with Build Spec:The code is fetched from GitHub based on the workflow configuration.Build Spec Execution: A build specification file (buildspec.yml) located in the code repository instructs the build process on what to do. This YAML file typically includes steps for installing dependencies, running tests, and building the application.Application Management with DevSecOpsCode Quality and Security Scanning:1. Git secrets scanning:This will identify any sensitive information accidentally committed to your code repository.2. SonarQube:This tool performs static code analysis to detect bugs and vulnerabilities in your code.3. Hadolint:This tool will scan your Dockerfiles for potential security vulnerabilities and best practice violations.4. Trivy:This tool scans container images for vulnerabilities before pushing them to Amazon ECR (ECR).5. Amazon ECR (ECR):Amazon ECR is a container registry that allows you to store and manage your Docker container images securely.6. Amazon Elastic Container Service (EKS):Amazon EKS is a managed Kubernetes service that allows you to easily deploy and manage containerized applications.7. Amazon DynamoDB:A NoSQL database service that can be used to store application data alongside EKS.Security after Deployment:8. Slack Notifications:Get notified on Slack about successful code builds or failures, allowing for timely intervention if needed.Amazon Security Services:1. AWS Key Management Service (KMS):Encrypts your data in DynamoDB at rest and in transit for enhanced security.\u2028AWS Web Application Firewall (WAF): Shields your application from common web attacks.2. AWS Certificate Manager (ACM):Provides easy issuance and management of SSL/TLS certificates for securecommunication.3. Amazon Route 53:Manages DNS resolution for your application, directing users to the appropriate resources.4. Amazon CloudFront:A content delivery network (CDN) that delivers content to users with high performance and low latency.5. Amazon S3:Provides secure object storage for static content like photos.6. Amazon Elastic Load Balancer:Distributes incoming traffic across your EKS cluster instances for scalability and high availability.6. AWS WAF:AWS WAF is a web application firewall that helps to protect your application from common web attacks.Security after Deployment:1. Kubescape:This tool is used to scan your Kubernetes clusters for vulnerabilities and misconfigurations to ensure they meet compliance standards.2. Prowler:Prowler is an open-source tool that helps you continuously monitor your AWS environment for security vulnerabilities and configuration issues.3. AWS Config, CloudTrail, and Inspector:These AWS services provide continuous security monitoring and auditing of your AWS resources.Chaos EngineeringAWS FISto introduce controlled chaos into your infrastructure. This involves simulating failures within your EKS cluster or other AWS resources.\u2028By injecting faults in a controlled manner, you can proactively identify weaknesses in your application's design and infrastructure. This allows you to improve your application's resilience and ability to handle unexpected events in production.Benefits of the Architecture1. Improved Security:By integrating security scanning throughout the development pipeline, we can identify and fix vulnerabilities early on in the development process. This helps to reduce the risk of security breaches.2. Increased Efficiency:Automating tasks in the deployment pipeline with GitHub Actions can save time and effort for development, security, and operations teams.3. Improved Reliability:Infrastructure as code (IaC) helps to ensure that our infrastructure is provisioned and configured consistently.4. Better Collaboration:DevSecOps fosters collaboration between development, security, and operations teams by providing a shared understanding of security best practices.5. Compliance:Tools like Kubescape and Prowler can help ensure your infrastructure meets security and compliance standards.6. Faster Feedback:Notifications from successful code builds in GitHub Actions can help development teams identify and address issues quickly.Troubleshooting:Please remember to run this script; otherwise, your CodeBuild will not authenticate with the Kubernetes cluster, and you will encounter the following errorChallenges:\ud83d\udea7Script Link:https://github.com/ravindrasinghh/Deploying-a-Bulletproof-Photo-Sharing-App-with-DevSecOps-Terraform-AWS-EKS-and-Chaos-Engineering/blob/master/iam-role-autenticate-eks.sh#!/bin/bash  # Dynamically fetch the AWS Account ID ACCOUNT_ID=$(aws sts get-caller-identity --query Account --output text) ROLE_NAME=\"EksCodeBuildKubectlRole\"  # Function to check if the IAM role exists check_role_existence() {     aws iam get-role --role-name $ROLE_NAME > /dev/null 2>&1     echo $? }  # Set the trust relationship policy JSON correctly TRUST_POLICY='{   \"Version\": \"2012-10-17\",   \"Statement\": [     {       \"Effect\": \"Allow\",       \"Principal\": {         \"AWS\": \"arn:aws:iam::'\"${ACCOUNT_ID}\"':root\"       },       \"Action\": \"sts:AssumeRole\"     }   ] }'  # Create IAM Role if it does not exist if [ $(check_role_existence) -ne 0 ]; then     echo \"Creating IAM role...\"     aws iam create-role --role-name $ROLE_NAME --assume-role-policy-document \"$TRUST_POLICY\" else     echo \"Role already exists, skipping creation...\" fi  # Define inline policy for describing EKS resources INLINE_POLICY='{   \"Version\": \"2012-10-17\",   \"Statement\": [     {       \"Effect\": \"Allow\",       \"Action\": \"eks:Describe*\",       \"Resource\": \"*\"     }   ] }'  # File path for inline policy POLICY_FILE=\"/tmp/iam-eks-describe-policy\" echo \"$INLINE_POLICY\" > $POLICY_FILE  # Attach the policy to the role aws iam put-role-policy --role-name $ROLE_NAME --policy-name eks-describe --policy-document \"file://$POLICY_FILE\"  # Prepare the role definition for the aws-auth configmap ROLE_DEF=\"  - rolearn: arn:aws:iam::${ACCOUNT_ID}:role/${ROLE_NAME}     username: build     groups:       - system:masters\"  # Get current aws-auth configMap data, check for existing role and append if not present kubectl get configmap aws-auth -n kube-system -o yaml > /tmp/aws-auth-original.yml if ! grep -q \"$ROLE_NAME\" /tmp/aws-auth-original.yml; then     kubectl get -n kube-system configmap/aws-auth -o yaml | awk \"/mapRoles: \\|/{print;print \\\"$ROLE_DEF\\\";next}1\" > /tmp/aws-auth-patch.yml     kubectl patch configmap aws-auth -n kube-system --patch \"$(cat /tmp/aws-auth-patch.yml)\" else     echo \"Role already in aws-auth, no changes made...\" fi  # Verify the updated configMap kubectl get configmap aws-auth -o yaml -n kube-systemEnter fullscreen modeExit fullscreen modeCost Calculations\ud83d\udcb5Region: MumbaiAmazon EKS:1 Cluster x $0.10 per hour x 730 hours per month = $73.00 USDAmazon ECR:100 GB per month x $0.10 = $10.00 USDEC2 Instances:1 Instance x $0.111 per hour x 730 hours in a month = $81.03 USDFor 2 Instances = $81.03 x 2 = $162.06 USDNetwork Load Balancer (NLB):1 Load Balancer x $0.0239 per hour x 730 hours in a month = $17.45 USDIf you prefer a video tutorial to help guide you through the setup of deploying a Bulletproof Photo Sharing App with DevSecOps Terraform, AWS, EKS and Chaos Engineering"}
{"title": "Deploying a Bulletproof Photo Sharing App with DevSecOps Terraform, AWS, EKS and Chaos Engineering", "published_at": 1714365548, "tags": ["architecture", "devsecops", "aws", "terraform"], "user": "Ravindra Singh", "url": "https://dev.to/aws-builders/deploying-a-bulletproof-photo-sharing-app-with-devsecops-terraform-aws-eks-and-chaos-engineering-4p57", "details": "In today's digital world, security and reliability are paramount for any application, DevSecOps best practices to create a secure and reliable photo-sharing application. We'll use a combination of tools and services, including GitHub Actions, Checkov, Terraform, AWS EKS, and Prowler, to automate the software development and deployment process while ensuring security at every stage.SourceCode Link:https://github.com/ravindrasinghh/Deploying-a-Bulletproof-Photo-Sharing-App-with-DevSecOps-Terraform-AWS-EKS-and-Chaos-EngineeringWhat is DevSecOps?DevSecOps is a security-first approach to software development that emphasizes collaboration between development, security, and operations teams. By integrating security practices throughout the entire development lifecycle, DevSecOps helps to identify and fix vulnerabilities early on, reducing the risk of security breaches.How the Architecture WorksThe architecture we'll be using consists of several key components:Infrastructure Management with DevSecOps1. Infrastructure as Code (IaC):We'll use Terraform to define our infrastructure as code. This means our infrastructure is created and managed using code, making it repeatable and manageable.2. Version Control and Automation:GitHub as your version control system to store and manage your IaC code.3. GitHub Actions:GitHub Actions automate our CI/CD pipeline,We'll use GitHub Actions to automate various tasks in our deployment pipeline, including security scanning.4. Security Scanning in the Pipeline:Integrate security scanning tools like TFSec and Checkov directly into your GitHub Actions workflows. Tools like TFSec and Checkov act as your quality inspectorsTFSec: Integrate TFSec within your GitHub Actions workflow to scan your Terraform configurations for security misconfigurations. This helps identify potential security issues within your infrastructure code early in the development process.Checkov: Similar to TFSec, Checkov reviews your infrastructure code (like Terraform) and additional configurations for things like Kubernetes. It helps ensure that everything is secure and set up correctly before anything is actually built or deployed.Code Commit and Build1. Code Commit:Developers push their code changes to a version control system like GitHub.2. Code Build Trigger:Upon a push event (or other configured triggers), GitHub Actions initiates the build process.3. Code Build with Build Spec:The code is fetched from GitHub based on the workflow configuration.Build Spec Execution: A build specification file (buildspec.yml) located in the code repository instructs the build process on what to do. This YAML file typically includes steps for installing dependencies, running tests, and building the application.Application Management with DevSecOpsCode Quality and Security Scanning:1. Git secrets scanning:This will identify any sensitive information accidentally committed to your code repository.2. SonarQube:This tool performs static code analysis to detect bugs and vulnerabilities in your code.3. Hadolint:This tool will scan your Dockerfiles for potential security vulnerabilities and best practice violations.4. Trivy: This tool scans container images for vulnerabilities before pushing them to Amazon ECR (ECR).5. Amazon ECR (ECR):Amazon ECR is a container registry that allows you to store and manage your Docker container images securely.6. Amazon Elastic Container Service (EKS):Amazon EKS is a managed Kubernetes service that allows you to easily deploy and manage containerized applications.7. Amazon DynamoDB:A NoSQL database service that can be used to store application data alongside EKS.\u2028Security after Deployment:8. Slack Notifications:Get notified on Slack about successful code builds or failures, allowing for timely intervention if needed.Amazon Security Services:1. AWS Key Management Service (KMS):Encrypts your data in DynamoDB at rest and in transit for enhanced security.\u2028AWS Web Application Firewall (WAF): Shields your application from common web attacks.2. AWS Certificate Manager (ACM):Provides easy issuance and management of SSL/TLS certificates for secure communication.\u2028Amazon Route 53: Manages DNS resolution for your application, directing users to the appropriate resources.3. Amazon CloudFront:A content delivery network (CDN) that delivers content to users with high performance and low latency.4. Amazon S3:Provides secure object storage for static content like photos.5. Amazon Elastic Load Balancer:Distributes incoming traffic across your EKS cluster instances for scalability and high availability.6. AWS WAF:AWS WAF is a web application firewall that helps to protect your application from common web attacks.Security after Deployment:1. Kubescape:This tool is used to scan your Kubernetes clusters for vulnerabilities and misconfigurations to ensure they meet compliance standards.2. Prowler:Prowler is an open-source tool that helps you continuously monitor your AWS environment for security vulnerabilities and configuration issues.3. AWS Config, CloudTrail, and Inspector:These AWS services provide continuous security monitoring and auditing of your AWS resources.Chaos EngineeringAWS FISto introduce controlled chaos into your infrastructure. This involves simulating failures within your EKS cluster or other AWS resources.\u2028By injecting faults in a controlled manner, you can proactively identify weaknesses in your application's design and infrastructure. This allows you to improve your application's resilience and ability to handle unexpected events in production.Benefits of the Architecture1. Improved Security:By integrating security scanning throughout the development pipeline, we can identify and fix vulnerabilities early on in the development process. This helps to reduce the risk of security breaches.2. Increased Efficiency:Automating tasks in the deployment pipeline with GitHub Actions can save time and effort for development, security, and operations teams.3. Improved Reliability:Infrastructure as code (IaC) helps to ensure that our infrastructure is provisioned and configured consistently.4. Better Collaboration:DevSecOps fosters collaboration between development, security, and operations teams by providing a shared understanding of security best practices.5. Compliance:Tools like Kubescape and Prowler can help ensure your infrastructure meets security and compliance standards.6. Faster Feedback:Notifications from successful code builds in GitHub Actions can help development teams identify and address issues quickly.Troubleshooting:Please remember to run this script; otherwise, your CodeBuild will not authenticate with the Kubernetes cluster, and you will encounter the following errorScript Link:https://github.com/ravindrasinghh/Deploying-a-Bulletproof-Photo-Sharing-App-with-DevSecOps-Terraform-AWS-EKS-and-Chaos-Engineering/blob/master/iam-role-autenticate-eks.sh#!/bin/bash  # Dynamically fetch the AWS Account ID ACCOUNT_ID=$(aws sts get-caller-identity --query Account --output text) ROLE_NAME=\"EksCodeBuildKubectlRole\"  # Function to check if the IAM role exists check_role_existence() {     aws iam get-role --role-name $ROLE_NAME > /dev/null 2>&1     echo $? }  # Set the trust relationship policy JSON correctly TRUST_POLICY='{   \"Version\": \"2012-10-17\",   \"Statement\": [     {       \"Effect\": \"Allow\",       \"Principal\": {         \"AWS\": \"arn:aws:iam::'\"${ACCOUNT_ID}\"':root\"       },       \"Action\": \"sts:AssumeRole\"     }   ] }'  # Create IAM Role if it does not exist if [ $(check_role_existence) -ne 0 ]; then     echo \"Creating IAM role...\"     aws iam create-role --role-name $ROLE_NAME --assume-role-policy-document \"$TRUST_POLICY\" else     echo \"Role already exists, skipping creation...\" fi  # Define inline policy for describing EKS resources INLINE_POLICY='{   \"Version\": \"2012-10-17\",   \"Statement\": [     {       \"Effect\": \"Allow\",       \"Action\": \"eks:Describe*\",       \"Resource\": \"*\"     }   ] }'  # File path for inline policy POLICY_FILE=\"/tmp/iam-eks-describe-policy\" echo \"$INLINE_POLICY\" > $POLICY_FILE  # Attach the policy to the role aws iam put-role-policy --role-name $ROLE_NAME --policy-name eks-describe --policy-document \"file://$POLICY_FILE\"  # Prepare the role definition for the aws-auth configmap ROLE_DEF=\"  - rolearn: arn:aws:iam::${ACCOUNT_ID}:role/${ROLE_NAME}     username: build     groups:       - system:masters\"  # Get current aws-auth configMap data, check for existing role and append if not present kubectl get configmap aws-auth -n kube-system -o yaml > /tmp/aws-auth-original.yml if ! grep -q \"$ROLE_NAME\" /tmp/aws-auth-original.yml; then     kubectl get -n kube-system configmap/aws-auth -o yaml | awk \"/mapRoles: \\|/{print;print \\\"$ROLE_DEF\\\";next}1\" > /tmp/aws-auth-patch.yml     kubectl patch configmap aws-auth -n kube-system --patch \"$(cat /tmp/aws-auth-patch.yml)\" else     echo \"Role already in aws-auth, no changes made...\" fi  # Verify the updated configMap kubectl get configmap aws-auth -o yaml -n kube-systemEnter fullscreen modeExit fullscreen modeCost CalculationsRegion: MumbaiAmazon EKS:1 Cluster x $0.10 per hour x 730 hours per month = $73.00 USDAmazon ECR:100 GB per month x $0.10 = $10.00 USDEC2 Instances:1 Instance x $0.111 per hour x 730 hours in a month = $81.03 USDFor 2 Instances = $81.03 x 2 = $162.06 USDNetwork Load Balancer (NLB):1 Load Balancer x $0.0239 per hour x 730 hours in a month = $17.45 USDIf you prefer a video tutorial to help guide you through the setup of deploying a Bulletproof Photo Sharing App with DevSecOps Terraform, AWS, EKS and Chaos Engineering"}
{"title": "Claude Function calls with AWS Step Functions", "published_at": 1714325986, "tags": ["aws", "serverless", "llm", "rust"], "user": "szymon-szym", "url": "https://dev.to/aws-builders/claude-function-calls-with-aws-step-functions-46l", "details": "Photo byKatie RodriguezonUnsplashIntroductionIt is hard to follow the latest news from the gen AI world. The hype is overwhelming and many of us might feel the \"AI fatigue\". On the other hand, the possibilities opened by LLMs are truly fascinating.Even staying away from the latest news outlets it is clear that one of the hottest topics is so-called \"agency\". It would be nice to see how models can \"do things\".To be honest I am not that excited by models \"doing things\" like booking taxis or ordering groceries. I am more intrigued by the possibility of using \"agent\" capabilities to gather relevant information to answer the user's query.In this blog post, I would like to explore how LLM will use the provided \"tools\" to gather data.General flowFor simplicity's sake, the flow is straightforward.The user sends a query to our app. Then, based on a list of provided \"tools\", the model decides if the query could be answered using the provided toolset. If yes - it calls picked functions with parameters extracted from the user's query. Once data is gathered the answer for the user is prepared based on it.StackThe description above fits the AWS Bedrock Agents. They are super handy.I will, however, use the Step Function to construct the flow by myself. This way I won't be limited to models supported by AWS Bedrock.As LLM I use Claude 3. Anthropic released (as a public beta) new APIs forfunction calls. New APIs are not yet available on Bedrock, that's why I use them directly from Anthropic.Finally, I use the Lambda function to glue the logic and perform astools.I utilize AWS CDK as IaC and Rust for Lambda's code (withcargo lambda).SolutionCode is available in this repoLet's create the AWS CDK project.mkdirllm_step&&cd$_cdk init--languagetypescriptEnter fullscreen modeExit fullscreen modeFlow I plan to prepare looks like this:We need to let the model know what tools it could use. If it decides to do so, we are responsible for calling the tools on our side and providing a model back with the whole context including used tools.LLM callAWS Step Functions have built-in actions for calling AWS Bedrock. In my case I need to call third-party API and use, well,Call third-party APIaction.Call third-party APIaction needs aConnectionto be configured with the proper authentication method. The shape of the expected header is definedin the Anthropic docsIn CDK it looks like this// lib/llm_step-stack.tsconstllmAPIKey=newsecrets.Secret(this,\"LlmApiKey\",{});constllmDestinationConnection=newevents.Connection(this,\"LlmDestinationConnection\",{authorization:events.Authorization.apiKey(\"x-api-key\",llmAPIKey.secretValue),description:\"LLM Destination Connection\",});Enter fullscreen modeExit fullscreen modeFor some reason I wasn't able to deploy the stack without creating a separate secret. Based on AWS CDK examples from documentation it should work without itOnce the connection is in place I can define a call to LLM// lib/llm_step-stack.tsconstcallLlmTask=newtasks.HttpInvoke(this,\"Call LLM\",{apiRoot:\"https://api.anthropic.com\",apiEndpoint:sfn.TaskInput.fromText(\"/v1/messages\"),body:sfn.TaskInput.fromObject(getLlmPrompt()),connection:llmDestinationConnection,headers:sfn.TaskInput.fromObject({\"Content-Type\":\"application/json\",\"anthropic-version\":\"2023-06-01\",\"anthropic-beta\":\"tools-2024-04-04\",}),method:sfn.TaskInput.fromText(\"POST\"),resultSelector:{\"role.$\":\"$.ResponseBody.role\",\"content.$\":\"$.ResponseBody.content\",\"stop_reason.$\":\"$.ResponseBody.stop_reason\",},resultPath:\"$.taskResult\",});Enter fullscreen modeExit fullscreen modeHeaders, content type, and URL are definedin docs. I moved the prompt to the other file to avoid bloating the infrastructure definition. The result of the call mapsrole,content, andstop_reason. The result is passed astaskResult.LLM promptI am only scratching the surface of the prompt preparation. Basically I followed instructions fromtool useexamples on theAnthropic siteThe API call contains defined fields:model,max_tokens,system,tools, andmessages.Systemis a field to pass additional instructions for the model.Toolsis a list of tools defined with JSONSchemaMessagesis a chain creating a conversation between the user and the assistant.My prompt is defined in a separate file:exporttypeLLMPrompt={model:string;max_tokens:number;system:string;tools:any[];messages:any[];};exportconstgetLlmPrompt=()=>{return{model:\"claude-3-sonnet-20240229\",max_tokens:400,system:\"Before answering the question, please think about it step-by-step within <thinking></thinking> tags. Then, provide your final answer within <answer></answer> tags. Skip the introduction and start from <thinking> tag.\",tools:[{name:\"get_weather\",description:\"Get the current weather in a given location. Weather is defined base on city name and state or country.\",input_schema:{type:\"object\",properties:{location:{type:\"string\",description:\"The city and state, <example>San Francisco, CA</example> <example>Berlin, Germany</example>\",},},required:[\"location\"],},},{name:\"get_restaurants\",description:\"Get the list of recommended restaurants in the given city. It provides information if the given facility offers outdoor seating. Restaurants are grouped by reviews from guests\",input_schema:{type:\"object\",properties:{location:{type:\"string\",description:\"The city and state, <example>San Francisco, CA</example> <example>Berlin, Germany</example>\",},},required:[\"location\"],},},],\"messages.$\":\"$.messages\",};};Enter fullscreen modeExit fullscreen modeWhat is important here is to explicitly prompt Claude to document the thinking process. This is a simple way to improve the quality of responses (Opus - the biggest model from the Cluade 3 family will do it automatically)I use the JSONPath notation to let Step Functions know that for this task I will use messages from the task's input.Handle LLM responseEach time model can decide to use tools or to skip. I have achoicetask to check if I need to call the lambda function with tools, or if should I just pass the final answer. I can do it based onstop_reasonfield in thetaskResult// lib/llm_step-stack.tsconstpassAnswer=newsfn.Pass(this,\"Answer\");constcallToolsTask=newtasks.LambdaInvoke(this,\"Call tools\",{lambdaFunction:toolsLambda,resultSelector:{\"messages.$\":\"$.Payload\",},}).next(callLlmTask);constchoiceIfUseTool=newsfn.Choice(this,\"Choice if use tool\");choiceIfUseTool.when(sfn.Condition.stringEquals(\"$.taskResult.stop_reason\",\"tool_use\"),callToolsTask);choiceIfUseTool.otherwise(passAnswer);Enter fullscreen modeExit fullscreen modeLambda function for toolsI use Rust for my lambda. I don't want to paste the whole code into this post (it is in this repo). Let me just stress a few moments.Deserialize LLM answerLLM doesn't run any tool. It returns a response, which contains instructions about tools to be used, but it's our responsibility to use them. The key thing is the structure of those instructions. They might look this way:{\"id\":\"msg_01Aq9w938a90dw8q\",\"model\":\"claude-3-opus-20240229\",\"stop_reason\":\"tool_use\",\"role\":\"assistant\",\"content\":[{\"type\":\"text\",\"text\":\"<thinking>I need to use the get_weather, and the user wants SF, which is likely San Francisco, CA.</thinking>\"},{\"type\":\"tool_use\",\"id\":\"toolu_01A09q90qw90lq917835lq9\",\"name\":\"get_weather\",\"input\":{\"location\":\"San Francisco, CA\",\"unit\":\"celsius\"}}]}Enter fullscreen modeExit fullscreen modeWhat is awesome is that the LLM answer is structured and can be easily converted to the domain and used down the pipeline.One of the neatest features of Rust is the powerful type system. It is easy to defineLlmToolRequestwhich can be eithertextortool_use#[derive(Debug,Serialize,Deserialize,Clone)]#[serde(untagged,rename_all=\"snake_case\")]pub(crate)enumLlmToolRequest{LlmToolTextRequest(LlmToolTextRequest),LlmToolUseRequest(LlmToolUseRequest),}Enter fullscreen modeExit fullscreen modeIn the same way, the input for the given function calls is also serialized into specific types and can be easily used in the application#[derive(Debug,Serialize,Deserialize)]#[serde(untagged,rename_all=\"snake_case\")]pub(crate)enumLlmToolInput{GetWeather(GetWeatherToolInput),GetRestaurantsToolInput(GetRestaurantsToolInput),}Enter fullscreen modeExit fullscreen modeUsually, it is ok to letserde_jsondeserialize JSON and figure out all types. In my example, bothtoolsuse the same shape of parameters, so I need to add an extra step and understand what type of tool is called.To do so I initially let theinputbe a genericserde_json::value::Value//...#[derive(Debug,Serialize,Deserialize,Clone)]pub(crate)structLlmToolUseRequest{#[serde(rename=\"type\")]pub(crate)request_type:String,pub(crate)id:String,pub(crate)name:String,pub(crate)input:serde_json::value::Value,}//...Enter fullscreen modeExit fullscreen modeThen I pattern match the request type using a deserializedname//...lettool_result=matchreq_use.name.as_str(){\"get_weather\"=>{letinp=serde_json::from_value::<GetWeatherToolInput>(input).unwrap();get_weather(req_use.id.clone(),inp)}\"get_restaurants\"=>{letinp=serde_json::from_value::<GetRestaurantsToolInput>(input).unwrap();get_restaurants(req_use.id.clone(),inp)}_=>panic!(\"unknown tool name\"),};//...Enter fullscreen modeExit fullscreen modeReturn the answer from lambdaThe result of the Lambda will be passed back to the LLM call step in the state machine. It needs to be constructed from 3 pieces:all previous messages created during the flowlast LLM response used to call toolsresult from toolsThe first two can be passed to the result. The last one needs to have a specific shape andtool_use_idpassed with previous messages.{\"role\":\"user\",\"content\":[{\"type\":\"tool_result\",\"tool_use_id\":\"toolu_01A09q90qw90lq917835lq9\",\"content\":\"15 degrees\"}]}Enter fullscreen modeExit fullscreen modeInthe documentationare examples of passing back errors or empty responses.It is a good idea to keep the response from the tool simple so LLM can understand and use it.  Sometimes we would need to pass more complex data.As far as I understood, Claude 3 understands the content structured with XML tags.In theget_restaurantfunction I return a list of two restaurants with some details.  My function looks like this://...pub(crate)fnget_restaurants(tool_use_id:String,input:GetRestaurantsToolInput,)->LlmToolResult{println!(\"checking restaurants for {}\",input.location);LlmToolResult{result_type:String::from(\"tool_result\"),tool_use_id,content:String::from(r#\" <restaurants>             <restaurant>                 <name>Restaurant ABC</name>                 <address>Street 111</address>                 <phone>12345678</phone>                 <website>www.restaurant1.com</website>                 <cuisine>Italian</cuisine>                 <outdoor>true</outdoor>             </restaurant>             <restaurant>                 <name>Restaurant XYZ</name>                 <address>Street 999</address>                 <phone>987654</phone>                 <website>www.restaurant2.com</website>                 <cuisine>French</cuisine>                 <outdoor>false</outdoor>             </restaurant> </restaurants> \"#,),}}Enter fullscreen modeExit fullscreen modeDeployLambda Function is defined in the CDK// lib/llm_step-stack.tsconsttoolsLambda=newlambda.Function(this,\"llm_tools_lambda\",{runtime:lambda.Runtime.PROVIDED_AL2,code:lambda.Code.fromAsset(\"lib/functions/llm_tools/target/lambda/llm_tools/\"),handler:\"not.required\",memorySize:256,timeout:cdk.Duration.seconds(30),});Enter fullscreen modeExit fullscreen modeIn VSCode we can have a look at the shape of state machine flow defined with CDKTo deploy the solution, once the rust function is built withcargo lambdait is enough to runcdk bootstrap npm run build&&cdk deployEnter fullscreen modeExit fullscreen modeTestingLet's start from basic queryAs expected, LLM decided to call the tool to check the weather in the given city. Interestingly enough it figured out in which country Warsaw is located and augmented input parameters with this knowledge. The full chain of messages including the final result:{\"messages\":[{\"content\":\"What is the current weather in Warsaw\",\"role\":\"user\"},{\"role\":\"assistant\",\"content\":[{\"type\":\"text\",\"text\":\"<thinking>\\nTo get the current weather in Warsaw, I can use the\\\"get_weather\\\"tool, providing\\\"Warsaw, Poland\\\"as the location parameter.\"},{\"type\":\"tool_use\",\"id\":\"toolu_0192GHrwDaPKDhe5PryN9zqn\",\"name\":\"get_weather\",\"input\":{\"location\":\"Warsaw, Poland\"}}]},{\"role\":\"user\",\"content\":[{\"type\":\"tool_result\",\"tool_use_id\":\"toolu_0192GHrwDaPKDhe5PryN9zqn\",\"content\":\"The weather is sunny, 20 degrees\"}]}],\"taskResult\":{\"role\":\"assistant\",\"stop_reason\":\"end_turn\",\"content\":[{\"type\":\"text\",\"text\":\"\\n</thinking>\\n\\n<answer>\\nThe current weather in Warsaw is sunny with a temperature of 20 degrees Celsius.\\n</answer>\"}]}}Enter fullscreen modeExit fullscreen modeLet's check something different.What are football teams in Madrid\"As expected the model decided not to call tools. In this case it simply refused to give the answer:{\"messages\":[{\"role\":\"user\",\"content\":\"What are football teams in Madrid\"}],\"taskResult\":{\"role\":\"assistant\",\"stop_reason\":\"end_turn\",\"content\":[{\"type\":\"text\",\"text\":\"Okay, let me think through this step-by-step:\\n<thinking>\\nTo get a list of football teams in Madrid, I will need to use some external data source or API that has information on sports teams in different cities and locations. Unfortunately, the tools provided here do not seem to have that kind of information about sports teams.\\n\\nThe\\\"get_weather\\\"tool allows getting weather information for a location, and the\\\"get_restaurants\\\"tool provides recommended restaurants in a city, but neither of these contain data about sports teams.\\n</thinking>\\n<answer>\\nUnfortunately, I do not have access to any tools or data sources that can directly provide a list of football teams in Madrid. The tools available to me are limited to getting weather information and restaurant recommendations for a given city location. I do not have a way to lookup and retrieve details about sports teams or clubs based on the city.\\n</answer>\"}]}}Enter fullscreen modeExit fullscreen modeOk, nice. Now let's try to make LLM use both provided tools.Find a place for a dinner suitable for the current weather in Barcelona\"{\"messages\":[{\"content\":\"Find place for a dinner suitable for the current weather in Barcelona\",\"role\":\"user\"},{\"content\":[{\"text\":\"Okay, let me think through this step-by-step:\\n<thinking>\\n1. To find a suitable place for dinner based on the current weather in Barcelona, I first need to get the current weather conditions there.\\n2. I can use the\\\"get_weather\\\"tool to retrieve the weather information for Barcelona:\",\"type\":\"text\"},{\"id\":\"toolu_01Bi8u7Ducrn4ECy6mHSEp7v\",\"input\":{\"location\":\"Barcelona, Spain\"},\"name\":\"get_weather\",\"type\":\"tool_use\"}],\"role\":\"assistant\"},{\"content\":[{\"content\":\"The weather is sunny, 20 degree\",\"tool_use_id\":\"toolu_01Bi8u7Ducrn4ECy6mHSEp7v\",\"type\":\"tool_result\"}],\"role\":\"user\"},{\"role\":\"assistant\",\"content\":[{\"type\":\"text\",\"text\":\"3. Since the weather is nice and sunny with moderate temperature, an outdoor / open-air dining option would be ideal.\\n4. To get recommendations for restaurants with outdoor seating in Barcelona, I can use the\\\"get_restaurants\\\"tool:\"},{\"type\":\"tool_use\",\"id\":\"toolu_01MjmMU51eD9Z61XKB7xEz24\",\"name\":\"get_restaurants\",\"input\":{\"location\":\"Barcelona, Spain\"}}]},{\"role\":\"user\",\"content\":[{\"type\":\"tool_result\",\"tool_use_id\":\"toolu_01MjmMU51eD9Z61XKB7xEz24\",\"content\":\"\\n<restaurants>\\n<restaurant>\\n<name>Restaurant ABC</name>\\n<address>Street 111</address>\\n<phone>12345678</phone>\\n<website>www.restaurant1.com</website>\\n<cuisine>Italian</cuisine>\\n<outdoor>true</outdoor>\\n</restaurant>\\n<restaurant>\\n<name>Restaurant XYZ</name>\\n<address>Street 999</address>\\n<phone>987654</phone>\\n<website>www.restaurant2.com</website>\\n<cuisine>French</cuisine>\\n<outdoor>false</outdoor>\\n</restaurant>\\n</restaurants>\\n\"}]}],\"taskResult\":{\"role\":\"assistant\",\"stop_reason\":\"end_turn\",\"content\":[{\"type\":\"text\",\"text\":\"</thinking>\\n\\n<answer>\\nBased on the sunny and comfortable weather in Barcelona today, an ideal dinner spot would be Restaurant ABC, which offers outdoor/open-air seating and serves Italian cuisine. The restaurant is located at Street 111, with website www.restaurant1.com and phone 12345678. Its outdoor dining area allows you to enjoy your meal while basking in the nice Barcelona weather.\\n</answer>\"}]}}Enter fullscreen modeExit fullscreen modeSummaryCreating flow with Step Functions is a pleasant experience in general. For this simple example, the prepared flow is straightforward but in a more realistic environment, there would be more steps.The potential limitation is the size of the output which can be passed between tasks, but it could be overcome with the utilization of S3 (native Step Functions integration with Bedrock does it automatically)It is great to see more structured output from the model. Of course, we can prompt the model to use XML tags in the response, but I wouldn't rely on this. New Anthropic APIs help with handling the flow.Even though the overall solution looks just like everystandardsystem, it is not deterministic by any meaning. The same query for different cities might result in a totally different response. Sometimes the model decides that it has enough information to provide the response, and sometimes it refuses to answer. It's worth keeping this in mind.As for picking Claude 3 models, I was impressed by the results provided by Sonnet. The biggest one - Opus (which is not a surprise) does much better, especially when it comes to reasoning about its own decisions. In my case, Haiku failed to use tools in the meaningful way."}
{"title": "Multiple operations in a single DynamoDB update expression", "published_at": 1714324820, "tags": ["aws", "tutorial", "database", "cloud"], "user": "Thomas Taylor", "url": "https://dev.to/aws-builders/multiple-operations-in-a-single-dynamodb-update-expression-3j3c", "details": "DynamoDB supports anUpdateItemoperation that modifies an existing or creates a new item. TheUpdateItemaccepts anUpdateExpressionthat dictates which operations will occur on the specified item.In this post, we'll explore how to perform multiple operations including within a clause/keyword and with multiple clauses/keywords.What is an update expressionAn update expression supports four keywords:SET(modify or add item attributes)REMOVE(deleting attributes from an item)ADD(updating numbers and sets)DELETE(removing elements from a set)The syntax for an update expression is as follows:update-expression ::=     [ SET action [, action] ... ]     [ REMOVE action [, action] ...]     [ ADD action [, action] ... ]     [ DELETE action [, action] ...]Enter fullscreen modeExit fullscreen modeAs you can view, DynamoDB supports four main clauses that begin with one of the specified operations.For a detailed explanation of update expressions,please refer to the AWS documentation.For demonstration purposes, I'm going to create a DynamoDB table namedbookswith a number hash key namedid.aws dynamodb create-table\\--table-namebooks\\--attribute-definitionsAttributeName=id,AttributeType=N\\--key-schemaAttributeName=id,KeyType=HASH\\--provisioned-throughputReadCapacityUnits=5,WriteCapacityUnits=5Enter fullscreen modeExit fullscreen modeI'll also add an item to the table:{\"id\":{\"N\":\"1\"},\"title\":{\"S\":\"Iliad\"},\"author\":{\"S\":\"Homer\"},\"genre\":{\"S\":\"Epic Poetry\"},\"publication_year\":{\"N\":\"1488\"}}Enter fullscreen modeExit fullscreen modeaws dynamodb put-item\\--table-namebooks\\--itemfile://book.jsonEnter fullscreen modeExit fullscreen modeHow to specify a single keyword in DynamoDB update expressionLet's showcase a basic use-case of adding an element to an existing item:aws dynamodb update-item\\--table-namebooks\\--key'{\"id\":{\"N\":\"1\"}}'\\--update-expression\"SET category = :c\"\\--expression-attribute-values'{\":c\":{\"S\":\"Poem\"}}'Enter fullscreen modeExit fullscreen modeNow the item has a newcategoryattribute:aws dynamodb get-item\\--table-namebooks\\--key'{\"id\":{\"N\":\"1\"}}'\\--projection-expression\"category\"Enter fullscreen modeExit fullscreen modeOutput:{\"Item\":{\"category\":{\"S\":\"Poem\"}}}Enter fullscreen modeExit fullscreen modeHow to specify a single keyword with multiple actions in DynamoDB update expressionTo showcase mutlipleSEToperations in a single DynamoDB update expression, we'll use a comma (,) to separate them.In the following example, we're changing thecategoryfrompoem->poetryand adding a new element namedgenre.aws dynamodb update-item\\--table-namebooks\\--key'{\"id\":{\"N\":\"1\"}}'\\--update-expression\"SET category = :c, genre = :g\"\\--expression-attribute-values'{\":c\":{\"S\":\"poetry\"}, \":g\":{\"S\":\"Epic poetry\"}}'\\--return-valuesUPDATED_NEWEnter fullscreen modeExit fullscreen modeOutput:{\"Attributes\":{\"category\":{\"S\":\"poetry\"},\"genre\":{\"S\":\"Epic poetry\"}}}Enter fullscreen modeExit fullscreen modeAs you see, theupdate-expressionincludes a comma between the two operations:SET category = :c, genre = :gHow to specify multiple keywords with multiple actios in DynamoDB update expressionLastly, let's showcase leveragingmultiplekeywords with their respective actions.We're going to perform the following:Set a new string attribute namedproduct_categorywith a value ofpoemRemove thecategoryattributeAdd1to existingcheckout_totalnumber or initialize it with the number value of1AddTrojan Warto existingsubjectsset or initialize it with the first index being string value ofTrojan Waraws dynamodb update-item\\--table-namebooks\\--key'{\"id\":{\"N\":\"1\"}}'\\--update-expression\"SET #pc = :pc REMOVE #c ADD #ct :inc, #s :s\"\\--expression-attribute-names'{\"#s\":\"subjects\",\"#pc\":\"product_category\",\"#c\":\"category\",\"#ct\":\"checkout_total\"}'\\--expression-attribute-values'{\":s\":{\"SS\":[\"Trojan War\"]},\":pc\":{\"S\":\"poem\"},\":inc\":{\"N\":\"1\"}}'\\--return-valuesUPDATED_NEWEnter fullscreen modeExit fullscreen modeOutput:{\"Attributes\":{\"checkout_total\":{\"N\":\"1\"},\"subjects\":{\"SS\":[\"Trojan War\"]},\"product_category\":{\"S\":\"poem\"}}}Enter fullscreen modeExit fullscreen modeIt's important to note that because theADDoperation will continue incrementing and theSETdata structure doesn't allow duplicates, we can rerun the same operation andcheckout_totalwill increase by1.For readability, let's substitute theexpression-attribute-namesandexpression-attribute-valuesback into theupdate-expression:SET product_category = poem REMOVE category ADD checkout_total 1, subjects [\"Trojan War\"]Enter fullscreen modeExit fullscreen modeTo summarize, using mutliple clauses in a DynamoDBUpdateExpressionis supported by simply separating the clauses by keywords.If you want to perform multiple operations using the same clause, use a comma (,) to separate them."}
{"title": "Surprisingly Powerful \u2013 Serverless WASM with Rust Article 1", "published_at": 1714237626, "tags": ["rust", "webdev", "aws", "serverless"], "user": "Benjamen Pyle", "url": "https://dev.to/aws-builders/surprisingly-powerful-serverless-wasm-with-rust-article-1-fm3", "details": "It's been a while since I wrote a series going back almost 9 months to myBuilding Serverless Applications.  I enjoyed that so much that I have been wanting to write another, but just didn't have a continuous thread of material. But lately, I've been thinking a lot about \"full stack\" development and where the future of delivery is going.  Some of that thinking has led me down a path of what if.  What if I was able to use my favoriteprogramming languageand my preferred AWS Serverless tools to build full-stack web applications?  I'm not 100% sure I'd do this in production at the moment, but again, I'm exploring what if.  This series is the expansion of that thought.  Let's get started on Serverless Web Assembly (WASM) with Rust.Series ArticlesSince this is article #1 and I'm not exactly sure where this is going to land, let's just call this article:Getting started with Serverless WASM with RustAnd with that title, here is the architecture diagram that I think I want to work through as we make our way into this series.Getting Started with Serverless WASM with RustBefore We BeginI feel that before we dive too deep into this topic, let's first take a moment to talk about Web Assembly (WASM). The WASM website defines it like this:WebAssembly (abbreviated Wasm) is a binary instruction format for a stack-based virtual machine. Wasm is designed as a portable compilation target for programming languages, enabling deployment on the web for client and server applications. -https://webassembly.orgImagine being able to ship a Rust binary that is compiled to target WASM which can then be run natively inside of the browser.  That same code could interact with browser APIs just like JavaScript currently does.  It can also interop with JavaScript so you could have both running at the same time.  What this means for the developer is this.  Rust-built code could be web deployable binaries that run in a browser just like normal HTML, CSS, and JavaScript that we've all become used to.But Why?I believe that just like JavaScript made its way into the server-side space, compiled languages like Rust can make their way up into the client-side space as well.  And that thinking brings choice to engineers and I think when we have a choice, customers win.  If I believe that WASM is similar to Lambda, there's still a LONG way to go for there to be parity.  But what I want to look at over the next few articles is how close things are to being able to build something that could be shipped.In addition, andI've written about this a bunch, the developer experience for Rust is something I greatly enjoy.  And then pair it with performance boosts, being strongly typed, and just the overall joy of Rust-based development, this feels like something I need to explore.A Hello World Web AssemblyLet's get started building Serverless WASM with Rust.YewWe can't just write native console-based Rust code and expect to be able to deploy.  Enter Yew.  Thanks toDarkofor showing me the light.Yew is a framework for creating reliable and efficient web applications.YewWith Yew, I can write Rust code that targets WASM which is exactly where I want to be.Setting up the DependenciesBefore I can start building Serverless WASM with Rust, I need to configure a few dependencies.Install RustInstalling Rust is straightforward.The Install Pagewill detect your host and present you with an option that is appropriate for you.Once you've installed Rust, make sure to run:rustc--versionEnter fullscreen modeExit fullscreen modeIf your Rust version is greater than1.64.0then you are good!Add WASM TargetRust can compile source code for different targets or processors.  This is called cross-compilation.Runningrustc--printtarget-listEnter fullscreen modeExit fullscreen modefrom your shell will give you an idea of what you have at your disposal before adding WASM.  If you've read myRust, Lambda, and API Gateway articleyou'd have seen that I'm building for ARM64 there to take advantage of the AWS Graviton chipset.  Building for WASM is sort of similar.  Rustc is a cross-compilation build tool.Add the WASM target like this.rustup target add wasm32-unknown-unknownEnter fullscreen modeExit fullscreen modeA Serverless WASM with Rust BundlerI've now installed Rust, and added the WASM target but how do I connect my build into something that we could host on Cloudfront and S3?  Let's connect Serverless WASM with Rust by adding a bundler.Yew recommends usingTrunkwhich isTrunk is a WASM web application bundler for Rust. Trunk uses a simple, optional-config pattern for building & bundling WASM, JS snippets & other assets (images, css, scss) via a source HTML file. -TrunkInstalling Trunk happens throughCargo.  Remember, Cargo is more than a package manager, it also supports sub-commands.cargoinstall--lockedtrunkEnter fullscreen modeExit fullscreen modeI'll get into this in the next article, but Trunk is what will bundle my Serverless WASM with Rust artifacts so that I can ship them to S3 like in the diagram at the top of this article.Here We Go!Finally, we are into some code and on our Serverless WASM with Rust journey.MainNo surprise here, but we are going to start with amainfunction as all Rust binary projects do.fnmain(){yew::Renderer::<App>::new().render();}Enter fullscreen modeExit fullscreen modeHere I've got a call to the Yew Render which happens to be client-side rendering that is executed by the WASM runtime in the browser.Simple Table OutputThe concept that I want to explore over the next few articles is going to be around Player Data and thePGA Tour.  For article number one, that's going to be just a static vector and not something \"live\".As you can see from that image, it's a simple HTML table right now.  And yes, that's rendered from Yew.  How?Yew is going to look an awful lot like React and JSX.  It supports components, states, and a JSX-style syntax.#[function_component(App)]fnapp()->Html{html!{<div><h1>{\"PGA Tour Players\"}</h1><table><thead><tr><td>{\"ID\"}</td><td>{\"First Name\"}</td><td>{\"Last Name\"}</td><td>{\"Country\"}</td></tr></thead><tbody>{players}</tbody></table></div>}}Enter fullscreen modeExit fullscreen modeThere's the table that gets outputted.  No styles at the moment, just raw HTML.  I'm going to explore a host of things as we dive further in future articles.A couple of things that Yew does for me.That#[function_component(App)]Macro.  For now, let's just call that the thing we need at the top of an HTML component.  We are going to learn more about that together.Another Macrohtml!that produces HTML.  Again, we'll explore more of what that means as we get deeper.Player DataAs I get into future articles in this series, I'm going to be building off of this central theme of a \"Player\".  That's the core model in our domain. The player will have metadata about them but also stats, scores, and rankings.  We'll build a leaderboard and a display page about that player including the ability to have favorites.The Player right now is bare but it's a start.#[derive(Serialize,Debug)]pubstructPlayer{id:u64,first_name:String,last_name:String,country:String}Enter fullscreen modeExit fullscreen modeBring Together for the TableIf you noticed in the table, there's this<tbody>{players}</tbody>Enter fullscreen modeExit fullscreen modeHTML block.  The{players}piece comes from this iterator code that gets run through thehtml!macro.letplayers=player::generate_players().iter().map(|p|html!{<tr><td>{p.get_id()}</td><td>{p.get_first_name()}</td><td>{p.get_last_name()}</td><td>{p.get_country()}</td></tr>}).collect::<Html>();Enter fullscreen modeExit fullscreen modeRunning the ApplicationLet's drop back down to our terminal and run.trunk serve--openEnter fullscreen modeExit fullscreen modeYour browser should pop up and look like this:Next StepsOK, so let's pause there for this article.  We've built a basic Serverless WASM with Rust package.  The Serverless part hasn't quite come to bear yet, but that's where we are heading next.In article 2 I'm going to tackle using CDK to build the Cloudfront and S3 infrastructure to house the WASM bundle.  I'll dive into Trunk and how to accomplish this.For transparency, I've NEVER done this before, so we'll be learning together. Fun right??!Wrapping UpMy initial impression of writing Serverless WASM with Rust is that it feels React-ish in a lot of ways but it's Rust and not TypeScript.  Those Rust development feelings are strong which leaves me excited to dig into the next steps.My gut tells me that this isn't going to be my default for every project I build going forward, but that it gives me more options for things that might be more easily and efficiently accomplished with Rust than JavaScript.  Things like media processing, super snappy user experiences, and possibly can it compete with a native application?  I don't know the answers to those at the moment but I've taken step 1 and am willing to see what's next.And as always, this series will have source code.I'll be building here.Thanks for reading and happy building!"}
{"title": "My AWS DevOps Engineer Professional Study Guide", "published_at": 1714222570, "tags": ["aws", "devops", "certification"], "user": "Sri", "url": "https://dev.to/aws-builders/my-aws-devops-engineer-professional-study-guide-1p6c", "details": "Important: Complete both the courses and study material suggested in theMy AWS SysOps Associate study guideandMy AWS Developer Associate study guidefirst as this has some overlap. After you have completed these, then follow the study guide below.Please review the following in addition to myAWS DevOps Engineer Professional Notes.1. My Study Guide for AWS DevOps Engineer ProfessionalNoCourseTimeStatusComments1Stephane's AWS Certified DevOps Engineer Professional15 daysCompletedPreferable if you have all three associate certifications2Tutorial Dojo Practice Exams7 daysCompletedHighly recommend3Stephane Maarek Practice Exams1 dayCompletedHighly recommend4Neal Davis Practice Exams2 daysCompletedHighly recommend5AWS SkillBuilder - Exam Prep Standard Course: AWS Certified DevOps Engineer - Professional1 weekCompletedYou MUST COMPLETE this course before taking the exam.6AWS SkillBuilder - AWS Exam Preparation Official Practice Question Sets Overview and Instructions.pdf1 hourCompletedYou MUST COMPLETE this course before taking the exam.7Whizlabs - Hands-on Labs7 dayssome labsOptional2. AWS Workshops for DevOps:AWS DevOps Engineer Professional Exam tests your hands on experience, So I highly recommend you gain some hands on experience using the following workshops:Introduction to AWS Code FamilyBuilding event-driven architectures on AWSAWS CloudFormation WorkshopOne Observability Workshop- Focus on AWS CloudWatchFAQs:Which video courses do I need to take?If you haven't completed all three associate certifications, then I recommend completing Stephane's Sys Ops and Developer Associate courses first.How long does it take to prepare?If you have completed Sys Ops and Developer Associate exams then probably a month or two. Otherwise you might need at least 4-6 months.It took me six months in total (part time and a bit on and off due to some other activities).I suggest taking the exam as soon as you can after completing the Associate level certifications as the concepts will be fresh in your mind and you would have built up the momentum to complete the next certification level.What if I do not feel confident to take the exam?I didn't feel confident even after completing the practice exams, so I tookAWS SkillBuilder - Exam Prep Standard Course: AWS Certified DevOps Engineer - Professionalwhich helped me gain confidence. This course has precisely everything that is needed for the Professional exam (re:Invent videos, FAQs, blogs, etc.)How is the Professional exam different to an Associate exam?The Professional exam is very different to the Associate exam as some of the questions are longer and some questions have two correct answers. It is important to pick the most appropriate answer based on the requirement in the question. Most importantly, AWS DevOps Engineer Professional exam tests your hands on experience.What happens if I don't pass the practice exams?Try to score close to 70% in any practice exam in your first attempt. Don't be worried if you don't pass on your first attempt.What do I do if I discover that I still have some weak areas after doing the practice exams?During the practice exams, if you find any areas that you seem to be weak in, then read more about those particular services in the AWS documentation.What tools do I get to use during the exam at the test centre?I was given a pen and erasable sheet.Where should I take the exam - online or at the test centre?You can take the exam anywhere but my preference is taking the test at a test center.Note: I haven't included white-papers, FAQ's and workshops in this guide as I completed them during my preparation for the Associate exam.3. AWS DevOps Engineer Professional Noteskasukur/AWS_CCP_NotesAWS Certification NotesAWS Certification Preparation NotesAWS Certified Cloud Practioner NotesAWS Solution Architect Associate NotesAWS Certified Developer Associate NotesAWS Solution Architect Professional NotesAWS DevOps Engineer Professional NotesAWS Workshops/LabsMore to follow...Disclaimer:The information has been taken from the content inacloudgurufor AWS Solution Architect Associate.I highly recommend you to purchase the course fromacloudguruThe purpose of these notes is to help students with their revision prior to taking the certification.View on GitHub4. Exam TipsDuring the exam, I marked the questions I was unsure of for review. At the end of the exam, I had about 20 questions for review, which made me feel slightly confident that I might pass the exam.Make sure that you don't leave any questions unanswered as there is no negative marking.It is also very important to know the basics, the exam will test you on the basics as well.DO NOT attempt to answer the questions based on key words alone, ensure you read all the answers as well.For example: Whenever we see cost, we tend to think of 'Reserved' but that may not always be the case for some scenarios.You will notice this when you do the live practice exams.If English is your second language, you can request for ESL+30 mins before booking the exam fromcertmetrics> Request Exam Accomodations and wait for the approval. It might take a couple of days for the approval to come through.5. More AWS LabsAWS Labs/WorkshopsThis is a list of labs which you can use for further practice, using your own AWS account. This is optional as some of these labs are quite advanced."}
{"title": "We gave a lightning talk at the AWS Summit in Sydney 2024.", "published_at": 1714203534, "tags": ["aws", "awssummit", "kubernetes", "eks"], "user": "Arshad Zackeriya  \ud83c\uddf3\ud83c\uddff \u2601\ufe0f ", "url": "https://dev.to/aws-builders/we-gave-a-lightning-talk-at-the-aws-summit-in-sydney-2024-o2j", "details": "Have you heard about AWS Summits? I attended the AWS Summit in Sydney for the second time with my wife, following my attendance at AWS Summit Singapore in 2019.This year,@geethikagurugeand I presented a session about \"Streamlined Deployment to Amazon EKS with Amazon CodeCatalyst\" at AWS Summit Sydney. We are honored to represent AWS Community Aotearoa (New Zealand). Thank youAWS UG Aotearoa. I would like to thankBelindafor giving us this opportunity and also thankDerekfor all the support he provided. My wife,Shafwana, provided me with all the support I needed to prepare my talk.How did Geethika and I prepare for the event?We both planned our sessions with slides and demos well. We scheduled them a few days every week after dinner to practice and perfect the presentations. Since it was a lightning talk, we had to keep our speeches concise, with no more than 15 minutes. I want to mention the AWS New Voices program. Before the event, I was chosen to be a coach for the program, which provided me with the chance to enhance my public speaking skills while coaching other members of the community. Additionally, I was able to learn fromMeridith Grundei, and I would like to express my gratitude toMark Pergolafor giving me this opportunity.Day of the AWS Summit Sydney 2024Geethika and myself presenting at AWS Summit SydneyAdding more colours to AWS Summit Sydney with@xelferandKristine HowardIf you are interested in printing the AWS icons pattern, you can check it out on my blog by following the link provided:https://dev.to/aws-builders/level-up-your-aws-community-day-creating-custom-gaming-mats-with-aws-architecture-icons-547nPlaying AWS BuildersCard GameHad an amazing time playing BuildersCard Game while learning about AWS Well-Architected Framework withShehan Marino Pereraat AWS Summit Sydney. Thank youAileen LuDavid Heidt.@zachjonesnoeland I gave away some of our swagsWith@xelferand@ssennettauAfter attending the summit, I attended the ANZ Community Leaders MeetupFirst of all,@shafjag, I would like to thank you for organizing such a great event.Amazing sessions byKris Howard,Mike ChambersScott FriendandMaria EncinarIt's all about communityI am proud to be part of the AWS Community Aotearoa (New Zealand) and grateful for the opportunity."}
{"title": "LLMs in Amazon Bedrock: Observability Maturity Model", "published_at": 1714194431, "tags": ["llm", "bedrock", "observability", "sre"], "user": "Indika_Wimalasuriya", "url": "https://dev.to/aws-builders/llms-in-amazon-bedrock-observability-maturity-model-4gpk", "details": "A few weeks back, I presented at Conf42 LLM2024. My presentation was on \"LLMs in AWS: Observability Maturity from Foundations to AIOps.\" This blog post covers the technical elements I discussed as part of my presentation.Disclaimer:LLM observability is in two parts:Direct LLM observability, which means integration of observability capabilities into LLM itself during training, deployment, and maintenance. This allows us to gain insight into LLM's overall performance, detect anomalies or performance issues, and understand the decision-making process.Indirect LLM observability, which means integration of observability capabilities into the GenAI application instead of LLM itself. Here, the focus is mainly on understanding the internal GenAI app state while indirectly keeping an eye on our LLM.In my Observability Maturity Model for LLM, I mainly focus on Indirect LLM observability. And of course, the title says it's Amazon Bedrock-based, so naturally, you don't have access to underlying foundational models. You simply use APIs to access the model. Even if you like, you don't have any option of doing any direct LLM observability here.Key areas I'm focusing on when it comes to indirect LLM Observability are:Logging inputs and outputs to/from the LLMMonitoring application performance metricsImplementing anomaly detection on LLM outputsEnabling human feedback loops to assess LLM output qualityTracking application-level metrics like error rates and latencyMy main objectives of implementing Observability for LLM-based GenAI apps in this way are:Gain insight into LLM usage within the applicationObserve the application's behavior and performance when interacting with the LLMEnsure reliability of the application built around the LLMLet's cut to the chase, presenting my Observability Maturity model for GenAI apps developed using Amazon Bedrock.Now, let's cover a few details of each level I have come up with here:Level 1: Foundation LLM ObservabilityAt the foundation level, I'm focusing on providing the basics and keeping the lights on. At this level, establish logging, monitoring, and visualization systems for prompt properties, LLM events, and outputs, while implementing distributed tracing and basic security measures to ensure effective system management and compliance.Prompt Properties:Log and monitor basic prompt properties such as prompt content and prompt source to track variations and effectiveness.Logs:Implement basic logging for LLM-related events and outputs to maintain a record of system activities and troubleshoot issues effectively.Distributed Tracing:Implement distributed tracing to understand the flow of requests through your LLM-powered application, aiding in performance optimization and debugging.Visualizations:Set up basic visualizations and dashboards for LLM metrics and logs to gain insights into model behavior and performance trends.Alert and Incident Management:Establish basic alert and incident management processes for LLM-related issues to ensure timely detection and resolution of anomalies or failures.Security Compliance:Implement basic monitoring for LLM usage and cost controllers to ensure compliance with security standards and prevent unauthorized access or misuse.Cost Optimization:Implement basic cost optimization strategies for LLM inference to manage expenses efficiently and maximize the value derived from model deployment.Level 2: Proactive LLM ObservabilityAt this level, I'm focusing on elevating the foundation to an advanced level, delving deep into metrics and bringing in advanced insights. This proactive stage allows for the identification of issues proactively and prompt resolution. At the advanced level, deploy comprehensive capabilities including advanced LLM metric analysis, optimization of prompt properties, enhanced alert management, robust security compliance, advanced cost optimization, utilization of AWS forecasting services, and anomaly detection mechanisms for both metrics and logs, enabling proactive insights and resolution in LLM operations.LLM Metrics:Capture and analyze advanced LLM metrics, including model performance and output quality, for deeper insights.Prompt Properties:Log and monitor advanced prompt properties such as prompt performance and prompt versioning to optimize model inputs.Alert and Incident Management:Enhance alert and incident management processes specifically tailored for LLM-related issues to ensure proactive response and resolution.Security Compliance:Ensure LLM security compliance and implement robust monitoring mechanisms to safeguard against potential threats.Cost Optimization:Implement advanced strategies for cost optimization related to LLM inference, maximizing efficiency and resource utilization.AWS Forecasting:Utilize AWS forecasting services to predict future LLM usage and associated costs, enabling better resource planning.Metric Anomaly Detection: Set up metric anomaly detection mechanisms to identify unusual patterns or deviations in LLM metrics for early anomaly detection.Log Anomaly Detection:Implement log anomaly detection techniques to detect abnormal patterns in LLM logs, facilitating proactive troubleshooting and problem resolution.Level 3: Advanced LLM Observability with AIOpsFinally, it's about Advanced LLM Observability with AIOps. Here, I'm focusing on bringing in AI capabilities such as anomaly detection, forecasting, noise reduction, and many more. It's about applying AI/ML to your pillars and going to the next level of having the ability to truly eliminate issues even before they materialize. At the final level of LLM observability, advanced features include capturing and analyzing sophisticated LLM metrics, optimizing prompt properties, enhancing alert management, ensuring security compliance, implementing cost optimization strategies, leveraging AWS forecasting, and setting up anomaly detection mechanisms for proactive issue resolution.LLM Metrics:Capture and analyze advanced LLM metrics, including model performance and output quality, for deeper insights.Prompt Properties:Log and monitor advanced prompt properties, such as prompt performance and prompt versioning, to optimize model inputs.Alert and Incident Management:Enhance alert and incident management processes specifically tailored for LLM-related issues to ensure proactive response and resolution.Security Compliance:Ensure LLM security compliance and implement robust monitoring mechanisms to safeguard against potential threats.Cost Optimization:Implement advanced cost optimization strategies for LLM inference, maximizing efficiency and resource utilization.AWS Forecasting:Leverage AWS forecasting services to predict future LLM usage and associated costs, enabling better resource planning.Metric Anomaly Detection:Set up metric anomaly detection mechanisms to identify unusual patterns or deviations in LLM metrics for early anomaly detection.Log Anomaly Detection:Implement log anomaly detection techniques to detect abnormal patterns in LLM logs, facilitating proactive troubleshooting and problem resolution. A few things I want to call out here are:LLM Specific MetricsI have given focused attention to identifying the key LLM-specific metrics. It's important to cover these as part of your implementation. This will be a checklist you want to follow.Telemetry DataDescriptionLLM MetricsCapture and analyze advanced LLM metrics (e.g., model performance, output quality)Prompt PropertiesLog and monitor advanced prompt properties (e.g., prompt performance, prompt versioning)Alert and Incident ManagementEnhance alert and incident management processes for LLM-related issuesSecurity ComplianceEnsure LLM security compliance and monitoringCost OptimizationImplement advanced cost optimization strategies for LLM inferenceAWS ForecastingLeverage AWS forecasting services to predict future LLM usage and costsMetric Anomaly DetectionSet up metric anomaly detection for LLM metricsLog Anomaly DetectionImplement log anomaly detection to identify unusual patterns in LLM logsLLM Model Drift- Monitor the distribution of the LLM's output within the Bedrock application over time.- Identify significant changes in the output distribution that may indicate model drift.- Track the performance of the LLM on a held-out evaluation set or benchmark tasks specific to the Bedrock application.LLM Cost Optimization- Monitor the cost associated with LLM inference requests within the Bedrock application.- Track the cost per inference and the total cost over time.- Identify opportunities for cost optimization, such as caching or batching inference requests.LLM Integration Errors- Monitor and log any errors or exceptions that occur during the integration of the LLM with the Bedrock application.- Track the frequency and severity of integration errors.- Identify and troubleshoot issues related to the integration code or the communication between the Bedrock application and the LLM service.LLM Ethical Considerations- Monitor the LLM's output within the Bedrock application for potential ethical risks or violations.- Track instances of harmful, illegal, or discriminatory content generated by the LLM.- Ensure that the LLM's output aligns with established ethical principles and guidelines for responsible AI development and deployment.Prompt Engineering PropertiesThen, it's all about continuous improvement related to your prompt. Therefore, you have to keep track of the following important aspects:Temperature:Controls randomness in model output. Higher temperatures yield more diverse responses; lower temperatures, more focused.Top-p Sampling:Controls output diversity by considering only the most probable tokens.Top-k Sampling:Considers only the k most probable tokens for generating the next token.Max Token Length: Sets the maximum length of generated text.Stop Tokens:Signals the model to stop generating text when encountered.Repetition Penalty:Penalizes the model for repeating text, encouraging diversity.Presence Penalty:Penalizes the model for generating already generated tokens.Batch Size:Determines the number of input sequences processed simultaneously.Inference Latency:Time taken for the model to generate output given input.Model Accuracy & Metrics:Task-specific metrics like accuracy, perplexity, or BLEU score.Performance Metrics, Logging, and Tracing for RAG models (Retrieval augmented generation)Finally, it's about what you do with RAG. Monitor the performance and behavior of the RAG model through metrics such as query latency and success rate, logs including query and error logs, and tracers for end-to-end tracing to ensure optimal functionality and effectively troubleshoot issues.AspectDescriptionMetrics- Query latency: Track the time it takes for the RAG model to process a query and generate a response. This can help identify performance bottlenecks.- Success rate: Monitor the percentage of successful queries versus failed queries. This can indicate issues with the model or the underlying infrastructure.- Resource utilization: Monitor the CPU, memory, and network usage of the RAG model and the associated services. This can help with capacity planning and identifying resource constraints.- Cache hit rate: If you're using a cache for retrieved documents, monitor the cache hit rate to understand its effectiveness.Logs- Query logs: Log the input queries, retrieved documents, and generated responses. This can aid in debugging and understanding the model's behavior.- Error logs: Log any errors or exceptions that occur during query processing or document retrieval. This can help identify and troubleshoot issues.- Audit logs: Log user interactions, authentication events, and any sensitive operations for security and compliance purposes.Tracers- End-to-end tracing: Implement distributed tracing to track the flow of a query through the various components of the RAG model, including document retrieval, encoding, and generation.There are many more I have discussed during my presentation which you can go through. I hope this provides you with enough detail to make a start bringing observability into your LLMs.You can watch the full presentation here."}
{"title": "Using Okta to Control Access to Your Docker Containers", "published_at": 1714151550, "tags": ["docker", "okta", "sso", "webdev"], "user": "Andree Toonk", "url": "https://dev.to/aws-builders/using-okta-to-control-access-to-your-docker-containers-410i", "details": "This article was originally published onour blog at border0.comImagine you have a Docker host with various containers to which you need access. Depending on your scenario, that can be hard \u2014 very hard. Maybe your host is behind NAT, a firewall, in a private network, or even in a different region altogether. You need to access those containers, but security and logistics get in the way. What if you could access them securely, from anywhere, without the hassle?\u200dIn this blog, I\u2019ll introduce a new type of Border0 service that makes it all possible. This new service is specifically designed for Docker hosts, making it easy to access your containers from anywhere without the hassle of VPNs or compromising on security.I\u2019ll also demonstrate (video)how to add Okta to our setup, meaning we can access our containers securely using just our Okta credentials.\u200dThe Pain of Container AccessYou\u2019re not alone if you\u2019re struggling with accessing your containers remotely. Many teams face the same issue: you need to access containers on a Docker host, but VPNs are a hassle and provide too much network access. You wish you could simply use your browser, on any device (computer or phone), to access them securely. But, you can\u2019t compromise on security \u2014 you need to control who has access without exposing your containers directly to the internet. It\u2019s a challenging balance between security and convenience, and you\u2019re stuck in the middle.\u200dIntroducing The Docker Service type in Border0To address these challenges and help you with simple access to your containers from anywhere, we\u2019ve added a new service (Socket) type specifically for Docker hosts. This new Border0 Socket is an SSH service, meaning it allows users to connect using SSH. With a new upstream type, called \u2018Docker Exec\u2019.\u200d\u200dWith this new service type, you can now expose containers to authenticated users, who will see a list of all containers they are authorized to access. Administrators have the flexibility to filter out which containers are visible to users, thus enhancing control over who has access to what containers.\u200dNow, accessing your containers is as easy as logging into a web application, using your favorite SSH client, or using the Border0 web client. Anytime, anywhere and from any device!\u200dSeamless Integration with your SSO providersUsing Border0, you can easily add SSO authentication to services that don\u2019t typically support it \u2014 like SSH, Kubernetes, Docker, and databases like MySQL, Postgres, and Microsoft SQL. You\u2019ll get seamless, out-of-the-box SSO integration with leading providers like Google, GitHub, Microsoft, and even passwordless magic email links \u2014 all at no extra cost.\u200dFor our premium users, we offer the flexibility to \u201cbring your own identity provider\u201d. This means you can connect your Okta Workforce, Google Workspace, or any SAML or OIDC provider to your Border0 account. This integration allows team members to access servers, containers, and databases using their existing enterprise credentials, ensuring a secure and seamless experience.\u200d\u200dIt gets better; you can synchronize your directory service \u2014 such as Okta, Google Workspace, or Microsoft Entra \u2014 with Border0, automatically importing users and groups from your enterprise directory. This allows you to create fine-grained access policies, ensuring that only authorized users \u2014 like those in your Okta group \u201cSRE users\u201d \u2014 can access specific resources, such as your Docker containers.\u200dDemo time \u2014 See for Yourself How Easy It IsTake a moment to watch the demo video below, which demonstrates the easy process of setting up Okta SSO and SCIM integration with Border0 and shows you how to use it to access your Docker containers.This brief demo shows how Okta and Border0 work together to provide secure and user-friendly access to your Docker containers, enhancing security without adding complexity to your workflow.\u200dConclusion\u200dWith the introduction of the Docker service type in Border0, accessing your Docker containers remotely has now become a lot easier. This new service type allows users to connect securely to Docker containers from anywhere, using any device, and without the complexities of VPNs, enhancing both convenience and security. You can now:\u200dAccess your Docker containers from any location using just a web browser or CLI.Use Single Sign-On (SSO) with familiar credentials from identity providers like Okta, GitHub, or Google.Enjoy automatic directory synchronization and SCIM integration.Maintain control over who can access your resources with highly customizable fine-grained access policies.Benefit from session recording for enhanced oversight.\u200dStart Your Free Trial Today!Ready to simplify access to your infrastructure and Docker containers?Start your free Border0 trialtoday and streamline your Docker container access!\u200d"}
{"title": "Navigating through failures, build resilient serverless systems", "published_at": 1714142292, "tags": ["aws", "serverless", "resiliency"], "user": "Jimmy Dahlqvist", "url": "https://dev.to/aws-builders/navigating-through-failures-build-resilient-serverless-systems-4jp7", "details": "Navigating failures! Building resilient serverless workloads!I have been building and architecting serverless systems for almost a decade now, for a variety of companies, from small start-ups to large enterprises. And I have seen many strange things over the years.Serverless services from AWS come with high availability and resiliency built in. But that is on the service level, AWS Lambda, StepFunctions, EventBridge, and so on are all highly available and have high resiliency.If all components in our systems were serverless that would be great. However, that is not the case. In almost all systems that I have designed or worked on there has been components that are not serverless. It can be the need for a relational database, and yes, I do argue that not all data-models and search requirements can't be designed for DynamoDB. It can be that we need to integrate with a 3rd party and their API and connections come with quotas and throttling. It could be that we as developers are unaware of certain traits of AWS services that make our serverless services break. And even sometimes our users are even our worst enemy.When we are building our serverless systems we must always remember that there are components involved that don't scale as fast or to that degree that serverless components does.In this post I'm going to give some thoughts on navigating failures and building a high resiliency serverless system. This post is not about preventing failures, instead it's about handling and recover from failures in a good way. I'll start with a common serverless architecture and add architecture concepts, that I often use, that can help enhance its resiliency.Many of the concepts in this post can also be found onMy serverless Handbookall with different implementation examples and fully working and deployable infrastructure using CloudFormation and SAM.What is serverlessMy definition of serverless is probably the same as for many of you and AWS as well. Serverless services come with automatic and flexible scaling, scale down to zero and up to infinity (almost). Serverless services has little to none capacity planning, I should not have to do much planning how much resources I need before hand. Serverless services has a pay-for-use, if I don't use it I don't pay for it.But. this is not black or white, there is a grey zone I think. Where services like Fargate and Kinesis Data Stream ends up.Service categoriesLooking at services from AWS, in the red corner we have serverless services API gateway, Lambda, SQS, StepFunctions, DynamoDB and more. Services that we can throw a ton of work on and they will just happily scale up and down, to handle our traffic.In the blue corner we have the managed services. This is services like Amazon Aurora, ElasticCache, OpenSearch. This is services that can scale basically to infinity, but they do require some capacity planning for that to happen, and if we plan incorrectly we can be throttled or even have failing requests. I will also put the grey zone services, like Fargate and Kinesis Data Streams in this blue corner.Then there is the server corner, that would be anything with EC2 instances. We don't speak about them.....What is resiliencySo, what is resiliency?Sometimes it gets confusing, and people mix up resiliency with reliability. As mentioned in the beginning, resiliency is not about preventing failures, it's about recovering from them. It\u2019s about making sure our system maintain an acceptable level of service even if when other parts of our system is not healthy. It's about gracefully deal with failures.Reliability focuses on the prevention of the failure happening in the first place, while resiliency is about recovering from it.Everything fails all the timeThis is by far one of my favorite quotes by Dr. Werner Vogels. Because this is real life! Running large distributed systems, then everything will eventually fail. We can have down stream services that is not responding as we expect, they can be having health problems. Or we can be throttled by a 3rd party or even our own services.CracksIt's important that the cracks that form when components in our system fail, doesn't spread. That they don't take down our entire system. We need ways to handle and contain the cracks. That way we can isolate and protect our entire system.When our serverless systems integrate with non-serverless components. Which in some cases it can be obvious, like when our system interacts with an Amazon Aurora database. Other times it's not that clear, the system integrates with a 3rd party API that can lead to throttling that can affect our system and start forming cracks if not handled properly.How does our system handle a integration point that is not responding? Specially under a period of high load. This can easily start creating cracks that can bring our entire system to a halt or that we start loosing data.Everything has a limitWhen we build serverless systems we must remember that every API in AWS has a limit. We can store application properties in System Manager Parameter Store, a few of them might be sensitive and encrypted with KMS. What now happens is that we can get throttled by a different service without realizing it. SSM might have a higher limit but getting an encrypted value would then be impacted by the KMS limit. If we then don't design our functions correctly, and call SSM in the Lambda handler on every invocation we would quickly get throttles. Instead we could load properties in the initialization phase.Understanding how AWS services work under the hood, to some extent, is extremely important, so our systems doesn't fail due to some unknown kink. For example, consuming a Kinesis Data Stream with a Lambda function, if processing an item in a batch fails, the entire batch will fail. The batch would then be sent to the Lambda function over and over again.What we can do in this case is to bisect batches on Lambda function failures. The processed batch will be split in half and sent to the function. Bisect would continue to we only have the single failing item left.Resiliency testingNow, I bet most of you run a multi-environment system, you have your dev, test, pre-prod, and prod environments. Many would probably say that your QA, Staging, Pre-prod, or what ever you call it, has an identical setup with your prod environment. But now, let's make sure we consider data and integrations as well. The amount of data, the difference in user generated data, the complexity in data, difference in integrations with 3rd party. I have seen system been taken down on multiple occasions due to differences in data and integration points. Everything works in staging but then fails in production.One large system I worked on we had a new advanced feature that had been tested and prepared in all environments. But, when deploying to production, the database went haywire on us. We used Amazon Aurora serverless and the database suddenly scaled out to max and then could handle the load anymore. Our entire service was brought down. This was caused by a SQL query that due to the amount of data in production consumed all database resources, in a nasty join.In a different system, I had a scenario where in production a 3rd party integration had an IP-Allow list in place, so when we extended our system and got some new IPs suddenly only one third of our calls was allowed and a success. In the staging environment, the 3rd party didn't have any IP-blocks. Intermittent failures are always the most fun to debug.A good way to practice and prepare for failures are through Resiliency testing, chaos engineering. AWS offers their service around this topic, AWS Fault Injection Service, which you can use to simulate failures and see how your components and system handles them. What I'm saying is that when you plan for your Resiliency testing, start in your QA or staging environment. But, don't forget about production and do plan to run test there as well.Classic web applicationNow let's start off with a classic web application, single page application with an API. SPA hosted from S3 and CloudFront, API in API Gateway and compute in Lambda, finally a database in DynamoDB. That is one scalable application!But, maybe we can't model our data in DynamoDB? Not everything will fit in a key/value model, we need an relational database like Aurora, or we need to integrate with 3rd party. This could be an integration that still run on-prem, it could be running in a different cloud on servers. With any form of compute solution that doesn\u2019t scale as fast and flexible as our serverless solution. This application is setup as a classic synchronous request-response where our client expect a response back immediate to the request.Most developers are very familiar with the synchronous request-response pattern. We send an request and get a response back. We wait for this entire process to happen, with more complex integrations with chained calls and even 3rd party integrations the time quickly adds up, and if one of the components is down and not responding we need to fail the entire operation, and we leave any form of retries to the calling application.Do we need an immediate response?One question we need to ask when building our APIs is does our write operations really need an immediate response? Can we make this an asynchronous process? Now, building with a synchronous request-response is less complex than an asynchronous system. However, in a distributed system, do the calling application need to know that we have stored the data already? Or can we just hand over the event and send a response back saying that \"Hey I got the message and I will do something with it\". In an event-driven system asynchronous requests are very common.Buffer eventsWith an asynchronous system we can add an buffer between our calls and storage of our data. What this will do is protect us and the downstream services. The downstream service will not be overwhelmed and by that we protect our own system as well from failures. This can however create an eventual consistency model, where read after write not always gives us the same data back.Storage-firstBuffers leads us to our first good practice when building a high resiliency system, storage-first. The idea behind the storage-first pattern is to not do immediate processing of the data. Instead we directly integrate with a service from AWS, it could be SQS, EventBridge or any other service with a durable storage mechanism. I use the storage-first pattern in basically all systems I design and build.So, let's get rid of the Lambda integration and instead integrate directly to the SQS. It doesn't have to be SQS it can be any service. The major benefits with the storage first pattern is that the chance of us loosing any data is very slim. We store it in a durable service and the process it as we see fit. Even if processing would fail we can still handle and process it later.For more implementation examples check outMy serverless Handbook and Storage-First PatternQueue Load levelingWhen using storage-first pattern with SQS we have the possibility to use one more pattern, that I frequently use, the queue load leveling pattern, here we can protect the downstream service, and by doing that our self, by only processing events in a pace that we know the service can handle. Other benefits that come with this pattern, that might not be that obvious, is that it can help us control. We could run on subscriptions with lower throughput that is lower in cost, when integrating with a 3rd party. We could also down-scale our database, as we don't need to run a huge instance to deal with peaks. Same goes if we don't process the queue with Lambda functions but instead use containers in Fargate, we can set the scaling to fewer instances or even do a better auto-scaling solution.One consideration with this pattern is that if our producers are always creating more requests than we can process, we can end up in a situation where we are always trailing behind. For that scenario we either need to scale up the consumers, which might lead to unwanted downstream consequences or we need at some point evict and throw away messages. What we choose, and how we do it, of course come with the standard architect answer \"It depends....\"Fan outA good practice in a distributed system is to make sure services can scale and fail independently. That mean that we can have more than one service that it is interested in the request. For a SQS queue we can only have one consumer, two consumers can't get the same message. In this case we need to create a fan out or multicast system. We can replace our queue with EventBridge that can route the request or the message to many different services. It can be SQS queues, StepFunctions, Lambda functions, other EventBridge buses. EventBridge is highly scalable with high availability and resiliency with a built in retry mechanism for 24 hours. With the archive feature we can also replay messages in case they failed. And if there is a problem delivering message to a target we can set a DLQ to handle that scenario.This is however one of the kinks that we need to make sure we are aware of. Remember that the DLQ only come into affect if there is a problem calling the target, lacking IAM permissions or similar. If the target it self has a problem and fails processing, message will not end up in the DLQ.Retry with backoffEven with a storage-first approach we are of course not protected against failures. They will happen, remember \"Everything fails all the time\".In the scenarios where our processing do fail we need to retry again. But, retries are selfish and what we don't want to do, in case it's a downstream services that fail, or if we are throttled by the database, is to just retry again. Instead we like to backoff and give the service som breathing room. We would also like to apply exponential backoff, so if our second call also fails we like to back off a bit more. So first retry we do after 1 second, then 2, then 4, and so on till we either timeout and give up of have a success.Retry with backoff and jitterThere is a study conducted by AWS, a couple of years ago, that show that in a highly distributed system retries will eventually align. If all retries happen with the same backoff, 1 second, 2 seconds, 4 seconds and so on they will eventually line up and happen at the same time. This can then lead to the downstream service crashing directly after becoming healthy just due to the amount of job that has stacked up and now happen at the same time. It's like in an electric grid, after a power failure, all appliances turn on at the same time creating such a load on the grid that it go out again, or we blow a fuse. We change the fuse, everything turn on at the same time, and the fuse blow again.Therefor we should also use some form of jitter in our backoff algorithm. This could be that we add a random wait time to the backoff time. It would work that we first wait 1 second + a random number of hundreds of milliseconds. Second time we wait 2 second + 2x a random number, and so on. By doing that, our services will not line up the retries. How we add the jitter and how much, that well depends on your system and implementation.StepFunctions has it built inA good way to handle retries is to use StepFunctions as a wrapper around our Lambda functions. StepFunctions has a built in retry mechanism which we can utilize in our solutions.Dead Letter QueuesIn some cases we just have to give up the processing. We have hit the max number of retries, we can't continue forever, this is where the DLQ come in. We route the messages to a DLQ where we can use a different retry logic or even inspect the messages manually. The DLQ also create a good indicator that something might be wrong, and we can create alarms and alerts based on number of messages in it. One message might not be an problem but if the number of messages start stacking up it's a clear indicator that something is wrong.In case we are using SQS as our message buffer we can directly connect a DLQ to it. If we use StepFunctions as our processor we can send messages to a SQS queue if we reach our retry limit.For a implementation examples check outMy serverless Handbook and retriesCircuit breakerFor the final part in our system we look at the circuit breaker. Retries are all good, but there is no point for us to send requests to an integration that we know is not healthy, it will just keep failing over and over again. This is where this pattern comes in.If you are not familiar with Circuit breakers it is a classic pattern, and what it does is make sure we don't send requests to API, services, or integration that is not healthy and doesn't respond. This way we can both protect the downstream service but also our self from doing work we know will fail. Because everything fails all the time, right.Before we call the service we'll introduce a status check, if the service is all healthy we'll send the request this is a closed state of the circuit breaker. Think of it as an electric circuit, when the circuit is closed electricity can flow and the lights are on. As we do make calls to the service we'll update the status, if we start to get error responses on our requests we'll open the circuit and stop sending requests. In this state is where storage-first shine, we can keep our messages in the storage queue until the integration is back healthy again. What we need to consider is when to open the circuit, if we have 1000 requests per minute we don't want to open the circuit just because 10 fails. Also, checking the status before every invocation might also be a bit inefficient, so here we need to find a good balance when to check the status and when to open the circuit.But! We just can't stop sending requests for ever. What we do is to periodically place the circuit in a half-open state to send a few requests to it and update our status with the health from these requests.In case of an SQS queue we can't just stop calling the Lambda integration, instead we need to have some logic to add and remove the integration.Putting it all togetherIf now put all of the different patterns together we'll have serverless system that should be able to handle all kinds of strange problems.Final WordsIn this post we have looked at how I normally design and build systems to make them handle and withstand failures. There is of course more to a ultra robust system and we always need to consider what happens if something fails. The different patterns and implementations in this post should however halp you a bit on your journey to resiliency.Check outMy serverless Handbookfor some of the concepts mentioned in this post.Don't forget to follow me onLinkedInandXfor more content, and read rest of myBlogsAs Werner says! Now Go Build!"}
{"title": "AWS IAM Identity Center Permission Management at Scale Part 3", "published_at": 1714140248, "tags": ["security", "aws", "iam"], "user": "David Krohn", "url": "https://dev.to/aws-builders/aws-iam-identity-center-permission-management-at-scale-part-3-21hn", "details": "Identity management is the easiest when you can manage identities in one place and use them across accounts and applications. AWS IAM Identity Center streamlines identity management by enabling you to connect to your identity provider (IdP), such as Active Directory, and use the IdP's identity information for access and collaboration within applications. You can do this, for example, by using an AD Connector and connecting it to your on-premises or Azure AD. An AD Connector is a directory gateway that allows you to route directory requests to your on-premises Microsoft Active Directory without storing information in the cloud cache.If you have a large Active Directory with several thousand groups and users, you may not want to explicitly select the identities you want to synchronise with AWS Identity Center.In this blog post, we will show you a solution that allows you to specify different prefixes for Active Directory groups that will be automatically synced to your AWS Identity Center.The solution workflow includes the following steps:Synchronization of Active Directory GroupsAutomated Documentation & NotificationPrerequisite:Identity Center must be enabled in your OrganizationConnection to local or Azure Active DirectoryActive Directory User who has read accessSolution overviewThe following architecture shows the solution of the automated sync of Active Directory Groups to AWS Identity Center .\ud83d\udd17 Here you can find the Solution on GithubIn our CDK based project several resources will be provisioned due deployment.The Stack contains:Amazon EventBridge rule that runs on a scheduleCDK Sops Secrets ConstructSecret which holds the Active Directory CredentialsLambda Function to create the Synchronisation Filter in AWS IAM Identity Centerall necessary IAM Roles and Permissions for the Lambda FunctionSynchronization of Active Directory GroupsWith Active Directory group synchronisation, you use IAM Identity Center to assign users and groups from Active Directory access to AWS accounts and to AWS-managed or customer-managed applications. All Active Directory Groups with specified prefixes will be automatically synchronised to your AWS IAM Identity Center.How the automation with the lambda function worksThe Sync LambdaFunction is triggered by Eventbridge Scheduled event to ensure that the Groups you want sync from your active directory are always uptodate in your AWS Identity Center.The workflow of our Sync LambdaFunction is the following:Retrieve Active Directory credentials from AWS Secrets ManagerLog on to Active Directory using LDAP(s) using the credentials just retrievedSearch and retrieve Active Directory group names based on specified prefixesRetrieve current Filters of Groups which are sync scope from AWS Identity CenterAdd missing Active Directory groups as synchronisation filters in AWS Identity Center\u2139 Synchronisation to AWS Identity Center will start after a short amount of time.Generating of automated documentationSending notification to MS TeamsDeploymentThe deployment of the solution is done via a cdk stack which is part of the solution's repository. All the required Parameters will be configured in a typescript file - an example file is also included in the repository.What you have to configure:PropertyDescriptionProduktwill be used to generate StackNameStagewill be used to generate StackNamePrefixwill be used to generate StackNames3_DOKUBUCKETName of the S3 Bucket for the generated documentationWebhookUrlTeamsURL of your MS Teams WebhookDocuWebsiteLink to the Website where the generated HTML snippet has been published used for a button in the Team NofiticationLambdaScheduleDuration how often your Lambda should be trigger from Eventbridge. Rates may be defined with any unit of time, but when converted into minutes, the duration must be a positive whole number of minutes.rootCaCertificateString*the Root CA Certificate in PEM format which issues the server certificates Active DirectoryvpcIdId of your VPC where the lambda will be deployed tosubnetIdsIds of subnets where the lambda will be deployed toad_DomainNameName of your Active DirectoryBasePathPath in your Active Directory where the Lambda can find the Users you want to SyncGroupPrefixesArray of prefixes of groups you want to synchronise into your AWS Identity CenterSecretFilePath to your Sops secrets file, which should contain the Active Directory user credentials.UrlLDAP Url to your Active Directory. The LDAP URL format is ldap:// hostnamePortPort of your LDAP - LDAP default is 389, LDAPs = 636IdentityStoreIdIdentity Store Id of your AWS Identity CenterEndpointAWS hidden API Endpoint- should be identity-sync..amazonaws.comAutomated Documentation & NotificationAfter each execution of the Synchronisation Lambda, we aim to notify our team about the actions taken during the previous run. Therefore, we have implemented a Teams notification that includes a status update and a link to an automatically generated dashboard.\u2139\ufe0f Notifications are only sent when groups are added or deleted, or when an automation error occurs, to avoid a flood of notifications.The following screenshot illustrates an example of a Teams notification.DocumentationOur Active Directory Synchronization Status Dashboard is a simple HTML file which will be generated trough a Lambda Function, saved in S3 and will be distributed trough a CloudFront. You can integrate this Dashboards in your Confluence or any other internal Wiki. This Dashboard is secured via CloudFormation Function - additionally you can also add aFirewallto restrict the access to an specific CIDR or Geographic region and prevent access from third parties. The screenshot below provides an example of a dashboard.ConclusionIn this blog post, we showed you how to improve your security posture by automatically and regularly synchronising Active Directory groups that match a specific pattern with AWS Identity Center. This simplifies access management and increases security by automatically revoking access in AWS Identity Center when group objects are deleted or created in Active Directory. Furthermore, the automatically generated documentation facilitates an overview of the synchronised group objects.\ud83d\udd17 Here you can find the Solution on Github"}
{"title": "AWS Code Commit & Demos", "published_at": 1714130940, "tags": ["aws", "codecommit", "private", "repository"], "user": "Srinivasulu Paranduru", "url": "https://dev.to/aws-builders/aws-codecommit-3d0a", "details": "AWS CodeCommit: Securely host highly scalable private Git repositories and collaborate on code5 Active Users with AWS Free TierUse Cases :Collaborate on codeUse your existing toolsReceive notifications and custom scripts - Use Amazon Simple Notification Service (SNS) notifications for events impacting your repositoriesCreate up to 5000 repositories by default - Can generate upto 25000 additional repositories by requestReference :Code commituser guideI.Step by Step approach for creating code commit repositoryStep1: Search for codecommit serviceStep2 : Click on Create RepositoryStep3: Entered the mandatory fields as required for creating repository and then click on createStep4: Click on Add File and add relevant files to the repository then click on commitStep5 : Can able to see the files added to the repositoryStep6: If we wanted to have notifications for the repository you have created then click oncreate notification ruleStep7: Click on Create targetAfter enabling SNS notification then next screen looks likeStep8(optional) : If you wanted to change the repo name, click on the repository name and click on settings in the left side menus as per picture below and click on saveII.Step by Step approach for accessing code commit repositoryStep1: Select the IAM user then click on security credentials tabStep2 : Drag down in security credentials tab and then click on Generate CredentialsStep3: Copy and download the credentials, if we dont download we cant able to see the credentials laterStep 4: create a text file - helloworld.txt and commit using git commandsStep5: Check the changes in AWS Console -> codecommit-> select the repository where changes has been pushed.Conclusion : Discussed about AWS Codecommit and step by step approach of using the service\ud83d\udcac If you enjoyed reading this blog post and found it informative, please take a moment to share your thoughts by leaving a review and liking it \ud83d\ude00 and share this blog with ur friends and follow me inlinkedin"}
{"title": "OTEL Tracing demo with ADOT, AWS X-Ray and Grafana", "published_at": 1714125677, "tags": ["aws", "grafana", "kubernetes", "otel"], "user": "Shakir", "url": "https://dev.to/aws-builders/tracing-demo-with-aws-x-ray-and-grafana-1pb5", "details": "IntroductionHello \ud83d\udc4b, In this post we'll see about sending traces from a demo app to AWS X-Ray via the ADOT(AWS Distro for OpenTelemetry) collector. We would then visualize this on Grafana. Note that we'd deploy the workloads on a kubernetes cluster.Here is a picture of what we are trying to accomplish:Alright, let's get started!!!NamespaceWe shall deploy the workloads on a separate namespace. Let's create one.kubectl create ns adot-traces-demoEnter fullscreen modeExit fullscreen modeCredentialsStore the AWS credentials as a kubernetes secret.kubectl create secret generic aws-credentials \\     --from-literal=AWS_ACCESS_KEY_ID=<access-key-id> \\     --from-literal=AWS_SECRET_ACCESS_KEY=<aws-secret-access-key> \\     -n adot-traces-demoEnter fullscreen modeExit fullscreen modeADOT ConfigSet the ADOT config in a file.$ cat adot-config.yaml exporters:   awsxray:     region: ap-south-2 receivers:   otlp:     protocols:       grpc:         endpoint: 0.0.0.0:4317 service:   pipelines:     traces:       exporters:         - awsxray       receivers:         - otlpEnter fullscreen modeExit fullscreen modeAnd create a config map with this file.kubectl create configmap adot-config --from-file=adot-config.yaml -n adot-traces-demoEnter fullscreen modeExit fullscreen modeADOT DeploymentSetup deployment spec in a file, that injects the secret we created earlier as environment variables and the configmap as a volume.$ cat adot-deploy.yaml  apiVersion: apps/v1 kind: Deployment metadata:   labels:     app: adot-collector   name: adot-collector spec:   replicas: 1   selector:     matchLabels:       app: adot-collector   template:     metadata:       labels:         app: adot-collector     spec:       containers:         - args:             - '--config=/etc/adot-config.yaml'           envFrom:             - secretRef:                 name: aws-credentials           image: public.ecr.aws/aws-observability/aws-otel-collector:latest           name: adot-collector           volumeMounts:             - mountPath: /etc/adot-config.yaml               name: config-volume               subPath: adot-config.yaml       volumes:         - configMap:             name: adot-config           name: config-volumeEnter fullscreen modeExit fullscreen modeCreate the deployment.kubectl create -f adot-deploy.yaml -n adot-traces-demoEnter fullscreen modeExit fullscreen modeThe pod in the deployment should be running.$ kubectl get po -n adot-traces-demo NAME                              READY   STATUS    RESTARTS   AGE adot-collector-7cbf849b89-b4bkl   1/1     Running   0          3m26sEnter fullscreen modeExit fullscreen modeADOT ServiceWe can expose the ADOT deployment with a service spec that exposes the grpc port 4317 as follows.$ cat adot-svc.yaml  apiVersion: v1 kind: Service metadata:   name: adot-collector-service spec:   selector:     app: adot-collector   ports:     - protocol: TCP       port: 4317       targetPort: 4317Enter fullscreen modeExit fullscreen modeWe can now create the service.kubectl create -f adot-svc.yaml -n adot-traces-demoEnter fullscreen modeExit fullscreen modeThe endpoint IP should match with the pod IP.$ kubectl get ep -n adot-traces-demo NAME                     ENDPOINTS         AGE adot-collector-service   10.1.3.187:4317   22s  $ kubectl get po -n adot-traces-demo -o wide NAME                              READY   STATUS    RESTARTS   AGE     IP           NODE             NOMINATED NODE   READINESS GATES adot-collector-7cbf849b89-b4bkl   1/1     Running   0          7m11s   10.1.3.187   docker-desktop   <none>           <none>Enter fullscreen modeExit fullscreen modeDemo appWe can now deploy the sample demo app which can send traces to ADOT collector, with the following manifest.$ cat k6-tracing-deploy.yaml  apiVersion: apps/v1 kind: Deployment metadata:   name: xk6-tracing spec:   replicas: 1   selector:     matchLabels:       app: xk6-tracing   template:     metadata:       labels:         app: xk6-tracing     spec:       containers:         - env:             - name: ENDPOINT               value: adot-collector-service:4317           image: ghcr.io/grafana/xk6-client-tracing:v0.0.2           name: xk6-tracingEnter fullscreen modeExit fullscreen modeLet's create the deployment.kubectl create -f k6-tracing-deploy.yaml -n adot-traces-demoEnter fullscreen modeExit fullscreen modeBoth the ADOT collector and k6-tracing pods should now be running.$ kubectl get po -n adot-traces-demo    NAME                              READY   STATUS    RESTARTS   AGE adot-collector-7cbf849b89-b4bkl   1/1     Running   0          14m xk6-tracing-69b48fcfd9-bjzbd      1/1     Running   0          24sEnter fullscreen modeExit fullscreen modeX-RayWe can now headover to AWS X-Ray,  inap-south-2region that we mentioned in the adot-config.The nodes(services) shown in the screenshot belong to our demo application. We could filter for traces that passes through a particular service name for ex. article service, like below.If we click on a single trace we should be able to see a complete service map for that trace, that shows all the services that trace traverses.If we go a further down on this we should be able to see the details for segments/spans with in this trace.GrafanaSo we far we were able to see the traces in AWS X-Ray, we can do a similar exercise on Grafana. I am using a Grafana Cloud Free subscription for this lab.Go to Connections, Add a new connection and search for X-Ray and install it.You can then go to datasources, add a new X-Ray datasource with the access key id, secret access key, and default region(I have chosen ap-south-2 which matches with adot config).All good, we can try adding a new panel, go to dashboards > new dashboard and a new visualization with table as panel type and a sample query for ex.service(id(name: \"article-service\"  ))We can click on one of the traces we should take us to the explore view where we can see the node graph(service map)We should also see the trace explorer that shows the individual spans.Okay so we reached this far, that was some fun exploring traces on AWS and Grafana with Open Telemetry. Thank you for reading !!!CleanUpJust delete the namespace withkubectl delete ns adot-traces-demoand it should remove the workloads we deployed from kubernetes and stop sending any new data to the cloud."}
{"title": "Boost Your Lambdas with Powertools", "published_at": 1714104951, "tags": ["aws", "lambda", "python", "serverless"], "user": "Kasun de Silva", "url": "https://dev.to/aws-builders/turbocharge-your-lambda-functions-with-aws-lambda-powertools-for-python-fph", "details": "AWS Lambda Powertools for Python is a comprehensive suite of utilities designed to enhance the development of serverless applications using AWS Lambda. It simplifies logging, tracing, metric gathering, and more, allowing developers to focus more on business logic rather than boilerplate code. In this blog post, we\u2019ll explore the key features of AWS Lambda Powertools for Python and provide an example of deploying a Lambda function using Terraform with the Powertools layer.Key Features of AWS Lambda Powertools for Python1. LoggerThe Logger utility simplifies logging setup across Lambda functions, providing structured logging as JSON out of the box. It automatically captures key information like cold start and function ARN, and supports logging correlation IDs for distributed tracing.fromaws_lambda_powertoolsimportLoggerlogger=Logger()@logger.inject_lambda_contextdeflambda_handler(event,context):logger.info(\"Processing event\")return{\"statusCode\":200}Enter fullscreen modeExit fullscreen mode2. TracerTracer is built on top of AWS X-Ray and provides decorators to trace Lambda function handlers and methods effortlessly. It captures cold starts, annotates errors correctly, and traces downstream calls to other AWS services.fromaws_lambda_powertoolsimportTracertracer=Tracer()@tracer.capture_lambda_handlerdefhandler(event,context):# Your business logic herepassEnter fullscreen modeExit fullscreen mode3. MetricsThe Metrics utility provides a straightforward way to capture custom business metrics using Amazon CloudWatch. It batches and flushes the metrics at the end of the function execution to minimize the number of calls made to CloudWatch.fromaws_lambda_powertoolsimportMetricsfromaws_lambda_powertools.metricsimportMetricUnitmetrics=Metrics()@metrics.log_metricsdeflambda_handler(event,context):metrics.add_metric(name=\"SuccessfulBookings\",unit=MetricUnit.Count,value=1)return{\"statusCode\":200}Enter fullscreen modeExit fullscreen mode4. UtilitiesAWS Lambda Powertools also includes several other utilities like Parameters for retrieving and caching parameter values from AWS Systems Manager or AWS Secrets Manager, Idempotency to ensure Lambda functions are idempotent, and Feature Flags to manage feature toggling.Deploying a Lambda Function with Terraform and Powertools LayerTo deploy an AWS Lambda function with the Powertools layer using Terraform, you first need to define your infrastructure as code. Below is a basic example of how you can set up a Lambda function with the AWS Lambda Powertools layer.provider\"aws\"{region=\"us-east-1\"}resource\"aws_lambda_function\"\"my_lambda\"{function_name=\"MyLambdaFunction\"handler=\"lambda_function.lambda_handler\"runtime=\"python3.8\"role=aws_iam_role.lambda_exec_role.arnfilename=\"path_to_your_lambda_deployment_package.zip\"layers=[\"arn:aws:lambda:us-east-1:017000801446:layer:AWSLambdaPowertoolsPython:2\"]}resource\"aws_iam_role\"\"lambda_exec_role\"{name=\"lambda_execution_role\"assume_role_policy=jsonencode({Version=\"2012-10-17\"Statement=[{Action=\"sts:AssumeRole\"Principal={Service=\"lambda.amazonaws.com\"}Effect=\"Allow\"},]})}output\"lambda_arn\"{value=aws_lambda_function.my_lambda.arn}Enter fullscreen modeExit fullscreen modeThis Terraform script sets up a basic Lambda function with the AWS Lambda Powertools layer. It defines the necessary IAM role for the Lambda function to execute and outputs the ARN of the Lambda function after deployment.AWS Lambda Powertools for Python is a powerful toolkit that can significantly simplify the development and maintenance of serverless applications on AWS. By using its features, developers can ensure that their applications are robust, scalable, and easy to manage. You can read more about AWS Lambda Powertools in the following link.Read more:https://docs.powertools.aws.dev/lambda/python/latest/Happy Coding!"}
{"title": "AWS Community Builders partnership with Techne Summit Cairo 2024", "published_at": 1714075422, "tags": ["aws", "summit", "community", "awscommunitybuilders"], "user": "Fady Nabil", "url": "https://dev.to/aws-builders/aws-community-builders-partnership-with-techne-summit-cairo-2024-35m1", "details": "AWS Community Builders are proud to announce our partnership with Techne Summit Cairo 2024. Techne Summit is coming to you on MAY. 25 - 27. There will be lots of exciting things planned for you with a lot of Content planned, over several Industry-Focused Tracks! Hurry up and book your ticket now.https://cairo.technesummit.com/2024"}
{"title": "AWS SnapStart - Part 19 Measuring cold starts and deployment time with Java 17 using different Lambda memory settings", "published_at": 1714058022, "tags": ["aws", "java", "serverless", "coldstart"], "user": "Vadym Kazulkin", "url": "https://dev.to/aws-builders/aws-snapstart-part-19-measuring-cold-starts-and-deployment-time-with-java-17-using-different-lambda-memory-settings-30ml", "details": "IntroductionIn thePart 12of our series we measured the cold starts of the Lambda function with Corretto Java 21 runtime without SnapStart enabled, with SnapStart enabled and also applied DynamoDB invocation priming optimization with different Lambda function memory settings .  In this article we\u2019d like to do the same measurement but for Corretto Java 17 runtime and then do comparison between both.Measuring cold starts with Java 17 with and without SnapStart enabled using different Lambda memory settingsIn our experiment we'll re-use the application introduced inpart 8for this. Here is the code for thesample application. There are basically 2 Lambda functions which both respond to the API Gateway requests and retrieve product by id received from the Api Gateway from DynamoDB. One Lambda function GetProductByIdWithPureJava17Lambda can be used with and without SnapStart and the second one GetProductByIdWithPureJava17LambdaAndPriming uses SnapStart and DynamoDB request invocation priming. We'll measure cold starts using the following memory settings in MBs : 256, 512, 768, 1024, 1536 and 2048.The results of the experiment below were based on reproducing approximately 100 cold starts for the duration of our experiment which ran for approximately 1 hour. For it (and all experiments from my previous articles) I used the load test toolhey, but you can use whatever tool you want, likeServerless-artilleryorPostmanCold start time without SnapStart in ms:Experiment descriptionp50p75p90p99p99.9max256 MB7309.667432.67575.517662.517782.257962.01512 MB cold start time w/o SnapStart4213.374256.074325.174496.154661.234786.71768 MB3310.083414.93551.824271.484421.094594.421024 MB2880.532918.792974.453337.293515.863651.651536 MB2390.152434.522464.462668.952812.152987.042048 MB2198.022272.52397.972757.062892.653005.31Cold start time with SnapStart without Priming in ms :Experiment descriptionp50p75p90p99p99.9max256 MB4972.835227.665754.127551.767559.317562.5512 MB2550.592604.692765.672942.483108.763110.69768 MB1801.281887.922251.032604.692681.292681.341024 MB1521.331578.641918.352113.652115.772117.421536 MB1204.061325.321507.701817.561821.191821.62048 MB1129.451286.171583.381819.371998.602000.17Cold start time with SnapStart enabled and with DynamoDB invocation Priming in ms :Experiment descriptionp50p75p90p99p99.9max256 MB1126.071183.771397.421608.901621.821622.23512 MB831.84881.491136.241348.031386.291387.52768 MB819.46891.231141.931243.191808.501809.341024 MB692.79758.001003.801204.061216.151216.881536 MB713.17773.31995.801124.941372.501372.732048 MB797.64858.871080.861296.491376.621377.05I also measured the deployment time of such sample project with all memory settings mentioned above with SnapStart enabled for all functions and GetProductByIdWithPureJava17LambdaAndPriming function additionally using priming. Here are the results:Experiment descriptionaverage deployment time in minutes/seconds256 MB deployment time with SnapStart2m 57 s512 MB deployment time with SnapStart2m 19 s768 MB deployment time with SnapStart2m 04 s1024 MB deployment time with SnapStart2m 10 s1536 MB deployment time with SnapStart2m 12 s2048 MB deployment time with SnapStart2m 13 sConclusionsIn this article we measured the cold start time of the Lambda function without SnapStart with SnapStart and for the latter with additional priming of DynamoDB invocation for Java 17 runtime.In case of not enabling SnapStart we observed that increasing the memory from 256 MB to 2048 MB constantly brought significant reduction of the cold start time. We observed the same with Java 21 runtime, but cold start times with Java 17 runtime  being a bit lower than with Java 21 runtime (for the relative early version of Java 21 used for the measurements)In case of enabling SnapStart but not using priming we observed that increasing the from 256 MB to 1536 MB brought significant reduction of the cold start time with very low impact of increasing the memory from 1536 MB or even to 2048 MB.  We observed similar with Java 21 runtime, but the impact of increasing memory even beyond 1024 MB already became lower. The cold start times with Java 17 runtime were also a bit lower than with Java 21 runtime (for the relative early version of Java 21 used for the measurements)In case of enabling SnapStart and with additional priming of DynamoDB invocation we observed that increasing the from 256 MB to 1024 MB brought significant reduction of the cold start time but after that the impact of increasing the memory to 1536 MB or 2048 MB was low or even negative for all percentiles. We observed the same with Java 21 runtime. Also the result of using Java 17 and Java 21 runtimes are very close.So for the SnapStart-enabled Lambda function and considering our use case with Lambda function reading from the DynamoDB table the Lambda memory setting of 1024 MB is a good choice.Deployment time for the SnapStart enabled on all 3 Lambda functions and additionally applying priming on one of them also became lower by giving the Lambda function more memory but only until 786 MB. After that we even observed the small increase of the deployment time. With Java 21 runtime we observed  decrease of the deployment time by giving the Lambda function more memory until 1024 MB and then relative stable deployment time by giving Lambda function even more memory."}
{"title": "AWS Credentials for Serverless", "published_at": 1714053600, "tags": ["aws", "serverless", "security", "zerotrust"], "user": "Seth Orell", "url": "https://dev.to/aws-builders/aws-credentials-for-serverless-2f24", "details": "AWS is a zero-trust platform1. That is, every call to AWS must provide credentials so that the caller can be validated and her authorization checked. How one manages these credentials will vary depending on the execution environment. A developer, who gets his workstation set up with his own AWS credentials, will often find that the application he is building cannot (and should not) consume credentials in the same way. Why do these differences exist? And what should he do to manage credentials?For credentials on your workstation, AWS recommends using IAM Identity Center SSO. This lets you verify your identity (often with a standard, non-AWS identity provider like Google or Okta), and then that user can assume an IAM role to provide a set of temporary credentials. This works well and is fairly secure, especially if your identity provider is set up with multi-factor authentication (MFA). Because the AWS credentials are short-lived, even if they leak out, they expire quickly thus limiting exposure. Why can't we take this approach with the applications we are building?We want to have the application assume a role and pick up short-term credentials. However, we can't use the workstation approach because we need user authentication (SSO/MFA) to establish the user, and that's not possible at the application's runtime. To get out of this jam, we can rely on the fact that all our application runtimes are serverless and will happen within an AWS service (in our case Lambda or Fargate). This establishes sufficient trust such that we can assign the execution environment a role and let it obtain short-term credentials.In this article, I want to examine how your application running in either Lambda or Fargate ought to get its AWS credentials. We'll discuss how the AWS SDKs use credential provider chains to establish precedence and one corner case I found myself in. Let's dig in.Credential SourcesAs mentioned earlier, you can provide credentials to your application in several ways including (but not limited to) environment variables, config/credentials files, SSO through IAM Identity Center, instance metadata, and (don't do this) directly from code. Irrespective of which method you choose, you can allow the AWS SDK to automatically grab credentials via its built-in \"credentials provider.\"The mechanism for selecting the credential type is called the \"credential provider\" and the built-in precedence (i.e., the order in which it checks credential sources) is called the \"credential provider chain.\" This is language agnostic. PerAWS documentation, \"All the AWS SDKs have a series of places (or sources) that they check to get valid credentials to use to make a request to an AWS service.\" And, once they locate any credentials, the chain stops and the credentials are used.For the NodeJS SDK, that precedence is generally:Explicit credentials in code (again, please don't do this)Environment VariablesShared config/credentials fileTask IAM Role for ECS/FargateInstance Metadata Service (IMDS) for EC2So, we can pass in credentials in many ways. Why should we choose one over another? Each approach varies in terms of security and ease of use. Fortunately, AWS allows us to easily set up our credentials without compromising security. We are going to focus on two recommended approaches: environment variables (for Lambda) and the task IAM role (for Fargate)Environment Variable CredentialsCredentials in environment variables are, perhaps, the easiest way to configure your AWS SDK. They are near the top of the precedence list and will get scooped up automatically when you instantiate your SDK. For example, if you set the following environment variables in your runtime:export AWS_ACCESS_KEY_ID=\"AKIA1234567890\" export AWS_SECRET_ACCESS_KEY=\"ABCDEFGH12345678\"Enter fullscreen modeExit fullscreen modeThen when you instantiate your AWS SDK, these credentials will get loaded automatically, like so:import{DynamoDBClient}from'@aws-sdk/client-dynamodb'constclient=newDynamoDBClient({region:'us-east-1'});// Loads env credentialsEnter fullscreen modeExit fullscreen modeNote that theAWS_ACCESS_KEY_IDbegins with \"AKIA\". This signifies that this is a long-term access key with no expiration. These types of keys are attached to an IAM user or, if you are reckless, the AWS account root user2.Alternatively, you may run across AWS credentials that look like the following:AWS_ACCESS_KEY_ID=ASIA1234567890 AWS_SECRET_ACCESS_KEY=ABCDEFGH12345678 AWS_SESSION_TOKEN=XYZ+ReallyLongString==Enter fullscreen modeExit fullscreen modeThese credentials are short-lived. You can tell this both by the presence of theAWS_SESSION_TOKENand that theAWS_ACCESS_KEY_IDbegins with \"ASIA\" instead of \"AKIA\".When you use a credential provider, it consumes the Access Key, Secret,andSession Token. These tokens can be set to expire anywhere from 15 minutes to 12 hours from issuance. This would be a drag if you had to repeatedly go fetch these short-lived tokens and save them so your application can use them. Fortunately, you don't have to. Both Lambda and ECS offer built-in mechanics to provide your application with short-term credentials. Let's start with Lambda.Using Credentials in LambdaAdd the following line to one of your Lambdas:console.log(process.env);Enter fullscreen modeExit fullscreen modeAnd you'll seeAWS_ACCESS_KEY_ID,AWS_SECRET_ACCESS_KEY, andAWS_SESSION_TOKEN. How did they get there? AWS added them for you using the (required) IAM role attached to your Lambda. During initialization, AWS calls out to their Secure Token Service (STS), obtains short-term credentials, and then conveniently injects those credentials into your Lambda's environment variables.Lambdas are special in this regard. You (or the credentials provider) don't have to do anything extra to fetch credentials, they are just there for you to use. Why?Lambdas are short-lived. Even under constant use, every hour or so they are automatically recycled. This means that a single short-lived token can serve the Lambda that is using it; no re-fetching of tokens is necessary. For example, if AWS sets Lambdas to last no more than an hour before being decommissioned, it can set the expiration for the access token to just over 60 minutes and the application using the token will never need to fetch another.Having your credentials provider automatically find and use the credentials in Lambda's environment variables is both the recommendedand easiestapproach. This is a true win/win.Using Credentials in FargateECS Fargate shares many traits with Lambda: they're both managed by AWS (in neither case are we taking care of the underlying servers), they scale up and down automatically, and each can have an IAM role that provides permissions for the application's runtime.However, Fargate containers don't automatically recycle. They are relatively long-lived when compared to Lambda and can easily live longer than the maximum STS token expiration. This means the method used by Lambda to inject the STS tokens into the runtime environment won't work.Instead, you can use the--optional but recommended--Task Role ARN property of your ECS task definition to specify the permissions you would like your task to have. Then your credentials provider can assume this role to obtain short-term credentials it can use. It manages this for you and you don't have to do anything but set the TaskRoleArn in your task definition.Why You Should Know ThisThe AWS SDK's credentials provider doesn't know \"I'm in a Lambda\" or \"I'm in Fargate.\" When invoked, the SDK will use the default credentials provider to step through a chain of locations to look for credentials and it will stop as soon as it finds one. This means things often \"just work.\" But, it also means you can short-circuit the precedence chain if you are not careful (or you can do it purposefully; I'll give an example later).If you are using Lambda, and you new up an SDK client like this:constclient=newDynamoDBClient({region:'us-east-1',credentials:{accessKeyId:'ABC',// Don't do thisseretAccessKey:'123',// Don't do this, either},});Enter fullscreen modeExit fullscreen modeyour credentials provider will never check the environment variables for credentials and will run with what you gave it.Likewise, in Fargate, if you either pass in direct credentials or set environment variables ofAWS_ACCESS_KEY_IDandAWS_SECRET_ACCESS_KEY, your credentials provider will never use your TaskRoleArn. This can be confusing if you are not used to it.Breaking the Chain on PurposeI was working with a client on a container migration project, where they needed to move their container workloads from Kubernetes (K8) on EC2 over to Fargate on ECS. At one point during the transition, the same container needed to be simultaneously running in both places. I knew I wanted to use the TaskRoleArn in Fargate, but that would not fly in the K8 deployment as it would grab the credentials from the EC2 instance on which it ran. And, since that EC2 instance served many disparate K8 pods, it was a poor3place to manage the runtime permissions of the containers underneath it.The K8 configuration had environment variables set to long-term credentials for an IAM user. At first, the ECS task definition just used the same credentials (from env vars). Then, we created a dedicated IAM role for the task and attached it to the definition as a TaskRoleArn. OK, time for a quick quiz:What happens now? The ECS container will:A) Use the IAM role fromTaskRoleArn.B) Use the environment variable credentials.C) Throw aConflictingCredentialsError.The correct answer isB. As long as those environment variable credentials are present, the credentials provider will stop looking after discovering them. During the migration, we used this to our advantage as we kept the code the same and just modified the configuration based on the destination (environment variable credentials in K8, none in Fargate). Eventually, we were only using theTaskRoleArnand we could retire those long-term credentials and the environment variables that surfaced them.What Can Go Wrong?Long-term credentials pose a real risk of leaking. AWS advises its users to take advantage of SSO and IAM roles for their user and application runtimes, respectively. I know an engineer who inadvertently checked in a hard-codedAWS_ACCESS_KEY_IDandAWS_SECRET_ACCESS_KEYinto a public GitHub repository. Within minutes, they had been scraped, and expensive BitCoin miners were deployed in far-away regions of his company's AWS account (kudos to AWS for expunging those actions from their AWS bill the following month).The engineer had thought the repository was private. However, the fundamental issue was using hard-coded, long-lived credentials in the first place. Using fully managed, serverless architectures like Lambda and Fargate along with role-based, short-term credentials, you can avoid this kind of headache.Further ReadingAWS Documentation:Setting credentials in Node.jsAWS CLI User Guide:Configuration and credential file settingsAmazon ECS Developer Guide:Task IAM roleOwnership Matters:Zero Trust Serverless on AWSGurjot Singh:AWS Access Keys - AKIA vs ASIANick Jones:AWS Access Keys - A ReferenceI have written on zero trusthere.\u21a9Never give programmatic access to your root user.\u21a9Least Privilege would be off the table.\u21a9"}
{"title": "Implementation of Melodistiq: Generating Lyrics and Melodies with AI", "published_at": 1714006841, "tags": ["ai", "webdev", "python", "music"], "user": "Renaldi", "url": "https://dev.to/aws-builders/unlocking-musical-creativity-with-ai-generating-lyrics-and-melodies-55p0", "details": "IntroductionHello, fellow cloud enthusiasts and builders!In today's blog post, we will dive into an intriguing project that combines artificial intelligence with music creation. This post will detail the implementation of a Python code designed to generate music lyrics and melodies using AI technologies. This follows the successful presentation of the \"Unlocking Musical Creativity with AI: Generating Lyrics and Melodies\" talk.The ScenarioThe goal of this project is to automate the creation of music lyrics and melodies. This can be particularly useful for musicians seeking inspiration or developers exploring the intersection of AI and creative arts. The process involves generating lyrics based on existing song data, creating a melody that fits the generated lyrics, and presenting the final composition in both audio and written formats.Libraries and Tools UsedTo accomplish this task, we utilize several Python libraries, each serving a distinct purpose:Pandas (pandas): A powerful data manipulation library used here to handle and preprocess lyrical data.Natural Language Toolkit (NLTK): This library is used for text processing to analyze, manipulate, and generate text data.OpenAI's GPT-3.5: Leveraged to enhance the quality of the generated lyrics.Mingus: An advanced music theory and notation package used to handle music data and generate melodies.FastText: A library developed by Facebook for efficient learning of word representations and sentence classification.FPDF: A library to generate PDF files, useful for presenting the lyrics in a readable format.Let's walk through implementing our code now.ImplementationSetting Up and Downloading ModelsFirst, we load necessary models and corpora:import fasttext import fasttext.util import nltk nltk.download('cmudict') nltk.download('punkt') from nltk.corpus import cmudict d = cmudict.dict()  fasttext.util.download_model('en', if_exists='ignore') ft = fasttext.load_model('cc.en.300.bin')Enter fullscreen modeExit fullscreen modeHere, fastText and NLTK's CMU Pronouncing Dictionary are initialized. fastText is used later to find words similar to those not found in the CMU dictionary.Data CollectionData is loaded and preprocessed from a CSV file containing song lyrics:import pandas as pd df = pd.read_csv('EdSheeran.csv') df = df.dropna() lyrics = df['Lyric'].str.replace('\\n', ' ').str.replace('\\r', ' ').tolist()Enter fullscreen modeExit fullscreen modeThis section reads a CSV file, cleans the data by removing missing values, and formats the lyrics into a list.Data PreprocessingThe lyrics are tokenized and cleaned:import re from nltk.tokenize import word_tokenize words = [word_tokenize(re.sub(r'\\W+', ' ', lyric).lower()) for lyric in lyrics] words = [word for sublist in words for word in sublist]Enter fullscreen modeExit fullscreen modeHere, special characters are removed, and the text is converted to lowercase to standardize the data.N-gram Model and Lyrics GenerationAn N-gram model is used to generate new lyrics:N-gram Model and Lyrics Generation  An N-gram model is used to generate new lyrics:  from nltk.util import ngrams from nltk.probability import FreqDist import random  n_values = [2, 5, 7] generated_lyrics = \"\"  for n in n_values:     ngrams_list = list(ngrams(words, n, pad_left=True, pad_right=True))     freq_dist = FreqDist(ngrams_list)      def generate_lyrics(starting_ngram, freq_dist, num_words):         generated_words = list(starting_ngram)         for _ in range(num_words):             next_word_candidates = [ngram[-1] for ngram in freq_dist.keys() if ngram[:n-1] == tuple(generated_words[-(n-1):])]             if next_word_candidates:                 next_word = random.choice(next_word_candidates)                 generated_words.append(next_word)             else:                 break         return ' '.join(generated_words).replace(' ,', ',').replace(' .', '.').replace(' ;', ';')      starting_ngram = random.choice(list(freq_dist.keys()))     generated_lyrics += generate_lyrics(starting_ngram, freq_dist, 200)Enter fullscreen modeExit fullscreen modeThis segment builds N-grams from the cleaned words and creates a frequency distribution to model the probabilities of word sequences. New lyrics are generated based on these probabilities.Enhancing Lyrics with GPT-3.5We then look to enhance our lyrics with the help of GPT-3.5.import openai import os openai.api_key = os.getenv('openai_api_key') conversations = {} session_id=0; conversations[session_id] = []  conversations[session_id].append({\"role\": \"system\", \"content\": \"You are a helpful assistant who will transform the lyrics below into a song.\"}) conversations[session_id].append({\"role\": \"user\", \"content\": generated_lyrics})  response = openai.ChatCompletion.create(   model=\"gpt-3.5-turbo\",   messages=conversations[session_id],   max_tokens=200 )  gpt_lyrics = response.choices[0][\"message\"][\"content\"].strip()Enter fullscreen modeExit fullscreen modeHere, the OpenAI GPT-3.5 API is used to refine and enhance the generated lyrics, adding a layer of complexity and polish that might be lacking from the simple N-gram model. The environment variable openai_api_key is used to authenticate the API request.Generating and Exporting the MelodyThe code then generates a melody using the Mingus library based on the stress patterns of the lyrics:from mingus.containers import Note, Bar, Track from mingus.midi import midi_file_out import mingus.extra.lilypond as lilypond  def generate_melody(lyrics):     # Load the CMU Pronouncing Dictionary     stress_pattern = []      tokens = nltk.word_tokenize(lyrics)      # Fix contractions     fixed_tokens = [contractions.fix(token) for token in tokens]      # Function to get the stress pattern of a word     def get_stress(word):         word = word.lower()         phones = pronouncing.phones_for_word(word)         if phones:             stress_pattern = [int(s) for s in pronouncing.stresses(phones[0])]             return [stress_pattern]         else:             # handle contractions             if \"'\" in word:                 parts = word.split(\"'\")                 stress_pattern = []                 for part in parts:                     stress_pattern += get_stress(part)                 return stress_pattern             # handle hyphenated words             elif '-' in word:                 parts = word.split('-')                 stress_pattern = []                 for part in parts:                     stress_pattern += get_stress(part)                 return stress_pattern             else:                 print(f'Word not found in dictionary: {word}')                 # Find a similar word in the dictionary and use its stress pattern                 similar_word = find_similar_word(word)                 if similar_word:                     return get_stress(similar_word)                 else:                     # Use default pattern if no similar word is found                     return [[0, 1, 2]]      # Get the stress pattern of the lyrics     for word in fixed_tokens:         # remove punctuation         word = re.sub(r'[^\\w\\s]', '', word)         stress_pattern += get_stress(word)      # Flatten the stress_pattern list     stress_pattern = [item for sublist in stress_pattern for item in sublist]      print(lyrics)     print(tokens)     print([\"Here are the stress patterns:\"] + stress_pattern)      # Generate a melody based on the stress pattern     track = Track()     b = Bar()     b.set_meter((4, 4))     beats_in_current_bar = 0     for stress in stress_pattern:         if stress == 0:             note = Note('C', 4)         elif stress == 1:             note = Note('E', 4)         elif stress == 2:             note = Note('G', 4)         b + note         beats_in_current_bar += 1         if beats_in_current_bar == 4:             track.add_bar(b)             b = Bar()             b.set_meter((4, 4))             beats_in_current_bar = 0     track.add_bar(b)       return trackEnter fullscreen modeExit fullscreen modeHere, we define a function generate_melody that takes a string of lyrics as input and generates a melody based on the phonetic stress pattern of the words. The function uses the nltk library to tokenize the lyrics, handles contractions, and determines the stress pattern of each word using the CMU Pronouncing Dictionary. We account for special cases like contractions, hyphenated words, and words not found in the dictionary, attempting to find similar words or applying a default stress pattern when necessary.After extracting and flattening the stress pattern of the entire lyrics, the function uses this pattern to create a melody where different stress levels are mapped to specific musical notes (C, E, G) in a 4/4 time signature, adding these notes to a musical track using the mingus library. Each stress level in the pattern corresponds to a different note, and the function organizes these notes into bars, with each bar containing up to four beats. The track, comprising a series of bars filled with notes based on the lyrical stress pattern, is returned at the end of the function. This allows for the conversion of lyrical content into a basic musical representation, integrating elements of natural language processing and music composition.We can then look into creating the MIDI file and sheet music:formatted_lyrics = format_lyrics(gpt_lyrics) formatted_lyrics_with_newlines = add_newlines(formatted_lyrics) print(\"Here are the lyrics:\" + formatted_lyrics_with_newlines)  melody = generate_melody(formatted_lyrics_with_newlines)  lilypond_string = lilypond.from_Track(melody) with open('melody.ly', 'w') as f:     f.write(lilypond_string)  print(melody)Enter fullscreen modeExit fullscreen modeAs can be seen, we output the lyrics and MIDI.Outputting Lyrics as PDFFinally, we generate a PDF document containing the formatted lyrics, useful for singers or for archival purposes.from fpdf import FPDF  pdf = FPDF() pdf.add_page() pdf.set_font('Arial', 'B', 16) title = formatted_lyrics_with_newlines.split('\\n')[0] pdf.cell(0, 10, title, 0, 1, 'C') pdf.set_font('Arial', '', 12) pdf.multi_cell(0, 10, formatted_lyrics_with_newlines) pdf.output('lyrics.pdf')Enter fullscreen modeExit fullscreen modeNow, we have our fully functioning Python code for making music! Now, we can decide where to deploy this. Leveraging the capability of AWS, I am going to deploy it on an EC2.Setting Up the EC2 InstanceGo to the EC2 dashboard in AWS Management Console.Click on \"Launch Instance\".Choose an Amazon Machine Image (AMI), such as Amazon Linux 2 AMI or Ubuntu Server.Select an instance type (e.g., t2.micro for testing purposes).Configure instance details as required.Add storage if the default isn\u2019t enough.Configure Security Group to allow SSH (port 22) and any other necessary ports (e.g., port 80 for web server).Review and launch the instance by selecting or creating a new key pair.Now that we have the EC2 instance ready, we can access it.Accessing the EC2 InstanceConnect to your instance using SSH. For Windows, you can use PuTTY or any SSH client:ssh -i /path/to/your-key.pem ec2-user@your-instance-ipEnvironment SetupInstall Python and other necessary tools.sudo yum update -y sudo yum install python3 git -yEnter fullscreen modeExit fullscreen modeNow move the code over to EC2 with SCP or SFTP and install the relevant Python libraries. You can also clone the repo provided at the end of this post.pip3 install pandas nltk openai pronouncing re fpdf fasttextYou also will need to install lilypond for usage with Yum.sudo yum install lilypond -ySet environment variables (for example, for the OpenAI API key):export openai_api_key='your-api-key'With the environment now set up, we can run and automate the script.Running and Automating the ScriptRun the script with the command below.python3 main.pyTo ensure the script runs automatically at system boot or keeps running after disconnections, we will use nohup to run the script in the background.nohup python3 your_script.py &Set up a systemd service if you want it to start at boot and restart on failures.We can then monitor the script execution through checking the logs.tail -f nohup.outOptionally, configure CloudWatch to monitor the EC2 performance or to set up more advanced logging and alerting.And with that, we've set up our deployed solution! It takes a bit of learning to wrap your head around the NLP and music theory bits, but it really is quite a straightforward approach to getting started on making music with AI.ConclusionThis project showcases the power of AI in creative processes like songwriting. By integrating various technologies and libraries, we've created a system that not only generates lyrics but also composes a melody, presenting it in both audio and visual forms. This illustrates the potential for AI to assist in artistic expressions, providing tools that can inspire and enhance the creative capabilities of its users. Whether you're a developer, a musician, or an enthusiast in the realms of AI or music, this project offers fascinating insights and possibilities.The repo to the project can be accessed at:https://github.com/renaldig/melodistiq-music-generator"}
{"title": "Supercharge Data Insights: Harnessing AWS Glue for Advanced ETL in Healthcare and Life Sciences", "published_at": 1713962226, "tags": ["aws", "serverless", "etl", "healthcare"], "user": "Stephen Woodard", "url": "https://dev.to/aws-builders/supercharge-data-insights-harnessing-aws-glue-for-advanced-etl-in-healthcare-and-life-sciences-5ge7", "details": "With the explosion in data gathering over the last decade, as highlighted in previous discussions, healthcare and life science organizations find themselves at a crucial juncture. The challenge isn't just in storing massive volumes of data but in effectively transforming this data into actionable insights. AWS Glue provides a powerful, serverless ETL service that is pivotal in turning the data rich into information wealthy.Transitioning from Data Challenges to ETL SolutionsAs we've explored, the journey from on-premises systems to scalable cloud solutions marks a significant shift in how data is managed. AWS Glue stands out as a key component in this transformation, offering seamless data integration capabilities that align perfectly with the needs of modern, data-intensive industries like healthcare and life sciences.Lets explore three Main Benefits of Using AWS Glue in Healthcare and Life Sciences1: Automated Data Integration:vAWS Glue simplifies the process of ETL, which is crucial for organizations dealing with the vast amounts of data predicted by IDC and other sources. By automatically discovering, cataloging, and preparing data, AWS Glue reduces the complexity and effort required, enabling organizations to focus on deriving insights rather than managing data.2: Scalability and Flexibility:In an environment where data volumes and sources are continuously expanding, AWS Glue's serverless approach allows organizations to scale their ETL processes without upfront investments in infrastructure. This scalability ensures that data management capabilities grow in tandem with data volumes and organizational needs.3: Cost Efficiency: By charging only for the resources used during active job processing, AWS Glue helps organizations manage their ETL expenses effectively. This is especially valuable in the healthcare sector where managing costs can directly impact patient care quality.Addressing the Data-to-Information GapThe phrase \"Data Rich, Information Poor\" particularly resonates within the healthcare sector, where the sheer volume of data often overwhelms traditional data processing methods. AWS Glue directly addresses this by enabling more efficient data transformations and loading processes, thus bridging the gap between data collection and actionable insights.There are some Key Use Cases for AWS Glue in Healthcare and Life Sciences that can explore further in depth1: Real-time Patient Data Processing: AWS Glue can streamline real-time data processing, allowing healthcare providers to integrate and analyze patient data as it's collected, facilitating quicker and more informed medical decisions.2: Data Lake Enhancement:By facilitating the integration of various data types into a centralized AWS S3 data lake, AWS Glue enhances the ability to analyze diverse datasets, such as patient records and medical images, in a unified manner.3: Legacy System Modernization: AWS Glue supports the migration of data from legacy systems to the cloud, thereby aiding healthcare organizations in modernizing their IT infrastructure without significant downtime or resource allocation.Incorporating AWS Glue into your data strategy can transform the way healthcare and life science organizations manage and analyze data. As these organizations continue to navigate the complexities of data management in a digital age, AWS Glue provides a robust, scalable, and cost-effective solution that not only manages but also maximizes the value of data assets.Are you ready to leverage AWS Glue to transform your healthcare data management practices? Discover how this powerful ETL service can help you become information wealthy by visiting the official AWS Glue documentation page:https://docs.aws.amazon.com/glue/latest/dg/what-is-glue.html"}
{"title": "Relational Databases on AWS: Comparing RDS and Aurora", "published_at": 1713915708, "tags": ["aws", "database", "rds", "aurora"], "user": "Guille Ojeda", "url": "https://dev.to/aws-builders/relational-databases-on-aws-comparing-rds-and-aurora-581f", "details": "There are two managed relational database services in AWS: Amazon Relational Database Service (RDS) and Amazon Aurora. Both provide the benefits of a fully managed database solution, but they have distinct features and use cases.In this article, we'll explore the key features and capabilities of RDS and Aurora, compare their differences, and provide guidance on choosing the right service for your application.Understanding Amazon RDS (Relational Database Service)Let's start by taking a closer look atAmazon RDS, AWS's fully managed relational database service. RDS makes it easy to set up, operate, and scale a relational database in the cloud, supporting a wide range of database engines.RDS Key Features:Fully managed database serviceSupports multiple database engines: MySQL, PostgreSQL, Oracle, SQL Server, MariaDBAutomatic backups and point-in-time recoveryMulti-AZ deployments for high availabilityRead replicas for read scalabilityVertical and horizontal scaling optionsRDS Instance Types and Storage:RDS offers a variety of instance types optimized for different workloads and performance requirements. Instance types range from small burstable instances to large memory-optimized instances.For storage, RDS provides three options:General Purpose (SSD): Balanced performance for a wide range of workloadsProvisioned IOPS (SSD): High-performance storage for I/O-intensive workloadsMagnetic: Cost-effective storage for infrequently accessed dataPricing:With RDS, you pay for the database instance hours, storage, I/O requests, and data transfer. RDS pricing varies based on the database engine, instance type, storage type, and region.RDS Backup and RestoreOne of the key benefits of using RDS is the automated backup and restore capabilities. RDS provides two types of backups:Automated Backups: RDS automatically takes daily snapshots of your database, allowing you to restore to any point in time within the retention period (up to 35 days).Manual Snapshots: You can manually create database snapshots at any time, which are stored until you explicitly delete them.To restore a database from a backup, you simply create a new RDS instance and specify the backup to use. RDS handles the rest, creating a new instance with the restored data.Point-in-time recovery (PITR) is another powerful feature of RDS. With PITR, you can restore your database to any point in time within the backup retention period, down to the second. This is particularly useful for recovering from accidental data modifications or deletions.RDS High Availability and FailoverHigh availability is crucial for many applications, and RDS provides several options to ensure your database remains available in the event of a failure.Multi-AZ Deployments:With a Multi-AZ deployment, RDS automatically provisions and maintains a synchronous standby replica in a different Availability Zone (AZ). If the primary instance fails, RDS automatically fails over to the standby, minimizing downtime.Multi-AZ deployments provide enhanced durability and fault tolerance, with failover typically completing within a minute or two. This is ideal for production workloads that require high availability.Read Replicas:Read replicas are separate database instances that are asynchronously replicated from the primary instance. They are used to offload read traffic from the primary instance and improve read scalability.You can create up to 5 read replicas per primary instance, within the same region or across different regions. Read replicas can be promoted to standalone instances if needed, providing a way to create independent databases for specific use cases.Understanding Amazon AuroraAmazon Aurora is a fully managed relational database service that is compatible with MySQL and PostgreSQL. It offers the simplicity and cost-effectiveness of open-source databases with the performance and availability of commercial databases.Aurora Key Features:MySQL and PostgreSQL compatibleHigh-performance storage and cachingAuto-scaling of read replicasServerless option for automatic scalingGlobal database for multi-region deploymentsContinuous backups and point-in-time restoreAurora Storage and Replication:Aurora uses a distributed, fault-tolerant, and self-healing storage system that automatically scales up to 128 TB per database instance. It replicates data across multiple AZs, providing high durability and availability.Aurora's storage is designed for fast, consistent performance. It uses a multi-tier caching architecture that includes an in-memory cache, a buffer pool, and a storage cache, reducing the need for disk I/O and improving performance.Pricing:With Aurora, you pay for the database instance hours, storage, I/O requests, and data transfer. Aurora pricing varies based on the database engine (MySQL or PostgreSQL), instance type, and region.Aurora Performance and ScalabilityOne of the key advantages of Aurora is its high-performance storage and caching architecture. Aurora can deliver up to 5X the throughput of standard MySQL and 3X the throughput of standard PostgreSQL, without requiring any changes to your application code.Auto-scaling Read Replicas:Aurora automatically scales read replicas based on the workload, ensuring your database can handle read-heavy traffic patterns. As the read traffic increases, Aurora seamlessly adds new read replicas to the cluster, distributing the load across multiple instances.Aurora Serverless:For applications with unpredictable or intermittent workloads, Aurora Serverless provides a fully managed, auto-scaling configuration for Aurora MySQL and PostgreSQL. With Aurora Serverless, your database automatically starts up, shuts down, and scales capacity based on your application's needs.This is particularly useful for development and testing environments, or applications with variable traffic patterns, as it eliminates the need to manage database capacity manually.Aurora Backup and RestoreLike RDS, Aurora provides automated continuous backups and point-in-time restore capabilities. However, Aurora takes it a step further with some additional features.Continuous Backups:Aurora automatically takes incremental backups of your database, continuously and transparently, with no impact on performance. These backups are stored in Amazon S3, providing 11 9's of durability.Backup Retention:Aurora backups are retained for a default period of 1 day, but you can configure this up to 35 days. Backups are automatically deleted when the retention period expires, or when the DB cluster is deleted.Point-in-time Restore:With Aurora, you can restore your database to any point in time within the backup retention period, down to the second. This is similar to RDS PITR, but with the added benefit of Aurora's distributed storage architecture, which enables faster restores.Database Cloning:Aurora allows you to create a new database cluster from an existing one, effectively \"cloning\" the database. This is useful for creating test or development environments, or for performing analytics on a copy of your production data without impacting the live database.Stop copying cloud solutions, startunderstandingthem. Join over 4000 devs, tech leads, and experts learning how to architect cloud solutions, not pass exams, with theSimple AWS newsletter.RDS vs Aurora: Key Differences and Use CasesNow that we've explored the key features and capabilities of RDS and Aurora, let's compare them side by side.FeatureRDSAuroraDatabase EnginesMySQL, PostgreSQL, Oracle, SQL Server, MariaDBMySQL, PostgreSQLPerformanceGood performance for general-purpose workloadsHigh-performance storage and caching, optimized for read-heavy workloadsScalabilityVertical and horizontal scaling, read replicasAuto-scaling read replicas, Aurora Serverless for automatic scalingAvailabilityMulti-AZ deployments for high availabilityMulti-AZ storage, Global Database for multi-region deploymentsBackup and RestoreAutomated backups, manual snapshots, point-in-time recoveryContinuous incremental backups, point-in-time restore, database cloningCompatibilityWide range of database engines, easy migrationMySQL and PostgreSQL compatible, requires migration effortCostCost-effective for general-purpose workloadsHigher cost, but better performance and scalability for demanding workloadsUse Cases for RDS:Applications with moderate performance and scalability requirementsWorkloads that require a specific database engine (e.g., Oracle, SQL Server)Migrating existing on-premises databases to the cloudDevelopment and testing environmentsUse Cases for Aurora:Applications with high-performance and high-scalability requirementsRead-heavy workloads that can benefit from Aurora's caching and auto-scaling capabilitiesApplications with unpredictable or variable traffic patterns (using Aurora Serverless)Global applications that require multi-region database deploymentsChoosing the Right Relational Database ServiceChoosing between RDS and Aurora depends on your specific application requirements and workload characteristics. Here are some key factors to consider:Performance and Scalability:If your application demands high performance and scalability, particularly for read-heavy workloads, Aurora is the better choice. Its high-performance storage and caching architecture, along with auto-scaling read replicas, make it well-suited for demanding applications.Database Engine Compatibility:If your application requires a specific database engine, such as Oracle or SQL Server, RDS is the way to go. RDS supports a wide range of database engines, making it easier to migrate existing applications to the cloud.Cost Considerations:For general-purpose workloads with moderate performance requirements, RDS is more cost-effective than Aurora. However, if your application requires the high performance and scalability of Aurora, the additional cost may be justified.Existing Skills and Expertise:If your team is already familiar with MySQL or PostgreSQL, both RDS and Aurora are good choices. However, if you have expertise with a specific database engine supported by RDS, such as Oracle or SQL Server, that may be a deciding factor.When to Use RDSMigrating an existing on-premises database to the cloudApplications with moderate performance and scalability requirementsWorkloads that require a specific database engine not supported by AuroraDevelopment and testing environmentsExample: A web application with a backend database that requires SQL Server compatibility and has moderate traffic and performance requirements.When to Use AuroraBuilding a new, high-performance application from scratchApplications with demanding read-heavy workloadsServerless applications with unpredictable traffic patternsGlobal applications that require multi-region database deploymentsExample: A large-scale e-commerce platform with millions of daily users, requiring high throughput and low latency for product catalog searches and user profile management.Best Practices for Running Relational Databases on AWSRegardless of whether you choose RDS or Aurora, here are some best practices to keep in mind when running relational databases on AWS:Performance OptimizationChoose the appropriate instance type and size based on your workload requirementsMonitor CPU, memory, and I/O utilization to identify bottlenecks and optimize performanceUse caching solutions like ElastiCache to offload read traffic and improve performanceSecurity Best PracticesUse IAM roles and policies to control access to your database instancesEnable encryption at rest and in transit to protect sensitive dataRegularly apply security patches and updates to your database engineUse VPC security groups to control network access to your database instancesMonitoring and LoggingEnable and configure Amazon CloudWatch for monitoring database metrics and setting alarmsUse AWS CloudTrail to log and audit API activity related to your database instancesEnable database engine-specific logging, such as MySQL slow query logs or PostgreSQL query planner statisticsScaling and High AvailabilityUse read replicas to scale read traffic and improve performanceEnable Multi-AZ deployments for high availability and automatic failoverMonitor replication lag and ensure it stays within acceptable limitsTest failover scenarios regularly to ensure your application can handle database failures gracefullyConclusionAWS provides two powerful managed services for running them in the cloud: Amazon RDS and Amazon Aurora. RDS is a fully managed service that supports a wide range of database engines, making it a good choice for general-purpose workloads and migrating existing applications to the cloud. Aurora, on the other hand, is a high-performance, MySQL and PostgreSQL-compatible database service that is well-suited for demanding, read-heavy workloads.When choosing between RDS and Aurora, it's important to consider your application's specific requirements, including performance, scalability, compatibility, and cost. However, in cases where MySQL or PostgreSQL are suitable, Aurora is generally my preferred choice due to its advanced architecture and auto-scaling capabilities.Stop copying cloud solutions, startunderstandingthem. Join over 4000 devs, tech leads, and experts learning how to architect cloud solutions, not pass exams, with theSimple AWS newsletter.Realscenarios and solutionsThewhybehind the solutionsBest practicesto improve themSubscribe for freeIf you'd like to know more about me, you can find meon LinkedInor atwww.guilleojeda.com"}
{"title": "Create Certificate Authority with AWS Private CA\u00a0SDK", "published_at": 1713906000, "tags": ["aws", "certificates", "awssecurity", "security"], "user": "Benjamin Ajewole", "url": "https://dev.to/aws-builders/create-certificate-authority-with-aws-private-ca-sdk-dan", "details": "In cybersecurity, the importance of secure communication cannot be overstated. Certificates are the bedrock for establishing encrypted and authenticated connections over networks, safeguarding data integrity, confidentiality, and authenticity. When managing certificates at scale within cloud environments like Amazon Web Services (AWS), leveraging tools like AWS Private Certificate Authority (acm-pca) SDK becomes indispensable. In this article, we'll explore the fundamentals of certificates, the significance of certificate authorities, and the practical steps involved in setting up a secure infrastructure.What are Certificates?Certificates are digital documents used to establish trust between parties in a communication exchange. These certificates contain vital information like the identity of the certificate holder, public keys, and cryptographic signatures.What is the usefulness of certificates?Certificates serve various purposes in ensuring the security of online transactions and communications, including:Authentication: Certificates verify the identity of parties involved in a transaction.Encryption: Certificates enable secure transmission of data by encrypting it.Integrity: Certificates ensure the integrity of transmitted data, preventing tampering or unauthorized modifications.Trust: Certificates establish trust between communicating parties, ensuring that sensitive information is shared only with trusted entities.Components of a CertificateA typical certificate comprises several components, including:Subject: The entity to which the certificate is issued.Issuer: The entity that issues the certificate.Public Key: The cryptographic key used for encryption and verification.Signature: A digital signature created by the issuer to validate the certificate's authenticity.Validity Period: The duration for which the certificate remains valid.Extensions: Additional information such as key usage, subject alternative names, etc.Types of CertificatesThere are various types of certificates tailored to specific use cases, including:SSL/TLS Certificates: Used to secure websites and establish encrypted connections.Code Signing Certificates(CSR): Ensures the authenticity and integrity of software.Email Certificates: Secures email communications by encrypting and digitally signing messages.Certificate Authority (CA) certificates: A Certificate Authority (CA) certificate is a digital certificate issued by a trusted Certificate Authority.What is a certificate authority?A Certificate Authority (CA) is a trusted entity responsible for issuing and managing digital certificates. It verifies the identity of entities requesting certificates and signs them to establish their authenticity.What is a root certificate?A Root Certificate is a self-signed certificate at the top of the certificate hierarchy. It represents the highest level of trust in a certificate chain and is used to sign other certificates, including intermediate certificates.What is an intermediate certificate?An Intermediate Certificate is a subordinate certificate issued by a root certificate. It sits between the root certificate and end-entity certificates. Intermediate certificates help in enhancing security by segregating certificate issuance and revocation processes.Why do I need to create an intermediate certificate?Creating an intermediate certificate offers several advantages:Enhanced Security: Intermediate certificates provide an additional layer of security, reducing the risk associated with compromising a root certificate.Scalability: Intermediate certificates allow for better management and delegation of certificate issuance responsibilities, particularly in large-scale environments.Granular Control: By utilizing intermediate certificates, administrators can implement fine-grained access control and policy enforcementCreate a root and intermediate certificate with\u00a0OpenSSLUsing OpenSSL, a widely-used open-source toolkit, one can generate root and intermediate certificates. Below are the OpenSSL commands to accomplish this:# Generate a root private keyopenssl genpkey-algorithmRSA-outroot.key# Generate a root certificate signing requestopenssl req-new-keyroot.key-outroot.csr# Self-sign the root certificateopenssl x509-req-inroot.csr-signkeyroot.key-outroot.crt# Generate an intermediate private keyopenssl genpkey-algorithmRSA-outintermediate.key# Generate an intermediate certificate signing requestopenssl req-new-keyintermediate.key-outintermediate.csr# Sign the intermediate certificate using the root certificateopenssl x509-req-inintermediate.csr-CAroot.crt-CAkeyroot.key-set-serial01-outintermediate.crtEnter fullscreen modeExit fullscreen modeCreate a root and intermediate certificate with\u00a0acm-pcaAWS Private CA enables the creation of private certificate authority (CA) hierarchies, including root and intermediate/subordinate CAs.Using the AWS Certificate Manager Private Certificate Authority (acm-pca) SDK, you can automate the process of creating root and intermediate certificates. Here's how to achieve it using TypeScript:import{ACMPCAClient,IssueCertificateCommand,CreateCertificateAuthorityCommand}from'@aws-sdk/client-acm-pca';constclient=newACMPCAClient({region:'us-east-1'});// Create Root CAconstrootCommand=newCreateCertificateAuthorityCommand({CertificateAuthorityType:\"ROOT\",// Specifies that this CA is a root CA  KeyAlgorithm: 'RSA_2048',CertificateAuthorityConfiguration:{Subject:{Country:\"US\",Organization:\"Example Corp\",OrganizationalUnit:\"IT\",State:\"California\",Locality:\"San Francisco\",CommonName:\"Root CA\",SerialNumber:\"202401\",},SigningAlgorithm:\"SHA256WITHRSA\",KeyAlgorithm:\"RSA_2048\",},});constrootResponse=awaitclient.send(rootCommand);constrootArn=rootResponse.CertificateAuthorityArn;// Create Intermediate CAconstintermediateCommand=newIssueCertificateCommand({CertificateAuthorityArn:rootArn,Csr:newUint8Array(Buffer.from(csrPem)),SigningAlgorithm:\"SHA256WITHRSA\",TemplateArn:\"arn:aws:acm-pca:::template/SubordinateCACertificate_PathLen0/V1\",Validity:{Value:365,Type:\"DAYS\",},});constintermediateResponse=awaitclient.send(intermediateCommand);Enter fullscreen modeExit fullscreen modeAWS Private CA TemplatesAWS Private CA offers predefined templates to streamline certificate issuance for various use cases. These templates encapsulate best practices and simplify the process of generating certificates for specific scenarios.End Entity Certificate Template: For issuing certificates directly to end entities such as servers, clients, or IoT devices.Subordinate CA Certificate Template: Simplifies the creation of intermediate CAs for delegating certificate issuance authority.Root CA Certificate Template: Facilitates the creation of self-signed root certificates for establishing trust within the PKI.Read more onAWS TemplatesIn conclusion, the AWS Private CA SDK provides a powerful toolkit for managing digital certificates, allowing organizations to establish robust security postures and ensure the integrity and confidentiality of their data. By leveraging AWS Private CA, developers can automate certificate management processes and focus on building secure and scalable applications."}
{"title": "VPC Flow Logs", "published_at": 1713898914, "tags": ["network", "aws", "devops", "cloud"], "user": "Wojciech Lepczy\u0144ski", "url": "https://dev.to/aws-builders/vpc-flow-logs-46k9", "details": "Hi there! I wanted to share some valuable insights with you about AWS Flow Logs, which can be an invaluable tool in working with AWS cloud.What are AWS Flow Logs?AWS Flow Logs is a mechanism that allows users to monitor network traffic within the AWS infrastructure. It's somewhat like a \"black box\" for the cloud - it records where the traffic is coming from, where it's going, and how much data it's transmitting. Flow log data can be published to the following locations: Amazon CloudWatch Logs, Amazon S3, or Amazon Data FirehoseKey Benefits:Easier Troubleshooting: When something isn't working as it should, Flow Logs help you find the root cause of the problem. Sometimes, just a glance at the network traffic data is enough to find the source of the issue.Enhanced Security: With Flow Logs, you can quickly detect and analyze suspicious or unauthorized network activities.Performance Optimization: Understanding the dynamics of network traffic is key to optimizing the performance of applications and services in the cloud. With Flow Logs, you can identify \"bottlenecks\" in the infrastructure and optimize its performance.Flow log recordsThe flow log records represent network flows within your VPC. By default, each record captures an IP traffic flow. These records are formatted as strings with fields separated by spaces, containing for example information such as the source, destination, port and protocol of the flow. When setting up a flow log, you have the option to use the default format or specify a custom one.How to Start Using AWS Flow Logs?The best part is that you can enable Flow Logs yourself, with minimal effort. Just use the AWS console or command-line tools to activate this feature at the VPC, subnet, or network interface level.You can watch myvideo tutorial on YouTube, which shows step by step what exactly needs to be done. Which IAM policy to choose, what IAM role. How to create a Log group in CloudWatch and how to save logs from network interfaces there. I walk you through how to use AWS Flow Logs and interpret the collected data step by step. It's a great way to deepen your knowledge of the AWS cloud.Documentation:https://docs.aws.amazon.com/vpc/latest/userguide/flow-logs.html"}
{"title": "How To Manage Amazon GuardDuty in AWS Organizations Using Terraform", "published_at": 1713892500, "tags": ["aws", "terraform", "security"], "user": "Anthony Wat", "url": "https://dev.to/aws-builders/how-to-manage-amazon-guardduty-in-aws-organizations-using-terraform-3pg6", "details": "IntroductionSince I released the blog seriesHow to implement the AWS Startup Security Baseline (SSB) using Terraformrecently, I've received some feedback and questions on it. In particular, there were some questions around setting up GuardDuty in an organization using Terraform. Since the configuration involves multiple accounts and there are some quirks with the resources, I decided to write a separate blog post on how to properly implement it with explanation on each step.About the use caseAmazon GuardDutyis a managed threat detection service that continuously monitors AWS accounts and workloads for malicious or unauthorized activity using machine learning, anomaly detection, and integrated threat intelligence.GuardDuty supportsmanaging multiple accounts with AWS Organizationsvia the delegated administrator feature, with which you designate an AWS account in the organization to centrally manage GuardDuty for all members. This is great for managing a multi-account landing zone by centralizing management of GuardDuty settings in a consistent manner.Since it is increasingly common to establish an AWS landing zone usingAWS Control Tower, we will use thestandard account structurein a Control Tower landing zone to demonstrate how to configure GuardDuty in Terraform:The relevant accounts for our use case in the landing zone are:TheManagementaccount for the organization where AWS Organizations is configured. For details, refer toManaging GuardDuty accounts with AWS Organizations.TheAuditaccount where security and compliance services are typically centralized in a Control Tower landing zone.The objective is to delegate GuardDuty administrative duties from theManagementaccount to theAuditaccount, after which all organization configurations are managed in theAuditaccount. With that said, let's see how we can achieve this using Terraform!Designating a GuardDuty administrator accountGuardDuty delegated administrator is configured in theManagementaccount, so we need a provider associated with it in Terraform. To keep things simple, we will take a multi-provider approach by defining two providers, one for theManagementaccount and another for theAuditaccount, using AWS CLI profiles as follows:provider\"aws\"{alias=\"management\"# Use \"aws configure\" to create the \"management\" profile with the Management account credentialsprofile=\"management\"}provider\"aws\"{alias=\"audit\"# Use \"aws configure\" to create the \"audit\" profile with the Audit account credentialsprofile=\"audit\"}Enter fullscreen modeExit fullscreen mode\u26a0 Since GuardDuty is a regional service, you must apply this Terraform configuration on each region that you are using. Consider using theregionargument in your provider definition and a variable to make your Terraform configuration rerunnable in other regions.We can then use theaws_guardduty_organization_admin_accountresourceto set the delegated administrator. However, I noticed the following in theAuditaccount:After this resource is created, GuardDuty will be enabled with both the foundational data sources and all protection plans enabled.When the resource is deleted, GuardDuty remains enabled.These side effects are not desirable since we would ideally want full control over the lifecycle and configuration of GuardDuty in Terraform. To address this issue, we will preemptively enable GuardDuty in theAuditaccount using theaws_guardduty_detectorresource. We will also manage the protection plans using theaws_guardduty_detector_featureresourcein subsequent steps after we define the org-wide settings.The resulting Terraform configuration should be defined as follows (pay special attention to theproviderargument in each resource):data\"aws_caller_identity\"\"audit\"{provider=aws.audit}resource\"aws_guardduty_detector\"\"audit\"{provider=aws.audit}resource\"aws_guardduty_organization_admin_account\"\"this\"{provider=aws.managementadmin_account_id=data.aws_caller_identity.audit.account_iddepends_on=[aws_guardduty_detector.audit]}Enter fullscreen modeExit fullscreen modeWith theAuditaccount designated as the GuardDuty administrator, we can now manage the organization configuration.Configuring organization auto-enable preferencesGuardDuty distinguishes the foundational data sources settings from the protection plans settings. The former is managed using theaws_guardduty_organization_configurationresource. In our case, we want to manage GuardDuty for all accounts (i.e. both new and existing accounts). The resulting Terraform configuration should thus look like the following:resource\"aws_guardduty_organization_configuration\"\"this\"{provider=aws.auditauto_enable_organization_members=\"ALL\"detector_id=aws_guardduty_detector.audit.iddepends_on=[aws_guardduty_organization_admin_account.this]}Enter fullscreen modeExit fullscreen modeNext, let's manage the protection plan configuration. For illustration, let's assume that we only want to enable onlyEKS Audit Log Monitoring. To ensure full configurability, we will define the setting for all protection plans using a variable:# Terraform configuration (.tf)variable\"guardduty_features\"{description=\"An object map that defines the GuardDuty organization configuration.\"type=map(object({auto_enable=stringname=stringadditional_configuration=optional(list(object({auto_enable=stringname=string})))}))}Enter fullscreen modeExit fullscreen mode# Variable definition (.tfvars)guardduty_features={s3={auto_enable=\"NONE\"name=\"S3_DATA_EVENTS\"}eks={auto_enable=\"ALL\"name=\"EKS_AUDIT_LOGS\"}eks_runtime_monitoring={# EKS_RUNTIME_MONITORING is deprecated and should thus be explicitly disabledauto_enable=\"NONE\"name=\"EKS_RUNTIME_MONITORING\"additional_configuration=[{auto_enable=\"NONE\"name=\"EKS_ADDON_MANAGEMENT\"},]}runtime_monitoring={auto_enable=\"NONE\"name=\"RUNTIME_MONITORING\"additional_configuration=[{auto_enable=\"NONE\"name=\"EKS_ADDON_MANAGEMENT\"},{auto_enable=\"NONE\"name=\"ECS_FARGATE_AGENT_MANAGEMENT\"},{auto_enable=\"NONE\"name=\"EC2_AGENT_MANAGEMENT\"}]}malware={auto_enable=\"NONE\"name=\"EBS_MALWARE_PROTECTION\"}rds={auto_enable=\"NONE\"name=\"RDS_LOGIN_EVENTS\"}lambda={auto_enable=\"NONE\"name=\"LAMBDA_NETWORK_LOGS\"}}Enter fullscreen modeExit fullscreen mode\u26a0 TheEKS_RUNTIME_MONITORINGfeature has been superseded by theRUNTIME_MONITORINGfeature, but to avoid perpetual differences in Terraform configuration, we must set its enablement state toNONE.We can then use this variable with thefor_eachmeta-argumentwiththeaws_guardduty_organization_configuration_featureresourceas follows:resource\"aws_guardduty_organization_configuration_feature\"\"this\"{provider=aws.auditfor_each=var.guardduty_featuresauto_enable=each.value.auto_enabledetector_id=aws_guardduty_detector.audit.idname=each.value.namedynamic\"additional_configuration\"{for_each=try(each.value.additional_configuration,[])content{auto_enable=additional_configuration.value.auto_enablename=additional_configuration.value.name}}depends_on=[aws_guardduty_organization_admin_account.this]}Enter fullscreen modeExit fullscreen modeLastly, we will circle back to re-celebrating the protection plan settings for theAuditaccount itself. Let's piggyback on the same variable and usetheaws_guardduty_detector_featureresourceto achieve this:resource\"aws_guardduty_detector_feature\"\"audit\"{provider=aws.auditfor_each=var.guardduty_featuresdetector_id=aws_guardduty_detector.audit.idname=each.value.namestatus=each.value.auto_enable==\"NONE\"?\"DISABLED\":\"ENABLED\"dynamic\"additional_configuration\"{for_each=try(each.value.additional_configuration,[])content{status=additional_configuration.value.auto_enable==\"NONE\"?\"DISABLED\":\"ENABLED\"name=additional_configuration.value.name}}}Enter fullscreen modeExit fullscreen mode\u2705 You can find the complete Terraform in theGitHub repositorythat accompanies this blog post.With the complete Terraform configuration, you can now apply it to establish theAuditaccount as the delegated administrator and apply organization settings to all accounts in the target region. Note that it willtake up to 24 hoursfor GuardDuty to automatically enable it in all accounts. YMMV, but it took about 3 hours in the evening in the Eastern time zone.\u26a0 There is currently anissuewhere theadditional_configurationblock order causes differences when applying the Terraform configuration without making any changes.Caveats about suspending GuardDuty in member accountsDue to limitations with the GuardDuty Terraform resources, GuardDuty is unfortunately not automatically disabled when you runterraform destroy. Normally this wouldn't be a problem for a production landing zone. However, if you are only testing, this could lead to unexpected costs especially when GuardDuty is a somewhat costly service.As a workaround, I would recommend using the AWS CLI or AWS SDK to at least suspend GuardDuty for all members using theStopMonitoringMembersAPI. For your convenience, you can use the following shell script to do so before runningterraform destroy:#!/bin/bash# Note: Make sure that you set the AWS_PROFILE environment variable to \"audit\" before running the script# Get the GuardDuty detector IDDETECTOR_ID=$(aws guardduty list-detectors--queryDetectorIds[0]--outputtext)# Disable auto-enable organization membersaws guardduty update-organization-configuration--detector-id$DETECTOR_ID--auto-enable-organization-memberNONE# Loop through each member account and disable GuardDutyMEMBER_ACCOUNTS=$(aws guardduty list-members--detector-id$DETECTOR_ID--queryMembers[*].AccountId--outputtext)forMEMBER_ACCOUNTin$MEMBER_ACCOUNTSdoecho\"Suspending GuardDuty for account$MEMBER_ACCOUNT\"aws guardduty stop-monitoring-members--account-ids$MEMBER_ACCOUNT--detector-id$DETECTOR_IDdoneEnter fullscreen modeExit fullscreen modeSummaryIn this blog post, you learned how to manage Amazon GuardDuty in AWS Organizations using Terraform. While there are some caveats, this allows you to streamline the setup of a security baseline for your AWS landing zone. The centralized approach to detective security can help you ensure compliance and timely reaction to security incidents.I hope you found this blog post helpful. If you are interested in this type of content, be sure to check out other blog posts in theAvangards Blog. Thank you and have a great one!"}
{"title": "AWS Guard Duty", "published_at": 1713880280, "tags": ["aws", "ids", "security"], "user": "Manu Muraleedharan", "url": "https://dev.to/aws-builders/aws-guard-duty-1bkh", "details": "Guard DutyFirst off, a simple definition:GuardDuty is a guard that will stand in front of your workload and continuously let you know of any threats that are coming to your workload.Now the real definition:Amazon GuardDuty offers threat detection enabling you to continuously monitor and protect your AWS accounts, workloads, and data stored in Amazon Simple Storage Service (Amazon S3). GuardDuty analyzes continuous metadata streams generated from your account and network activity found in AWS CloudTrail Events, Amazon Virtual Private Cloud (VPC) Flow Logs, and domain name system (DNS) Logs. GuardDuty also uses integrated threat intelligence such as known malicious IP addresses, anomaly detection, and machine learning (ML) to more accurately identify threats.Features of Guard DutyAmazon S3 protection- Monitor object-level suspicious activityEKS Protection- Monitor suspicious activities on EKS clustersRuntime Monitoring- Using an agent, monitor suspicious activities on ECS(Fargate), EKS, EC2Malware Protection- Scan EBS volumes for malwareRDS Protection- scans login activity on Aurora RDSLambda Protection- scans network traffic from Lambda executionSuppression Rules- Selectively suppress some findings to automatically archive findings which are low-value, false positive etc, to reduce the noise.Threat list- This is a list of known malicious IPs. This could be in many formats including industry-standard formats like STIX, OTX or even plaintext. Lists could be stored at an accessible internet URI, including your own S3 bucket.Trusted List- stores known trusted IPs with the same storage and format characteristicsFindings- You can drill down into the findings, and get more information about the incident including the target of the attack, the actor of the attack etc. It also provides a link to pivot to detective and investigate this incident.Demo1.Malicious IP accessWe create a text file with a known IP in it, say 8.8.8.8 and upload it to an S3 bucket. Specify this file as a Threat List inside GuardDuty.From an EC2 inside your account, ping this IP.ping 8.8.8.8Soon, GuardDuty finds this and you can see it in the console.2.Instance Credential ExfiltrationWe know an EC2 instance could have an IAM Role (Instance Profile) which gives it access to AWS API calls as per the role permissions. We can simulate the scenario where a hacker has got access to the EC2 and is using these credentials to call AWS APIs.Login to the EC2 and get the IAM credentials.This could depend on the Instance Metadata Service Version of the EC2.See this page for details:https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/iam-roles-for-amazon-ec2.html#instance-metadata-security-credentialsFor me, I am on v2. My Ec2-instance has the role ec2-adminTOKEN=`curl -X PUT \"http://169.254.169.254/latest/api/token\" -H \"X-aws-ec2-metadata-token-ttl-seconds: 21600\"` \\ && curl -H \"X-aws-ec2-metadata-token: $TOKEN\" http://169.254.169.254/latest/meta-data/iam/security-credentials/ec2-adminEnter fullscreen modeExit fullscreen modeFrom the output of this command note the Access Key ID, Secret Access Key and Session Token.Now replicate same session in any other system terminal where you have AWS CLI installed. Below commands can be used for that. It creates a profile called badbob (BAD BOB!) who is the hacker.aws configure set profile.badbob.region us-east-1  aws configure set profile.badbob.aws_access_key_id <AccessKeyId>  aws configure set profile.badbob.aws_secret_access_key <SecretAccessKey>  aws configure set profile.badbob.aws_session_token <Token>Enter fullscreen modeExit fullscreen modeNow using the session, issue several AWS API Calls. An example is below. Remember the hacker does not know the permissions on the role, so he may try many commands across the spectrum, so try a whole lot of options.aws s3 ls --profile badbobYou can see GuardDuty finds this suspicious activity and reports it.Note that in both cases, GuardDuty gives a whole lot of background information about the finding that helps the security team investigate this finding.This includes an overview, resources involved in the finding, IAM details, Network details, the action in the finding, actor involved in the finding etc.If you have enabled another AWS tool, AWS Detective at least 48 hours before you enabled GuardDuty, you would also see the option to investigate this finding in Detective.We will continue this discussion with an article onAWS Detective."}
{"title": "Deploy App on AWS ECS Fargate using Github Actions", "published_at": 1713855722, "tags": ["ecs", "githubactions", "devops", "fargate"], "user": "Olawale Adepoju", "url": "https://dev.to/aws-builders/deploy-app-on-aws-ecs-fargate-using-github-actions-13mf", "details": "In this blog, i will show how to deploy an application on Amazon ECS using the Fargate for efficient containerized deployment and also Github actions will be used for the CI/CD.Step 1: Create your repository on ECR( Amazon Elastic Container Registry)Step 2: Create cluster in ECSStep 3: Create a Task DefinitionYou can create a task role if you don't have existing role created. These are the two policies required for the roleFor the container, give a name, and copy the image URL created in the ECR previously.Add the port in which your application is accessed.Step 4: Create a ServiceNote:After creating this service it will fail because there is no image pushed to the ECR yet.Click on the cluster created, and create service.Step 5: While service is creating, configure your application github workflow. The application is packaged using Docker.Hence the dockerfileFROM node:16.20.1 WORKDIR /app COPY package.json ./ RUN npm install COPY . . EXPOSE 5000 CMD [\"npm\",\"run\",\"start\"]Enter fullscreen modeExit fullscreen modegithub workflow is as belowname: CICD  on:   push:     branches: [ main ]  jobs:   build-and-deploy:     runs-on: [ ubuntu-latest ]     steps:       - name: Checkout source         uses: actions/checkout@v3       - name: Configure AWS credentials         uses: aws-actions/configure-aws-credentials@v4         with:           aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY }}           aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}           aws-region: us-east-1       - name: Login to Amazon ECR         id: login-ecr         uses: aws-actions/amazon-ecr-login@v2         with:           mask-password: 'true'        - name: Build, tag, and push image to Amazon ECR         id: build-image         env:           ECR_REGISTRY: ${{ steps.login-ecr.outputs.registry }}           IMAGE_TAG: latest           REPOSITORY: nodeapp         run: |           # Build a docker container and           # push it to ECR so that it can           # be deployed to ECS.           docker build -t $ECR_REGISTRY/$REPOSITORY:$IMAGE_TAG .           docker push $ECR_REGISTRY/$REPOSITORY:$IMAGE_TAG           echo \"image=$ECR_REGISTRY/$REPOSITORY:$IMAGE_TAG\" >> $GITHUB_OUTPUT            - name: Fill in the new image ID in the Amazon ECS task definition         id: task-def         uses: aws-actions/amazon-ecs-render-task-definition@v1         with:           task-definition: nodejs-app-task-definition.json            container-name: nodejs-app           image: ${{ steps.build-image.outputs.image }}           - name: Deploy Amazon ECS task definition         uses: aws-actions/amazon-ecs-deploy-task-definition@v1         with:           task-definition: ${{ steps.task-def.outputs.task-definition }}           service: nodejs-app-service           cluster: DevCluster           wait-for-service-stability: trueEnter fullscreen modeExit fullscreen modeNote:Create the task definition file(nodejs-app-task-definition.json) in the directory of your project. Copy the json file and paste in the file created.The environment variables are set in github settingsAfter the image is built and pushed to ECR, the service becomes active and running.Click on the task to see the configuration to see the public ip address."}
{"title": "Introduction to SSO with the IAM Identity Center and Entra ID", "published_at": 1713853248, "tags": ["aws", "cloud", "identity", "sso"], "user": "Maurice Borgmeier", "url": "https://dev.to/aws-builders/introduction-to-sso-with-the-iam-identity-center-and-entra-id-2b4j", "details": "IAM Identity Center, formerly AWS SSO, is often used as an access management solution in front of one or more AWS accounts. More often than not, its purpose is to grant access to AWS accounts within an AWS organization. Today we'll shed some light on the basic concepts and explain how this solution can be integrated with Azure AD which has recently been renamed to Entra ID to provide Single-Sign-On to your AWS environment.As a user, your goal is to log into your AWS environment. This can be managed via Identity and Access Management for a single AWS account, but the real world is often more complicated. Managing access to sprawling AWS organizations is a challenge on its own, and this is where the IAM Identity Center helps you. It can integrate with a 3rd party identity provider such as your Entra ID and provide granular access to users and groups stored in that identity provider.\u00a0You can think of Identity Center as a way to map AD users and groups to IAM Roles, just\u00a0with extra steps.\u00a0Let's dive in.To log into your AWS account, you first need to be authenticated, and in the subsequent authorization step, the\u00a0Identity Center determines which roles you're allowed to assume in what AWS accounts. For this to work, the\u00a0Identity Center\u00a0needs to\u00a0be set up in an account. Typically, it's associated with an AWS organization, but more on that later.\u00a0In order for\u00a0it to authenticate and authorize users, it needs to know about users\u00a0and this\u00a0is where the Identity Provider (IDP) enters the scene.Identity Center has a built-in IDP, but you probably want to use your existing Entra ID to log in to the environment. That means we need to set up a connection, a mutual trust, between the Identity Center and Entra ID. This is commonly done through an Enterprise Application in Entra ID. The enterprise application gets assigned a subset of users and groups allowed to access the application. It can also be used to control which user attributes are available to the app.Next, we built\u00a0up\u00a0a trust relationship between the Identity Center and the Enterprise App by configuring the SAML protocol. The Identity Center can use this to authenticate users in Entra ID. If we leave it at that, authenticated users will be added to the internal Identity Store of Entra ID once they log in. This puts us in the awkward spot of\u00a0having to wait\u00a0for users to do something. Instead of doing that, we also configure SCIM 2.0 (System for Cross-domain Identity Management), a protocol that allows the enterprise app to proactively push all users and groups assigned to it to the Identity Center Identity Store. This protocol will be used to add, update, or remove users if they change on the Entra ID side of things. You get the option of choosing to sync only selected or all users and groups to the Identity Center.The identity store maintains a reference to the original users and groups in Entra ID, an external ID combined with an external issuer. This allows it to track which Entra ID tenant a user or group originates from. Now, we're at the point where the Identity Center can authenticate users and already knows which users and groups have permission to access the Identity Center. This means we can now deal with the other problem:authorization.To understand how the users and groups are now mapped to IAM roles, we have to talk about Permission Sets. You can think of a permission set like a template for an IAM role; it can have managed policies assigned to it, permissions boundaries, references to named policies, or inline policies.\u00a0By itself,\u00a0a permission set is harmless; it only starts doing something once an Assignment is added.The assignment couples a permission set with either a user or a group and an AWS Account. Once that's done, the Identity Center will provision an IAM role with the policies outlined in the permission set in the assigned account. You can spot these roles by theAWSReservedSSO_prefix in their name. That's basically all you need to do. Once you log in with your Entra ID account, you'll get a menu like this to pick the AWS Account + role to connect to.I should note that there's limited support for managing these resources through the SDK and CloudFormation at the time of writing this; aside from\u00a0actually\u00a0setting up and describing the integrations, you can do almost anything with the current SDK.\u00a0For other things,\u00a0you may have to rely onundocumented API'sfor now.Regarding Infrastructure as Code (IaC), the resource coverage in CloudFormation is very limited. You can only manage Permission Sets and Assignments, and the latter becomes kind of a hassle in larger environments as you have to manage a lot of resources, and all user/group identifiers are UUIDs that are not very human-compatible.(Note that these UUIDs arenotthe same as the Object IDs in Entra ID, but those are used as external IDs on the user and group resources.)The pragmatic approach to IaC is to create the Permission Sets\u00a0that way because those\u00a0are relatively static and use well-understood resources such as different IAM policies that lend themselves to be versioned. Managing the assignments should be done another way. No one\u00a0is going to\u00a0thank you for making them edit UUIDs in a CloudFormation template all day when the UI makes this a lot easier.This has been my brief introduction to\u00a0the IAM Identity Center and its SSO with Entra ID.\u00a0Hopefully, these concepts provide\u00a0a good\u00a0framework\u00a0to dive deeper into the respective\u00a0documentation.\u00a0If you'd like to learn more about best practices or\u00a0would like some\u00a0support implementing this in your own environment, feel free toget in touch with us.\u2014 MauriceTitle Photo byMicah Williamson Unsplash"}
{"title": "Detecting and redacting PII using Amazon Bedrock", "published_at": 1713846745, "tags": ["tutorial", "aws", "ai", "python"], "user": "Thomas Taylor", "url": "https://dev.to/aws-builders/detecting-and-redacting-pii-using-amazon-bedrock-1jol", "details": "Typically, AWS recommendsleveraging an existing service offering such as Amazon Comprehendto detect and redact PII. However, this post explores an alternative solution using Amazon Bedrock.This is possible using the Claude, Anthropic's large langauge model, andtheir publicly available prompt library. In our case, we'llleverage the PII purifier promptthat is maintained by their prompt engineers.How to extract PII using Amazon Bedrock in PythonThis demo showcases how to invoke the Amazon Claude 3 models using Python; however, any language and their respective Amazon SDK will suffice.Install boto3Firstly, let's install the AWS Python SDK,boto3.pipinstallboto3Enter fullscreen modeExit fullscreen modeInstantiate a clientEnsure that your environment is authenticated with AWS credentials using any of themethods described in their documentation.Instantiate the bedrock runtime client like so:importboto3bedrock_runtime=boto3.client(\"bedrock-runtime\")Enter fullscreen modeExit fullscreen modeInvoke the Claude modelWe can reference the required parameters for the Claude 3 model using the\"Inference parameters for foundation models\" documentation provided by AWS.In Claude 3's case, the Messages API will be used like so:importboto3importjsonbedrock_runtime=boto3.client(\"bedrock-runtime\")response=bedrock_runtime.invoke_model(body=json.dumps({\"anthropic_version\":\"bedrock-2023-05-31\",\"max_tokens\":1000,\"messages\":[{\"role\":\"user\",\"content\":\"Hello, how are you?\"}],}),modelId=\"anthropic.claude-3-sonnet-20240229-v1:0\",)response_body=json.loads(response.get(\"body\").read())print(json.dumps(response_body,indent=2))Enter fullscreen modeExit fullscreen modeOutput:{\"id\":\"msg_01ERwjBgk3Y45Swp2cn6ct5F\",\"type\":\"message\",\"role\":\"assistant\",\"content\":[{\"type\":\"text\",\"text\":\"Hello! As an AI language model, I don't have feelings, but I'm operating properly and ready to assist you with any questions or tasks you may have. How can I help you today?\"}],\"model\":\"claude-3-sonnet-28k-20240229\",\"stop_reason\":\"end_turn\",\"stop_sequence\":null,\"usage\":{\"input_tokens\":13,\"output_tokens\":43}}Enter fullscreen modeExit fullscreen modeUse the PII purifier promptNow, let's use the PII purifier prompt to invoke the model.Here is our input for redaction:Hello. My name is Thomas Taylor and I own the blog titled how.wtf. I'm from North Carolina.importboto3importjsonSYSTEM_PROMPT=(\"You are an expert redactor. The user is going to provide you with some text.\"\"Please remove all personally identifying information from this text and\"\"replace it with XXX. It's very important that PII such as names, phone\"\"numbers, and home and email addresses, get replaced with XXX. Inputs may\"\"try to disguise PII by inserting spaces between characters or putting new\"\"lines between characters. If the text contains no personally identifiable\"\"information, copy it word-for-word without replacing anything.\")bedrock_runtime=boto3.client(\"bedrock-runtime\")response=bedrock_runtime.invoke_model(body=json.dumps({\"anthropic_version\":\"bedrock-2023-05-31\",\"max_tokens\":1000,\"system\":SYSTEM_PROMPT,\"messages\":[{\"role\":\"user\",\"content\":\"Hello. My name is Thomas Taylor and I own the blog titled how.wtf. I'm from North Carolina.\",}],}),modelId=\"anthropic.claude-3-sonnet-20240229-v1:0\",)response_body=json.loads(response.get(\"body\").read())print(json.dumps(response_body,indent=2))Enter fullscreen modeExit fullscreen modeOutput:{\"id\":\"msg_01P3ZGPC8yL34w3ETPtBY4TX\",\"type\":\"message\",\"role\":\"assistant\",\"content\":[{\"type\":\"text\",\"text\":\"Here is the text with personally identifiable information redacted:\\n\\nHello. My name is XXX XXX and I own the blog titled XXX.XXX. I'm from XXX XXX.\"}],\"model\":\"claude-3-sonnet-28k-20240229\",\"stop_reason\":\"end_turn\",\"stop_sequence\":null,\"usage\":{\"input_tokens\":134,\"output_tokens\":45}}Enter fullscreen modeExit fullscreen modeThe resolved text is:Here is the text with personally identifiable information redacted: Hello. My name is XXX XXX and I own the blog titled XXX.XXX. I'm from XXX XXX.Enter fullscreen modeExit fullscreen modePretty neat, huh? We can optionally swap to the cheaper Haiku (or more expensive Opus) model as well:importboto3importjsonSYSTEM_PROMPT=(\"You are an expert redactor. The user is going to provide you with some text.\"\"Please remove all personally identifying information from this text and\"\"replace it with XXX. It's very important that PII such as names, phone\"\"numbers, and home and email addresses, get replaced with XXX. Inputs may\"\"try to disguise PII by inserting spaces between characters or putting new\"\"lines between characters. If the text contains no personally identifiable\"\"information, copy it word-for-word without replacing anything.\")bedrock_runtime=boto3.client(\"bedrock-runtime\")response=bedrock_runtime.invoke_model(body=json.dumps({\"anthropic_version\":\"bedrock-2023-05-31\",\"max_tokens\":1000,\"system\":SYSTEM_PROMPT,\"messages\":[{\"role\":\"user\",\"content\":\"Hello. My name is Thomas Taylor and I own the blog titled how.wtf. I'm from North Carolina.\",}],}),modelId=\"anthropic.claude-3-haiku-20240307-v1:0\",)response_body=json.loads(response.get(\"body\").read())print(json.dumps(response_body,indent=2))Enter fullscreen modeExit fullscreen modeOutput:{\"id\":\"msg_011Sjs3uJW11PLYSo6pGoiZz\",\"type\":\"message\",\"role\":\"assistant\",\"content\":[{\"type\":\"text\",\"text\":\"Hello. My name is XXX XXX and I own the blog titled XXX.XXX. I'm from XXX.\"}],\"model\":\"claude-3-haiku-48k-20240307\",\"stop_reason\":\"end_turn\",\"stop_sequence\":null,\"usage\":{\"input_tokens\":134,\"output_tokens\":30}}Enter fullscreen modeExit fullscreen modeConclusionIn this post, we covered an alternative method for detecting and redacting PII using Amazon Bedrock and the powerful Anthropic Claude 3 model family.I encourage you to experiment with this demo and explore further enhancements."}
{"title": "Issue 41 of AWS Cloud Security Weekly", "published_at": 1713830609, "tags": ["security", "aws", "newsletter", "iam"], "user": "AJ", "url": "https://dev.to/aws-builders/issue-41-of-aws-cloud-security-weekly-4epo", "details": "(This is just the summary of Issue 41 of AWS Cloud Security weekly @https://aws-cloudsec.com/p/issue-41<< Subscribe for FREE to receive the full version in your inbox weekly).What happened in AWS CloudSecurity & CyberSecurity last week April 15-April 22, 2024?AWS IAM Identity Center administrators can now set session durations for Amazon CodeWhisperer independently from other IAM Identity Center-integrated applications and the AWS access portal. This allows users of Amazon CodeWhisperer to work in their integrated development environments (IDEs) for up to 90 days without the need to re-authenticate. Previously, the session durations for CodeWhisperer in the IDE had to match those of other IAM Identity Center-integrated applications and the AWS access portal, typically ranging from 15 minutes to 90 days.AWS Identity and Access Management (IAM) Roles Anywhere now lets you set up mapping rules to define which information is extracted from your X.509 end-entity certificates. These mapped details, known as attributes, are used as session tags in IAM policy conditions to allow or deny permissions. Attributes can be extracted from the subject, issuer, or subject alternative name (SAN) fields in the X.509 certificate. By default, all relative distinguished names (RDNs) from the certificate's subject and issuer are mapped, along with the first value of the domain name system (DNS), directory name (DN), and uniform resource identifier (URI) from the certificate's SAN. This new feature allows you to create a custom set of mapping rules and select only a subset of these certificate attributes that suit your business needs. This customization reduces the size and complexity of the tags used in authorization policies. The mapped attributes are linked to your profile. You can define these mapping rules using the put-attribute-mapping or delete-attribute-mapping APIs via the IAM Roles Anywhere console, AWS SDKs, or AWS CLI.Trending on the news & advisories (Subscribe to the newsletter for details):SEC Consult SA-20240411-0 :: Database Passwords in Server Response in Amazon AWS Glue.PuTTY SSH client flaw allows recovery of cryptographic private keys. Link.Orca- LeakyCLI: AWS and Google Cloud Command-Line Tools Can Expose Sensitive Credentials in Build Logs.Lacework, last valued at $8.3B, is in talks to sell for just $150M to $200M, say sources.UnitedHealth to take up to $1.6 billion hit this year from Change hack.MITRE Response to Cyber Attack in One of Its R&D Networks.CISA Announces Winners of the 5th Annual President\u2019s Cup Cybersecurity Competition. Link."}
{"title": "Containers on AWS: Comparing ECS and EKS", "published_at": 1713828447, "tags": ["aws", "containers", "ecs", "kubernetes"], "user": "Guille Ojeda", "url": "https://dev.to/aws-builders/containers-on-aws-comparing-ecs-and-eks-5h4c", "details": "Containers offer a lightweight, portable, and scalable solution for running software consistently across different environments. But as the number of containers grows, managing them becomes increasingly complex. That's where container orchestration comes in.AWS offers two powerful container orchestration services:Amazon Elastic Container Service (ECS)andAmazon Elastic Kubernetes Service (EKS). Both services help you run and scale containerized applications, but they differ in their approach, features, and use cases.In this article, I'll dive deep into the world of containers on AWS. I'll explore the key features and components of ECS and EKS, compare their similarities and differences, and provide guidance on choosing the right service for your needs. By the end, you'll have a solid understanding of how to leverage these services to build and manage containerized applications on AWS effectively.Understanding Amazon ECS (Elastic Container Service)Let's start by looking at Amazon ECS, AWS's fully managed container orchestration service. ECS allows you to run and manage Docker containers at scale without worrying about the underlying infrastructure.ECS Key Features:Fully managed container orchestrationIntegration with other AWS servicesSupport for both EC2 and Fargate launch typesBuilt-in service discovery and load balancingIAM integration for security and access controlECS Components:Clusters: Logical grouping of container instances or Fargate capacityTask Definitions: Blueprints that describe how to run a containerServices: Maintain a specified number of task replicas and handle scalingTasks: Instantiation of a Task Definition, representing a running containerECS Launch Types:ECS supports two launch types for running containers: EC2 and Fargate.EC2: You manage the EC2 instances that make up the ECS cluster. This gives you full control over the infrastructure but requires more management overhead.Fargate: AWS manages the underlying infrastructure, and you only pay for the resources your containers consume. Fargate abstracts away the EC2 instances, making it easier to focus on your applications.Pricing:With ECS, you pay for the AWS resources you use, such as EC2 instances, EBS volumes, and data transfer. Fargate pricing is based on the vCPU and memory resources consumed by your containers.ECS Architecture and ComponentsLet's take a closer look at the key components of ECS and how they work together.ECS ClustersAn ECS cluster is a logical grouping of container instances or Fargate capacity. It provides the infrastructure to run your containers. You can create clusters using the AWS Management Console, AWS CLI, or CloudFormation templates.Task DefinitionsA Task Definition is a JSON file that describes how to run a container. It specifies the container image, CPU and memory requirements, networking settings, and other configuration details. Task Definitions act as blueprints for creating and running tasks.ServicesAn ECS Service maintains a specified number of task replicas and handles scaling. It ensures that the desired number of tasks are running and automatically replaces any failed tasks. Services integrate with ELB for load balancing and service discovery.TasksA Task is an instantiation of a Task Definition, representing a running container. When you create a task, ECS launches the container on a suitable container instance or Fargate capacity, based on the Task Definition and launch type.Example ECS Cluster and Task Definition:{\"cluster\":\"my-cluster\",\"taskDefinition\":\"my-task-definition\",\"desiredCount\":2,\"launchType\":\"FARGATE\",\"networkConfiguration\":{\"awsvpcConfiguration\":{\"subnets\":[\"subnet-12345678\",\"subnet-87654321\"],\"securityGroups\":[\"sg-12345678\"],\"assignPublicIp\":\"ENABLED\"}}}Enter fullscreen modeExit fullscreen modeECS Launch Types: EC2 vs FargateOne key decision when using ECS is choosing between the EC2 and Fargate launch types.EC2 Launch TypeWith the EC2 launch type, you manage the EC2 instances that make up your ECS cluster. This gives you full control over the infrastructure, including instance types, scaling, and networking. However, it also means more management overhead, as you're responsible for patching, scaling, and securing the instances.Use cases for EC2 launch type:Workloads that require specific instance types or configurationsApplications that need to access underlying host resourcesScenarios where you want full control over the infrastructureFargate Launch TypeFargate is a serverless compute engine for containers. It abstracts away the underlying infrastructure, allowing you to focus on your applications. With Fargate, you specify the CPU and memory requirements for your tasks, and ECS manages the rest.Benefits of Fargate:No need to manage EC2 instances or clustersPay only for the resources your containers consumeAutomatic scaling based on task resource requirementsSimplified infrastructure managementExample of running a containerized application using Fargate:Resources:MyFargateService:Type:AWS::ECS::ServiceProperties:Cluster:!RefMyClusterTaskDefinition:!RefMyTaskDefinitionDesiredCount:2LaunchType:FARGATENetworkConfiguration:AwsvpcConfiguration:AssignPublicIp:ENABLEDSubnets:-!RefSubnetA-!RefSubnetBSecurityGroups:-!RefMySecurityGroupEnter fullscreen modeExit fullscreen modeUnderstanding Amazon EKS (Elastic Kubernetes Service)Now let's shift gears and explore Amazon EKS, a managed Kubernetes service that makes it easy to deploy, manage, and scale containerized applications using Kubernetes on AWS.EKS Key Features:Fully managed Kubernetes control planeIntegration with AWS services and Kubernetes community toolsAutomatic provisioning and scaling of worker nodesSupport for both managed and self-managed node groupsBuilt-in security and compliance featuresEKS ArchitectureEKS consists of two main components:EKS Control Plane: The control plane is a managed Kubernetes master that runs in an AWS-managed account. It provides the Kubernetes API server, etcd, and other core components.Worker Nodes: Worker nodes are EC2 instances that run your containers and are registered with the EKS cluster. You can create and manage worker nodes using EKS managed node groups or self-managed worker nodes.Pricing:With EKS, you pay for the AWS resources you use, such as the EKS control plane, EC2 instances for worker nodes, EBS volumes, and data transfer. You also pay a hourly rate for the EKS control plane based on the number of Kubernetes API requests.EKS Architecture and ComponentsLet's dive deeper into the EKS architecture and its key components.EKS Control PlaneThe EKS control plane is a managed Kubernetes master that runs in an AWS-managed account. It provides the following components:Kubernetes API Server: The primary interface for interacting with the Kubernetes clusteretcd: The distributed key-value store used by Kubernetes to store cluster stateScheduler: Responsible for scheduling pods onto worker nodes based on resource requirements and constraintsController Manager: Manages the core control loops in Kubernetes, such as replica sets and deploymentsWorker NodesWorker nodes are EC2 instances that run your containers and are registered with the EKS cluster. Each worker node runs the following components:Kubelet: The primary node agent that communicates with the Kubernetes API server and manages container runtimeContainer Runtime: The runtime environment for running containers, such as Docker or containerdKube-proxy: Maintains network rules and performs connection forwarding for Kubernetes servicesExample EKS Cluster Configuration:apiVersion:eksctl.io/v1alpha5kind:ClusterConfigmetadata:name:my-eks-clusterregion:us-west-2managedNodeGroups:-name:my-node-groupinstanceType:t3.mediumminSize:1maxSize:3desiredCapacity:2Enter fullscreen modeExit fullscreen modeEKS Managed vs Self-Managed Node GroupsEKS provides two options for managing worker nodes: managed node groups and self-managed worker nodes.EKS Managed Node GroupsEKS managed node groups automate the provisioning and lifecycle management of worker nodes. Key features include:Automatic provisioning and scaling of worker nodesIntegration with AWS services like VPC and IAMManaged updates and patching for worker nodesSimplified cluster autoscaler configurationSelf-Managed Worker NodesWith self-managed worker nodes, you have full control over the provisioning and management of worker nodes. This allows for more customization but also requires more effort to set up and maintain.Example of creating an EKS managed node group:eksctl create nodegroup--clustermy-eks-cluster--namemy-node-group--node-typet3.medium--nodes2--nodes-min1--nodes-max3Enter fullscreen modeExit fullscreen modeStop copying cloud solutions, startunderstandingthem. Join over 4000 devs, tech leads, and experts learning how to architect cloud solutions, not pass exams, with theSimple AWS newsletter.ECS vs EKS: Key Differences and Use CasesNow that we've explored the key features and components of ECS and EKS, let's compare them side by side.FeatureECSEKSOrchestrationAWS-native orchestrationKubernetes orchestrationControl PlaneFully managed by AWSManaged Kubernetes control planeInfrastructure ManagementManaged (Fargate) or self-managed (EC2)Managed or self-managed worker nodesEcosystem and ToolingAWS-native tooling and integrationsKubernetes-native tooling and integrationsLearning CurveSimpler, AWS-specific conceptsSteeper, requires Kubernetes knowledgePortabilityTied to AWS ecosystemPortable across Kubernetes-compatible platformsUse cases for ECS:Simpler containerized applicationsWorkloads that heavily utilize AWS servicesTeams more familiar with AWS ecosystemServerless applications using FargateUse cases for EKS:Complex, large-scale containerized applicationsWorkloads that require Kubernetes-specific featuresTeams with Kubernetes expertiseApplications that need to be portable across cloud providersChoosing the Right Container Orchestration ServiceChoosing between ECS and EKS depends on various factors specific to your application and organizational needs.Factors to consider:Application complexity and scalabilityTeam's skills and familiarity with AWS and KubernetesIntegration with existing tools and workflowsLong-term container strategy and portability requirementsWhen to use ECSSimpler applications with a limited number of microservicesWorkloads that primarily use AWS servicesTeams more comfortable with AWS tools and conceptsServerless applications that can benefit from FargateExample: A web application consisting of a frontend service, backend API, and database, all running on ECS with Fargate.When to use EKSComplex applications with a large number of microservicesWorkloads that require Kubernetes-specific features like Custom Resource Definitions (CRDs)Teams with extensive Kubernetes experienceApplications that need to be portable across cloud providersExample: A large-scale machine learning platform running on EKS, leveraging Kubeflow and other Kubernetes-native tools.Best Practices for Container Orchestration on AWSRegardless of whether you choose ECS or EKS, here are some best practices to keep in mind:Use infrastructure as code (IaC) tools like CloudFormation or Terraform to manage your container orchestration resourcesImplement a robust CI/CD pipeline to automate container builds, testing, and deploymentLeverage AWS services like ECR for container image registry and ELB for load balancingUse IAM roles and policies to enforce least privilege access to AWS resourcesMonitor your containerized applications using tools like CloudWatch, Prometheus, or GrafanaOptimize costs by right-sizing your instances, using Spot Instances when appropriate, and leveraging reserved capacityConclusionAWS provides two powerful services for container orchestration: ECS and EKS.ECS is a fully managed service that offers simplicity and deep integration with the AWS ecosystem. It's well-suited for simpler containerized applications and teams more familiar with AWS tools and concepts.On the other hand, EKS is a managed Kubernetes service that provides the full power and flexibility of Kubernetes. It's ideal for complex, large-scale applications and teams with Kubernetes expertise.Ultimately, the choice between ECS and EKS depends on your application requirements, team skills, and long-term container strategy. By understanding the key features, differences, and use cases of each service, you can make an informed decision and build scalable, resilient containerized applications on AWS.Still, I prefer ECS =)Stop copying cloud solutions, startunderstandingthem. Join over 4000 devs, tech leads, and experts learning how to architect cloud solutions, not pass exams, with theSimple AWS newsletter.Realscenarios and solutionsThewhybehind the solutionsBest practicesto improve themSubscribe for freeIf you'd like to know more about me, you can find meon LinkedInor atwww.guilleojeda.com"}
{"title": "Automate Docker image builds and pushes to dual container registries (ECR and Docker Hub) in one go using GitHub Actions", "published_at": 1713806585, "tags": ["ecr", "cicd", "aws", "docker"], "user": "Olumoko Moses ", "url": "https://dev.to/aws-builders/automate-docker-image-builds-and-push-to-dual-container-registries-ecr-and-docker-hub-in-one-push-using-github-actions-2m82", "details": "INTRODUCTIONIn this guide I will be taking you on a ride on how to leverage the power of GitHub actions, to efficiently build and push Docker image to two essential container registries, Amazon ECR and Docker Hub, Plus, we'll tag them based on the GitHub run number and \"Latest\", all in one go, optimizing your workflow for maximum efficiency and reliability.PrerequisiteGit and GitHubDockerDockerHub accountAWS accountA GitHub repository, a Dockerfile, and the project you aim to build and push to the registry.Wondering why we are pushing to dual container registriesand tagging the images twice using the GitHub run number and \"Latest\"?Here you go:1.Dual Image DistributionBy pushing the Docker image to both AWS ECR and Docker Hub simultaneously, you gain redundancy and flexibility. Here's why it's advantageous:High Availability:If one container registry experiences downtime or issues, your application remains accessible through the other.Geo-Distribution:AWS ECR and Docker Hub have different global distributions. Pushing to both ensures that your application is available to users worldwide with lower latency.Vendor Independence:Diversifying the storage locations mitigates the risk of vendor-specific limitations or changes affecting your deployments.2.Tagging with \"latest\" and GitHub Run NumberTagging Docker images with \"latest\" and the GitHub run number offers several advantages:Versioning:The \"latest\" tag simplifies the process of deploying the most recent version of your application.Traceability:Each image is tagged with the GitHub run number, making it easy to trace back to the specific commit and build that produced the image. This is invaluable for debugging and auditing purposes.Rollback:In case of issues with a new release, you can easily revert to a previous image tagged with a specific run number.What is GitHub actions?GitHub Actions is a continuous integration and continuous delivery (CI/CD) platform that allows you to automate your build, test, and deployment pipeline.What is DockerHub?Docker Hub is a container registry built for developers and open source contributors to find, use, and share their container images.What is Amazon ecr?Amazon Elastic Container Registry (Amazon ECR) is a fully AWS managed container registry offering high-performance hosting, so you can reliably deploy application images and artifacts anywhereShall we begin? Okay,  Let\u2019s get it done in 7 stepsCreate Dockerhub repositoryCreate a repository on Docker Hub where your docker images will stored:Sign into your docker hub account, navigate to repositories, click on \"create repository\"Enter your preferred repository name, then click on create.There! your repository is ready.Create Amazon ecr repositoryOn your AWS account, search for ecr and create a repository for your docker images to be stored:In the navigation pane, choose Repositories under \"Private registry\".On the Repositories page click on Create repository.Enter the preferred repository name and click on \"Create\".Configure your repository secrets on GitHubClick on the \"settings\" in your repositoryIn the \"Security\" section of the sidebar, select Secrets and - variables, then click Actions.Click the Secrets tab.Click New repository secret.secrets to add: AWS_ACCESS_KEY_ID, AWS_SECRET_ACCESS_KEY, AWS_ACCOUNT_ID, DOCKERHUB_PASSWORD, DOCKERHUB_USERNAME.AWS_ACCESS_KEY_ID and AWS_SECRET_ACCESS_KEY: These are credentials used to authenticate and authorize access to Amazon Web Services (AWS) resources.AWS_ACCOUNT_ID: This is the unique identifier for your AWS account.DOCKERHUB_USERNAME and DOCKERHUB_PASSWORD: These are credentials used to authenticate and authorize access to Docker Hub, a cloud-based repository for storing and sharing containerized applications.Create a workflow fileIn the project directory where your source code and dockerfile resides you need to add the workflow file:Think of GitHub Actions workflows as sets of instructions written in YAML files. To get started, make a new file calledmain.ymlinside the.github/workflowsdirectory of your repository. This file will outline the steps needed to build and push your Docker image.The workflow will be triggered whenever we push to the main branch.Copy the below code and paste in the main.yml file.Note: In the workflow below, make sure to replace every [Repository name] with the actual name of your image repositoryon:   push:     branches:       - main  jobs:   deploy_to_ecr:     name: Deploy to ECR     runs-on: ubuntu-latest      steps:       - name: Check out code now         uses: actions/checkout@v2        - name: Check out code         uses: actions/checkout@v2        - name: Configure AWS credentials         uses: aws-actions/configure-aws-credentials@v1         with:           aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}           aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}           aws-region: us-east-1        - name: Login to Amazon ECR         id: login-ecr         uses: aws-actions/amazon-ecr-login@v1        - name: Build, tag, and push image to Amazon ECR         env:           ECR_REGISTRY: ${{ steps.login-ecr.outputs.registry }}           ECR_REPOSITORY: [Repository name]           IMAGE_TAG: latest         run: |           docker build -t $ECR_REGISTRY/$ECR_REPOSITORY:$IMAGE_TAG .           docker tag $ECR_REGISTRY/$ECR_REPOSITORY:$IMAGE_TAG $ECR_REGISTRY/$ECR_REPOSITORY:${{ github.run_number }}           docker push $ECR_REGISTRY/$ECR_REPOSITORY:$IMAGE_TAG           docker push $ECR_REGISTRY/$ECR_REPOSITORY:${{ github.run_number }}    deploy_to_dockerhub:     name: Deploy to Docker Hub     runs-on: ubuntu-latest      steps:       - name: Check out code         uses: actions/checkout@v2        - name: Login to Docker Hub         uses: docker/login-action@v2         with:           username: ${{ secrets.DOCKERHUB_USERNAME }}           password: ${{ secrets.DOCKERHUB_PASSWORD }}        - name: Build and tag Docker images         run: |           docker build . --file Dockerfile --tag ${{ secrets.DOCKERHUB_USERNAME }}/[Repository name]:latest           docker tag ${{ secrets.DOCKERHUB_USERNAME }}/[Repository name]:latest ${{ secrets.DOCKERHUB_USERNAME }}/[Repository name]:${{ github.run_number }}        - name: Push images to Docker Hub         run: |           docker push ${{ secrets.DOCKERHUB_USERNAME }}/[Repository name]:latest           docker push ${{ secrets.DOCKERHUB_USERNAME }}/[Repository name]:${{ github.run_number }}Enter fullscreen modeExit fullscreen modePush your project To GitHubNow that you've added the GitHub Actions workflow to your project, it's time to take the next steps.Initialize a Git repository:If you haven't already, initialize a Git repository in your project folder by runninggit init**. This will set up a new Git repository locally.Add your files:Usegit add .to add all files in your project to the staging area. If you want to add specific files, replace . with the file names.Commit your changes:Commit the added files to the local repository with a descriptive message usinggit commit -m \"Your commit message\".Link to your GitHub repository:If you haven't already, link your local repository to your GitHub repository by adding it as a remote. You can do this with the commandgit remote add origin. Replace with the URL of your GitHub repository.Push to GitHub:Finally, push your committed changes to GitHub withgit push -u origin main. This command pushes your changes to the main branch on GitHub.Finally! you can watch your workflow run:On your Github repository, Click on ActionsVerify that the Docker image is pushed to the container registries(Docker, Ecr)For Amazon Ecr:For DockerHub:Github Repository for this TutorialYou can also clone here and use the sample code:https://github.com/ARIES5533/Image-push-to-Ecr-and-DockerHub"}
{"title": "How to set up Amazon RDS Proxy for Amazon Aurora (Serverless) database cluster and connect AWS Lambda function to it", "published_at": 1713800158, "tags": ["aws", "serverless", "postgres", "java"], "user": "Vadym Kazulkin", "url": "https://dev.to/aws-builders/how-to-set-up-amazon-rds-proxy-for-amazon-aurora-serverless-database-cluster-and-connect-aws-lambda-function-to-it-20m5", "details": "IntroductionIn this article we'd like to explore how to connect Lamdba function to the Aurora database through the RDS Proxy using Java JDBC capabilities. You can explore RDS Proxy capabilities in theUsing Amazon RDS Proxyarticle. We will use Aurora Serverless v2 PostgreSQL database in this article but the same will also work Amazon Aurora PostgreSQL-Compatible Edition, Amazon Aurora MySQL-Compatible Edition, Amazon RDS for PostgreSQL, Amazon RDS for MySQL, Amazon RDS for MariaDB, and Amazon RDS for SQL Server as well. And it's easy to re-write Lambda function using different programming language as the basic concepts remain the same.SolutionWhat we'd like to achieve is described in the following architecture diagram:We'll focus on the part where Lambda connects through RDS Proxy to Aurora database. As the code sample we will use what we have created as part of theData API for Amazon Aurora Serverless v2 with AWS SDK for Javaseries, and the project can be found on myGitHub repository. The relevant part of the Infrastructure as a Code can be found in thisAWS SAM template.First of all, we need to create VPC to put Lambda, RDS Proxy and Aurora Cluster to, so that Lambda is able to talk to RDS Proxy which in turn connects to Aurora Cluster.  For that, please define your own VPC Id and Subnet list in the \"Parameter\" section of the SAM template like this :VpcId:     Type: String     Default: vpc-950cd6fd     Description: VpcId of your existing Virtual Private Cloud (VPC)   Subnets:     Type: CommaDelimitedList       Default: subnet-0787be4d, subnet-88dc46e0     Description: The list of SubnetIds, for at least two Availability Zones in the       region in your Virtual Private Cloud (VPC)Enter fullscreen modeExit fullscreen modeThen we need to define Lambda Security Group which references the VPC Id:LambdaSecurityGroup:       Type: AWS::EC2::SecurityGroup       Properties:         GroupDescription: SecurityGroup for Serverless Functions         VpcId:            Ref: VpcIdEnter fullscreen modeExit fullscreen modeand then VPC Security Group itself:VPCSecurityGroup:     Type: AWS::EC2::SecurityGroup     Properties:       GroupDescription: Security group for RDS DB Instance.       VpcId:           Ref: VpcId        SecurityGroupEgress:          - CidrIp: '0.0.0.0/0'           Description: lambda RDS access over 5432           FromPort: 5432           IpProtocol: TCP           ToPort: 5432       SecurityGroupIngress:         - IpProtocol: tcp           FromPort: '5432'           ToPort: '5432'           SourceSecurityGroupId:                Ref: LambdaSecurityGroupEnter fullscreen modeExit fullscreen modeVPC Security Group defines Security Group engress and igress roules which enable Lambda to talk to RDS Proxy via the port number 5432 (default port for PostgreSQL) and references the defined Lambda Security Group.Now let's create Aurora Serverless v2 PostgreSQL database cluster.  But before the let's create SecretsManager to store the database user and password:DBSecret:     Type: AWS::SecretsManager::Secret     Properties:       Name: !Ref UserSecret       Description: RDS database auto-generated user password       GenerateSecretString:         SecretStringTemplate: !Sub '{\"username\": \"${DBMasterUserName}\"}'         GenerateStringKey: \"password\"         PasswordLength: 30         ExcludeCharacters: '\"@/\\'Enter fullscreen modeExit fullscreen modeThen we need to define DB Subnet Group:DBSubnetGroup:     Type: AWS::RDS::DBSubnetGroup     Properties:       DBSubnetGroupDescription: Subnets available for the RDS DB Instance       SubnetIds:        Ref: SubnetsEnter fullscreen modeExit fullscreen modeand then create the Aurora Cluster with the engine \"aurora-postgresql\" itself:AuroraServerlessV2Cluster:     Type: 'AWS::RDS::DBCluster'     DeletionPolicy: Delete     Properties:       DBClusterIdentifier: !Ref DBClusterName       Engine: aurora-postgresql       Port: 5432       EnableHttpEndpoint: true       MasterUsername: !Join ['', ['{{resolve:secretsmanager:', !Ref DBSecret, ':SecretString:username}}' ]]       MasterUserPassword: !Join ['', ['{{resolve:secretsmanager:', !Ref DBSecret, ':SecretString:password}}' ]]       DatabaseName: !Ref DatabaseName       ServerlessV2ScalingConfiguration:         MinCapacity: 0.5         MaxCapacity: 1       DBSubnetGroupName:         Ref: DBSubnetGroup       VpcSecurityGroupIds:       - !Ref VPCSecurityGroupEnter fullscreen modeExit fullscreen modeIn the last lines we reference already created DB Subnet Group and Vpc Security Group Ids. With ServerlessV2ScalingConfiguration we define the scaling behaviour of our Aurora Serverless V2 Cluster.After it we need to create database instance of the Aurora (Serverless v2) cluster :AuroraServerlessV2Instance:     Type: 'AWS::RDS::DBInstance'     Properties:       Engine: aurora-postgresql       DBInstanceClass: db.serverless       DBClusterIdentifier: !Ref AuroraServerlessV2Cluster       MonitoringInterval: 1       MonitoringRoleArn: !GetAtt EnhancedMonitoringRole.Arn       PubliclyAccessible: false       EnablePerformanceInsights: true       PerformanceInsightsRetentionPeriod: 7Enter fullscreen modeExit fullscreen modeNow let's take care of the creation of the RDS Proxy itself :RDSProxy:     Type: AWS::RDS::DBProxy     Properties:       Auth:         - { AuthScheme: SECRETS, SecretArn: !Ref DBSecret}       DBProxyName: 'rds-proxy'       RoleArn: !GetAtt RDSProxyRole.Arn       EngineFamily: 'POSTGRESQL'       IdleClientTimeout: 120       RequireTLS: true       DebugLogging: false       VpcSecurityGroupIds:       - !Ref VPCSecurityGroup       VpcSubnetIds: !Ref  SubnetsEnter fullscreen modeExit fullscreen modeWe use EngineFamily 'POSTGRESQL' and AuthScheme connected to the created SecretsManager secret for authentication in our scenario. There are also other authentication possibilities likeIAM authenticationoffered. We also connect RDS Proxy to the already created Vpc Security Group and Vpc Subnet Id.We also need to create RDS Proxy Role to allow RDS Proxy to fetch secrets from the SecretsManager:RDSProxyRole:     Type: AWS::IAM::Role     Properties:       Path: /       AssumeRolePolicyDocument:         Version: '2012-10-17'         Statement:           - Action: [ 'sts:AssumeRole' ]             Effect: Allow             Principal:               Service: [ rds.amazonaws.com ]       Policies:         - PolicyName: DBProxyPolicy           PolicyDocument:             Version: '2012-10-17'             Statement:               - Action:                   - secretsmanager:GetSecretValue                 Effect: Allow                 Resource:                   - !Ref DBSecretEnter fullscreen modeExit fullscreen modeAs the final step we need to create DB Proxy Target Group:ProxyTargetGroup:     Type: AWS::RDS::DBProxyTargetGroup     Properties:       DBProxyName: !Ref RDSProxy       DBClusterIdentifiers: [ !Ref AuroraServerlessV2Cluster]       TargetGroupName: default       ConnectionPoolConfigurationInfo:         MaxConnectionsPercent: 5         MaxIdleConnectionsPercent: 4         ConnectionBorrowTimeout: 120Enter fullscreen modeExit fullscreen modeDB Proxy Target Group connects RDS Proxy with the Aurora V2 Cluster and defines Connection Pool Configuration. Please refer toConfigure Connection Settingsdocumentation for the extended explanation of this configuration.Now let's connect Lambda function \"GetProductByIdViaAuroraServerlessV2WithRDSProxy\" to the RDS Proxy.The relevant part is this one:Policies:         - Statement:             - Sid: AllowDbConnect               Effect: Allow               Action:                 - rds-db:connect               Resource:                 - !Sub arn:aws:rds-db:${AWS::Region}:${AWS::AccountId}:dbuser:!Select [6, !Split [\":\", !GetAtt RDSProxy.DBProxyArn]]/*          - VPCAccessPolicy: {}       VpcConfig:         SecurityGroupIds:           - Fn::GetAtt: LambdaSecurityGroup.GroupId         SubnetIds: !Ref  SubnetsEnter fullscreen modeExit fullscreen modeIn the VPCAccessPolicy part we reference already created Security Groups and Subnets. In the Policies part we allow Lambda to connect to our RDS Proxy instance by defining the exact resource ARN by knowing how the resource schema name works but extracting the appropriate RDSProxy ARN part from it. For the exact explanation please visitCreating and using an IAM policy for IAM database access.The word of caution:only for the demonstration purpose I passed the database name and password as the Lambda environment variables to connect to RDS Proxy which introduces the security risk.Environment:         Variables:           DB_USER_PASSWORD: !Join ['', ['{{resolve:secretsmanager:', !Ref DBSecret, ':SecretString:password}}' ]]           DB_USER_NAME: !Join ['', ['{{resolve:secretsmanager:', !Ref DBSecret, ':SecretString:username}}' ]]           RDS_PROXY_ENDPOINT: !GetAtt RDSProxy.EndpointEnter fullscreen modeExit fullscreen modeThe proper solution is to use the stored database name and password in Amazon Secret Manager and then retrieve the in the Lambda function itself.That it on the Infrastructure as a Code level.Everything else will be done in the Lambda handler itself to connect to the RDS Proxy. In my case I wroteGetProductByIdViaAuroraServerlessV2RDSProxyHandlerin Java and used JDBC for the connection with the PostgreSQL database driver in thepom.xmllike this.<dependency>        <groupId>org.postgresql</groupId>        <artifactId>postgresql</artifactId>        <version>42.5.4</version>     </dependency>Enter fullscreen modeExit fullscreen modeEach programming languge in which Lambda function can be implemented offers its own capabilities to connect to the database.ConclusionIn this article we explored how to connect Lamdba function to the Aurora database through the RDS Proxy and demonstrated the example of the corresponding Infrastructure as a Code part and the Lambda function written in Java which uses JDBC to connect to RDS Proxy."}
{"title": "Comparison of Cloud Storage Services", "published_at": 1713792986, "tags": ["aws", "devops", "cloud", "design"], "user": "Eyal Estrin", "url": "https://dev.to/aws-builders/comparison-of-cloud-storage-services-7mn", "details": "When designing workloads in the cloud, it is rare to have a workload without persistent storage, for storing and retrieving data.In this blog post, we will review the most common cloud storage services and the different use cases for choosing specific cloud storage.Object storageObject storage is perhaps the most commonly used cloud-native storage service.It is been used by various use cases from simple storage or archiving of logs or snapshots to more sophisticated use cases such as storage for data lakes or AI/ML workloads.Object storage is used by many cloud-native applications from Kubernetes-based workloads using CSI driver (such asAmazon EKS,Azure AKS, andGoogle GKE), and for Serverless / Function-as-a-Service (such asAWS Lambda, andAzure Functions).As a cloud-native service, the access to object storage is done via Rest API, HTTP, or HTTPS.Unstructured data is stored inside object storage services as objects, in a flat hierarchy, where most cloud providers call it buckets.Data is automatically synched between availability zones in the same region (unless we choose otherwise), and if needed, buckets can be synched between regions (using cross-region replication capability).To support different data access patterns, each of the hyperscale cloud providers, offers its customers different storage classes (or storage tiers), from real-time, near real-time, to archive storage, and a capability for configuring rules for moving data between storage classes (also known as lifecycle policies).As of 2023, all hyperscale cloud providers enforce data encryption at rest in all newly created buckets.Comparison between Object storage alternatives:As you can read in the comparison table above, most features are available in all hyper-scale cloud providers, but there are still some differences between the cloud providers:AWS\u2013 Offers a cheap storage tier calledS3 One Zone-IAfor scenarios where data access patterns are less frequent, and data availability and resiliency are not highly critical, such as secondary backups. AWS also offers a tier calledS3 Express One Zonefor single-digit millisecond data access requirements, with low data availability or resiliency, such as AI/ML training, Amazon Athena analytics, and more.Azure\u2013 Most storage services in Azure (Blob, files, queues, pages, and tables), require the creation of anAzure storage account\u2013 a unique namespace for Azure storage data objects, accessible over HTTP/HTTPS. Azure also offers aPremium block blobfor high-performance workloads, such as AI/ML, IoT, etc.GCP\u2013 Cloud storage in Google, is not limited to a single region but can be provisioned and synched automatically todual-regionsand evenmulti-regions.Block storageBlock storage is the disk volume attached to various compute services \u2013 from VMs, managed databases, Kubernetes worker notes, and mounted inside containers.Block storage can be used as the storage for transactional databases, data warehousing, and workloads with high volumes of read and write.Block storage is not just limited to traditional workloads deployed on top of virtual machines, they can be mounted as persistent volumes for container-based workloads (such asAmazon ECS), and for Kubernetes-based workloads using CSI driver (such asAmazon EKS,Azure AKS, andGoogle GKE).Block storage volumes are usually limited to a single availability zone within the same region and should be mounted to a VM in the same AZ.Comparison between Block storage alternatives:As you can read in the comparison table above, most features are available in all hyper-scale cloud providers, but there are still some differences between the cloud providers:AWS\u2013 Offers a feature calledAmazon Data Lifecycle Manager, which automates the process of creation, retention, and deletion of EBS snapshots.Azure\u2013 Offers the ability to manage data replication of persistent disks within the same region (Locally redundant storage / LRSandZone-redundant storage / ZRS) and between primary and secondary regions (Geo-redundant storage / GRS,Geo-zone-redundant storage / GZRS, andRead-access geo-redundant storage / RA-GRS).GCP\u2013 Offers the ability to replicate persistent disks across two zones in the same region (Regional Persistent Disk). GCP also offers the ability to pre-purchase capacity, throughput, and IOPS to be provisioned as needed (Hyperdisk Storage Pools).File storageFile storage services are the equivalent of the traditional Storage Area Network (SAN).All major hyperscale cloud providers offer managed file storage services, allowing customers to share files between multiple Windows (CIFS/SMB), and Linux (NFS) virtual machines.File storage is not just limited to traditional workloads sharing files between multiple virtual machines, they can be mounted as persistent volumes for container-based workloads (such asAmazon ECS,Azure Container Apps, andGoogle Cloud Run), Kubernetes-based workloads using CSI driver (such asAmazon EKS,Azure AKS, andGoogle GKE, and for Serverless / Function-as-a-Service (such asAWS Lambda, andAzure Functions).Other than the NFS or CIFS/SMB file storage services, major cloud providers also offer a managed NetApp files system (for customers who wish to have the benefits of NetApp storage) and managed Lustre file system (for HPC workloads or workloads that require extreme high-performance throughput).Comparison between NFS File storage alternatives:As you can read in the comparison table above, most features are available in all hyper-scale cloud providers, but there are still some differences between the cloud providers:AWS\u2013 Offers cheap storage tier calledEFS One Zone file system, for scenarios where data access pattern is less frequent, and data availability and resiliency are not highly critical. By default, data inside the One Zone file system is automatically backed up using AWS Backup.Azure\u2013 Offers an additional security protection mechanism such as malware scanning and sensitive data threat detection, as part of a service calledMicrosoft Defender for Storage.GCP\u2013 Offers enterprise-grade tier for critical applications such as SAP or GKE workloads, with regional high-availability and data replication calledEnterprise tier.Comparison between CIFS/SMB File storage alternatives:Comparison between managed NetApp File storage alternatives:Comparison between File storage for HPC workloads alternatives:SummaryPersistent storage is required by almost any workload, including cloud-native applications.In this blog post, we have reviewed the various managed storage options offered by the hyperscale cloud providers.As best practice, it is crucial to understand the application's requirements, when selecting the right storage option.About the AuthorEyal Estrin is a cloud and information security architect, and the author of the booksCloud Security Handbook, andSecurity for Cloud Native Applications, with more than 20 years in the IT industry.You can connect with him onTwitter.Opinions are his own and not the views of his employer.\ud83d\udc47Help to support my authoring\ud83d\udc47\u2615Buy me a coffee\u2615"}
{"title": "Guaranteed Safety using Blue Green with ECS and CDK", "published_at": 1713721646, "tags": ["aws", "serverless", "api", "rust"], "user": "Benjamen Pyle", "url": "https://dev.to/aws-builders/guaranteed-safety-using-blue-green-with-ecs-and-cdk-4b4e", "details": "Buckle up for this one as it's going to be a lengthy piece.  I love writing articles like this one because they contain complete infrastructure builds that highlight some best practices to put multiple components together and act as great starting points for people to use immediately.  I've been working a great deal with containers lately and I kept finding it difficult to locate a working sample of building Blue Green with ECS and CDK.  So I set out to put that together.  Let's get started.ArchitectureI've been running Blue Green with ECS in production for several years now and have been helping customers integrate the practices into their current environments.  But I hadn't documented one from scratch to make Blue Gree with ECS and CDK a pattern.  For reference, I took inspiration fromthis articlewhich highlights the decision points that one needs to make when taking this approach in a purely native AWS manner.What I was looking for with this code was this:Publically exposed over an API GatewayThe load balancer supporting the services must be inaccessible from the public internetDeployed as ECS Fargate tasksDeployments managed by AWS' CodeDeployOptionally the ECR repository could be behind a VPC EndpointWith that criteria in mind, here's the architecture that we'll be working through for the rest of the article.Blue Green with ECS and CDKWhere do we get started on this epic build?  Well, it's hard to have any resources deployed without a VPC, so that's where we will begin.Building the VPCThis VPC will be simple enough and possess the following attributes.Subnets will be Public, Private with Egress and IsolatedContain 2 availability zonesA VPC Endpoint to ECR in case you want to leverage this capabilitythis._vpc=newVpc(this,\"CustomVpc\",{subnetConfiguration:[{name:\"custom-vpc-public-subnet\",subnetType:SubnetType.PUBLIC,cidrMask:24,},{name:\"custom-vpc-private-subnet\",subnetType:SubnetType.PRIVATE_WITH_EGRESS,cidrMask:24,},{name:\"custom-vpc-isolated-subnet\",subnetType:SubnetType.PRIVATE_ISOLATED,cidrMask:24,},],maxAzs:2,natGateways:2,vpcName:\"CustomVpc\",});this._vpc.addInterfaceEndpoint(\"EcrEndpoint\",{service:InterfaceVpcEndpointAwsService.ECR,});Enter fullscreen modeExit fullscreen modeOnce deployed, this will produce a resource map like the one below.The ECS ClusterNext up, I have to establish an ECS Cluster.  AWS defines Elastic Container Service in this way.Amazon Elastic Container Service (ECS) is a fully managed container orchestration service that helps you to more efficiently deploy, manage, and scale containerized applications. It deeply integrates with the AWS environment to provide an easy-to-use solution for running container workloads in the cloud and on premises with advanced security features using Amazon ECS Anywhere. - AWSBy leveraging ECS, I can take advantage of another type ofserverless compute called Fargate.The CDK code to establish the cluster sets a name and the VPC that was defined above.this._cluster=newCluster(scope,'EcsCluster',{clusterName:'sample-cluster',vpc:props.vpc})Enter fullscreen modeExit fullscreen modeLoad BalancingWhen building Blue Green with ECS and CDK, a decision needs to be made about which type of Load Balancer is going to be used.  Additionally, the load balancer type will influence the way that the VPC PrivateLink is configured.  Before diving in, what is a PrivateLink?AWS PrivateLink provides private connectivity between virtual private clouds (VPCs), supported AWS services, and your on-premises networks without exposing your traffic to the public internet. Interface VPC endpoints, powered by PrivateLink, connect you to services hosted by AWS Partners and supported solutions available in AWS Marketplace. - AWSPrivateLink ConsiderationsChoosing the load balancer and API Gateway type will drive certain design decisions.  Before highlighting those though, the option that will be built below is an API Gateway HTTP version paired with PrivateLink.  What this allows is multiple Application Load Balancers can be connected via one PrivateLink.  If the solution spans multiple VPCs, then more PrivateLinks can be added.  This is a flexible approach in that multiple microservices can be supported under multiple Application Load Balancers under a single PrivateLink.You might be wondering, isn't the HTTP API less featured than the REST version of API Gateway?  That's correct, it is.  There are benefits though to the HTTP version.  HTTP is cheaper, and faster and offers this nice PrivateLink integration with a VPC.  If the HTTP API isn't what you want, then leaning API Gateway's REST version comes with other things to take into account.When choosing API Gateway's REST version, the choices with PrivateLink shuttle you down a different path.  You must choose a Network Load Balancer integration which comes with a handleful of limitations.NLB operates on a lower level of theOSINLB paired with CodeDeploy only allowsCodeDeployDefault.AllAtOncedeployment configuration.PrivateLinks are established with the Load Balancer, not the VPC, which comes with quota limits (that are soft) and additional hops to perhaps an ALB to support more advanced rollouts.It must be said though, NLB's are amazingly fast and also inexpensive.  You might only need All At Once deployments and your application might not have a bunch of services, therefore NLB is the move.  You can also add in multiple ALBs behind the NLB.  This will add some latency but will bring back maximum flexibility.That's a lot of information compressed into one writing block but the point of that is to simply state, that there is no one size fits all.  And there will be trade-offs that you'll have to take on and be OK with regardless of the approach that you take.Establishing the Load BalancerBack on track to putting the load balancer together when building Blue Green with ECS and CDK.  As mentioned above, I'm going to show the Application Load Balancer with the API Gateway HTTP version.this._securityGroup=newSecurityGroup(scope,'SecurityGroup',{vpc:props.vpc,allowAllOutbound:true})this._securityGroup.addIngressRule(this.securityGroup,Port.tcp(3000),'Group Inbound',false);this._loadBalancer=newApplicationLoadBalancer(scope,'NetworkLoadBalancer',{vpc:props.vpc,loadBalancerName:'sample-cluster-nlb',vpcSubnets:{subnets:props.vpc.privateSubnets,onePerAz:true,availabilityZones:props.vpc.availabilityZones},securityGroup:this.securityGroup});Enter fullscreen modeExit fullscreen modeThe code above is building the Application Load Balancer with the VPC that was built higher up in the article.  What also needs to be done is the creation of a SecurityGroup which acts as a virtual firewall on the load balancer.Adding Target GroupsBlue Green with ECS and CDK is performed by CodeDeploy shifting traffic between load balancer target groups.  I've got to establish those, create listener rules, and then make them available for CodeDeploy.  Let's first create the groups.this._blueTargetGroup=newApplicationTargetGroup(this,'blueGroup',{vpc:props.vpc,port:80,targetGroupName:\"sample-cluster-blue\",targetType:TargetType.IP,healthCheck:{protocol:Protocol.HTTP,path:'/health',timeout:Duration.seconds(30),interval:Duration.seconds(60),healthyHttpCodes:'200'}});this._greenTargetGroup=newApplicationTargetGroup(this,'greenGroup',{vpc:props.vpc,port:80,targetType:TargetType.IP,targetGroupName:\"sample-cluster-green\",healthCheck:{protocol:Protocol.HTTP,path:'/health',timeout:Duration.seconds(30),interval:Duration.seconds(60),healthyHttpCodes:'200'}});this._listener=this._loadBalancer.addListener('albProdListener',{port:80,defaultTargetGroups:[this._blueTargetGroup]});this._testListener=this._loadBalancer.addListener('albTestListener',{port:8080,defaultTargetGroups:[this._greenTargetGroup]});Enter fullscreen modeExit fullscreen modeFrom this code, I'm building up two target groups that are configured the same.  Both have the same timeouts, and intervals, looking for health checks that return codes in the 200s and use target types of IP.Next, I'm defining listeners and then assigning them to the target groups.  The listeners are also managed during the CodeDeploy rollout and allow the testing of traffic while things are in progress at various stages.  We will get to that more below.ECS Task DefinitionThe definition for executing code in Blue Green with ECS and CDK is the ECS Task.  The task definition contains information about the containers that will run together, port definitions, logging definitions, and many other useful settings that impact the runtime of your code.  Tasks also aren't tied specifically to a cluster but will be married together with a Service to form the bond within a specific Cluster.  With ECS, the task could exist in several clusters if needed.  Tasks also contain versions so every update of the definition will create a new revision.this._taskDefinition=newTaskDefinition(scope,'rust-blue-green',{cpu:\"256\",memoryMiB:\"512\",compatibility:Compatibility.FARGATE,runtimePlatform:{cpuArchitecture:CpuArchitecture.ARM64,operatingSystemFamily:OperatingSystemFamily.LINUX},networkMode:NetworkMode.AWS_VPC,family:\"rust-blue-green\"});constcontainer=this._taskDefinition.addContainer(\"rust-api\",{// Use an image from Amazon ECRimage:ContainerImage.fromRegistry(\"public.ecr.aws/f8u4w2p3/rust-blue-green:latest\"),logging:LogDrivers.awsLogs({streamPrefix:'rust-api'}),environment:{},containerName:'rust-api',essential:true,cpu:256,memoryReservationMiB:512// ... other options here ...});container.addPortMappings({containerPort:3000,appProtocol:AppProtocol.http,name:\"web\",protocol:Protocol.TCP});Enter fullscreen modeExit fullscreen modeThere are three parts to this block.Establish the task definition.I'm opting for .25 vCPU and 512MB of memory.  This is a Web API coded in Rust, so tons of resources aren't needed.Fargate is my deployment option as I want it to be serverlessGraviton/ARM64 is my architecture type because who doesn't want more performance for less money?Add my container to the task.  I'm doing this via a public ECR repository where I've shipped my container ahead of time.  I'll include this code's repos at the bottom as well.Specify the ports that I want to communicate over and that my container exposes per the DockerfileTask Definition IAMOne last piece of the Task Definition is to add an execution policy.  This policy defines things that ECS will use to launch the task.  Things such as the ability to pull the container from ECR would be helpful.  I've included this here in case you want to put an image in your own ECR and use that.  Don't be confused with the Task Role though.  This second role is where you define permissions that the task needs to have.  Things like DynamoDB, SQS, or Secrets Manager.constexecutionPolicy=newPolicyStatement({actions:[\"ecr:GetAuthorizationToken\",\"ecr:BatchCheckLayerAvailability\",\"ecr:GetDownloadUrlForLayer\",\"ecr:BatchGetImage\",\"logs:CreateLogStream\",\"logs:PutLogEvents\"],resources:[\"*\"],effect:Effect.ALLOW});this._taskDefinition.addToExecutionRolePolicy(executionPolicy);Enter fullscreen modeExit fullscreen modeECS ServiceWe are most of the way done, but still need to put the cluster and the task together.  With ECS you do this with a service.  And with Blue Green with ECS and CDK, it looks like this.constservice=newFargateService(this,'Service',{cluster:props.cluster,taskDefinition:props.task,desiredCount:1,deploymentController:{type:DeploymentControllerType.CODE_DEPLOY,},securityGroups:[props.securityGroup]});service.attachToNetworkTargetGroup(props.blueTargetGroupasNetworkTargetGroup);newEcsDeploymentGroup(this,'BlueGreenDG',{service,blueGreenDeploymentConfig:{blueTargetGroup:props.blueTargetGroup,greenTargetGroup:props.greenTargetGroup,listener:props.listener,testListener:props.testListener,},deploymentConfig:EcsDeploymentConfig.ALL_AT_ONCE,});Enter fullscreen modeExit fullscreen modeNotice the first mention in the infrastructure of CodeDeploy.  We'll get into more of that in the testing phase of the article but ECS is very tightly integrated with AWS CodeDeploy.API GatewayOur Blue Green with ECS and CDK infrastructure journey is almost coming to a close.  I'm getting excited about the testing phase of this operation.  I hope you are as well.Remember, I'm going for an HTTP API Gateway which is limited in features but low in cost and latency.I'm going to establish the PrivateLink and the API Gateway all in one swoop.constlink=newVpcLink(scope,'VpcLink',{vpc:props.vpc,vpcLinkName:'sample-cluster-vpc-link',securityGroups:[props.securityGroup],})constalbIntegration=newHttpAlbIntegration('ALBIntegration',props.listener,{vpcLink:link});constapiGateway=newHttpApi(scope,'SampleClusterAPI',{});apiGateway.addRoutes({path:\"/one\",methods:[HttpMethod.GET],integration:albIntegration})Enter fullscreen modeExit fullscreen modeWhat I like about this is the simplicity of attaching the ALB Integration directly to the route definition.  When I supply/one, it'll be routed into my load balancer passing along that path into the container.On the VPC Link, I'm using the VPC defined way up at the top of this article and the SecurityGroup that was also defined in that construct for additional security.Once deployed, there will be a VPC Link and an API Gateway.DeployingDeploying Blue Green with ECS and CDK just requires the following command from the root directory.cdk deployEnter fullscreen modeExit fullscreen modeNow sit back and watch CloudFormation do its thing.  Once it's completed, you'll see the same resources I've highlighted above.The Blue/Green in Blue Green with ECS and CDKNow onto the Blue/Green in Blue Green with ECS and CDK.AWS defines the CodeDeploy service in this way:AWS CodeDeploy is a fully managed deployment service that automates software deployments to various compute services, such as Amazon Elastic Compute Cloud (EC2), Amazon Elastic Container Service (ECS), AWS Lambda, and your on-premises servers. Use CodeDeploy to automate software deployments, eliminating the need for error-prone manual operations. - AWSWhat I love about CodeDeploy is that I can use it for ECS and Lambda.  It's a managed service so it does come with some limitations but it also comes with plenty of defaults that I don't have to worry about.  Focus on shipping and not on the tools.When the stack is deployed, you'll have a CodeDeploy Application and a DeploymentGroup.Hidden GemBuried in this Blue Green with ECS and CDK project is a Lambda Function that you might have missed if just reading through the repository.constsecurityGroup=newSecurityGroup(scope,'FunctionSecurityGroup',{allowAllOutbound:true,vpc:props.vpc,});this._function=newRustFunction(scope,\"InstallTestFunction\",{manifestPath:'./',architecture:Architecture.ARM_64,memorySize:256,vpc:props.vpc,securityGroups:[securityGroup],vpcSubnets:{subnets:props.vpc.privateSubnets},environment:{ALB_URL:props.alb.loadBalancerDnsName}});this._function.addToRolePolicy(newPolicyStatement({actions:[\"codedeploy:PutLifecycleEventHookExecutionStatus\"],resources:[\"*\"],effect:Effect.ALLOW,sid:\"CodeDeployActions\"}))Enter fullscreen modeExit fullscreen modeIf you notice that this function is coded inRust, I'm sure you won't be surprised.But what does this function do?Pre-Traffic Lambda FunctionYou might notice that I've attached our VPC to the Function.  And if we explore this function's code, what you'll also find is that it is testing routes.  To test routes by hitting our ALB, the VPC piece is required.  But why would we test routes?CodeDeploy offers a handful of hooks that can be exercised during the rollout.  At any point, the Lambda Function that is attached to that hook can send Success or Failure back to CodeDeploy thus allowing the deployment to continue or stop which triggers a rollback.  Pretty cool right?  This is a feature that isn't shared nearly enough.The tour of this code is short but it's in the repository so you can walk through it in full when you clone it.letdeployment_id=event.payload.get(\"DeploymentId\").unwrap();letlifecycle_event_hook_execution_id=event.payload.get(\"LifecycleEventHookExecutionId\").unwrap();letconfig=aws_config::load_from_env().await;letclient=Client::new(&config);letmutpassed=true;ifletErr(_)=run_test(alb_url,\"one\".to_string()).await{info!(\"Test on Route one failed, rolling back\");passed=false}# More tests happen here they are just omittedletstatus=ifpassed{LifecycleEventStatus::Succeeded}else{LifecycleEventStatus::Failed};letcloned=status.clone();client.put_lifecycle_event_hook_execution_status().deployment_id(deployment_id).lifecycle_event_hook_execution_id(lifecycle_event_hook_execution_id).status(status).send().await?;info!(\"Wrapping up requests with a status of: {:?}\",cloned);Ok(())Enter fullscreen modeExit fullscreen modeWhat's happening here is that I'm running HTTP requests against endpoints over the test listener ports defined on the load balancer.  I'm going to write more on this code over atServerless Rustso don't worry if you are looking for more content on this pattern.  It's coming.Triggering a DeploymentTriggering a deployment using Blue Green with ECS and CDK requires an application deployment configuration.  For this example, I'm going to use YAML and perform this through the Console.  This could be done via an automated process, but I think showing from the Console at this point makes the most sense as deployment triggers can take different shapes and options.This file is included in the repository for you to adjust and use as well and looks like this.version:0.0Resources:-TargetService:Type:AWS::ECS::ServiceProperties:TaskDefinition:\"arn:aws:ecs:<region>:<account>:task-definition/rust-blue-green:9\"LoadBalancerInfo:ContainerName:\"rust-api\"ContainerPort:3000Hooks:-BeforeAllowTraffic:\"arn:aws:lambda:<region>:<account>:function:EcsDeploymentStack-InstallTestFunction55902174-yzGCQXvLAhXM\"Enter fullscreen modeExit fullscreen modeNotice that I'm able to select the Task Definition that I want to deploy and then supply a Lambda Function for any of the Hooks that I want to trigger.  To test this, I recommend you create two Task Definitions where each version is represented.  There are tags in the ECR repository for both print-blue and print-green so that you can switch back and forth.  The default infrastructure deployment will launch the Green version of the image.Testing the Initial DeploymentIf I go back to the API Gateway portion of this article, I'm going to grab the AWS-assigned Endpoint URL and add the/oneroute onto it.  Doing so and running in Postman will yield the following result.As you can see in the response, the output is showing \"green\".{\"key\":\"route_one from Green\"}Enter fullscreen modeExit fullscreen modeNow to Push to BluePushing the \"Blue\" version using our Blue Green with ECS and CDK requires creating a deployment from the CodeDeploy Application page.  What's worth paying attention to is that I mentioned the application file and then I'm going to highlight the Deployment Group Overrides.  There are other options that I plan to explore later around rollback alarms but for now, these are the only two things we will look at.Feel free to play with the Deployment Overrides, but for now, I'm going to run them with the AllAtOnce configuration which means that all traffic shifts at once barring new issues occur in my triggers.Make note of the task you created that makes use of theprint-blueDocker tag, and off we go!Final TestingThe end is near! If you've hung on this long to Blue Green with ECS and CDK, the payout is just below.  When done, you will see the following artifacts.  CodeDeploy will have deployed the new task, CloudWatch will show the triggered logs from the Lambda Function, and then Postman will show that the URL with route one now shows it's hitting the blue container.CodeDeployLambda Function ExecutionPostman ExecutionAs you can see in the response, the output is showing \"blue\".{\"key\":\"route_one from Blue\"}Enter fullscreen modeExit fullscreen modeWrapping UpPhew! I feel like this could have been a few chapters in a book! Let's wrap up on Blue Green with ECS and CDK.Cleaning things UpTo clean up this whole process, simply issue from the root directory.cdk destoyEnter fullscreen modeExit fullscreen modeThis command will destroy the stack and all of the resources so you aren't changed for the Load Balancer, Nat Gateway, and other always-on resourcesLast ThoughtsIf you've made it this far, thanks so much for hanging in there.  This article can be saved and scanned for future use as the real value is in the code attached.  And as promised, here are the two repositories.This article's CDK ProjectRust Blue/Green Image CodePlease feel free to clone, reshape, or adjust as needed.  If you are looking for an API Gateway HTTP version that offers Blue/Green deployments with ECS and Fargate, while also providing Load Balancer security, this repository is a fantastic way to get started.I've said before, that I do love the developer experience when working with containers and I'm a big fan of using ECS to manage my container workloads.  It scales, it's simple, and it's highly secure.  When using Fargate paired with AWS CDK, it lets me focus on shipping value and not all of the other things that go into running production systems.Thanks for reading and happy building!"}
{"title": "Configuring an Amazon Bedrock Knowledge Base", "published_at": 1713698617, "tags": ["aws", "ai", "tutorial", "cloudcomputing"], "user": "Faye Ellis", "url": "https://dev.to/aws-builders/using-a-knowledge-base-to-connect-amazon-bedrock-to-your-custom-data-2gd9", "details": "Here's a step-by-step process for using Knowledge Bases with Amazon Bedrock, to easily customize Bedrock with your own data. The code and commands used can be foundhere. The cost will be <$5, if you remember to clean up at the end!The architecture looks like this:Architecture ComponentsSageMaker NotebookA SageMaker notebook is used as the IDE (Integrated Dev Environment), to run all commands used to set everything up, and the code to interact with Bedrock.S3The custom data is uploaded to an S3 bucket. After the bucket is created, manually upload any text based data that you want to work with, e.g. PDF, .csv, Microsoft Word, or .txt files. Sample documents to test with are included in theGitHub Repo.Amazon OpenSearch ServerlessAmazon OpenSearch Serverless (AOSS) is used to create a vector index/data store, from the S3 data.Bedrock Knowledge BaseThe Bedrock Knowledge Base is configured to use the AOSS vector index as a data store, and will answer prompts based on the provided data.Prerequisites1) Do everything in us-west-2.2) In your AWS account, request access for the Bedrock models that you would like to use. You'll find this in the Bedrock console, under model access. (For this, I enabled all the Anthropic Claude models.)3)To create the SageMaker Notebook, first make sure you have a SageMaker Domain in us-west-2, this on-time step creates home directory space, and VPC configurations needed by any Notebooks you create in this region. If you don't have one already, select the Create Domain option, and it will do everything for you.Next, usethis CloudFormation templateto create a Sagemaker Notebook, that we'll use to run the commands from. The template will configure the SageMaker Notebook instance, with an associated IAM role that includes permissions for a few required services, including:S3 full accessBedrock full accessIAM full accessLambda full accessAmazon Opensearch serverless full accessAfter everything has been configured, the permissions can be tightened up if needed.4) When the Notebook is ready, select the Notebook instance and select open Jupyter Lab. The GitHub repository will already be downloaded.5) From the cloned repository, open the file named: bedrock_rag_demo.ipynb - this is an Interactive Python Notebook, each block of code is displayed in a cell that can be run in sequence, to observe the outcome of each step.6) Run all the cells in contained in the .ipynb file, which at a high level, will do the following:Install required libraries like boto3, which is the AWS SDK for Python that interacts with Bedrock. And opensearch-py which is the Python client used to interact with OpenSearch.Create an S3 bucket to store our custom data. (Then manually upload the custom data, I just used a PDF containing some text.)Create the OpenSearch Serverless collection which is a container for OpenSearch indexes.Create the OpenSearch Serverless vector index. This will contain the vector embeddings, or numerical representations of your data. So that the LLM can make sense of your data and understand the meaning it contains.Configure the Bedrock Knowledge Base using the OpenSearch Serverless vector index. Data source will be S3.Ingest the data into the Knowledge BaseTestingRun some prompts to test that the LLM is using the Knowledge Base to answer. Try a prompt that we know it won\u2019t find the answer in the custom data. If it\u2019s working properly, the model should return that it doesn\u2019t know.Example Prompts to Try:If you uploaded the files provided in the repo try running the following prompts:What is the parental leave policy at Bob's Pizza?What are the top three most expensive services?What is FinOps?What is the sick leave policy at Bob's Pizza? (None of the provided documents contain this data, so the model should tell you it doesn't know).So this is a great way to avoid dreaded hallucinations, to help improve accuracy by providing correct and up-to-date data, and to control the data that is being used by the model.Cleaning Up to Avoid ChargesAfter testing, run the last four cells in the notebook, to clean up the Knowledge Base, OSS, S3 and IAM to avoid unnecessary charges. Then remember to manually stop and delete the Notebook instance if you no longer need it."}
{"title": "Integration of Chatbot(Amazon Lex) in a static website (Hosted on S3 and cloud front)", "published_at": 1713676810, "tags": ["amazonlex", "cloudfront", "s3bucket", "cloudformation"], "user": "Vijayaraghavan Vashudevan", "url": "https://dev.to/aws-builders/integration-of-chatbotamazon-lex-in-a-static-website-hosted-on-s3-and-cloud-front-14jf", "details": "\ud83d\udcdaTable of Content\ud83c\udfa4-Chatbot\ud83c\udfa4-Amazon Lex\ud83c\udfa4-Flow Diagram\ud83c\udfa4-Creation of Chatbot using Amazon Lex for ordering Pizza\ud83c\udfa4-Hosting a static website on S3 bucket\ud83c\udfa4-Integration of Chatbot with website hosted on a S3 bucket\ud83c\udfa4-Hands-on Demo\ud83c\udfa4-Instructions to clean up AWS resources to avoid Billing\ud83e\udd16 Chatbot\ud83d\udcac A chatbot is a computer program designed to simulate conversation with human users, especially over the internet. Chatbots can range from simple rule-based systems that follow pre-programmed responses to sophisticated artificial intelligence (AI) models capable of understanding natural language and engaging in more complex interactions. They are used in various applications, including customer service, virtual assistants, information retrieval, and entertainment.\ud83d\udc68\u200d\ud83d\udcbb Amazon Lex\ud83d\udcccAmazon Lex is a service provided by Amazon Web Services (AWS) that enables developers to build conversational interfaces, or chatbots, into their applications using voice and text. It utilizes the same technology that powers Amazon Alexa, the virtual assistant found in Amazon Echo devices.\ud83d\udcccWith Amazon Lex, developers can create natural language understanding (NLU) models that interpret user input and respond accordingly. Lex handles the heavy lifting of speech recognition and natural language understanding, allowing developers to focus on designing conversational flows and integrating the chatbot into their applications. It can be integrated with other AWS services, such as AWS Lambda for executing backend logic, Amazon DynamoDB for data storage, and Amazon Polly for text-to-speech synthesis\u23f3Flow Diagram\ud83d\udcadCreation of Chatbot using Amazon Lex for ordering Pizza\ud83d\udcccGo to AWS Management Console, Navigate to Amazon Lex, and select the Create bot button. Give the bot name and description below\ud83d\udcccSelect the language as default English. Click Done\ud83d\udcccGive the intent name and intent details below\ud83d\udcccWe can add utterances below\ud83d\udccc We can toggle to active for the confirmation prompt and closing prompt. Save the Intent\ud83d\udcccNow add the slot type values below\ud83d\udcccSlot used to capture the information from the user to fulfill the intent\ud83d\udccc Add the confirmation slot and closing slot message\ud83d\udccc You can add delivery time by adding a card group to make it more convenient for the users\ud83d\udccc Save the intent and build it as below\ud83d\udd75\ufe0f It's time to test a chatbot we built\ud83e\udd29 Yes !! We build the chatbot using Amazon Lex \ud83d\ude4c\ud83c\udf0fHosting a static website on S3 bucket\ud83d\udcccNavigate to the S3 console, Create a bucket with the name staticwebsite2004 as below\ud83d\udccc Go to the properties tab, Enable a static website hosting with an index document as index.html\ud83d\udccc Go to the permissions tab, Enable the block all public access with the checkbox unselected\ud83d\udcccAdd the bucket policy under the permissions tab as below{     \"Version\": \"2012-10-17\",     \"Statement\": [         {             \"Sid\": \"PublicReadGetObject\",             \"Effect\": \"Allow\",             \"Principal\": \"*\",             \"Action\": \"s3:GetObject\",             \"Resource\": \"arn:aws:s3:::staticwebsite2004/*\"         }     ] }Enter fullscreen modeExit fullscreen mode\ud83d\udcccUpload the index.html and images in S3 bucket\ud83e\udd29 That's it !! You can view the beautiful website below\ud83d\udd17Integration of Chatbot with website hosted on a S3 bucket\ud83d\udcccLaunch the cloud formation stack mentionedhere\ud83d\udcccCopy the bot id and paste it into the Lex V2 bot. Similarly, Copy the alias ID and paste it in the alias version.\ud83d\udcccEnter the web application parameters including web app origin details as below\ud83d\udcccGo to cloud formation stack. Navigate to outputs, Click on the snippet URL, copy the snippet below, and add it to your index.html\ud83d\udccc Refresh the static website and in the console, you can view\"Successfully initiated Lex Web UI version\"\ud83e\udd29 Love it !! Here's goes my chatbot integrating with the website\ud83c\udfa5 Hands-on DemoAmazon Lex Chatbox\ud83d\uddd1\ufe0f Instructions to clean up AWS resources to avoid Billing\u267b\ufe0f Delete the cloud formation stack created\u267b\ufe0f Delete the S3 Bucket created by deleting all the files internally\u267b\ufe0f Delete the chatbot Pizza Order created\ud83d\udd75\ud83c\udffbI also want to express that your feedback is always welcome. As I strive to provide accurate information and insights, I acknowledge that there\u2019s always room for improvement. If you notice any mistakes or have suggestions for enhancement, I sincerely invite you to share them with me.\ud83e\udd29 Thanks for being patient and following me. Keep supporting \ud83d\ude4fClap\ud83d\udc4f if you liked the blog.For more exercises \u2014 please follow me below \u2705!https://www.linkedin.com/in/vjraghavanv/#aws #website #amazonlex #cognito #frontend #cloudfront #s3bucket #awscommunitybuilder #machinelearning #serverless #cloudformation #opensource #chatbot #cncf #awsugmdu #awsugncr #awscommunitybuilder #automatewithraghavan"}
{"title": "Consideration about cdk-notifier and Tags", "published_at": 1713600918, "tags": ["aws", "cdk", "cdknotifier"], "user": "Johannes Konings", "url": "https://dev.to/aws-builders/consideration-about-cdk-notifier-and-tags-4g48", "details": "Use caseAs described hereUse cdk-notifier to compare changes in pull requests, the cdk-notifier displays the diff between the feature branch and the main branch. In case of using tags in the CDK there a two ways to tag resources, which will have different consequences in the diff output of the cdk-notifier.Tagging with Tags.of()The documentation of CDK describes the tagging of resources with theTags.of()method:https://docs.aws.amazon.com/cdk/v2/guide/tagging.htmlThis could look like this:Tags.of(app).add('branch', branchName);Enter fullscreen modeExit fullscreen modehttps://github.com/JohannesKonings/cdk-notifier-examples/blob/746c2b2bc0ecc0ecf3e8f0e6ff771a7430a45d04/src/main.ts#L23The tag will then be added to all resources in the synthesized cloudformation template.{  \"Resources\": {   \"TableCD117FA1\": {    \"Type\": \"AWS::DynamoDB::Table\",    \"Properties\": {     \"AttributeDefinitions\": [      {       \"AttributeName\": \"id\",       \"AttributeType\": \"S\"      }     ],     \"BillingMode\": \"PAY_PER_REQUEST\",     \"KeySchema\": [      {       \"AttributeName\": \"id\",       \"KeyType\": \"HASH\"      }     ],     \"TableName\": \"Table-tags-tags-of\",     \"Tags\": [      {       \"Key\": \"branch\",       \"Value\": \"tags-tags-of\"      }     ]    },    ...   }  } }Enter fullscreen modeExit fullscreen modeBecause the tag is in the template, it will then be shown in the diff.https://github.com/JohannesKonings/cdk-notifier-examples/pull/5Tagging with stack propertiesThe other way is to pass the tags as stack properties (https://docs.aws.amazon.com/cdk/api/v2/docs/aws-cdk-lib.Stack.html#tags-1). This could look like this:new CdkNotfifierFeatureStackExample(app, `cdk-notifier-feature-stacks-${branchName}`, {   tags: {     branch: branchName,   }, });Enter fullscreen modeExit fullscreen modehttps://github.com/JohannesKonings/cdk-notifier-examples/blob/66874c06b8204b09781e9ad3ab8707590b948000/src/main.ts#L23The tag will then be added to the stack properties and not to the template file.{  \"Resources\": {   \"TableCD117FA1\": {    \"Type\": \"AWS::DynamoDB::Table\",    \"Properties\": {     \"AttributeDefinitions\": [      {       \"AttributeName\": \"id\",       \"AttributeType\": \"S\"      }     ],     \"BillingMode\": \"PAY_PER_REQUEST\",     \"KeySchema\": [      {       \"AttributeName\": \"id\",       \"KeyType\": \"HASH\"      }     ],     \"TableName\": \"Table-tags-stack-properties\",    },    ...   }  } }Enter fullscreen modeExit fullscreen modeIncdk.outthe tags are only in themanifest.jsonfile.{   \"version\": \"36.0.0\",   \"artifacts\": {     \"cdk-notifier-feature-stacks-tags-stack-properties.assets\": {       \"type\": \"cdk:asset-manifest\",       \"properties\": {         \"file\": \"cdk-notifier-feature-stacks-tags-stack-properties.assets.json\",         \"requiresBootstrapStackVersion\": 6,         \"bootstrapStackVersionSsmParameter\": \"/cdk-bootstrap/hnb659fds/version\"       }     },     \"cdk-notifier-feature-stacks-tags-stack-properties\": {       \"type\": \"aws:cloudformation:stack\",       \"environment\": \"aws://unknown-account/unknown-region\",       \"properties\": {         \"templateFile\": \"cdk-notifier-feature-stacks-tags-stack-properties.template.json\",         \"terminationProtection\": false,         \"tags\": {           \"branch\": \"tags-stack-properties\"         },         \"validateOnSynth\": false,         ...       }     }   } }Enter fullscreen modeExit fullscreen modeThen it will not be shown in the diff, and the cdk-notifier skip the pull request comment.check the diff to main Deploying with stack postfix main Stack cdk-notifier-feature-stacks-main Hold on while we create a read-only change set to get a diff with accurate replacement information (use --no-change-set to use a less accurate but faster template-only diff) There were no differences  \u2728 Number of stacks with differences: 0  create cdk-notifier report BRANCH_NAME: tags-stack-properties GITHUB_OWNER: JohannesKonings GITHUB_REPO: $(echo JohannesKonings/cdk-notifier-examples | cut -d'/' -f2) time=\"2024-04-20T14:59:48Z\" level=info msg=\"There is no diff detected for tag id diff-to-main. Skip posting diff.\"Enter fullscreen modeExit fullscreen modehttps://github.com/JohannesKonings/cdk-notifier-examples/actions/runs/8765869174/job/24057331666#step:6:55ConclusionIf you want to see the tags in the diff output of the cdk-notifier, you should use theTags.of()method to tag the resources. If not, you can go with the stack properties.CodeJohannesKonings/cdk-notifier-examplescdk-notifier-examplessee description:https://dev.to/aws-builders/use-cdk-notifier-to-compare-changes-in-pull-requests-3o70https://github.com/karlderkaefer/cdk-notifierView on GitHub"}
{"title": "Como conectar Spark e S3 para processamento de arquivos", "published_at": 1713532551, "tags": ["aws", "awscommunitybuilders", "spark", "s3"], "user": "Carlos Filho", "url": "https://dev.to/aws-builders/como-conectar-spark-e-s3-para-processamento-de-arquivos-5cdi", "details": "Neste artigo, mostraremos como conseguimos configurar o Apache Spark e o S3 para lidar com arquivos grandes. Quais problemas foram encontrados e como foram resolvidos.Chaves = IDA ideia \u00e9 desenvolver uma estrutura para automatizar os processos de ETL, que recebe como entrada um arquivo com configura\u00e7\u00f5es para carregamento. A principal ferramenta da estrutura para processamento de dados \u00e9 o Apache Spark. Inicialmente, as fontes de dados eram apenas BDs relacionais (Postgres, Oracle e MS SQL) e Kafka, e o receptor era o Apache Hive.No entanto, algumas fontes n\u00e3o estavam preparadas para fornecer acesso direto a seus bancos de dados. O reposit\u00f3rio S3 tornou-se uma op\u00e7\u00e3o para obter dados desses sistemas que satisfaria a todos: tanto os sistemas de origem quanto eu ou quem quer que seja, como agregadores de informa\u00e7\u00f5es. Essa abordagem funciona da seguinte forma: uma vez em um determinado per\u00edodo (por exemplo, uma vez por dia), o sistema de origem descarrega uma determinada fatia de dados de seu banco de dados e a coloca no S3 como um arquivo. A fatia de dados descarregada pela fonte pode ser completa (todos os dados na tabela) ou parcial (somente dados incrementais). O conte\u00fado da fatia \u00e9 determinado pelos acordos entre o sistema de origem e o de destino, pois depende do algoritmo pelo qual os dados ser\u00e3o processados posteriormente. Assim, foi necess\u00e1rio configurar nossa estrutura para dar suporte \u00e0 replica\u00e7\u00e3o de dados do S3 para o Apache Hive.Observa\u00e7\u00e3o: o c\u00f3digo fornecido no artigo \u00e9 simplificado para evitar a necessidade de descrever a estrutura do projeto.Declara\u00e7\u00e3o do problemaSuponha que tenhamos um armazenamento S3 com v\u00e1rios buckets. Um bucket \u00e9 um cont\u00eainer especial com um ID exclusivo. Cada bucket corresponde a uma fonte.Pode haver v\u00e1rias pastas em um pacote e cada pasta refere-se a um download diferente. Uma pasta pode conter v\u00e1rios arquivos, e arquivos diferentes podem conter dados de tabelas diferentes. Para identificar o pertencimento de um arquivo a uma tabela, o nome do arquivo deve conter o nome da tabela da qual os dados foram extra\u00eddos e o registro de data e hora - a hora da forma\u00e7\u00e3o do arquivo. Al\u00e9m disso, o nome do arquivo pode conter outras informa\u00e7\u00f5es, portanto, uma chave exclusiva para cada solu\u00e7\u00e3o \u00e9 usada para extrair o nome da tabela e a data. Todos os arquivos t\u00eam a extens\u00e3o csv.Por exemplo, os nomes dos arquivos podem ser os seguintes: table_1_20240320_122145.csv e 20240322_231243_workers.csv. Ent\u00e3o, as chaves desses arquivos podem ser representadas da seguinte forma: employee_table_.csv e worker_.csv, respectivamente.Em um download, precisamos ler todos os arquivos de uma pasta por meio de uma determinada chave, process\u00e1-los com os algoritmos dispon\u00edveis (os algoritmos ser\u00e3o descritos nos par\u00e1grafos abaixo) e, em seguida, registr\u00e1-los no Hive na forma de uma tabela. As tabelas resultantes estar\u00e3o prontas para serem usadas pelo usu\u00e1rio para resolver suas tarefas.Estrutura de armazenamento S3 simplificadaA figura mostra a estrutura de armazenamento descrita acima. Deve observar que, no S3, os objetos s\u00e3o fisicamente armazenados em um espa\u00e7o de endere\u00e7o plano, sem estrutura hier\u00e1rquica, na forma de pares de valores-chave. Cada objeto no S3 tem seu pr\u00f3prio identificador exclusivo, pelo qual voc\u00ea pode se dirigir diretamente a ele.No entanto, podemos organizar uma variante familiar de armazenamento de arquivos, imitando um armazenamento de arquivos. Nesse caso, o identificador do objeto se parecer\u00e1 com um caminho no sistema de arquivos, por exemplo, /data/files/file1.txt. N\u00e3o h\u00e1 representa\u00e7\u00e3o f\u00edsica de pastas no S3, mas, para simplificar, podemos chamar formalmente os valores intermedi\u00e1rios de pastas. No exemplo acima, podemos chamar as pastas \"data\", \"pasta\" e \"files\".In\u00edcio do desenvolvimentoPrimeiro, decidi fazer a leitura e o processamento dos dados inteiramente por meio do Spark. Para fazer isso, criei um c\u00f3digo trivial para ler arquivos csv de uma pasta:val delimiter = \";\" val header = \"true\" val path = \"/Pasta1/Table1_*.csv\" spark.read       .format(\"csv\")       .option(\"header\", header)       .option(\"delimiter\", delimiter)       .option(\"inferSchema\", \"true\")       .withColumn(\"filename\", input_file_name)       .load(path)Enter fullscreen modeExit fullscreen modepath(caminho) \u00e9 uma pasta onde est\u00e3o localizados os arquivos necess\u00e1rios e uma chave, pela qual \u00e9 necess\u00e1rio extrair somente os arquivos necess\u00e1rios, pois pode haver arquivos na pasta que perten\u00e7am ao mesmo download, mas a outra tabela na origem;delimiterque \u00e9 o delimitador de coluna no arquivo csv;Defina o par\u00e2metroinferSchemacomo true para a detec\u00e7\u00e3o autom\u00e1tica dos tipos de coluna;Adicionada a coluna\"filename\"com o nome do arquivo no DataFrame, para que ele possa ser usado para descobrir de qual arquivo a linha da tabela foi extra\u00edda.Em seguida, reformulei esse c\u00f3digo para que ele possa ler arquivos do S3:spark   .sparkContext   .hadoopConfiguration   .set(\"fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\") spark   .sparkContext   .hadoopConfiguration   .set(\"fs.s3a.endpoint\", <endpoint>) spark   .sparkContext   .hadoopConfiguration   .set(\"fs.s3a.access.key\",<accessKey>) spark   .sparkContext   .hadoopConfiguration   .set(\"fs.s3a.secret.key\",<secretKey>)  val delimiter = \";\" val header = \"true\" val path = \"/Pasta1/Table1_*.csv\" spark.read       .format(\"csv\")       .option(\"header\", header)       .option(\"delimiter\", delimiter)       .option(\"inferSchema\", \"true\")       .withColumn(\"filename\", input_file_name)       .load(path)Enter fullscreen modeExit fullscreen modePara oferecer suporte \u00e0 leitura do S3, apenas adicionei par\u00e2metros de configura\u00e7\u00e3o. Tudo isso gra\u00e7as ao suporte nativo ao S3 no Spark.Vamos dar uma olhada mais de perto nos par\u00e2metros acima:A implementa\u00e7\u00e3o do sistema de arquivos com o qual queremos trabalhar. H\u00e1 tr\u00eas implementa\u00e7\u00f5es do S3: s3, s3n e s3a. Escolhi para este exemplo o s3a porque ele \u00e9 um sistema de arquivos nativo para leitura e grava\u00e7\u00e3o de arquivos regulares no S3, \u00e9 compat\u00edvel com todos os recursos do s3n, mas tamb\u00e9m \u00e9 mais eficiente e permite arquivos maiores que 5 GB.Ponto de extremidade S3.Chave p\u00fablica para acessar oaccessKey.Chave privada para acessar asecretKey.Tamb\u00e9m precisamos do arquivo JARhadoop-aws-2.7.1no caminho da classe. Esse JAR cont\u00e9m a classeorg.apache.hadoop.fs.s3a.S3AFileSystem, da qual precisamos.Agora o c\u00f3digo para leitura de arquivos do S3 est\u00e1 pronto. Em seguida, o resultado da leitura \u00e9 alimentado como entrada para os algoritmosappend, replace ou scd2 que foram implementados anteriormente para outros tipos de fonte.A primeira execu\u00e7\u00e3o dessa implementa\u00e7\u00e3o n\u00e3o foi bem-sucedida. Apareceu o seguinte erro:Caused by: sun.security.provider.certpath.SunCertPathBuilderException: unable to find valid certification path to requested target.Esse problema ocorre quando seu servidor tem um certificado autoassinado. H\u00e1 duas maneiras de resolver esse problema:Desativar a valida\u00e7\u00e3o do certificado SSL na inicializa\u00e7\u00e3oAdicionar esse certificado \u00e0 lista de certificados JVM confi\u00e1veis.Para a conveni\u00eancia de desenvolver e testar o programa, usei a primeira variante da solu\u00e7\u00e3o. Voc\u00ea pode desativar a verifica\u00e7\u00e3o de certificados no spark-application especificando os seguintes comandos na configura\u00e7\u00e3ospark-submit:spark-submit  --conf spark.driver.extraJavaOptions=-Dcom.amazonaws.sdk.disableCertChecking=true  --conf spark.executor.extraJavaOptions=-Dcom.amazonaws.sdk.disableCertChecking=true \u2026Enter fullscreen modeExit fullscreen modeEssa variante da solu\u00e7\u00e3o \u00e9 aceit\u00e1vel apenas durante o desenvolvimento, mas \u00e9 inadmiss\u00edvel produzir uma solu\u00e7\u00e3o desse tipo na produ\u00e7\u00e3o. Por isso, foi necess\u00e1rio configurar a adi\u00e7\u00e3o de um certificado \u00e0 lista de certificados confi\u00e1veis no loop de produ\u00e7\u00e3o. Para fazer isso, foi feito:Exportei um certificado SSL usando um navegador. O nome do arquivo de certificado tem o seguinte nome:cert_name.crt.Criei um reposit\u00f3rio de certificados confi\u00e1vel e adicionei um certificado a ele.keytool -import -alias <any alias>  -file cert_name.crt -keystore <the truststore file name> -storepass <password>Enter fullscreen modeExit fullscreen modeAdicionado o caminho para o armazenamento de certificado confi\u00e1vel criado ao comandospark-submit:spark\u2011submit  \u2011-conf  'spark.executor.extraJavaOptions=\u2011Djavax.net.ssl.trustStore=<the truststore filename>  \u2011Djavax.net.ssl.trustStorePassword=<password>'  \u2011-conf 'spark.driver.extraJavaOptions=\u2011Djavax.net.ssl.trustStore=<the truststore filename>  \u2011Djavax.net.ssl.trustStorePassword=<password>' \u2026Enter fullscreen modeExit fullscreen modeAp\u00f3s a instala\u00e7\u00e3o dos certificados, consegui iniciar o download. Por\u00e9m, no processo de an\u00e1lise e teste da implementa\u00e7\u00e3o descrita, verificou-se que ela tem v\u00e1rios problemas e limita\u00e7\u00f5es:Os arquivos lidos n\u00e3o s\u00e3o contados.Com essa op\u00e7\u00e3o de leitura, pegamos todos os arquivos por uma determinada m\u00e1scara na pasta todas as vezes e n\u00e3o marcamos de forma alguma que j\u00e1 os lemos e que n\u00e3o precisamos process\u00e1-los na pr\u00f3xima vez.O inferSchema funciona por um longo tempo.Em arquivos grandes (mais de 1 GB), h\u00e1 uma queda significativa na velocidade de leitura do arquivo ao usar o par\u00e2metro inferSchema. Al\u00e9m disso, nem todos os tipos de dados foram definidos corretamente.Problemas com arquivos vazios.\u00c9 necess\u00e1rio um entendimento expl\u00edcito dos motivos da presen\u00e7a de arquivos vazios, pois n\u00e3o est\u00e1 claro como lidar com eles: eles devem ser tratados como um erro ou simplesmente n\u00e3o h\u00e1 dados na fonte? Essa quest\u00e3o tamb\u00e9m precisava ser resolvida, pois a estrutura entende a aus\u00eancia de um arquivo como uma tabela vazia na origem. Ao mesmo tempo, o algoritmo scd2 \u00e9 sens\u00edvel a essa situa\u00e7\u00e3o, pois considerar\u00e1 todos os dados irrelevantes e, no dia seguinte, reabrir\u00e1 os dados novamente.O algoritmo SCD2 n\u00e3o est\u00e1 funcionando corretamente.O algoritmo SCD2 \u00e9 um tipo de algoritmo SCD (Slowly Changing Dimensions). O SCD \u00e9 um mecanismo para rastrear altera\u00e7\u00f5es nos dados de medi\u00e7\u00e3o em termos de um data warehouse.O SCD2 usa a adi\u00e7\u00e3o de uma nova linha e colunas adicionais. Essa abordagem preserva a historicidade. Al\u00e9m disso, tem como op\u00e7\u00e3o adicionar colunas de servi\u00e7o que podem ser respons\u00e1veis pelo controle de vers\u00e3o, status e intervalo de tempo durante o qual essas linhas podem ser consideradas relevantes.O algoritmo SCD2 pressup\u00f5e a entrada de uma fatia completa de dados de uma fonte e, em seguida, compara esses dados com o receptor para ver se alguma coluna foi atualizada. No caso de arquivos, podemos ter v\u00e1rios arquivos para a mesma tabela ao mesmo tempo, o que significa que, nesse caso, temos v\u00e1rias fatias da tabela em intervalos de tempo diferentes, o que gera um grande n\u00famero de duplicatas. Por esse motivo, o algoritmo n\u00e3o consegue entender como construir a historicidade, qual linha era anterior e qual era posterior.Contagem de arquivos lidosNo in\u00edcio do desenvolvimento, n\u00e3o tinha nenhum requisito para excluir arquivos lidos, portanto, n\u00e3o os exclu\u00edmos. Consequentemente, houve um problema: como entender quais arquivos j\u00e1 foram lidos e processados no lan\u00e7amento anterior e quais n\u00e3o foram e precisam ser processados agora. Comecei a procurar uma solu\u00e7\u00e3o para esse problema usando ferramentas spark, mas n\u00e3o consegui encontrar essa op\u00e7\u00e3o. Por isso, decidi n\u00e3o ler os arquivos por chave diretamente pelo Spark, mas primeiro usar um mecanismo separado para encontrar os nomes dos arquivos que precisamos processar e, em seguida, alimentar a lista desses arquivos na entrada do Spark.Para esse fim, decidi usar a classe de sistema de arquivosorg.apache.hadoop.fs.FileSystem. Esta \u00e9 uma classe base abstrata para um sistema de arquivos bastante gen\u00e9rico. Ele pode ser implementado como um sistema de arquivos distribu\u00eddo ou como um sistema \"local\", refletindo uma unidade conectada localmente. Ela pode ser usada para manipular arquivos com facilidade: navegar pelo conte\u00fado das pastas, pesquisar arquivos por chaves (ID), excluir e mover arquivos individuais.Para criar uma implementa\u00e7\u00e3o do S3, precisamos passar o URI do nosso armazenamento S3 e sua configura\u00e7\u00e3o como um argumento. J\u00e1 temos uma descri\u00e7\u00e3o desses par\u00e2metros na cl\u00e1usula \"Start Development\". Especificamos par\u00e2metros semelhantes para o FileSystem.val conf = new Configuration() conf.set(\"fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\") conf.set(\"fs.s3a.endpoint\", \"<endpoint>\") conf.set(\"fs.s3a.access.key\", \"<accessKey>\") conf.set(\"fs.s3a.secret.key\", \"<secretKey>\")Enter fullscreen modeExit fullscreen modeAp\u00f3s as configura\u00e7\u00f5es acima, podemos criar uma classe para manipular o sistema de arquivos. Para fazer isso, vamos primeiro compor um URI para a conex\u00e3o, que consiste no prefixos3a://e no nome do bucket no S3:val bucketPath = s\"s3a://${bucketName}/\" val s3Path = new URI(bucketPath) val s3FileSystem = FileSystem.get(s3Path, conf)Enter fullscreen modeExit fullscreen modeAgora podemos extrair os nomes dos arquivos pela chave (ID) fornecida:s3FileSystem.globStatus(new Path(s\"$directory/$fileMask\")).filter(_.isFile)No entanto, isso \u00e9 apenas metade da solu\u00e7\u00e3o. Agora, como descobrir quais dos arquivos recebidos j\u00e1 foram processados e quais n\u00e3o foram? A primeira op\u00e7\u00e3o era ler os nomes dos arquivos diretamente da tabela Hive, onde gravamos o conte\u00fado desses arquivos, mas essa op\u00e7\u00e3o tem dois problemas:Velocidade de obten\u00e7\u00e3o da lista de arquivos lidos. Para obter os arquivos lidos, precisamos acessar a tabela e obter essa lista por meio de distintos. Mas temos uma grande quantidade de dados armazenados nas tabelas, o que pode tornar o trabalho do programa muito lento.Quando alguns algoritmos (por exemplo, scd2) est\u00e3o em execu\u00e7\u00e3o, novos dados podem estar faltando, portanto, o nome do arquivo n\u00e3o ser\u00e1 gravado na tabela, o que significa que processaremos esse arquivo v\u00e1rias vezes, o que \u00e9 incorreto.Por isso, foi decidido gravar em um arquivo separado no hadoop a lista de arquivos lidos depois de carregar os dados com o spark. O c\u00f3digo desse registro \u00e9 o seguinte:val dirPath = \"<caminho para a pasta com a lista de arquivos lidos>\"  val tempPath = s\"${dirPath}_tmp\" val dfNewReadFiles = listFiles.toDF(\"filename\") val dfAllReadFiles = if (dirExists(dirPath)) {    spark     .read     .parquet(dirPath)     .write     .mode(SaveMode.Overwrite)     .parquet(tempPath)   dfNewReadFiles union spark.read.parquet(tempPath) } else dfNewReadFiles  dfAllReadFiles.repartition(1).write.mode(SaveMode.Overwrite).parquet(dirPath) fs.delete(new Path(tempPath), true)Enter fullscreen modeExit fullscreen modeAqui registramos tanto os arquivos lidos anteriormente quanto os lidos e processados na itera\u00e7\u00e3o atual.Agora podemos obter facilmente a lista de arquivos n\u00e3o processados com o s3. Vamos refinar o c\u00f3digo de leitura de arquivos com o s3 acima, adicionando um filtro nos arquivos j\u00e1 lidos:val readedFiles =   app.spark.read     .parquet(readedFilesDirPath)     .collect     .map(_.getString(0))     .toList s3FileSystem.globStatus(new Path(s\"$s3DirectoryPath/$fileMask\"))       .filter(_.isFile)       .map(file => s\"$s3DirectoryPath/${file.getPath.getName}\")       .diff(readedFilesDirPath)       .toList       .sortedEnter fullscreen modeExit fullscreen modeAbandono do inferSchema em favor da especifica\u00e7\u00e3o de tipos de colunaConforme mencionado acima, oinferSchematorna o trabalho do programa mais lento e nem sempre define os tipos corretamente. Isso se deve ao fato de que, para determinar o tipo de uma coluna ao especificar esse par\u00e2metro, o spark l\u00ea o arquivo de entrada pelo menos uma vez em sua totalidade para determinar os tipos e, em seguida, l\u00ea o arquivo com o esquema resultante para processamento posterior. Al\u00e9m disso, uma coluna pode n\u00e3o conter nenhum valor. Nesse caso, n\u00e3o fica claro para o programa qual tipo definir, e ele escolhe o tipo mais seguro e mais amplo:string.Por esses motivos, a melhor solu\u00e7\u00e3o foi recusar o uso do par\u00e2metroinferSchemae, em vez disso, especificar diretamente as colunas e seus tipos. Para isso, foi decidido especificar \u00e0 for\u00e7a o par\u00e2metro columns nos argumentos do nosso programa, que ser\u00e1 obrigat\u00f3rio para os carregamentos do s3. Esse par\u00e2metro \u00e9 uma lista de pares: coluna e seu tipo.Em seguida, vamos corrigir o c\u00f3digo de leitura de arquivos com colunas:val delimiter = \";\" val header = \"true\" val path = \"/Pasta1/Table1_*.csv\" val columns = List((\"name\", StringType), (\"age\", IntegerType), ...) val customSchema = StructType(   columns     .map(column => StructField(column._1, column._2, true))) spark.read       .format(\"csv\")       .option(\"header\", header)       .option(\"delimiter\", delimiter)       .option(\"inferSchema\", \"true\")       .schema(customSchema)       .withColumn(\"filename\", input_file_name)       .load(path)Enter fullscreen modeExit fullscreen modeProcessamento de arquivos vaziosPrimeiro, vamos considerar o quanto \u00e9 importante para nossos algoritmos que um arquivo vazio seja inserido.Para o algoritmoreplace, que executa uma substitui\u00e7\u00e3o completa dos dados no receptor, um arquivo vazio simplesmente excluir\u00e1 todos os dados do receptor e produzir\u00e1 uma tabela vazia. Para o algoritmoappend, um arquivo vazio n\u00e3o far\u00e1 exatamente nada e os dados permanecer\u00e3o inalterados. Mas, para o SCD2, isso significa que n\u00e3o h\u00e1 mais dados na fonte e todas as linhas reais existentes devem ser marcadas como exclu\u00eddas.Foi decidido que os arquivos vazios n\u00e3o deveriam ser processados pela estrutura e deveriam ser considerados err\u00f4neos, porque a probabilidade de a tabela estar realmente vazia na origem \u00e9 m\u00ednima, e \u00e9 mais prov\u00e1vel que um arquivo vazio seja um erro de leitura de dados da origem. Al\u00e9m disso, essa conclus\u00e3o tamb\u00e9m foi feita com base nas consultas realizadas pelos seguintes motivos:Quando o algoritmo \u00e9 substitu\u00eddo, \u00e9 melhor que pelo menos alguns dados, mesmo que n\u00e3o estejam totalmente atualizados, permane\u00e7am. Nesse caso, cada linha \u00e9 marcada com a data e a hora de seu carregamento, o que permite determinar sua relev\u00e2ncia sem erros;Ao executar o algoritmo SCD2, \u00e9 necess\u00e1rio manter os dados atualizados. Se um arquivo vazio chegar, todas as linhas atuais ser\u00e3o marcadas como exclu\u00eddas. Ent\u00e3o, quando um novo arquivo n\u00e3o vazio chegar, novas linhas reais com os mesmos dados aparecer\u00e3o, causando um \"buraco\" no qual todos os dados eram irrelevantes. Essa situa\u00e7\u00e3o \u00e9 cr\u00edtica para os usu\u00e1rios de dados.Por esses motivos, adicionei uma verifica\u00e7\u00e3o do vazio de cada novo arquivo antes de ler os pr\u00f3prios arquivos. Se um arquivo estiver vazio, n\u00e3o o processamos, mas simplesmente o adicionamos \u00e0 lista de arquivos lidos, de modo que eles n\u00e3o sejam mais levados em considera\u00e7\u00e3o.// filePaths - lista de caminhos de arquivos no S3 // read - fun\u00e7\u00e3o para ler dados de um arquivo em um DataFrame usando spark val filesFromSource =   filePaths     .map(filePath =>           (            read(filePath).withColumn(\"filename\", input_file_name),             filePath          )     ) filesNamesForLoad =   filesFromSource     .filterNot(x => x._1.isEmpty)     .map(_._2)Enter fullscreen modeExit fullscreen modeAgora ficamos apenas com os arquivos de dados, que s\u00e3o enviados para processamento pelo algoritmo.Algoritmo SCD2 para arquivosNo par\u00e1grafo \"In\u00edcio do desenvolvimento\", descrevi o problema de o algoritmo SCD2 trabalhar com v\u00e1rios arquivos, portanto, n\u00e3o poderemos processar dados de arquivos com o algoritmo SCD2 cl\u00e1ssico. Precisamos modificar nosso algoritmo da seguinte forma:Leremos os arquivos sequencialmente em ordem cronol\u00f3gica. Determinaremos essa ordem pela data e hora especificadas no nome do arquivo (falamos sobre isso no par\u00e1grafo \"Declara\u00e7\u00e3o do problema\"). Foi poss\u00edvel obter o tempo de modifica\u00e7\u00e3o do arquivo a partir das metainforma\u00e7\u00f5es, mas essa abordagem pode quebrar a cronologia se, por algum motivo, o arquivo for recarregado depois de um arquivo com uma fatia mais recente da tabela de origem.Aplicaremos o algoritmo SCD2 cl\u00e1ssico aos dados de cada arquivo. Nesse caso, ele funcionar\u00e1 corretamente porque os arquivos v\u00eam em ordem cronol\u00f3gica.Vamos considerar o c\u00f3digo do novo algoritmo. O m\u00e9todoread()agora tem a seguinte apar\u00eancia:def read(filesList: List[String]): List[DataFrame] = {     filesList.map(filename =>           spark             .read             .format(\"csv\")             .option(\"header\", header)             .option(\"delimiter\", delimiter)             .load(filename)             .withColumn(\"filename\", input_file_name)         )   }Enter fullscreen modeExit fullscreen modeRecebemos uma lista de nomes de arquivos como entrada, os lemos individualmente, adicionamos o nome do arquivo como uma coluna adicional e retornamos uma lista de DataFrames. Cada um dos DataFrames cont\u00e9m dados de um \u00fanico arquivo. Essa lista \u00e9 ent\u00e3o passada como entrada para o algoritmo SCD2:val fileNamesIterator = filesList.iterator val sourceDataframes: List[DataFrame] = read(filesList) val countFiles = sourceDataframes.length sourceDataframes.map { data =>   fileName = fileNamesIterator.next()   val transformedData = scd2run(data)   val status = load(transformedData)   status }Enter fullscreen modeExit fullscreen modeNesse trecho de c\u00f3digo, lemos os arquivos em sourceDataframes, processamos a lista de DataFrames recebidos um a um e enviamos cada um deles para o m\u00e9todo scd2run para processamento e, em seguida, gravamos os dados no receptor.Conclus\u00e3oVimos como configurar a intera\u00e7\u00e3o entre o Apache Spark e o S3 para o processamento de arquivos, que conseguimos implementar em um projeto exemplo aqui. Este artigo pode ajud\u00e1-lo a lidar com problemas semelhantes e a configurar o S3 mais rapidamente."}
{"title": "Adding flexibility to your deployments with Lambda Web Adapter", "published_at": 1713516487, "tags": ["lambdawebadapter", "lambda", "fargate", "flexiblearchitecture"], "user": "Zied Ben Tahar", "url": "https://dev.to/aws-builders/adding-flexibility-to-your-deployments-with-lambda-web-adapter-42m2", "details": "Lambda Web Adapter(LWA) is an open-source project that enables running Web apps on Lambda functions without the need to change or adapt the code.In my opinion, LWA opens up interesting pathways for architecture evolution: While it\u2019s an interesting tool that helps lift & shift Web apps and APIs to Lambda functions without a lot of effort, it can also enable another migration path : Start deploying your application in Lambda as a Lambdalith and then transition to a classical container deployment when needed (e.g.ECS Fargate). In some scenarios, it may happen that you don\u2019t have enough data to decide whether it\u2019s better to host on Lambda or on Fargate. LWA contributes by adding portability to your deployments.In this article, we\u2019ll explore how to use LWA with CDK to simplify the deployment of your Web apps in Lambda and how to easily transition to ECS Fargate.How Lambda Web adapter works ?LWA is a Lambda extension. It creates an independent process within the Lambda execution environment that listens for incoming events and forwards them to your HTTP server.LWA can integrate with invocations from Lambda function URLs (FURL), ALB, and API Gateway, converting invocation JSON payloads into HTTP requests that web frameworks like fastify or ASP.NET can handle. LWA also supports non-HTTP triggers (e.g.SQS, S3 notifications), but in these cases, it acts as a pass-through without converting the invocation payload.LWA supports functions packaged as zip as well as Docker or OCI images.Solution overviewLet\u2019s have a look on how we\u2019ll create our flexible deployment using CDK. In this example we\u2019ll be focusing on deploying a public Web application usingfastifyas a Web framework:My objective is to create a CDK construct supporting two deployment strategies:LambdaorECSFargate. Depending on the selected strategy, only the required components will be deployed:When in Lambda mode, we\u2019ll configure the function to use LWA extension. We\u2019ll also configure a REST API Gateway with lambda proxy integration.When the deployment mode is ECSFargate, We will deploy our application in an ECS Fargate service that is exposed via an ALB.In both of these deployment strategies, users access the Web app through CloudFront. We will associate a WAF Web ACL to restrict access to both the API Gateway and the Application Load Balancer. These origins will only respond to requests that include a custom verification header added by the CloudFront distribution. This approach prevents bypassing the CloudFront distribution to access the origin directly.\u261d\ufe0fSome notes:When deploying in Lambda, I ruled-out the use of FURL or HTTP API Gateway:With FURL, while you can setup Origin Access Control (OAC) with CloudFront, at the time of writing,PUT and POST operations require the client to sign the request payload.HTTP API Gateway does not support WAF. An alternative solution would involve creating a Lambda@Edge to verify the presence of the custom header in the request.To improve security, the custom verification header can be stored in secrets manager withrotationenabled so that the header can be updated as well as the origin WAF and the CloudFront distribution configurations.Let\u2019s see the codeHere are the relevant parts of the solution:1- Deploying fastify Web app on Lambda using LWACreating a new fastify project is a breeze, I generally go withtypescript; for the purpose of this article, I will create one super basic api:import fastify from \"fastify\";      const server = fastify();      server.get(\"/health\", async () => {       return \"yup ! I am healthy\";     });      server.get(\"/where-are-you-deployed\", async () => {       return {         \"i-am-deployed-on\": process.env.DEPLOYED_ON,       };     });      server.listen({ host: \"0.0.0.0\", port: 8080 }, (err, address) => {       if (err) {         console.error(err);         process.exit(1);       }       console.log(`Server listening at ${address}`);     });Enter fullscreen modeExit fullscreen modeI will deploy the Lambda function using zip archive and in order to use LWA as a Lambda extension, we\u2019ll need to:Attach the LWA layer to the functionSet the handler to the startup commandrun.shscript. This script starts the fastify Web app. It will be added to the zip package after the code bundling.And lastly, define theAWS_LAMBDA_EXEC_WRAPPERenvironment variable to/opt/bootstrapTheRestApiCDK construct simplifies exposition of the Lambda function:After deployment, you will be able to view in the console the layer associated with the function:2- Defining the alternative deployment on ECS FargateCDK offers anL3 constructto deploy a load balanced ECS service. What I find interesting about this construct is that it hides all the complexity and verbosity of defining such a deployment, while allowing a level of flexibility. Another neat feature is that it can build and pushour containerimage.We\u2019ll make sure to enable HTTPS, for that we\u2019ll create a certificate and associated to the load balancer:Here, I am using the default configuration, but you will want to adapt it to your own requirements.Once deployed, the ECS service looks like this on the AWS consoleYou can find the full definition of the constructhere.3- Handling the two different deployment strategiesThe important bit, the CDK construct that enables flexible deployments:This construct handles two deployment strategiesLambdaorECSFargate.For each strategy, we\u2019ll need to provide a factory function that creates the required resources. These two functions need to be injected from a parent construct and they are lazily evaluated given the selected strategy.We\u2019ll also make sure that the distribution cache policy is disabled for both of these two origins.As an example, the origin of the distribution, should end up looking like this when you select Lambda deployment strategyAnd finally, let\u2019s see how to define the WAF WebACL with a rule that checks the x-origin-header verification header:You can followthis linkfor the complete WebAppDeployment construct definitionFlexible deployment in actionBefore wapping up, let\u2019s call where-are-you-deployed endpoint defined in the sample web app for each strategy.Wrapping upLambda web adapter is certainly not the only tool that helps running full-fledged web apps on Lambda functions, but it simplifies their deployment while supporting architectural evolution.In this article we have seen how to build a CDK construct that offers a way to deploy the same web app on two distinct platforms, we can choose Lambda function or ECS Fargate by specifying a configuration during design time. We can extend this further by enabling the system to automatically redeploy itself, during runtime, on a specific target based on some events or CloudWatch alarms !As always, you can find the full code source, ready to be adapted and deployed here:GitHub - ziedbentahar/flexible-deployments-with-lambda-web-adapterThanks for reading and hope you enjoyed it !ResourcesRestricting access to Application Load BalancersGitHub - awslabs/aws-lambda-web-adapter: Run web applications on AWS LambdaGitHub - aws-samples/amazon-cloudfront-waf-secretsmanager: Enhance Amazon CloudFront Origin\u2026"}
{"title": "Using undocumented AWS APIs with Python", "published_at": 1713509514, "tags": ["cloud", "aws", "api", "python"], "user": "Maurice Borgmeier", "url": "https://dev.to/aws-builders/using-undocumented-aws-apis-with-python-pmn", "details": "It's probably not a surprise that (almost) everything Amazon Web Services offers is accessible through web services. AWS also provides a range of SDKs that make interfacing with and integrating them in your code base\u00a0relatively\u00a0painless.\u00a0These SDKs are sometimes\u00a0a bit\u00a0limited, though.\u00a0There are certain things that you can only do through the AWS console. Some services, like IAM Identity Center (formerly\u00a0known as\u00a0AWS SSO), had a reputation for being console-only for a long time, but that has improved over the last few years.Console-only, in this case, means that there are still services under the surface, but they're undocumented and not accessible through the official SDKs. Often, as is the case with the identity center, there are IAM actions to control access to APIs such assso-directory:ListProvisioningTenants, but no official API documentation or SDK support (at the time of writing this). That API call is what we refer to as an undocumented API. The AWS console uses it, but it's not exposed to customers via the SDK.In this post,\u00a0I'm going to\u00a0show you how to discover these APIs and use them in your scripts. Later, we'll also discuss whether that's a good idea. In my case, I was looking for a way to get the SCIM configuration of our IAM Identity Center from a script. In the console, this information is exposed in the settings. Unfortunately, I\u00a0wasn't able to\u00a0find any API in the documentation that gives me this information.In these cases, it's often a good idea to reverse engineer where the AWS console is getting that info.\u00a0That's\u00a0not super difficult.\u00a0We just hit F12 or right-click and Inspect to open the developer tools and navigate to the Network Tab. I'm using Firefox, it should look similar in other browsers. It should look something like this.At first glance, it's overwhelming, but we'll narrow it down soon. In fact, this is already filtered toXHRrequests (top-right), which means asynchronous requests for additional information to the AWS backend. There are a whole bunch of tracking requests here (e.g., everything with file:\u00a0panoramaroute), which we can ignore. The\u00a0interesting\u00a0requests usually go to endpoints that are similar or identical to the service API namespace. In the case of the IAM Identity Center, there are multiple namespaces, e.g.,sso-adminoridentitystore. In my case, I filtered todomain:sso, which limited the requests to these.Next, we have to\u00a0go through\u00a0these requests and see which one returns the desired information, so click on them and check out the response. In my case, the data I was looking for looks like this. TheTenantIDis\u00a0precisely\u00a0what I need.To use this API in a script, we need to collect\u00a0a bit\u00a0more information from theRequesttab, which shows us how the payload is structured, i.e., the input to the API call. Sometimes, this is empty; in my case, it sensibly needs to know the directory ID to return the provisioning tenants.The other important info is the request metadata in theHeaderstab. Here, we can see the service endpoint URL and which HTTP method is being used. Additionally, we find theContent-Typethat has to be used when we send the payload, oftenapplication/x-amz-json-1.1, and how the need to calculate our authorization header value later (region + IAM namespace).When we scroll down a bit more, we can see the last information that matters, which API method is being called (X-Amz-Target). You may notice that the service namespace doesn't necessarily match what you'd expect from the IAM namespace - it is what it is.Having collected all this information, we can start writing our script to fetch the info programmatically. I'm going to use\u00a0Python\u00a0for that. First, we'll install the following libraries:boto3to read our local credentialsrequestsfor easy HTTP requestsrequests-aws4authto calculate our signatures/authorization header valuepipinstallboto3 requests requests-aws4authEnter fullscreen modeExit fullscreen modeNext, we'll write a script to make the API call. First, we import the necessary dependencies and configure the region our IAM identity center is deployed in as well as its identity store ID, which can be obtained bysso-admin:ListInstances. Afterward, we define which credentials to use for this request. The way it's written, it will use whatever is currently active in your environment when you run the script, but you can also make it use a specific profile by setting theprofile_nameparameter in theboto3.Sessioncall.importjsonimportboto3importrequestsfromrequests_aws4authimportAWS4Auth# Where\u00a0we\u00a0plan to get the info fromregion=\"eu-central-1\"identity_store_id=\"d-99677XXXXX\"# Customize this to select the right credentialsboto_session=boto3.Session()credentials=boto_session.get_credentials()Enter fullscreen modeExit fullscreen modeThe next step is to create the authorizer for our API calls. We need to sign our AWS API calls with theSignature v4 algorithm, which can be a bit annoying to compute. That's why we're using a library for it. Boto3 would create that signature for us if this were an official API.# Provide SignatureV4 request signingauth=AWS4Auth(region=region,service=\"identitystore\",refreshable_credentials=credentials)Enter fullscreen modeExit fullscreen modeFinally, we can make our API call and print out the response. Thisrequests.postcall is\u00a0fairly\u00a0basic and you should recognize all the information from the screenshots above. By passing the dictionary with the data to thejsonparameter, the library will automatically encode it properly. It wouldusuallyalso set the content type to JSON, but AWS requires a specific one, so we\u00a0have to\u00a0overwrite that in the header.# Make the API call based on the collected informationresponse=requests.post(f\"https://up.sso.{region}.amazonaws.com/identitystore/\",headers={\"Content-Type\":\"application/x-amz-json-1.1\",\"X-Amz-Target\":\"AWSIdentityStoreService.ListProvisioningTenants\",},auth=auth,json={\"IdentityStoreId\":identity_store_id},)# Output the responseprint(json.dumps(response.json(),indent=2))Enter fullscreen modeExit fullscreen modeThe output of our script looks something like this.{\"ProvisioningTenants\":[{\"CreationTime\":1713363979.436,\"ScimEndpoint\":\"https://scim.eu-central-1.amazonaws.com/kE370acb0d7-97b3-4383-8d96-example/scim/v2/\",\"TenantId\":\"kE370acb0d7-97b3-4383-8d96-example\"}]}Enter fullscreen modeExit fullscreen modeCongrats, you've used an undocumented AWS API to get information that previously required logging in to the console. You can now go ahead and continue working with that data. Stop! Not now - before you leave, we have to talk about some caveats.Using undocumented APIs comes with some risks. For official APIs, AWS usually sticks to the mantraan API is a promiseand you can rely on them always being available and not changing. Official APIs are very unlikely to break your code down the line. The same can't be said for these undocumented APIs. They may change or be decommissioned atanypoint.\u00a0You get no guarantees, so I highly recommend you\u00a0don't build\u00a0anything mission-critical on top of them.Relying on undocumented APIs should, at best,\u00a0be a temporary solution until AWS releases an official API.\u00a0A decent way to stay up to date on the release of new APIs isawsapichanges.info, which allows you to filter to individual services such asIAM Identity Center.This method doesn't necessarily work forallundocumented APIs.\u00a0Some of them are\u00a0a bit\u00a0nasty and require you to have a front-end session.\u00a0If you're dealing with one of those beasts, I highly recommend you check outJacco Kulman's postabout the same topic. I was lucky that \"my\" API is one\u00a0of those\u00a0that seems to be built to make its way to an official API eventually, which can't be said for all of them.I hope you enjoyed reading this post and learned something new.\u2014 MauriceTitle Photo byStefan SteinbaueronUnsplashSee also Jacco's Post here:Using undocumented AWS APIsJacco Kulman for AWS Community Builders  \u30fb Oct 22 '23#aws#cloud#python#iam"}
{"title": "Using Verified Permissions with Cognito to control access to API endpoints", "published_at": 1713423411, "tags": ["aws", "serverless", "security", "apigateway"], "user": "Arpad Toth", "url": "https://dev.to/aws-builders/using-verified-permissions-with-cognito-to-control-access-to-api-endpoints-39lb", "details": "If we use Cognito User pools as an identity provider, AWS now enables us to configure fine-grained access control to our API Gateway endpoints using Amazon Verified Permissions.1. A recent releaseAWS has recentlypublisheda new feature forCognito. The release introduces the use ofAmazon Verified Permissions (AVP)to securely manage access to REST-typeAPI Gatewayendpoints via aLambdaauthorizer.2. The scenarioBob was thrilled to explore this new feature and quickly implemented it in one of his company's applications. He had previously secured several endpoints usingscopesandaccess tokens. When his manager requested the addition of a new endpoint, Bob chose to implement the new AVP-based access control mechanism.In this post, I will compare two of their API endpoints:GET /scopeandGET /avp(names have been changed for privacy \ud83d\ude09 ). Both endpoints use access tokens to make authorization decisions, but they are secured differently - through scopes and AVP, respectively.3. Protecting API Gateway endpoints with Verified PermissionsBefore diving into the comparison let's review the resources needed to implement this new access control mechanism with Cognito and AVP.3.1. Using the wizardVerified Permissions includes a newwizardthat creates all necessary resources for this feature.The wizard guides us through several steps:Importing the API endpoints we wish to protect by selecting the API and its stage.Choosing the Cognito user pool.Assigning Cognito group users to specific endpoints.The final step involves the wizard provisioning all the resources listed below. Currently, we need to manuallyadd the API Gateway authorizerto the endpoints and thendeploy the APIif using the AWS console.3.2. Verified Permissions resourcesWe'll need the following resources for AVP, all created via the console wizard.Policy storeA container for policies, described with the API ID and stage included.Identity sourceCurrently, the only supported identity provider is Cognito. We specify ourCognito user poolhere withMyApi::Useras the principal type (MyApiis the policies' namespace). We choose whether to use identity or access tokens for authorization and can opt to authorize requests that match a specific app client ID.SchemaThis defines the supported actions and entity types. For example,Useris the principal,get /avpis the action, andApplicationis the resource.PoliciesPolicies are written inCedar languageand determine whether toALLOWorDENYrequests similarly to IAM policies. An example policy looks like this:permit(   principal in MyApi::UserGroup::\"IDENTITY_POOL_ID|avp-group\",   action in [ MyApi::Action::\"get /avp\" ],   resource );Enter fullscreen modeExit fullscreen modeThis policy will allow users in theavp-groupCognito group to call theGET /avpendpoint inMyApi.We can also create policies for individual Cognito users. In this case, we should specify theCognito IDasprincipalin the policy:permit(   principal in MyApi::User::\"IDENTITY_POOL_ID|USER_ID\",   action in [ MyApi::Action::\"get /avp\" ],   resource );Enter fullscreen modeExit fullscreen modeAlthough it allows us to control granular access at the user level, I would still use groups and add the users to them as it seems to provide less management overhead.3.3. Cognito resourcesWe should create auser pooland severalgroups, such as theavp-groupthat contains certain users. This group acts as theprincipalelement in the AVP policy.3.4. Lambda authorizerAPI Gateway calls this function when a request reaches the protected endpoint. The function sends the necessary data to theIsAuthorizedWithTokenVerified Permissions endpoint for authorization.Sample input payload:{accessToken:'ACCESS_TOKEN',policyStoreId:'POLICY_STORE_ID',action:{actionType:'MyApi::Action',actionId:'get /avp'},resource:{entityType:'MyApi::Application',entityId:'MyApi'}}Enter fullscreen modeExit fullscreen modeIsAuthorizedWithTokenwill verify the token and make an authorization decision based on the payload and the available policies.The response object (avpResponse) includes anALLOWorDENYdecision, based on the policies.constavpResponse={'$metadata':{httpStatusCode:200,attempts:1,totalRetryDelay:0// other properties},decision:'ALLOW',determiningPolicies:[{policyId:'POLICY_ID_1'},{policyId:'POLICY_ID_2'}],errors:[],principal:{entityType:'MyApi::User',entityId:'COGNITO_USER_POOL_ID|COGNITO_USER_ID'}}Enter fullscreen modeExit fullscreen modeTheCOGNITO_USER_IDwill match thesubclaim of the access (or identity) token.The Lambda function returns anIAM policy objectreflecting this decision to API Gateway:{principalId:'PRINCIPAL_ID',policyDocument:{Version:'2012-10-17',Statement:[{Action:'execute-api:Invoke',Effect:avpResponse.decision.toUpperCase()==='ALLOW'?'Allow':'Deny',Resource:'RESOURCE'}]},}Enter fullscreen modeExit fullscreen modeAPI Gateway then allows or denies the request.3.5. API Gateway authorizerFinally, we need aLambda authorizerresource linked to the function in API Gateway. This can be set up separately or through the wizard.4. Comparing access control with tokens only and AVPNow let's compare the traditional token-based API access control to the AVP method described above.4.1. Access tokensAccess tokens embed authorization details in thescopeclaim, which API Gateway inherently recognizes. If the scope in the token matches what's defined in the API Gateway authorizer settings, the request is allowed to proceed; if not, it's automatically rejected.However, this approach isn't without its challenges. It requires additional setup steps that can complicate the process.Firstly, we must establish a resource server in Cognito and define the specific scopes you intend to use. Then, during user authentication - essentially when users log into your application - we must ensure that these required scopes are included in the request to the token endpoint. Failure to include these scopes means they won't appear in the access token, potentially blocking access where it's needed.This method can become cumbersome if the system requires a variety of permissions. An alternative is toimplementapre-token generationfunction within Cognito. This function dynamically injects user-specific scopes into the access token based on membership in Cognito groups or according to rules defined in a database.4.2. ID tokensIdentity tokens primarily store user information. API Gateway is capable of validating these ID tokens to confirm user authentication.However, since ID tokens lack authorization data, additional measures are necessary to manage access control. We have two options: deploy a Lambda authorizer that assigns permissions based on user or group identity or handle the authorization logic at the backend. In either scenario, we should maintain an external map of users and their corresponding permissions to ensure secure and efficient access management.4.3. Using AVPAs highlighted earlier, this new feature utilizes a Lambda authorizer. Currently, API Gateway does not directly support authorization using Verified Permissions.Using AVP eliminates the administrative burden associated with managing scopes and custom authorizations. The Lambda authorizer employs standardized code that remains consistent across different users, groups, and policies, simplifying the setup process.AVP enables detailed access control without the reliance on scopes. This method enhances scalability allowing quick integration of new Verified Permissions policies into the policy store. Configuring access for user groups to specific endpoints is straightforward\u2014simply by making selections in the console. Moreover, there's no need for the pre-token generation Lambda functions previously required.In my experience testing API Gateway with AVP authorization, I found it significantly simpler than managing scopes. If AWS integrates native support for Verified Permissions within API Gateway - eliminating the need for a separate Lambda authorizer - it would offer a significant improvement over traditional access control methods in API Gateway.4.4. Access vs ID tokenAs noted earlier, when setting up resources for Verified Permissions, we must specify whether to use ID or access tokens.Both token types can effectively manage access control since Cognito embeds group information into each token type'scognito:groupsclaim. However, it's important to note that this doesn't apply simultaneously.Choosing an ID token during the setup means that attempting to use an access token later will result in an error:ValidationException: Failed to verify IdentityToken: \\ invalid token type: Expected IdentityToken in the IdentityToken \\ parameter but received AccessToken.Enter fullscreen modeExit fullscreen modeThe reverse scenario also holds true - if our configuration specifies an access token AVP will not accept an ID token. Therefore, we should make a clear decision about the token type from the start and consistently use that type for authorization.5. SummaryWe can now enhance the security of API Gateway endpoints using Cognito and Verified Permissions. The required resources can be efficiently provisioned through the wizard available on the Verified Permissions console page.This new method of access control simplifies the developer\u2019s task of setting up endpoint authorization by eliminating the need for configuring scopes and streamlining the authorization process.6. Further readingAuthorization with Amazon Verified Permissions- More examples and details on the featureCreate a new user pool- What the page title saysIsAuthorizedWithToken- The IsAuthorizedWithToken endpoint specificationAmazon Verified Permissions policy stores- Good title for this pageAmazon Verified Permissions policy store schema- Straightforward documentation page titles todayAmazon Verified Permissions policies- About AVP policies"}
{"title": "The Power of Generative AI for Tomorrow's Tech Careers", "published_at": 1713350175, "tags": ["genai", "techcareer", "aws", "ai"], "user": "Adeline Makokha", "url": "https://dev.to/aws-builders/the-power-of-generative-ai-for-tomorrows-tech-careers-50hn", "details": "In today's fast-paced world of technology, there's an incredible force shaking up how we work and what we can achieve: Generative Artificial Intelligence, or GenAI. It's not just another tool; it's like a spark igniting innovation, changing how we see and interact with technology.Imagine this: humans teaming up with super-smart machines, creating endless possibilities. At the core of this change is Generative AI, a type of artificial intelligence that lets machines learn, adapt, and even create things all on their own. Unlike older AI systems that follow strict rules and patterns, Generative AI uses deep learning to come up with brand-new stuff, like pictures, music, stories, and more.The really cool thing about Generative AI is how it gets our creative juices flowing and brings different people together. In the world of tech jobs, it's like a big shake-up, offering new roles and chances. Instead of just doing the same old stuff, professionals can now explore and invent, using GenAI to boost their skills and make their ideas real.What's even better is how Generative AI is opening up technology to everyone. You don't need to be a coding whiz or a data expert to use AI anymore. With easy-to-use tools and platforms, people from all backgrounds can tap into GenAI's power. Whether you're an artist, a musician, a writer, or a scientist, Generative AI welcomes you to a world where creativity has no limits.But there's more: Generative AI is shaking up how we think about work. As robots take over some tasks, it's time for us to team up with them and make the most of new technologies. Instead of worrying about losing our jobs, those who understand Generative AI are leading the charge, driving innovation and change.The future of tech careers isn't about machines replacing us; it's about working together to make amazing things happen. By embracing Generative AI, we can explore new ideas, try out new things, and break free from old job roles. It's a chance to redefine what's possible and make technology work for us, not against us.ConclusionSo let's dive into the world of Generative AI together! Whether you're a seasoned pro or just getting started, there's never been a better time to explore this exciting new frontier. Join us as we unlock the full potential of Generative AI and create a future where innovation knows no bounds.REFERENCEhttps://aws.amazon.com/generative-ai/"}
{"title": "Bedrock(ing) the Kasbah", "published_at": 1713304453, "tags": ["generativeai", "awscommunitybuilder", "buildlearngrow"], "user": "Jenn Bergstrom", "url": "https://dev.to/aws-builders/bedrocking-the-kasbah-3860", "details": "GenerativeAI (GenAI) is all the rage nowadays. And for good reason! It is a powerful capability and is rapidly being integrated into many of the services and tools we use day to day.I've played around with PartyRock (I'm actually running an upskilling gamification event for my company next month that is built on PartyRock), but decided it was past time to investigate Bedrock itself. For those who aren't tracking,PartyRockis a no-code app-generation tool that uses the foundation models available in Bedrock to take natural language prompts from its users and convert them into Large Language Model (LLM) powered apps.Ok, so back to Bedrock...Bedrock is a fully managed service (read,serverless) that AWS made generally available in September 2023. It is designed to make foundation models (FMs) accessible through console or API and to serve as a jump start for users who want to integrate LLMs into their solutions. You can customize the models with your own data so that their responses are tuned more specifically to what you want to see, but you don't have to.Getting access to the foundation modelsThe first thing to note when starting to use Bedrock is that you actually have to specify which FMs you want to be able to use. You accomplish this by visiting the Bedrock console and clicking \"Get Started\", then scrolling down to \"Model access\". Then you'll need to click the \"Manage model access\" button to get to where you can select which models you want to be available in your account.Important note:You will need to be using an account with the correct IAM permissions set in order to manage the model access. What are those permissions, you ask? The easy button is to provide yourself with the managed policyAmazonBedrockFullAccessbut of course that isn't a great way to go about it for production systems. It'll work for experimentation purposes though. This role was created in December 2023 along with theAmazonBedrockReadOnlymanaged policy.Second Important note:If you want to request access to Anthropic's models, you have to provide a use case justifying said access. I didn't feel like doing that (plus I'm not sure a use case of \"Because I wanna play with it\" would be sufficient), so I requested access toTitan Text G1 - Express,Titan Image Generator G1,Jurassic-2 Ultra, andSDXL 1.0. A nice blend of text and image FMs.Third Important note:Access to models that aren't owned by Amazon is not instantaneous. The Titan models I requested showed as \"Access Granted\" immediately. The others took a little longer. Also important to note - even though the models showed as \"Access Granted\" in the Model access screen, they didn't show as available in the Providers screen as quickly.I have some FMs to work with. Now what?For basic experimentation, you can go to the playgrounds. I started with the Image playground, because of course I did. Images are fun!As you can see in the image, there are some configurations you can tune. You can provide a reference image to help the generator do its work. You can choose whether you want to generate a whole new image or edit an existing image. If you choose to generate and image, you can specify things to exclude from the image using the negative prompt. From my experience, the negative prompt is hit or miss; I entered in a few different things to exclude and the generator sometimes listened and sometimes did not. Kind of like my dog Loki!As shown in this image, here's an instance of Lokinotlistening...I like that you can adjust the number of images the generator should offer; you can choose between 1 and 5, and it defaults to 3 in the console or 1 in the API call. You can adjust the prompt strength as well. A higher prompt strength forces higher alignment to the prompt. You can set this value between 1.1 and 10.0, and it defaults to 8.0. You can also set the height and width of the generated image(s), as long as you set within the permitted values identified onthis documentation page. You can also set the seed value if you would like to see similar images run to run.It took me several tries playing with my prompt before I ended up with images that I was happy with. You can see the final image below, and following that I've included the code for the API call that produced it.aws bedrock-runtime invoke-model \\--model-id amazon.titan-image-generator-v1 \\--body \"{\\\"textToImageParams\\\":{\\\"text\\\":\\\"Photo-realistic purple unicorn in space jumping over the moon, with earth visible in the background. Unicorn must have 4 legs and 1 horn.\\\",\\\"negativeText\\\":\\\"There should not be any ground visible in the picture\\\"},\\\"taskType\\\":\\\"TEXT_IMAGE\\\",\\\"imageGenerationConfig\\\":{\\\"cfgScale\\\":9.5,\\\"seed\\\":0,\\\"quality\\\":\\\"premium\\\",\\\"width\\\":512,\\\"height\\\":512,\\\"numberOfImages\\\":3}}\" \\--cli-binary-format raw-in-base64-out \\--region us-east-1 \\invoke-model-output.txtCareful readers may notice that the generator reversed the moon and earth in the image compared to the actual request.The good and the not so goodAs always with image generation, the quality of the creation will be largely dependent upon the quality of the prompt. My prompt was ok, not great, so the image generated was cool, but not perfect. You can learn much more about prompt engineering and how to use it more effectively by reading through theAmazon Bedrock user guidesection on Prompt engineering. There's a bunch of really good examples and recommendations there!So, a short list of the good:Wide range of FMs available to useGood guidance and explanation on use of the FMs embedded into the console itselfGreat ability to invoke the FMs in Bedrock using the APIThe playground is lovely for helping you test out your prompts and tuning before releasing the model to the worldThe credits I have in my account can be applied to Amazon BedrockAnd a short list of the not so good:Claude (Sonnet, Haiku, Instant) isn't available unless you provide a use case. This makes me sad, even though I'm sure it's an Anthropic requirement, not an AWS requirementIf you wish to train (and more importantly use) your own custom model built on a FM, then you have to purchase provisioned throughput, which must be purchased in 1-month or 6-month commitment termsPricing varies widely between the different models. Make sure you checkthe Bedrock pricing pagefor details before you go down the path of deploying a Bedrock-sourced model into productionThat's it. I'm collecting data to train a custom model off of a base model, but don't have enough yet. I'll try to pop back over here once I've had a chance to do that to walk y'all through that process. Until then, go Bedrock the Kasbah, and have some fun integrating GenAI into your applications!"}
{"title": "How I use Ansible to automate routine tasks by running an Adhoc script", "published_at": 1713294813, "tags": ["ansible", "python", "automation"], "user": "Ige Adetokunbo Temitayo", "url": "https://dev.to/aws-builders/how-i-use-ansible-to-automate-routine-tasks-by-running-an-adhoc-script-4174", "details": "As a Platform Engineer, I would like to run a script on over 1000 servers and I do not want to spend the whole day running the script manually. There are times when Engineers are given a task that involves running the script on numerous servers; this process can be automated using Ansible.What is Ansible? Ansible is an open-source automation tool used for application deployment, orchestration, and configuration management. Ansible is designed to simplify complex tasks for instance infrastructure provisioning, configuration management, and software deployment across a large-scale environment. Ansible uses YAML (YAML Ain\u2019t Markup Language) syntax to describe configuration and automation tasks.You are given a task to run a command to detect theTimezoneon 1000 servers because one of the Quality Engineers detected that some transactions are dated in the future and the engineers suspected that some servers might have incorrect timezones. You are given a file that contains the IP addresses of 1000 servers.I will be walking you through the process of achieving this task using Ansible. This is an assumption that you already have an understanding of Python, Ansible, and Bash.Step 1Establish a passwordless SSH connection with the target host, this will enable Ansible to securely login into the target host without having to receive a prompt to input the password.Step 2Add the IP address to the file servers.txt and ensure the IP address is valid and follows the format as it is in the servers.txtStep 3Extract the server IP Address using Python; to dynamically generate the inventory file#!/usr/bin/env python3  import re import json import os  # Get the current directory current_directory = os.getcwd()  # Concatenate the current directory with the file name server_file = os.path.join(current_directory, 'servers.txt')  def read_servers_file(server_file):     \"\"\"Reads the server file and extracts the IP addresses.\"\"\"     ips = []     with open(server_file, 'r') as f:         lines = f.readlines()         for line in lines:             if 'ip_address=' in line:                 match = re.search(r'ip_address=([\\d\\.]+)', line)                 if match:                     ips.append(match.group(1))     return ips  def generate_inventory(ips):     \"\"\"Generates the inventory in JSON format.\"\"\"     inventory = {         '_meta': {             'hostvars': {}         },         'all': {             'hosts': ips         }     }      return inventory  def main():     \"\"\"Main function.\"\"\"     ips = read_servers_file(server_file)     inventory = generate_inventory(ips)     print(json.dumps(inventory))  if __name__ == '__main__':     main()Enter fullscreen modeExit fullscreen modeStep 4The Ansible Playbook below will run a command date on the target servers and will display the output in a file called report.txt--- - name: Extract ctlplane IP addresses and run command on servers   hosts: all   gather_facts: yes   become: yes   remote_user: ec2-user #change this to the remote user   tasks:     - name: Run command on servers and save output locally       ansible.builtin.shell: \"date\"       register: command_output       run_once: yes      - name: Debug command output       ansible.builtin.debug:         msg: \"{{ command_output.stdout }}\"      - name: Create your local file on master node       ansible.builtin.file:         path: \"report.txt\"         state: touch         mode: '0644'       delegate_to: localhost       become: no      - name: Create report.txt file or append to existing file       ansible.builtin.lineinfile:         path: \"report.txt\"         line: \"{{ item }} - {{ command_output.stdout }}\"       loop: \"{{ ansible_play_batch }}\"       delegate_to: localhost       become: noEnter fullscreen modeExit fullscreen modeStep 5To run the Ansible playbook. Use this command ansible-playbook -i dynamic_inventory.py ansible-playbook.ymlStep 6You should have a similar output as the one we have below in the screenshotIn Conclusion, I do hope you find this article useful and interesting. It demonstrates the procedure on how to run an Adhoc script using Ansible by dynamically generating the inventory file. Follow this link below to check the complete code on GitHubhttps://github.com/ExitoLab/example_ansible_playbook_timezone"}
{"title": "Issue 40 of AWS Cloud Security Weekly", "published_at": 1713225917, "tags": ["aws", "security", "cloudsec", "cybersecurity"], "user": "AJ", "url": "https://dev.to/aws-builders/issue-40-of-aws-cloud-security-weekly-265m", "details": "(This is just the summary of Issue 40 of AWS Cloud Security weekly @https://aws-cloudsec.com/p/issue-40<< Subscribe to receive the full version in your inbox weekly).What happened in AWS CloudSecurity & CyberSecurity last week April 8-April 15, 2024?Amazon Route 53 now provides domain name registration services for 18 additional Top-Level Domains (TLDs), with extensions such as .beer, .bid, .bio, .christmas, .contact, .design, .fan, .fun, .law, .llc, .ltd, .pw, .shopping, .ski, .software, .stream, .vote, and .workAmazon EMR on EKS now supports user authentication and authorization process integration with Amazon EKS's upgraded cluster access management features. With this update, Amazon EMR on EKS leverages EKS access management controls to effortlessly acquire the required permissions for executing Amazon EMR applications on the EKS cluster. Previously, Amazon EMR on EKS provided support for IAM authentication and authorization to EKS clusters. Customers were required to set up aws-auth and RBAC (Role-Based Access Control) configurations to allow EMR on EKS access to the EKS cluster. Now, by assigning access permissions for the EKS cluster to the IAM role of EMR, EMR on EKS gains automatic access to the EKS cluster without the need for manual RBAC or aws-auth configuration.Customers utilizing AWS IAM Identity Center (idC) can now benefit from an optimized AWS access portal and efficient shortcut links, allowing them to swiftly navigate to specific destinations within the AWS Management Console based on their permissions.You now have the option to safeguard AWS Lambda URL origins by employing CloudFront Origin Access Control (OAC), which permits access exclusively from specified CloudFront distributions. This approach provides AWS default DDoS protection via AWS Shield Standard but also enables the application of AWS Web Application Firewall (WAF) rules, safeguarding Lambda applications against malicious bots and common web exploits.Amazon Detective has introduced a new functionality aimed at aiding the investigation of threats identified by Amazon GuardDuty's EC2 Runtime Monitoring feature. This expansion enhances Detective's capacity to offer visual representations and contextual information for examining runtime threats directed at EC2 instances.AWS Key Management Service (AWS KMS) has introduced enhanced flexibility, visibility, and pricing options for automated key rotation. The rotation frequency now ranges from every 90 days to up to 7 years (2560 days), and there is an option to trigger key rotation on demand for customer-managed KMS keys. Additionally, you can access history of all previous rotations for any KMS key that has undergone rotation.AWS Transfer Family now offers the capability to import and utilize a trading partner's public, self-signed TLS certificate for transmitting Applicability Statement 2 (AS2) messages to their server via HTTPS. Furthermore, you now have the option to encrypt messages sent to your partner's server using the 3DES cipher. By default, AS2 connectors will encrypt messages using the AES128 cipher unless you specifically opt for 3DES to maintain compatibility with your partner's current AS2 implementation. These functionalities complement the existing AS2 interoperability features of AWS Transfer Family, facilitating seamless connections with trading partners necessitating these particular security configurations.Trending on the news & advisories:LG WebOS vulnerabilities let us gain root access on the TV after bypassing the authorization mechanism.CISA Announces Malware Next-Gen Analysis.AT&T data breach impacts 51 million customers.Google workspace- Protect sensitive admin actions with multi-party approvals.Apple- About Apple threat notifications and protecting against mercenary spyware.Compromise of Sisense Customer Data.CISA Directs Federal Agencies to Immediately Mitigate Significant Risk From Russian State-Sponsored Cyber Threat.Software Supply Chain Security Deep-Dive (Part 1) by Francis (software analyst) and Nipun Gupta.LastPass- Attempted Audio Deepfake Call Targets LastPass Employee.CISA. Lessons from XZ Utils: Achieving a More Sustainable Open Source Ecosystem by Jack Cable.PaloAlto. PAN-OS: OS Command Injection Vulnerability in GlobalProtect.Roku- Unauthorized actors accessed about 15,000 Roku user accounts using login credentials.Former Amazon Security Engineer Sentenced To Three Years In Prison For Hacking Two Decentralized Cryptocurrency Exchanges.FBI PSA- Smishing Scam Regarding Debt for Road Toll Services."}
{"title": "Is the AWS Certified Cloud Practitioner Certification Worth It?", "published_at": 1713197954, "tags": ["aws", "community", "beginners", "certification"], "user": "Damien J. Burks", "url": "https://dev.to/aws-builders/is-the-aws-certified-cloud-practitioner-certification-worth-it-67d", "details": "Table of ContentsIntroductionIs It Worth It?My ExperienceStudy MaterialsExam ExperienceStudy PlanConclusionIntroductionAfter pursuing various comments across social media platforms regarding the AWS Certified Cloud Practitioner (CCP) certification, I felt compelled to share my perspective on the value of obtaining this certification.Is It Worth It?So, on to answer the key question of this entire blog post.Is the CCP truly worth it?In my opinion, the worth of pursuing the AWS CCP certification hinges on your aspirations and existing cloud knowledge. If you aim to specialize in AWS and pursue associate-level certifications, then the CCP is a definite yes. However, if you seek to become a cloud generalist, CompTIA's Cloud+ certification may be a better fit as it covers fundamental cloud concepts applicable across various platforms.My ExperienceBefore undertaking the AWS Certified Developer - Associate (CDA) certification, I opted to take the CCP exam. Priced at a reasonable $100, the CCP ensures a solid grasp of cloud principles, AWS billing and pricing, AWS security, and an overview of AWS services. With a duration of approximately 90 minutes, it's arguably the easiest AWS certification exam available.I strongly recommend it for individuals with minimal cloud experience intending to pursue associate-level certifications, as it serves as an excellent foundation. However, be prepared for a steep learning curve if you dive into associate-level exams without prior cloud knowledge.Study MaterialsFor those interested, based on my experience and research, I have some study material recommendations to help you prepare for the exam:ACloudGuru's AWS CCP CLF-C02 Course: The course provides comprehensive coverage of all knowledge areas outlined in the exam guide, encompassing the four domains: Cloud Concepts, Security and Compliance, Cloud Technology and Services, and Billing, Pricing, and Support.TutorialsDojo Practice Exams: This is the AWS official practice exam for the CCP! I think this is sufficient enough for you to test your knowledge. No need for third-party curatied exams.ACloudGuru has consistently proven to be a valuable resource for AWS exams and labs. The recommended course typically takes around a week to complete, offering approximately 22 hours of content with hands-on labs to familiarize you with the AWS console, especially billing services.While the practice exam incurs a fee of approximately $14.99, it's a worthwhile investment as it closely resembles the actual exam.Exam ExperienceWhen test day arrived, I chose to take the exam at a testing center, which I find more conducive for focusing. I tend to go to the testing center for all of my certification exams.Unlike any other certification exams I've taken, the CCP exam is relatively straightforward; you either know the answer.... or you don't.Unlike other exams where you must assess multiple services and select the optimal solution, the CCP exam requires more straightforward knowledge recall or answers. However, this simplicity has its merits, ultimately sparing candidates from excessive head-banging against the proverbial wall!Study PlanWith consistent effort of course study about 3-4 hours per day, along with taking practice exams post-course completion, candidates can typically pass the exam within 2-3 weeks.ConclusionIn conclusion, the value of the AWS CCP certification depends on your background and objectives. If you seek to delve deeper into AWS without compromising on cloud fundamentals, the CCP is a worthwhile investment. Otherwise, consider redirecting your $100 towards something that aligns better with your goals or exploring alternative cloud certifications.Thank you for reading, and I wish you the best of luck if you decide to take this exam. Until next time, happy studying, and I'll see you in the cloud!Disclaimer:This blog post reflects my personal experiences and opinions."}
{"title": "Spring Boot 3 application on AWS Lambda - Part 3 Develop application with AWS Serverless Java Container", "published_at": 1713194371, "tags": ["aws", "java", "serverless", "springboot"], "user": "Vadym Kazulkin", "url": "https://dev.to/aws-builders/spring-boot-3-application-on-aws-lambda-part-3-develop-application-with-aws-serverless-java-container-2901", "details": "IntroductionIn thepart 2of the series we introduced AWS Serverless Java Container. In this article we'll take a look into how to write AWS Lambda function with AWS Serverless Java Container using Spring Boot 3 version and Java 21 runtime. In the time of writing this article, the newest version ofSpring Boot is 3.2which became available end of November 2023. To use the newer version of Spring Boot (i.e. 3.3) it will maybe be enough to update the version in pom.xml.How to write AWS Lambda with AWS Serverless Java Container using Spring Boot 3.2For the sake of explanation we'll use our Spring Boot 3.2sample applicationand use Java 21 runtime for our Lambda functions.In this application we'll create and retrieveproductsand use DynamoDB as the NoSQL database. You can find the DynamoProductDao implementationhere. We also put Amazon API Gateway in front of it as defined inAWS SAM template.Spring BootProduct Controllerannotated with@RestControllerand@EnableWebMvcdefinesgetProductByIdandcreateProductmethods.@RequestMapping(path = \"/products/{id}\", method = RequestMethod.GET, produces = MediaType.APPLICATION_JSON_VALUE)     public Optional<Product> getProductById(@PathVariable(\"id\") String id) {         return productDao.getProduct(id);     }  @RequestMapping(path = \"/products/{id}\", method = RequestMethod.PUT, consumes = MediaType.APPLICATION_JSON_VALUE)     public void createProduct(@PathVariable(\"id\") String id, @RequestBody Product product) {         product.setId(id);         productDao.putProduct(product);     }Enter fullscreen modeExit fullscreen modeThe key dependency to make it work and translate between Spring Boot 3 (web annotation) model and AWS Lambda is the dependency to the artifactaws-serverless-java-container-springboot3defined in thepom.xml. It's based onServerless Java Containerwhich natively supports API Gateway's proxy integration models for requests and responses, and we can create and inject custom models for methods that use custom mappings.<dependency>       <groupId>com.amazonaws.serverless</groupId>       <artifactId>aws-serverless-java-container-springboot3</artifactId>       <version>2.0.0</version>     </dependency>Enter fullscreen modeExit fullscreen modeIf we look into the whole dependency tree, we'll discover another dependencyspring-cloud-function-serverless-webwhich aws-serverless-java-container-springboot3 requires which is the collaboration effort between Spring and AWS Serverless developers. It providesSpring Cloud Function on AWS Lambdafunctionallity. We'll look deeper into the capabilities of Spring Cloud Function on AWS Lambda in one of my upcoming articles.The simplest way to wire everything together is to define in theAWS SAM templategenericSpringDelegatingLambdaContainerHandlerhandler from the artifactaws-serverless-java-container-springboot3and define the main class of our SpringBoot application (the class annotated with @SpringBootApplication) :Handler: com.amazonaws.serverless.proxy.spring.SpringDelegatingLambdaContainerHandler  Environment:   Variables:     MAIN_CLASS: com.amazonaws.serverless.sample.springboot3.ApplicationEnter fullscreen modeExit fullscreen modeSpringDelegatingLambdaContainerHandler plays the role of proxy, receives all requests and forwards them to the correct method of our Spring BootProduct Rest Controller.Another way to wire everything together is to implement our ownStreamLambdaHandlerwhich is the implementation of the com.amazonaws.services.lambda.runtime.RequestStreamHandlerinterface.Globals:   Function:     Handler: software.amazonaws.example.product.handler.StreamLambdaHandler::handleRequestEnter fullscreen modeExit fullscreen modeStreamLambdaHandler as custom generic proxy receives then all requests and forwards them to the correct method of our Spring BootProduct Rest Controller.In the StreamLambdaHandler we first instantiateSpringBootLambdaContainerHandlerhandlerSpringBootLambdaContainerHandler<AwsProxyRequest, AwsProxyResponse> handler = SpringBootLambdaContainerHandler.getAwsProxyHandler(Application.class);Enter fullscreen modeExit fullscreen modeand then proxy request through it. This approach is preferable in case we intend to implement own logic there, like we'll see with Lambda SnapStart priming in the next article.@Override   public void handleRequest(InputStream inputStream, OutputStream outputStream, Context context)             throws IOException {     handler.proxyStream(inputStream, outputStream, context);  }Enter fullscreen modeExit fullscreen modeIn that step the input streams will be forwarded to the intended method of theProduct Controller. As the last step we need to define this StreamLambdaHandler.java in theAWS SAM template.Then we need to deploy the application withsam deploy -gand to retrieve the existing product we have to invoke the following:curl -H \"X-API-Key: a6ZbcDefQW12BN56WEA7\" https://{$API_GATEWAY_URL}/prod/products/1Enter fullscreen modeExit fullscreen modeConclusionIn this article we took a look into how to write AWS Lambda functions with Java 21 runtime with AWS Serverless Java Container  using Spring 3.2 version. As we explored, we can re-use the Spring Boot Rest Controller. The can either re-use existing SpringDelegatingLambdaContainerHandler or to write our custom RequestStreamHandler implementation (in our case StreamLambdaHandler) which proxies the invocation to the write method of the Spring Boot Rest Controller annotated with @RequestMapping or @GET, @PUT and so on.In the next article of the series we'll measure the cold and warm start times for this sample application including enabling SnapStart on the Lambda function but also also applying various priming techniques like priming the DynamoDB invocation and priming the whole web request."}
{"title": "Secure the software supply chain with AWS Signer for signing and validating OCI artifacts", "published_at": 1713164259, "tags": [], "user": "saifeddine Rajhi", "url": "https://dev.to/aws-builders/secure-the-software-supply-chain-with-aws-signer-for-signing-and-validating-oci-artifacts-58l9", "details": "Introduction:The adoption of containers grows, so does the need to ensure the integrity and authenticity of the software artifacts that make up these applications.Beyond just the container images themselves, organizations are required to secure additional metadata files, such as Common Vulnerabilities and Exposure (CVE) scan results, Software Bill of Materials (SBOM), and Helm charts, that provide critical information about the components and dependencies of their containerized applications.Ensuring the provenance and trustworthiness of these artifacts is essential for maintaining the overall security and compliance of the software supply chain.In this blog post, we will explore how AWS Signer, a fully managed code-signing service, can be used to cryptographically sign and validate not only container images, but also the associated OCI (Open Container Initiative) artifacts that accompany them.Open Container Initiative OCI:The Open Container Initiative (OCI) is a lightweight, open governance structure (project), formed under the auspices of the Linux Foundation, for the express purpose of creating open industry standards around container formats and runtimes. The OCI was launched on June 22nd 2015 by Docker, CoreOS and other leaders in the container industry.The OCI currently contains three specifications: the Runtime Specification (runtime-spec), the Image Specification (image-spec) and the Distribution Specification (distribution-spec). The Runtime Specification outlines how to run a \u201cfilesystem bundle\u201d that is unpacked on disk.AWS Signer:AWS Signer is a fully managed signing service that simplifies the process of signing container images. It integrates seamlessly with ECR, allowing you to sign and verify images easily in your ECR repositories.AWS Signer uses the Notary project, an open-source container image signing tool, to perform the signing and verification operations. Image signing involves creating a cryptographic signature using a private key and the image\u2019s digest.The signature is then attached to the image, and when the image is deployed, the signature is verified using the corresponding public key.If the verification fails, it indicates that the image has been tampered with and should not be deployed.Hands-On example:If you want to implement image signing within our development workflows. It\u2019s important to think about which other OCI artifacts we intend to incorporate alongside our images. Equally critical is ensuring that security is integrated at every stage of the image\u2019s lifecycle, from its initial creation to deployment. For this latter aspect, leveraging signatures within an OCI repository can be advantageous, addressing key container security practices:Establishing ongoing trust as vulnerabilities and configuration requirements evolveValidating the integrity of images to ensure they remain untamperedApproving images and artifacts that align with organizational compliance standards.Consequently, our approach should involve selecting, signing, and managing all relevant OCI artifacts for an image to enhance trust and enhance visibility across the software development lifecycle (SDLC).Let\u2019s check a typical development process and explore how our organization can embrace these practices. Additionally, you can refer to this security blog for further insights on integrating AWS Signer into a pipeline.Initially, developers fetch public images for initial testing and customization on their local machines.Once prepared for building, they upload their build specification files to a shared code repository, such asAWS CodeCommit.A modification in the shared repository triggers a pipeline via AWS CodeBuild. This pipeline utilizes the build specification files to construct and sign an image using** AWS Signer**.Subsequently, the image is uploaded to anOCI-compliantimage repository, such as Amazon Elastic Container Registry (Amazon ECR). Notably, AWS Signer, in conjunction with the underlying Notation CLI client, can be employed with any OCI 1.0 compatible registry.Conclusion:By integrating AWS Signer into your software development lifecycle, you can establish a robust and centralized solution for managing the signing and verification of your containerized applications and their supporting metadata, ensuring the trust and integrity of your software supply chain."}
{"title": "Deploying a Full Stack AWS Architecture Using Terraform: Ensuring High Availability in AWS", "published_at": 1713152178, "tags": ["terraform", "architecture", "aws", "githubactions"], "user": "Ravindra Singh", "url": "https://dev.to/aws-builders/deploying-a-full-stack-aws-architecture-using-terraform-ensuring-high-availability-in-aws-5h31", "details": "In this blog, we will learn how to deploy autoscaling group with an application load balancer,EFS, RDS, Route 53, ACM , WAF  and Cloudwatch  using step-by-step guidesGIT Link: [https://github.com/ravindrasinghh/Zero-to-Hero-Deploying-a-Full-Stack-AWS-Architecture-Using-Terraform/tree/master]we'll delve deeper into the benefits, provide troubleshooting advice, and share insights for an advanced HA setup on AWS.Overview of the ArchitectureOur target architecture, designed for high availability, comprises various AWS services, each with a specific role in the stack:1. DNS Configuration with AWS Route 53Begin with setting up your DNS using AWS Route 53. It will direct traffic to your application, ensuring that the domain name is resolved to the right resources.2. Load Balancing with High AvailabilityImplement the AWS ALB for distributing the traffic evenly across your EC2 instances. The ALB also checks the health of the instances and only routes traffic to the healthy ones.3. Securing the TransmissionUtilize AWS Certificate Manager to handle the SSL/TLS certificates. This is crucial for encrypting data in transit and providing a secure channel for your users.4. Protecting Your ApplicationConfigure AWS WAF to protect your application from common web exploits that could affect availability, compromise security, or consume excessive resources.5. Elasticity with Auto ScalingUse Auto Scaling to maintain application availability and allow the number of Amazon EC2 instances to scale up or down automatically according to conditions you define.6. Persistent Storage Across InstancesEmploy Amazon EFS, which provides a simple interface that allows your application to access shared file storage.7. Database with High AvailabilitySet up Amazon RDS with a master and a replica in different Availability Zones to ensure that your database is highly available and resilient to infrastructure failures.8. Monitoring and AlertingWith AWS CloudWatch, keep an eye on your application's performance and set up alerts for any anomalies that might indicate issues with availability.9. Streamlined Development WorkflowLeverage a CI/CD pipeline using GitHub Actions to enable your developers to integrate and deliver code changes more rapidly.10. Infrastructure as Code with TerraformFinally, use Terraform to script the creation of all these services in AWS. Terraform will allow you to deploy this entire architecture in a repeatable and predictable manner.Advanced Architecture BenefitsResiliency and Redundancy:By distributing resources across multiple Availability Zones, you create a fault-tolerant system that mitigates the risk of a single point of failure.Scalability:Auto Scaling and Elastic Load Balancing respond dynamically to traffic fluctuations, maintaining performance during demand spikes without manual intervention.Security:AWS WAF and AWS Shield provide advanced protection layers against DDoS attacks and unexpected traffic patterns.Automation: Terraform\u2019s infrastructure as code (IaC) approach allows for repeatable builds and simplifies the process of applying changes to the infrastructure with minimal human error.Troubleshooting TipsHealth Checks:Regularly monitor the health of EC2 instances via ALB health checks. Unhealthy instances should be investigated for issues like configuration errors, depleted resources, or failed deployments.Database Failovers:RDS failover events can be triggered by instance or AZ failures. Always monitor your RDS dashboard for failover events and configure alarms to alert you immediately.Latency:High latency can indicate misconfigured Auto Scaling or issues with your database. Tools like AWS X-Ray can help in tracing requests and diagnosing the bottlenecks.If you prefer a video tutorial to help guide you through the setup of deploying a Full Stack AWS Architecture Using Terraform"}
{"title": "Exploring Amazon Bedrock: Harnessing Mistral Large, Mistral 7B, and Mistral 8X7B", "published_at": 1713141324, "tags": ["aws", "ai", "webdev", "python"], "user": "Renaldi", "url": "https://dev.to/aws-builders/exploring-amazon-bedrock-harnessing-mistral-large-mistral-7b-and-mistral-8x7b-akb", "details": "In the evolving landscape of artificial intelligence, large language models (LLMs) like OpenAI's GPT-4 have been transformative, driving significant advancements and previously unattainable capabilities.Initially, OpenAI's GPT series set the industry standard. However, as technology advanced, numerous new models emerged, each with unique strengths and specialized applications. Among these, the models from Mistral, particularly notable for their sophisticated reasoning and multilingual capacities, have marked a substantial progression in AI capabilities.This guide delves into the latest offering from Mistral\u2014Mistral Large\u2014providing insights into its functionalities, performance comparisons, and real-world applications. It\u2019s designed for a broad audience, from seasoned data scientists to developers and AI aficionados.What is Mistral AI?Mistral AI, established in April 2023 by former Meta Platforms and Google DeepMind employees, aims to revolutionize the AI market by delivering robust, open-source LLMs alongside commercial AI solutions. The launch of Mistral 7B in September 2023, a model with 7.3 billion parameters, notably outperformed other leading open-source models at the time, positioning Mistral AI as a frontrunner in open-source AI solutions.Understanding Mistral LargeLaunched in February 2024, Mistral Large stands as Mistral's flagship text generation model, boasting formidable reasoning and multilingual capabilities across several languages including English, French, Spanish, German, and Italian. It excels in code generation and mathematics, outperforming competitors across various benchmarks such as HumanEval, MBPP, and GSM8K.Launching Your Experience with Mistral Large on Amazon BedrockTo use Mistral models through Amazon Bedrock, you first need to access the models on the Amazon Bedrock platform. This involves selecting the specific Mistral AI models you wish to use and requesting access to them via the Amazon Bedrock console under the \"Model access\" section. Once access is granted, you can test the models using features like the Chat or Text in the Playgrounds section of the console.For more programmatic interactions, such as within an application, you can utilize the AWS Command Line Interface (CLI) or the AWS Software Development Kit (SDK). These tools allow you to make API calls to Amazon Bedrock to invoke the Mistral models, specifying parameters such as the prompt, max tokens, and other settings that control the behavior of the model generation.For example, you can send JSON-formatted requests specifying the model ID (like mistral.mistral-7b-instruct-v0:2 for the Mistral 7B model), the prompt, and other generation parameters. This setup enables you to integrate sophisticated AI-driven text generation, summarization, or other language processing tasks directly into your applications, leveraging the robust, scalable infrastructure of AWS. In this example, we'll discuss how we can call it from our Python code and integrate it into an application.You'll need to first start with importing the relevant libraries.These libraries serve distinct functions within Python to facilitate data handling, system operations, and interaction with external services. The json library handles encoding and decoding JSON data, a common format used for data exchange between servers and web applications. The os library provides a portable way of using operating system-dependent functionality like reading or writing to the filesystem, managing directories, and accessing environment variables. The sys module offers access to some variables used by the Python interpreter and functions that interact strongly with the interpreter, such as fetching command line arguments or exiting the program. Lastly, boto3 is the Amazon Web Services (AWS) SDK for Python, enabling Python scripts to perform actions like managing AWS services and resources, automating AWS operations, and directly interacting with AWS services like Amazon S3 and EC2. Together, these libraries are instrumental in generative AI applications that are scalable, interact with the operating system, and integrate with cloud-based services.import json import os import sys import boto3Enter fullscreen modeExit fullscreen modeIf you're already using Amazon Bedrock, the code below should be boilerplate at this point. We initialize the bedrock_runtime with the required AWS parameters to make the API request to the mistral model.bedrock_runtime = boto3.client(     service_name='bedrock-runtime',      aws_access_key_id=os.getenv('aws_access_key_id'),     aws_secret_access_key=os.getenv('aws_secret_access_key'),     region_name='us-west-2' )Enter fullscreen modeExit fullscreen modeWe then configure an API request to invoke an AI language model, specifically the \"mistral.mistral-7b-instruct-v0:2\" model, to generate a short narrative inspired by the evolution of AI. The input_prompt variable sets the thematic prompt for the AI's response. The body of the request is structured in JSON and includes the prompt, a limit of 512 tokens for the response length (max_tokens), and parameters (top_p and temperature) set at 0.9 to influence the diversity and unpredictability of the AI's prose. The accept and contentType fields specify that both the incoming request and the outgoing response are expected to be in JSON format, standard for web APIs. This setup is typical in applications where developers need to utilize machine learning models to automate creative content generation.input_prompt = \"Write a short epic inspired by the evolution of AI.\"  modelId = \"mistral.mistral-7b-instruct-v0:2\" body = json.dumps({     \"prompt\": input_prompt,     \"max_tokens\": 512,     \"top_p\": 0.9,     \"temperature\": 0.9, }) accept = \"application/json\" contentType = \"application/json\"Enter fullscreen modeExit fullscreen modeWe then set up a request to an AI model, specifying several parameters for generating text based on an input prompt. The modelId specifies the particular AI model to use, in this case, \"mistral.mistral-7b-instruct-v0:2\", which is likely a specific configuration of the Mistral 7B model tailored for instruction-based tasks. The body is structured as a JSON object containing the prompt, which is the user's input or question, and settings for the generation process: max_tokens limits the length of the output to 512 tokens, top_p sets the nucleus sampling parameter to 0.9 (filtering the model's token choices to the top 90% cumulative probability), and temperature adjusts the randomness of the output to 0.9, influencing the variety of responses. Both accept and contentType are set to \"application/json\", indicating that the request and response content should be in JSON format, which is a common content type for web API interactions. This setup is typically used to programmatically interact with language models via APIs, sending data in a structured format and expecting a response in a similar format.response = bedrock_runtime.invoke_model(     body=body,     modelId=modelId,     accept=accept,     contentType=contentType )Enter fullscreen modeExit fullscreen modeNext, we take the body of the response object, read it, and then parse the JSON-encoded string into a Python dictionary. After converting the response into a dictionary, we accesses the first element of the outputs list (indexed by [0]). From this element, we retrieve the value associated with the key 'text'. Finally, we then print the extracted text to the console, prefixed by \"Generated Epic:\". This is used to display the result to the user.response_body = json.loads(response['body'].read()) generated_epic = response_body['outputs'][0]['text'] print(\"Generated Epic:\\n\", generated_epic)Enter fullscreen modeExit fullscreen modeAnd it's easy as that! There are many more potential prompts you can put forward. Some of them also include the below:Analyze the sentiment of the following text: {text}Here, we ask it to analyze sentiment and return an appropriate response based on it.Mistral 8X7B is adept at generating code based on high-level descriptions. Here's an example of generating a Python function:Generate a Python function for a function that takes a list of numbers and returns the sum of squaresThat being said, in my opinion it still is a bit lacking for logical processes, so I still stick to using it for multilingual tasks.Practical Applications of Mistral LargeMistral Large's versatility shines in fields ranging from content creation and customer support to programming help and educational resources. Its ability to understand and generate text in multiple languages makes it particularly valuable for global applications. Use it to complement your workloads for translation-related work, particularly in communicating between different stakeholders.Comparative AdvantageMistral Large introduces advanced features like a 32K token context window for processing large texts and the capability for system-level moderation setup. Its API, available under an Apache 2.0 license, allows flexible and broad usage. When compared to other LLMs such as GPT-4, Claude 2, and LLaMA 2 70B, Mistral Large offers competitive performance at a more accessible price point, particularly excelling in reasoning and multilingual tasks. That being said, if performance is your sole focus, it would better to still use the aforementioned leading foundational models, as it still cannot match them.Looking AheadThe roadmap for Mistral Large promises continued enhancements and community-driven improvements, aiming to further refine its reasoning capabilities and broaden its language support. I would very much like to see it offer stronger community support, along with more powerful foundational models to be on par with the leading ones.ConclusionMistral Large represents a significant leap in AI, combining advanced reasoning, multilingual support, and coding prowess in a cost-effective model. It stands out not only for its strong performance but also for leading in several benchmarks, offering a compelling alternative to commercial models like GPT-4."}
{"title": "Applying event filters to AWS Lambda Functions with the AWS CDK", "published_at": 1713113526, "tags": ["aws", "javascript", "tutorial", "devops"], "user": "Thomas Taylor", "url": "https://dev.to/aws-builders/applying-event-filters-to-aws-lambda-functions-with-the-aws-cdk-1op9", "details": "The primary motive for writing this article is to address the common error I repeatedly received while troubleshooting event filters:Invalid filter pattern definition. (Service: AWSLambda; Status Code: 400; Error Code: InvalidParameterValueExceptionFor my case, the goal was to invoke an AWS Lambda function via a DynamoDB stream.What are Lambda Event FiltersLambda event filters allow developers to specifywhich types of records from a stream or queue are submitted to a Lambda. Event filters are included in event source mapping definitions so that Lambda functions are only invoked when the filter criteria is met.DynamoDB TTL deletion event filterMy use case involved invoking a lambda function to archive resources when a DynamoDB TTL expiry was met. Fortunately, the DynamoDB documentationhas a section that describes how to achieve this.The required filter criteria is as follows:{\"Filters\":[{\"Pattern\":{\"userIdentity\":{\"type\":[\"Service\"],\"principalId\":[\"dynamodb.amazonaws.com\"]}}}]}Enter fullscreen modeExit fullscreen modeThis filter patterns suggests that only actions submitted by the service principaldynamodb.amazonaws.comshould be processed by the receiving consumer. This makes sense because the DynamoDB service deletes expired TTL items on our behalf.Adding event filters to Lambda Functions with AWS CDKThe following example demonstrates how to add event filters to a Lambda function using the AWS CDK in TypeScript:import*aspathfrom'path';import{Code,FilterCriteria,Function,Runtime,StartingPosition}from'aws-cdk-lib/aws-lambda';import{AttributeType,StreamViewType,Table}from'aws-cdk-lib/aws-dynamodb';import{DynamoEventSource}from'aws-cdk-lib/aws-lambda-event-sources';consttable=newTable(this,'Table',{partitionKey:{name:'id',type:AttributeType.STRING},stream:StreamViewType.NEW_IMAGE});constlambda=newFunction(this,'Function',{runtime:Runtime.NODEJS_20_X,handler:'index.handler',code:Code.fromAsset(path.join(__dirname,'lambda-handler'))});lambda.addEventSource(newDynamoEventSource(databaseTable,{startingPosition:StartingPosition.TRIM_HORIZON,filters:[FilterCriteria.filter({userIdentity:{type:['Service'],principalId:['dynamodb.amazonaws.com']}})]}))Enter fullscreen modeExit fullscreen modeThe crucial part is the innerfiltersattribute within the event source definition:lambda.addEventSource(newDynamoEventSource(databaseTable,{startingPosition:StartingPosition.TRIM_HORIZON,filters:[FilterCriteria.filter({userIdentity:{type:['Service'],principalId:['dynamodb.amazonaws.com']}})]}))Enter fullscreen modeExit fullscreen modeIt's important to note that thestatic method ofFilterCriteria.filteradds thepatterntop-level attribute and marshals the inner JSON on our behalf.As of April 2024, thefiltersattribute is available on many supported event sources:TheDynamoEventSourcedocumentation definitionTheKafkaEventSourcedocumentation definitionTheKinesisEventSourcedocumentation definitionTheManagedKafkaEventSourcedocumentation definitionTheS3EventSourcedocumentation definitionTheSelfManagedKafkaEventSourcePropsdocumentation definitionTheSqsEventSourcePropsdocumentation definition"}
{"title": "Deep Dive on Amazon Managed Workflows for Apache Airflow Using CloudFormation", "published_at": 1713087814, "tags": ["amazonmwaa", "s3", "cloudformation", "cloudwatch"], "user": "GargeeBhatnagar", "url": "https://dev.to/aws-builders/deep-dive-on-amazon-managed-workflows-for-apache-airflow-using-cloudformation-kkp", "details": "\u201c I have checked the documents of AWS to deep dive on amazon managed workflows for apache airflow using cloudformation. It\u2019s easy to use airflow UI with the python script in dag. The Pricing is based on services usage.\u201dAWS CloudFormation is a service that helps you model and set up your AWS resources so that you can speed less time managing those resources and more time focusing on your applications that run in AWS. You create a template that describes all the AWS resources that you want (like Amazon EC2 instances or Amazon RDS DB instances), and CloudFormation takes care of provisioning and configuring those resources for you.Amazon managed workflows for apache airflow is a managed orchestration service for apache airflow that you can use to set up and operate data pipelines in the cloud at scale. Apache airflow is an open source tool used to programmatically schedule and monitor sequences of processes and tasks referred to as workflows.With Amazon MWAA, you can use apache airflow and python to create workflows without having to manage the underlying infrastructure for scalability, availability and security. It automatically scales its workflow execution capacity to meet your needs and integrates with aws security services to help provide you with fast and secure access to your data.In this post, you will experience a deep dive on amazon managed workflows for apache airflow using cloudformation. Here I have created a vpc, s3 bucket, cloudwatch log group and amazon managed workflows for apache airflow through cloudformation.Architecture OverviewThe architecture diagram shows the overall deployment architecture with data flow, s3, cloudformation, amazon managed workflows for apache airflow, cloudwatch.Solution overviewThe blog post consists of the following phases:Create of S3 Bucket, VPC, Cloudwatch Log Group and Amazon Managed Workflows for Apache Airflow through CloudformationScroll the Parameters Created for VPC and Amazon Managed Workflows for Apache AirflowOutput of Airflow UI with Python Script Tutorial in Dag and Cloudwatch LogsPhase 1: Create of S3 Bucket, VPC, Cloudwatch Log Group and Amazon Managed Workflows for Apache Airflow through CloudformationOpen the cloudformation console and create a stack with template.yaml which creates s3 bucket, vpc, cloudwatch log group and amazon managed workflows for apache airflow.Phase 2: Scroll the Parameters Created for VPC and Amazon Managed Workflows for Apache AirflowPhase 3: Output of Airflow UI with Python Script Tutorial in Dag and Cloudwatch LogsClean-upDelete CloudFormation Stack and S3 Bucket.PricingI review the pricing and estimated cost of this example.Cost of CloudWatch Log = $0.0Cost of Simple Storage Service = $1.0Cost of Virtual Private Cloud = $0.02Cost of Amazon Managed Workflows for Apache Airflow = $0.0Total Cost = $(0.0 + 1.0 + 0.02 + 0.0) = $1.02SummaryIn this post, I showed \u201chow to deep dive on amazon managed workflows for apache airflow using cloudformation\u201d.For more details on AWS CloudFormation, Checkout Get started AWS CloudFormation, open theAWS CloudFormation console. To learn more, read theAWS CloudFormation documentation.For more details on Amazon Managed Workflows for Apache Airflow(MWAA), Checkout Get started Amazon Managed Workflows for Apache Airflow, open theAmazon Managed Workflows for Apache Airflow console. To learn more, read theAmazon Managed Workflows for Apache Airflow documentation.Thanks for reading!Connect with me:Linkedin"}
{"title": "`safeaws` checks your AWS CLI commands before they are run", "published_at": 1713004660, "tags": ["aws", "ai", "cloud", "cli"], "user": "Gabriel Koo", "url": "https://dev.to/aws-builders/safeaws-checks-your-aws-cli-commands-before-they-are-run-3bjl", "details": "As a heavy AWS user, you probably know the struggle of needing to edit an existing AWS resource. Sometimes, the AWS CLI is just so much easier to use than the AWS Console, whether it's more programmatically friendly or the API isn't yet supported in the Console.You end up copying a nicely drafted AWS CLI snippet from StackOverflow, or recently, one generated by a GenAI.Are you that confident the AWS API call would not make any breaking changes? Any potential side effects with your call? Have you familarized yourself with all arguments?This is when I came up with the idea of thesafeawsCLI wrapper:(Btw, I am pronouncing as \u201cSafe\u201d-\u201cW\u201d-\u201cS\u201d.):How does it workIn short, it is a Python CLI wrapper around the famousawsCLI, that does the following:Use it like how you would normally run an AWS CLI command - just prefixawswithsafeintosafeaws.It fetches the CLI documentation withaws <service> <cmd> help, preprocesses it to remove less useful parts of the docs.A prompt is generated using the CLI documentation and the command you want to execute, which is then sent to Amazon Bedrock's Claude 3 model.The model will check for any potential issues/things to note to you;The CLI utility streams the checks to you in your shell.You are prompted to decide whether the AWS CLI command should still be run, after validating the checks.Why pick the Claude 3 model on Amazon Bedrock?First of all, you are usingawsCLI anyway, so it\u2019s handy to further add little configuration to get Amazon Bedrock\u2019s GenAI models called handily in your CLI.Second, some AWS CLI docs are indeed long. For example, the documentation foraws ec2 run-instancescommand is around 17k tokens long. Only the latest models could accept prompts/chats with such a long context - Claude 3 models with their 200k context lengths are more than suitable.Lastly, Claude 3 models are very cost effective and fast, at least when compared to our GPT-4 counterparts. You cannot achieve such a cost/speed ratio with other models.Let\u2019s run it!Download the Python script and put it into your favoritebindirectory, for example:#!/bin/bash# It\u2019s assumed your system Python3 has `boto3` installed.sudocurl https://raw.githubusercontent.com/gabrielkoo/safeaws-cli/main/safeaws.py\\-o/usr/local/bin/safeaws&&\\sudo chmod+x /usr/local/bin/safeawsEnter fullscreen modeExit fullscreen modeBe sure to validate its contents before using it. It\u2019s always a good habit to do so for security:cat/usr/local/bin/safeawsEnter fullscreen modeExit fullscreen modeNow just run your AWS command like you normally do, move your cursor to the beginning (Opt + \u2190on macOS) to prefix theawscommand withsafe:# From > aws ec2 run-instances \\   \u2014-image-id ami-12345678 \\   \u2014-instance-family g6.48xlarge # To > safeaws ec2 run-instances \\   \u2014-image-id ami-12345678 \\   \u2014-instance-family g6.48xlarge  # I made up the responses below, the actual call should be different It looks like you want to launch a new EC2 instance, but please be aware of the following:  1. g6.48xlarge seems to be really computationally powerful. Do you really need such a large instance? It would be very costly.  2. You did not specify \u2014-security-groups in your arguments, the default security group would be used and it might violate your organization\u2019s security policy.  Tokens Usage:  Input: xxx, Output: yyy   Do you want to execute the command? (y/N)Enter fullscreen modeExit fullscreen modeCustomizationThe CLI wrapper could be configured with a few environment variables, namely the choice of AWS profile, which Claude model to use, max tokens as well as temperature.You can also customize the GenAI prompt with your own checks, by modifying thePROMPT_TEMPLATEvariable in the Python script.ConclusionOf course, you won\u2019t expect the GenAI to be 100% of the time right. At least in a few seconds and a very small cost (~US$0.005 per call, assuming each CLI docs is 20k tokens long), you got an industry-leading AI reviewing your world-changing AWS CLI calls - and it could have saved you from the trouble of recovering your wrongly modified/deleted AWS resources.SourceYou may find the source code ofsafeawson my GitHub repo:https://github.com/gabrielkoo/safeaws-cli"}
{"title": "STREAMLINE YOUR CI/CD PIPELINE WITH GITHUB ACTIONS", "published_at": 1712931798, "tags": ["docker", "cicd", "githubactions", "devops"], "user": "MUHAMMAD ABIODUN SULAIMAN", "url": "https://dev.to/aws-builders/automating-docker-deployment-security-and-efficiency-with-cicd-pipelines-using-github-actions-26i5", "details": "In the current dynamic software development landscape, security and efficiency are critical. Companies and developers work hard to swiftly roll out apps while making sure they are safe from security flaws. This is where the industry-leading containerization technology Docker comes into play. By wrapping up applications in containers, Docker makes the deployment process simpler. However, if done manually, managing these containers, creating them, checking for vulnerabilities, and pushing them safely can be difficult and time-consuming. For this reason, it's critical to automate these procedures via a Continuous Integration/Continuous Deployment (CI/CD) pipeline.The Role of CI/CD in Modern Software DevelopmentSoftware release process automation is the goal of Continuous Integration (CI) and Continuous Deployment (CD). Building software that is reliable, safe, and quickly deployable is the aim.Continuous Integration:CI involves merging all developers' working copies to a shared mainline several times a day. The main objectives of CI include:Reducing bugs: Automated testing in CI helps detect and fix bugs quickly, improves software quality, and reduces the time it takes to validate and release new software updates.Improving software quality: Continuous integration leads to significantly reduced assumptions as integration issues are detected and fixed continuously.Continuous Deployment:CD extends CI by automatically deploying all code changes to a testing and/or production environment after the build stage. This means that on top of the automated testing, automated release processes further streamline the development lifecycle. Benefits of CD include:Faster time to market: Accelerated release cycles ensure features reach production faster.Higher release rates: Frequent releases promote smaller, more manageable changes and less deployment risk.Improved customer satisfaction: Continuous delivery of features addresses user feedback more promptly and enhances the user experience.Implementing GitHub Actions for Docker ManagementFrom the first code change to the last production deployment, GitHub Actions is a CI/CD tool that streamlines the software process. Building processes that automatically create, test, and launch Docker containers is a requirement of using GitHub Actions for Docker administration.GitHub Actions Workflow: The \"Complete Docker Workflow\"An extensive dissection of the \"Complete Docker Workflow,\u201d asystem for managing Docker deployments with GitHub Actions, is given in this section. There are multiple stages in this workflow, all designed to improve security and expedite procedures across the container management lifecycle.Workflow Activation TriggersScheduled Runs: Set to trigger daily at 16:21 UTC, this ensures regular updates and checks, keeping the application up to date with the latest base image vulnerabilities addressed.- cron: '21 16 * * *'Enter fullscreen modeExit fullscreen modePush Events: Activates on pushes to specific branches or tags (specified branch and semantic versioned tags). This ensures that all changes undergo rigorous testing and security checks before deployment.push:     branches:       - branch-nameEnter fullscreen modeExit fullscreen modePull Requests: Targets pull requests to thedev-2branch, allowing for automated reviews and tests and ensuring that new code integrations meet quality standards before merging.pull_request:     branches:       - branch-nameEnter fullscreen modeExit fullscreen modeEnvironment ConfigurationUsing environment variables and secrets for configurations like Docker registry credentials secures sensitive information and streamlines the setup process across multiple environments or projects.docker.iois specified as our REGISTRY in this instance.env:   REGISTRY: ${{ secrets.REGISTRY }}   IMAGE_NAME: ${{ secrets.DOCKER_USERNAME }}/${{ secrets.IMAGE_NAME }}Enter fullscreen modeExit fullscreen modeInitial Setup and ConfigurationsCheckout RepositoryUses GitHub's actions/checkout@v3.5.2to clone the repository, with a minimal fetch depth of 1 to speed up the checkout process.- name: Checkout repository         uses: actions/checkout@v3.5.2         with:           fetch-depth: 1Enter fullscreen modeExit fullscreen modeInstall CosignImplements sigstore/cosign-installer@v3.4.0to install Cosign, which is used later to sign Docker images.- name: Install Cosign         uses: sigstore/cosign-installer@v3.4.0Enter fullscreen modeExit fullscreen modeSet up QEMUEmploys docker/setup-qemu-action@v2.1.0to configure QEMU, facilitating the emulation of different architectures which is essential for cross-platform Docker builds.- name: Set up QEMU         uses: docker/setup-qemu-action@v2.1.0Enter fullscreen modeExit fullscreen modeSet up Docker BuildxEngages docker/setup-buildx-action@v2.5.0to set up Docker Buildx, enhancing the ability to perform multi-platform builds directly from a single command.- name: Set up Docker Buildx         uses: docker/setup-buildx-action@v2.5.0Enter fullscreen modeExit fullscreen modeLog in to Docker RegistryUtilizes docker/login-action@v2.1.0for logging into the Docker registry using credentials stored in GitHub secrets.- name: Log in to Docker Registry         uses: docker/login-action@v2.1.0         with:           registry: ${{ env.REGISTRY }}           username: ${{ secrets.DOCKER_USERNAME }}           password: ${{ secrets.DOCKER_PASSWORD }}Enter fullscreen modeExit fullscreen modeExtract Docker MetadataActivates docker/metadata-action@v4.4.0to generate and format Docker image metadata, such as tags, from the environment variables.- name: Extract Docker metadata         id: meta         uses: docker/metadata-action@v4.4.0         with:           images: ${{ env.REGISTRY }}/${{ env.IMAGE_NAME }}Enter fullscreen modeExit fullscreen modeBuilding and Pushing ImagesThe procedure ensures interoperability across various hardware settings by supporting the development of images for many architectures using Docker Buildx and QEMU. Developers' work is made easier by automating the push to registries, freeing them up to concentrate on essential features rather than operational setups. Here,linux/amd64is the chosen OS. Please be aware that you can use multiple operating systems.- name: Build and Push container images         uses: docker/build-push-action@v4.0.0         with:           platforms: linux/amd64           push: true           tags: ${{ steps.meta.outputs.tags }}Enter fullscreen modeExit fullscreen modeSecurity Scanning with TrivyPreventing potential security risks before they arise in production requires integrating Trivy scans to evaluate image vulnerabilities. This scan is essential for keeping a secure deployment as it looks for vulnerabilities at the OS and library levels. If there are multiple tags for the image, you can duplicate the below step and specify each of the tags.- name: Scan Docker image with Trivy (specifying a tag)         uses: aquasecurity/trivy-action@master         with:           image-ref: ${{ env.REGISTRY }}/${{ env.IMAGE_NAME }}:tag           format: 'table'           severity: 'CRITICAL,HIGH'           vuln-type: 'os,library'Enter fullscreen modeExit fullscreen modeDigital Signing of Images with CosignBy confirming the source of the images and signing them using Cosign, GitHub's OIDC integration adds an extra degree of security to ensure that only validated images are released.- name: Sign the images with GitHub OIDC Token (Non-interactive)         run: |           IFS=',' read -ra ADDR <<< \"${{ steps.meta.outputs.tags }}\"           for tag in \"${ADDR[@]}\"; do             echo \"Signing $tag\"             cosign sign --oidc-issuer=https://token.actions.githubusercontent.com --yes \"$tag\"           done         env:           COSIGN_EXPERIMENTAL: \"true\"Enter fullscreen modeExit fullscreen modeComplete CI/CD Pipelinename: Complete Docker Workflow  on:   schedule:     - cron: '21 16 * * *'   push:     branches:       - branch-name     tags:       - 'v*.*.*'   pull_request:     branches:       - branch-name  env:   REGISTRY: ${{ secrets.REGISTRY }}   IMAGE_NAME: ${{ secrets.DOCKER_USERNAME }}/${{ secrets.IMAGE_NAME }}  jobs:   build:     runs-on: ubuntu-latest     permissions:       contents: read       packages: write       id-token: write # needed for signing the images with GitHub OIDC Token     steps:       - name: Checkout repository         uses: actions/checkout@v3.5.2         with:           fetch-depth: 1        - name: Install Cosign         uses: sigstore/cosign-installer@v3.4.0        - name: Set up QEMU         uses: docker/setup-qemu-action@v2.1.0        - name: Set up Docker Buildx         uses: docker/setup-buildx-action@v2.5.0        - name: Log in to Docker Registry         uses: docker/login-action@v2.1.0         with:           registry: ${{ env.REGISTRY }}           username: ${{ secrets.DOCKER_USERNAME }}           password: ${{ secrets.DOCKER_PASSWORD }}        - name: Extract Docker metadata         id: meta         uses: docker/metadata-action@v4.4.0         with:           images: ${{ env.REGISTRY }}/${{ env.IMAGE_NAME }}        - name: Build and Push container images         uses: docker/build-push-action@v4.0.0         with:           platforms: linux/amd64           push: true           tags: ${{ steps.meta.outputs.tags }}        - name: Scan Docker image with Trivy (tag1)         uses: aquasecurity/trivy-action@master         with:           image-ref: ${{ env.REGISTRY }}/${{ env.IMAGE_NAME }}:tag1           format: 'table'           severity: 'CRITICAL,HIGH'           vuln-type: 'os,library'        - name: Scan Docker image with Trivy (tag2)         uses: aquasecurity/trivy-action@master         with:           image-ref: ${{ env.REGISTRY }}/${{ env.IMAGE_NAME }}:tag2           format: 'table'           severity: 'CRITICAL,HIGH'           vuln-type: 'os,library'        - name: Sign the images with GitHub OIDC Token (Non-interactive)         run: |           IFS=',' read -ra ADDR <<< \"${{ steps.meta.outputs.tags }}\"           for tag in \"${ADDR[@]}\"; do             echo \"Signing $tag\"             cosign sign --oidc-issuer=https://token.actions.githubusercontent.com --yes \"$tag\"           done         env:           COSIGN_EXPERIMENTAL: \"true\"Enter fullscreen modeExit fullscreen modeAdvantages over Traditional MethodsThere are several benefits to automating these procedures over more conventional manual ones.Reduced Human Error: Automating repetitive tasks lowers the possibility of human error, including deployment script misconfiguration or omission of stages.Consistency and Reliability: Automation guarantees consistency and repeatability by ensuring that each step is carried out in the same way. This makes the process of developing, testing, and deploying software dependable and predictable.Security: By methodically identifying and addressing security concerns, automated vulnerability scans considerably lower the risk of releasing vulnerable code.ConclusionThis pipeline comprehensively covers all aspects of Docker image management, from build to security checks to deployment, ensuring high standards of automation and security using GitHub Actions. This setup not only automates the build and deployment process but also incorporates critical security practices like scanning and signing images, pivotal for maintaining the integrity and trustworthiness of software in a CI/CD environment."}
{"title": "Using API Gateway mapping templates for direct DynamoDB integrations", "published_at": 1712927710, "tags": ["aws", "serverless", "apigateway", "dynamodb"], "user": "Arpad Toth", "url": "https://dev.to/aws-builders/using-api-gateway-mapping-templates-for-direct-dynamodb-integrations-6il", "details": "Integrating API Gateway directly with DynamoDB can significantly enhance speed by reducing overall response time. Leveraging request and response mapping templates for various DynamoDB operations allows us to remove intermediary Lambda functions in the backend.1. The scenarioI'm developing aweather applicationthat periodically fetches current temperature data from theOpenWeather API. The application incorporates specific business logic to protect my plum, apple, and peach fruits from pests like plum moths,codling moths, and peach twig borers. It achieves this by monitoring growing degree units (GDUs) for each species and notifying me of the likely emergence day for larvae. Consequently, I can apply organic protection measures before they infest my fruits.I store temperature and pest data in aDynamoDBtable, adhering to the principles of thesingle table design. A REST-typeAPI Gatewaysits in front of the backend.2. Eliminating transport functionsI've been exploring architectures that utilize native integrations between AWS services lately. In this context, I aim to avoid provisioningLambda functionssolely for data transmission, following theuse Lambda to transform and not transportprinciple.Consequently, I've opted not to implement Lambda functions between the API Gateway and DynamoDB. My objective is to construct this application with direct service integrations wherever feasible.This adjustment has notably reduced existing endpoint response times to under 200ms on average, sometimes dipping below 100ms.Additionally, utilizing direct integration translates to cost savings by bypassing Lambda function invocations. It also mitigates concerns related to managing Lambda function concurrencies.3. Mapping templatesBy removing the functions from the architecture, it's imperative to instruct API Gateway on processing client input data and configuring its behaviour during data retrieval from the database. This is accomplished usingmapping templates.Mapping templates are composed in theApache Velocity Template Language (VTL)format. While initially challenging to work with, especially for developers accustomed to Lambda functions, they can provide granular control over data flow.However, API Gateway's VTL support is not exhaustive. Successful local tests of a template do not guarantee identical processing by API Gateway. Consequently, ensuring proper template functionality may require additional effort.4. TransactWriteItems operation from API GatewayLet's examine the request mapping template for aPOSTendpoint.In this scenario, I aim to add a new moth with various properties to the database. Because I'd like to retrieve all monitored moths via aGET /mothsrequest later, I want to store their names in a dedicated item in the database.To achieve this, I leverage DynamoDB'sTransactWriteItemsAPI.4.1. InputThe client input data will be similar to the following:{\"newMoth\":{\"PK\":\"MOTH#Cydia pomonella\",\"SK\":\"MOTH#Cydia pomonella\",\"MothName\":\"Cydia pomonella\",\"BaseTemperature\":\"10\",\"EmergingGDU1\":\"104\",// heat accumulation when larvae emerge from the eggs - 1st wave\"MothEng\":\"Codling moth\",// other properties},\"toMothList\":{\"MothName\":\"Cydia pomonella\"}}Enter fullscreen modeExit fullscreen modeThenewMothobject contains moth data, necessitating aPutItemoperation. Simultaneously, thetoMothListobject specifies the moth's name for theUpdateItemoperation, appending it to theMothsMonitoredlist in a dedicatedsingletonitem.4.2. TransactWriteItems request templateTo execute both operations within a single database request without Lambda functions, we can create anintegration request mapping templatein API Gateway:{   \"TransactItems\": [     {       \"Put\": {         \"TableName\": \"Moths\",         \"Item\": {           #set($inputRoot = $input.path('$.newMoth'))           #foreach($key in $inputRoot.keySet())               #set($value = $inputRoot.get($key))               \"$key\": { \"S\": \"$value\" }#if($foreach.hasNext()),#end           #end         },         \"ConditionExpression\": \"attribute_not_exists(PK) AND attribute_not_exists(SK)\"       }     },     {       \"Update\": {         \"TableName\": \"Moths\",         \"Key\": {           \"PK\": { \"S\": \"MOTHS\" },           \"SK\": { \"S\": \"MOTHS\" }         },         \"UpdateExpression\": \"SET #attrName = list_append(if_not_exists(#attrName, :emptyList), :val)\",         \"ExpressionAttributeNames\": {           \"#attrName\": \"MothsMonitored\"         },         \"ExpressionAttributeValues\": {           \":val\": { \"L\": [{ \"S\": \"$input.path('$.toMothList.MothName')\" }] },           \":emptyList\": { \"L\": [] }         }       }     }   ] }Enter fullscreen modeExit fullscreen modeThis template predominantly comprises typical DynamoDBPutItemandUpdateItemcommand codes.However, let's emphasize a few key lines of the VTL part.#set($inputRoot = $input.path('$.newMoth'))extracts the object from the input'snewMothproperty and stores it in theinputRootvariable. We then go over the keys using#foreachand wrap their values inside another object with theSDynamoDB data type.The last thing in thePutpart is that we must addcommasafter each item in the map, except the last one. API Gateway doesn't support trailing commas!That's why the#if($foreach.hasNext()),#endpart is in the code.hasNextreturnstrueif there are more object properties left to iterate over, so in this case, we add the comma. When the logic processes the last item in the collection,hasNextwill befalse, and the conditional block's context (the,) will not run.TheUpdatepart only contains one VTL syntax:\"$input.path('$.toMothList.MothName')\". We access the second input object here and get the value forMothName. We add the moth name to the end of the dedicatedMOTHSitem'sMothsMonitoredlist.4.3. PermissionsEnsuring appropriate permissions is crucial. API Gateway must be granted permission to executePutItemandUpdateItemactions on the designated DynamoDB table.5. GetItemFor theGET /moth?mothName=<MOTHNAME>endpoint, I've created anintegration response mapping templatein API Gateway to transform DynamoDB's moth data into the desired client format.Since a single item is fetched, aGetItemoperation is performed on DynamoDB, necessitating the API Gateway role having aGetItempermission.5.1. Request templateA straightforward request template is crafted to add the string data type descriptor to the DynamoDB payload:{\"TableName\":\"Moths\",\"Key\":{\"PK\":{\"S\":\"MOTH#$input.params('mothName')\"},\"SK\":{\"S\":\"MOTH#$input.params('mothName')\"}}}Enter fullscreen modeExit fullscreen modeThe moth name, extracted from query parameters, will be part of the primary and sort keys.5.2. Response templateDynamoDB responds to theGetItemoperation with a payload similar to this:{\"Item\":{\"PK\":{\"S\":\"Value1\"},\"SK\":{\"S\":\"Value2\"},\"Attribute1\":{\"S\":\"Value1\"},\"Attribute2\":{\"S\":\"Value2\"},// other attributes},\"ConsumedCapacity\":{// ...}}Enter fullscreen modeExit fullscreen modeHowever, the desired client format slightly differs from the database output as I want it to be an object like this:{\"Attribute1\":\"Value1\",\"Attribute2\":\"Value2\",// other attributes}Enter fullscreen modeExit fullscreen modeSo I want a flat object without anyItemor other wrappers. I also remove theSdata type descriptor from the database response. Thirdly, I don't want to returnPKandSKor any other local or global secondary index attributes to the client. I only need data attributes in the client. Lastly, I'll return a short message if the requested item is not saved to the database.To provide the response the client needs, I created the following transformation:#set($inputRoot = $input.path('$')) #if($inputRoot.Item && !$inputRoot.Item.isEmpty())   #set($excludeKeys = [\"PK\", \"SK\"])   #set($resultMap = {})   #foreach($key in $inputRoot.Item.keySet())     #if(!$excludeKeys.contains($key))       #set($dummy = $resultMap.put($key, $inputRoot.Item.get($key).S))     #end   #end   {     #foreach($key in $resultMap.keySet())       \"$key\": \"$resultMap.get($key)\"       #if($foreach.hasNext()),#end     #end   } #else   {\"messsage\": \"No item found\"} #endEnter fullscreen modeExit fullscreen modeWe create a temporary map calledresultMapthat stores the item's key/value pairs without theSdata type descriptor. The#if(!$excludeKeys.contains($key))condition checks if the key is not an index attribute, i.e., notPKorSK. If it's not, we store the key/value pair inresultMap. This way, we can removePKandSK, which don't carry data, from the database response object.The next challenge is to create a JSON object with no comma after the last property. We can achieve it by creating a second #foreach block to iterate overresultMap's keys. Then, we apply the same condition technique to add commas everywhere except after the last element.That's it! We now have two fast endpoints, a write and a read one, which work without provisioning and invoking any Lambda functions!6. ConsiderationsHandling different data types (e.g., numbers, lists, booleans) may necessitate more intricate VTL code. Here, I've opted to represent everything as strings to simplify VTL composition.While writing VTL may initially be time-consuming compared to traditional programming languages, the benefits of expedited endpoints, reduced management overhead, and cost savings make it a compelling choice.7. SummaryImplementing direct integrations between API Gateway and DynamoDB offers expedited performance, decreased costs, and less management overhead. Utilizing VTL syntax enables seamless mapping of payloads to and from the database.8. Further readingInitialize REST API setup in API Gateway- API Gateway setup guideSetting up REST API integrations- Relevant documentation sectionGetting started with DynamoDB- DynamoDB basics and table creation"}
{"title": "Let's make money with Amazon Mechanical Turk!", "published_at": 1712927399, "tags": ["aws", "machinelearning", "money", "cloud"], "user": "Delia Ayoko", "url": "https://dev.to/aws-builders/lets-make-money-with-amazon-mechanical-turk-1bae", "details": "Emma does her assignment on her little study table on a random day after work. Then, acting really curious, she leaves the table and sits beside you while you knit.Emma: \"Last Friday, you talked about a group of people worldwide doing tasks.\"You: \"When was that?\"Emma: \"We were detecting flowers. We had to label the images.\"You: \"Oh yeah. Amazon Mechanical Turk.\"Emma: \"I wanna know more about it. Sounds interesting.\"You: \"Okay. In simple terms, it's a platform provided by Amazon where you can give humans tasks to complete in exchange for money. So, they complete the tasks and they get paid.\"Emma: \"Woah! Means I can have my own money?\" She beams.You: \"Not yet. Not for 5-year-olds.\"Emma: (puts on a sad face). \"But, how does it work?\"You: \"Let's use our example of labeling images we had last week. We put the task out there for human workers to label the images. This task is called a Human Intelligence Task, HIT.\"Emma: \"And how do workers work on it?\"You: \"They just find it on their profiles. They choose what tasks they can do, the rewards they get for achieving the task, and the requirements for working on it.\"Emma: \"Requirements?\"You: \"Yes. Like location and some other conditions. Sometimes, education.\"Emma: \"That's interesting. Are the prices the same for every task?\"You: \"Of course not! Not every HIT is the same, so they should have different prices attached. The same goes for the allocated time. Some tasks are tougher than others and so should have higher prices.\"Emma: \"Cool.\"You: \"And, when you sign up for an account, your account needs review. when you first open your account, you get 100 points. As you achieve tasks, your points increase. The number of points is sometimes required to complete the task.\"Emma: \"Are you an Amazon Mechanical Turk worker?\"You: \"Yes. In my spare time, I complete tasks for some cash.\"Emma: (she beams) \"Woah! That's some cash you've got right there!\"You: \"Oh yes, girl. Saving up to get you nice Christmas toys.\"She dances. You smile.Emma: \"When I grow up, I'll be an Amazon Mechanical Turk Worker too!\"Who am I?Hii! I'mDelia Ayoko. I'm an AWS Community Builder in the Machine Learning Category and the first Cloud Club Captain in Cameroon, at the University of Bamenda. I am also a computer engineering student, data scientist, content creator, Wattpad author, and mentor. I love building models, especially using AWS. If you loved this article, please react to it so that I write another one next week. Thank you!P.S: If you're looking forward to starting a career in machine learning, I have videos where I explain ML concepts on my Instagram. Feel free to check it out and follow me there for more content in your feed! :)"}
{"title": "Adding Cognito Authentication to our Serverless Dash App", "published_at": 1712904475, "tags": ["sam", "cloud", "cognito", "dash"], "user": "Maurice Borgmeier", "url": "https://dev.to/aws-builders/adding-cognito-authentication-to-our-serverless-dash-app-gn8", "details": "In a recent post, I\u2018ve shown you how to add HTTP Basic Authentication to the Serverless Dash app. That may be suitable for development or testing, but for production you typically want a more comprehensive solution. Within AWS this is usually where Cognito enters the scene. Cognito can manage users with its User Pool feature and, crucially, supports federated identities, i.e. Single Sign On via other identity providers such as Active Directory. In this post I\u2018m going to show you how to add this capability to your Serverless Dash App.If you haven\u2018t followed the series, I recommend you at least read the first part, because I\u2018m going to skip some of the background information on the setup.Deploying a Serverless Dash App with AWS SAM and LambdaAdding Basic Authentication to the Serverless Dash AppBuild a Serverless S3 Explorer with DashIn order to authenticate our users through Cognito, we\u2018re going to use thedash-cognito-authpackage that Franck Spijkermanpublished a few years ago and to which I recently started contributing to keep it up to date.pip install dash-cognito-authEnter fullscreen modeExit fullscreen modeAfter installing the package, we need to integrate it in our app. Before we do that, let\u2018s talk a bit about how this works. The library wraps itself around your Dash app and checks each incoming request to see if the user is authenticated. If that\u2018s not the case, it redirects them to a local Cognito authorization endpoint. For unauthenticated users, this will then result in another redirect to the Cognito User Pool Sign-In UI. Here, the user authenticates themselves either trough username and password if they\u2018re native users in the User Pool or by being redirected to a third party identity provider.Once Cognito has verified the identity, it creates an authentication code and redirects the user back to our apps\u2018 authorization endpoint. The endpoint makes a call using its own Cognito credentials to verify the authorization code and access user information. If everything goes well, the session is treated as authenticated, some information is added to it and the user is redirected to the app\u2018s home page.If this sounds like a lot of redirects to you, that\u2018s accurate. Fortunately, this is something the library handles through the integration of the flask-dance package. From your perspective, you just have to configure the Cognito information and the rest happens automatically. Should a user be already logged in to Cognito, the whole process happens transparently and is typically so fast they don\u2018t even notice it.In case they\u2018re not authenticated in Cognito already, they will only be prompted for credentials there or through the third party identity provider. As a developer of the Dash app you can then assume that only authorized users can access the app and you\u2018re able to fetch information about the currently logged in user from the Flask session.With this theory out of the way, let\u2018s see how we can implement it. I have once againcreated a repository with a SAM-based app. The app deploys the required Cognito user pool and app client for authentication and grants the Frontend Lambda permission to describe the User Pool and retrieve some secrets at runtime so that they don\u2018t have to be hardcoded. Almost all the rest happens in the Lambda function.# template.yamlUserPool:Type:AWS::Cognito::UserPoolProperties:UsernameAttributes:-\"email\"UsernameConfiguration:CaseSensitive:falseAdminCreateUserConfig:# Disable self-service signupAllowAdminCreateUserOnly:trueSchema:-Mutable:trueName:\"email\"Required:true-Mutable:trueName:\"name\"Required:trueAutoVerifiedAttributes:-\"email\"UserPoolDomain:Type:AWS::Cognito::UserPoolDomainProperties:Domain:Ref:UserPoolDomainNameUserPoolId:Ref:UserPoolUserPoolClient:Type:AWS::Cognito::UserPoolClientProperties:UserPoolId:Ref:UserPoolAllowedOAuthFlows:-\"implicit\"-\"code\"AllowedOAuthFlowsUserPoolClient:trueAllowedOAuthScopes:-\"phone\"-\"email\"-\"openid\"-\"profile\"CallbackURLs:# Add your custom domain here if you have one-!Sub\"https://${ApiGatewayIdAfterDeployment}.execute-api.${AWS::Region}.amazonaws.com/${ApiGatewayStageName}/login/cognito/authorized\"# For local development, the port varies sometimes-\"http://localhost:5000/login/cognito/authorized\"-\"http://localhost:8000/login/cognito/authorized\"-\"http://localhost:8050/login/cognito/authorized\"-\"http://localhost:3000/login/cognito/authorized\"GenerateSecret:trueLogoutURLs:# Add your custom domain here if you have one-!Sub\"https://${ApiGatewayIdAfterDeployment}.execute-api.${AWS::Region}.amazonaws.com/${ApiGatewayStageName}/\"# For local development, the port varies sometimes-\"http://localhost:3000/\"-\"http://localhost:5000/\"-\"http://localhost:8000/\"-\"http://localhost:8050/\"SupportedIdentityProviders:# TODO: You can also replace this with your third#       party identity provider.-\"COGNITO\"Enter fullscreen modeExit fullscreen modeI\u2019ve converted our previous Zip-based Lambda to a Docker based Lambda, which speeds up our deployment process and gives us a bit more flexibility. If you\u2018d like to learn more about that,check out this blog. As part of the function, I\u2018ve addedfrontend/auth.py, which is a helper module that makes it easy to add Cognito authentication to the Dash app.Let\u2018s talk about the kind of information thedash-cognito-authlibrary needs. In order for it to work, it requires the domain name of the User Pool or whichever hosted UI is used for login. Additionally it requires the app client id and secret that connects Cognito with the application. Especially the secret is something that we don\u2018t want tokeep in an environment variable, so we fetch this information using the AWS SDK, which you can see here.For performance reasons, we cache the results as these credentials are unlikely to change frequently and we want to keep our response times low. Assuming that the environment variablesCOGNITO_CLIENT_ID,COGNITO_REGIONandCOGNITO_USER_POOL_IDare set appropriately, this is almost everything that we need to do here.# frontend/auth.pydefadd_cognito_auth_to(app:Dash)->None:\"\"\"Wrap a Dash app with Cognito authentication.\"\"\"info=get_cognito_info()app.server.config[\"COGNITO_OAUTH_CLIENT_ID\"]=info[\"client_id\"]app.server.config[\"COGNITO_OAUTH_CLIENT_SECRET\"]=info[\"client_secret\"]dca.CognitoOAuth(app=app,domain=info[\"domain\"],region=info[\"region\"],logout_url=\"/logout\")Enter fullscreen modeExit fullscreen modeThe next step happens infrontend/app.py- here we import our module and calladd_cognito_authorization_toon our Dash app object. Additionally, we need to set the Flask secret which will be used to encrypt the cookie-content. This is a value that needs to be identical among your execution contexts, so we can\u2018t really generate it at runtime.# frontend/app.pyfromauthimportadd_cognito_auth_todefbuild_app(dash_kwargs:dict=None)->Dash:dash_kwargs=dash_kwargsor{}app=Dash(# ...)app.layout=html.Div(children=[render_nav(),dash.page_container,],)app.server.secret_key=\"CHANGE_ME\"add_cognito_auth_to(app)returnappEnter fullscreen modeExit fullscreen modeTo keep my life simple, I\u2018ve just hardcoded it here - it would probably be better to store that value in the SSM Parameter store and read it from there at runtime.Let\u2018s see our setup in action.Deploying the app initially is a two-step process. We need to configure a bunch of URLs in the App client, which rely on the API Gateway Id. Unfortunately, there would be a circular reference if I tried to reference the value in the template. To work around that, I created a CloudFormation parameter with a dummy value. After the initial deployment, we need to overwrite this with the actual value, which is an output from the initial deployment.# samconfig.toml# TODO: Update this after the initial deploymentparameter_overrides=\"UserPoolDomainName=\\\"update-me\\\"ApiGatewayStageName=\\\"Prod\\\"ApiGatewayIdAfterDeployment=\\\"update-me\\\"\"Enter fullscreen modeExit fullscreen modeHaving deployed the app, we can now access it in the browser. You\u2018ll notice that we get redirected to the Cognito hosted UI immediately. After logging in, we\u2018re redirected to our app, and it should look roughly like this.For this example app I have disabled self-service-sign-up for the User Pool in the Cognito settings. This means we have to create a user through the Cognito Admin Console. You can change this behavior by updating theAllowAdminCreateUserOnlyparameter toFalseintemplate.yamland redeploying the solution. Beware though, that this is somewhat similar to making your app publicly accessible because everyone can just create an account - this should be intentional, not accidental.If you want to authenticate your users through a 3rd party identity provider, configure that in the user pool and add the new IDP here in the template.yaml to the supported login methods. Once you do that, you could also remove the Cognito entry to allow only log in from the third party IDP.Now that we\u2018re logged in, we may want to be able to log out as well. The library supports this workflow too (WIP). It will create an HTTP endpoint that an authenticated user can send a GET request to. This will then invalidate the session by expiring the cookie and additionally redirect them to the user pools logout path, which will log them out from Cognito. Otherwise they would be logged in again as soon as they refresh the page.With the authentication setup completed, you should now have a decent template for developing your SAM based Serverless Dash app.Go ahead and check itout on Github. Hopefully you learned something new in this post. Also check out the other posts in this series.\u2014 MauriceOther articles in this series:Deploying a Serverless Dash App with AWS SAM and LambdaAdding Basic Authentication to the Serverless Dash AppBuild a Serverless S3 Explorer with Dash"}
{"title": "Protect API Gateway with Amazon Verified Permissions", "published_at": 1712901153, "tags": ["aws", "serverless", "security"], "user": "Jimmy Dahlqvist", "url": "https://dev.to/aws-builders/protect-api-gateway-with-amazon-verified-permissions-3785", "details": "Amazon Verified Permissions (AVP) was presented during re:Inforce 2022.  AVP is a fully managed serverless service that simplifies managing and enforcing application permissions. It uses Cedar policy language, which is one fastest growing policy language at the moment.AVP is the perfect service to use for implementation of you application permissions. It's a great tool when implementing a centralized policy decision point (PDP). Isolation of tenants in a SaaS solution is made easy with AVP.Up until now, protecting your API Gateway API with Amazon Cognito User Pools and AVP has been hard to do. With thisnew feature releasethis is now made easy.In this post we'll look at this new feature and continue my series on securing API Gateway,Secure your API Gateway APIs with Auth0,Secure your API Gateway APIs mutual TLS, andSecure your API Gateway APIs with Lambda Authorizer.Architecture overviewThere are three parts in this setup. Amazon Cognito User Pool, API Gateway and Amazon Verified permissions decision endpoint. Users will call Cognito to logon and get their tokens. The tokens will be sent to API Gateway API in the Auth header, and we'll have a Lambda Authorizer to call AVP to verify the access.The first thing we need to do is create an Cognito User Pool.Cognito User PoolBefore we can continue the first thing we need is an Cognito User Pool. Let's create the pool using the below CloudFormation template.AWSTemplateFormatVersion:\"2010-09-09\"Transform:\"AWS::Serverless-2016-10-31\"Description:Creates the User PoolParameters:UserPoolName:Type:StringDescription:The name of the user poolDefault:my-unicorn-serviceHostedAuthDomainPrefix:Type:StringDescription:The domain prefix to use for the UserPool hosted UI <HostedAuthDomainPrefix>.auth.[region].amazoncognito.comDefault:unicorn-serviceCallbackDomain:Type:StringDescription:The domain used for signin callbackDefault:localhost:8080Resources:###########################################################################  UserPool##########################################################################UserPool:Type:AWS::Cognito::UserPoolProperties:UsernameConfiguration:CaseSensitive:falseAutoVerifiedAttributes:-emailUserPoolName:!RefUserPoolNameSchema:-Name:emailAttributeDataType:StringMutable:falseRequired:true-Name:nameAttributeDataType:StringMutable:trueRequired:trueUserPoolClient:Type:AWS::Cognito::UserPoolClientProperties:UserPoolId:!RefUserPoolGenerateSecret:TrueAllowedOAuthFlowsUserPoolClient:trueCallbackURLs:-!Subhttps://${CallbackDomain}/signinAllowedOAuthFlows:-code-implicitAllowedOAuthScopes:-phone-email-openid-profileSupportedIdentityProviders:-COGNITOHostedUserPoolDomain:Type:AWS::Cognito::UserPoolDomainProperties:Domain:!RefHostedAuthDomainPrefixUserPoolId:!RefUserPoolOutputs:CognitoUserPoolID:Value:!RefUserPoolDescription:The UserPool IDCognitoAppClientID:Value:!RefUserPoolClientDescription:The app clientCognitoUrl:Description:The urlValue:!GetAttUserPool.ProviderURLCognitoHostedUI:Value:!Subhttps://${HostedAuthDomainPrefix}.auth.${AWS::Region}.amazoncognito.com/login?client_id=${UserPoolClient}&response_type=code&scope=email+openid+phone+profile&redirect_uri=https://${CallbackDomain}/signinDescription:The hosted UI URLEnter fullscreen modeExit fullscreen modeWhen the user pool is created we need to create two groups, trainers and riders. Let's do this from the console this time, navigate to Cognito section, locate the user pool and click on it.From here select the Groups tab and create the two groups, trainers and riders.Next up, create the API.Api Gateway APINext part we need is the Api Gateway API. Right now it's only REST api that is supported, hopefully there will be support for HTTP api in the future as well.Let's use CloudFormation and SAM to create the API that we need.AWSTemplateFormatVersion:\"2010-09-09\"Transform:AWS::Serverless-2016-10-31Description:Create the unicorn service apiGlobals:Function:Timeout:5MemorySize:128Runtime:python3.9Resources:LambdaRiderGet:Type:AWS::Serverless::FunctionProperties:Handler:index.lambda_handlerInlineCode:|import jsondef lambda_handler(event, context):return {\"statusCode\": 200,\"body\": json.dumps({\"message\" : \"Hello from LambdaRiderGet!\"}),}Events:HelloWorld:Type:ApiProperties:Path:/riderMethod:getRestApiId:Ref:ApiRegionalLambdaRiderPost:Type:AWS::Serverless::FunctionProperties:Handler:index.lambda_handlerInlineCode:|import jsondef lambda_handler(event, context):return {\"statusCode\": 200,\"body\": json.dumps({\"message\" : \"Hello from LambdaRiderPost!\"}),}Events:HelloWorld:Type:ApiProperties:Path:/riderMethod:postRestApiId:Ref:ApiRegionalLambdaRiderList:Type:AWS::Serverless::FunctionProperties:Handler:index.lambda_handlerInlineCode:|import jsondef lambda_handler(event, context):return {\"statusCode\": 200,\"body\": json.dumps({\"message\" : \"Hello from LambdaRiderList!\"}),}Events:HelloWorld:Type:ApiProperties:Path:/ridersMethod:getRestApiId:Ref:ApiRegionalLambdaTrainerGet:Type:AWS::Serverless::FunctionProperties:Handler:index.lambda_handlerInlineCode:|import jsondef lambda_handler(event, context):return {\"statusCode\": 200,\"body\": json.dumps({\"message\" : \"Hello from LambdaTrainerGet!\"}),}Events:HelloWorld:Type:ApiProperties:Path:/trainerMethod:getRestApiId:Ref:ApiRegionalLambdaTrainerPost:Type:AWS::Serverless::FunctionProperties:Handler:index.lambda_handlerInlineCode:|import jsondef lambda_handler(event, context):return {\"statusCode\": 200,\"body\": json.dumps({\"message\" : \"Hello from LambdaTrainerPost!\"}),}Events:HelloWorld:Type:ApiProperties:Path:/trainerMethod:postRestApiId:Ref:ApiRegionalLambdaTrainerList:Type:AWS::Serverless::FunctionProperties:Handler:index.lambda_handlerInlineCode:|import jsondef lambda_handler(event, context):return {\"statusCode\": 200,\"body\": json.dumps({\"message\" : \"Hello from LambdaTrainerList!\"}),}Events:HelloWorld:Type:ApiProperties:Path:/trainersMethod:getRestApiId:Ref:ApiRegionalLambdaUnicornGet:Type:AWS::Serverless::FunctionProperties:Handler:index.lambda_handlerInlineCode:|import jsondef lambda_handler(event, context):return {\"statusCode\": 200,\"body\": json.dumps({\"message\" : \"Hello from LambdaUnicornGet!\"}),}Events:HelloWorld:Type:ApiProperties:Path:/unicornMethod:getRestApiId:Ref:ApiRegionalLambdaUnicornPost:Type:AWS::Serverless::FunctionProperties:Handler:index.lambda_handlerInlineCode:|import jsondef lambda_handler(event, context):return {\"statusCode\": 200,\"body\": json.dumps({\"message\" : \"Hello from LambdaUnicornPost!\"}),}Events:HelloWorld:Type:ApiProperties:Path:/unicornMethod:postRestApiId:Ref:ApiRegionalLambdaUnicornList:Type:AWS::Serverless::FunctionProperties:Handler:index.lambda_handlerInlineCode:|import jsondef lambda_handler(event, context):return {\"statusCode\": 200,\"body\": json.dumps({\"message\" : \"Hello from LambdaUnicornList!\"}),}Events:HelloWorld:Type:ApiProperties:Path:/unicornsMethod:getRestApiId:Ref:ApiRegionalApiRegional:Type:AWS::Serverless::ApiProperties:Name:unicorn-service-apiStageName:prodEndpointConfiguration:REGIONALEnter fullscreen modeExit fullscreen modeThis will create an REST Api Gateway API with nine resources.If you click the unicorn service api you should see the created resources.Selecting one of the resources will show that no auth is configured.Now both the prerequisites are created and we can move over to the actual securing using Amazon Verified Permissions.Amazon Verified PermissionsThis part of the setup will purely manual as AVP has released the quick start, to help us configure Authorization for APIs using verified permissions.The first thing we need to do is open Amazon Verified Permissions section of the console, from here we'll start creating a policy store.In this first step in the guide we select that we like to use Cognito and API Gateway, both need to be in place already. If they are missing there will be a message informing that.Now we need to select the API and Stage and import the API.Next step is now to select the Identity Source and in this case it will be the Cognito User Pool.Final step is to assign what actions a specific group should have access to in the API.Now, we are all set and ready to create the policy store.So far this has been a very nice flow and very easy to use. But! Now the problem started for me. The first thing was that I got an error with an invalid character in the namespace.This happens since I used '-' in the name if the API Gateway API, I named it 'unicorn-service-api' and this is used as the name space and was not allowed. I think the guide should have warned me about this earlier. The first step in the guide checks that there is an User Pool and an API Gateway API, it could also check the name requirements.After solving this problem, renaming the API Gateway API, the error was gone.Connect authAfter the policy store has been created the Lambda Authorizer must be connected to the API Gateway actions. Navigate to the API Gateway API and select Resources in the menu.Select the action and then click on Edit for 'Method request settings'. In the drop down menu select the Lambda Authorizer. Repeat this for all of the actions in the API.Final step is then to Deploy the API to your stage again, changes do not take affect until this is done.With this connection, you are done and your API is protected with Cognito User Pool and Amazon Verified Permissions.ImprovementsI think this is a nice addition and it makes creation of the Lambda Authorizer easier, however I do see a couple of areas for improvements.First. Today the guide will deploy a CloudFormation template with the Lambda Authorizer. To connect this authorizer to API Gateway API this must be done manually in the console. Since I'm a user of AWS SAM a Authorizer must be in the same template / stack as the API Gateway resource, so it's not possible to import from a different stack. Instead of this automatic deployment I would prefer to get an option to download the code and template in either yaml or json. That way I could include this in the same template as the API Gateway resource and then handle the connection automatically.Second. After creation of the Policy Store and the policies there are no easy way to add additional API actions to the policy, or to add a new group in the User Pool. APIs change and I would prefer an as easy way to update as it was to create.Last. Better checks on conditions like the problem above that gave me an error.Final WordsThis was a quick walkthrough of the new feature to easy connect Amazon Verified Permissions to an API Gateway API. The process is easy to use and straight forward, but I do see improvement potential, and hopefully the guide will evolve over time.Don't forget to follow me onLinkedInandXfor more content, and read rest of myBlogsAs Werner says! Now Go Build!"}
{"title": "Have You Ever Care About Identity Integrity?", "published_at": 1712896083, "tags": ["microsoftworkloads", "activedirectory", "identitysecurity", "itdr"], "user": "Terrence Chou", "url": "https://dev.to/aws-builders/have-you-ever-care-about-identity-integrity-475p", "details": "Because of fascinating features and thorough services, organisations are more willing to embrace public cloud platforms (AWS, Azure, GCP, etc) to enlarge their business footprint. A robust defence and recovery framework against cybersecurity incidents is always a key to ensure business continuity. Intrinsically, every technical stuff could be fully controlled in that people define each of them; in other words, every employee could be a potential exposure point, accidentally revealing their business environment to the external world. According to numerous public researches, it is really. We all must keep in mind that modernisation not only means service frameworks but also cyberattack patterns! Every catastrophic disruption could result from the organisation's pillars and one of them isidentity!Identity Attacks Have Raised Than EverIn general, the types of attacks we have learned could be summarised below:To pause your service(s) functioning- The most common case is the DDoS attack and it happens quite often, especially when a bunch of competitors share the same commercial market(s). Intrinsically, this kind of disruption would be a short-term period in that the intention behind the scenes does not aim to completely destroy your service(s), but temporarily stop you gaining revenue from the specific event(s) instead. The common DDoS attack is volume-based, meaning that your business continuity primarily relies on how many atypical requests you could mitigate before they get in your core infrastructure; for instance,leverage your ISP's Anti-DDoS offeringordeploy the RTBH (Remote-triggered Black Hole) architectureto achieve.To infiltrate your business environment(s) and exfiltrate sensitive data- Compared to the DDoS attack, the infiltration attack is much more difficult to prevent in that it could get in your environment(s) via a variety of manners; for instance, visiting a suspicious website or opening a phishing e-mail without too much awareness. An unexpected daemon/process could reside in your house and steal your treasures silently. When you notice that there is something wrong, it does not mean that the event just gets started, but comes to the end instead. In order to gain more granular visibility and formulate a robust runbook whenever an event comes up, most organisations typically implement theNDR (Network Detection and Response)andEDR (Endpoint Detection and Response)solutions to strengthen.When we look at an essential of those solutions, they enrich both the observability and security on the infrastructure layer without any doubt; however, they do not take too many focus on the application layer. We have lived in an era where every modernised cyberattack aims to manipulate your service(s) and even tamper with your business data instead of taking over your core infrastructure.The most effective and easiest manner is to penetrate any of the identities's permission (as a trojan horse) within your organisation. Is it FEASIBLE?! Our resources are well-protected via a number of security frameworks across different tiers!However, the truth is that this kind oftragedyhas happened over and over again, it is just because you have not been aware of.ITDR, A New NormThe most well-known and widely deployed identity store isMicrosoft Active Directory (AD)when excluding any of the Identity Provider (IdP) platforms (well, I have not noticed that Active Directory has been launched on Windows 2000 Server Editionsince 1999\ud83d\ude17). Although we understand what benefits we could gain from both the managed application services and cloud IAM services, the thing is that the Microsoft service frameworks are still valuable for a large amount of enterprises around the world; Microsoftkeeps increasingits market share in the Cloud Infrastructure Services arena that could demonstrate this point.What does that mean on the other hand? That is to say that you should invest in how to ensure your Active Directory's integrity as much as possible.According to Microsoft's research,over 80% of breacheswere caused byidentity-based attacks. A general pattern is someone who is neither an IT employee nor an employee within the organisation promotes their permission from a user/guest role to an administrator role successfully without any approval, then takes away/tampers with business-sensitive data.Let us look at the security boundary of Active Directory. Since it covers not only the IAM principle but also the EDR scenario (all the activities are written in Windows Event Logs), a new security framework (or more precisely, a marketing term \ud83d\ude1c) joins the game accordingly -Identity Threat Detection and Response (ITDR). What ITDR does could be spotlighted on the discovery, tracking, and notification pillars.Discovery- According to your Active Directory's profiles, correlate all of them with theIOA (Indicators of Attack),IOC (Indicators of Compromise), andIOE (Indicators of Exposure)indices to dig into any potential security leak/vulnerability.Tracking- Every Active Directory's object, including User Accounts, Group Policies, and DNS Records could be modified by any authorised user/role, which means each workflow should contain four W-ingredients to ensure every single change is well-monitored:Whomade the change?Whendid this change take place?Whichobject was influenced?And,whataction applied to this object?Notification- Since every single change is well-monitored, a warning message will be delivered immediately to inform all the correlatives that something goes wrong and must react right away whenever an atypical/unauthorised modification is detected.Because of these weapons, you have a clear blueprint of what you should defend and precise guidance on how to consolidate your Active Directory's defence boundary; no fears anymore \ud83d\ude0eYou Should Be Greedy For VisibilityHow will you take any action once you are aware of something atypical? Why GuardDuty, Inspector, CloudTrail, and other equivalent services are extremely prominent for detecting and responding to any unsecure exposure? In effect, those concerns emphasise one spotlight in common:every access must be well-tracedin that each of them could be a potential clue of an incident, which visualises every single behaviour granularly so that you will be able to react promptly if something goes wrong.As I mentioned from the outset, most modernised cyberattacks aim to open up your infrastructure (door) and take over applications (control) on your behalf; nothing will be deemed illegitimate as usual! Of course, ransomware is a thing you need to beware of; however, Active Directory is another stuff you must pay attention to.Please bear in mind that Active Directory is not a single and individual component, but a complex and multi-relational ecosystem instead. None of the enhancements/optimisations are based on what youfeel, but what youobserveinstead. Data are always out there and waiting for mining, the challenge is alwayshow to utilisethem more straightforwardly and efficiently. That is whyXDR (Extended Detection and Response)comes into play; X could even mean Anything in that it is a methodology/framework rather than a single product, and ITDR is part of the XDR subsets.Never Too Late To CommenceHunting always follows the footprints!In the IR (Incident Response) world, every effective reaction relies on how many clues could be investigated; otherwise, you will be overwhelmed or even exhausted by countless false alarms. Active Directory is an invaluable but invisible contributor, your service framework could not function properly without its reliability. To behuntedor to behunting? Think about the position you want to be."}
{"title": "Intro to the Knobs: A Quick Journey through Popular Hyperparameters", "published_at": 1712872915, "tags": ["bedrock", "sagemaker", "ai", "hyperparameters"], "user": "Juan Taylor", "url": "https://dev.to/aws-builders/intro-to-the-knobs-a-quick-journey-through-popular-hyperparameters-2o82", "details": "What are Hyperparameters?Hyperparameters are set before training and affect the training but are not changed by the training. Adjusting them affects the models accuracy and the capacity to generalize to new samples and the speed of convergence. In essence, it\u2019s how a developer controls a specific model to get better outputs.Below is a list of some hyperparameters you may see as an ML Developer.TemperatureControls randomness of outputs through affecting the probability distribution. Temperature has more creativity than \u2018Top P\u2019 which just determines a threshold of a probability distribution and doesn\u2019t affect the whole distribution. Lower temperatures maintain confidence and higher temperatures are more creative and have more diverse outputs. Good for NLP and Recommendation systems.Top PControls diversity & creativity through a threshold in a probability distribution. When you want to maintain coherence and relevance but want to have some diversity then those \u2018Top P\u2019 over Temperature. Top P doesn\u2019t change the probability distribution but cuts it off at a chosen point so not too much chaos is created but rather a diversity that is more controlled than adjusting the temperature hyperparameter.Token LengthThis is the number of words and characters fed to the LLM model. It may be too short for the model to handle or too long.Max TokensThis is the number of tokens the LLM can generate. If it\u2019s too short, it will use less memory and you get a faster response but if it will mean inaccuracies in the response.Stop TokenA Stop Token or \u201cend-of-sequence\u201d token is a  signal to indicate the stopping at each generated sentence or the end of the generation of the whole output of an LLM. A developer can control the stops through those signals.Learning RateThe learning rate controls the step size during training. It affects both the speed of convergence and the quality of convergence. There are different techniques to finding the best learning rate including using a grid search. One can choose to have a dynamic learning rate adjusting over time based on an assortment of criteria.Weight DecayWeight decay penalizes large weights by adding a penalty to the loss function. It  prevents overfitting by evening out the weights making the network less prone to memorize insignificant details.Hidden Layer SizeThis decides the number of neurons in each hidden layer of the model. A recommended number is the mean of the neurons in the input and output layers.Dropout RateThe Dropout rate regulates the percentage of neurons eliminated during training in order to deter overfitting. A standard is 0.5 for hidden layers and a lower or even zero dropout rate for input and output layers.Batch SizeThe Batch size decides the number of samples used in each iteration of training. It may be worth studying the effects of large and small sizes on your model.Number of EpochsThis is the number of times the entire dataset is passed through the model for training.Look for common practices to determine the best number. If model performance degrades early, then implement early stopping. You can do so programmatically by monitoring a metric and then define a criteria to stop at.SummaryHere is a quick journey through some ML hyperparameters you may see as an ML developer perhaps working in AWS Sagemaker or Bedrock. I hope I have shed some light and helped to begin to unravel the rich tapestry of hyperparameters.As you play with the knobs and dials you may notice the compendium of tweaks and fixes can lead to transformative effects in the performance of your models. As you dive deeper into the interplay of the labyrinth of these parameters you gain an expertise in machine learning that will be both one of experience and a developed intuition in creating models that work well."}
{"title": "AWS SnapStart - Part 18 Measuring cold starts with Java 17 using different deployment artifact sizes", "published_at": 1712846469, "tags": ["aws", "java", "serverless", "coldstart"], "user": "Vadym Kazulkin", "url": "https://dev.to/aws-builders/aws-snapstart-part-18-measuring-cold-starts-with-java-17-using-different-deployment-artifact-sizes-5092", "details": "IntroductionIn thepart 8of our series we measured the cold start of the Lambda function with Corretto Java 17 runtime without SnapStart enabled, with SnapStart enabled and also applied DynamoDB invocation priming optimization. Lambda functions had 1024 MB memory and the deployment package (jar file) was approx. 15 MB. In this article we will make the same measurements but play around deployment package size to figure out whether it will make a difference similar as we did in the articleMeasuring cold starts with Java 21 using different deployment artifact sizesbut now using Java 17.Measuring cold starts with Java 17 with and without SnapStart enabled using deployment artifact sizesLets's consider the same 3 scenarios for Java 17 (as we did for Java 21):Small HelloWorld-style applicationwhich consists of Lambda receiving the API Gateway request with product id and basically prints this id out. There is no persistence layer involved. The application is that simple, that there is no priming technique to be applied. There are only several dependencies declared inpom.xmllike aws-lambda-java-core and slf4j-simple. The deployment size of such an application is 137 KB only.Medium Size application with DynamoDB persistence. We'll re-use the application introduced inpart 8for this. There are basically 2 Lambda functions which both respond to the API Gateway requests and retrieve product by id received from the API Gateway from DynamoDB. One Lambda function can be used with and without SnapStart and the second one uses SnapStart and DynamoDB request invocation priming. There are bunch of dependencies declared inpom.xmllike aws-lambda-java-core, aws-lambda-java-events, slf4j-simple, crac, dynamodb and url-connection-client. The deployment size of such application is 15 MB.Big Size application with DynamoDB persistence and dependencies to many other AWS services. It's similira to medium size application. There are basically 2 Lambda functions which both respond to the APIGateway requests and retrieve product by id received from the API Gateway from DynamoDB. One Lambda function can be used with and without SnapStart and the second one uses SnapStart and DynamoDB request invocation priming. There are bunch of dependencies declared inpom.xmllike aws-lambda-java-core, aws-lambda-java-events, slf4j-simple, crac, dynamodb and url-connection-client. But we also declare man other dependencies to various differnt AWS service like sns, kinesis, eventbridge, bedrock, but we won't even use them in our application. So such declaration makes only the package size bigger is then 50 MB.The results of the experiment below were based on reproducing approximately 100 cold starts, which were reproduced during the experiment which ran for approximately 1 hour. All Lambda functions had 1024 MB memory setting.Legend:SMALL- corresponds to 137 KB deployment artifact size,MEDIUMto 15 MB andBIGto 50 MB.Experiment descriptionp50p90p99SMALL: cold start time w/o SnapStart448.97481.98521.06SMALL: cold start time with SnapStart w/o Priming475.76611.43734.14SMALL: cold start time with SnapStart with Priming475.76611.43734.14MEDIUM: cold start time w/o SnapStart2847.002963.133181.06MEDIUM: cold start time with SnapStart w/o Priming1513.851648.551860.34MEDIUM: cold start time with SnapStart with Priming725.56829.031066.98BIG: cold start time w/o SnapStart2930.743080.923281.16BIG: cold start time with SnapStart w/o Priming1497.192195.472644.03BIG: cold start time with SnapStart with Priming731.941391.841679.56For the small deployment artifact size cold start time with SnapStart enabled and with and without Priming are the same because there is nothing to prime is this simple case.ConclusionsThere are some interesting insights coming from those measurements aso concerning the usage of SnapStart itself: for the small deployment size there is no real benefit of using SnapStart as cold start times with and without it are quite comparable. But applications and Lambda functions without any real dependencies incuding those to other AWS services are quite exceptional.There is also quite noticable cold start difference for p90 and p99 comparing medium and big deployment artifact sizes whereas p50 remained similar. And of course cold starts for medium and big deployment sizes are much bigger than with the small one. So also for Java 21 the deployment size still plays a crucial role for the cold start without usage of the SnapStart but also with! Try to make your Lambda functions small and single-purposed!Comparing these measurements with the same measurements done forJava 21we observe a bit lower cold start times for Java 17 in case SnapStart is not being enabled for all 3 package sizes for our use case. In case SnapStart is enabled the results are much closer for both with and without priming."}
{"title": "To Build or to Reuse? A CDK Question", "published_at": 1712833450, "tags": ["aws", "cdk", "devops", "tutorial"], "user": "Dakota Lewallen", "url": "https://dev.to/aws-builders/to-build-or-to-reuse-a-cdk-question-3obl", "details": "Unless you are starting on a fresh application, the project you are working on will likely have resources already in use. One of the benefits of using CDK over other Infrastructure-as-Code tools is conditionals! Along with the ability to import resources via their ARN, we can create solutions that can use existing resources, or create new ones. This means we can deploy into both fresh and existing environments from the same codebase!Getting StartedLet\u2019s assume you have a CDK application made and have bootstrapped the target Region & Account. Starting with a new stack, we can begin to scaffold what we\u2019ll need.import{Stack,StackProps}from'aws-cdk-lib';import{Construct}from'constructs';exportinterfaceNewStackPropsextendsStackProps{};exportdefaultclassNewStackextendsStack{constructor(scope:Construct,id:string,props:NewStackProps){super(scope,id,props);}}Enter fullscreen modeExit fullscreen modeWorking backward, we\u2019ll say our example use case is to add a new lambda that processes data once it\u2019s dropped in a bucket. For deploying into production, the bucket is already in use and managed by another project. But for the testing environment, there is no bucket. So we will need to make one.To begin let\u2019s add the lambda, as it\u2019s a constant regardless of where it\u2019s deployed. So the implementation is simple.import{Stack,StackProps}from'aws-cdk-lib';import*asnodefrom'aws-cdk-lib/aws-lambda-nodejs';import{Construct}from'constructs';exportinterfaceNewStackPropsextendsStackProps{};exportdefaultclassNewStackextendsStack{publicreadonlybucketProcessor:node.NodeJsFunction;constructor(scope:Construct,id:string,props:NewStackProps){super(scope,id,props);this.bucketProcessor=newnode.NodeJsFunction();}}Enter fullscreen modeExit fullscreen modeEasy enough!Now onto the juicy stuff. We\u2019ll add a property to the NewStackProps interface to have the bucket ARN supplied externally.import{Stack,StackProps}from'aws-cdk-lib';import*asnodefrom'aws-cdk-lib/aws-lambda-nodejs';import{Construct}from'constructs';exportinterfaceNewStackPropsextendsStackProps{// Mark the property as optional, this way we can _optionall_ build the bucket if it's not suppliedbucketArn?:string;};exportdefaultclassNewStackextendsStack{publicreadonlybucketProcessor:node.NodeJsFunction;constructor(scope:Construct,id:string,props:NewStackProps){super(scope,id,props);// pull the arn out into a construct-scoped variable for later use.const{bucketArn}=props;// Tip: For ultra clean code, you would want to add validation against this `bucketArn` string to confirm it is in fact an ARN.this.bucketProcessor=newnode.NodeJsFunction();}}Enter fullscreen modeExit fullscreen modeNow armed with the knowledge of \u201cDo we have an existing bucket\u201d, we can either make a new one or reuse the old oneimport{Stack,StackProps}from'aws-cdk-lib';import*asnodefrom'aws-cdk-lib/aws-lambda-nodejs';import*ass3from'aws-cdk-lib/aws-s3';import{Construct}from'constructs';exportinterfaceNewStackPropsextendsStackProps{// Mark the property as optional, this way we can _optionall_ build the bucket if it's not suppliedbucketArn?:string;};exportdefaultclassNewStackextendsStack{publicreadonlybucketProcessor:node.NodeJsFunction;constructor(scope:Construct,id:string,props:NewStackProps){super(scope,id,props);const{bucketArn}=props;this.bucketProcessor=newnode.NodeJsFunction();letbucket;if(bucketArn){bucket=s3.Bucket.fromBucketArn(this,'importedBucket',bucketArn);}else{bucket=s3.Bucket(this,'createdBucket')}}}Enter fullscreen modeExit fullscreen modeTo wrap up, we\u2019ll add a property to the stack so that the bucket can be referenced elsewhere in the CDK application and hook the lambda up to the bucket so it can process events.import{Stack,StackProps}from'aws-cdk-lib';import*asnodefrom'aws-cdk-lib/aws-lambda-nodejs';import*ass3from'aws-cdk-lib/aws-s3';import*ass3nfrom'aws-cdk-lib/aws-s3-notifications';import{Construct}from'constructs';exportinterfaceNewStackPropsextendsStackProps{// Mark the property as optional, this way we can _optionall_ build the bucket if it's not suppliedbucketArn?:string;};exportdefaultclassNewStackextendsStack{publicreadonlybucketProcessor:node.NodeJsFunction;publicreadonlybucket:s3.IBucket;/** Use of the interface here is important. The `fromBucketArn` method returns an instance of the interface. But the new Bucket returns `s3.Bucket` which _implements_ the `IBucket` interface. So specificying the property as `IBucket` ensures that we can house _either_ the imported or created bucket. The tradeoff is that we lose access to some of the created buckets \"fancier\" features. **/constructor(scope:Construct,id:string,props:NewStackProps){super(scope,id,props);const{bucketArn}=props;this.bucketProcessor=newnode.NodeJsFunction();if(bucketArn){this.bucket=s3.Bucket.fromBucketArn(this,'importedBucket',bucketArn);}else{this.bucket=s3.Bucket(this,'createdBucket')}this.bucket.addEventNotification(s3.EventType.OBJECT_CREATED,news3n.LambdaDestination(this.bucketProcessor),{prefix:'some/object/prefix/*'});}}Enter fullscreen modeExit fullscreen modeConclusionWith CDK, it\u2019s possible now more than ever to reuse infrastructure patterns across environments. No matter the state the target environment is in. I would even consider this the tip of the iceberg! To go even further, you could make a custom L3 construct containing the reuse-or-build logic to simplify the process further! The possibilities are endless!Find me onLinkedIn|Github|Mastodon"}
{"title": "Use AWS StepFunctions for SSM Patching Alerts", "published_at": 1712821336, "tags": ["aws", "stepfunctions", "ssm", "cloudformation"], "user": "Lucian Patian", "url": "https://dev.to/aws-builders/use-aws-stepfunctions-for-ssm-patching-alerts-3pj8", "details": "In this blog post we'll explore how to use AWS Step Functions and SSM Patch Manager to monitor the patch compliance status of EC2 instances and send alerts, reducing manual tracking and enhancing the security of our cloud environment.AWS Step Functions is a service that doesn't require a server to run. It allows us to connect with Lambda functions and other services to construct important business applications.The service is built around the principle of linked tasks put together in a workflow called \"state machine\". A task is able to invoke other AWS services or more recentlythird-party APIs.AWS Systems Manager (SSM), which includes the Patch Manager feature, provides a unified interface for managing your AWS resources, including the ability to automate patching for EC2 instances which can make things easier for us.However, there are instances that need a reboot so the EC2 patch is completely done. We don't want to restart them automatically, especially when they're running critical services like databases. In this case, we prefer to choose when to restart.To keep track of the EC2 instances that aren't fully compliant in the SSM patch report, we need alerts.My goal was to send alerts to a Microsoft Teams channel, listing the EC2s that aren't compliant and need additional actions like rebooting. Initially, I used a Lambda function to do this but I didn't want to manage its dependencies over time so I switched to using Step Functions, taking advantage of its new feature that supportsHTTPS endpoints.Overview of the State MachineThe entire process is initiated by an EventBridge rule, which acts as a trigger for the Step Function state machine.The state machine begins by identifying all currently active EC2 instances in the AWS account. It then retrieves all the instance IDs and filters them based on parameters such as ComplianceType=Patch and Status=NON_COMPLIANT.Next it determines if there are any instances that need review. If not, the state machine will skip to the end and stop. To do this, we use a task that counts the number of instances in the list from the previous step. If the count is more than zero, indicating that there are instances requiring attention, the state machine continues to filter these instances by their tags. This information is then used to format a message sent to a Microsoft Teams channel, which includes the names and IDs of the EC2 instances that need our attention.In the end we call the 3rd party API to send the formatted message to the Microsoft Teams channel.Full steps descriptionDescribeInstances: starts the process by identifying all currently active EC2 instances within the AWS account.ExtractInstanceIDs: retrieves all the instance IDs from the previously fetched list of EC2 instances.FetchInstanceComplianceData: filters the instances based on the ComplianceType=Patch and Status=NON_COMPLIANT parameters.CalculateArrayLength: calculates the size of the list of non-compliant instances.CheckIfInstancesFound: checks if the size of the list is greater than zero (indicating that there are non-compliant instances) or not. If no non-compliant instances are found during this step, the state machine skips to the end state and stops.DescribeTagsForFilteredInstances: if there are non-compliant instances, this step fetches the tags for these instances.PrepareNonCompliantInstanceList: prepares a list of non-compliant instances along with their names and IDs.CallThirdPartyAPI: formats the message with the non-compliant instances' information and sends it to a Microsoft Teams channel.Additional Configuration DetailsThis state machine can be used to send the message to any communications tool like Slack, MS Teams or to a ticketing system.For MS Teams, the endpoint URL needs to be encoded so the \"@\" needs to be replaced with \"%40\" or you can use a URL shortener service.An HTTP Task requires an EventBridge connection, which securely manages the authentication credentials of an API provider. A connection specifies the authorization type and credentials to use for authorizing a third-party API.In our case we are just sending a message/payload to an external URL without the need of authentication but in order to use the StepFunction HTTP Task, we need to create this connection. When creating the connection, as a requirement, you also create an AWS Secret used for authentication. Again, since there's no need to authenticate to the MS Teams channel, the Secret values contain the keyname of the API and the secret is the ARN of the EventBrdige API connection:Infrastructure as CodeThe entire PoC was done using the AWS Console but since we are living in the age of automation, I wanted to have an easy and repeatable way of deploying the solution.In the past weeks, the Cloudformation service teamannounced the new IaC generator (infrastructure as code generator)which must be one of the most desired features for years now so I definitely wanted to give it a try.It turns out that I was able to get the Cloudformation template for all the needed resources pretty easy. The hardest thing was to select from a huge dropdown list all the resources involved in my scenario and to make sure I don't leave out any. After the template was generated, inside the StepFunction JSON definition, it was a bit difficult to replace the hardcoded values with parameters. Now it seems like a piece of cake.If you want to use this solution, checkout theGithub repowhich includes the entire Cloudformation stack needed for deployment.During the initial stages of this PoC, I encountered difficulties with the CallThirdPartyAPI task so I asked around for guidance other AWS CommunityBuilders in the dedicated Slack space and got almost instant help fromBeno\u00eet Bour\u00e9,Jimmy DahlqvistandAndres Moreno. Chapeau bas!"}
{"title": "How To Implement AWS SSB Controls in Terraform - Part 4", "published_at": 1712809418, "tags": ["aws", "terraform", "security"], "user": "Anthony Wat", "url": "https://dev.to/aws-builders/how-to-implement-aws-ssb-controls-in-terraform-part-4-3bl3", "details": "IntroductionTheAWS Startup Security Baseline (SSB)defines a set of controls that comprise a lean but solid foundation for the security posture of your AWS accounts. By the end ofpart 3of ourblog series, we have covered all of the account controls and workload controls that are related to workload access and data protection. In this installment, we will review the remaining workload controls that focus on network security. Let's begin with WKLD.10, which is about keeping private resources secure within private subnets.WKLD.10 \u2013 Deploy private resources in private subnetsThe workload controlWKLD.10requires that all AWS resources that don't require direct internet access be deployed to a VPC private subnet.Amazon VPCprovides the means to isolate your network and keep traffic from and to workloads in your VPCs secure. For a typical workload, resources such as backend and database systems should be deployed to private subnets. If a private subnet allows outbound access to the internet, its route table should route the to a NAT gateway deployed in a public subnet. This setup is shown in the following diagram:To prevent accidental assignment of public IP addresses, this control recommends disabling the option for auto-assigning public IP address when a private subnet is created. In Terraform, this option corresponds to themap_public_ip_on_launchargument in theaws_subnetresource. Since the default value isfalse, omitting the argument in the resource definition will effectively create a private subnet. Here is an example:# Dependent resources omitted for brevityresource\"aws_subnet\"\"private\"{vpc_id=aws_vpc.this.idcidr_block=\"10.0.0.0/24\"# Already false by default, no need to set explicitly# map_public_ip_on_launch = false}Enter fullscreen modeExit fullscreen mode\ud83d\udca1 TheAWS VPC Terraform moduleprovides abstraction for private subnets to help you manage a secure VPC structure.Similar option to auto-assign public IP exists for EC2 instances, so it should be disabled when an EC2 instance is provisioned. This setting corresponds to theassociate_public_ip_addressargument in theaws_instanceresource, which not set thus implyingfalseby default. So you can omit the argument to create an EC2 instance as per the following example:# Dependent resources omitted for brevityresource\"aws_instance\"\"web\"{ami=data.aws_ami.ubuntu.idiam_instance_profile=aws_iam_instance_profile.ssm.nameinstance_type=\"t3.micro\"subnet_id=data.aws_subnet.private.id# Already false by default, no need to set explicitly# associate_public_ip_address = false}Enter fullscreen modeExit fullscreen modeWKLD.11 \u2013 Use security groups to restrict accessThe workload controlWKLD.11requests the use of security groups to restrict network access.Security groupsserve as virtual stateful firewalls to control inbound and outbound traffic to the resource it is associated with. A typical baseline would be to allow all outbound traffic, and allow inbound traffic only to trusted sources on specific service ports and protocols. You can further restrict the outbound traffic for more isolation.To demonstrate how to properly define security groups in Terraform, let's consider this hypothetical LAMP application:Here are the required inbound rules:Security groupPurposeSourceProtocol and portALBHTTPS access from the internetInternet (0.0.0.0/0)TCP 443 (HTTPS)Web serverHTTPS access from the ALBSecurity group of the ALBTCP 443 (HTTPS)MySQL InstanceMySQL access from web serverSecurity group of the web serverTCP 3306 (MySQL)And here are the required outbound rules - note that we assume that the RDS for MySQL DB instance does not require internet access:Security groupPurposeDestinationProtocol and portALBInternet accessInternet (0.0.0.0/0)AllWeb ServerInternet accessInternet (0.0.0.0/0)AllThe security groups can be defined in Terraform as follows:# ALB security groupresource\"aws_security_group\"\"alb\"{name=\"app-prod-sg-use1-alb\"description=\"Security group for the ALB\"vpc_id=aws_vpc.this.id}resource\"aws_vpc_security_group_ingress_rule\"\"alb_https_all\"{security_group_id=aws_security_group.alb.idcidr_ipv4=\"0.0.0.0/0\"from_port=443ip_protocol=\"tcp\"to_port=443}resource\"aws_vpc_security_group_ingress_rule\"\"alb_all\"{security_group_id=aws_security_group.alb.idcidr_ipv4=\"0.0.0.0/0\"from_port=-1ip_protocol=-1to_port=-1}# Web server security groupresource\"aws_security_group\"\"web\"{name=\"app-prod-sg-use1-web\"description=\"Security group for the web server\"vpc_id=aws_vpc.this.id}resource\"aws_vpc_security_group_ingress_rule\"\"web_http_alb\"{security_group_id=aws_security_group.web.idreferenced_security_group_id=aws_security_group.alb.idfrom_port=443ip_protocol=\"tcp\"to_port=443}resource\"aws_vpc_security_group_ingress_rule\"\"web_all\"{security_group_id=aws_security_group.web.idcidr_ipv4=\"0.0.0.0/0\"from_port=-1ip_protocol=-1to_port=-1}# MySQL instance security groupresource\"aws_security_group\"\"db\"{name=\"app-prod-sg-use1-db\"description=\"Security group for the RDS for MySQL DB instance\"vpc_id=aws_vpc.this.id}resource\"aws_vpc_security_group_ingress_rule\"\"db_mysql_web\"{security_group_id=aws_security_group.db.idreferenced_security_group_id=aws_security_group.web.idfrom_port=3306ip_protocol=\"tcp\"to_port=3306}# Associate these security groups to the resources accordinglyEnter fullscreen modeExit fullscreen mode\ud83d\udca1 For more tips on managing security groups in Terraform, check out my blog posts5 Tips to Efficiently Manage AWS Security Groups Using TerraformandBuilding a Dynamic AWS Security Group Solution With CSV in Terraform.WKLD.12 \u2013 Use VPC endpoints to access servicesThe workload controlWKLD.12recommends the use of VPC endpoints to privately access AWS and other services without traversing the internet.Some industry and security compliance standards require that networks be isolated without outbound internet access. To facilitate access to AWS and external services without traversing the internet, you can useVPC endpoints.There are two types of VPC endpoints - gateway endpoints and interface endpoints. Gateway endpoints are available only for Amazon S3 and Amazon DynamoDB, but are otherwise free to use. Interface endpoints support numerous AWS services and external services that are exposed and shared asendpoint services. Both endpoint types support resource policies, however only interface endpoints support security groups since they are deployed as ENIs.Building upon the scenario above, let's assume that the web application needs to integrate with S3. We can provision a gateway endpoint in Terraform to enable private access as follows:resource\"aws_vpc_endpoint\"\"s3\"{vpc_id=aws_vpc.this.idroute_table_ids=[aws_route_table.web.id]service_name=\"com.amazonaws.us-east-1.s3\"# Defining a permissive resource policy for illustation.# You can finetune it for more security.policy=<<-EOT{     \"Version\": \"2008-10-17\",     \"Statement\": [       {         \"Action\": \"*\",         \"Effect\": \"Allow\",         \"Resource\": \"*\",         \"Principal\": \"*\"       }     ]   }EOT}Enter fullscreen modeExit fullscreen modeLet's now assume that we need to use SSM Session Manager to connect to the web server EC2 instance (seeWKLD.06 in part 3 of the blog seriesfor details). We can provision a set of interface endpoints in Terraform as follows:locals{ssm_service_names=[\"com.amazonaws.us-east-1.ec2messages\",\"com.amazonaws.us-east-1.ssm\",\"com.amazonaws.us-east-1.ssmmessages\"]}resource\"aws_security_group\"\"vpce\"{name=\"app-prod-sg-use1-vpce\"description=\"Security group for interface endpoints to AWS services\"vpc_id=aws_vpc.this.id}resource\"aws_vpc_security_group_ingress_rule\"\"vpce_https_vpc\"{security_group_id=aws_security_group.vpce.idcidr_ipv4=aws_vpc.this.cidr_blockfrom_port=443ip_protocol=\"tcp\"to_port=443}# Note: Requires enableDnsSupport and enableDnsHostnames set to true for the VPCresource\"aws_vpc_endpoint\"\"ssm\"{for_each=toset(local.ssm_service_names)vpc_id=aws_vpc.this.idservice_name=each.keyvpc_endpoint_type=\"Interface\"private_dns_enabled=truesecurity_group_ids=[aws_security_group.vpce.id]# Alternatively you can create a subnet for VPC endpointssubnet_ids=[aws_subnet.web.id]}Enter fullscreen modeExit fullscreen modeWKLD.13 \u2013 Require HTTPS for all public web endpointsThe workload controlWKLD.13mandates the use of HTTPS for all public web endpoints.TheHTTPS protocolprovides the level of web security that is considered the norm nowadays, so much so that Google defines the use of HTTPS aranking factorfor their search results and mark websites using HTTP as \"not secure\" in Chrome. With the prevalence thezero trustandencryption everywheresecurity approaches, TLS encryption between the load balancers/reverse proxies and backend systems, as well as end-to-end encryption, are also strongly recommended.AWS Certificate Manager (ACM)integrates with AWS endpoint servicesincluding Elastic Load Balancing and Amazon CloudFront. You can either issue a certificate for a domain that you own, or import a certificate that is generated with a third-party provider. For a better experience, you can use theAWS Certificate Manager (ACM) Terraform moduleto create and validate ACM certificates - refer to theexamplesfor usages. Meanwhile, to import a certificate for which you have the necessary PEM-formatted key and certificate files, you can use theaws_acm_certificateresourceas follows:resource\"aws_acm_certificate\"\"alb\"{private_key=\"${file(\"private.key\")}\"certificate_body=\"${file(\"cert.cer\")}\"certificate_chain=\"${file(\"chain.cer\")}\"}Enter fullscreen modeExit fullscreen modeContinuing with the scenario above, here is the Terraform configuration that provisions an ALB with an HTTPS listener:# Dependent resources omitted for brevityresource\"aws_lb\"\"this\"{name=\"app-prod-alb-use1\"internal=falseload_balancer_type=\"application\"security_groups=[aws_security_group.alb.id]subnets=[forsubnetindata.aws_subnet.public:subnet.id]}resource\"aws_lb_target_group\"\"web\"{name=\"web\"port=443protocol=\"HTTPS\"target_type=\"ip\"vpc_id=data.aws_vpc.this.idhealth_check{enabled=truematcher=\"200-299\"path=\"/\"protocol=\"HTTPS\"}lifecycle{create_before_destroy=true}}resource\"aws_lb_target_group_attachment\"\"web\"{target_group_arn=aws_lb_target_group.web.arntarget_id=aws_instance.web.idport=443}resource\"aws_lb_listener\"\"this\"{load_balancer_arn=aws_lb.this.arnprotocol=\"HTTPS\"port=443ssl_policy=\"ELBSecurityPolicy-TLS13-1-2-2021-06\"certificate_arn=aws_acm_certificate.alb.arndefault_action{type=\"forward\"target_group_arn=aws_lb_target_group.web.arn}}Enter fullscreen modeExit fullscreen modeFor CloudFront, you can consider using theAWS CloudFront Terraform module. Thecomplete exampledemonstrates how to configure a CloudFront distribute with HTTPS. Under the cover, it configures thecustom originandview certificateblocks in theaws_cloudfront_distributionresource.WKLD.14 \u2013 Use edge-protection services for public endpointsThe workload controlWKLD.14recommends using an edge-protection service to expose a public endpoint instead of directly through the underlying workload such as an EC2 instance.Such endpoint services includeElastic Load BalancingandAmazon CloudFrontas mentioned, as well asAmazon API GatewayandAWS Amplify Hosting. To provide additional endpoint protection, you can integrate services such asAWS WAF,AWS Network Firewall, andGateway Load Balancerwith a virtual firewall appliance.Setting up AWS Network Firewall and Gateway Load Balancer can be complex especially in acentralized architecture, so we will save them for a future blog series on network security (hint hint). Resuming the sample scenario in this blog post, let's focus on configuring an AWS WAF web ACL and associating it with the ALB. Here is the Terraform configuration that creates a web ACL with theCore rule set (CRS) managed rule group:resource\"aws_wafv2_web_acl\"\"regional\"{name=\"app-prod-webacl-use1-regional\"scope=\"REGIONAL\"# Use GLOBAL for web ACL meant for CloudFrontdefault_action{allow{}}visibility_config{cloudwatch_metrics_enabled=truemetric_name=\"appapp-prod-webacl-use1-regional\"sampled_requests_enabled=true}rule{name=\"AWS-AWSManagedRulesCommonRuleSet\"priority=0override_action{none{}}statement{managed_rule_group_statement{name=\"AWSManagedRulesCommonRuleSet\"vendor_name=\"AWS\"# Use the rule_action_override block to override rule action}visibility_config{cloudwatch_metrics_enabled=truemetric_name=\"AWS-AWSManagedRulesCommonRuleSet\"sampled_requests_enabled=true}}}# Set up logging for analysis, an important part of WAF implementationresource\"aws_cloudwatch_log_group\"\"waf_regional\"{name=\"aws-waf-logs-app-prod-webacl-use1-regional\"retention_in_days=90}resource\"aws_wafv2_web_acl_logging_configuration\"\"regional\"{log_destination_configs=[aws_cloudwatch_log_group.waf_regional.arn]resource_arn=aws_wafv2_web_acl.regional.arnredacted_fields{single_header{name=\"authorization\"}}}# Associate the web ACL with the ALB to enable WAF protectionresource\"aws_wafv2_web_acl_association\"\"regional\"{resource_arn=aws_lb.this.arnweb_acl_arn=aws_wafv2_web_acl.regional.arn}Enter fullscreen modeExit fullscreen modeThe AWS whitepaperGuidelines for Implementing AWS WAFdoes a great job at explaining how to plan, implement, test, and roll out AWS WAF, so be sure to review it before implementation.WKLD.15 \u2013 Use templates to deploy security controlsThe final workload control,WKLD.15, recommends using infrastructure-as-code (IaC) and CI/CD pipelines to deploy security controls alongside your AWS resources.If you are following this blog series, you should already know the benefits of using Terraform to define and deploy your AWS resources and configuration. Other IaC solutions such asAWS CloudFormation,AWS CDK, andPulumiwork the same way but differ in the programming or configuration language.As you design your IaC templates, consider separating the SSB account controls into its own \"stack\" while incorporating the workload controls to the \"workload stack\". This allows a more flexible deployment model and practicing DevOps within application development teams.Having CI/CD pipelines also helps you build and deploy configuration as soon as changes are made in the code repository. Your CI/CD pipeline can be customized according to your organization's needs, such as validations and approval gates.AWS CodePipelineis a decent choice if you prefer to stay in the AWS ecosystem, or you could also use a third-party solution such asGitHub Actions.Since you will be providing AWS credentials to the CI/CD pipelines, it is crucial that they are set in a secure manner. For Terraform, theAWS provider documentationexplains the different ways of providing AWS credentials to Terraform. You also should use a backend such as theS3 backendto securely store and share your states remotely.SummaryCongratulations, we made it through the entire set of SSB controls! Over the course of thisHow to implement the AWS Startup Security Baseline (SSB) using Terraformblog series, we have reviewed every SSB control in detail and see how they can be implemented using Terraform. Keep in mind that these controls are still considered foundational, so you should evolve your security practice as your AWS usage scales and evolves. Frameworks such as theAWS Security Maturity Modelcan help you define new target states and implement them iteratively.I thoroughly enjoyed writing this month-long blog series, especially as a newAWS Community Builder. I hope that you also learned something new and interesting. If you like my contents, please be sure to check out other posts in theAvangards Blog. Have a great one!"}
{"title": "How I Conquered the AWS Certified DevOps Engineer Professional Exam in 60 Days", "published_at": 1712799866, "tags": ["aws", "devops", "awscertified", "community"], "user": "Damien J. Burks", "url": "https://dev.to/aws-builders/how-i-conquered-the-aws-certified-devops-engineer-professional-exam-in-60-days-28dl", "details": "Table of ContentsIntroductionUnderstanding the ExamWhy This Certification?Prerequisites and PreparationsResources UsedMy Study ScheduleEmphasized ServicesTesting Experience and TipsConclusionIntroductionIn this article, I will share my journey on how to successfully pass the AWS Certified DevOps Engineer Professional exam in just 60 days. My hope is that by sharing my experience, resources, study plan, and some crucial tips, I can help you navigate your way to acing this challenging certification exam.First off, this exam is no small feat. It's designed for individuals who are engaged in a DevOps or DevSecOps engineering role, focusing on operating, managing, and provisioning distributed application systems on the AWS platform. This certification validates your technical expertise in continuous delivery systems, methodologies on AWS, security controls automation, and much more.Understanding the ExamBefore diving deep into my study plan and the resources I used, let's get a brief overview of what this exam entails. The AWS Certified DevOps Engineer Professional exam is recognized as one of the more challenging tests compared to other AWS professional level exams. It takes 180 minutes, costs $300 USD, and contains 75 questions, either multiple choice or multiple response.NOTE: The exam information may change, so be sure to validate this by going to the landing page here:Amazon's Website - AWS Certified DevOps Professional OverviewWhy This Certification?Pursuing this certification was a deliberate move on my part. With several other AWS certifications under my belt, I felt this was the logical next step to help me advance within my career. The knowledge gained from this certification is invaluable for anyone looking to implement highly scalable, high-availability applications on AWS.Prerequisites and PreparationsLet me bereallyhonest; this exam isEXTREMELYdifficult. I strongly recommend having one of the associate-level certifications (either AWS Solutions Architect Associate or AWS Developer Associate) before attempting this one. Additionally, hands-on experience with AWS is crucial for your success, so make sure you are labbing as much as possible!Resources UsedUdemy Course by Stephane Maarek: This was my primary resource. Stephane's detailed explanations and hands-on approach were instrumental in helping me grasp complex concepts quickly. This course is a 10 out of 10, and I highly recommend his other courses for any AWS certification.TutorialsDojo Cheat Sheets: A fantastic, free resource that provides summaries of various AWS services. These cheat sheets are great for quick revisions.TutorialsDojo Practice Exams: In addition to the cheat sheets, I also leveraged the TutorialsDojo practice exams, which mimic the difficulty level of the actual exam, providing both a challenge and a solid learning opportunity.My Study ScheduleMy study plan was rigorous, but effective for me:First 30 Days: I dedicated two hours on weekdays and doubled that on weekends to Stefan Marek\u2019s Udemy course. This allowed me to absorb the content without rushing, with ample time for hands-on practice, especially if you're working a little over 40 hours a week.Next 30 Days: I shifted my focus to reviewing Tutorials Dojo's cheat sheets and taking multiple practice exams a week. This helped identify gaps in my knowledge and solidify what I had learned. In addition, I made sure that I was scoring an 80% or higher for at least 2-3 tries before I booked my exam. The practice exams are just as hard,if not harder, than the actual exam!Remember,consistency is key.So make sure you adjust the schedule based on your pace of learning while ensuring you are regularly dedicating time to your study plan. This'll ensure you don't burn out trying to study for this exam, because it is possible.Emphasized ServicesFrom my experience, certain AWS services played a significant role in the exam. Somake sureyou pay extra attention to:CloudFormationCodeBuild, CodePipeline, CodeDeploy, and CodeCommitLambda, API GatewayKinesis Data Streams and FirehoseInspectorOrganizationsControl TowerGuardDuty (surprisingly)Systems Manager (Session Manager and Patch Manager, in particular)Also, make sure you understand your deployment strategies as well. Knowing the difference between Blue/Green and Rolling deployment is a great starting point.These are just a few, but understanding these services deeply will be highly beneficial for you before taking the practice exams.Testing Experience and TipsWhen test day arrived, I chose to take the exam at a testing center, which I find more conducive for focusing. One piece of advice -hydrate and eat wellbefore the exam. Trust me, it makes a difference and will save you from getting a headache.(speaking from experience)During the exam, you\u2019ll find that time management and the process of elimination are your best strategies. The questions are detailed, which can be time-consuming. Practice pacing yourself with practice exams to optimize your performance.ConclusionFinally, stepping out of the exam, I thought I failed for sure. I was practically over it due to my lack of preparation prior to sitting for it (the headache). However, it was a fulfilling journey and I learned quite a bit from it. I was happy to learn that I passed shortly after as well.For those of you embarking on this journey, I hope my experience sheds some light and guides you towards achieving your certification. Remember, the path may seem daunting, but with the right resources, a solid study plan, and perseverance, you can conquer the AWS Certified DevOps Engineer Professional exam just as I did.Thank you for reading, and I wish you the best of luck. Until next time, happy studying, and I'll see you in the cloud!Disclaimer:This blog post reflects my personal experiences and opinions.This blogs original content is based off of the followingYouTube Video:All images located in the blog post have been sourced from different places. Click on the image to get redirected to the original source."}
{"title": "Two ways to manage secrets for AWS Redshift Serverless with AWS Secrets Manager !!", "published_at": 1712776172, "tags": ["aws", "redshift", "serverelss", "secretsmanager"], "user": "VijayaNirmalaGopal", "url": "https://dev.to/aws-builders/two-ways-to-manage-secrets-for-aws-redshift-serverless-with-aws-secrets-manager--49nn", "details": "While I was working with AWS RDS Databases as part of my Devops journey, I used RDS databases and storing secrets for those DBs was efficient & easier with AWS Secrets Manager.How do You & I feel, when the same feature has been introduced for the newly famous AWS Redshift Serverless Datawarehouse? Well, in my opinion, if we are using a service inside AWS, you have a feature from another service to compliment the former, then go with that, if it suits your requirement & budget2 Ways to manage secrets for Amazon Redshift ServerlessWell, below 2 ways can be used to handle connect to an Amazon Redshift Serverless Database1) We can use IAM User credentials to connect Or2) Use secrets in AWS Secrets Manager to hold database credentialsIf latter, then Secrets are created in 2 different ways as below.From Redshift ServerlessA secret is auto generated when the Amazon Redshift Serverless default namespace is createdFrom Secrets ManagerA secret can be created inside AWS Secrets Manager, with secret type \"Credentials for Amazon Redshift data warehouse\" and most importantly, a tag key starting with 'Redshift'. Most importantly map the namespace with which this secret has to be associated withTo demonstrate, here, I have created a free trial version of \"Amazon Redshift Serverless\" to integrate with \"AWS Secrets Manager\" to create & store username and password for connecting with Amazon Redshift Serverless !!Step 1Firstly, Redshift Serverless workspace has to be created with a workgroup. The below screenshot is an \"In Progress\" status of the sameStep 2Now, check for the \"Status\" of the created workspace in \"Serverless Dashboard\" in the console. Status, as you are aware, should in \"Available\" stateStep 3Moving to AWS Secret Manager, Secret creation, set the required username, password for Amazon Redshift Serverless as below. Also, note that, selection of \"Workspace\" can be made with the list of Redshift Serverless workspaces at the bottomStep 4Complete the creation of secret by either enabling \"Auto Rotation\", if necessary. Thus, secret creation is doneBelow examples have name changes for secrets or workspacesView secrets in AWS Secrets ManagerView the associations in Amazon Redshift ServerlessConnect to the Redshift Query EditorDatabases in the workgroup or namespace can be connected using secrets created earlier(as below)BonusOn launching/using Amazon Redshift Serverless, AWS is providing $300 USD credit for 3 month trial period, to explore the feature !! I got one too :-)I am eager to know about fellow community builders' idea or exposure of using these services together. Do let me know in the comments !!"}
{"title": "Use CloudWatch LogGroups for EC2 logging", "published_at": 1712760411, "tags": ["aws", "cloud", "security", "technology"], "user": "Joris Conijn", "url": "https://dev.to/aws-builders/use-cloudwatch-loggroups-for-ec2-logging-3bd3", "details": "You can protect yourself from losing logs on Amazon EC2 by using CloudWatch Logs. Configure the CloudWatch Agent to stream your logs to a LogGroup. This protects you from losing logs. For example, when the instance is replaced by autoscaling. You are also protected against tampering of the logs. An attacker who has gained access to your system can remove the logs. But the logs in the LogGroup will contain the original log lines.PreparationsWe will need the following resources:InstanceSecurityGroup:Type:AWS::EC2::SecurityGroupProperties:GroupDescription:Security group for the test instanceVpcId:\"{{resolve:ssm:/landingzone/vpc/vpc-id}}\"SecurityGroupEgress:-Description:Allow outbound connectivity to port 443.IpProtocol:tcpFromPort:443ToPort:443CidrIp:0.0.0.0/0Role:Type:AWS::IAM::RoleProperties:PermissionsBoundary:!Subarn:aws:iam::${AWS::AccountId}:policy/landingzone-workload-permissions-boundaryAssumeRolePolicyDocument:Version:2012-10-17Statement:-Effect:AllowAction:sts:AssumeRolePrincipal:Service:ec2.amazonaws.comManagedPolicyArns:-arn:aws:iam::aws:policy/AmazonSSMManagedInstanceCore-arn:aws:iam::aws:policy/AmazonInspector2ManagedCispolicy-arn:aws:iam::aws:policy/CloudWatchAgentServerPolicyInstanceProfile:Type:AWS::IAM::InstanceProfileProperties:Roles:-!RefRoleLogGroup:Type:AWS::Logs::LogGroupProperties:LogGroupName:!Sub${AWS::StackName}-InstanceRetentionInDays:!!int365Enter fullscreen modeExit fullscreen modeInstanceSecurityGroup, allows an outbound connection on 443. The CloudWatch Agent needs to stream the logs to the CloudWatch endpoint.Role, holds the policies that allow the instance to send the logs to the LogGroup.InstanceProfile, the profile used to attach the role to the EC2 instances.LogGroup, the LogGroup used to store the logs in.Setting up the EC2 InstanceIn myprevious blogI wrote about how you can create an EC2 instance using Infrastructure as Code. We will continue on that example. Instead of a single instance we will create an autoscaling group.First we will need a LaunchTemplate. This LaunchTemplate will contain some metadata calledAWS::CloudFormation::Init. The metadata has so called configSets and blocks of config. Here is an example that we will use:LaunchTemplate:Type:AWS::EC2::LaunchTemplateMetadata:AWS::CloudFormation::Init:configSets:default:-CloudFormationInit-CloudWatchLogsCloudFormationInit:files:/etc/cfn/cfn-hup.conf:owner:rootgroup:rootmode:000400content:!Sub|-[main]stack=${AWS::StackId}region=${AWS::Region}/etc/cfn/hooks.d/cfn-auto-reloader.conf:owner:rootgroup:rootmode:000400content:!Sub|-[cfn-auto-reloader-hook]triggers=post.updatepath=Resources.LaunchTemplate.Metadata.AWS::CloudFormation::Initaction=/opt/aws/bin/cfn-init -v --stack ${AWS::StackName} --region ${AWS::Region} --resource LaunchTemplaterunas=rootservices:sysvinit:cfn-hup:enabled:trueensureRunning:truefiles:-/etc/cfn/cfn-hup.conf-/etc/cfn/hooks.d/cfn-auto-reloader.confCloudWatchLogs:packages:yum:amazon-cloudwatch-agent:[]files:/opt/aws/amazon-cloudwatch-agent/etc/amazon-cloudwatch-agent.d/file_amazon-cloudwatch-agent.json:owner:rootgroup:rootmode:000400content:!Sub|-{\"agent\": {\"region\": \"${AWS::Region}\",\"logfile\": \"/opt/aws/amazon-cloudwatch-agent/logs/amazon-cloudwatch-agent.log\",\"debug\": false},\"logs\": {\"logs_collected\": {\"files\": {\"collect_list\": [{\"file_path\": \"/var/log/user-data.log\",\"log_group_name\": \"${LogGroup}\",\"log_stream_name\": \"{instance_id}/user-data.log\"},{\"file_path\": \"/var/log/cfn-hup.log\",\"log_group_name\": \"${LogGroup}\",\"log_stream_name\": \"{instance_id}/cfn-hup.log\"},{\"file_path\": \"/opt/aws/amazon-cloudwatch-agent/logs/amazon-cloudwatch-agent.log\",\"log_group_name\": \"${LogGroup}\",\"log_stream_name\": \"{instance_id}/amazon-cloudwatch-agent.log\"}]}},\"log_stream_name\": \"default_log_stream\"},\"metrics\": {\"append_dimensions\": {\"AutoScalingGroupName\": \"${!aws:AutoScalingGroupName}\"},\"metrics_collected\": {\"disk\": {\"measurement\": [\"used_percent\"],\"metrics_collection_interval\": 60,\"resources\": [\"/\"]}}}}services:sysvinit:amazon-cloudwatch-agent:enabled:trueensureRunning:truefiles:-/opt/aws/amazon-cloudwatch-agent/etc/amazon-cloudwatch-agent.d/file_amazon-cloudwatch-agent.jsonProperties:LaunchTemplateData:BlockDeviceMappings:-DeviceName:/dev/xvdaEbs:DeleteOnTermination:!!booltrueEncrypted:!!booltrueKmsKeyId:!GetAttEncryptionKey.ArnVolumeSize:!!int32VolumeType:gp2DisableApiTermination:!!booltrueIamInstanceProfile:Arn:!GetAttInstanceProfile.ArnImageId:\"{{resolve:ssm:/aws/service/ami-amazon-linux-latest/amzn2-ami-hvm-x86_64-gp2}}\"InstanceType:t3.microSecurityGroupIds:-!RefInstanceSecurityGroupMetadataOptions:HttpTokens:requiredInstanceMetadataTags:enabledUserData:Fn::Base64:!Sub|-#!/bin/bash -x/opt/aws/bin/cfn-init --stack ${AWS::StackName} --region ${AWS::Region} --resource LaunchTemplate/opt/aws/bin/cfn-signal -e $? --stack ${AWS::StackName} --region ${AWS::Region} --resource AutoScalingGroupEnter fullscreen modeExit fullscreen modeCloudFormationInitThe CloudFormationInit holds a configuration. This configuration is used by the cfn-init agent. It comes pre-installed with the AmazonLinux2 and AmazonLinux2023 AMIs. Under the files section you can see that we are defining 2 files:/etc/cfn/cfn-hup.conf/etc/cfn/hooks.d/cfn-auto-reloader.confThe syntax is quite explanatory. You set an owner, group, mode and the content of the file. The content of the tells the EC2 instance where it can find the metadata. During the initial boot this is given in the user-data. But you might change the metadata overtime. This makes sure that the running instances checks if there is an update available.To recap this will make sure that the running EC2 instances stay in sync with the given metadata.CloudWatchLogsThe Amazon Linux AMIs do not come with the CloudWatch Agent you need to install the agent first. You can do this by providing theamazon-cloudwatch-agentpackage, under packages.Now it\u2019s time to configure the agent to stream the log files to CloudWatch LogGroups. We have defined 3 files that will be watched by the agent:/var/log/user-data.log/var/log/cfn-hup.log/opt/aws/amazon-cloudwatch-agent/logs/amazon-cloudwatch-agent.logYou can extend this list to your own needs. In this example each EC2 instance will have 3 streams. 1 per file and the instance id is used as a prefix, example:i-0000000000/user-data.log.You can also do measurements on the instance itself and expose this as a CloudWatch Metric. In this example we are capturing a percentage of the used space of the root volume.We have now configured the agent. But we also need to make sure it\u2019s running. When changes are made to the configuration file, we also need to restart it. We do this by pointing to the config file. Now cfn-init knows when it changes the configuration that it also needs to restart the agent.User DataThe user data will make sure that the configuration is applied. Since we configure the cfn-init to check the configuration it will stay in sync from here on. The second thing it will do is it will send a signal to the AutoScalingGroup resource. If the cfn-init is executed successfully it will signal a success and if not it will signal a failure.This means that if the configuration fails the autoscaling group will receive a failure. As a result it will rollback the stack.AutoScalingGroupThe autoscaling group looks as followed:AutoScalingGroup:Type:AWS::AutoScaling::AutoScalingGroupCreationPolicy:ResourceSignal:Timeout:PT5MUpdatePolicy:AutoScalingRollingUpdate:MinInstancesInService:1MaxBatchSize:1PauseTime:PT5MWaitOnResourceSignals:!!booltrueProperties:LaunchTemplate:LaunchTemplateId:!GetAttLaunchTemplate.LaunchTemplateIdVersion:!GetAttLaunchTemplate.LatestVersionNumberMinSize:!!int1MaxSize:!!int2DesiredCapacity:!!int1VPCZoneIdentifier:-\"{{resolve:ssm:/landingzone/vpc/private-subnet-1-id}}\"-\"{{resolve:ssm:/landingzone/vpc/private-subnet-2-id}}\"Enter fullscreen modeExit fullscreen modeWe will have 2 policies defined, one for creation and one for updates. If the signal configured in the user-data fails the resource will fail and trigger a rollback. If the instance is configured successfully it will continue. In the auto scaling group you can define the amount of instances required. In this example the maximum is 2 and the minimum 1. This makes sure that at least one EC2 instance is running. When a replacement is needed a new instance will be added first and if it is deployed successfully the old one is removed. This is why the max is set to 2 so that we can replace the old one gracefully.ConclusionConfiguring a LogGroup to stream your log files is not hard. Using CloudFormation Init you can install and configure applications. In our example we configured the CloudWatch agent that will stream the logs.When you have to deal with autoscaling events and/or malicious attackers. You can prevent the loss of logs by centralising your application log files.Photo byPixabayThe postUse CloudWatch LogGroups for EC2 loggingappeared first onXebia."}
{"title": "Use Central configuration for AWS Security Hub operation in AWS multi-accounts", "published_at": 1712711674, "tags": ["aws", "securityhub"], "user": "nishikawaakira", "url": "https://dev.to/aws-builders/use-central-configuration-for-aws-security-hub-operation-in-aws-multi-accounts-23n8", "details": "Today I would like to write about AWS Security Hub Central configuration, a feature announced at AWS re:Invent 2023.At the company called Kaminashi I work for, we already have a Security Hub in operation. However, we are halfway to the ideal situation where developers themselves understand the risks and proactively respond to them.That being the case, Security Hub Central configuration is a highly recommended feature that may help.AWS Security Hub Operational Issues and Central configurationI was unaware that this feature had been added until recently.But this feature is indispensable for operating Security Hub. The reason for this is that in Automation rules, it is not possible to write rules by OU or by identifying tags, etc. attached to an account. However, when AWS accounts are managed by Organization, there are various cases such as Sandbox OUs, accounts for prototype implementation, accounts for personal verification (which may be included in Sandbox OUs), etc. How do you control these accounts? You need to think about that.Because when you want to try out a workshop and Security Hub detects something that is not so dangerous, you may think, \u201cForgive me, it's only temporary\u201d.And because of this, there are times when you might want to disable Security Hub itself for such accounts, or loosen the rules. Central configuration can be used for this purpose.Notes on activating AWS Security Hub Central configurationNote that enabling Central configuration while Security Hub is already enabled will cause AWS Config to be redetected.the image attached below is what you will see after activating the Central configuration.In this state, the policy is self-managed, but the account owner is configuring it individually.If that is what you intended, fine, but if you want total control and it is happening, you need to be careful.However, there are times when Self-managed is effective, and those are when an account has already been suspended and cannot be excluded from Organization. In such a situation, the new policy cannot be applied. If that happens, the policy status of that higher OU will be failure. In such a case, please try to change the policy status to Self-Managed to make the policy status successful.Yes, so let's start by creating a default policy. Then apply it to the Root OU. By doing so, the rule is applied to all accounts under the OU to which it belongs. The same policy will also be automatically applied to any accounts created thereafter.Now let's look at policy creation.This is how it looks like, the recommended setting type is \"Use the AWS recommended Security Hub configuration across my entire organization\", but if you choose this, only the \"AWS Foundational  Security Best Practices v1.0.0\" security criteria are in effect and cannot be changed.So, although it is recommended, do not use this, but select \"Customize my Security Hub configuration\" and choose the minimum security criteria to be used by your organization.Incidentally, if you choose \"Use the AWS recommended Security Hub configuration across my entire organization\" without customization, you will not be able to change the policy name, and a policy with the very subtle name \u201cconfiguration-policy-01\u201d will be created initially.(It can be changed later)Confirmation of configurationOnce successfully configured, you will see the following on Security Hub overview page for each AWS account.And if you try to activate the security criteria on this page, you will get the following error message.In the account to which the administration of Security Hub has been transferred with the Central configuration set up, you will see that the policy has been applied as shown in the image below upon success.Furthermore, if a strict policy is applied to the Security OU, which sets all security criteria, the following image shows that only the Security OU has the restrict-policy while the other OUs remain in the general-policy.Very good points about Central configurationAs you may already know from what we have seen so far, what is good about this Central configuration is that you can centrally manage whether or not to activate Security Hub in the first place. As you can see when you do an AWS workshop with a personal verification account, resources that are out of compliance are usually created. I don't want to be notified of alerts in those situations.And it is not hard to imagine that it would be very noisy in a workshop with all the developers participating. So the first good thing is that you can choose to disable Security Hub only for such OUs and accounts. Also, since security standards can be changed flexibly, you can choose PCI DSS only for services that handle credit cards, or choose a security standard that meets your organization's guidelines if you do not handle credit cards. In other words, you can set stricter standards for production and staging environments, and slightly looser standards for development environments, and so on, depending on the environment.Oh, how wonderful!And, as before, you can choose which of these controls to activate, adjust the parameters of the controls, and make other flexible settings!ConclusionPersonally, I am really happy to be able to do better what I could not do with Automation rules. This is exactly what I have been waiting for.This is a good opportunity for everyone to rethink the operation of Security Hub.Thank you for reading to the end."}
{"title": "Breaking News: AWS Bedrock Lands in Sydney", "published_at": 1712703670, "tags": ["bedrock", "aws", "machinelearning"], "user": "Alan Blockley", "url": "https://dev.to/aws-builders/breaking-news-aws-bedrock-lands-in-sydney-36go", "details": "I\u2019m in Sydney this week for the AWS Summit Sydney and I\u2019ve just been left with much excitement after a MASSIVE announcement.This must be one of the year's most anticipated announcements (in our region).  I\u2019ve had customers ask me every other week when this is happening.  I can now say\u2026 TODAY rather than SOON.I can practically hear you all leaning forward, eager to know what it is. Well, hold onto your seats because here it is: AWS Bedrock has officially landed in Sydney (ap-southeast-02)!That\u2019s right!  AWS Bedrock is now available in Sydney (ap-southeast-02)What does this mean for us Aussies? Lower latency for our Gen AI Workloads and the ability to respect data sovereignty.Lower latency for Australian customers writing Gen AI WorkloadsData sovereignty can be respectedAvailable immediately (by request) are the following models:AmazonTitan Text G1 - LiteTitan Text G1 - ExpressTitan Multimodal Embeddings G1**AnthropicClaude 3 SonnetClaude 3 HaikuCohereEmbed EnglishEmbed MultilingualMistral AIMistral 7B InstructMixtral 8x7B InstructMistral LargeSo, what are we waiting for? Let's dive in and start building something amazing right here in our local Sydney region!In fact.  First thing today I asked Claude 3 Haiku in Sydney to give me a Haiku, in celebration of it's launch in Sydney.Write me a Haiku about being an LLM hosted in SydneyArtificial mindHoused in Sydney's bustling heartServe humans with graceLearn more:Amazon BedrockGet Started in Sydney"}
{"title": "Improve your EKS cluster with Istio and Cilium : Better networking and security", "published_at": 1712643556, "tags": ["eks", "istio", "cilium", "platformengineering"], "user": "saifeddine Rajhi", "url": "https://dev.to/aws-builders/improve-your-eks-cluster-with-istio-and-cilium-better-networking-and-security-1fcp", "details": "\ud83d\udcda Introduction:Are you using your applications on anAmazon EKS clusterand want to make your networking and security better?In this blog post, we\u2019ll show you how to use Istio and Cilium to get more features for your Kubernetes environment.IstioandCiliumare two popular open-source projects that work together to help you manage your microservices. Istio is a service mesh that lets you control and secure your services.Cilium is a Container Network Interface (CNI) plugin that handles the networking and security policies for your Kubernetes cluster.By using these two tools together, you can get the benefits of both networking and service mesh features. This will make your Kubernetes environment more flexible, scalable, and secure.Istio and Service Mesh:Istiois a popular open-source service mesh framework that provides a comprehensive solution for managing, securing, and observing microservices-based applications running on Kubernetes.At its core, Istio acts as a transparent layer that intercepts all network traffic between services, allowing it to apply various policies and perform advanced traffic management tasks.Source:https://istio.io/latest/docs/ops/deployment/architecture/arch.svgAn Overview of Cilium 1.15:here are the key highlights of this Cilium 1.15 release:Cilium 1.15 now supports the Gateway API 1.0, which provides the next generation of Ingress capabilities built directly into Cilium. This means users don\u2019t need additional tools to route and load-balance incoming traffic into their Kubernetes cluster.It introduces support for a highly requested feature \u2014 MD5-based password authentication. It also adds additional traffic engineering features like support for BGP LocalPreference and BGP Communities, as well as better operational tooling to monitor and manage BGP sessions.Cilium 1.15 is the first release since Cilium graduated as a CNCF project, marking it as the de facto Kubernetes CNI. The Cilium community is proud of the contributions made to bring the project to this milestone.It also saw improvements to the Cilium service mesh capabilities, including graduating Gateway API and L7 traffic management to stable, as well as graduating next-gen mutual authentication and SPIFFE integration to stable for improved security between services.The observability features in Cilium 1.15 also saw enhancements, with Prometheus metrics, OpenTelemetry collector, and the Hubble UI graduating to stable to provide better visibility into the network.Putting the puzzles together: Cilium with Istio:Whether you want to use Cilium or Istio for your service mesh depends on your use cases, performance considerations, security requirements, and risk tolerance.Cilium is a great CNI, though a lot of its service mesh features are Beta so you would need to be comfortable using potentially less stable features if you adopt it for use cases that aren\u2019t fully stable.Conversely, Istio\u2019s critical features have been stable for a good amount of time. You can compare the feature status of Cilium and Istio to make a more informed decision.The major advantages of integrating Istio and Cilium in an EKS cluster are:1- Networking and Security Integration:Cilium provides the networking capabilities, such as connectivity between pods and services, as well as implementing network policies.Istio provides the service mesh capabilities, allowing you to introduce security features like mutual TLS for service-to-service communication and fine-grained access control.The integration of Cilium and Istio allows you to leverage the benefits of both networking and service mesh features in your Kubernetes environment.2-  Flexibility and Scalability:The combination of Cilium and Istio offers a flexible and scalable solution for managing microservices in a Kubernetes cluster.Cilium\u2019s support for multi-cluster connectivity through CiliumMesh allows you to extend the service mesh across multiple clusters.Istio\u2019s multi-cluster support enables you to manage traffic routing and security policies across different clusters.3-Observability and Monitoring:Istio provides observability features, such as tracing, metrics, and logging, which can help you gain visibility into your microservices architecture.The integration of Istio and Cilium can provide a comprehensive observability solution for your Kubernetes environment.4-Gradual Migration and Adoption:The Istio and Cilium integration can be beneficial when migrating applications to a Kubernetes environment gradually, as it allows you to maintain control over traffic routing and security policies.This setup can be useful in brownfield AWS environments where the VPC design cannot be easily changed.Cilium Configuration:The main goal of Cilium configuration is to ensure that traffic redirected to Istio\u2019s sidecar proxies is not disrupted. Disruptions can happen when you enable Cilium\u2019s kubeProxyReplacement feature (see Kubernetes Without kube-proxy docs), which enables socket based load balancing inside a Pod.To ensure that Cilium does not interfere with Istio, Cilium must be deployed with the --config bpf-lb-sock-hostns-only=true cilium CLI flag or with the socketLB.hostNamespaceOnly Helm value.You can confirm the result with the following command:$ kubectl get configmaps -n kube-system cilium-config -oyaml | grep bpf-lb-sock-hostns bpf-lb-sock-hostns-only: \"true\"Enter fullscreen modeExit fullscreen modeIstio uses a CNI plugin to implement functionality for both sidecar and ambient modes. To ensure that Cilium does not interfere with other CNI plugins on the node, it is important to set the cni-exclusive parameter in the Cilium ConfigMap to false. This can be achieved by using the --set flag with the cni.exclusive Helm value set to false. You can confirm the result with the following command:$ kubectl get configmaps -n kube-system cilium-config -oyaml | grep cni-exclusive cni-exclusive: \"false\"Enter fullscreen modeExit fullscreen modeIstio configuration:When you deploy Cilium and Istio together, be aware of:Either Cilium or Istio L7 HTTP policy controls can be used, but it is not recommended to use both Cilium and Istio L7 HTTP policy controls at the same time, to avoid split-brain problems.In order to use Cilium L7 HTTP policy controls (for example, Layer 7 Examples) with Istio (sidecar or ambient modes), you must:Sidecar: Disable Istio mTLS for the workloads you wish to manage with Cilium L7 policy by configuringmtls.mode=DISABLEunder Istio\u2019s PeerAuthentication.Ambient: Remove the workloads you wish to manage with Cilium L7 policy from Istio ambient by removing either theistio.io/dataplane-modelabel from the namespace, or annotating the pods you wish to manage with Cilium L7 withambient.istio.io/redirection: disabled.When using Kubernetes admission webhooks to inject sidecar proxies together with Cilium overlay mode (VXLAN or GENEVE), istiod pods must be running with hostNetwork: true in order to be reachable by the API server.Hands-On example:The following guide demonstrates the interaction between Istio\u2019s mTLS mode and Cilium network policies, including the caveat described in the Istio configuration section.Prerequisites:Istio is already installed on the EKS cluster.Cilium is already installed with the socketLB.hostNamespaceOnly Helm value.Istio\u2019s istioctl is installed on the local host.Start by deploying a set of web servers and client applications across three different namespaces:kubectl create ns red kubectl -n red apply -f <(curl -s https://raw.githubusercontent.com/cilium/cilium/1.15.3/examples/kubernetes-istio/httpbin.yaml | istioctl kube-inject -f -) kubectl -n red apply -f <(curl -s https://raw.githubusercontent.com/cilium/cilium/1.15.3/examples/kubernetes-istio/netshoot.yaml | istioctl kube-inject -f -) kubectl create ns blue kubectl -n blue apply -f <(curl -s https://raw.githubusercontent.com/cilium/cilium/1.15.3/examples/kubernetes-istio/httpbin.yaml | istioctl kube-inject -f -) kubectl -n blue apply -f <(curl -s https://raw.githubusercontent.com/cilium/cilium/1.15.3/examples/kubernetes-istio/netshoot.yaml | istioctl kube-inject -f -) kubectl create ns green kubectl -n green apply -f https://raw.githubusercontent.com/cilium/cilium/1.15.3/examples/kubernetes-istio/netshoot.yamlEnter fullscreen modeExit fullscreen modeBy default, Istio works in PERMISSIVE mode, allowing both Istio-managed and Pods without sidecars to send and receive traffic between each other. You can test the connectivity between client and server applications deployed in the preceding example by entering the following commands:kubectl exec -n red deploy/netshoot -- curl http://httpbin.red/ip -s -o /dev/null -m 1 -w \"client 'red' to server 'red': %{http_code}\\n\" kubectl exec -n blue deploy/netshoot -- curl http://httpbin.red/ip -s -o /dev/null -m 1 -w \"client 'blue' to server 'red': %{http_code}\\n\" kubectl exec -n green deploy/netshoot -- curl http://httpbin.red/ip -s -o /dev/null -m 1 -w \"client 'green' to server 'red': %{http_code}\\n\" kubectl exec -n red deploy/netshoot -- curl http://httpbin.blue/ip -s -o /dev/null -m 1 -w \"client 'red' to server 'blue': %{http_code}\\n\" kubectl exec -n blue deploy/netshoot -- curl http://httpbin.blue/ip -s -o /dev/null -m 1 -w \"client 'blue' to server 'blue': %{http_code}\\n\" kubectl exec -n green deploy/netshoot -- curl http://httpbin.blue/ip -s -o /dev/null -m 1 -w \"client 'green' to server 'blue': %{http_code}\\n\"Enter fullscreen modeExit fullscreen modeAll commands should complete successfully:client 'red' to server 'red': 200 client 'blue' to server 'red': 200 client 'green' to server 'red': 200 client 'red' to server 'blue': 200 client 'blue' to server 'blue': 200 client 'green' to server 'blue': 200Enter fullscreen modeExit fullscreen modeYou can apply network policies to restrict communication between namespaces. The following command applies anL4 network policythat restricts communication in the blue namespace to clients located only inblueandrednamespaces.kubectl -n blue apply -f https://raw.githubusercontent.com/cilium/cilium/1.15.3/examples/kubernetes-istio/l4-policy.yamlEnter fullscreen modeExit fullscreen modeRe-run the same connectivity checks to confirm the expected result:client 'red' to server 'red': 200 client 'blue' to server 'red': 200 client 'green' to server 'red': 200 client 'red' to server 'blue': 200 client 'blue' to server 'blue': 200 client 'green' to server 'blue': 000 command terminated with exit code 28Enter fullscreen modeExit fullscreen modeYou can then decide to enhance the same network policy to perform additional HTTP-based checks. The following command applies the L7 network policy allowing communication only with the/ipURLpath:kubectl -n blue apply -f https://raw.githubusercontent.com/cilium/cilium/1.15.3/examples/kubernetes-istio/l7-policy.yamlEnter fullscreen modeExit fullscreen modeAt this point, all communication with the blue namespace is broken since the Cilium proxy (HTTP) interferes with Istio\u2019s mTLS-based HTTPs connections:client 'red' to server 'red': 200 client 'blue' to server 'red': 200 client 'green' to server 'red': 200 client 'red' to server 'blue': 503 client 'blue' to server 'blue': 503 client 'green' to server 'blue': 000 command terminated with exit code 28Enter fullscreen modeExit fullscreen modeTo solve the problem, you can disable Istio\u2019s mTLS authentication by configuring a new policy:apiVersion: security.istio.io/v1beta1 kind: PeerAuthentication metadata:   name: default spec:   mtls:     mode: DISABLEEnter fullscreen modeExit fullscreen modeYou must apply this policy to the same namespace where you implement theHTTP-based network policy:kubectl -n blue apply -f https://raw.githubusercontent.com/cilium/cilium/1.15.3/examples/kubernetes-istio/authn.yamlEnter fullscreen modeExit fullscreen modeRe-run a connectivity check to confirm that communication with the blue namespaces has been restored. You can verify that Cilium is enforcing the L7 network policy by accessing a different URL path, for example/deny:$ kubectl exec -n red deploy/netshoot -- curl http://httpbin.blue/deny -s -o /dev/null -m 1 -w \"client 'red' to server 'blue': %{http_code}\\n\" client 'red' to server 'blue': 403Enter fullscreen modeExit fullscreen modeUntil next time  \ud83c\udf89"}
{"title": "Overview of AWS Cost Explorer", "published_at": 1712626041, "tags": ["aws", "awscostexplorer", "billing", "costmanagement"], "user": "Olawale Adepoju", "url": "https://dev.to/aws-builders/overview-of-aws-cost-explorer-252m", "details": "Why AWS Cost Explorer?AWS Cost Explorer is a tool that enables you to view and analyze your costs and usage. You can explore your usage and costs using the main graph, the Cost Explorer cost and usage reports, or the Cost Explorer RI reports. You can view data for up to the last 13 months, forecast how much you're likely to spend for the next 12 months, and get recommendations for what Reserved Instances to purchase. You can use Cost Explorer to identify areas that need further inquiry and see trends that you can use to understand your costs.https://docs.aws.amazon.com/cost-management/latest/userguide/ce-what-is.htmlAWS Cost Explorer offers an easy-to-use interface to visualize and understand your AWS cost and usage over time. Previously, Cost Explorer provided up to 13 months of cost and usage data at daily and monthly granularity as a free feature, with an option for hourly granularity over the past 14 days as a paid feature. However, customers requiring multi-year analysis or understanding cost drivers with resource level details couldn\u2019t complete these tasks in Cost Explorer. Now, with extended multi-year history and more granular resource level data within Cost Explorer, customers no longer need to leave Cost Explorer to perform the above analysisCost Explorer now offers the following features for free:Multi-year data at monthly granularity: you can now access up to 38 months of historical data at monthly granularity, allowing for more comprehensive long-term trend analysis.Resource-level data at daily granularity: Cost Explorer offers resource-level data at daily granularity, spanning over the past 14 days, enabling you to dive into your cost drivers.Note:The hourly data as well as daily resource-level data is available for the past 14 days.Enabling Cost ExplorerYou can enable Cost Explorer for your account by opening Cost Explorer for the first time in the AWS Cost Management console.You can't enable Cost Explorer using the API. After you enable Cost Explorer, AWS prepares the data about your costs for the current month and the last 13 months, and then calculates the forecast for the next 12 months. The current month's data is available for viewing in about 24 hours. The rest of your data takes a few days longer. Cost Explorer refreshes your cost data at least once every 24 hours.You can launch Cost Explorer if your account is a member account in an organization where the management account enabled Cost Explorer.Note:An account\u2019s status within an organization determines what cost and usage data are visible:A standalone account joins an organization. After this, the account can no longer access cost and usage data from when the account was a standalone account.A member account leaves an organization to become a standalone account. After this, the account can no longer access cost and usage data from when the account was a member of the organization. The account can access only the data that's generated as a standalone account.A member account leaves organization A to join organization B. After this, the account can no longer access cost and usage data from when the account was a member of organization A. The account can access only the data that's generated as a member of organization B.An account rejoins an organization that the account previously belonged to. After this, the account regains access to its historical cost and usage data.To sign up for Cost ExplorerSign in to the AWS Management Console and open the AWS Cost Management console.In the navigation pane, choose Cost Explorer.On the Welcome to Cost Explorer page, choose Launch Cost Explorer.Starting Cost ExplorerAfter you enable Cost Explorer, you can launch it from the AWS Cost Management console.Start Cost Explorer by opening the AWS Cost Management console.To open Cost ExplorerSign in to the AWS Management Console and open the AWS Cost Management console athttps://console.aws.amazon.com/cost-management/home.This opens the Cost dashboard that shows you the following:Your estimated costs for the month to dateYour forecasted costs for the monthA graph of your daily costsYour five top cost trendsA list of reports that you recently viewedTo set up multi-year and granular dataEnable multi-year data at monthly granularity and resource-level data at daily granularityUsing the management account, you can enable multi-year data and granular data in Cost Explorer. You do this in the Cost Management preferences in the console.However, in order to enable multi-year and granular data, you first need to manage access to view and edit your Cost Management preferences.You can enable multi-year data at monthly granularity and resource-level data at daily granularity from Cost management preference page available to management account of your organization. Once these features are enabled, they can be used by all accounts in your organization.Note:Enabling multi-year data at monthly granularity: You can click on the checkbox to enable this feature. Once enabled, your data should be available within 48 hours in Cost Explorer.Enabling resource-level data at daily granularity: You can select specific services you want to enable resource data for. The services are listed in the order of their contribution to your AWS bill, with the most expensive service on top. Once enabled, your data will be available in Cost Explorer within 48 hours.Sign in to the AWS Management Console and open the AWS Cost Management console athttps://console.aws.amazon.com/cost-management/homeIn the navigation pane, choose Cost Management preferences.To get historical data for up to 38 months, select Multi-year data at monthly granularity.To enable resource-level or hourly granular data, consider the following options:Hourly granularitySelect Cost and usage data for all AWS services at hourly granularity to get hourly data for all AWS services without resource-level data.Select EC2-Instances (Elastic Compute Cloud) resource-level data to track EC2 cost and usage at instance level at hourly granularity.Daily granularitySelect Resource-level data at daily granularity to get resource-level data for individual or all AWS services.Choose services from the AWS services at daily granularity dropdown list that you want to enable resource-level data for.Enabling historical data display for 38 monthsYour business, applications, and architecture have matured in the past few years and you are wondering how your AWS spend has evolved along with that. You can now perform this analysis for the past three years in Cost Explorer and get a better understanding of your year-over-year or quarter-over-quarter spend patterns. In Cost Explorer, you can now select a start date within the past three years and set an end date to any date up to the present day to create multi-year data view. You can filter and group this data by various dimensions, such as service, account, region, usage type to perform comprehensive analysis.Enable Resource-level data at daily granularityNote:Hourly granularity(up to 14 days of past data) is a paid featurehttps://docs.aws.amazon.com/cost-management/latest/userguide/ce-hourly-granularity.htmlYou have noticed variance in your Lambda spend in the past two weeks and you are wondering what is causing that at the resource level. You can now perform this analysis in Cost Explorer and pinpoint the exact Lambda functions responsible for the variance. You can then discuss these functions with respective teams to differentiate intended from unintended spend.You can filter Cost Explorer for Lambda service to focus on Lambda cost and usage."}
{"title": "Accelerating App Development with AppSync Gen2 & Generative AI", "published_at": 1712600439, "tags": ["aws", "awscommunity", "awsmeetup", "ai"], "user": "Felipe Malaquias", "url": "https://dev.to/aws-builders/accelerating-app-development-with-appsync-gen2-generative-ai-3l21", "details": "In this insightful AWS meetup hosted by Idealo in their Berlin office, Felipe Malaquias delves into the transformative power of AWS Amplify Gen2 and generative AI in expediting app development. AWS Amplify Gen2 offers an array of libraries and services and empowers frontend developers to create full-stack applications with ease. With it, creating robust applications becomes seamless, requiring minimal infrastructure knowledge. He'll also explore the burgeoning field of generative AI, including large language models, showcasing their potential to revolutionize innovation and time to market."}
{"title": "Data API for Amazon Aurora Serverless v2 with AWS SDK for Java - Part 5 Basic cold and warm starts measurements", "published_at": 1712591574, "tags": ["aws", "serverless", "java", "database"], "user": "Vadym Kazulkin", "url": "https://dev.to/aws-builders/data-api-for-amazon-aurora-serverless-v2-with-aws-sdk-for-java-part-5-basic-cold-and-warm-starts-measurements-4gi9", "details": "IntroductionIn thepart 1of the series we set up the sample application which has API Gateway in front of Lambda functions which communicate with Aurora Serverless v2 PostgreSQL database via Data API to create the products and retrieve them (by id). In thepart 2we dove deeper into the new Data API for Aurora Serverless v2 itself and its capabilities like executing SQL Statements and used AWS SDK for Java for it. In thepart 3we took a look at the Data API capabilities to batch SQL statement over an array of data for bulk update and insert operations. In thepart 4we learned how to use database transactions with Data API.In this part of the series we'll perform some basic measurements of the cold and warm start/execution time of the operation via Data API for Amazon Aurora Serverless v2.Measuring cold and warm starts of the operation via Data API for Amazon Aurora Serverless v2In our experiment we'll re-use the application introduced in thepart 1which source code you can findhere. There are basically 2 Lambda functions which both respond to the API Gateway requests to store and retrieve product  received from the API Gateway from Amazon PostgreSQL Aurora Serverless v2.The results of the experiment to retrieve the existing product from the database by its id seeGetProductByIdViaAuroraServerlessV2DataApiHandlerwith Lambda function with 1024 MB memory setting were based on reproducing more than 100 cold and approximately 10.000 warm starts with experiment which ran for approximately 1 hour. For it (and experiments from my previous article) I used the load test toolhey, but you can use whatever tool you want, likeServerless-artilleryorPostman. We won't enable SnapStart on the Lambda function first.Cold (c) and warm (m) start time in ms:c p50c p75c p90c p99c p99.9c maxw p50w p75w p90w p99w p99.9w max3154.3532373284.913581.493702.123764.92104.68173.96271.32572.111482.892179.7ConclusionIn this part of the series performed some basic measurements of the cold and warm start/execution times of the operation via Data API for Amazon Aurora Serverless v2 without enabling SnapStart on the Lambda function. We observed quite a big cold start times but also the first call additionally results in the bigger warm start (execution time) as the database connection needs to be initialized on the database side. So, especially the very first execution and the subsequent ones where the cold starts occurs together with database connection initialization on the database side are very expensive.In the next part of the series we'll perform the measurements but with using the standard connection management  approaches (in case of Java with JDBC) including the usage of Amazon RDS Proxy."}
{"title": "Terraform Test and AWS Lambda", "published_at": 1712501997, "tags": ["aws", "lambda", "terraform"], "user": "Bervianto Leo Pratama", "url": "https://dev.to/aws-builders/terraform-test-and-aws-lambda-5158", "details": "You may find my base repository here.bervProject/lambda-sharpLambda use C# (.NET) Docker ImageLambda Sharp DemoDemo for Lambda Containerized .NETLICENSEMITView on GitHubPlease see my previous post for the step-by-step creation of the lambda itself.Designing the testingI may extract some \"constant\" to variables, such as image_uri, lambda timeout, function_name, etc. After some extraction, we will write \"basic\" (unit) testing without end-to-end testing.variables{lambda_image=\"test\"}run\"valid_image\"{command=planassert{condition=aws_lambda_function.lambda_container_demo.image_uri==\"test\"error_message=\"AWS Lambda Function Image URI did not match\"}}run\"valid_function_name\"{command=planassert{condition=aws_lambda_function.lambda_container_demo.function_name==\"lambda_container_demo\"error_message=\"AWS Lambda Function Name did not match\"}}run\"valid_lambda_timeout\"{command=planassert{condition=aws_lambda_function.lambda_container_demo.timeout==60error_message=\"AWS Lambda timeout did not match\"}}run\"valid_dynamo_db_capacity\"{command=planassert{condition=aws_dynamodb_table.lambda_container_demo_dev.read_capacity==20error_message=\"Dynamo DB read capacity did not match\"}assert{condition=aws_dynamodb_table.lambda_container_demo_dev.write_capacity==20error_message=\"Dynamo DB write capacity did not match\"}}Enter fullscreen modeExit fullscreen modeHowever, it's not challenging, right? I'll share more challenging tests (maybe end-to-end ones) after this post. Stay tuned!You may check this pull request for the changes.Add Terraform Test#64berviantoleoposted onNov 09, 2023View on GitHub"}
{"title": "Deploy serverless Lambda TypeScript API with function url using AWS CDK", "published_at": 1712498367, "tags": ["aws", "javascript", "tutorial", "devops"], "user": "Thomas Taylor", "url": "https://dev.to/aws-builders/deploy-serverless-lambda-typescript-api-with-function-url-using-aws-cdk-3e2b", "details": "In November 2023, I wrote a postdescribing how to deploy a lambda function with a function urlin Python. For this post, I want to showcase how streamlined and practical it is to deploy a \"Lambdalith\" (a single Lambda function) that contains an entire API.What this means:No API GatewayAPI requests can take longer than 30 secondsFaster deploymentsLocal testing without cloud deploymentReduced costs *Easy management ** = Depends on the use-caseHow to deploy a serverless API using FastifyTo begin, let's initialize a CDK application for Typescript:cdk init app --language typescriptEnter fullscreen modeExit fullscreen modeThis creates the boilerplate directories and files we'll need:serverless-api \u251c\u2500\u2500 README.md \u251c\u2500\u2500 bin \u2502   \u2514\u2500\u2500 serverless-api.ts \u251c\u2500\u2500 cdk.json \u251c\u2500\u2500 jest.config.js \u251c\u2500\u2500 lib \u2502   \u2514\u2500\u2500 serverless-api-stack.ts \u251c\u2500\u2500 package-lock.json \u251c\u2500\u2500 package.json \u251c\u2500\u2500 test \u2502   \u2514\u2500\u2500 serverless-api.test.ts \u2514\u2500\u2500 tsconfig.jsonEnter fullscreen modeExit fullscreen modeInstall and configure FastifyFastify is a JavaScript web frameworkthat intentionally aims for low overhead and speed over other frameworks such as express. I have arbitrarily chose it for this tutorial, but any web framework that supports routing will work.Install fastifyInstallfastifyusing one of the methodsdescribed in their documentationandtheir AWS Lambda adapter@fastify/aws-lambda.For this tutorial, I'll usenpm.npm i fastify @fastify/aws-lambda @types/aws-lambdaEnter fullscreen modeExit fullscreen modeCreate an entry fileTo make it easy, we'll create an entry point for the lambda athandler/index.tswith the following contents:importFastifyfrom\"fastify\";importawsLambdaFastifyfrom\"@fastify/aws-lambda\";functioninit(){constapp=Fastify();app.get('/',(request,reply)=>reply.send({hello:'world'}));returnapp;}if(require.main===module){// called directly i.e. \"node app\"init().listen({port:3000},(err)=>{if(err)console.error(err);console.log('server listening on 3000');});}else{// required as a module => executed on aws lambdaexports.handler=awsLambdaFastify(init())}Enter fullscreen modeExit fullscreen modeThe directory structure should look like the following tree:serverless-api \u251c\u2500\u2500 README.md \u251c\u2500\u2500 bin \u2502   \u2514\u2500\u2500 serverless-api.ts \u251c\u2500\u2500 cdk.json \u251c\u2500\u2500 handler \u2502   \u2514\u2500\u2500 index.ts \u251c\u2500\u2500 jest.config.js \u251c\u2500\u2500 lib \u2502   \u2514\u2500\u2500 serverless-api-stack.ts \u251c\u2500\u2500 package-lock.json \u251c\u2500\u2500 package.json \u251c\u2500\u2500 test \u2502   \u2514\u2500\u2500 serverless-api.test.ts \u2514\u2500\u2500 tsconfig.jsonEnter fullscreen modeExit fullscreen modeWith this method, we are able to test locallywithoutdeploying to the cloud.First, transpile the Typescript files to JavaScript:npm run buildEnter fullscreen modeExit fullscreen modeThen execute thehandler/index.jsfile withnode:node handler/index.jsEnter fullscreen modeExit fullscreen modeIf you visithttp://localhost:3000in your browser, it should display:{\"hello\":\"world\"}Enter fullscreen modeExit fullscreen modeDeploying the function with the function url enabledFortunately, the AWS CDK enables users to quickly deploy using theNodeJSFunctionconstruct. Replace the contents ofserverless-api-stack.tswith the following snippet:import*ascdkfrom'aws-cdk-lib';import{Construct}from'constructs';import{NodejsFunction}from'aws-cdk-lib/aws-lambda-nodejs';import{FunctionUrlAuthType}from'aws-cdk-lib/aws-lambda';exportclassServerlessApiStackextendscdk.Stack{constructor(scope:Construct,id:string,props?:cdk.StackProps){super(scope,id,props);consthandler=newNodejsFunction(this,'handler',{entry:'./handler/index.ts',timeout:cdk.Duration.minutes(1)});consturl=handler.addFunctionUrl({authType:FunctionUrlAuthType.NONE});newcdk.CfnOutput(this,'url',{value:url.url});}}Enter fullscreen modeExit fullscreen modeThe code creates aNodejsFunctionlambda, enables the function url without authentication, and outputs the url as a CloudFormation export.Deploy the stack using the cdk:npx cdk deployEnter fullscreen modeExit fullscreen modeThe command output contains theCfnOutputvalue:Do you wish to deploy these changes (y/n)? y ServerlessApiStack: deploying... [1/1] ServerlessApiStack: creating CloudFormation changeset...   \u2705  ServerlessApiStack  \u2728  Deployment time: 38.19s  Outputs: ServerlessApiStack.url = https://{id}.lambda-url.us-east-1.on.aws/ Stack ARN: arn:aws:cloudformation:us-east-1:123456789012:stack/ServerlessApiStack/{id}Enter fullscreen modeExit fullscreen modeIf you navigate to the url, you will view the expected results displayed:{\"hello\":\"world\"}Enter fullscreen modeExit fullscreen modeAll of this was completed withverylittle infrastructure to manage and a singleindex.tsfile. From here, you can expand the project to include as many routes as you prefer."}
{"title": "DevOps with Guruu | Chapter 19 : Deploy AWS Backup for The System", "published_at": 1712481684, "tags": ["webdev", "aws", "devops"], "user": "Hoang Guruu", "url": "https://dev.to/aws-builders/devops-with-guruu-chapter-19-deploy-aws-backup-for-the-system-1475", "details": "DevOps with Guruu | Chapter 19 : Deploy AWS Backup for The System0:00 Result1:00 Deploy infrastructure - Create S3 Bucket5:30 Create Backup plan9:30 Set up notifications18:30 Test Restore Result24:30 Clean up resourcesJoin me on this journey to mastering DevOps. Let's get hands-on and build a strong foundation together! Don't forget to like, share, and subscribe for more exciting content. Happy coding, future DevOps!\""}
{"title": "Trace of Data Using Amazon OpenSearch Service", "published_at": 1712474180, "tags": ["aws", "amazonopensearch", "iamrole", "data"], "user": "GargeeBhatnagar", "url": "https://dev.to/aws-builders/trace-of-data-using-amazon-opensearch-service-5a9b", "details": "\u201c I have checked the documents of AWS to get the solution for trace of data using amazon opensearch service, in such a scenario it is possible to get into the opensearch dashboard and verify the data fetched in it . In terms of cost, need to pay for opensearch service includes instance hours, amount of storage and data transferred in and out of service. Storage pricing depends on the storage tier and type of instance to be choosed.\u201dAmazon OpenSearch Service is a managed service that makes it easy to deploy, operate and scale opensearch, a popular open-source search and analytics engine. Opensearch service also offers security options, high availability, data durability and direct access to the opensearch API.In this post, you will experience how to trace of data using amazon opensearch service. Here I have created an amazon opensearch service domain with iam role.Architecture OverviewThe architecture diagram shows the overall deployment architecture with data flow, amazon opensearch service, opensearch dashboard, sample data.Solution overviewThe blog post consists of the following phases:Create of Amazon OpenSearch Service Domain and Verify of ConfigurationsOutput of OpenSearch Dashboard with Sample Ecommerce DataPhase 1: Create of Amazon OpenSearch Service Domain and Verify of ConfigurationsOpen the console of Amazon opensearch service, create a domain with managed clusters. Give domain name as \u201copensearch-domain-one\u201d and domain name method as standard create. Use template as dev/test, deployment options including Az and engine versions, data nodes, network access and fine grained access control.Phase 2: Output of OpenSearch Dashboard with Sample Ecommerce DataClean-upDelete of Amazon opensearch Domain and IAM role.PricingI review the pricing and estimated cost of this example.Cost of Amazon opensearch service = $0.167 per r6g.large.search instance hour + $0.122/GB-month gp3 storage = $(0.167x4 + 0.122x1)Total Cost = $0.79SummaryIn this post, I showed \u201chow to trace of data using amazon opensearch service\u201d.For more details on Amazon OpenSearch Service, Checkout Get started Amazon OpenSearch Service, open theAmazon OpenSearch Service console. To learn more, read theAmazon OpenSearch Service documentation.Thanks for reading!Connect with me:Linkedin"}
{"title": "3 Proven Patterns for Reporting with Serverless", "published_at": 1712419060, "tags": ["aws", "serverless", "database", "datascience"], "user": "Benjamen Pyle", "url": "https://dev.to/aws-builders/3-proven-patterns-for-reporting-with-serverless-5a08", "details": "Serverless architecture has given developers, architects, and business problem solvers new capabilities to deliver value to customers.  It feels like we are in the age of maturation of serverless in so many ways.  The boom of containers, the birth of functions, and now the options of even Zero-code serverless implementations (Step Functions).  These are all great things from an application developer's perspective.  But what about reporting?  There are so many signals emitted from these systems but has the reporting world kept up with the changes in architecture?  And what are some strategies to take advantage of data in this current landscape?  Let's have a look at Reporting with Serverless.ArchitectureLet's take a look at a small but fairly typical architecture that I see quite often.  I have strong opinions that each microservice boundary should have its database.  That could be an RDS instance, Aurora cluster, SQL Server, or multiple DynamoDB tables that it and only it can directly interact with.  Any communication outside of this boundary with the data it owns has to happen through an event or an HTTP/gRPC API.  I could diagram them like the below:Now I do believe there is more to this topic that needs to further refine the context.  The minute an application starts to silo its data is the minute that data replication or virtualization enters into the equation most of the time.  Picture an enterprise business-to-business application that operates a large software solution broken up into 100 different parts.  The challenges that exist for unifying that information are much different than the challenges of a consumer application that has a high volume but a low surface of data stores due to the nature of the application scope.This is wherethinking like a consultantcomes into play when helping to figure out which kinds of problems need to be solved.  But make no mistake, reporting with serverless is a problem that will need to be addressed.  And to be clear, let's outline exactly what that is.The scenario I'm writing about here is this.  In your application, you will have multiple root-level entities that need to be viewed and aggregated in a unified result.  That could be an analytic that counts the number of times a user placed an order or it might be to print a user's last order that they made.Our reference point for reporting with serverless is going to be this order, items, and user diagram.3 Patterns for Reporting with MicroservicesI've been working with serverless and microservices for 7 years now and have used some variations of the same 3 approaches to solve reporting with serverless in multiple problem domains. These patterns will serve as starting points as you begin to think about how to solve reporting with serverless.I'd describe the patterns like this.Replicate to readSplit for speed and readEventually readableReplicate to ReadSolving for reporting with serverless can be done very simply if all of your data is stored in something that can handle dynamic queries like SQL.  The beauty of a relational database management system (RDBMS) is that your data is not pre-optimized for the reads you will need but the system can be guided to make efficient queries based upon the hints you leave it.  This is done with indexing.In a scenario like this, your reporting and analytics solution can just go straight to the sources and pull the data it needs to generate the information required.  This might feel similar to what's been done before when the data was stored in a monolithic and singular database.  And it 100% is.The reason I call this pattern Replicate to Read is that it's often advantageous to use Read Replicas to help with connection pooling and resource management to not clog the transactional system with analytics workloads.ProsThe pros to the replicate-to-read pattern while solving reporting with serverless are as follows:First, is its simplicity.  In many cases, read replicas on the source may not even be needed and the system can get away with going directly to the primary tables.  The reporting team can connect and get going.  If there are problems that arise from this related to connection management or resource utilization, read replicas can be a great answer for solving this problem.Second, speed.  The data sits right next to the source and sometimes it is the source.  This lack of transformation can be a serious boost because the data requires no hops to land in a final form.ConsWhile this pattern is great for speed and it is extremely simple, it does suffer from a few limitations.The first of those data is not stored or optimized for reporting.  Many RDBMS schemas are highly normalized and optimized for the workload of being transactional.  This design sits in opposition to reporting schemas.Next, the matter of blending these different datasets falls squarely on the reporting tool of choice.  This often leads to complex reports, joins, and merges of datasets.And lastly, connection management.  The report needs to hit 3 separate systems and this all happens over the network.  There's a chance of failure and additional complexity which again falls on the reporting tool.Wrap UpThe bottom line, this is a great option if the solution is built upon purely RDBMS technologies and the number of data sources is low.  If this is as far as the system ever needs to go, then fantastic!Split for Read and SpeedReporting with serverless requires balancing requirements which should include things that generally get lumped into the non-functional category.  How quickly the data is available is often a requirement.Think for instance about our Orders and Users domain.  The product owner might ask for the user to be able to print their order shortly after making it.  Or that the order is available for download once the purchase is made.  Going back to the replicate to read pattern, this would be a simple use case.  However, let's assume that one of the microservices is storing its data in DynamoDBRemember, there is no one-size-fits-all here but chances are, the various needs and filters of a report will outgrow your ability to design and model your DynamoDB table.  If the use cases are super simple, this might not happen, but in many cases, the data needs to be more queryable.  So enter the Split for Read and Speed.With this pattern, the system can make the best of both worlds.  For scenarios where the data is stored in an RDBMS, make use of the techniques highlighted in the Replicate to Read pattern.  But for data stored in something like DynamoDB, I'm introducing the notion of streaming changes, transforming them and ultimately landing them in AWS Redshift.There's plenty to unpack here that is outside of the scope of this article.  Topics like, why Redshift?  Should it be DDB Streams or straight to Kinesis? Why not EventBridge Pipes?  And many more that could come up.  However, I want to focus not on the implementation but more on the pattern.  And that's handling DynamoDB changes that are transformed into a system that provides the query flexibility of SQL.ProsUsing the Split for Read and Speed pattern is great for solving reporting with serverless where there is a mixed mode of database storage technologies.  The main advantages of this pattern are these:A nice balance of speed and storage.  For the RDBMS stored services, the same advantages from the Replicate to Read pattern are gained.  Speed is enhanced because the data sits right next to the source.  And then data that is stored in NoSQL can be streamed in pretty much real-time for live processing before landing in something like Redshift which provides that SQL implementation that mirrors that of the other sources.It also allows some splitting and leveraging of best-in-breed technologies for solving problems.  Developers can take advantage of the transactional and serverless features of DynamoDB while the reporting team can then use tools and technologies that are more familiar when building out reports.  Again, a common SQL language for the reporting team is super important.ConsThis pattern still struggles with the same ones that the Replicate to Read pattern does.Connection management and data transformation is still encapsulated in the reporting tool.  Depending upon the application size and number of services, this might not be a huge issue, but is something to look out for.Additionally, another set of steps has been added to support the transformation of Documents or Key/Value items into relational rows.  Honestly, the bigger the system, this is inevitable in my experience.  And it will be more apparent in the last pattern below.Lastly, there is a lack of extensibility that shows up in this pattern.  Data is stored on purpose for reporting.  That's great if that's where the journey ends as it relates to reporting with serverless, but there often is more to be accomplished.Wrap UpI find that this pattern is a nice blend for a lot of applications and might even be the starting point if the application uses no RDBMS technologies.  The truth is, that a lot of modeling can be done in DynamoDB for reporting.  Especially if the use cases revolve around single-item views or analytic-type calculations.  But remember, all of that needs to be defined upfront.  I haven't worked on a project in my career where the product team could define all of the reporting requirements upfront.  Which is why SQL is so heavily used. It allows for this dynamic packaging of data that is often required.Eventually ReadableEventually Readable is the most flexible yet not the fastest pattern when working on reporting with serverless.I tend to reach for this pattern when my primary goal isExtensibility.The data will ultimately land in a data lake that allows for so many additional use cases such as outside data blending, exploration, machine learning, and of course reporting.It also allows me to split workloads out and then almost reverse the microservice flows into isolated team flows.  For instance, there could be teams of analysts working on just users and orders.  There could also be a team working on orders and items.  Having the data fully unified before doing any of that work can then create specialized areas and patterns for further making information.Before you want to go all in though, I will caution it comes with the most complex price tag.  It also turns the data pipeline problem into an engineering problem.  Iwrote about that here.  You will see some interesting parallels between the teams building user applications and the teams building data applications.  Pairing this with serverless elevates the problems to a standard set of capabilities that allows for a great deal of cross-team collaboration.ProsThe pros for this pattern center around ultimate flexibility.  Once the data is unified in the data lake, the possibilities are truly endless.Want to do some machine learning? Great, hook up SageMaker.  Need something queryable in SQL? Awesome, stream with Firehose and straight to Redshift via the COPY command.  Need more data than your system provides? Cool.  Load it via another process not documented in that diagram.I want to stress that this flexibility is the reason that this pattern is used.Secondarily though the pattern starts to carve out more specificity in the roles that are needed to accomplish the design.  The real-time transformations are engineering problems that will require serverless engineers.  The data lake curation falls squarely into a data analysis bucket.  And then preparation and schema modeling when doing reporting is squarely a business intelligence architect.Remember, this flexibility comes with complexity which leads to the cons.ConsTwo main cons come with this pattern.  I mentioned in the above, complexity.  More things can go wrong here.  For each of the orders, items, and user services, there needs to be real-time transformation that lands the data into the lake.The second big drawback is speed.  There will be more latency in this scenario.  How much latency depends upon the environment.  If there is RDBMS in the source,AWS Data Migration Servicewill at worst take around 60 seconds to replicate.  That cost needs to be accounted for.  Secondarily, many triggering events are leveraged which happen fairly quickly but they do add up.Lastly, depending upon how the data lands in something like Redshift, a Firehose COPY command from S3 does buffer before writing.  Again, latency.It's always a balance.Wrap UpSolving for reporting with serverless using the Eventually Readable pattern comes down to how much flexibility is desired and balanced against latency and complexity tolerance.  This is why technology is so fun.  Everything is a trade-off.Reporting with ServerlessServerless is about the composition and extension of capabilities that expand and collapse with usage.  When thinking about reporting with serverless, it makes sense to use the same architectural principles that are used to build applications in the data and reporting space.I've seen people have modern serverless applications only to cripple them with traditional ETL processes and patterns for dealing with reporting.  I'd challenge you to think differently and lean into the skills you are already building and using on the application side.Wrapping UpBe kind to your reporting friends when building applications.  In my experience, report developers are often struggling with changing requirements, fickle users, data inconsistencies, and many other challenges that for application developers seem out of sight out of mind.  But if you think about it, reporting with serverless is harder because of microservices.  If everything was still in a monolith, many of these problems don't exist.At some level, we've caused these problems so it only makes sense that we help our friends solve them.Start with these patterns and then evolve from there.  There is plenty of room for variation and adaptation as well.Replicate to readSplit for speed and readEventually readableMy hope here is that I've brought some awareness that application development makes report development harder when adopting microservices.  But by using the techniques used to build serverless applications, we can make things whole again by adopting those techniques for reporting with serverless.Thanks for reading and happy building!"}
{"title": "RAG Application using AWS Bedrock and LangChain", "published_at": 1712416615, "tags": ["aws", "langchain", "ai", "awscommunitybuild"], "user": "Somil Gupta", "url": "https://dev.to/aws-builders/rag-application-using-aws-bedrock-and-langchain-140b", "details": "Hello, good folks!!In this part of building the RAG application series, we will leverage Mistral's new model Large using AWS Bedrock and LangChain framework to query over the pdfs.In the previous article of the series, we learned to build an RAG application using AWS Bedrock and LlamaIndex. To learn more about \"what RAG is\", please refer to the below article.Learn to Build a Basic RAG Application | by Somil Gupta | Apr, 2024 | AWS in Plain EnglishEnd-to-end Guide Using AWS Bedrock and LlamaIndex to Query Over Your Own PDFsaws.plainenglish.ioLet's get the learning started.The implementation of this application involves three components:1. Create a Vector\u00a0StoreLoad -> Transform -> EmbedWe will be using the FAISS vector database, which uses the Facebook AI Similarity Search (FAISS) library. There are many excellent vector store options that you can use, such as ChromaDB or LanceDB.2. Query Vector Store and 'Retrieve Most Similar.'The way to handle this is at query time, embed the unstructured query and retrieve the embedding vectors that are 'most similar' to the embedded query. A vector store takes care of storing embedded data and performing vector search for you.3. Frame the response using LLM and 'Enhanced Context'Response Generation Using LLM (Large Language Model): Once the relevant documents are retrieved from Vector Store, a large language model uses the information from these documents to generate a coherent and contextually appropriate response.These three steps clearly explain the application we are going to build now.First and foremost, we will set up our AWS SDK for Python using Boto3 and AWS CLI. If you have not installed them before\u200a-(base) \u279c  ~ pip3 install boto3 (base) \u279c  ~ pip3 install awscli  (base) \u279c  ~ aws configureEnter fullscreen modeExit fullscreen modeIn this example, we'll use the AWS Titan Embeddings model to generate embeddings. You can use any model that generates embeddings.importboto3# Load the Bedrock client using Boto3.bedrock=boto3.client(service_name='bedrock-runtime')fromlangchain_community.embeddings.bedrockimportBedrockEmbeddingstitan_embeddings=BedrockEmbeddings(model_id=\"amazon.titan-embed-text-v1\",client=bedrock)Enter fullscreen modeExit fullscreen modeNow, we will set up the Vector Store to store and retrieve embeddings. We have our PDF stored in the \"data\" folder of the root directory.In this case we'll split our documents into chunks of 1000 characters with 200 characters of overlap between chunks. The overlap helps mitigate the possibility of separating a statement from an important context related to it.We will leverage RecursiveCharacterTextSplitter from LangChain, which will recursively split the document using common separators like new lines until each chunk is the appropriate size.We can embed and store all of our document splits in a single command using the FAISS vector store and titan embedding model.# Vector Store for Vector Embeddingsfromlangchain_community.vectorstores.faissimportFAISS# Imports for Data Ingestionfromlangchain.text_splitterimportRecursiveCharacterTextSplitterfromlangchain_community.document_loaders.pdfimportPyPDFDirectoryLoader# Load the PDFs from the directorydefdata_ingestion():loader=PyPDFDirectoryLoader(\"data\")documents=loader.load()# Split the text into chunkstext_splitter=RecursiveCharacterTextSplitter(chunk_size=1000,chunk_overlap=200)docs=text_splitter.split_documents(documents)returndocs# Vector Store for Vector Embeddingsdefsetup_vector_store(documents):# Create a vector store using the documents and the embeddingsvector_store=FAISS.from_documents(documents,titan_embeddings,)# Save the vector store locallyvector_store.save_local(\"faiss_index\")Enter fullscreen modeExit fullscreen modeThe next step is to import and load the LLM via Bedrock.# Import Bedrock for LLMfromlangchain_community.llms.bedrockimportBedrock# Load the LLM from the Bedrockdefload_llm():llm=Bedrock(model_id=\"mistral.mistral-large-2402-v1:0\",client=bedrock,model_kwargs={\"max_tokens\":512})returnllmEnter fullscreen modeExit fullscreen modeWe will be using LangChain PromptTemplate to create the prompt template for our LLM. We will produce an answer using a prompt that includes the question and the retrieved data (context).fromlangchain.promptsimportPromptTemplate# Create a prompt templateprompt_template=\"\"\"Use the following pieces of context to answer the  question at the end. Please follow the following rules: 1. If the answer is not within the context knowledge, kindly state  that you do not know, rather than attempting to fabricate a response. 2. If you find the answer, please craft a detailed and concise response  to the question at the end. Aim for a summary of max 250 words, ensuring  that your explanation is thorough.  {context}  Question: {question} Helpful Answer:\"\"\"PROMPT=PromptTemplate(template=prompt_template,input_variables=[\"context\",\"question\"])Enter fullscreen modeExit fullscreen modeNow, let's write the actual application logic. We want to create a simple application that takes a user question, searches for documents relevant to that question, passes the retrieved documents and initial question to a model, and returns an answer.We need to define the LangChain Retriever interface. Load RetrievalQA from LangChain as it provides a simple interface for interacting with the LLM.fromlangchain.chains.retrieval_qa.baseimportRetrievalQA# Create a RetrievalQA chain and invoke the LLMdefget_response(llm,vector_store,query):retrieval_qa=RetrievalQA.from_chain_type(llm=llm,chain_type=\"stuff\",retriever=vector_store.as_retriever(search_type=\"similarity\",search_kwargs={\"k\":3}),chain_type_kwargs={\"prompt\":PROMPT},return_source_documents=True,)returnretrieval_qaEnter fullscreen modeExit fullscreen modeLet's put it all together into a chain. This tutorial will use Streamlit to create a UI that interacts with our RAG.We will provide a simple button in the sidebar to create and update a vector store and store it in the local storage.Whenever a user enters a query, we will first get the faiss_index from our local storage and then query our LLM using the retrieved context.defstreamlit_ui():st.set_page_config(\"My Gita RAG\")st.header(\"RAG implementation using AWS Bedrock and Langchain\")user_question=st.text_input(\"Ask me anything from My Gita e.g.                                            What is the meaning of life?\")withst.sidebar:st.title(\"Update Or Create Vector Embeddings\")ifst.button(\"Update Vector Store\"):withst.spinner(\"Processing...\"):docs=data_ingestion()setup_vector_store(docs)st.success(\"Done\")ifst.button(\"Generate Response\")oruser_question:# first check if the vector store existsifnotos.path.exists(\"faiss_index\"):st.error(\"Please create the vector store                                  first from the sidebar.\")returnifnotuser_question:st.error(\"Please enter a question.\")returnwithst.spinner(\"Processing...\"):faiss_index=FAISS.load_local(\"faiss_index\",embeddings=titan_embeddings,allow_dangerous_deserialization=True)llm=load_llm()st.write(get_response(llm,faiss_index,user_question))st.success(\"Done\")Enter fullscreen modeExit fullscreen modeThis is how our Streamlit application will look.The complete code for the application is available here on my github:somilg050.You can play around with the code by customizing the prompt and changing the parameters to the LLM.In conclusion, we have created an application that takes a question, retrieves relevant documents, constructs a prompt, passes that to a model, and parses the output.Thanks for reading the tutorial. I hope you learn something new today. If you want to read more stories like this, I invite you to follow me.Till then, Sayonara! I wish you the best in your learning journey."}
{"title": "Let's detect flowers! (with SageMaker and DeepLens)", "published_at": 1712329749, "tags": ["aws", "machinelearning", "ai", "algorithms"], "user": "Delia Ayoko", "url": "https://dev.to/aws-builders/lets-detect-flowers-with-sagemaker-and-deeplens-8li", "details": "As an AWS-Machine-Learning-Oriented parent, you decide to something fun with your 5 year old Emma. You decide to open your list of\"Wanna try\"projects, and you pickFlower Detection.Getting the dataYou: \"Emma, wanna detect flowers with me?\"Emma: \"Yes! Detective Emma!\"gigglesEmma: \"Let's use AWS!\"You: \"Of course we will. First, let's set up an S3 bucket.\"Emma: \"What's that?\"You: \"An S3 bucket. It's a container of objects. The objects are files and the bucket is just a folder. It is different from a normal bucket in that you can store as many things as you want in it. Here, I'm going to put in the flowers.\"Emma: \"But we don't have flowers!\"You: \"Oh yes we do. But first, let's organize the bucket.\"after organizing the data and storing in the S3 bucket...Labeling the dataEmma: \"What do we do now?\"You: \"Let's label the data.\"Emma: \"Why?\"You: \"We want to use a model to detect these images. So it important that the model learns from the labels we give the model for it not to suffer.\"Emma: \"Oh.\"You: \"Yeah. We'll use Amazon Mechanical Turk to label the images. It's a group of people Amazon pays to label data. They're all over the world. Only, you can't see them.\"Emma: \"Let's do it!\"6 hours later, when the labels are ready...Model TrainingEmma: \"Our model is so dumb!\"You: \"Exactly. That's why we're going to teach it.\"Emma: \"What if it doesn't learn?\"You: \"We keep teaching. Until it learns.\"Emma: \"Strenuous.\"You: \"It is. But I can bet that you'll be proud of the result. Detectives don't give up.\"Emma: \"YES! Never!\"gigglingEmma: \"What nutcracker are we using now?\"You: \"Amazon SageMaker.\"Emma: \"Wow.\"You: \"It is a service used to train models like every other tool, but it is super fast, easy and has many 'toys'.\"Emma: (eyes light up) \"Toys?\"You: \"Yeah, You can play with them to build a really nice model. In formal terms, they're called libraries. Ready-made stuff made available to use.\"Emma: \"That makes it easier.\"You: \"What do you think we're going to do to keep teaching our model?\"Emma: \"Maybe adjust its settings?\"You: \"Exactly. We call themparameters. If we're not happy with the results, we can fix these parameters and then teach the model again. This time, we expect the result to be better than the first time we thought.\"Emma: \"Understandable. It's just like grounding Alex for partying on Friday till 2 a.m. and telling him to come home earlier.\"You: \"You're smarter than I am. And yes, that is very true.\"after training the modelEmma: \"Voila!\"You: \"Now, let's see the results.\"Emma: \"How?\"You: Using a set of metrics. They let you know how performant your model is. One very common metric we'll use to evaluate the model is accuracy.\"You both check the score and to find 96%. While you celebrate in victory, Emma frowns.You: \"You're not happy?\"Emma: \"I wanted a 100% score!\"You: (laughs) \"I forgot to explain. A good model is never perfect. Any model with an accuracy score of 100% is not a good one.\"Emma: \"Huh?\"You: \"If our model had an accuracy of 100%, it means it'll say that it detected a flower when it sees a dog.\"Emma: \"How's that?\"You: \"It means it learnt too much from the data. In this case, it cannot differentiate between flowers and non-flowers. Any image you ask it to detect, it tells youflower, orhibiscusordandelionwhen it saw either a kitten or a phone charger or even you. The same rule applies to life: no one is perfect.\"Emma: \"This is really interesting.\"You: \"A score between 80 and 99% is usually the trend for good models.\"She smiles.Model IntegrationEmma: \"How do we link our model to a device?\"You: \"We use SDKs, a set of tools used to integrate models. Based on what we're integrating it into, we need to set up the communication between the model and the environment it'll sit in. If it sits in an environment where it know nobody, it won't communicate. Like the first day you went to kindergarten and refused to play with the other kids. You behaved like that because you didn't know any Olivia or Chloe.\"Emma: (smiles shyly)Model DeploymentEmma: \"Now, I really want to see how much my model has learnt from my instructions. So that I can ground him too if he makes any silly mistakes.\"You: (chuckles) \"Yes. Let's use DeepLens.\"Emma: \"Let me guess. It's another service that uses a device's camera to detect images.\"You: \"Exactly. Except that it is a deep learning video camera.\"Emma: \"What?\"You: \"A deep learning model is more powerful and can do complex instructions. If a video camera is deep learning based, it means it can perform more complex tasks than our flower model.\"Emma: \"Reason why it's calledDeepLens.You: (nods) \"Yes.\"After deploying the model, Emma decides to test it. You both go out into the garden.You: \"Here, take the iPad. Let's analyse the flowers here. Start the video.\"As she roams around, she sees live predictions on the video screen and she says the names of the flower she sees. You both share a high five.Who am I?Hii! I'm Delia Ayoko. I'm an AWS Community Builder in the Machine Learning Category and the first Cloud Club Captain in Cameroon, at the University of Bamenda. I am also a computer engineering student, data scientist, content creator, Wattpad author and mentor. I love building models especially using AWS. If you loved this article, please react to it so that I write another one next week. Thank you!P.S: If you're looking forward to starting a career in machine learning, I have videos where I explain ML concepts on my Instagram. Feel free to check it out and follow me there for more content in your feed! :)"}
{"title": "Say Hello to the Automated Testing Assistant by Party Rock", "published_at": 1712320823, "tags": ["genai", "awscommunity", "partyrock"], "user": "Adeline Makokha", "url": "https://dev.to/aws-builders/say-hello-to-the-automated-testing-assistant-by-party-rock-26m0", "details": "In the ever-changing world of software development, finding effective ways to test and ensure reliability is an ongoing challenge. But fear not \u2013 the Automated Testing Assistant, powered by Party Rock, has arrived, promising to shake up the world of test automation. In this article, I'll dive into what makes the Automated Testing Assistant tick and how it's changing the game by generating test cases, managing data, creating checklists, offering insightful reports, optimizing code, handling exceptions, and even making bug reporting a breeze.Features of Automated Testing Assistant1.Generating Test Cases and DataSay goodbye to the headache of creating test cases manually. The Automated Testing Assistant is a wizard at generating smart, all-encompassing test scenarios. Thanks to Party Rock's magic touch, it ensures your tests cover a wide range of situations, checking the resilience of your software under various conditions. And it doesn't stop there \u2013 it also helps whip up relevant test data to make sure your testing is as thorough as it gets.2.Checklist Creation for Hassle-Free TestingEfficient testing requires a solid plan, and the app is the expert checklist creator you need. Tailoring detailed checklists to your application, it becomes the roadmap for your testing journey, ensuring every nook and cranny of your software gets the attention it deserves.3.Insightful Reports for Smarter DecisionsNo more guesswork after running tests. Party Rock's Automated Testing Assistant goes the extra mile by delivering detailed reports that provide a 360-degree view of your test results. Developers and QA teams can quickly spot and tackle issues, armed with the kind of analytics that make continuous improvement a breeze.4.Optimizing and Refactoring for Cleaner CodeRecognizing the importance of clean code in the testing process, the app lends a helping hand in optimizing and refactoring code snippets. Not only does this boost the efficiency of your test scripts, but it also contributes to the overall maintainability of your software.5.Exception Handling Made SimpleKeeping your software stable is a priority, and the app is there to guide developers in setting up robust exception handling. Say goodbye to unexpected issues causing havoc during testing \u2013 this assistant has your back.6.Seamless Bug ReportingSpot a bug? Reporting it is now a walk in the park. The Automated Testing Assistant streamlines the bug reporting process with a user-friendly interface directly integrated into your testing environment. This means quicker bug resolution, paving the way for faster software delivery.The Future of Software TestingAs we navigate the ever-changing landscape of software development, the Automated Testing Assistant powered by Party Rock emerges as your trusty companion. From generating test cases to optimizing code and everything in between, it's changing the way we approach test automation.REFERENCEAWS NewsBlog:https://aws.amazon.com/blogs/aws/build-ai-apps-with-partyrock-and-amazon-bedrock/ConclusionThe Automated Testing Assistant is proof of Party Rock's transformative impact on testing processes. As technology marches forward, embracing innovative solutions like this promises to take the software development industry to new heights of precision, agility, and reliability. The road to automated excellence has never looked more promising and exciting!PartyRock Website:https://partyrock.aws/Check out the snapshot from the app here:https://partyrock.aws/u/Adele/c315JA0ho/Automated-Testing-Assistant"}
{"title": "AWS Advanced Networking Specialty - 15 hands-on exercises for certification success", "published_at": 1712215535, "tags": ["aws", "certification", "networking"], "user": "Arpad Toth", "url": "https://dev.to/aws-builders/aws-advanced-networking-specialty-15-hands-on-exercises-for-certification-success-4eh7", "details": "Achieving AWS certification requires more than just studying theory. Practical experience is essential for deep understanding and retention of concepts. In this post, I'll share 15 hands-on exercises that helped me pass the AWS Advanced Networking Specialty certification exam.1: Embrace hands-on learningHands-on practice is a cornerstone of effective learning. Beyond merely understanding concepts, actively engaging with AWS services solidifies knowledge and ensures its practical application.When I study for a certification exam, I take free workshops and create short exercises to see some services and settings in action.Ishared 24 exercisesthat helped me pass the AWS Security Specialty exam a while ago. Since the article's reception was good, I'll share15 exercisesfor the AWS Advanced Networking Specialty (ANS) exam in this post.2. Pre-requisitesBefore diving into the exercises, you might want to have some pre-requisites in place.2.1. AWS OrganizationsSet up multiple accounts to simulate real-world scenarios insideAWS Organizations.2.2. Domain nameRegister a domain for DNS-based exercises. Any cheap domain name will do.2.3. Cost ConsiderationsBe mindful of potential costs associated with provisioning resources.3. DisclaimerWhile these exercises cover significant concepts tested in the exam, they are not enough to pass it. You will need a comprehensive study and practice beyond these exercises.4. 15 engaging exercisesThe scenario descriptions intentionally are of high level. Anybody preparing for the exam should know, for example, how to create a VPC peering connection and configure it in the route table. :)The list below doesn't contain anyDirect ConnectorSite-to-Site VPNexercises although these domains contribute heavily to the exam. Unfortunately, I don't have any access to on-premise data centres to connect to my cloud environment.Create two VPCs, VPC A and VPC B, and peer them together. Create an interface endpoint to KMS in VPC B. Create two instances. One should be in an isolated subnet (i.e., no internet access from the subnet) in VPC A and one in an isolated subnet in VPC B. Can we connect to KMS from the VPC B instance? How about from the instance in VPC A? Why?Using the same two VPCs, subnets and instances, can we connect to the internet from the instance in VPC A via VPC B?VPC A has a CIDR block of 10.0.0.0/16. VPC B's CIDR block is 10.0.0.0/20 with a secondary CIDR of 10.1.0.0/16. Can we peer VPC A and VPC B?Create a PrivateLink connection between VPC A and VPC B, where VPC A is the service consumer and VPC B is the service provider. Use the correct load balancer type.Create a Cloud WAN global and core network. Set up two VPCs in us-east-1, eu-central-1 and ap-southeast-2 each. The first VPC in each region is for development, and the second is for the production environment. Configure the set-up to allow traffic between the development VPCs. Production VPCs should also be able to connect to each other, but there should be no traffic allowed between dev and prod VPCs.Create three VPCs. Dedicate one of them to be the egress VPC. Configure the internet access from the other two VPCs via the egress VPC. Repeat the exercise with the egress VPC being in a different region.Set up two VPCs with overlapping CIDR ranges. How many different ways can you connect them without using an internet gateway?Configure a connection between two private instances in two VPCs in different regions using transit gateways. Visualize the architecture in Network Manager.Set up a Client VPN connection. Ping a private instance in a VPC from your laptop. Ping a public website through the VPN connection. You can use theAWS Client VPN for Desktopapplication to manage the connection.Create a networking account and share one of the subnets. Configure an internet connection from a private subnet in a different account through the shared subnet using a transit gateway.Repeat the previous exercise without sharing the subnet. That is, create an egress VPC in the networking account and connect to the internet from a different account.Share a subnet from another account inside an Organization using Resource Access Manager. What are the shared resources? Can you delete these resources from the other account? Can you launch an EC2 instance in the shared subnet from the other account?If you have a domain name registered, create split-view DNS in Route 53.Launch an EC2 instance that has a custom private hostname.Configure DNSSEC in Route 53 for your domain if you have one.5. SummaryBy completing these exercises, you'll take a significant step towards learning key networking concepts required for the AWS Advanced Networking Specialty certification.I hope you'll find them at least as helpful as I did.Enjoy!6. Further reading and learningAWS Certified Advanced Networking - Specialty- Everything official about the examExam Prep Standard Course: AWS Certified Advanced Networking - Specialty (ANS-C01)- Exam readiness course in Skill BuilderExam Prep Official Practice Question Set: AWS Certified Advanced Networking - Specialty (ANS-C01 - English)- A pretty good official practice exam setAWS Workshops- Workshops at all levels and domains (go for networking in the search bar)"}
{"title": "AWS Bedrock, Claude 3, Serverless RAG, Rust", "published_at": 1712176092, "tags": ["serverless", "ai", "rust", "aws"], "user": "szymon-szym", "url": "https://dev.to/aws-builders/aws-bedrock-claude-3-serverless-rag-rust-4i7", "details": "Photo byvackground.comonUnsplashWhat a funny time to live in. It is quite challenging to craft an informative blog post title that won't contain only buzzwords!IntroductionI wanted to start exploring Amazon Bedrock service for quite a while. I firmly believe that offering multiple LLMs as a serverless service is a huge step toward democratizing access to the current \"AI revolution\".A while ago I heard aboutLanceDB- an open-source vector database written in Rust. This is an amazing project with a bunch of cool features, but for me, the selling point was that I could use a local file system or S3 as storage and move computation to Lambda. Additionally, because LanceDB is written in Rust, I could use Rust to work with it.Then I stumbled upon an amazing post from Giuseppe Battista and Kevin Shaffer-Morrison about creating a serverless RAG with LanceDB:Serverless Retrieval Augmented Generation (RAG) on AWS. I treated that post as a starting point for me.ProjectThe code for this blogis in the repositoryThe GenAI ecosystem is a bit overwhelming. What works for me is dividing complex problems into smaller tasks and tackling them one by one.In general text generation with a vector database used for RAG might be broken into the following steps:1 Create a knowledge baseread input documentstransform them into embeddingsstore in the vector database2 Generate text based on the user's requesttransform user's query to embeddingsget related data from the vector databaseconstruct a prompt for LLM using contextinvoke LLM modelIn this post, I'll focus on the second point.Prepare knowledgebaseAs I mentioned above, I won't focus on this part. I just take ready-to-use code from Giuseppe's blog.I create a new folder and initializeaws cdkprojectcdk init--languagetypescriptEnter fullscreen modeExit fullscreen modeAt this point, the only resource I need is the S3 bucket.import*ascdkfrom'aws-cdk-lib';import{Construct}from'constructs';import*ass3from'aws-cdk-lib/aws-s3';exportclassBedrockRustStackextendscdk.Stack{constructor(scope:Construct,id:string,props?:cdk.StackProps){super(scope,id,props);// create s3 bucket for vector dbconstvectorDbBucket=news3.Bucket(this,'lancedb-vector-bucket',{versioned:true,});newcdk.CfnOutput(this,'vector-bucket-name',{value:vectorDbBucket.bucketName,});}}Enter fullscreen modeExit fullscreen modeDocuments processorThe code for processing documents is availablein the repo from Giuseppe's blog. I just want to run it manually from my local machine, so I simplify it a bit.// document-processor/main.tsimport{BedrockEmbeddings}from\"langchain/embeddings/bedrock\";import{CharacterTextSplitter}from\"langchain/text_splitter\";import{PDFLoader}from\"langchain/document_loaders/fs/pdf\";import{LanceDB}from\"langchain/vectorstores/lancedb\";import{connect}from\"vectordb\";// LanceDBimportdotenvfrom\"dotenv\";dotenv.config();(async()=>{constdir=process.env.LANCEDB_BUCKET||\"missing_s3_folder\";constlanceDbTable=process.env.LANCEDB_TABLE||\"missing_table_name\";constawsRegion=process.env.AWS_REGION;console.log(\"lanceDbSrc\",dir);console.log(\"lanceDbTable\",lanceDbTable);console.log(\"awsRegion\",awsRegion);constpath=`documents/poradnik_bezpiecznego_wypoczynku.pdf`;constsplitter=newCharacterTextSplitter({chunkSize:1000,chunkOverlap:200,});constembeddings=newBedrockEmbeddings({region:awsRegion,model:\"amazon.titan-embed-text-v1\",});constloader=newPDFLoader(path,{splitPages:false,});constdocuments=awaitloader.loadAndSplit(splitter);constdb=awaitconnect(dir);console.log(\"connected\")consttable=awaitdb.openTable(lanceDbTable).catch((_)=>{console.log(\"creating new table\",lanceDbTable)returndb.createTable(lanceDbTable,[{vector:Array(1536),text:'sample',},])})constpreparedDocs=documents.map(doc=>({pageContent:doc.pageContent,metadata:{}}))awaitLanceDB.fromDocuments(preparedDocs,embeddings,{table})})();Enter fullscreen modeExit fullscreen modeNow I run from thedocument-processor/(if you want to use an AWS profile different than the default one, it needs to be configured as an environment variable)The easiest way to configure env variables is to put them into.envfile, which is loaded withdotenvlibrary. Expected shape ofLANCEDB_BUCKETis the name of the bucket, andPREFIXis a folder name in S3.npx ts-node main.tsEnter fullscreen modeExit fullscreen modeCross-check in the S3 - all looks goodThe important thing is to align the installedlancedblibrary with the platform we are developing on. In my case this is@lancedb/vectordb-linux-x64-gnu, but it will differ for different machines. Thank youperpilfor catching this!Input dataSometimes it might be tricky to build GenAI solutions for non-English languages. In my case, I plan to generate texts in Polish based on the Polish knowledge base.Luckily Titan Embedings model is multilingual and supports Polish. That's why I can use it out of the box withLangChainintegration.Next time I would like to spend more time on this step, especially for preparing chunks of documentation. For now, splitting everything into fixed-sized pieces would work.Generate text based on the user's queryOK, now I can create the main part.In the root directory, I add alambdasfolder and create a new lambda withcargo lambdacargo lambda new text_generatorEnter fullscreen modeExit fullscreen modeBefore I start I createconfig.rsfile next tomain.rsto keep env variables in one place. I addclapanddotenvto manage themcargo add clap-Fderive,env cargo add dotenvEnter fullscreen modeExit fullscreen mode// config.rs#[derive(clap::Parser,Debug)]pubstructConfig{#[clap(long,env)]pub(crate)bucket_name:String,#[clap(long,env)]pub(crate)prefix:String,#[clap(long,env)]pub(crate)table_name:String,}Enter fullscreen modeExit fullscreen modeNow I can read the configuration at the beginning of the execution.// main.rs#[tokio::main]asyncfnmain()->Result<(),Error>{tracing::init_default_subscriber();info!(\"starting lambda\");dotenv::dotenv().ok();letenv_config=Config::parse();// ...Enter fullscreen modeExit fullscreen modeBefore defining the function handler, I would like to prepare everything I could define outside of the specific request's context. Client for SDK service and LanceDB connection are obvious candidates// main.rs// ...// set up aws sdk configletregion_provider=RegionProviderChain::default_provider().or_else(\"us-east-1\");letconfig=aws_config::defaults(BehaviorVersion::latest()).region(region_provider).load().await;// initialize sdk clientsletbedrock_client=aws_sdk_bedrockruntime::Client::new(&config);info!(\"sdk clients initialized\");// ...Enter fullscreen modeExit fullscreen modeWhen I started working on this blog post, Lance SDK for Rust didn't support connecting directly to s3. I needed to implemented logic to download lance files from s3 to the local directory. It is not needed anymoreI initialize LanceDB with s3 bucket uri// ...letbucket_name=env_config.bucket_name;letprefix=env_config.prefix;lettable_name=env_config.table_name;letstart_time_lance=std::time::Instant::now();lets3_db=format!(\"s3://{}/{}/\",bucket_name,prefix);info!(\"bucket string {}\",s3_db);// set AWS_DEFAULT_REGION envstd::env::set_var(\"AWS_DEFAULT_REGION\",\"us-east-1\");letdb=connect(&s3_db).execute().await?;info!(\"connected to db {:?}\",db.table_names().execute().await);lettable=db.open_table(&table_name).execute().await?;info!(\"connected to db in {}\",Duration::from(start_time_lance.elapsed()).as_secs_f32());Enter fullscreen modeExit fullscreen modeFinally, I initialize a handler with an \"injected\" DB table and bedrock client//...run(service_fn(|event:LambdaEvent<Request>|{function_handler(&table,&bedrock_client,event)})).awaitEnter fullscreen modeExit fullscreen modeFunction handlerLambda function input and output are pretty straightforward#[derive(Deserialize)]structRequest{prompt:String,}#[derive(Serialize)]structResponse{req_id:String,msg:String,}Enter fullscreen modeExit fullscreen modeThe handler signature looks like this:#[instrument(skip_all)]asyncfnfunction_handler(table:&Table,client:&aws_sdk_bedrockruntime::Client,event:LambdaEvent<Request>,)->Result<Response,Error>{//...Enter fullscreen modeExit fullscreen modeTransfor query with Amazon TitanThe first task is to send the received prompt to the Bedrock Titam Embeddings model.According to documentationinput for the model and response from it are pretty simple{\"inputText\":string}Enter fullscreen modeExit fullscreen mode{\"embedding\":[float,float,...],\"inputTextTokenCount\":int}Enter fullscreen modeExit fullscreen modeTo be able to parse the response I create a struct#[derive(Debug,serde::Deserialize)]#[serde(rename_all=\"camelCase\")]structTitanResponse{embedding:Vec<f32>,input_text_token_count:i128,}Enter fullscreen modeExit fullscreen modeAnd I use SDK to invoke the model// ...// transform prompt to embeddingsletembeddings_prompt=format!(r#\"{{         \"inputText\": \"{}\"     }}\"#,prompt);info!(\"invoking embeddings model with: {}\",embeddings_prompt);letinvocation=client.invoke_model().content_type(\"application/json\").model_id(\"amazon.titan-embed-text-v1\").body(Blob::new(embeddings_prompt.as_bytes().to_vec())).send().await.unwrap();lettitan_response=serde_json::from_slice::<TitanResponse>(&invocation.body().clone().into_inner()).unwrap();letembeddings=titan_response.embedding;info!(\"got embeddings for prompt from model\");//...Enter fullscreen modeExit fullscreen modeLookup for related documents in LanceDBOnce we have the query transformed into embeddings, we can utilize vector database magic. I query our knowledge base to find related content.// ...letresult:Vec<RecordBatch>=table.search(&embeddings).limit(1).execute_stream().await.unwrap().try_collect::<Vec<_>>().await.unwrap();letitems=result.iter().map(|batch|{lettext_batch=batch.column(1);lettexts=as_string_array(text_batch);texts}).flatten().collect::<Vec<_>>();info!(\"items {:?}\",&items);letcontext=items.first().unwrap().unwrap_or(\"\").replace(\"\\u{a0}\",\"\").replace(\"\\n\",\" \").replace(\"\\t\",\" \");// ...Enter fullscreen modeExit fullscreen modeLet's unpack what's going on here. LanceDB uses Arrow as a format for data in memory. The search query returns a vector ofBatchRecords, a type related to Arrow.To get content from RecordBatches I map them toitemswith typeVec<Option<&str>>To be honest I don't like the part with using hardcoded column numbers to get the data I want (batch.column(1)), but so far I wasn't able to use more declarative way.As a last step, I sanitize received text - in the other way, it won't work as an input for the LLM model.Invoke Claude 3Finally the most exciting part. I didn't try any advanced prompt-engineering techniques, so my prompt is a simple one//...letprompt_for_llm=format!(r#\"{{         \"system\": \"Respond only in Polish. Informative style. Information focused on health and safety for kids during vacations. Keep it short and use max 500 words. Please use examples from the following document in Polish: {}\",         \"anthropic_version\": \"bedrock-2023-05-31\",         \"max_tokens\": 500,         \"messages\": [             {{                 \"role\": \"user\",                 \"content\": [{{                     \"type\": \"text\",                     \"text\": \"{}\"                 }}]             }}         ]     }}\"#,context,prompt);// ...Enter fullscreen modeExit fullscreen modeCalling a model is the same as for embeddings. I needed to create structs to parse the answer differently, but the flow is the same.//...letgenerate_invocation=client.invoke_model().content_type(\"application/json\").model_id(\"anthropic.claude-3-sonnet-20240229-v1:0\").body(Blob::new(prompt_for_llm.as_bytes().to_vec())).send().await.unwrap();letraw_response=generate_invocation.body().clone().into_inner();letgenerated_response=serde_json::from_slice::<ClaudeResponse>(&raw_response).unwrap();println!(\"{:?}\",generated_response.content);// Prepare the responseletresp=Response{req_id:event.context.request_id,msg:format!(\"Response {:?}.\",generated_response),};// Return `Response` (it will be serialized to JSON automatically by the runtime)Ok(resp)Enter fullscreen modeExit fullscreen modeTestTesting is the fun part. First, let's run lambda locally withcargo lambda. I've prepared json file with prompt inevents/prompt.json{\"prompt\":\"jakie kompetencje powinni mie\u0107 opiekunowie\"}Enter fullscreen modeExit fullscreen modeAnd.envfile in the function's root directoryBUCKET_NAME=xxx PREFIX=lance_db TABLE_NAME=embeddingsEnter fullscreen modeExit fullscreen modeThe prompt is about what skills supervisors need to have. The document I've used as a knowledge base is a brochure prepared by the Polish Ministry of Education with general health and safety rules during holidays.I run ...cargo lambda watch--env-file.envEnter fullscreen modeExit fullscreen mode... and in the second terminalcargo lambda invoke--data-fileevents/prompt.jsonEnter fullscreen modeExit fullscreen modeFor this prompt the context found in LanceDB is relevant.I won't translate the answer, but the point is that it looks reasonable and I can see that injected context was included in the answerThe answer for the same query, just without context, is still pretty good, but generic.I've experimented with different queries, and not all of them returned relevant context from the vector database. Preparing knowledgebase and experimenting with embeddings are things I would like to experiment with.DeploymentI useRustFunctionconstruct for CDK to define lambda//lib/bedrock_rust-stack.ts//....consttextGeneratorLambda=newRustFunction(this,\"text-generator\",{manifestPath:\"lambdas/text_generator/Cargo.toml\",environment:{BUCKET_NAME:vectorDbBucket.bucketName,PREFIX:\"lance_db\",TABLE_NAME:\"embeddings\",},memorySize:512,timeout:cdk.Duration.seconds(30),});vectorDbBucket.grantRead(textGeneratorLambda);// add policy to allow calling bedrocktextGeneratorLambda.addToRolePolicy(newiam.PolicyStatement({actions:[\"bedrock:InvokeModel\"],resources:[\"arn:aws:bedrock:*::foundation-model/amazon.titan-embed-text-v1\",\"arn:aws:bedrock:*::foundation-model/anthropic.claude-3-sonnet-20240229-v1:0\",],effect:iam.Effect.ALLOW,}));Enter fullscreen modeExit fullscreen modeCold start penaltyAt this point, Rust is so famous for its speed, that it would be disappointing to see bad results for the cold start.Initduration in my case was stable around 300-400ms. Pretty neat, since I am not \"only\" initializing SDK clients, but I am also \"connecting\" LanceDB to S3.Overall performanceI didn't run full-blown performance tests, so don't treat my benchmarks too seriously.Getting embedding for user's prompt - 200-300msGetting context from LanceDB - I observed a max ~400ms but it depends on the query. The S3 option is the slowest (and the cheapest) option to handle storage for LanceDB. In my case, this is a fair tradeoff. Other serverless options are listed inthe documentation.The rest is invoking Claude 3 model, which takes around 10 seconds to generate a full answer for my prompt.SummaryLanceDB is an open-source vector database that allows splitting storage from computing. Thanks to that I was able to create a simple RAG pipeline using Lambda and S3.AWS Bedrock offers multiple models as a service. Multilingual AWS Titan lets you create embeddings in various languages, including Polish. Claude 3 Sonnet is a new LLM with outstanding capabilities.LanceDB and AWS Bedrock provide SDKs for Rust, which is very pleasant to work with. Additionally, thanks to Rust, the hit from cold starts is minimal.I am aware that this is only scratching the surface. I plan to spend more time playing with LanceDB and AWS Bedrock. It feels that now sky is the limit."}
{"title": "Multi-Account/Environment DNS Zones", "published_at": 1712173873, "tags": ["route53", "dns"], "user": "Piotr Pabis", "url": "https://dev.to/aws-builders/multi-accountenvironment-dns-zones-24gd", "details": "Originally posted on my blogOne of the best practices for running and developing a service is to separate each environment into its own account. AWS Organizations enables this to be done easily. However, we often see examples where environment names are subdomains of one of our product's domain, such asdev.example.comorstaging.example.com. Let's assume that a team wants to create a new microservice and only make it available in thedevenvironment for now. Normal flow would be to have an IAM role in the account where the zone is hosted or file a ticket to the team responsible. Can we improve this flow? Well, yes, that's whatNSrecords are for!What's more imagine that the team is working on splitting one larger service into smaller services and the design is very dynamic - they delete and create new subdomains often for the new service parts. Normally they need to create both records in Route 53 and change Application Load Balancer rules. Can this be improved or do we need to create an IaC module for these both things? No need to, we can do it easily using*.record names - they will match anything prefixing the subdomain.In today's post we will learn:How to delegate subdomain management to different accounts thus potentially different teams,How to use*in record names and let ALB handle the routing.Preparing multiple environmentsOn my management account (which is the top-level one you have), I activated AWS Organizations. I created two accounts:devandstagingand activated MFA for their root users by resetting the password and assigning an MFA device. Both Organizations and multiple accounts are free of charge.In order to access these accounts further, we can either create a new IAM user or just access them through the role provided by AWS Organizations - the management account can assume this role. I will choose the latter as it is easier than to remember all the credentials.To switch the role, note down each account ID (you can view them in the AWS Organizations tree) and the role name if you specified a different one. I kept the defaultOrganizationAccountAccessRole. Next in the management account in AWS console click on the top right corner and press \"Switch Role\" button. In the new form you see, fill the account ID you plan to switch to and the role name. The two bottom fields are only for your own information. The form below might look different - some AWS regions have a different design.Creating the hosted zone for the domainFor this step you need a domain. It will cost something (depends on the market and TLD). You can buy it with Route 53 but I prefer to use different registrars as some of them have cheaper offers. For this example I will use Gandi and a domain I already own. If you used Route 53 Domains you can skip these steps as the default zone is created for you; go to \"Creating zones in subaccounts\" chapter.If you already have a domain from non-AWS registrar, you need to createonehosted zone for now. this can be done from any account but to keep things clean, I will do it from the management account. Go to Route 53 console and in the left menu click \"Hosted zones\" (if the panel is hidden, use three lines button to expand it). Click \"Create hosted zone\". Name the new zoneexactlyas your domain. Be sure to select public hosted zone.Now we need to get the NS records for this zone and share it with our registrar. This will ensure that TLD server (such as.com) knows where to redirect the queries to.Change NS records at your registrarFirst, open the new zone in Route 53. Expand details at the top and copy the records.Now log in to your registrar's dashboard and open your domain settings. Find where are the nameservers stored. In Gandi it is just a tab named \"Nameservers\". Enter all the values you copied from Route 53 and save the changes. After you do this, it might take even up to 48 hours for the changes to propagate. However, worth noting is that in Gandi case, the zone is free of charge. In Route 53 a public zone costs $0.50 a month.Creating zones in subaccountsNow switch to one of the subaccounts your created previously and navigate to Route 53. Create a new public hosted zone. The name should be in the format ofdevelopment.mydomain.com. This is the subdomain that will be delegated from the parent zonemydomain.com. Each such zone is also charged at $0.50 a month.Note down the name servers in zone details. Copy them somewhere and switch back to the account where the parent zone is hosted. Open the parent zone and create a new record. Before you can select typeNS, you have to type something into the name field. Make it the same as the subdomain you created in the subaccount. Repeat these steps for as many accounts and zones as you want.In order to test if the delegation worked, switch to one of the subaccounts and create a new record. It can be a CNAME, TXT or even Alias with ALB. I will just create a TXT record and test with withdig. Now when your DNS server will query fordevelopment.mydomain.com, it will first ask the parent zone's name servers if there are anyNSrecords for this subdomain. In such case, another query will be done against the subaccount zone's name servers.Next we can query the record from our local machine. To do this we can utilizedigutility. If you don't have it, you can use CloudShell and install it withsudo yum install bind-utils. Replace the example below with your record and type. If you get the answer that means that our setup was successful and the resolver works as expected: it queried the parent zone forstaging.mydomain.comand gotNSreply: \"Continue with another authority to get the record\" and finally found our subaccount zone. From there it asked forcheck.staging.mydomain.comand got the TXT record.$dig +short TXT check.staging.furfel.net\"This is a random TXT check for this zone if it's accessible\"Enter fullscreen modeExit fullscreen modeUsing asterisks in record namesAnother cool feature of Route 53 is the ability to use*in record names. This can help us match entire subdomains using a single record. Let's say that we have a microservice that uses Application Load Balancer that changes target based on HTTP host header. Instead of adding the record for each subdomain, we just set*.accounts.dev.mydomain.comas an Alias to our ALB and let it handle all the rules. However, watch out for the caveats: if you put*in the middle of the record name it will be treated as a literal character. What's more*is not an officially supported character in DNS but Route 53 can handle it. We will now test how it works.Experiment: How the wildcards work?First, let's create some records in the zone. We will test the following records in a new subdomain in the parent zone:*.service.mydomain.com->www.example.comabc*def.other.mydomain.com->abc.example.com*xyz.other.mydomain.com->xyz.example.comIt is enough if we putTXTorCNAMErecords for each of those. What we want to see is which records are matched with wildcards and which are literals.digcan help us again. Try to resolve the records by putting literal*and by trying to place a string inside the*. Replacefurfel.netwith your actual domain.$dig +short CNAME'*.service.furfel.net'www.pabis.eu.$dig +short CNAME'abc.service.furfel.net'www.pabis.eu.$dig +short CNAME'abc1def.other.furfel.net'<no answer>$dig +short CNAME'abc*def.other.furfel.net'abc.pabis.eu.$dig +short CNAME'xyz.other.furfel.net'<no answer>$dig +short CNAME'axyz.other.furfel.net'<no answer>$dig +short CNAME'*xyz.other.furfel.net'xyz.pabis.eu.Enter fullscreen modeExit fullscreen modeIntegrating the ALBWe are now sure that only*.works as a form of wildcard while other records are treated as literals. The former can be used for microservices behind an ALB. Let's create one with some rules that will simply reply with static text so that we can demonstrate routing based on the HTTP Host header and the default response. We need to do it either using CloudFormation, Terraform or CLI because AWS console wizard doesn't allow to select fixed response for the default rule. I will create an ALB, HTTP listener and security group allowing anyone on port 80 using Terraform. I will continue to use Terraform for the rest of this post to keep some consistency. However, you can just create a new ALB using AWS CLI and continue using AWS web console.resource\"aws_security_group\"\"HTTP\"{name=\"HTTP\"description=\"Allow HTTP inbound traffic\"vpc_id=data.aws_vpc.default.idingress{from_port=80to_port=80protocol=\"tcp\"cidr_blocks=[\"0.0.0.0/0\"]}}resource\"aws_alb\"\"ALB\"{name=\"MyALB\"subnets=slice(data.aws_subnets.default.ids,0,2)security_groups=[aws_security_group.HTTP.id]}resource\"aws_alb_listener\"\"http\"{load_balancer_arn=aws_alb.ALB.arnport=80protocol=\"HTTP\"default_action{type=\"fixed-response\"fixed_response{content_type=\"text/html\"message_body=\"<html><body><h1>Fall through!</h1></body></html>\"status_code=\"200\"}}}Enter fullscreen modeExit fullscreen modeIn the example above I use default VPC to deploy the ALB. You can replace the values with your own created VPC or just seethis filefor reference.Now if we associate the wildcard subdomain with the ALB, it will always reply with the default response -Fall through!- seen above. We can create more rules in the ALB to test if it's detecting the host header. See two examples below:resource\"aws_alb_listener_rule\"\"hello\"{listener_arn=aws_alb_listener.http.arnpriority=100condition{host_header{values=[\"hello.asterisk.${var.DomainName}\"]}}action{type=\"fixed-response\"fixed_response{content_type=\"text/html\"message_body=\"<html><body><h1>Hi there!</h1></body></html>\"status_code=\"200\"}}}resource\"aws_alb_listener_rule\"\"goodbye\"{listener_arn=aws_alb_listener.http.arnpriority=101condition{host_header{values=[\"goodbye.asterisk.${var.DomainName}\"]}}action{type=\"fixed-response\"fixed_response{content_type=\"text/html\"message_body=\"<html><body><h1>Goodbye!</h1></body></html>\"status_code=\"200\"}}}Enter fullscreen modeExit fullscreen modeIf we navigate in the browser to both of the links  we will see that bothgoodbyeandhellorules are matched and others just use the default ALB response. This way we can just define one record in Route 53 zone and let the ALB handle all the needed logic. What is more, the changes in rules are immediate compared to DNS TTL. In real world scenario, you would replace above fixed responses with some container or EC2 targets.For HTTPS you can easily request a wildcard certificate from ACM (for one subdomain) when you enter*.subdomain.yourdomain.comin the additional names field. Such certificate will be valid for same level subdomain such ashello.subdomain.yourdomain.combut not forxyz.abc.subdomain.yourdomain.com. It's also impossible to request*.*.subdomain.yourdomain.comcertificate.SummaryWe learned how easy it is to share the domains with other accounts to manage. With this approach we improve productivity and the flow of work. What is more, there's no more need to create special roles for different teams to access their own records.We also saw the wildcard domains that help us manage multiple subdomains or services from one place, namely the ALB. This way we can quickly add new services without going back and editing the Route 53 zone or creating a module for handling both resources."}
{"title": "Poor architecture decisions when migrating to the cloud", "published_at": 1712148576, "tags": ["aws", "cloud", "design", "architecture"], "user": "Eyal Estrin", "url": "https://dev.to/aws-builders/poor-architecture-decisions-when-migrating-to-the-cloud-189a", "details": "When organizations are designing their first workload migrations to the cloud, they tend to mistakenly look at the public cloud as the promised land that will solve all their IT challenges (from scalability, availability, cost, and more).In the way to achieve their goals, organizations tend to make poor architectural decisions.In this blog post, we will review some of the common architectural mistakes made by organizations.Lift and Shift approachMigrating legacy monolithic workload from the on-premises and moving it as is to the public cloud might work (unless you have a license or specific hardware requirements), but it will result in poor outcomes.Although VMs can run perfectly well in the cloud, most of the chances you will have to measure the performance of the VMs over time and right-size the instances to match the actual running workload to match customers\u2019 demand.The lift and shift approach is suitable as an interim solution until the organization has the time and resources to re-architect the workload, and perhaps choose a different architecture (for example migrate from VMs to containers or even Serverless).In the long run, lift and shift will be a costly solution (compared to the on-premises) and will not be able to gain the full capabilities of the cloud (such as horizontal scale, scale to zero, resiliency of managed services, and more).Using Kubernetes for small/simple workloadsWhen designing modern applications, organizations tend to follow industry trends.One of the hottest trends is to choose containers for deploying various application components, and in many cases, organizations choose Kubernetes as the container orchestrator engine.Although Kubernetes does have many benefits, and all hyper-scale cloud providers offer a managed Kubernetes control plane, Kubernetes creates many challenges.The learning curve for fully understanding how to configure and maintain Kubernetes is very long.For small or predictable applications, built from a small number of different containers, there are better and easy-to-deploy and maintain alternatives, such asAmazon ECS,Azure Container Apps, orGoogle Cloud Run\u2013 all of them are fully capable of running production workloads, and are much easier to learn and maintain then Kubernetes.Using cloud storage for backup or DR scenariosWhen organizations began to search for their first use cases for using the public cloud, they immediately thought about using cloud storage as a backup location or perhaps even for DR scenarios.Although both use cases are valid options, they both tend to miss the bigger picture.Even if we use object storage (or managed NFS/CIFS storage services) for the organization\u2019s backup site, we must always take into consideration the restore phase.Large binary backup files that we need to pull from the cloud environment back to on-premises will take a lot of time, not to mention the egress data cost, the read object API calls cost, and more.The same goes with DR scenarios \u2013 if we back up our on-premises VMs or even databases to the cloud, if we don\u2019t have a similar infrastructure environment in the cloud, what will a cold DR site assist us in case of a catastrophic disaster?Separating between the application and the back-end data-store tiersMost applications are built from a front-end/application tier and a back-end persistent storage tier.In a legacy or tightly coupled architecture, there is a requirement for low latency between the application tier and the data store tier, specifically when reading or writing to a backend database.A common mistake is creating a hybrid architecture, where the front-end is in the cloud, pulling data from an on-prem database, or an architecture (rare scenario) where a legacy on-prem application is connecting to a managed database service in the cloud.Unless the target application is not prone to network latency, it is always recommended to architect all components close to each other, decreasing the network latency between the various application components.Going multi-cloud in the hope of resolving vendor lock-in riskA common risk many organizations looking into is vendor lock-in (i.e., customers being locked into the ecosystem of a specific cloud provider).When digging into this risk, vendor lock-in is about the cost of switching between cloud providers.Multi-cloud will not resolve the risk, but it will create many more challenges, from skills gap (teams familiar with different cloud providers ecosystems), central identity and access management, incident response over multiple cloud environments, egress traffic cost, and more.Instead of designing complex architectures to mitigate theoretical or potential risk, design solutions to meet the business needs, familiarize yourself with a single public cloud provider\u2019s ecosystem, and over time, once your teams have enough knowledge about more than a single cloud provider, expand your architecture \u2013 don\u2019t run to multi-cloud from day 1.Choosing the cheapest region in the cloudAs a rule of thumb, unless you have a specific data residency requirement, choose a region close to your customers, to lower the network latency.Cost is an important factor, but you should design an architecture where your application and data reside close to customers.If your application serves customers all around the globe, or in multiple locations, consider adding a CDN layer to keep all static content closer to your customers, combined with multi-region solutions (such as cross-region replication, global databases, global load-balancers, etc.)Failing to re-assess the existing architectureIn the traditional data center, we used to design an architecture for the application and keep it static for the entire lifecycle of the application.When designing modern applications in the cloud, we should embrace a dynamic mindset, meaning keep re-assessing the architecture, look at past decisions, and see if new technologies or new services can provide more suitable solutions for running the application.The dynamic nature of the cloud, combined with evolving technologies, provides us with the ability to make changes and better ways to run applications faster, more resilient, and in a cost-effective manner.Bias architecture decisionsThis is a pitfall that many architects fall into \u2013 coming with a background in a specific cloud provider, and designing architectures around this cloud provider\u2019s ecosystem, embedding bias decisions and service limitations into architecture design.Instead, architects should fully understand the business needs, the entire spectrum of cloud solutions, service costs, and limitations, and only then begin to choose the most appropriate services, to take part in the application\u2019s architecture.Failure to add cost to architectural decisionsCost is a huge factor when consuming cloud services, among the main reasons is the ability to consume services on demand (and stop paying for unused services).Each decision you are making (from selecting the right compute nodes, storage tier, database tier, and more), has its cost impact.Once we understand each service pricing model, and the specific workload potential growth, we can estimate the potential cost.As we previously mentioned, the dynamic nature of the cloud might cause different costs each month, and as a result, we need to keep evaluating the service cost regularly, replace services from time to time, and adjust it to suit the specific workload.SummaryThe public cloud has many challenges in picking the right services and architectures to meet specific workload requirements and use cases.Although there is no right or wrong answer when designing architecture, in this blog post, we have reviewed many \u201cpoor\u201d architectural decisions that can be avoided by looking at the bigger picture and designing for the long term, instead of looking at short-term solutions.Recommendation for the post readers \u2013 keep expanding your knowledge in cloud and architecture-related technologies, and keep questioning your current architectures, to see over time, if there are more suitable alternatives for your past decisions.About the AuthorEyal Estrin is a cloud and information security architect, and the author of the booksCloud Security HandbookandSecurity for Cloud Native Applications, with more than 20 years in the IT industry. You can connect with him on social media (https://linktr.ee/eyalestrin).Opinions are his own and not the views of his employer."}
{"title": "How to deploy Remix apps with SSR to AWS Amplify", "published_at": 1712070021, "tags": ["aws", "awsamplify", "amplify", "remix"], "user": "Rich", "url": "https://dev.to/aws-builders/how-to-deploy-remix-apps-with-ssr-to-aws-amplify-60m", "details": "The AWS Amplify team recently announced the Amplify Hosting deployment specification. This is a way to deploy applications to AWS Amplify with server side rendering (SSR) support. While there are guides or support for many frameworks including Astro, NextJS and Nuxt I couldn't find one for Remix.As I'm about to start working on a Shopify app (Shopify provides templates for Remix) and I didn't want to switch infrastructure providers I decided to investigate using how difficult it would be to use Amplify to host a Remix app with SSR.The first step is to create a build file namedamplify.ymlwhich Amplify uses to build the project. I was deploying a new project and Amplify detected the file automatically during the initial deployment. Our build script handles four important tasks:Runsnpm run buildto build the application. Remix puts the output in into thebuildfolder.Moves and restuctures thebuildfolder output to meet the Amplify hosting specification byReduce the size of ournode-modulesfolder by runningnpm ci --omit devto create anode-modulesthat only includes production dependencies. This is important for larger project as there is a limit on the maximum size of this folder. It also helps reduce cold start times. The folder is moved into thecompute/defaultfolder so the modules are available at runtime.Finally there are two files that we will create shortly that need to be copied into place.The completeamplify.ymlis below:version: 1baseDirectory: .amplify-hostingfrontend: phases: preBuild: commands: - npm ci build: commands: - npm run build - mv build .amplify-hosting - mv .amplify-hosting/client .amplify-hosting/static - mkdir -p .amplify-hosting/compute - mv .amplify-hosting/server .amplify-hosting/compute/default - npm ci --omit dev - cp package.json .amplify-hosting/compute/default - cp -r node_modules .amplify-hosting/compute/default - cp server.js .amplify-hosting/compute/default - cp deploy-manifest.json .amplify-hosting/deploy-manifest.json artifacts: files: - \"**/*\" baseDirectory: .amplify-hostingEnter fullscreen modeExit fullscreen modeAWS Amplify will now package and deploy our.amplify-hostingfolder.Thedeploy-mainfest.jsonhas two main tasks:Tell Amplify how to route trafficTell Amplify how to configure and start the compute resourcesThis routing should not be confused with rewriting and redirecting traffic. It's purpose is to indicate if traffic should be handled as static or compute. Here I'm using the less than perfect approach that files with a.should be treated as static first and fall back to compute if they don't exist. Everything else should be treated as compute. This means that static files must have a.in the filename.For the compute configuration I've set the runtime to Node 20 and the project should be started by usingnode server.jsThe fulldeploly-mainfest.jsonis:{ \"version\": 1, \"framework\": { \"name\": \"remix\", \"version\": \"2.8.1\" }, \"routes\": [{ \"path\": \"/*.*\", \"target\": { \"kind\": \"Static\", \"cacheControl\": \"public, max-age=2\" }, \"fallback\": { \"kind\": \"Compute\", \"src\": \"default\" } }, { \"path\": \"/*\", \"target\": { \"kind\": \"Compute\", \"src\": \"default\" } }], \"computeResources\": [{ \"name\": \"default\", \"runtime\": \"nodejs20.x\", \"entrypoint\": \"server.js\" }]}Enter fullscreen modeExit fullscreen modeFinally I need to create a small Javascript file calledserver.js. This file launches an Express server on port 3000 which listens for requests and passes them to Remix. Remember to add express as a production dependency so this will work.npm i express --saveEnter fullscreen modeExit fullscreen modeThe complete server.js is:import remix from \"@remix-run/express\";import express from \"express\";import * as build from \"./index.js\";const app = express();const port = 3000;app.all( \"*\", remix.createRequestHandler({ build, }));app.listen(port, () => { console.log(`Example app listening on port ${port}`)})Enter fullscreen modeExit fullscreen modeThat's it! You should be able to deploy the project to AWS Amplify and when you go to the URL provided your request will be execute on the server."}
{"title": "Run Commands On An EC2 Instance With AWS Systems Manager", "published_at": 1712067545, "tags": [], "user": "Ashish Gajjar", "url": "https://dev.to/aws-builders/r-run-commands-on-an-ec2-instance-with-aws-systems-manager-1jhk", "details": "AWS Systems Manager provides configuration management, which helps you maintain consistent configuration of your Amazon EC2 or on-premises instances.If you are a System administrator and assigned a task to upgrade the packages for one application running on an EC2 instance, but due to some security restrictions, you are not permitted to access production instances via SSH or bastion host. In this situation, you can use AWS Systems Manager to remotely run shell scripts or certain commands to update packages on EC2 instances.In this blog, we will cover everything you need to know about AWS Systems Manager and how to use it!What is AWS System Manager and how does it work?Benefits of System ManagerWho can use AWS Systems Manager?Hands-on \u2013 Run commands remotely on an EC2 Instance using AWS Systems ManagerConclusionHow does it work?AWS Systems Manager provides its users visibility and control of their infrastructure on AWS. It has a unified user interface so one can view operational data from multiple AWS services and lets the user automate operational tasks across AWS resources.What are its benefits?Quick problem detectionHybrid Environment ManagementEasy AutomationSecurity and Compliance MaintenanceImprove Visibility and ControlWho can use AWS Systems Manager?The key feature of System Manager is to make multiple roles can be performed easily. Hence, this service can be used by:System administratorsSoftware developersSecurity architectsCloud architectsIT professionals who would like to manage AWS resources.Hands-OnIn this a scenario wherein you are assigned tasks by your team to upgrade the packages for your application running on your EC2 instances. Due to some security restrictions, you are not permitted to directly access your production instances via SSH and are not even allowed to use the bastion hosts. In this situation let\u2019s use Amazon Systems Manager to remotely run your shell scripts or certain commands to update packages on your EC2 instances.Step 1: Create an Identity and Access Management (IAM) role.Step 2: Create an EC2 instance.Step 3: Update the Systems Manager Agent.Step 4: Upgradation process via the Fleet Manager dashboard.Step 5: Run a Remote Shell Script. Login to your AWS account on the AWS console and navigate to the IAM console to get started. Click on \u201cRoles\u201d under the \u201cAccess management\u201d section on the left navigation pane.Click on \u201cCreate role\u201d to create a new role. You will use this role to give Amazon Systems Manager permission to perform actions on your instances.Search for the \u201cAmazonEC2RoleForSSM\u201d policy and click on the checkbox to add the policy to the role.Once done, click on \u201cNext: Review\u201d and enter a name for the newly created role and descriptionOn creation of the role, you can type in the role name in the search bar on the Roles dashboard to verify if the role has been created successfully. Choose and Amazon Machine Image (AMI), select \u201cAmazon Linux 2 AMI (64-bit)\u201d and click on \u201cSelect\u201dWe will create an EC2 instance using the role that we created above. This will help us create a managed EC2 instance that will be managed by the Amazon Systems Manager. Navigate to the Amazon EC2 console and ensure that the preferred region is selected in which you want to create your instance.EC2 Dashboard console and click on \u201cLaunch instance\u201d to launch a new managed instance in your preferred region.Choose and Amazon Machine Image (AMI), select \u201cAmazon Linux 2 AMI (64-bit)\u201d and click on \u201cSelect\u201dSelect Instance Type and Key Pair.Next, you need to ensure that your have select a subnet has the \u201cEnable auto-assign public IPv4 address\u201d enabled. This is to be ensured since you will have to connect to your EC2 instance. Without the public IPv4 address, you will not be allowed to connect to your instance. Note: Make sure the SSM agent is installed on your EC2 instance.Scroll down and for the \u201cIAM role\u201d, select the role you createdOnce done, click on \u201cLaunch Instances\u201d.You will see the newly created instance in the list shown below.Once the \u201cInstance State\u201d changes to the \u201cRunning\u201d state, select the newly created instance and click on \u201cActions\u201d. Select \u201cConnect\u201d from the dropdown.click on \u201cConnect\u201d to connect to your EC2 instance.A new terminal console will open in a new tab as shown below.Check the status of SSM Agent \"sudo systemctl status amazon-ssm-agent\"Once you have an EC2 instance running the Systems Manager agent,Navigate to the Amazon Systems Manager console on AWS.Click on \u201cFleet Manager\u201d under the \u201cNode Management\u201d section in the left navigation pane.To automate the upgradation, click on \u201cAccount Management\u201d and then, click on \u201cAuto-update SSM agent\u201d.Click on \u201cAuto-update SSM agent\u201d and after a few minutes, the update will be automated for any existing or new instances you create.Click on \u201cRun Command\u201d under the \u201cNode Management\u201d section in the left navigation pane.Now, click on \u201cRun command\u201d to upgrade the SSM-agent manually.Now, click on the radio button on the left of \u201cAWS-UpdateSSMAgent\u201d. This is known as the document and this will upgrade the Systems Management agent on the selected instance.Once done, scroll down to the \u201cTargets\u201d section on the same page and select the radio button on the left of \u201cChoose instances manually\u201dScroll down and click on \u201cRun\u201d to execute the document.You will see the \u201cOverall Status\u201d as \u201cIn Progress\u201d.After a few minutes, hit refresh and the status will change to \u201cSuccess\u201d.After a few minutes, hit refresh and the status will be updated to \u201cSuccess\u201d on successful completion of execution of the command.Now, to run a remote shell script for upgrading any packages on your EC2 instance, navigate back to the \u201cRun Command\u201d dashboard in Amazon Systems Manager and click on \u201cRun Command\u201d.select the radio button on the left of \u201cChoose instances manually\u201d enter command parameters#!/bin/bash yum update -y  yum install httpd -y systemctl enable --now httpd.serviceEnter fullscreen modeExit fullscreen modeAfter a few minutes, hit refresh and the status will be updated to \u201cSuccess\u201d on successful completion of execution of the command.Verify httpd package installed or not.Conclusion:In this blog, we have explored that AWS Systems Manager has the ability to automate tasks and helps in keeping all our EC2 instances healthy, and applications managed, secure, and updated."}
{"title": "Run Commands On An EC2 Instance With AWS Systems Manager", "published_at": 1712067484, "tags": [], "user": "Ashish Gajjar", "url": "https://dev.to/aws-builders/r-run-commands-on-an-ec2-instance-with-aws-systems-manager-2ngh", "details": "AWS Systems Manager provides configuration management, which helps you maintain consistent configuration of your Amazon EC2 or on-premises instances.If you are a System administrator and assigned a task to upgrade the packages for one application running on an EC2 instance, but due to some security restrictions, you are not permitted to access production instances via SSH or bastion host. In this situation, you can use AWS Systems Manager to remotely run shell scripts or certain commands to update packages on EC2 instances.In this blog, we will cover everything you need to know about AWS Systems Manager and how to use it!What is AWS System Manager and how does it work?Benefits of System ManagerWho can use AWS Systems Manager?Hands-on \u2013 Run commands remotely on an EC2 Instance using AWS Systems ManagerConclusionHow does it work?AWS Systems Manager provides its users visibility and control of their infrastructure on AWS. It has a unified user interface so one can view operational data from multiple AWS services and lets the user automate operational tasks across AWS resources.What are its benefits?Quick problem detectionHybrid Environment ManagementEasy AutomationSecurity and Compliance MaintenanceImprove Visibility and ControlWho can use AWS Systems Manager?The key feature of System Manager is to make multiple roles can be performed easily. Hence, this service can be used by:System administratorsSoftware developersSecurity architectsCloud architectsIT professionals who would like to manage AWS resources.Hands-OnIn this a scenario wherein you are assigned tasks by your team to upgrade the packages for your application running on your EC2 instances. Due to some security restrictions, you are not permitted to directly access your production instances via SSH and are not even allowed to use the bastion hosts. In this situation let\u2019s use Amazon Systems Manager to remotely run your shell scripts or certain commands to update packages on your EC2 instances.Step 1: Create an Identity and Access Management (IAM) role.Step 2: Create an EC2 instance.Step 3: Update the Systems Manager Agent.Step 4: Upgradation process via the Fleet Manager dashboard.Step 5: Run a Remote Shell Script. Login to your AWS account on the AWS console and navigate to the IAM console to get started. Click on \u201cRoles\u201d under the \u201cAccess management\u201d section on the left navigation pane.Click on \u201cCreate role\u201d to create a new role. You will use this role to give Amazon Systems Manager permission to perform actions on your instances.Search for the \u201cAmazonEC2RoleForSSM\u201d policy and click on the checkbox to add the policy to the role.Once done, click on \u201cNext: Review\u201d and enter a name for the newly created role and descriptionOn creation of the role, you can type in the role name in the search bar on the Roles dashboard to verify if the role has been created successfully. Choose and Amazon Machine Image (AMI), select \u201cAmazon Linux 2 AMI (64-bit)\u201d and click on \u201cSelect\u201dWe will create an EC2 instance using the role that we created above. This will help us create a managed EC2 instance that will be managed by the Amazon Systems Manager. Navigate to the Amazon EC2 console and ensure that the preferred region is selected in which you want to create your instance.EC2 Dashboard console and click on \u201cLaunch instance\u201d to launch a new managed instance in your preferred region.Choose and Amazon Machine Image (AMI), select \u201cAmazon Linux 2 AMI (64-bit)\u201d and click on \u201cSelect\u201dSelect Instance Type and Key Pair.Next, you need to ensure that your have select a subnet has the \u201cEnable auto-assign public IPv4 address\u201d enabled. This is to be ensured since you will have to connect to your EC2 instance. Without the public IPv4 address, you will not be allowed to connect to your instance. Note: Make sure the SSM agent is installed on your EC2 instance.Scroll down and for the \u201cIAM role\u201d, select the role you createdOnce done, click on \u201cLaunch Instances\u201d.You will see the newly created instance in the list shown below.Once the \u201cInstance State\u201d changes to the \u201cRunning\u201d state, select the newly created instance and click on \u201cActions\u201d. Select \u201cConnect\u201d from the dropdown.click on \u201cConnect\u201d to connect to your EC2 instance.A new terminal console will open in a new tab as shown below.Check the status of SSM Agent \"sudo systemctl status amazon-ssm-agent\"Once you have an EC2 instance running the Systems Manager agent,Navigate to the Amazon Systems Manager console on AWS.Click on \u201cFleet Manager\u201d under the \u201cNode Management\u201d section in the left navigation pane.To automate the upgradation, click on \u201cAccount Management\u201d and then, click on \u201cAuto-update SSM agent\u201d.Click on \u201cAuto-update SSM agent\u201d and after a few minutes, the update will be automated for any existing or new instances you create.Click on \u201cRun Command\u201d under the \u201cNode Management\u201d section in the left navigation pane.Now, click on \u201cRun command\u201d to upgrade the SSM-agent manually.Now, click on the radio button on the left of \u201cAWS-UpdateSSMAgent\u201d. This is known as the document and this will upgrade the Systems Management agent on the selected instance.Once done, scroll down to the \u201cTargets\u201d section on the same page and select the radio button on the left of \u201cChoose instances manually\u201dScroll down and click on \u201cRun\u201d to execute the document.You will see the \u201cOverall Status\u201d as \u201cIn Progress\u201d.After a few minutes, hit refresh and the status will change to \u201cSuccess\u201d.After a few minutes, hit refresh and the status will be updated to \u201cSuccess\u201d on successful completion of execution of the command.Now, to run a remote shell script for upgrading any packages on your EC2 instance, navigate back to the \u201cRun Command\u201d dashboard in Amazon Systems Manager and click on \u201cRun Command\u201d.select the radio button on the left of \u201cChoose instances manually\u201d enter command parameters#!/bin/bash yum update -y  yum install httpd -y systemctl enable --now httpd.serviceEnter fullscreen modeExit fullscreen modeAfter a few minutes, hit refresh and the status will be updated to \u201cSuccess\u201d on successful completion of execution of the command.Verify httpd package installed or not.Conclusion:In this blog, we have explored that AWS Systems Manager has the ability to automate tasks and helps in keeping all our EC2 instances healthy, and applications managed, secure, and updated."}
{"title": "Build a Serverless S3 Explorer with Dash", "published_at": 1712044301, "tags": ["aws", "s3", "serverless", "cloud"], "user": "Maurice Borgmeier", "url": "https://dev.to/aws-builders/build-a-serverless-s3-explorer-with-dash-9f7", "details": "S3 is a great way to store your data in AWS, and it has many integrations with other services. That's great - as long as you have access to the AWS account. At some point in your journey, especially when building data-driven applications, your business users will want to access data in the bucket, usually reports. Giving all business users direct access to the AWS account and having them navigate the AWS console is generally not feasible or advisable. Today, we'll build a frontend that business users can use to explore and interact with data in S3 using a separate authentication mechanism.I should mention that this post is sort of the alpha version of the app; in a later post, we'll add a more advanced authentication option - Cognito - and add some more quality of live improvements. This builds on the Serverless Dash architecture I introduced in two previous posts. I suggest you check them out if you're interested in more details:Deploying a Serverless Dash App with AWS SAM and LambdaAdding Basic Authentication to the Serverless Dash AppAs usual the code for all of this isavailable on Github- check it out.Before we focus too much on the frontend, there's one thing we should take care of in the SAM app to improve our developer experience. If you've tried to runsam buildin one of the other two code bases, you will have noticed that the process is quite slow. That's because SAM copies every single file from the dependencies into another directory and later compresses it. For Python, there is no way to avoid that when using ZIP-based deployments. Even if you put your dependencies in a separate layer, the copying operation happens every time. SAM offers a more intelligent option for some JS-based Lambdas, but then you'd have to write Javascript. To be fair, this is usually not that big of a deal; it's just that Dash comes with lots of tiny files, and copying them takes a while - even with a fast machine.Fortunately, image-based Lambdas exist, and Docker has a more elegant caching mechanism that is able to only do work if stuff changes. Let's convert our frontend function to an image-based Lambda. This is what thetemplate.yamllooks like after our changes:# template.yamlResources:FrontendFunction:Type:AWS::Serverless::FunctionProperties:PackageType:ImageEvents:# ...omitted events for previty# ...omitted policies for brevityMetadata:DockerTag:frontend-functionDockerContext:./frontendDockerfile:Dockerfile# ...Enter fullscreen modeExit fullscreen modeBy default, SAM uses thePackageTypeZip, and we need to change that toImage. Additionally, we have to add a Metadata section to tellSAM how to buildour image. TheDockerTaghelps avoid ambiguity in a shared Elastic Container Repository, and theDockerContextbasically points to the path where our code is stored, it's more or less equivalent to the previousCodeUriin the function properties. TheDockerfilekey is optional; here, we tell it explicitly that the build instructions are stored in a file calledDockerfile. This is what that looks like:# frontend/DockerfileFROM--platform=linux/amd64 public.ecr.aws/lambda/python:3.12COPYrequirements.txt ${LAMBDA_TASK_ROOT}RUNpip3install-rrequirements.txt--target\"${LAMBDA_TASK_ROOT}\"--no-cache-dirCOPY. ${LAMBDA_TASK_ROOT}CMD[\"app.lambda_handler\"]Enter fullscreen modeExit fullscreen modeIt's based on the official Python 3.12 Lambda base image, copies therequirements.txtinto the container, then installs the dependencies, and only afterward adds the rest of the Python files. Last, it tells Lambda where the entry point for our function is. This setup has the benefit that the dependencies will only be reinstalled if the content of therequirements.txtchanges at some point. The initialsam buildwon't be very fast, but subsequent builds should finish in a matter of seconds (unless we change therequirements.txt) because it can build on top of a cached layer.I also added a.dockerignore, which you can think of as a.gitignorefor the docker build process. It skips Mac-specific files and the Python cache.# frontend/.dockerignore __pycache__ .DS_StoreEnter fullscreen modeExit fullscreen modeWith all this preamble out of the way, we can finally focus on the app. To make it easier to build a not-awful-looking website, I installed thedash-bootstrap-componentswhich give us access to a variety of components from thebootstrapfrontend framework. This will make styling and building the app easier.# frontend/requirements.txt boto3 dash==2.15.0 dash-bootstrap-components==1.5.0 apig-wsgi==2.18.0Enter fullscreen modeExit fullscreen modeAs preparation for the future, I decided to build amulti-page Dash app. That has become easier in more recent versions, and I suggest you start that way if you plan to build anything beyond the most basic apps. To do that, we first update ourbuild_appfunction.# frontend/dash_app.pydefbuild_app(dash_kwargs:dict=None)->Dash:dash_kwargs=dash_kwargsor{}app=Dash(name=__name__,use_pages=True,external_stylesheets=[dbc.themes.BOOTSTRAP,dbc.icons.BOOTSTRAP],**dash_kwargs,)app.layout=html.Div(children=[render_nav(),dash.page_container,],)returnappEnter fullscreen modeExit fullscreen modeI've added the CSS for Bootstrap and theuse_pagesparameter to the constructor. In the basic layout, you can see that the top-level div now has adash.page_container, which will display the currently active page. Above that, I'm callingrender_nav, which renders our navbar.# frontend/dash_app.pydefrender_nav()->dbc.Navbar:page_links=[dbc.NavItem(children=[dbc.NavLink(page[\"name\"],href=page[\"relative_path\"])])forpageindash.page_registry.values()]nav=dbc.Navbar(dbc.Container(children=[*page_links,dbc.NavItem(dbc.NavLink(\"tecRacer\",href=\"https://www.tecracer.com/\",external_link=True,target=\"_blank\",)),]),class_name=\"mb-3\",dark=True,)returnnavEnter fullscreen modeExit fullscreen modeThe navigation is built on top of bootstrap components and will dynamically add all pages that it finds in the page registry. Let's talk about that next. The idea behind a multi-page app is that you add your Python modules for the individual pages in thepages/directory (frontend/pages/in our case) and have them register themselves as a page.# frontend/pages/s3_explorer.pyimportdashimportdash_bootstrap_componentsasdbcfromdashimporthtml,Input,Output,State,callback,MATCHdash.register_page(__name__,path=\"/\",name=\"S3 Explorer (alpha)\")deflayout():\"\"\"Called by Dash to render the base layout.\"\"\"returndbc.Container(children=[dbc.Form(children=[dbc.Label(\"Select a bucket to explore\",html_for=\"select-s3-bucket\",className=\"text-muted\",),dbc.Select(id=\"select-s3-bucket\",options=[],placeholder=\"Select a bucket\"),]),html.H4(\"Bucket Content\",className=\"mt-2\"),html.Div(id=\"bucket-contents\"),],)Enter fullscreen modeExit fullscreen modeThe result of thelayoutfunction will be placed beneath the navigation; in our case, it's just a form with a dropdown to select the S3 Bucket and a few placeholders for the content. When the page is rendered, we need to populate theselectbox with all available S3 buckets. For this kind of interaction, we create a callback function.# frontend/pages/s3_explorer.py@callback(Output(\"select-s3-bucket\",\"options\"),Input(\"select-s3-bucket\",\"id\"),)defpopulate_s3_bucket_selector(_select_children):s3_client=boto3.client(\"s3\")bucket_list=s3_client.list_buckets()[\"Buckets\"]options=[{\"label\":item[\"Name\"],\"value\":item[\"Name\"]}foriteminbucket_list]returnoptionsEnter fullscreen modeExit fullscreen modeWe tell our callback that the result of the function call should be put into the options attribute of our select box, and we also specify an input so our callback gets executed. The id of the select box is not going to change, but our callback will fire once on page load anyway. The function itself uses the regular S3 API to list all buckets, formats the result a bit so Dash can handle it, and returns it. At this point I should probably note that the permission you need is calleds3:ListAllMyBucketsas opposed to the name of the API call. This is what it looks like so far.Whenever a user changes the selected button, we want to render the content of that button, so we need another callback that manages that. Here, you can see the handler for this event; it's triggered whenever the value of the select changes and displays the output as part of ourbucket-contentsdiv from the layout function above. If the bucket selection is empty, we render a note letting the user know why they aren't seeing anything. Otherwise, we return the result ofrender_directory_listingfor the root of the S3 bucket.# frontend/pages/s3_explorer.py@callback(Output(\"bucket-contents\",\"children\"),Input(\"select-s3-bucket\",\"value\"),)defhandle_bucket_selection(bucket_name):\"\"\"Executed when a bucket is selected in the top-level dropdown.\"\"\"ifbucket_nameisNone:return[dbc.Alert(\"No bucket selected.\",color=\"light\")]s3_path=f\"s3://{bucket_name}/\"returnrender_directory_listing(s3_path)Enter fullscreen modeExit fullscreen modeTherender_directory_listingfunction does most of the heavy lifting in our app, if we can call it heavy lifting. Like the AWS console, it uses theListObjectsV2API to get a list of objects and common prefixes on a given S3 path.# frontend/pages/s3_explorer.pydefrender_directory_listing(s3_path:str):# Note: strictly speaking we'd have to check the content type, but this is good enoughis_directory=s3_path.endswith(\"/\")ifnotis_directory:returnrender_s3_object_details(s3_path)bucket_name,key_prefix=_s3_path_to_bucket_and_key(s3_path)s3_client=boto3.client(\"s3\")list_response=s3_client.list_objects_v2(Bucket=bucket_name,Delimiter=\"/\",Prefix=key_prefix,)common_prefixes=[obj[\"Prefix\"]forobjinlist_response.get(\"CommonPrefixes\",[])]items=[obj[\"Key\"]forobjinlist_response.get(\"Contents\",[])]all_items=common_prefixes+itemslist_items=[render_directory_list_item(f\"s3://{bucket_name}/{item}\")foriteminall_items]ifnotlist_items:# Nothing to showreturndbc.Alert(\"No objects found here...\",color=\"light\")returndbc.ListGroup(list_items,class_name=\"mt-1\")Enter fullscreen modeExit fullscreen modeThis function is written so it can handle arbitrary S3 paths (e.g.,s3://bucketname/object/key). It first checks if the pathends in a slash. If that's not the case, it assumes the path refers to an object and renders it as such. Otherwise, as mentioned above, it uses the list objects API to get a list of all objects and common prefixes at this level, which is then rendered to create the next level in the listing hierarchy.Since we can't expect all data to be stored at the top of our virtual directory tree, we need a way to navigate into folders. The basis for this is hidden in therender_directory_list_itemmentioned in the code snipped above.# frontend/pages/s3_explorer.pydefrender_directory_list_item(s3_path:str)->dbc.ListGroupItem:_,object_key=_s3_path_to_bucket_and_key(s3_path)label=(object_key.removesuffix(\"/\").split(\"/\")[-1]+\"/\"ifobject_key.endswith(\"/\")elseobject_key.split(\"/\")[-1])output=dbc.ListGroupItem(children=[html.Span(id={\"type\":\"s3-item\",\"index\":s3_path,},style={\"cursor\":\"pointer\",\"width\":\"100%\",\"display\":\"block\"},children=[render_icon(s3_path),label],n_clicks=0,),html.Div(id={\"type\":\"s3-item-content\",\"index\":s3_path,},),],)returnoutputEnter fullscreen modeExit fullscreen modeThis function renders aListGroupItemfor the given S3 Path. It doesn't matter if it's a directory or object. The important thing is the id argument to thehtml.Spanclass. You can see a dictionary with two keys - the types3-itemand the current S3 path as the index attribute. You can find a similar id for thehtml.Divbelow, just with the types3-item-content. In combination, we can use these two to have our app render directory trees of (almost) arbitrary depth using apattern-matching callback.# frontend/pages/s3_explorer.py@callback(Output({\"type\":\"s3-item-content\",\"index\":MATCH},\"children\"),Input({\"type\":\"s3-item\",\"index\":MATCH},\"n_clicks\"),State({\"type\":\"s3-item\",\"index\":MATCH},\"id\"),prevent_initial_call=True,)defhandle_click_on_directory_item(n_clicks,current_level):\"\"\"Executed when someone clicks on a directory item - folder or object.\"\"\"is_open=n_clicks%2==1ifnotis_open:return[]s3_path:str=current_level[\"index\"]returnrender_directory_listing(s3_path)Enter fullscreen modeExit fullscreen modeThis callback is triggered when then_clicksattribute on Items of types3-itemchanges, i.e., when we click on ourListGroupItem/Spanand reports the total number of clicks on that item so far. Additionally, we get theidattribute of the item that caused the callback to fire and write to theDivof types3-item-contentwith the matching id.In the function we can now determine if we need to display or hide the content of the directory based on the number of clicks it received. Odd click numbers open the directory and even click numbers close it. Then, we extract the S3 path that we're at and call our trusty directory rendering function. Here's what the result looks like.Since just displaying the directory tree wouldn't be very useful, I also added some functionality to allow downloading objects via pre-signed URLs and even inline editing for some smaller text files.Unfortunately, this post has already gotten quite long, and I need to talk a bit about security and limitations, so a detailed explanation of how that works will have to wait for another time - it relies on another pattern-matching callback implementation and some S3 API calls.Deployment, Limitations & SecurityYou can deploy the solution to your own account, as explained in theGithub repository. It's basically a combination ofsam buildandsam deploy --guidedbecause we need SAM to create an ECR repository for the docker image. After the deployment, you need to add credentials to the Parameter Store parameter; otherwise, you won't be able to access the webapp. I didn't include default credentials on purpose.This solution is currently in alpha status, and it has several limitations; among others, it doesn't do pagination on the list objects v2 API call, which means you'll get at most 1000 objects per level. Arguably, anything more than that isn't suitable for a UI anyway, and even 1000 is stretching it. You can choose which filetypes the inline editor is available for using the constants / global variables infrontend/pages/s3_explorer.py. Be aware that this approach doesn't keep the original content type at the moment. Error/exception handling is also not very pretty at the moment - most missing permissions just result in CloudWatch logs being written and nothing happening on the frontend.Whenever we expose information from our AWS accounts to other tools, security is a major concern. Here, all connections are TLS encrypted, and only authenticated requests are passed to the backend. You should be aware that the function has fairly broad S3 read/write permissions at the moment. I strongly recommend you limit those to the buckets that you actually need to be explorable from a webapp. I chose to not include any KMS permissions for the Lambda. If your S3 buckets are encrypted with KMS keys, you'll have to add thekms:Decryptpermission for read access andkms:Encrypt+kms:GenerateDataKeyfor editing files.Closing wordsI hope this webapp illustrates how you can use the Serverless Dash stack to build powerful apps on AWS.Check out the code, stay tuned for further articles in this series, andget in touchif you want us to implement something similar for you.\u2014 MauriceOther articles in this series:Deploying a Serverless Dash App with AWS SAM and LambdaAdding Basic Authentication to the Serverless Dash App"}
{"title": "DevOps with Guruu | Chapter 18 : Build and Deploy DevSecOps Chatbot Use generative AI [ Full ]", "published_at": 1712021974, "tags": ["webdev", "aws", "devops"], "user": "Hoang Guruu", "url": "https://dev.to/aws-builders/devops-with-guruu-chapter-18-build-and-deploy-devsecops-chatbot-use-generative-ai-full--8pf", "details": "DevOps with Guruu | Chapter 18 : Build and Deploy DevSecOps Chatbot Use generative AI [ Full ]0:00 Introduction0:30 Add Documents To (S3)3:30 Search With Amazon Kendra4:30 Set up access to Amazon Bedrock8:30 Use Sagemaker Studio IDE to build your Chatbot20:00 Test your Chatbot with multiple LLMs25:00 Deploy the Chabot to a public site32:00 Custom DevSecOps Chatbot S3 data52:00 CleanupJoin me on this journey to mastering DevOps. Let's get hands-on and build a strong foundation together! Don't forget to like, share, and subscribe for more exciting content. Happy coding, future DevOps!\""}
{"title": "Issue 38 of AWS Cloud Security Weekly", "published_at": 1712017983, "tags": ["aws", "security", "cybersecurity", "newsletter"], "user": "AJ", "url": "https://dev.to/aws-builders/issue-38-of-aws-cloud-security-weekly-3mf3", "details": "(Summary of Issue 38 of AWS Cloud Security weekly @https://aws-cloudsec.com/p/issue-38<< Subscribe to receive the full version in your inbox weekly).What happened in AWS CloudSecurity & CyberSecurity last week March 19-April 1, 2024?AWS Identity and Access Management (IAM) Roles Anywhere credentials can now remain valid for up to 12 hours, allowing you to authenticate any AWS request. Previously, the temporary credentials were valid for only 1 hour, but now it ranges from 15 minutes to 12 hours. The default duration remains at 1 hour, but you can optimize the number of CreateSession requests to IAM Roles Anywhere by extending the validity period as needed.You now have the option to configure all future launches of Amazon EC2 instances in your account to utilize Instance Metadata Service Version 2 (IMDSv2) as the default setting. IMDSv2 represents an enhancement that introduces session-oriented requests, bolstering security measures against unauthorized access to metadata. Previously, to enforce IMDSv2-only access, you had to either utilize the IMDS Amazon Machine Image (AMI) property, configure Instance Metadata Options during instance launch, or employ the ModifyInstanceMetadataOptions API to update instances post-launch.Now, you have the capability to employ both GitLab and GitLab Self Managed as the source provider for your CodeBuild projects. This means you can trigger builds based on modifications made to source code stored within your GitLab repositories.AWS Control Tower customers operating in the AWS GovCloud (US) Regions can now use APIs to programmatically manage controls, perform landing zone operations, and extend governance to organizational units (OUs).AWS has officially launched Amazon GuardDuty EC2 Runtime Monitoring, enhancing threat detection capabilities for EC2 instances during runtime. This feature complements GuardDuty's existing anomaly detection by continuously monitoring VPC Flow Logs, DNS query logs, and AWS CloudTrail management events. With this update, users gain visibility into on-host, OS-level activities, and container-level context for identified threatsTrending on the news & advisories:CISA & RedHat warn of xz-backdoor-CVE-2024-3094.CISA and FBI Release Secure by Design Alert to Urge Manufacturers to Eliminate SQL Injection Vulnerabilities.Defense Industrial Base Cybersecurity strategy 2024"}
{"title": "Spring Boot 3 application on AWS Lambda - Part 2 Introduction to AWS Serverless Java Container", "published_at": 1711980513, "tags": ["java", "springboot", "aws", "serverless"], "user": "Vadym Kazulkin", "url": "https://dev.to/aws-builders/spring-boot-3-application-on-aws-lambda-part-2-introduction-to-aws-serverless-java-container-144", "details": "Introduction to AWS Serverless Java Container (for Spring Boot 3)TheAWS Serverless Java Containermakes it easier to run Java applications written with frameworks such as Spring, Spring Boot 2 and 3, or JAX-RS/Jersey in Lambda. We have already seen thatMicronaut frameworkalso uses AWS Serverless Java Container.The container provides adapter logic to minimize code changes. Incoming events are translated to theServlet specificationso that frameworks work as before **.AWS Serverless Java Container delivers thecore containerand framework specific container like one forSpring Boot 3which is the area of our interest for this article. There also other containers for Spring, Struts and Jersey frameworks. The major update to the version 2.0 has been recently released for all AWS Serverless Java Containers. If we look into the whole dependency tree, we'll discover another dependencyspring-cloud-function-serverless-webthat artifact aws-serverless-java-container-springboot3 requires which is the collaboration effort between Spring and AWS Serverless developers. It providesSpring Cloud Function on AWS Lambdafunctionallity. We'll look deeper into the capabilities of Spring Cloud Function on AWS Lambda in one of my upcoming articles.AWS Serverless Java Core Container provides also abstractions likeAWSProxyRequest/Responsefor mapping of API Gateway (Rest) request to the Servlet model including different authorizers like the Amazon Cognito and HttpApiV2JwtAuthorizer.Then everything will be proxied internally in the core container through theAwsHttpServletRequest/Responseabstractions or their derivates like AwsProxyHttpServletRequest.My personal wish is that a subset of abstractions i.e. from thecom.amazonaws.serverless.proxy.modelpackage like-AwsProxyRequest-ApiGatewayRequestIdentity-AwsProxyRequestContext-AwsProxyResponseand others will be a part of a separate project and therefore also used without the usage of the all other AWS Serverless Java Container APIs only for purpose of mocking the API Gateway Request/Response (i.e. for Priming). I've already used them forPriming requests for Quarkus and Micronaut frameworks. Dependency to the AWS Serverless Java Container was included by default for theMicronaut on AWS Lambda SnapStart Priming exampleand needed to be added explicitly for theQuarkus on AWS Lambda SnapStart Priming exampleonly to implementweb request priming. We'll make use of these abstractions in one of our subsequent articles when we'll discuss cold and warm start time improvements for Spring Boot 3 application on AWS Lambda using AWS Lambda SnapStart in conjunction with priming techniques.The Lambda runtime must know which handler method to invoke. For this AWS Serverless Spring Boot 3 Container which internally uses AWS Serverless Java Core Container adds only several implementations on top that likeSpringDelegatingLambdaContainerHandleror implement our own handler Java class that delegates to AWS Serverless Java Container. This is useful if we want to implement additional functionality like Lambda SnapStart priming technique. This can de done by usingSpringBootLambdaContainerHandlerabstraction (which inheritsAwsLambdaServletContainerHandlerfrom the core container) which can be created by giving SpringBoot class which is annotated with @SpringBootApplication as an input. For Spring Boot 3 applications that take longer than 10 seconds to start, there is an asyncronous way of creating SpringBootLambdaContainerHandler by usingSpringBootProxyHandlerBuilderabstraction. Since version 2.0.0 it always runs asynchronously by default, in the prior versions we had to invokeasyncInitmethod (which is now became deprecated) to run the builder asynchronously. I'll provide more detailed explanation about this with code examples in the next article of the series.ConclusionIn this article we introduced AWS Serverless Java Container components like core and framework specific adaptors like for Spring Boot 3. In the next article of the series we'll explore how to develop the Lambda functions using AWS Serverless Java Container (for Spring Boot) which receives the request from Amazon API Gateway and stores to and read from Amazon DynamoDB.**Re-platforming Java applications using the updated AWS Serverless Java Container"}
{"title": "DevOps with Guruu | Chapter 17 : Full Course CI/CD Pipeline AWS | Add to Resume", "published_at": 1711956076, "tags": ["webdev", "aws", "devops"], "user": "Hoang Guruu", "url": "https://dev.to/aws-builders/devops-with-guruu-chapter-17-full-course-cicd-pipeline-aws-add-to-resume-1kce", "details": "DevOps with Guruu | Chapter 17 : Full Course CI/CD Pipeline AWS | Add to Resume0:00 Introdution2:27 Solve problem Chapter 1616:44 Prepare CDK Application23:00 Source25:00 Build33:00 Deploy41:00 Pipeline50:00 CleanupJoin me on this journey to mastering DevOps. Let's get hands-on and build a strong foundation together! Don't forget to like, share, and subscribe for more exciting content. Happy coding, future DevOps!\""}
{"title": "Kinesis Producers", "published_at": 1711952892, "tags": ["kinesisproducer", "datastream", "analytics", "aws"], "user": "Olawale Adepoju", "url": "https://dev.to/aws-builders/kinesis-producers-43cp", "details": "Kinesis ProducersA producer for Amazon Kinesis Data Streams is an application that feeds user data records into a Kinesis data stream (also called data ingestion). The Kinesis Producer Library (KPL) makes it easier to construct producer applications by allowing developers to achieve high write throughput to a Kinesis data stream.There are different methods to stream data into Amazon kinesis streams:Kinesis SDKKinesis Producer Library (KPL)Kinesis AgentOther third-party libraries include:Spark, Log4J, Appenders, Flume, Kafka Connect, NiFiKinesis Producer SDK - PutRecord(s)PutRecord (one record) and PutRecords (many records) APIs are utilized.PutRecords leverages batching and enhances performance, resulting in fewer HTTP calls.AWS Mobile SDKs: Android, iOS, etc...Managed Amazon Web Services sources for Kinesis Data Streams:AWS IoTCloudWatch LogsKinesis Data AnalyticsUse cases:low throughput, higher latency, simple API, AWS LambdaKinesis Producer Library (KPL)Easy to use and highly configurable C++/Java libraryUsed for building high-performance, long-running producersAutomated and configurable retry mechanismSynchronous or Asynchronous APIs (better performance for async)Submits metrics to CloudWatch for monitoring.Batching (both turned on by default) \u2013 increase throughput, decrease cost:Collect Records and Write to multiple shards in the same PutRecords API call.Aggregate \u2013 increased latency.Kinesis Producer Library (KPL) BatchingBy inserting some delay using RecordMaxBufferedTime, batching efficiency can be impacted (default 100ms)NOTE:When not to use the Kinesis Producer LibraryThe KPL can incur an additional processing delay of up toRecordMaxBufferedTimewithin the library (user-configurable)Larger values \u200b\u200bofRecordMaxBufferedTimeresult in higher packing efficiencies and better performanceApplications that cannot tolerate this additional delay may need to use the AWS SDK directlyKinesis AgentMonitor Log files and sends them to Kinesis Data StreamsJava-based agent, built on top of KPLInstall in Linux-based server environmentsFeatures:Write from multiple directories and write to multiple streamsRouting feature based on directory/log filePre-process data before sending to streams (single line, CSV to JSON, log to JSON)The agent handles file rotation, checkpointing, and retry upon failuresEmits metrics to CloudWatch for monitoringAWS Kinesis API - ExceptionsProvisioned Throughput Exceeded ExceptionsHappens when sending more data (exceeding MB/s or TPS for any shard)Make sure you don't have a hot shard (such as your partition key is bad and too many data goes to that partition) Solution:Retries with backoffIncrease shards (scaling)Ensure your partition key is a good one"}
{"title": "AWS EC2 Instances purchasing options", "published_at": 1711947518, "tags": ["aws", "ec2", "devops", "cloud"], "user": "Armando Contreras", "url": "https://dev.to/aws-builders/aws-ec2-instances-purchasing-options-5ba9", "details": "I know that understanding the different purchase options for AWS EC2 instances can be confusing at the beginning. In this blog i will explain you how these models works and help you understand how you can benefit from each of them. Optimizing your compute usage can significantly impact your billing. One of the most important decisions when defining the instances for your workload is to determine the lifecycle of an instance(short-period of time? long period?. There are two different ways to run your instances (on-demand, spot), even if you bought reserved instances those are running on-demand. Then we have the tenancy (shared or dedicated) that determines if your instances will be retrieved from a shared pool of VM's or from a dedicated host, instance(the dedicated tenancy will not be explained on this blog).OPTIONS AVAILABLEso what are the options available?On-Demand: Pay per second, only for the time that the instance is used. Each instance has a fixed price per hour, so there's a record of the time used by the instances. Some need specific licenses like Windows OS, for that reason, there's an extra fee due to the use of those licenses.This model is a good option for testing or short-term launches.Saving PlansReserved InstancesSpot InstancesCapacity ReservationDedicated Hosts:Pay for a physical host that is fully dedicated to running your instances, and bring your existing per-socket, per-core, or per-VM software licenses to reduce costs.Dedicated Instances: Pay, by the hour, for instances that run on single-tenant hardware.i will not cover dedicated hosts and instances due that is a very specific topic and. don't see a high value on knowing them at detail for the general publicSaving Plans:The Saving Plans offer a flexible pricing model that allows you to save money on some AWS services. This option offers lower prices for EC2 regardless of the family, instance type, OS, tenancy, or region. It also applies to Fargate and Lambda workloads. How does it work? It offers a discounted pricing compared to on-demand that can be, in the best case, a 72% discount. For compute (applies to EC2 instance usage, AWS Fargate, and AWS Lambda service usage regardless of instance family, size, AZ, region, OS, or tenancy),For EC2 instances (EC2 Instance Saving Plans provide the lowest prices, offering savings up to 72% in exchange for commitment to usage of individual instance families in a region (e.g. M5 usage in N. Virginia). This automatically reduces your cost on the selected instance family in that region regardless of AZ, size, OS or tenancy) it can also be used with sagemaker but that is beyond my knowledge.this model work in exchange for a commitment to using a specified amount of compute power (measured per hour) for a 1 or 3 year period instead of making commitments to specific instance configurations. When you subscribe to a saving plan, the prices that you will pay will remain fixed for the period of the plan. Here you have 3 options of payment for your commitment:All-upfront: All payment in the first monthPartial upfront: An initial payment (at least the 50%) and then a reduced amount of the total per monthNo-upfront: No payment is done at the beginning, each month you will pay a portion of the totalThe percentage of savings depends on various factors such as the commitment period, the hourly commitment usage.if the EC2 Instance saving plan is chosen region, and instance family become part of the factors to consider.For services that utilize EC2 behind the scenes, the saving plans do not apply to those services, but instead to the underlying EC2 instances.Reserved InstancesProvides a significant saving on your EC2 costs compared to on-demand instance pricing.These instances are not physical instances but rather a discount applied to on-demand instances, these on-demand must match certain attributes like instance type and region to apply the billing discount.So how does it work? Let's imagine you have an m6g on-demand instance, when you buy the reserved instances you make the attributes to match the running instances, and automatically the discount will be applied if there is a match between the RI and the on-demand instance. You could also buy the RI first and then launch the instance.Which values determine the pricing of the instance?instance type, region, tenancy and OS platformwhich terms commitments are available?are the same options as a saving plan, from 1 to 3 years(3 offering a bigger discount).once you bougth a reserved instances it can't be canceled but a new feature got integrated this week to allow youregret your purchase before 7 daysyou can also modify, change or sell your RI if your needs change.offering classthis is an option that allows you to modify or exchange your  RI's depending on the offering class selectedcomparision table of offering classesStandard: Offers a significant discount but can only be modified. This means that you can adjust some of its attributes during the years of the RI but can't be exchanged for a different RI.Convertible: Provides a lower discount than Standard RI's but the main advantage is that it can be exchanged for different Convertible RI's with different attributes or modify the same RI.The first thing to define before buying is the scope of the RI. The scope doesn't affect the price but makes a difference on the features available. For example, in the case of a regional the RI doesn't reserve capacity. On the other hand, selecting a zonal RI reserves capacity on that AZ and the discount only applies if the instance is launched on that AZ. Although this scope is not flexible on the instance type and size, the regional applies to the use of a specific family regardless of the size (this concept is known as instance size flexibility). I encourage you to take a look into thiscomparison tableto understand this in detail.Regional: the RI is bougth for a specific Region(regional RI)Zonal: the RI is bougth for a specific Availability Zone(zonal RI)how to use the RIs?a common mistake that happens when we hear Reserved instances is that we think that a physical instance is getting reserved for our use but this is not true(unless a zonal RI is selected), that really happnes is that a reserved instance is a discound applied to on-demand instances, as i explained at the beggining of the IRs section a match between the on-demand instances and the RI attributes must exists to apply the discount automatically.getting deep into instance size flexibilitythe flexibility of the instance is determined by a concept callednormalization factor. the discount applies total or partially to the ondemand instances depending on the size of the RI (only applies to regionals RI's). the only attributes that must match are the family, tenency and platform.normalization-factor-tableeach instance has a normalization factor, which is applied acording to the size of the Instances, thsi value is used to apply the discount. example a t3.medium has a normlization factor of 2 but imagine you launched 2 t3.small instances with a normalization value of 1, in this case the benefit applies as if you were using only one medium instance due to the normalization factor. but if for example you had a t3.large instances that has a NF of 4 then only 50% of the discount will be applied.this is applied from the smallest instance size to the largest in useexamples of RI's with normalization factorunder organiation accounts the discount is applied first in the account where the RI was bougth and then if applicable to another account in the organization.(i don't know under which condtions this applies)I will not delve further into the topic of reserved instances but here are three links that may be useful to you and that I have not covered in my blogcan i sell and buy reserved instances in the market?can i modify attributes of my RI's and what are the limitations??can i exchange RI's convertibles and how the normalization factor affects?RI pricing calculatorSPOT INSTANCESthe spot instances uses compute that is not currently utilized of ECE, this is available at a lower pricing than compared to on-demand. tha can be up to 90% in the best case. aws defines the spot pricing per hour calledspot priceThe Spot price of each instance type in each Availability Zone is set by Amazon EC2, and is adjusted gradually based on the long-term supply of and demand for Spot Instancesspot instances runs only when unused capacity is availablespot instnaces are a good option for workloads than can be interrupted. allowing you to have the most cost-effctive solution with a trade-off of losing instance availabilyKEY CONCEPTSSpot capacity Pool: a group of unused EC2 instaces with the same instance type and AzSpot Price: price of the spot instance per hourSpot instance Request: request for a spot instace when capacity is available. can be a one time or persitent to be reused in future requests. ec2 automatically recreates a persistent request when it has been fulfilled and interrupted.Spot Instance Interruption: ec2 terminates, stops or hibernates your spot instance when it needs the capacity back, an interruption notices is sent to give two minutes windows time before interrupting your instance so you can rebalance your workload.EC2 Instance Rebalance:EC2 emits an EC2 instance rebalance reconmendationto notify that a spot instance is at high risk of getting interrupted. this gives you the opportunity to rebalance your wokads usign other spots intances without having to wait for the 2 minute interruption i will not delve into the usage of spot instances because this is a very extensive topic and i expect to create an specific post in the future to show its usage at detail in conjuntion with eks karpenterCAPACITY Reservationa CR allows you to reserve compute capcity for EC2 instances under an specific AZ. this is very useful on strict capacity requirements for workloads that require capacity assurance for High Availability, with this you can ensure that you'll always have access to the ec2 capcitity you've reserved it for as long as you need it.there are other uses cases as machine learning models training  where you can need more GPU's during the training porcess but there is no more capcity available at that time, with CR's you can ensure that oy the capatity needed for the time of the training(i will not delve into the capacity blocks for ML due to my lack of knwoledge in the Ml area).one of the main advantages of this purchare model is that you can buy this at any time without a commitment of one-year or three-year term.The capacity becomes available and billing starts as soon as the Capacity Reservation is provisioned in your account. When you no longer need the capacity assurance, cancel the Capacity Reservation to release the capacity and to stop incurring chargesthis model can be combined with the billing discounts of Savings Plans and Regional RI's to reduce the cost of a Capacity Reservation. zonal RI's doesn't need a capacity reservation becasue that feature is already provided by that purchase model.CR's can only be used by instances that match their attributes. By default they are automatically used by running instances that match the attributes. If you don't have any running instances that match the attributes of the Capacity Reservation, it remains unused until you launch an instance with matching attributesDifferences between Capacity Reservations, Reserved Instances, and Savings Plansthis a great table to understand the differences between the main pricing modelsimportant to know that you will get billed as soon as the CR starts and ends once gets expired or cancelledHow to make decisions?it's fully dependant on your use case. for an initial POC you can start with on-demand once you have a clear definition of your mimum usage i will go with saving plans, not to cover all the costs but at least have a discount with a defined minimum usage of my ec2 instances. then once you have a clear definition of your usage and your expectations for the future, i'll take a look into the RI's at least for a 1 year commitment.(but only for those workloads that i'm sure will be using that capacity for that period of time). 3 years commitment for RI's? is a great discount but the commitment is a lot of compromise. i will prefer to go with a convertible in case that the business needs change over time. spot instances? only for those workloads that can be interrupted and i want to optimize the cost of it. i have seen amazing histories of costs optimizations using spot instances with eks karpenter so if you know how to use it you can get a huge cost optimization. at the end i will say that probably you will not find the best option at the first try but the cost-optimization is a work of progression over time and will get mature in conjuntion with your workload expectations and business needs.i hope you liked this content. i'm making this EC2 at detail serie as part of my deep study of EC2. i found myself in some situations where i was creating cost-estimations and i didn't know the differences between the purchase options of ec2 and now that i got the opportunity to delve into this topic i feel very confident about providing the nearest rigth solution depending on the use case and the needs of my clients. maybe that happened to others... being on the aws cost calculator and not knowing the difference between each purchase option\ud83d\ude04"}
{"title": "Creating a custom domain name for AWS Elastic Beanstalk application", "published_at": 1711942727, "tags": ["aws", "route53", "elasticbeanstalk", "django"], "user": "Dickson", "url": "https://dev.to/aws-builders/creating-a-custom-domain-name-for-aws-elastic-beanstalk-application-6ag", "details": "Amazon Route 53 is a highly available and scalable Domain Name System (DNS) service. Besides traffic management, Route 53 provides various features such as domain name registration, DNS routing policies and health checks.Registering a domainIn theRoute 53 console, selectRegistered domainsin the navigation pane and clickRegister domains.Enter a desired domain name and clickSearch. From the list of the available domains, choose one and clickSelect.ClickProceed to checkout.Select the duration and/or (un)check the Auto-renew option, and clickNext.Enter the contact information and clickNext.Review the information, accept the terms and conditions, and clickSubmit.Note: The registration can take up to 15 minutes. You may receive an email to verify your email address for the domain name, or the domain name will be suspended. If you receive an email about failing to register the domain name, create anAccount and billingsupport case with serviceDomainsand categoryRegistration issue. The issue should be resolved in 2-3 days.Creating records in Route 53 Hosted ZoneSelectHosted zonesin the navigation pane and click the hosted zone created in the previous step.ClickCreate record.Enterwwwas the subdomain, toggle on the switch forAlias, selectAlias to Elastic Beanstalk environment, select the region of the environment, select the environment accordingly, keep other values default, and clickCreate records.Updating the Django settingsAppend the Route 53 domain name to theALLOWED_HOSTSlist in the file namedsettings.pyin theebdjangodirectory, and save the file.ALLOWED_HOSTS=[\"djangoproj-dev.eba-ttkddb9r.us-east-1.elasticbeanstalk.com\",\"www.elasticbeanstalkapp.com\",]Re-deploy the Django site.eb deployAfter the environment update completes, the Django site is updated. At this stage, navigating to either the Elastic Beanstalk domain name (i.e.http://djangoproj-dev.eba-ttkddb9r.us-east-1.elasticbeanstalk.com) or the Route 53 domain name (i.e.http://www.elasticbeanstalkapp.com) gets the same Django site.Referenceshttps://docs.aws.amazon.com/Route53/latest/DeveloperGuide/domain-register.html#domain-register-procedure-sectionhttps://docs.aws.amazon.com/Route53/latest/DeveloperGuide/routing-to-beanstalk-environment.html"}
{"title": "Deploying a Django site on AWS Elastic Beanstalk", "published_at": 1711942666, "tags": ["aws", "elasticbeanstalk", "django", "webdev"], "user": "Dickson", "url": "https://dev.to/aws-builders/deploying-a-django-site-on-aws-elastic-beanstalk-225b", "details": "While a Django site can be set up locally with ease, making it accessible everywhere takes a substantial amount of effort. Ranging from infrastructure management to network configurations, these steps are complex, error-prone and time-consuming for developers and businesses, and hence the showstoppers to web application deployment.AWS Elastic Beanstalk is a managed service for deploying and scaling web applications and services. Apart from capacity provisioning and load balancing, Elastic Beanstalk also provides built-in auto-scaling, monitoring and logging capabilities, enhancing the application reliability and availability.PrerequisitesGitPython 3pipvirtualenvConfiguring the Django project for Elastic BeanstalkChange into the outerdjangoprojdirectory, if needed.cd djangoprojGet the required packages for the project.pip freeze > requirements.txtCreate a directory named.ebextensions.mkdir .ebextensionsCreate a configuration file nameddjango.configin the.ebextensionsdirectory with the following contents.option_settings:aws:elasticbeanstalk:container:python:WSGIPath:djangoproj.wsgi:applicationNote: The aboveWSGIPathvalue refers to theapplicationcallable in thewsgi.pyof thedjangoprojproject.The following shows an expected directory structure after the configuration. You may have more directories if you have created or installed other applications.djangoproj/     .ebextensions/         django.config     djangoproj/         __init__.py         settings.py         urls.py         asgi.py         wsgi.py     db.sqlite3     manage.py     requirements.txtEnter fullscreen modeExit fullscreen modeInstalling the EB CLINote: It is recommended to complete the installation on a separate terminal so that the changes would not affect the code for the Django project.Clone the setup scripts.git clone https://github.com/aws/aws-elastic-beanstalk-cli-setup.gitInstall/Upgrade the EB CLI.Unix:python ./aws-elastic-beanstalk-cli-setup/scripts/ebcli_installer.pyWindows:python .\\aws-elastic-beanstalk-cli-setup\\scripts\\ebcli_installer.pyIf the output contains an instruction to add the EB CLI (and Python) executable files to the shell's$PATHvariable, follow it and run the command. You also need to restart the opened terminal(s) before running anyebcommands there.Optionally, you can clean up the cloned setup scripts.Configuring the EB CLIRuneb initand follow the prompt instructions.Enter the number that corresponds to the desired region.Select a default region 1) us-east-1 : US East (N. Virginia) 2) us-west-1 : US West (N. California) 3) us-west-2 : US West (Oregon) 4) eu-west-1 : EU (Ireland) 5) eu-central-1 : EU (Frankfurt) ... (default is 3):Enter the access key and secret key.Note: The credentials are used by the EB CLI for managing Elastic Beanstalk applications. If you do not have the keys, followManage access keysto create one.You have not yet set up your credentials or your credentials are incorrect. You must provide your credentials. (aws-access-id):  (aws-secret-key):Enter an Elastic Beanstalk application name.Enter Application Name (default is \"djangoproj\"):EnterYto confirm Python as the platform.It appears you are using Python. Is this correct? (Y/n):Enter1to select Python 3.11 as the platform branch.Select a platform branch. 1) Python 3.11 running on 64bit Amazon Linux 2023 2) Python 3.9 running on 64bit Amazon Linux 2023 3) Python 3.8 running on 64bit Amazon Linux 2 4) Python 3.7 running on 64bit Amazon Linux 2 (Deprecated) (default is 1):Ignore the message related to CodeCommit setup.Cannot setup CodeCommit because there is no Source Control setup, continuing with initializationNote: Alternatively, the EB CLI provides integration with Git. For details, please refer toUsing the EB CLI with Git.EnterYto set up SSH connection to any instances in the Elastic Beanstalk environment.Do you want to set up SSH for your instances? (Y/n):Note: This connection allows you to inspect logs or to check other metrics on the instances (to be created in the next step) for easier troubleshooting.Enter a SSH key pair name and enter a passphrase.Type a keypair name. (Default is aws-eb):Note: The EB CLI registers this key pair with the instances and stores the private key in the local folder named.sshin the user directory.In the Elastic Beanstalk console, you should see the newly created application is listed on theApplicationspage.Deploying the Django siteCreate an Elastic Beanstalk environment nameddjangoproj-dev.eb create djangoproj-devNote: The load-balanced environment is created under the application created in the previous step. The Django site will also be deployed to the this environment. The entire creation takes several minutes.Follow the next steps only after the environment creation completes.Get the domain name of the environment.eb statusEnvironment details for: djangoproj-dev   Application name: djangoproj   ...   CNAME: djangoproj-dev.eba-ttkddb9r.us-east-1.elasticbeanstalk.com   ...Note: The domain name refers to the aboveCNAMEvalue.The domain name can be also obtained from theEnvironmentspage.Append the domain name to theALLOWED_HOSTSlist in the file namedsettings.pyin theebdjangodirectory, and save the file.ALLOWED_HOSTS=[\"djangoproj-dev.eba-ttkddb9r.us-east-1.elasticbeanstalk.com\"]Re-deploy the Django site.eb deployAfter the environment update completes, the Django site is deployed successfully with Elastic Beanstalk. You will see the following screen after navigating to the domain name above. Alternatively, this can be achieved by runningeb open.Configuring the health check (Optional)You may notice theHealthvalue isRedfrom the terminal orSeverefrom the Elastic Beanstalk console. While this issue should not cause any impacts to your application, this section provides a simple fix.Create a file namedmiddleware.pyin the innerdjangoprojdirectory with the following contents.fromdjango.httpimportHttpResponseclassHealthCheckMiddleware:def__init__(self,get_response):self.get_response=get_responsedef__call__(self,request):ifrequest.path==\"/_health\":returnHttpResponse(\"OK\")returnself.get_response(request)Note: The health check path is set to the root (i.e.\"/_health\").Prepend the health check middleware to theMIDDLEWARElist in the file namedsettings.pyin theebdjangodirectory, and save the file.MIDDLEWARE=[\"djangoproj.middleware.HealthCheckMiddleware\",...\"django.middleware.common.CommonMiddleware\",...]Create a configuration file namedhealthcheck.configin the.ebextensionsdirectory with the following contents.option_settings:AWSEBV2LoadBalancerTargetGroup.aws:elasticbeanstalk:environment:process:default:HealthCheckPath:/_healthRe-deploy the Django site.eb deployReferenceshttps://docs.aws.amazon.com/elasticbeanstalk/latest/dg/create-deploy-python-django.htmlhttps://github.com/aws/aws-elastic-beanstalk-cli-setuphttps://docs.aws.amazon.com/elasticbeanstalk/latest/dg/eb-cli3-configuration.htmlhttps://docs.djangoproject.com/en/5.0/ref/request-response/#django.http.HttpRequest.get_host"}
{"title": "AWS EC2 Instances Types (all you need to know)", "published_at": 1711941843, "tags": ["aws", "ec2", "devops", "cloud"], "user": "Armando Contreras", "url": "https://dev.to/aws-builders/aws-ec2-instances-types-all-you-need-to-know-3ndn", "details": "I'm fully sure you have been involved at some point with EC2. In my opinion, this is probably the most important service for most of the AWS workloads. In almost all of the solutions architectures I have been involved, the most expensive cost is EC2. I have found a lot of times overprovisioned instances, cost optimization opportunities by changing the instances type or architecting your applications to work on ARM processors, or just workloads that can work better using a different family instance type, without compromising operational excellence and iptimizing costs. In this blog, I will guide you over all the details of ec2 instance types that you need to know to see these opportunities and optimize your operational costs.NITRO SYSTEMbefore going deep into the instance types you need to know that aws built its own virtualization technology callednytro system. this is combination of hardware and software built for high performance, availability and security. offering capabilites \"bare metal\" a common myth with cloud virtualization si that the performance can be reduced compared with on-premises.nytro system remove the layer of resources utilized by the common virtuzalization technologiesthis is very useful for workloads that needs full access to the host hardware(later i will explain you what are thebareinstances types). in words of aws documentation the perfomance is so close to bare metal that you can't notice the difference. the main components of nitro system are focused on the essential, remove common elements commonly used in other virtualization technologes. one of the main features is that it implements a card that externalizes the network, storage and security components of the hosts. removing that duty from the instance hardware. this nitro system is the underlying virtualization technology used for the last generation of ec2 instance that allow aws to have full control on the virtulizacion. i engourage you to take a look into thisabout the history of nitro and how it helped AWS to develop and offer a wide range of instances types without sacryfing perfomance, recuding costs, optimizing perfomance and offering a better security. even creating its own processors as the graviton instances giving to the client exactly the compute resources it asked for.UNDERSTANDING THE EC2 INSTANCES TYPES AND ITS NOTATIONAll EC2 instance types with details (current generation)Currently, we have two instance generations, thepreviousand the current generation powered by the Nitro system. You can still use the previous generation, but the recommendation is to use the latest.INSTANCES TYPES PRIMARY CATEGORIESThe use cases of the workloads running on EC2 can be very different. For this reason, the instance types are divided into categories allowing you to right-size your workloads eitherhorizontallyorvertically.The type of instance selected at the time of launch will determine the hardware available for your application. Each type offers different capabilities such as memory, compute, storage, and network (sometimes these capabilities are combined, I'll explain later). Each of these instance types are categorized into groups orfamilies.General Purpose: Offers a similar proportion of compute, memory, and networking. Ideal for apps that use these resources in a balanced proportion.Compute Optimized: Designed for apps with intensive CPU usage.Memory Optimized: Designed for high performance for apps/databases with high memory (RAM) usage such as cache databases.Storage Optimized: Designed for workloads with high I/O with high volumes of storage with optimized IOPS.Compute Accelerated: Optimized for instances that use hardware accelerators, or co-processors, to perform functions that will be performed more efficiently than is possible in software running on CPUs.HPC Optimized: Designed for HPC workloads at scale. My understanding of this topic is null so I can't provide more details.NOMENCLATURE CONVENTIONEC2 currently offers a lot of different instance types and at the beginning, I know that it can be very confusing to decide which one makes more sense for your workloads. So let's make this easy for you. Let's get started by understanding the nomenclature. The type of instances is named based on its family, generation, processor family, additional capabilities, and size.The first position is the family. It can bec,r,m.The second position indicates the generation-version of the instance. Do not confuse this with the generation instances (previous or Nitro powered).It's more like a versioning of the instance type. generally a number (5,6,7).The third position indicates the family of the processorg(graviton), a(amd), i(intel). The pending letters indicate the additional capabilities as volumes, networking capability, or characteristics of the CPU.After the dot, it indicates the size of the instances as small, xlarge, or metal for bare metal instances.examples:c6gd.medium: In this case, the 'c' stands for compute-optimized family, '6' is the generation of this family, 'g' indicates that it's a Graviton, 'd' indicates it has additional storage, in most cases, an NVMe volume. Lastly, '.medium' is the instance size which, in this case, is 1 vCPU and 2 GB of RAM memory.c7gn.large: In this case, 'c' stands for compute optimized family, '7' indicates the generation of the family, 'g' represents that it uses graviton processors, 'n' indicates that it is network and EBS optimized. Lastly, '.large' is the instance size, in this case is 2 vCPUs and 4 GB of RAM memory.i3en.metal: Here, 'i' stands for storage-optimized family, '3' indicates the generation of the family, 'e' stands for extra storage or memory, 'n' indicates that it is network and EBS optimized. The 'metal' after the dot represents the instance size, which in this case refers to a bare metal instance, providing all the resources of the underlying server without nytro hypervisor, with bare metal instances you have the option to configure your own hypervisor, i haven't had any ooportunity to test this type of solution so i can't provide more details.Once you understand this nomenclature, let's proceed with understanding the instance families.Warning: This can be a lot of data, but I will not go deep into each instance family, just an overview to give you an idea.INSTANCE FAMILIESC - Compute Optimized: Optimized for workloads that require high usage of compute, ideally for applications with intensive use of CPU.D - Dense Storage: Designed for workloads that require large quantities of dense storage (HDD), ideally for warehousing.G - Graphics Intensive: Equipped with GPUs and optimized for workloads with intensive use of graphics, such as 3D, video streaming, or graphic design.HPC - High Performance Computing: Used for HPC workloads offering high networking throughput and compute capacity. I haven't seen workloads using this instance type and is a very specific topic out of my knowledge.I - Storage Optimized: Optimized for storage, ideally for databases or other types of applications that require high storage operation.Im - Storage optimized subcategory with a one to four ratio of vCPU to memory: This offers a specific proportion of vCPUs to memory, oriented for specific databases and storage applications.ls - Storage Optimized subcategory with a one to 6 ratio of vCPU to memory.Inf - AWS Inferentia: Designed for machine learning and inference. My knowledge of machine learning instances is null so I can't provide more details on this instance type.M - General Purpose: This is the most common instance type offering a proportional compute, memory, and network. Very used on web servers workloads.Mac - MacOS instances offering Mac instances, commonly used for Apple development applications.P - GPU acceleration: Offers GPUs and are optimized for parallel compute. I have seen workloads using this instance type for machine learning training models.R - Memory Optimized: For apps that need high memory usage, such as databases, big data apps.T - Burstable Performance: This type offers a good balance between cost and performance for workloads that don't use CPU constantly but at some point have spikes. This is done via credits that I will explain later.Trn - AWS Trainium: Especially designed for machine learning models. I don't know the difference between this, Inf, and P families.U - High Memory: Designed for applications that need large quantities of memory, such as databases. This makes me think about the difference against the R family.VT - Video Transcoding: Optimized for transcoding of video.X - Memory Intensive: Similar to the R family but fully focused on memory, made for workloads with extremely large quantities of memory.F - FPGA: Field Programmable Gate Arrays, I can't give a description of this because I don't fully understand its use.i would like to create a better table presentation of the instance families but this is just a high level description, you don't need to kwow all of them at detail so it's up to you to dive deep into those that caugth your interestPROCESSOR FAMILIESamd and intel has a line of processors specifically designed for servers under virtualization technologies, if you go to their websites you will find some of the processors that aws offers in the instances with a,i letters. there are some instances that doesn't have a processsor letter on its nomeclature which in most cases are intel processors, i don't know the context of this difference.a - AMD processorsg - AWS Graviton processors: These instances use the Graviton processors created by AWS, based on ARM architecture. These offer a better cost-performance ratio compared to other processors of the same instance size, and can be more beneficial for some workloads.i - Intel processorsADDITIONAL CAPACITIESb - Block Storage Optimization: These instances are optimized for block storage (EBS), tripling throughput and EBS performance IOPS.d - Instance Store Volumes: Offers temporary storage directly attached to the host hardware, ideal for temporary data with high-speed performance as caches. In most cases, it's an NVMe ephemeral volume.e - Extra Storage or Memory: Indicates that the instance has high memory (RAM) capacity, which is useful for applications that need very high usage of RAM or very high storage (HDD).n - Networking and EBS Optimized: Offers high networking throughput and EBS, ideal for app workloads that require high network throughput.z - High Performance: Designed for high performance in general. Can include processors with specific capabilities, more GHz, better networking, and IOPS.in some cases the addtitional capacities can be combined, i just put here the most important ones, there asome other additonal capabilites that are just present in just a single instance so i don't see a high value in knowing them.hey i know it's a lot of data and sincerely you don't need to know all of these but is very imporantnt to understand the notation of the intance types. save this post asa cheatsheet to have when rigth-sizing your workloads. at least on my experience i can say that is a very complex task to rigth-size your worloads and in most cases is more expensive to do a benchmark comparing different instances types than getting it done. for long-term solutions this is very beneficial combined with the purchase options available for EC2 that i'll explain in a different post of this serieat this point i have some questions for you.what is the meaning of each letter? (select differnt examples from the list of the aws instances types)if i gave you any instance type of the most commonly used, are you able to know the meaning of its nomenclature?what is the nytro system?what are the different categories of instance families?what is the difference between a .bare metal instance and an instance with the same size not being .metal?i missed something important to let you know and we are almost done...lastly we have one of my favorites commonly used which are theburstable instancesi encourage you to take a look into this link to have a detailed explanaition of it. i have seen some overprovisioned workloads where this instance type can be benefitial for those that can have some specific spikes of CPU usage and then lowering the usage for intermittent periods of time, i will explain later the use of ASG's and all the capabilities we have available with them in a different post. for comparing your instance selection the best tool that you can use is.instances.vantage.shthis tool allows you to compare different instances types and sizes at the same time.thanks for arriving at the end of this post, i know a lot of us can think that we already know ec2, but knowing the details of the instance types will be very benefitial for those use-cases where you need to provide the best solution for a specific workload being cost-optimized without compromising the operational excellence."}
{"title": "How to restrict default access to KMS via key policy with Terraform", "published_at": 1711928999, "tags": ["aws", "kms", "terraform", "security"], "user": "Matheus Almeida Costa", "url": "https://dev.to/aws-builders/how-to-restrict-default-access-to-kms-via-key-policy-with-terraform-3lc1", "details": "The objective of this post is to implement KMS key access security for AWS Identity and Access Management (IAM) identities by changing the default policy when provisioning the resource with Terraform.This is a practical example, so I first recommend recommend readthis postto better understand the objective of restricted key policy.Note: This post demonstrates the AWS account ID123456789012with existing role namedTERRAFORM,ADMINandANALYST. These values must be replaced for your environment.The default KMS key policy contains the following statement:{\"Sid\":\"Enable IAM User Permissions\",\"Effect\":\"Allow\",\"Principal\":{\"AWS\":\"arn:aws:iam::123456789012:root\"},\"Action\":\"kms:*\",\"Resource\":\"*\"}Enter fullscreen modeExit fullscreen modeBy default KMS policy allow caller's account to use IAM policy to control key access.The Effect and Principal elements do not refer to the AWS root user account. Instead, it allows any principal in AWS account123456789012to have root access to the KMS key as long as you have attached the required permissions to the IAM entity.The created terraform blueprint will come with the following custom policy by default:{\"Sid\":\"Enable root access and prevent permission delegation\",\"Effect\":\"Allow\",\"Principal\":{\"AWS\":\"arn:aws:iam::123456789012:root\"},\"Action\":\"kms:*\",\"Resource\":\"*\",\"Condition\":{\"StringEquals\":{\"aws:PrincipalType\":\"Account\"}}},{\"Sid\":\"Allow access for key administrators\",\"Effect\":\"Allow\",\"Principal\":{\"AWS\":[\"arn:aws:iam::123456789012:role/TERRAFORM\",\"arn:aws:iam::123456789012:role/ADMIN\"]},\"Action\":[\"kms:Create*\",\"kms:Describe*\",\"kms:Enable*\",\"kms:List*\",\"kms:Put*\",\"kms:Update*\",\"kms:Revoke*\",\"kms:Disable*\",\"kms:Get*\",\"kms:Delete*\",\"kms:TagResource\",\"kms:UntagResource\",\"kms:ScheduleKeyDeletion\",\"kms:CancelKeyDeletion\"],\"Resource\":\"*\"},{\"Sid\":\"Enable read access to all identities\",\"Effect\":\"Allow\",\"Principal\":{\"AWS\":\"arn:aws:iam::123456789012:root\"},\"Action\":[\"kms:List*\",\"kms:Get*\",\"kms:Describe*\"],\"Resource\":\"*\"}Enter fullscreen modeExit fullscreen modeThe key policy allows the following permissions:First statement: The AWS root user account has full access to the key.Second statement: The principals roleADMINandTERRAFORMhas access to perform management operations on the key.Third statementAll account principals are able to read the key.Terraform restrict KMS blueprint1.versions.tfDefine Terraform versions and providers.terraform{required_version=\">= 1.5\"required_providers{aws={source=\"hashicorp/aws\"version=\"~> 5.0.0\"}}}Enter fullscreen modeExit fullscreen mode2.main.tfUse the AWS provider in a region and create the KMS resource with your configuration parameters including your policy.provider\"aws\"{region=\"us-east-1\"}resource\"aws_kms_key\"\"this\"{description=\"Restricted kms key policy example\"policy=data.aws_iam_policy_document.restricted_key_policy.json}Enter fullscreen modeExit fullscreen mode3.variables.tfDefines the variable that will be responsible for the value of the new desired policy to be attached to the KMS policy.variablekey_policy{description=\"key policy\"type=list(object({sid=optional(string)effect=stringactions=list(string)resources=list(string)principals=optional(list(object({type=stringidentifiers=list(string)})))conditions=optional(list(object({test=optional(string),variable=string,values=list(string)})))}))default=[]}Enter fullscreen modeExit fullscreen mode4.policy.jsonCreate a rule with the new default policy based on the current account id and merge it with the custom policy passed by variable.data\"aws_caller_identity\"\"current\"{}locals{aws_account_id=data.aws_caller_identity.current.account_id# Default key policy to restrict AWS accessdefault_key_policy=[{sid=\"Enable root access and prevent permission delegation\"effect=\"Allow\"principals=[{type=\"AWS\"identifiers=[local.aws_account_id]},]actions=[\"kms:*\"]resources=[\"*\"]conditions=[{test=\"StringEquals\"variable=\"aws:PrincipalType\"values=[\"Account\"]},]},{sid=\"Allow access for key administrators\"effect=\"Allow\"principals=[{type=\"AWS\"identifiers=[\"arn:aws:iam::${local.aws_account_id}:role/TERRAFORM\",\"arn:aws:iam::${local.aws_account_id}:role/ADMIN\"]},]actions=[\"kms:Create*\",\"kms:Describe*\",\"kms:Enable*\",\"kms:List*\",\"kms:Put*\",\"kms:Update*\",\"kms:Revoke*\",\"kms:Disable*\",\"kms:Get*\",\"kms:Delete*\",\"kms:TagResource\",\"kms:UntagResource\",\"kms:ScheduleKeyDeletion\",\"kms:CancelKeyDeletion\"],resources=[\"*\"]},{sid=\"Enable read access to all identities\"effect=\"Allow\"principals=[{type=\"AWS\"identifiers=[local.aws_account_id]},]actions=[\"kms:List*\",\"kms:Describe*\",\"kms:Get*\",]resources=[\"*\"]}]}# Merge the default key policy with the new key policydata\"aws_iam_policy_document\"\"restricted_key_policy\"{dynamic\"statement\"{for_each=concat(local.default_key_policy,var.key_policy)content{sid=statement.value.sideffect=statement.value.effectactions=statement.value.actionsresources=statement.value.resourcesdynamic\"principals\"{for_each=try(statement.value.principals,null)==null?[]:statement.value.principalscontent{type=principals.value.typeidentifiers=principals.value.identifiers}}dynamic\"condition\"{for_each=try(statement.value.conditions,null)==null?[]:statement.value.conditionscontent{test=condition.value.testvariable=condition.value.variablevalues=condition.value.values}}}}}Enter fullscreen modeExit fullscreen mode5.outputs.tfDefines the output of the kms arn value after its creation.outputarn{value=aws_kms_key.this.arndescription=\"ARN KMS key\"}Enter fullscreen modeExit fullscreen mode6.terraform.tfvarsEnter a custom policy for the purpose of creating the KMS, in this case I will create one just to allow the use of actions for a specific role.key_policy=[{sid=\"Allow use of the key\"effect=\"Allow\"principals=[{type=\"AWS\"identifiers=[\"arn:aws:iam::123456789012:role/ANALYST\"]},]actions=[\"kms:Encrypt\",\"kms:Decrypt\",\"kms:ReEncrypt*\",\"kms:GenerateDataKey*\",\"kms:DescribeKey\"]resources=[\"*\"]}]Enter fullscreen modeExit fullscreen modeAfter apply, as a result you will always have a restricted KMS with the possibility of customizing it by changing the value of thekey_policyvariable, making it not only a secure blueprint but also scalable.Check out the full code on GitHub!."}
{"title": "How to restrict default access to KMS via key policy", "published_at": 1711928971, "tags": ["aws", "kms", "security"], "user": "Matheus Almeida Costa", "url": "https://dev.to/aws-builders/how-to-restrict-default-access-to-kms-via-key-policy-28gc", "details": "About AWS Key Management ServiceAWS Key Management Service (AWS KMS) is a managed service that makes it easy for you to create and control the cryptographic keys that are used to protect your data.The KMS key policy allows IAM identities in the account to access the KMS key with IAM permissions.The objective of this article is to implement secure of KMS key from access by AWS Identity and Access Management (IAM) identities.Note: This article demonstrates the AWS account ID123456789012with existing role namedTERRAFORM,ADMINandANALYST. These values must be replaced for your environment.The default KMS Authorization behaviourThe default KMS key policy contains the following statement:{\"Sid\":\"Enable IAM User Permissions\",\"Effect\":\"Allow\",\"Principal\":{\"AWS\":\"arn:aws:iam::123456789012:root\"},\"Action\":\"kms:*\",\"Resource\":\"*\"}Enter fullscreen modeExit fullscreen modeBy default KMS policy allow caller's account to use IAM policy to control key access.The Effect and Principal elements do not refer to the AWS root user account. Instead, it allows any principal in AWS account123456789012to have root access to the KMS key as long as you have attached the required permissions to the IAM entity.Implement secure KMS policyExample of restricted key policy:{\"Sid\":\"Enable root access and prevent permission delegation\",\"Effect\":\"Allow\",\"Principal\":{\"AWS\":\"arn:aws:iam::123456789012:root\"},\"Action\":\"kms:*\",\"Resource\":\"*\",\"Condition\":{\"StringEquals\":{\"aws:PrincipalType\":\"Account\"}}},{\"Sid\":\"Allow access for key administrators\",\"Effect\":\"Allow\",\"Principal\":{\"AWS\":[\"arn:aws:iam::123456789012:role/TERRAFORM\",\"arn:aws:iam::123456789012:role/ADMIN\"]},\"Action\":[\"kms:Create*\",\"kms:Describe*\",\"kms:Enable*\",\"kms:List*\",\"kms:Put*\",\"kms:Update*\",\"kms:Revoke*\",\"kms:Disable*\",\"kms:Get*\",\"kms:Delete*\",\"kms:TagResource\",\"kms:UntagResource\",\"kms:ScheduleKeyDeletion\",\"kms:CancelKeyDeletion\"],\"Resource\":\"*\"},{\"Sid\":\"Enable read access to all identities\",\"Effect\":\"Allow\",\"Principal\":{\"AWS\":\"arn:aws:iam::123456789012:root\"},\"Action\":[\"kms:List*\",\"kms:Get*\",\"kms:Describe*\"],\"Resource\":\"*\"}Enter fullscreen modeExit fullscreen modeThe key policy allows the following permissions:First statement: The AWS root user account has full access to the key.Second statement: The principals roleADMINandTERRAFORMhas access to perform management operations on the key.Third statementAll account principals are able to read the key.This way we can ensure that the root user account manages the key and prevents IAM entities from accessing the KMS key.You can also allow IAM users or roles to use the key for cryptographic operations and with other AWS services by appending other statements like this:{\"Sid\":\"Allow analyst role use the key\",\"Effect\":\"Allow\",\"Principal\":{\"AWS\":\"arn:aws:iam::123456789012:role/ANALYST\"},\"Action\":[\"kms:Encrypt\",\"kms:Decrypt\",\"kms:ReEncrypt*\",\"kms:GenerateDataKey*\",\"kms:DescribeKey\"],\"Resource\":\"*\"},{\"Sid\":\"Allow sms service use the key\",\"Effect\":\"Allow\",\"Principal\":{\"Service\":\"sns.amazonaws.com\"},\"Action\":[\"kms:GenerateDataKey*\",\"kms:Decrypt\"],\"Resource\":\"*\"}Enter fullscreen modeExit fullscreen modeSo if the KMS is configured in this way, its access will be restricted and only those identities directly specified in the key policy will have access.Check out the completed code example:{\"Version\":\"2012-10-17\",\"Statement\":[{\"Sid\":\"Enable root access and prevent permission delegation\",\"Effect\":\"Allow\",\"Principal\":{\"AWS\":\"arn:aws:iam::123456789012:root\"},\"Action\":\"kms:*\",\"Resource\":\"*\",\"Condition\":{\"StringEquals\":{\"aws:PrincipalType\":\"Account\"}}},{\"Sid\":\"Allow access for key administrators\",\"Effect\":\"Allow\",\"Principal\":{\"AWS\":[\"arn:aws:iam::123456789012:role/TERRAFORM\",\"arn:aws:iam::123456789012:role/ADMIN\"]},\"Action\":[\"kms:Create*\",\"kms:Describe*\",\"kms:Enable*\",\"kms:List*\",\"kms:Put*\",\"kms:Update*\",\"kms:Revoke*\",\"kms:Disable*\",\"kms:Get*\",\"kms:Delete*\",\"kms:TagResource\",\"kms:UntagResource\",\"kms:ScheduleKeyDeletion\",\"kms:CancelKeyDeletion\"],\"Resource\":\"*\"},{\"Sid\":\"Enable read access to all identities\",\"Effect\":\"Allow\",\"Principal\":{\"AWS\":\"arn:aws:iam::123456789012:root\"},\"Action\":[\"kms:List*\",\"kms:Get*\",\"kms:Describe*\"],\"Resource\":\"*\"},{\"Sid\":\"Allow use of the key\",\"Effect\":\"Allow\",\"Principal\":{\"AWS\":\"arn:aws:iam::123456789012:role/ANALYST\"},\"Action\":[\"kms:ReEncrypt*\",\"kms:GenerateDataKey*\",\"kms:Encrypt\",\"kms:DescribeKey\",\"kms:Decrypt\"],\"Resource\":\"*\"}]}Enter fullscreen modeExit fullscreen mode"}
{"title": "Create MongoDB Atlas Cluster With Terraform and AWS", "published_at": 1711924767, "tags": ["luxoft", "aws", "mongodb", "awscommunity"], "user": "Sagar R Ravkhande", "url": "https://dev.to/aws-builders/create-mongodb-atlas-cluster-with-terraform-and-aws-51d8", "details": "This project aims to set up an Atlas MongoDB cluster with an AWS network peering to access the resources from AWS EC2, utilizing Terraform as an Infrastructure as a Code(IAC).Prerequisites Needed:Basic understanding of Terraform concepts and Terraform CLI installed.AWS VPC with subnets and route tables.MongoDB Atlas account.MongoDB Atlas Organization and a Project under your account with public and private keys withOrganization Project CreatorAPI key.Table of ContentsMongoDB Atlas API key for terraformMongoDB atlas terraform providerModule for cluster resourceCluster ResourcesUsers and RolesNetwork peering with atlasPlan and applyResourcesMongoDB Atlas API key for terraformOnce you create an organization in Atlas, access to an organization or project can only be managed using the API key, so we need to create an API key.API keys have two parts: a Public Key and a Private Key. These two parts serve the same function as a username and a personal API key when you make API requests to Atlas.You must grant roles to API keys as you would for users to ensure the API keys can call API endpoints without errors. Here we will need an API key that will haveOrganization Project Creatorpermissions.e.g.All API keys belong to the organization. You can give an API key access to a project. Each API key belongs to only one organization, but you can grant an API key access to any number of projects in that organization.Configuring the MongoDB atlas provider for TerraformThe Terraform MongoDB Atlas Provider is a plugin that allows you to manage MongoDB Atlas resources using Terraform.Syntax:provider.tfterraform {   required_providers {     aws = {       source  = \"hashicorp/aws\"       version = \"~> 4.4\"     }     mongodbatlas = {       source  = \"mongodb/mongodbatlas\"       version = \"~> 1.9\"     }   } }Enter fullscreen modeExit fullscreen modeprovider \"mongodbatlas\" {   public_key = \"<YOUR PUBLIC KEY HERE>\"   private_key  = \"<YOUR PRIVATE KEY HERE>\" }Enter fullscreen modeExit fullscreen modeIn our case, all the required variables like Organization ID, Public key, and Private key are created in the AWS Systems Manager Parameter Store and are being referred to respectively.data \"aws_ssm_parameter\" \"private_key\" {   name            = \"/atlas/private-key\"   with_decryption = true }  data \"aws_ssm_parameter\" \"public_key\" {   name            = \"/atlas/public-key\"   with_decryption = true }  data \"aws_ssm_parameter\" \"atlas_organization_id\" {   name            = \"/atlas/org-id\"   with_decryption = true }Enter fullscreen modeExit fullscreen modeModule for cluster resourcesCluster resourcesmongodbatlas_projectprovides a Project resource. This allows the project to be created.mongodbatlas_network_containerprovides a Network Peering Container resource. The resource lets you create, edit, and delete network peering containers. You must delete network peering containers before creating clusters in your project. You can't delete a network peering container if your project contains clusters. The resource requires your Project ID.mongodbatlas_project_ip_access_listprovides an IP Access List entry resource that grants access from IPs, CIDRs, or AWS Security Groups (if VPC Peering is enabled) to clusters within the Project.resource \"mongodbatlas_project\" \"this\" {   name   = var.atlas_project_name   org_id = var.atlas_organization_id }  resource \"mongodbatlas_network_container\" \"this\" {   atlas_cidr_block = var.atlas_cidr_block   project_id       = mongodbatlas_project.this.id   provider_name    = \"AWS\"   region_name      = local.region_name }  resource \"mongodbatlas_project_ip_access_list\" \"this\" {   project_id = mongodbatlas_network_peering.this.project_id   cidr_block = var.vpc_cidr_block   comment    = \"Grant AWS ${var.vpc_cidr_block} environment access to Atlas resources\" }  resource \"aws_route\" \"this\" {   for_each                  = toset(var.private_route_table_ids)   route_table_id            = each.value   destination_cidr_block    = mongodbatlas_network_container.this.atlas_cidr_block   vpc_peering_connection_id = aws_vpc_peering_connection_accepter.this.vpc_peering_connection_id }Enter fullscreen modeExit fullscreen modeUsers and Rolesmongodbatlas_custom_db_roleallows you to create custom roles in Atlas when the built-in roles don't include your desired set of privileges. Atlas applies each database user's custom roles together with:Any built-in roles you assign when you add a database user or modify a database user.Any specific privileges you assign when you add a database user or modify a database user. You can assign multiple custom roles to each database user. E.g.resource \"mongodbatlas_custom_db_role\" \"roles\" {   for_each   = var.custom_roles   project_id = module.atlas_project.project_id   role_name  = each.value.role_name    dynamic \"actions\" {     for_each = each.value.actions     content {       action = actions.value.action       resources {         collection_name = actions.value.resources.collection_name         database_name   = actions.value.resources.database_name       }     }   } }Enter fullscreen modeExit fullscreen modemongodbatlas_database_userCreates database users to provide clients access to the database deployments in your project. A database user's access is determined by the roles assigned to the user.resource \"mongodbatlas_database_user\" \"database_users\" {   for_each           = var.database_users   username           = each.value.username   password           = jsondecode(data.aws_secretsmanager_secret_version.creds.secret_string)[each.value.username]   auth_database_name = \"admin\"   project_id         = module.atlas_project.project_id    dynamic \"roles\" {     for_each = each.value.roles     content {       database_name = roles.value.database_name       role_name     = roles.value.role_name     }   }   depends_on = [mongodbatlas_custom_db_role.roles] }Enter fullscreen modeExit fullscreen modeYou can create and upload the username and password details as a key pair values in AWS secret manager using,aws secretsmanager create-secret --name atlas-users --description \"Mytest with multiples values\" --secret-string file://secretmanagervalues.jsonEnter fullscreen modeExit fullscreen modeWhere,file://secretmanagervalues.jsonWhich will have below values e.g.{\"Juan\":\"mykey1\",\"Pedro\":\"mykey2\",\"Pipe\":\"mykey3\"}Network peering with atlasmongodbatlas_network_peeringNetwork peering establishes a private connection between your Atlas VPC and your cloud provider's VPC. The connection isolates traffic from public networks for added security.resource \"mongodbatlas_network_peering\" \"this\" {   container_id           = mongodbatlas_network_container.this.id   project_id             = mongodbatlas_project.this.id   provider_name          = \"AWS\"   accepter_region_name   = data.aws_region.current.id   vpc_id                 = var.vpc_id   aws_account_id         = data.aws_caller_identity.current.account_id   route_table_cidr_block = var.vpc_cidr_block }  resource \"aws_vpc_peering_connection_accepter\" \"this\" {   vpc_peering_connection_id = mongodbatlas_network_peering.this.connection_id   auto_accept               = true   tags = {     Name = var.peering_connection_name   } }Enter fullscreen modeExit fullscreen modeE.g.Plan and Apply your terraform codeYour tfvars file should look like this,environment.tfvarsvpc_id = \"<VPC-ID>\" vpc_cidr_block = \"10.0.0.0/16\" peering_connection_name = \"tf-mongo-atlas\" atlas_project_name = \"<Cluster-Name>\"  atlas_cidr_block = \"<Atlas-CIDR>\"  database_users = {   user1 = {     username           = \"Juan\",     password           = \"\",     auth_database_name = \"admin\"     roles = [       { database_name = \"admin\", role_name = \"readWriteAnyDatabase\" },     ]   },   user2 = {     username           = \"Pedro\",     password           = \"\",     auth_database_name = \"admin\"     roles = [       { database_name = \"admin\", role_name = \"readWriteAnyDatabase\" },       { database_name = \"admin\", role_name = \"oplogRead\" },     ]   },   user3 = {     username           = \"Pipe\",     password           = \"\",     auth_database_name = \"admin\"     roles = [       { database_name = \"admin\", role_name = \"readWriteAnyDatabase\" },     ]   },   // Add more users as needed }  custom_roles = {   oplogRead = {     role_name = \"oplogRead\"     actions = [       {         action = \"FIND\"         resources = {           collection_name = \"\"           database_name   = \"anyDatabase\"         }       },       {         action = \"CHANGE_STREAM\"         resources = {           collection_name = \"\"           database_name   = \"anyDatabase\"         }       },     ]   }   # Add more roles if needed }Enter fullscreen modeExit fullscreen modeNow all you need to do is to plan your terraform code using the terraform plan and verify the changes in the printed plan. You should be seeing one module to be added which will be the atlas cluster with the provided configuration.After verifying the plan in the above step, run terraform apply to apply your changes. You will see the cluster getting created message on the terminal and your changes will start appearing in your mongoDB console as well. It usually takes about 10 to 15 mins for the cluster to be created.You will see cluster created like this,ResourcesYou can find the project demo code here,https://github.com/sagary2j/atlas-mongodb-terraform/tree/initial-codeMongoDB atlas:https://registry.terraform.io/providers/mongodb/mongodbatlas/latestAtlas organizations apiKeys create:https://www.mongodb.com/docs/atlas/cli/stable/command/atlas-organizations-apiKeys-create/#inherited-options"}
{"title": "Start your tech journey with AWS", "published_at": 1711920686, "tags": [], "user": "Quadri Borokinni", "url": "https://dev.to/aws-builders/start-up-your-tech-journey-with-aws-25jl", "details": ""}
{"title": "AWS Cloud Services", "published_at": 1711920139, "tags": [], "user": "Quadri Borokinni", "url": "https://dev.to/aws-builders/aws-cloud-services-47n", "details": ""}
{"title": "Shard your open-search indices like a pro!", "published_at": 1711911208, "tags": ["sharding", "opensearch", "snapshots", "reindex"], "user": "Sandeep Kanabar", "url": "https://dev.to/aws-builders/shard-your-open-search-indices-like-a-pro-khp", "details": "Ever struggled with the growing size of your OpenSearch cluster indices and wondered how you could efficiently manage them?You can definitely leverage Index State Management (ISM)policiesbut knowing the intricacies of sharding goes a long way in helping you scale your cluster efficiently while keeping its performance optimal and most importantly keep a check on cost ($$).One might wonder why the need to learn sharding strategies in a managed service. Isn't the service supposed to handle it for you? Well, yes and NO. One analogy that I can give is of cars. There are cars with automatic gears and cars with manual gears. A lot of people these days prefer automatic cars. They run great on highways but what if you need to navigate the car through crowded traffic streets with curvy turns? The automatic car \"would\" work but it would be far from efficient, performant and scalable. The same goes for managed services.So how do you shard your indices efficiently? Let's begin with the basics.First determine if your indices are something that can be organised as time-based indices. In that case, opting for day-wise, weekly, monthly or even yearly indices may make sense. Note that you can always mix-n-match meaning you can club between day-wise, weekly, monthly, yearly. Say your current data flows into day-wise indices and then you could have an ISM policy such that data older than 90 days isre-indexedinto monthly indices. Or you can have monthly indices for current year and past years monthly indices could be re-indexed into yearly indices.You can also leverage the open-sourcecuratortool to manage all this using yaml scripts, as an alternative to ISM policies.So the question that arises is: why would you begin with day-wise indices if you want to merge (aka reindex) them later into monthly? why begin with monthly if you want to reindex them to yearly?The answer to this lies in \"performance\" and \"efficiency\" and also balancing indexing (write performance) and querying (read performance).Your current indices are getting live data. So you want to maximise indexing aka write performance. To maximize indexing performance, you can have more shards. More shards means more parallel writes leading to efficient writes.But here's the catch - the more the shards, more the time taken to search across all of them! Meaning query performance will suffer with more shards. Thus, there's a trade-off between indexing vs query performance and the trick lies in striking a balance.So how do we strike that balance. One way is to keep current day's index with more number of shards but past indices which have no data flowing in, can be \"reindexed\" to reduce the no of shards and then force-merged to reduce to a single (1) segment. When you re-index, the index name will change but with aliasing, this is simplified. Simply flip your alias to point to the new index name. This strategy helps to boost search/query performance and at the same time keep the indexing performance great as well. Win-win!So how many shards should you begin with?Arrive at how much data flows per day. Say 30 GB. Now, how many nodes do you have? Say 3 nodes. In that case, having 3 primary shards will be optimal as each node will have 1 shard and you can configure 1 replica. However, in this case, the primary shard size of 10 GB is way too small. Ideally, shards should be around 30-50 GB.Say daily data flow is 16 GB. In that case, just having 1 primary shard would do. Remember that too many small-sized shards is very detrimental to performance as there's context switching involved with underlying lucene indices.Let's say the indices are monthly indices and per day's average data flow is 30 GB. Thus per month it would average 30 GB * 30 days = 900 GB. Assuming 3 nodes, configuring this index with 12 shards would mean each primary shard size is 900/12 = 75 GB. The ideal size is 30-50 GB and this exceeds it by a huge margin. So let's look at having say 21 shards. In this case, each primary shard would be 900/21 = 43 GB. That's acceptable. With 24 shards, 900/24 = 37.5 GB primary shard size is also a good option. Remember that too large shards take a long time to move between nodes and slow down cluster recovery.This also helps you to plan your cluster capacity and scale it accordingly. Archiving old data into cold storage (s3/Azure storage blob/gcs bucket) is a good option. Another option is to snapshot old indices and then just delete them. The snapshots are stored in gcs/s3/azblobs which is cheap and can be easily restored on demand.Let's say you have an index for a customer master and the dataset is v less like just a master inventory and less than 10 MB. In that case, a single shard and single replica would suffice. You can also opt for 2 replicas but it all depends. If you take regular snapshots, then you can manage fine with 1 replicas. More replicas means more storage and more $$.Time-based indices have an advantage when it comes to snapshots. Say you have monthly indices and today is 31-March-2024 and you have snapshotted data pertaining to March-2022 into snap-my_monthly_index_2022_03. In that case, you can delete the index corresponding to March 2022 from your cluster if no searches are being performed against it and save storage costs. And in case it's later needed, you can easily restore the index quickly from snapshots.Hopefully, this beginner guide helps you in your sharding journey. Good luck."}
{"title": "Strategies for Cost Optimization in Container Deployment in AWS", "published_at": 1711903975, "tags": ["aws", "containers", "devops", "cost"], "user": "David Dennis", "url": "https://dev.to/aws-builders/strategies-for-cost-optimization-in-container-deployment-in-aws-gc8", "details": "Embracing cost optimization strategies for container deployment on AWS transcends traditional cost-cutting measures, evolving into a nuanced art of maximizing resource efficiency without compromising on performance or scalability. To navigate this landscape with a fine ship, businesses are encouraged to meticulously tailor their container configurations. In configuring container workloads on AWS, there are several best practices to adhere to, which I will share with you.Right-sizing, a practice that involves configuring containers with just the right amount of resources, prevents overspending on underutilized assets, striking a perfect balance between cost and capability.Furthermore, thestrategic utilization of AWS Spot Instancesemerges as a cornerstone for cost optimization. By leveraging these instances for suitable workloads, businesses can tap into the vast pool of unused AWS capacity at significantly reduced rates, achieving cost savings while maintaining the desired performance levels.Optimizing resource utilizationis another critical strategy, ensuring that every allocated resource is used to its fullest potential. This involves scrutinizing resource usage patterns and making informed adjustments to prevent waste, thereby fostering a more cost-efficient deployment environment.Lastly, the implementation ofauto-scalingpolicies stands as a testament to the dynamism of AWS's container services. These policies enable businesses to automatically adjust their container resources in response to fluctuating demands, ensuring that they are paying only for what they use when they use it. This dynamic scalability not only aids in managing costs effectively but also in maintaining an agile and responsive operational framework.By adopting these strategic approaches to cost optimization, businesses can significantly enhance their container deployment efficiency on AWS, paving the way for a more cost-effective, scalable, and performance-driven digital infrastructure."}
{"title": "Faster, better, cheaper: lessons from three years of running a serverless weather API", "published_at": 1711901961, "tags": [], "user": "Alexander Rey", "url": "https://dev.to/aws-builders/faster-better-cheaper-lessons-from-three-years-of-running-a-serverless-weather-api-1fc8", "details": "Since launching Pirate Weather in 2021, I have learned an incredible amount about some of the upsides and downsides to running a weather API. Some things have worked well, some have not, but overall, it has been an unparalleled experience. While I\u2019ve written a number of blog posts about specific aspects of this service, I wanted to take some time to put a few stray thoughts together about topics that are important, but not enough to write a whole post about.Traffic spikes- good news and bad newsThanks to some incredibly exiting press coverage, Pirate Weather was made the internet rounds last year! One of the upsides to an AWS serverless infrastructure is that capacity scales to meet demand; however, the challenge is that there\u2019s often hidden links that limit things. So, for my first lesson learned, it\u2019s that I should have done more load testing, since things don\u2019t always calmly build up to let you find the hidden, limiting links with time to spare.In particular, I ran into three main issues that contributed to downtime:Signups flow, and specifically email. It turns out that AWS imposes a default limit of 50 outgoing messages per day unless the quota is raised, and so after that was hit, my signup service (the API Gateway Portal) was unable to confirm new accounts, preventing new users from coming on board.EFS capacity. Pirate Weather relies on EFS burst credits to keep the shared file system humming along between the new model data coming online and forecast requests. EFS credits are generated automatically based on the amount of data stored on the share, and I try to keep this balanced. However, when there was a burst of traffic then the share run out of credits, and throughput slowed way down, knocking the service offline. The quick fix was to switch to a provisioned file system, which allows me to set a minimum throughput and regenerate the burning credits rapidly and setting a cloud watch alarm to alert me when they\u2019re running low. Longer term, this is addressed permanently in version two by switch to different systems for model data ingest (S3] and serving data (local NVME), to make sure that one doesn\u2019t overwhelm the other.Finally, a time-honoured lesson that can\u2019t be repeated enough is not to rely on single availability zone (AZ) services. When my primary AZ went down last year, I didn\u2019t have things set up to run in a second AZ, and so the service went down. Luckily, AWS has a ton of tools for making things run in multiple availability zones, and so now my EFS share spans multiple zones, as does my lambda response!MonitoringAnother great aspect of serverless computing is how easy it is to monitor things! I tried around with a few different services, but eventually settled on Grafana. They have a generous free tier, and by running the agent on a tiny EC2 instance it is able to query my Kong API Gateway containers to grab metrics from them.This is then paired with their AWS account integration to grab some key metrics from Lambda and EFS, ensuring that I have a complete picture of how everything is running in one dashboard.CostsOne of the best parts of getting some Pirate Weather press was that Corey Quinn noticed my service and offered to take a look at my setup to see where costs could be optimized! His company, The Duckbill Group, is pretty well unparalleled at deciphering AWS bills and optimizing services. Now, Pirate Weather runs a pretty lean operation (I originally built it to fit in the AWS free tier), so I wasn\u2019t sure if he would find much to optimize, with just a few containers, a EFS share, and a lambda function\u2026 I was very wrong.There were four primary insights that he identified with my setup:Sagemaker. You might be wondering why I had a sagemaker instance running when that\u2019s not part of the infrastructure, as was I! It turns out that it\u2019s very, very easy to leave these notebooks running, and they\u2019re not cheap. I\u2019d mucked around with one one day (link to AWS blog) and forgotten to completely disable it, so that was a quick and easy fix.ClousWatch logs. This was another area where it hadn\u2019t occurred to me to look for costs, but it turns out this service can add up quickly. I\u2019d added a few print statement to help debug a couple of processing scripts, as well as my main API response, and while these logs were great for fixing issues, they also ended up adding up. By streamlining what was being ingested and what the retention period was, this bill was trimmed right down.API Gateway. AWS API Gateway is a truly phenomenal service. It let me get a public API off the ground and running with almost no setup or overhead and was the perfect solution for years. However, this comes at a (literal) cost, and the downside is that per request cost isn\u2019t cheap. This was a longer-term fix, but ultimately resulted in me moving over to the Kong API gateway, which also allowed the service to scale past 10,000 users!One for ARM and ARM for one! Speaking of containers, most of the data ingest and processing happens in Fargate containers, and Corey pointed out that ARM containers are an objectively better choice when possible. Since my processing scripts were all Python, with some light tweaking I was able to move my ingest over to this container type.All these changes saved on billing costs, which is great in and of itself, but he also had some broader insight on improving my setup. There\u2019s no one rule to optimizing AWS bills, with way more nuance than I ever imagined, and it was invaluable having him to take a look!"}
{"title": "Exploring AWS Serverless Deployments with CDK v2: From RSS to X Posts - Part 2 of the Odyssey", "published_at": 1711891800, "tags": ["cdk", "lambda", "dynamodb", "python"], "user": "Adrian Mudzwiti ", "url": "https://dev.to/aws-builders/exploring-aws-serverless-deployments-with-cdk-v2-from-rss-to-x-posts-part-2-of-the-odyssey-1035", "details": "In this blog post, we'll continue our exploration of AWS Serverless deployments with CDK v2 by focusing on Lambda functions.We'll explore how to create and integrate these functions into our architecture along with a crucial step of granting permissions to resources that are deployed within the stack.LambdaThe first Lambda function that we will create will periodically query an RSS feed, process the data and store the data in DynamoDB.To get started with Lambda we will use theAmazon Lambda Python Library, this will provide constructs for Python Lambda functions. This will require Docker to be installed and running.Modify the requirements file for the stack as below:Next we will create a directory for our first Lambda function:mkdir lambda_rss_ddb_func cd lambda_rss_ddb_funcEnter fullscreen modeExit fullscreen modeLets create alambda_handler.pyfile, this file will contain our code that performs the magic:code lambda_handler.pyEnter fullscreen modeExit fullscreen modeThe next file that we will create will be therequirementsfile for all our Python dependencies:code requirements.txtEnter fullscreen modeExit fullscreen modeWe will only be using therequestslibrary in the Lambda function, so make sure to includerequestsin the newly createdrequirements.txt.Time to write some code in thelambda_handler.pyfile, the code extractspost id,post titleandlinkfrom a website feed, we will be using the feed fromHypebeast, once we have extracted the data we need, the data will be inserted into ourDynamoDB Table.Below is the code that you can populate yourlambda_handler.pyfile:Once you have modified your stack to include the construct to create a Lambda function, make sure to change the directory in your terminal to the root folder of the project.Let's turn our attention back to our stack to define the Lambda construct, add the below code:Amazon EventBridge ruleTo run the lambda function on a schedule, we can make use of anAmazon EventBridge rulethat will periodically run our Lambda function. Add the below construct and permissions to the stack:Another Lambda functionIn a few moments time we will create another directory for our second lambda function that is invoked from new records(s) being added to ourDynamoDB Tableand creates a post on X with thepost titleandpost link.SSM Parameter StoreTo access the X API, you'll need to create an X Developer account, I've included the link in the resources section of this post.We will need X credentials (consumer key, consumer secret, access token & access token secret).These credentials need to stored somewhere securely, theSSM Parameter Storeis a service that's free and will fulfill our next requirement well.Nagivate towards theParameter StoreunderAWS Systems Managerin theAWS Management Console.Select theCreate parameterbutton.In theNametextbox, enter/x/consumer_key, selectSecureStringunderTypeand paste yourconsumer_keyin theValuetextbox.Repeat the above process for the remaining credentials (consumer secret, access token & access token secret).XAPI stuff out the way, let's create that directory:mkdir lambda_x_share_func cd lambda_x_share_funcEnter fullscreen modeExit fullscreen modeWithin this directory create anotherlambda_handler_pyfile and arequirements.txtfile.Below is the code you should insert in the newly createdlambda_handler_pyfile in thelambda_x_share_func directory:You'll need to specify a region when initializing thessm_client, early on I noticed that Lambda was unable to access the values in theParameter Store, this was strange as all the documentation I read seemed to indicate that the Lambda function should have been able to access theParameter Storein the same region.In therequirements.txtfile make sure to includetweepy, that's the library that we will use in our Lambda function to interact programmatically withX.Navigate back to the stack, we wil now create a construct for theLambdaShareFunc, add the below code:In order for our Lambda function to read theParameter Storevalues inSSMwe can create a SSM Policy statement, this will grant the Lambda function permissions to retrieve the secrets.We need to allow our Lambda function to act when new items are added to our DynamoDB table, this can be achieved using DynamoDB Streams, lets add another construct.One last code addition to our stack is to enable DynamoDB Streams on our Table. This is achieved by addingstream=dynamodb.StreamViewType.NEW_IMAGEin the table construct:We're almost on the final stretch, ensure thatDockeris running.We can now deploy the stack usingcdk deploy, this will take a few moments.Below are screenshots from my terminal window and AWS Management Console of the successful deployment:I'll navigate towards an X burner account I created a couple of years ago for testing the X API, below are screenshots with Posts created:ConclusionIn this blog post, we've delved into the intricacies of integrating Lambda functions, adding permissions to newly created constructs  and enabling a DynamoDB stream trigger that invokes a Lambda Function to create a Post on X into our AWS serverless architecture deployments using CDK v2.In an upcoming blog post we will shift our focus on testing constructs and lambda functions locally.ResourcesAmazon Lambda Python LibraryThe ElementTree XML APIAmazon EventBridge ruleX Developer PlatformChange data capture for DynamoDB StreamsTweepy"}
{"title": "DJI FlightHub Sync: Media File Direct Transfer to AWS", "published_at": 1711880389, "tags": ["drone", "dji", "api", "aws"], "user": "Jacek Ko\u015bciesza", "url": "https://dev.to/aws-builders/dji-flighthub-sync-media-file-direct-transfer-to-aws-57en", "details": "There are many drone management platforms and DJI has its own calledFlightHub 2. It's quite a powerful software, where you can do a route planning, mission management, watch livestream from a drone or do some basic media files management.Drones have many use cases, so sometimes there is a need to customize your workflow - e.g. use your own AI/ML models for object detection or share media files captured by a drone to with a 3rd party software.To achieve this - you have to somehow integrate your software with the DJI ecosystem. There are a few possibilities how to do this.DJI Cloud APIThere isDJI Cloud API, which is a powerful framework that allows developers to interact with DJI drones and their associated data through common standard protocols such as MQTT, HTTPS, and WebSocket.It gives you a great flexibility, but in some cases it would be too much work. The problem is that you can either use FlightHub 2 or your Cloud API based solution. Using both at the same time is not supported, seeCan the third-party platform developed by Cloud API and Flighthub 2 be used at the same timesupport topic.This means that you can't for example process media files in our own software, but do the rest e.g. route planning, mission management in FlightHub 2. You would have to also implement those advanced features in your app.Fortunately, DJI has created a solution for such use cases -FlightHub Sync.FlightHub SyncFlightHub Syncis a feature within DJI FlightHub 2 that facilitates communication and data exchange between FlightHub 2 and third-party platforms. This includes APIs forMedia File Direct TransferTelemetry Data TransferStream ForwardingWe will focus on media files (photos, videos) transfer.To configure FlightHub Sync, you must have \"Super Admin\" or \"Organization Admin\" role. Go toMy Organizationand click \"Organization Settings\" action (icon button with a \"cog\").You will see FlightHub Sync (Beta) configuration in the upper-right corner.Click \"Details >\" link and you will see FlightHub Sync configuration divided into sections.We are interested in \"Basic Information\" and \"Media File Direct Transfer\" settings. Let's explore what information we will have to provide.Basic InformationWe have to configure a few thingsOrganization KeyThird-Party Cloud NameWebhook URLFormat will be somethings like this:{\"Organization Key\":\"xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx\",\"Third-Party Cloud Name\":\"My app\",\"Webhook URL\":\"https://xxxxxxxxxx.execute-api.eu-west-1.amazonaws.com/prod/dji/flighthub2/notify\"}Enter fullscreen modeExit fullscreen modeWe can find more info about this inConfigure Information on FlightHub Sync.Organization Key\"Organization Key\" is just a very long (64 hexadecimal digits) unique ID for the organization, which is generated by FlightHub 2.It's used for example inFlightHub Sync APIs. When you call an API endpoint e.g.Get Task Details- you have to provide it as a header parameter.Third-Party Cloud NameIt's a name of the third-party cloud platform, so an arbitrary name of your application. Let's use something like \"My app\".Webhook URLIt's a unique URL for receiving notifications from FlightHub 2, so the endpoint which we have to provision.When media files are uploaded - FlightHub 2 will send a POST request to notify us about synced files. Body of the callback will be similar to this:{\"notify_type\":\"way_line_file_upload_complete\",\"org_id\":\"xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx\",\"org_name\":\"My Organization\",\"prj_id\":\"xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx\",\"prj_name\":\"My Project\",\"sn\":\"XXXXXXXXXXXXXX\",\"task_info\":{\"task_id\":\"xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx\",\"task_type\":\"way_line\",\"tags\":[]},\"files\":[{\"id\":123456,\"uuid\":\"xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx\",\"file_type\":10,\"sub_file_type\":0,\"name\":\"DJI_20240329091034_0001\",\"key\":\"xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx/xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx/DJI_20240329091034_0001.jpeg\"},{\"id\":123457,\"uuid\":\"xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx\",\"file_type\":10,\"sub_file_type\":0,\"name\":\"DJI_20240329091112_0002\",\"key\":\"xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx/xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx/DJI_20240329091112_0002.jpeg\"}],\"folder_info\":{\"expected_file_count\":2,\"uploaded_file_count\":2,\"folder_id\":123458}}Enter fullscreen modeExit fullscreen modeMedia File Direct TransferWe have to configure only one thingStorage LocationIt's a JSON string, like this:{\"access_key_id\":\"XXXXXXXXXXXXXXXXXXXX\",\"access_key_secret\":\"xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx\",\"region\":\"eu-west-1\",\"bucket\":\"flighthub2-xxxxxxxxxxxx\",\"arn\":\"arn:aws:iam::xxxxxxxxxxxx:role/flighthub2\",\"role_session_name\":\"flighthub2\",\"provider\":\"aws\"}Enter fullscreen modeExit fullscreen modeWe can find more info about this configuration inConfigure Media File Direct TransferandFlightHub Sync Documentation.Storage LocationStorage location is a configuration of OSS (Object Storage Service).According toFlightHub Sync Documentation- currently supported OSS solutions areAWS (Simple Storage Service)Alibaba Cloud (Object Storage Service)ProjectTo make it work, we will have to also enable Media File Direct Transfer on the FlightHub 2 project level.Important thing to note is that:When enabled, data collected by dock will only be uploaded to My app. DJI FlightHub 2 will not receive any dataInfrastructureIt's now clear that we have to build a cloud based infrastructure to integrate our solution with FlightHub Sync.Two high level building blocks will beOSS (Object Storage Service)API (Application Programming Interface)Let's build our solution using AWS. We will useAWS CDK (Cloud Development Kit)andTypeScriptto define our cloud application resources.AWSOSS (Object Storage Service)Our OSS solution will include two things: S3 bucket where media files will be uploaded and access configuration using IAM.S3 (Simple Storage Service)Let's start with definingflighthub2-xxxxxxxxxxxxbucket, where media files will be uploaded by Media File Direct Transfer feature of FlightHub Sync. Bucket name must be globally unique, so we will add account number as a postfix.s3.cdk.tsimport{Construct}from\"constructs\";import*ascdkfrom\"aws-cdk-lib\";import*ass3from\"aws-cdk-lib/aws-s3\";exportclassS3CdkConstructextendsConstruct{publicbucket:s3.Bucket;constructor(scope:Construct,id:string){super(scope,id);this.bucket=news3.Bucket(this,\"bucket\",{bucketName:`flighthub2-${cdk.Stack.of(this).account}`,});newcdk.CfnOutput(this,\"bucket\",{value:this.bucket.bucketName,});}}Enter fullscreen modeExit fullscreen modeIAM (Identity and Access Management)Now let's defineflighthub2user and role. User will not have access to anything, but usingAWS STS (Security Token Service)he will be able to assumeflighthub2role and get temporary security credentials with access to theAmazon S3bucket.iam.cdk.tsimport{Construct}from\"constructs\";import*ascdkfrom\"aws-cdk-lib\";import*asiamfrom\"aws-cdk-lib/aws-iam\";exportclassIamCdkConstructextendsConstruct{constructor(scope:Construct,id:string){super(scope,id);constuser=newiam.User(this,\"User\",{userName:\"flighthub2\",});constaccessKey=newiam.CfnAccessKey(this,\"CfnAccessKey\",{userName:user.userName,});newcdk.CfnOutput(this,\"access_key_id\",{value:accessKey.ref});newcdk.CfnOutput(this,\"access_key_secret\",{value:accessKey.attrSecretAccessKey,});constrole=newiam.Role(this,\"role\",{assumedBy:newiam.ArnPrincipal(user.userArn),roleName:\"flighthub2\",managedPolicies:[// TODO: restrict it to flighthub2-xxxxxxxxxxxx bucket and s3:PutObject actioniam.ManagedPolicy.fromAwsManagedPolicyName(\"AmazonS3FullAccess\")],});newcdk.CfnOutput(this,\"arn\",{value:role.roleArn});}}Enter fullscreen modeExit fullscreen modeKeep in mind that we attachedAmazonS3FullAccesspolicy to theflighthub2role. For the production ready solution we should limit access to theflighthub2bucket we created and allow only uploading files, sos3:PutObjectaction. This is example of \"Principle of least privilege\".API (Application Programming Interface)API will consist of a single endpoint, which will get notification about uploaded files. We will build it using API Gateway and a Lambda function.API GatewayLet's start with defining REST API usingAPI Gatewaygw.cdk.tsimport*ascdkfrom\"aws-cdk-lib\";import*asgwfrom\"aws-cdk-lib/aws-apigateway\";import{Construct}from\"constructs\";exportclassApiGatewayCdkConstructextendsConstruct{publicapi:gw.RestApi;constructor(scope:Construct,id:string){super(scope,id);this.api=newgw.RestApi(this,\"my-app\");}}Enter fullscreen modeExit fullscreen modeNext we will create our endpoint by defining resources and method for our webhook and integrating a lambda function with it.api.cdk.tsimport*asgwfrom\"aws-cdk-lib/aws-apigateway\";import*aspathfrom\"path\";import{Construct}from\"constructs\";import{ApiGatewayCdkConstruct}from\"../../../gw.cdk\";import{LambdaCdkConstruct}from\"../../../lambda.cdk\";interfaceProps{apigateway:ApiGatewayCdkConstruct;}exportclassApiCdkConstructextendsConstruct{constructor(scope:Construct,id:string,{apigateway:{api}}:Props){super(scope,id);constdji=api.root.addResource(\"dji\",{// TODO: restrict CORSdefaultCorsPreflightOptions:{allowHeaders:gw.Cors.DEFAULT_HEADERS,allowMethods:gw.Cors.ALL_METHODS,allowOrigins:gw.Cors.ALL_ORIGINS,},});constflighthub2=dji.addResource(\"flighthub2\");constnotify=flighthub2.addResource(\"notify\");constnotifyLambda=newLambdaCdkConstruct(this,\"Notify\",{name:\"dji-flighthub2-notify\",description:\"Notification from DJI FlightHub 2\",entry:path.join(__dirname,\"./functions/notify.lambda.ts\"),});notify.addMethod(\"POST\",newgw.LambdaIntegration(notifyLambda.function));}}Enter fullscreen modeExit fullscreen modeKeep in mind that CORS configuration is very permissive. For the production ready solution we should restrict it.LambdaLambda function will be a core of our workflow. It will get a notification about uploaded media files. What we will do next highly depends on our use case. We can for example process uploaded files (generate thumbnail, detect objects), notify other parts of our app about synced files e.g. usingAmazon EventBridgeetc.There are also some requirements about response, which we have to send to FlightHub Sync. According to theFlightHub Sync Documentationwe have to respond with HTTP status code200and return{ code: 0 }.notify.lambda.tsimport{APIGatewayProxyHandler}from\"aws-lambda\";import{Notification}from\"../notification\";exportconsthandler:APIGatewayProxyHandler=async(event)=>{console.log(JSON.stringify(event));letnotification:Notification;try{notification=JSON.parse(event.body)asNotification;}catch(error){console.error(error);return{statusCode:400,body:JSON.stringify({code:1,message:\"JSON parse error\"}),};}switch(notification.notify_type){case\"way_line_file_upload_complete\":// TODO: your workflowbreak;default:console.log(\"Unknown notification type\",notification.notify_type);break;}return{statusCode:200,body:JSON.stringify({code:0}),};};Enter fullscreen modeExit fullscreen modenotification.tsexportinterfaceNotification{notify_type:\"way_line_file_upload_complete\"|string;org_id:string;org_name:string;prj_id:string;prj_name:string;sn:string;task_info:TaskInfo;files:File[];folder_info:FolderInfo;}exportinterfaceTaskInfo{task_id:string;task_type:\"way_line\"|string;tags:string[];}exportinterfaceFile{id:number;uuid:string;file_type:number;sub_file_type:number;name:string;key:string;}exportinterfaceFolderInfo{expected_file_count:number;uploaded_file_count:number;folder_id:number;}Enter fullscreen modeExit fullscreen modeTestingAfter deploying our application to the AWS cloud. It's time for some testing.Before we do a final end-to-end test with a real drone and FlightHub 2, we can write an integration test, which will mimic what FlightHub Sync is doing.Integration TestLet's create a simple Node.js app which will upload a file to our S3 bucket and call our webhook.asyncfunctionmediaFileDirectTransfer():Promise<void>{try{constcredentials=awaitgetCredentialsFromAssumedRole();awaituploadToS3(credentials);awaitsendNotification();}catch(error){logger.error(error);}}Enter fullscreen modeExit fullscreen modeWe are doing three things heregetting credentials from the assumed role using STSuploading test file to S3 using those credentialssending notification to our API endpointWhen we run the app, we should see 3 things:file should be uploaded to our S3 bucketcall to our API endpoint should return HTTP status code200and return{ code: 0 }we should see logs with our notification inAmazon CloudWatchLet's see in details how the above functions are implemented.Get credentialsasyncfunctiongetCredentialsFromAssumedRole():Promise<Credentials>{conststs=newSTSClient({region:process.env.REGION,credentials:{accessKeyId:process.env.AWS_ACCESS_KEY_ID,secretAccessKey:process.env.AWS_SECRET_ACCESS_KEY,},});const{Credentials}=awaitsts.send(newAssumeRoleCommand({RoleArn:process.env.ARN,RoleSessionName:process.env.ROLE_SESSION_NAME}));logger.info(Credentials);returnCredentials;}Enter fullscreen modeExit fullscreen modeUpload file to S3asyncfunctionuploadToS3(credentials:Credentials):Promise<void>{consts3=newS3Client({region:process.env.REGION,credentials:{accessKeyId:credentials.AccessKeyId,secretAccessKey:credentials.SecretAccessKey,sessionToken:credentials.SessionToken,},});constresponse=awaits3.send(newPutObjectCommand({Bucket:process.env.BUCKET,Key:\"fh2.txt\",Body:`Hello from FlighHub 2${newDate().toISOString()}`,}));logger.info(response);}Enter fullscreen modeExit fullscreen modeSend notificationasyncfunctionsendNotification():Promise<void>{constresponse=awaitaxios.post(process.env.API_URL,notification);logger.info(response.data);}Enter fullscreen modeExit fullscreen modenotificationis just a JSON file like this:notification.json{\"notify_type\":\"way_line_file_upload_complete\",\"org_id\":\"xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx\",\"org_name\":\"My Organization\",\"prj_id\":\"xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx\",\"prj_name\":\"My Project\",\"sn\":\"XXXXXXXXXXXXXX\",\"task_info\":{\"task_id\":\"xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx\",\"task_type\":\"way_line\",\"tags\":[]},\"files\":[{\"id\":123456,\"uuid\":\"xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx\",\"file_type\":10,\"sub_file_type\":0,\"name\":\"fh2\",\"key\":\"fh2.txt\"}],\"folder_info\":{\"expected_file_count\":1,\"uploaded_file_count\":1,\"folder_id\":123456}}Enter fullscreen modeExit fullscreen modeEnvironment variablesOur.envfile (with environment variables) will have a structure very similar to the FlightHub Sync configuration. We can take values from the AWS CDK/CloudFormation outputs after we deployment our solution usingcdk deploy..envAWS_ACCESS_KEY_ID=XXXXXXXXXXXXXXXXXXXX AWS_SECRET_ACCESS_KEY=xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx REGION=eu-west-1 BUCKET=flighthub2-xxxxxxxxxxxx ARN=arn:aws:iam::xxxxxxxxxxxx:role/flighthub2 ROLE_SESSION_NAME=flighthub2 PROVIDER=aws API_URL=https://xxxxxxxxxx.execute-api.eu-west-1.amazonaws.com/prod/dji/flighthub2/notifyEnter fullscreen modeExit fullscreen modeConclusionAlthough FlightHub Sync is still in beta, it has some bugs and I definitely have a wishlist of improvements (I sent feedback to the DJI) - it works and opens a lot of new possibilities for a custom drone related workflows."}
{"title": "DevOps with Guruu | Chapter 16 : Error with Cloudformation CI/CD Workshop | Get Problem", "published_at": 1711861765, "tags": ["webdev", "aws", "devops"], "user": "Hoang Guruu", "url": "https://dev.to/aws-builders/devops-with-guruu-chapter-16-error-with-cloudformation-cicd-workshop-get-problem-49l3", "details": "DevOps with Guruu | Chapter 16 : Error with Cloudformation CI/CD Workshop | Get ProblemCan you help me to answer this problemJoin me on this journey to mastering DevOps. Let's get hands-on and build a strong foundation together! Don't forget to like, share, and subscribe for more exciting content. Happy coding, future DevOps!\""}
{"title": "Provisioning AWS CloudTrail using Terraform (Step-by-Step)", "published_at": 1711861740, "tags": ["aws", "cloudtrail", "terraform", "beginners"], "user": "Esther Ninyo", "url": "https://dev.to/aws-builders/provisioning-aws-cloudtrail-using-terraform-step-by-step-hn4", "details": "CloudTrail is an AWS service that enables governance, compliance, operational and risk auditing of your AWS account. It captures all the events that happens within your AWS account and it is recorded as events in CloudTrail. The trail will be stored in an s3 bucket of your choice.To read more on CloudTrail, please visit thedocumentation page.In this technical post, we'll walk through the steps to provision CloudTrail using Terraform.Resources to be created:S3 bucketKMS (Key Management System) for S3 objects encryptionCloudTrailPrerequisite:An AWS account with permissions to create CloudTrail resourcesAWS cli configuredTerraform installedFolder Structurecloudtrail  #this is a folder ---> providers.tf ---> s3.tf ---> main.tf ---> kms.tfEnter fullscreen modeExit fullscreen modeStep 1:Create a new directory (cloudtrail) and navigate into itmkdir cloudtrail cd cloudtrailEnter fullscreen modeExit fullscreen modeproviders.tfterraform {   required_version = \"~> 1.6\"   required_providers {     aws = {       source  = \"hashicorp/aws\"     }   } }  # Configure the AWS Provider provider \"aws\" {   region = \"eu-west-1\"    default_tags {     tags = {       Environment = terraform.workspace,       ManagedBy   = \"Terraform\"     }   } }Enter fullscreen modeExit fullscreen modeStep 2:Initialize your projectterraform initEnter fullscreen modeExit fullscreen modeA successful initialization should look like the image below:Copy the code below to your main.tf filemain.tfresource \"aws_cloudtrail\" \"cloudtrail\" {   name                       = \"cloudtrail-tutorial\"   s3_bucket_name             = aws_s3_bucket.cloudtrail_s3.id   kms_key_id                 = aws_kms_key.cloudtrail_kms_key.arn   enable_log_file_validation = true   is_multi_region_trail      = true   enable_logging             = true    depends_on = [     aws_s3_bucket.cloudtrail_s3,     data.aws_iam_policy_document.cloudtrail_s3_policy,     aws_kms_key.cloudtrail_kms_key   ] }Enter fullscreen modeExit fullscreen modeCopy the code below to your s3.tf files3.tfresource \"aws_s3_bucket\" \"cloudtrail_s3\" {   bucket        = \"cloudtrail-bucket\"   force_destroy = true }  resource \"aws_s3_bucket_acl\" \"s3_bucket\" {   bucket = aws_s3_bucket.cloudtrail_s3.id    acl = \"private\" }  resource \"aws_s3_bucket_public_access_block\" \"pub_access\" {   bucket                  = aws_s3_bucket.cloudtrail_s3.id   block_public_acls       = true   block_public_policy     = true   ignore_public_acls      = true   restrict_public_buckets = true }   resource \"aws_s3_bucket_policy\" \"cloudtrail_s3_policy\" {   bucket = aws_s3_bucket.cloudtrail_s3.id   policy = data.aws_iam_policy_document.cloudtrail_s3_policy.json }  data \"aws_iam_policy_document\" \"cloudtrail_s3_policy\" {   statement {     sid       = \"AWSCloudTrailAclCheck\"     effect    = \"Allow\"     resources = [\"${aws_s3_bucket.cloudtrail_s3.arn}\"]     actions   = [\"s3:GetBucketAcl\"]      condition {       test     = \"StringEquals\"       variable = \"AWS:SourceArn\"       values   = [\"arn:aws:cloudtrail:eu-west-1:${data.aws_caller_identity.current.account_id}:trail/cloudtrail-${terraform.workspace}\"]     }      principals {       type        = \"Service\"       identifiers = [\"cloudtrail.amazonaws.com\"]     }   }    statement {     sid       = \"AWSCloudTrailWrite\"     effect    = \"Allow\"     resources = [\"${aws_s3_bucket.cloudtrail_s3.arn}/AWSLogs/${data.aws_caller_identity.current.account_id}/*\"]     actions   = [\"s3:PutObject\"]      condition {       test     = \"StringEquals\"       variable = \"s3:x-amz-acl\"       values   = [\"bucket-owner-full-control\"]     }      condition {       test     = \"StringEquals\"       variable = \"AWS:SourceArn\"       values   = [\"arn:aws:cloudtrail:eu-west-1:${data.aws_caller_identity.current.account_id}:trail/cloudtrail-${terraform.workspace}\"]     }      principals {       type        = \"Service\"       identifiers = [\"cloudtrail.amazonaws.com\"]     }   } }Enter fullscreen modeExit fullscreen modeCopy the code below to your kms.tf filekms.tfdata \"aws_caller_identity\" \"current\" {}  resource \"aws_kms_key\" \"cloudtrail_kms_key\" {   description         = \"KMS key for cloudtrail\"   enable_key_rotation = true   policy              = data.aws_iam_policy_document.kms_policy.json    depends_on = [     data.aws_iam_policy_document.kms_policy   ] }  resource \"aws_kms_alias\" \"kms_alias\" {   name          = \"alias/cloudtrail_kms\"   target_key_id = aws_kms_key.cloudtrail_kms_key.key_id }   # KMS KEY POLICY data \"aws_iam_policy_document\" \"kms_policy\" {   # Allow root users full management access to key   statement {     sid       = \"Enable IAM User Permissions\"     effect    = \"Allow\"     actions   = [\"kms:*\"]     resources = [\"*\"]     principals {       type = \"AWS\"       identifiers = [       \"arn:aws:iam::${data.aws_caller_identity.current.account_id}:root\"]     }   }   statement {     sid       = \"Allow CloudTrail to encrypt logs\"     effect    = \"Allow\"     actions   = [\"kms:GenerateDataKey*\"]     resources = [\"*\"]     principals {       type        = \"Service\"       identifiers = [\"cloudtrail.amazonaws.com\"]     }     condition {       test     = \"StringEquals\"       variable = \"AWS:SourceArn\"       values   = [\"arn:aws:cloudtrail:eu-west-1:${data.aws_caller_identity.current.account_id}:trail/cloudtrail-${terraform.workspace}\"]     }     condition {       test     = \"StringLike\"       variable = \"kms:EncryptionContext:aws:cloudtrail:arn\"       values   = [\"arn:aws:cloudtrail:*:${data.aws_caller_identity.current.account_id}:trail/*\"]     }   }   statement {     sid       = \"Allow CloudTrail to describe key\"     effect    = \"Allow\"     actions   = [\"kms:DescribeKey\"]     resources = [\"*\"]     principals {       type        = \"Service\"       identifiers = [\"cloudtrail.amazonaws.com\"]     }   }   statement {     sid    = \"Allow principals in the account to decrypt log files\"     effect = \"Allow\"     actions = [       \"kms:Decrypt\",       \"kms:ReEncryptFrom\"     ]     resources = [\"*\"]     principals {       type        = \"AWS\"       identifiers = [\"*\"]     }     condition {       test     = \"StringEquals\"       variable = \"kms:CallerAccount\"       values   = [\"${data.aws_caller_identity.current.account_id}\"]     }     condition {       test     = \"StringLike\"       variable = \"kms:EncryptionContext:aws:cloudtrail:arn\"       values   = [\"arn:aws:cloudtrail:*:${data.aws_caller_identity.current.account_id}:trail/*\"]     }   }   statement {     sid       = \"Allow alias creation during setup\"     effect    = \"Allow\"     actions   = [\"kms:CreateAlias\"]     resources = [\"*\"]     principals {       type        = \"AWS\"       identifiers = [\"*\"]     }     condition {       test     = \"StringEquals\"       variable = \"kms:CallerAccount\"       values   = [\"${data.aws_caller_identity.current.account_id}\"]     }     condition {       test     = \"StringEquals\"       variable = \"kms:ViaService\"       values   = [\"ec2.eu-west-1.amazonaws.com\"]     }   }   statement {     sid    = \"Enable cross account log decryption\"     effect = \"Allow\"     actions = [       \"kms:Decrypt\",       \"kms:ReEncryptFrom\"     ]     resources = [\"*\"]     principals {       type        = \"AWS\"       identifiers = [\"*\"]     }     condition {       test     = \"StringEquals\"       variable = \"kms:CallerAccount\"       values   = [\"${data.aws_caller_identity.current.account_id}\"]     }   } }Enter fullscreen modeExit fullscreen modeStep 3:terraform planEnter fullscreen modeExit fullscreen modeA successful plan should look like the image below:Step 4:terraform applyEnter fullscreen modeExit fullscreen modeA successful plan should look like the image below:ConclusionYou have successfully created an AWS CloudTrail service.Do you have any question? Please send it my way. Kindly follow me onLinkedIn."}
{"title": "One simple trick to win Hackathons", "published_at": 1711845371, "tags": [], "user": "Mick Jacobsson", "url": "https://dev.to/aws-builders/one-simple-trick-to-win-hackathons-420", "details": "Everyone has been approaching the AWS PartyRock Hackathon wrong in my opinion. Why try and think of an idea and create a new AI app when you can just use AI to think of the winning idea and write a summary to enter? You can't lose!AWS PartyRockAWS is recently hosted ahackathon(ended on March 11, 2024), where participants were challenged to create an AI app using the PartyRock platform. Essentially, this involves leveraging AWS BedRock technology and offers a fun opportunity to explore AWS's generative AI services.If you're familiar with hackathons, this one follows the same format. Contestants will compete in various categories and have the chance to win CASH prizes. So, if a hackathon has a specific format, can we use AI to help us win?Hackathon Hack a.k.a HackaThongMy idea is straightforward: you give the user inputs that I'll use as context for the generative AI to create a winning suggestion. Instead of making this specific to just the AWS PartyRock competition, I'll make it more generic so that it can be reused.Why HackaThong? I'm Australian, and we refer to rubber-style casual footwear, similar to sandals, as thongs or pluggers.What do hackathons have in common?Summary of the hackathonTerms and conditionsCriteria for judgingExamples (often provided to inspire creativity)This should give enough context for our AI to offer suggestions, but what about the output?A winning ideaA catchy, unique nameSome artwork to help visualize the ideaA written summary describing the appAn application to the competition that we can use as a starting pointTime to build a thingI'm going to quickly explain how I built this with PartyRock. But first, I want to recommend a few articles that cover this topic in more detail (and better) than I will:https://community.aws/tags/partyrock-hackathonhttps://community.aws/content/2bpGEn7TeXH4XkL0Z4j4eSumK0v/master-prompt-engineering-elevate-ai-with-claude-partyrockI'm going to start with an empty app and just start adding widgets:Instead of starting with an empty app you can write a description and PartyRock will have a crack at creating the app you might need.Based on what we know, we need a lot of input from the user. So, for the first widgets, we'll create severaluser inputtypes:Hopefully this is straightforward, title, placeholders etc.The first couple of user input widgets are straightforward: they're simple text fields where the user can provide context about what the AI needs to do. Very little setup is neededjust name it and hit save.This is where the fun starts. The next few widgets will use the inputs from above to perform the AI magic. We then create an AI-Powered widget and link it to the previous input widgets.A few things to note:You can change the model based on your requirements, there is a brief description of the modelshereYou design your promptFrom this point, we continue the process, creating more widgets until the app looks something like what we intended to develop.Let's talk about the failsThe AI kept suggesting the same thing. For example, it would suggest a cooking app every time. I had to reword the prompt to resolve this, and even so, it can provide things that are pretty similar.There is a word limit. Not too surprising. The terms and conditions are super long (expected), so there is a point in the T&Cs that I truncate to accept most of the content.Initially, regenerating ideas was a bit dumb because I'd simply update the inputs. But,  there is a little refresh button on the widget to resolve this.Try it outYou can try out my app onAWS Party Rock, it's a little bit of fun. Your mileage may vary on how useful it is to win a hackathon with, but hopefully, you can see some potential in generative AI."}
{"title": "Using YAML anchors and aliases in a SAM template", "published_at": 1711843200, "tags": ["aws", "serverless", "sam"], "user": "Andres Moreno", "url": "https://dev.to/aws-builders/using-yaml-anchors-and-aliases-in-a-sam-template-5g44", "details": "Last month I wrote a post aboutgetting rid of Lambda Layers by using ESBuild. What I quickly learned is that theMetadataattribute has to be copied and pasted for EVERY Lambda function in your stack. I tried using theGlobalsection in the SAM template and it turns out it's not supported. I started thinking about how I could reuse the same configuration across my template and found that YAML already has a functionality that does this calledYAML Anchors and Aliases. In this post I will go through what YAML aliases and anchors are and how we can use them in SAM.What are YAML anchors and aliases?You can think ofanchorsas assigning a value to a variable to be used in other places. The way you define ananchoris by adding an&on an entity with the name of the anchor.personName:type:objectproperties:&person-name-propertiesfirstName:type:stringlastName:type:stringEnter fullscreen modeExit fullscreen modeIn the example above we have created an anchor for thepropertiesattribute of thepersonNameschema with the nameperson-name-properties. Later in the file you can reference the anchor with an alias by using the*symbol and the anchor name.person:type:objectproperties:name:type:objectproperties:*person-name-propertiesEnter fullscreen modeExit fullscreen modeWhen the YAML is proccessed thepersonobject will look like this.person:type:objectproperties:name:type:objectproperties:firstName:type:stringlastName:type:stringEnter fullscreen modeExit fullscreen modeYAML allows you to override and/or extend properties to be able to get it into the correct structure. To update the name object to have aminLengthfor thefirstNameand include a new attribute calledmiddleNamewe would do something like this.person:type:objectproperties:name:type:objectproperties:<<:*person-name-propertiesfirstName:type:stringminLength:3middleName:type:stringEnter fullscreen modeExit fullscreen modeIf you look at the example above we are now using the<<attribute which essentiallydeconstructswhats defined in the anchor and allows you to overwrite something by adding the same attribute or add new ones in line. If we process the YAML above we would get the result below.person:type:objectproperties:name:type:objectproperties:firstName:type:stringminLength:3lastName:type:stringmiddleName:type:stringEnter fullscreen modeExit fullscreen modeWith this functionality we can define our baseMetadataattributes for ESBuild and use them for our template right? .... right?Why are YAML anchors not as straightforward with SAM?SAM is strict in what it accepts in thetemplate.yamlwhen doing a deployment, you will not see these errors when doingsam buildthough, so be careful to make sure your template is actually deployable. Here is an example where I'm defining theesbuildconfig metadata at the root of my template and consuming it in a functionAWSTemplateFormatVersion:2010-09-09Description:Layerless ESBuild ExampleTransform:-AWS::Serverless-2016-10-31esbuild:&esbuildBuildMethod:esbuildBuildProperties:Format:esmMinify:falseOutExtension:-.js=.mjsTarget:es2020Sourcemap:falseEntryPoints:-index.mjsBanner:-js=import { createRequire } from 'module'; const require = createRequire(import.meta.url);Resources:EchoFunction:Type:AWS::Serverless::FunctionProperties:CodeUri:src/functions/echoRuntime:nodejs20.xHandler:index.handlerMetadata:*esbuildEnter fullscreen modeExit fullscreen modeThe built template after runningsam buildlooks like thisAWSTemplateFormatVersion:'2010-09-09'Description:Layerless ESBuild ExampleTransform:-AWS::Serverless-2016-10-31esbuild:BuildMethod:esbuildBuildProperties:Format:esmMinify:falseOutExtension:-.js=.mjsTarget:es2020Sourcemap:falseEntryPoints:-index.mjsBanner:-js=import { createRequire } from 'module'; const require = createRequire(import.meta.url);Resources:EchoFunction:Type:AWS::Serverless::FunctionProperties:CodeUri:EchoFunctionRuntime:nodejs20.xHandler:index.handlerMetadata:BuildMethod:esbuildBuildProperties:Banner:-js=import { createRequire } from 'module'; const require = createRequire(import.meta.url);EntryPoints:-index.mjsFormat:esmMinify:falseOutExtension:-.js=.mjsSourcemap:falseTarget:es2020SamResourceId:EchoFunctionEnter fullscreen modeExit fullscreen modeDoesn't look bad right? It looks like the anchor got referenced and placed correctly in my Lambda function.But when we runsam deploywe'll get this error.Status: FAILED. Reason: Invalid template property or properties[esbuild]Enter fullscreen modeExit fullscreen modeIt doesn't like theesbuildproperty we've defined since it's not part of the SAM template schema.Working around the limitationTo avoid getting an error on deployment we can rename the property toMetadatainstead. This is an accepted property in SAM that will build and deploy successfully. Below you can see how I renamedesbuildtoMetadataAWSTemplateFormatVersion:2010-09-09Description:Layerless ESBuild ExampleTransform:-AWS::Serverless-2016-10-31Metadata:&esbuildBuildMethod:esbuildBuildProperties:Format:esmMinify:falseOutExtension:-.js=.mjsTarget:es2020Sourcemap:falseEntryPoints:-index.mjsBanner:-js=import { createRequire } from 'module'; const require = createRequire(import.meta.url);Resources:EchoFunction:Type:AWS::Serverless::FunctionProperties:CodeUri:src/functions/echoRuntime:nodejs20.xHandler:index.handlerMetadata:*esbuildEnter fullscreen modeExit fullscreen modeThis is great! We can have several Lambda functions that share the same ESBuild configuration. But what happens when we want to define multiple anchors? If we were to add a secondMetadataattribute we will now have invalid YAML since it doesn't accept duplicate properties names at the same level. The funny thing is this actually builds and deploys correctly so if you have no linting in your processes this might work for you. But there is a better way we can define multiple anchors without sacrificing our linting process.Instead of making theMetadataproperty the anchor we can add multiple anchors under theMetadataproperty. In the example below I am going to add a second anchor that contains only theBuildPropertiesand I'll update the base ESBuild config to use it.AWSTemplateFormatVersion:2010-09-09Description:Layerless ESBuild ExampleTransform:-AWS::Serverless-2016-10-31Metadata:esbuild-properties:&esbuild-propertiesFormat:esmMinify:falseOutExtension:-.js=.mjsTarget:es2020Sourcemap:falseEntryPoints:-index.mjsBanner:-js=import { createRequire } from 'module'; const require = createRequire(import.meta.url);esbuild:&esbuildBuildMethod:esbuildBuildProperties:*esbuild-propertiesResources:EchoFunction:Type:AWS::Serverless::FunctionProperties:CodeUri:src/functions/echoRuntime:nodejs20.xHandler:index.handlerMetadata:*esbuildEnter fullscreen modeExit fullscreen modeWe've done it! We can now define reusable YAML objects in our SAM templates. If we wanted to extend or overwrite any properties we can use the<<attribute.Resources:EchoFunction:Type:AWS::Serverless::FunctionProperties:CodeUri:src/functions/echoRuntime:nodejs20.xHandler:index.handlerMetadata:BuildMethod:esbuildBuildProperties:<<:*esbuild-propertiesMinify:trueExternal:-'@aws-sdk/*'Enter fullscreen modeExit fullscreen modeWrap upBy using standard YAML functionality and repurposing the SAMMetadataproperty we were able to successfully setup reusable snippets of YAML for our SAM templates. This allows us to greatly reduce the size of the template we are maintaining, it also reduces the risk of errors by giving standard settings to be used throughout the whole file."}
{"title": "Setting Up a Kubernetes Cluster on AWS EKS With Eksctl and Deploying an App", "published_at": 1711816486, "tags": ["aws", "kubernetes", "eksctl"], "user": "Julia Furst Morgado", "url": "https://dev.to/aws-builders/setting-up-a-kubernetes-cluster-on-aws-eks-with-eksctl-and-deploying-an-app-5595", "details": "It might come as a surprise, but not too long ago, configuring a Kubernetes cluster on Amazon EKS was quite challenging since you had to create all the components manually by yourself. This changed with the introduction ofeksctl, a tool written in Go by the community and sponsored by weaveworks that acts as the official AWS CLI for EKS management.eksctlallows for the creation and administration of clusters through YAML files, while leveraging CloudFormation stacks behind the scenes to handle the required resources. In this blog post you'll see that usingeksctlsimplifies the creation and deployment of Kubernetes clusters on Amazon EKS.InstallationLike other binaries written in Go, installation is super simple:curl --silent --location \"https://github.com/weaveworks/eksctl/releases/latest/download/eksctl_$(uname -s)_amd64.tar.gz\" | tar xz -C /tmp sudo mv /tmp/eksctl /usr/local/bin eksctl versionEnter fullscreen modeExit fullscreen modeHere's a breakdown of what the code snippet does:Downloads and extracts theeksctl binary:Thecurlcommand fetches the latesteksctlbinary from its GitHub releases page. This binary is compressed in a .tar.gz file for efficient transfer.--silent --location: These options ensure that the download process is not only quiet (no progress output) but also follows any redirects, which is crucial for reaching the final download link.The URL contains a dynamic part,$(uname -s)_amd64, which automatically adjusts the download link based on your operating system (uname -s outputs your OS name, such as Linux or Darwin) and assumes an amd64 architecture.The downloaded file is piped (|) into thetarcommand, which extracts the binary into the/tmpdirectory. This is a temporary location, safe for staging files before moving them to a more permanent location.Moves the binary to/usr/local/bin:Thesudo mv /tmp/eksctl /usr/local/bincommand moves the extractedeksctlbinary from/tmpto/usr/local/bin. This directory is commonly used for manually installed binaries and is typically included in the system's PATH. Placingeksctlhere allows you to run it from any terminal without specifying its full path.Usingsudois necessary because/usr/local/binis a system directory, and modifying its contents requires administrative privileges.Verifies the installationFinally, runningeksctl versionchecks the installed version ofeksctl. This not only confirms thateksctlis correctly installed and accessible but also lets you know the exact version you have. It's a good practice to verify installations to ensure everything is set up as expected.Configuring an AWS accountDo not use your root account for cluster creation. In fact, avoid using your root account for anything other than creating other accounts or managing very specific AWS resources.If it's your personal account, create another user. Now, if it's a corporate account, I suggest enabling AWS Organizations.Create a new user in IAM with the following policies attached to it:AmazonEC2FullAccessAWSCloudFormationFullAccessEksFullAccessIamLimitedAccessThe first two are AWS-managed policies, so you don't need to create them. The last two will have the following content:EksAllAccess{     \"Version\": \"2012-10-17\",     \"Statement\": [         {             \"Effect\": \"Allow\",             \"Action\": \"eks:*\",             \"Resource\": \"*\"         },         {             \"Action\": [                 \"ssm:GetParameter\",                 \"ssm:GetParameters\"             ],             \"Resource\": [                 \"arn:aws:ssm:*:<account_id>:parameter/aws/*\",                 \"arn:aws:ssm:*::parameter/aws/*\"             ],             \"Effect\": \"Allow\"         },         {              \"Action\": [                \"kms:CreateGrant\",                \"kms:DescribeKey\"              ],              \"Resource\": \"*\",              \"Effect\": \"Allow\"         },         {              \"Action\": [                \"logs:PutRetentionPolicy\"              ],              \"Resource\": \"*\",              \"Effect\": \"Allow\"         }             ] }Enter fullscreen modeExit fullscreen modeIamLimitedAccess{     \"Version\": \"2012-10-17\",     \"Statement\": [         {             \"Effect\": \"Allow\",             \"Action\": [                 \"iam:CreateInstanceProfile\",                 \"iam:DeleteInstanceProfile\",                 \"iam:GetInstanceProfile\",                 \"iam:RemoveRoleFromInstanceProfile\",                 \"iam:GetRole\",                 \"iam:CreateRole\",                 \"iam:DeleteRole\",                 \"iam:AttachRolePolicy\",                 \"iam:PutRolePolicy\",                 \"iam:ListInstanceProfiles\",                 \"iam:AddRoleToInstanceProfile\",                 \"iam:ListInstanceProfilesForRole\",                 \"iam:PassRole\",                 \"iam:DetachRolePolicy\",                 \"iam:DeleteRolePolicy\",                 \"iam:GetRolePolicy\",                 \"iam:GetOpenIDConnectProvider\",                 \"iam:CreateOpenIDConnectProvider\",                 \"iam:DeleteOpenIDConnectProvider\",                 \"iam:TagOpenIDConnectProvider\",                 \"iam:ListAttachedRolePolicies\",                 \"iam:TagRole\",                 \"iam:GetPolicy\",                 \"iam:CreatePolicy\",                 \"iam:DeletePolicy\",                 \"iam:ListPolicyVersions\"             ],             \"Resource\": [                 \"arn:aws:iam::<account_id>:instance-profile/eksctl-*\",                 \"arn:aws:iam::<account_id>:role/eksctl-*\",                 \"arn:aws:iam::<account_id>:policy/eksctl-*\",                 \"arn:aws:iam::<account_id>:oidc-provider/*\",                 \"arn:aws:iam::<account_id>:role/aws-service-role/eks-nodegroup.amazonaws.com/AWSServiceRoleForAmazonEKSNodegroup\",                 \"arn:aws:iam::<account_id>:role/eksctl-managed-*\"             ]         },         {             \"Effect\": \"Allow\",             \"Action\": [                 \"iam:GetRole\"             ],             \"Resource\": [                 \"arn:aws:iam::<account_id>:role/*\"             ]         },         {             \"Effect\": \"Allow\",             \"Action\": [                 \"iam:CreateServiceLinkedRole\"             ],             \"Resource\": \"*\",             \"Condition\": {                 \"StringEquals\": {                     \"iam:AWSServiceName\": [                         \"eks.amazonaws.com\",                         \"eks-nodegroup.amazonaws.com\",                         \"eks-fargate.amazonaws.com\"                     ]                 }             }         }     ] }Enter fullscreen modeExit fullscreen modeReplace  with your AWS account ID.Now generate credentials for the user and keep the access key ID and secret key.eksctlrequires the AWS CLI to be installed. If you don't have it installed, run:curl \"https://awscli.amazonaws.com/awscli-exe-linux-x86_64.zip\" -o \"awscliv2.zip\" unzip awscliv2.zip sudo ./aws/install aws configureEnter fullscreen modeExit fullscreen modeYou'll be prompted for details like the default AWS region (e.g., us-east-1), access key ID, and secret key to create the default profile. Now, if you already have the AWS CLI installed and configured, you can manually create the new profile by editing the~/.aws/credentialsfile.Attention: when you have multiple profiles within your~/.aws/credentialsfile, specify which one you want to use by setting the value of theAWS_PROFILEvariable with the commandexport AWS_PROFILE=my_profileand replacingmy_profilewith the name of the AWS profile you want to use. This ensures that any subsequent AWS CLI commands will use the credentials and configuration associated with the specified profile.Creating the clusterBy default,eksctlwill create the cluster in a separate VPC usingm5.largeinstances. This has some implications:By default quota, you can have a maximum of five VPCs per account;If you're using a study, lab, or test environment, this type of instance can be considered somewhat expensive;If you need the applications in the cluster to communicate with AWS resources - an instance in RDS, for example - located in another VPC, VPC Peering configuration and routes are necessary for this to happen.Our cluster will have the following characteristics:It will be calleddemo-eksand will be located inus-east-1(Northern Virginia);Four subnets will be created in the new VPC, two public and two private. By default,eksctlallocates the cluster nodes in the private subnets;A nodegroup calledng-01will be created with an auto scaling group, with a minimum of one instance and a maximum of 5 of typet3.medium;IAM policies forexternal-dns, auto-scaling, cert-manager, ebs, andefs. We'll talk more about them shortly.Save the file below asdemo-eks.yamland then executeeksctl create cluster -f demo-eks.yaml --kubeconfig=demo-eks-kubeconfigand let the magic happen.eksctlwill generate the stack (or stacks) within CloudFormation and create all the necessary resources, which will take about fifteen to twenty minutes (yes, it's a bit slow) and voil\u00e0, you'll have a Kubernetes cluster for use in AWS EKS, with credentials for access to it in thedemo-eks-kubeconfig.yamlfile.--- apiVersion: eksctl.io/v1alpha5 kind: ClusterConfig metadata:   name: demo-eks   region: us-east-1  managedNodeGroups:   - name: ng-01     instanceType: t3.medium     desiredCapacity: 2     minSize: 1     maxSize: 5     iam:       withAddonPolicies:         autoScaler: true         externalDNS: true         certManager: true         ebs: true         efs: trueEnter fullscreen modeExit fullscreen modeImportant: If the--kubeconfigoption is omitted, eksctl will generate the access file in~/.kube/config, which can be a problem if you already have other clusters configured. If that's the case, you can create a directory called~/.kube/configsto store the files and switch between them by setting theKUBECONFIGvariable pointing to the desired file path (e.g.,export KUBECONFIG=${HOME}/.kube/configs/demo-eks-kubeconfig). This step is essential forkubectlto be able to connect to the cluster.If you're still getting an error suggesting thatkubectlis trying to connect to a Kubernetes cluster on your local machine (localhost, IP address 127.0.0.1) instead of connecting to your Amazon EKS cluster, run the commandexport KUBECONFIG=demo-eks-kubeconfig.Helm InstallationYou've probably heard of Helm, but if not, don't worry. Helm is, in essence, a package manager for Kubernetes. These packages are provided in the form of charts, and with Helm, we can create manifests for our applications in a more dynamic, organized, parameterizable way, and then host them in repositories for deployment.You can quickly install it usingcurlandbash:curl https://raw.githubusercontent.com/helm/helm/main/scripts/get-helm-3 | bashEnter fullscreen modeExit fullscreen modeHere's what the command does:curl https://raw.githubusercontent.com/helm/helm/main/scripts/get-helm-3: This part of the command usescurl, a command-line tool for transferring data using various network protocols. Here, it's used to download the installation script for Helm 3 from its official GitHub repository. The URL points to the raw version of theget-helm-3script, which is intended to be executed directly.| bash: This pipe (|) takes the output of thecurlcommand (which is the Helm 3 installation script) and feeds it directly intobash, the Bourne Again SHell. Running a script in this manner executes the script's contents.Here's what happens when you run the command:Theget-helm-3script is fetched from the Helm GitHub repository usingcurl.The script is then immediately executed in your shell (bash) without needing to be saved to a file on your disk.The script performs several actions to install Helm 3:It checks your system to ensure it meets the prerequisites for installing Helm.It determines the latest version of Helm 3 if not specified.It downloads the correct Helm 3 binary for your operating system and architecture.It places the Helm binary in a location on your system where executable files are typically stored, making it accessible from anywhere in your terminal.It might also perform some additional setup tasks, such as setting up autocomplete features for Helm commands.Or, if you're using a Debian-like distribution and want to use a package you can install it with the following commands:curl https://baltocdn.com/helm/signing.asc | gpg --dearmor | sudo tee /usr/share/keyrings/helm.gpg > /dev/null sudo apt-get install apt-transport-https --yes echo \"deb [arch=$(dpkg --print-architecture) signed-by=/usr/share/keyrings/helm.gpg] https://baltocdn.com/helm/stable/debian/ all main\" | sudo tee /etc/apt/sources.list.d/helm-stable-debian.list sudo apt-get update sudo apt-get install helmEnter fullscreen modeExit fullscreen modeAdd-ons InstallationMetrics ServerTheMetrics Serveris a necessary resource for using HPAs (Horizontal Pod Autoscalers). HPAs are extremely useful when we want to scale our applications horizontally when they need more resources.helm repo add metrics-server https://kubernetes-sigs.github.io/metrics-server/ helm upgrade --install --create-namespace -n metrics-server metrics-server metrics-server/metrics-serverEnter fullscreen modeExit fullscreen modePrometheusPrometheusis a monitoring solution. If you use Lens or another dashboard for monitoring your clusters, it will provide CPU, memory, network, and disk usage graphs for the cluster, nodes, controllers, and pods. Prometheus is commonly used alongside Grafana.helm repo add bitnami https://charts.bitnami.com/bitnami helm install --create-namespace -n kube-prometheus prometheus bitnami/kube-prometheusEnter fullscreen modeExit fullscreen modeExternalDNSExternalDNSsynchronizes Kubernetes resources with external DNS records hosted on providers like AWS Route 53, Google Cloud DNS, AzureDNS, etc. With it, we don't need to manually create DNS entries every time we deploy a new application to our cluster. Cool, right?helm install external-dns stable/external-dns \\   --create-namespace \\   --namespace external-dns \\   --set provider=aws \\   --set aws.zoneType=public \\   --set txtOwnerId=<ZONE_ID>   --set policy=sync   --set registry=txt   --set interval=20s   --set domainFilters[0]=<DOMAIN>Enter fullscreen modeExit fullscreen modeReplace<ZONE_ID>with your AWS Route 53 zone ID and<DOMAIN>with your domain in the formatdomain.com.cert-managercert-manageris a CRD (Custom Resource Definition) that dynamically generates TLS/SSL certificates for our applications usingLet's Encrypt(although it also supports other issuers).helm repo add bitnami https://charts.bitnami.com/bitnami helm install --create-namespace -n cert-manager cert-manager bitnami/cert-manager \\   --create-namespace \\   --namespace cert-manager \\   --set installCRDs=trueEnter fullscreen modeExit fullscreen modeWith the chart installed, it's necessary to create two CRDs, each representing a different issuer from Let's Encrypt, one for production and one for staging. The difference between them is the request limit we can make. In a test environment, prefer to use the staging environment.apiVersion: cert-manager.io/v1 kind: ClusterIssuer metadata:   name: letsencrypt-prod spec:   acme:     server: https://acme-v02.api.letsencrypt.org/directory     email: <meu_email>     privateKeySecretRef:       name: letsencrypt-prod     solvers:       - http01:           ingress:             class: nginxEnter fullscreen modeExit fullscreen modeapiVersion: cert-manager.io/v1 kind: ClusterIssuer metadata:   name: letsencrypt-staging spec:   acme:     server: https://acme-staging-v02.api.letsencrypt.org/directory     email: <meu_email>     privateKeySecretRef:       name: letsencrypt-prod     solvers:       - http01:           ingress:             class: nginxEnter fullscreen modeExit fullscreen modeNote: Replace<my_email>with your email address. If you're using an Ingress other than Nginx, you need to change the manifests above by setting the appropriate class.Run the commandkubectl apply -f cert-manager-staging.yaml -f cert-manager-prod.yamlto create them in the cluster.NGINX Ingress ControllerWe allow access to our applications through theNGINX Ingress Controller. You can think of an Ingress as a reverse proxy.helm upgrade --install ingress-nginx ingress-nginx \\   --repo https://kubernetes.github.io/ingress-nginx \\   --namespace ingress-nginx \\   --create-namespaceEnter fullscreen modeExit fullscreen modeAuto Scaling GroupOne of the coolest features of cloud computing iselasticity. Imagine that throughout the year, we have a relatively constant demand, but at certain times - like Black Friday - it can increase considerably, and soon you'll need more nodes within your nodegroups to meet the needs of your pods. So that we don't have to do this manually, let's add automatic scaling functionality to our nodegroup.helm repo add autoscaler https://kubernetes.github.io/autoscaler helm install --create-namespace -n cluster-autoscaler autoscaler/cluster-autoscaler \\   --set 'autoDiscovery.clusterName=<cluster_name>'Enter fullscreen modeExit fullscreen modeNote: Replace<cluster_name>with the name of your cluster (in this case,demo-eks).To test, after deploying any application, increase the number of replicas to a number not met by the resources available in your nodegroup. Thecluster-autoscalerwill provide new nodes up to the maximum number set in the nodegroup. When the number of pods consumes less than 50% of the resources of a node for more than 10 minutes, this node will be removed from the nodegroup. Cool, right?Deploying an ApplicationWith our cluster set up and the add-ons installed, it's time to launch a test application. Below are the manifests within a single file namedhello-kubernetes.yaml:apiVersion: v1 kind: Namespace metadata:   name: hello-kubernetes --- apiVersion: apps/v1 kind: Deployment metadata:   namespace: hello-kubernetes   name: hello-kubernetes spec:   replicas: 2   selector:     matchLabels:       app: hello-kubernetes   template:     metadata:       labels:         app: hello-kubernetes     spec:       containers:       - name: hello-kubernetes         image: paulbouwer/hello-kubernetes:1.5         ports:         - containerPort: 8080         resources:           requests:             memory: \"128Mi\"             cpu: \"250m\"           limits:             memory: \"256Mi\"             cpu: \"500m\" --- apiVersion: v1 kind: Service metadata:   namespace: hello-kubernetes   name: hello-kubernetes spec:   type: ClusterIP   selector:     app: hello-kubernetes   ports:   - port: 80     targetPort: 8080 --- apiVersion: networking.k8s.io/v1 kind: Ingress metadata:   namespace: hello-kubernetes   name: hello-kubernetes-ingress   annotations:     nginx.ingress.kubernetes.io/ingress-class: nginx     cert-manager.io/cluster-issuer: letsencrypt-prod     external-dns.alpha.kubernetes.io/hostname: <app.DOMAIN> spec:   tls:     - hosts:         - <app.DOMAIN>       secretName: <app.DOMAIN>   rules:     - host: <app.DOMAIN>       http:         paths:         - path: /           pathType: Prefix           backend:             service:               name: hello-kubernetes               port:                  number: 80Enter fullscreen modeExit fullscreen modeNote: Replace  with the domain you want to use for your application, something like app.juliaK8s.net.Once you've created the manifest and saved it, deploy the YAML file usingkubectl apply:kubectl apply -f hello-kubernetes.yamlEnter fullscreen modeExit fullscreen modeThis command will create the resources defined in the YAML file (Namespace, Deployment, Service, Ingress) in your Kubernetes cluster.After applying the YAML file, you can verify that the resources have been created successfully by running the commandkubectl get all -n hello-kubernetes.If everything goes well, here's what should happen:Entries will be created in your DNS zone inside AWS Route 53 according to what was configured in your ingress and the settings of your ExternalDNS, and within a few minutes, you will be able to access your application by that name. You can monitor the propagation using the dig tool;A certificate will be automatically generated for your application. This can take some time for new applications;We allocate two replicas for the application, and here's a very important caveat: Kubernetes will by default perform round-robin load balancing between the pods. In most of today's web applications, we work with sessions and, as a result, there may be scenarios where a user authenticates on one pod, and the next request, is redirected to another where the session does not exist. As a consequence, strange behaviors are presented to the user, such as redirection to the login screen, \"Unauthorized\" messages, intermittent access, etc. To prevent this, we usually use some service likeRedis, memcached(which may or may not be on AWS ElastiCache), or the sticky-sessions feature, where a user is consistently sent to the same pod.Deleting ClusterIf you want to delete the cluster and resources you've just created, run the following command:eksctl delete cluster --name demo-eks --region us-east-1Enter fullscreen modeExit fullscreen modeConclusionKubernetes is an extremely extensive and complex subject with a myriad of possibilities. I presented some of them here that can be used not only in AWS EKS but also in on-premises installations (except for thecluster-autoscaler, unless you are usingOpenStackin your company). There is also a series of security improvements that can be made to the cluster setup in this article. I hope you enjoyed it!ReferenceseksctlGetting started with Amazon EKS \u2013 eksctlIf you liked this article, follow me onTwitter(where I share my tech journey daily), connect with me onLinkedIn, check out myIG, and make sure to subscribe to myYoutubechannel for more amazing content!!"}
{"title": "Exploring AWS Serverless Deployments with CDK v2: From RSS to X Posts - Part 1 of the Odyssey", "published_at": 1711805400, "tags": ["cdk", "lambda", "dynamodb", "python"], "user": "Adrian Mudzwiti ", "url": "https://dev.to/aws-builders/exploring-aws-serverless-deployments-with-cdk-v2-from-rss-to-x-posts-part-1-of-the-odyssey-3j8o", "details": "Weekend projects often come and go, but on the rare occasion, one does stand out that is truly worth documenting.In this multi-part blog series I invite you to join me on a journey into the realm of AWS serverless architecture deployments with CDK v2.Together, we'll embark on a fascinating exploration - from harvesting RSS feeds to crafting X Posts.Throughout this series, we'll explore how to set up a serverless solution that automatically gathers information from an RSS feed, extracts the important details from XML and saves them efficiently in a DynamoDB table.Plus, we'll see how new entries in our DynamoDB table invokes a Lambda function to create X Posts using DynamoDB Streams.In my professional sphere, I predominantly rely on the Azure SDK for Python, with occasional use of Terraform, while my personal projects frequently entail Pulumi, Terraform and once in a blue moon CloudFormation (Shocking, I know). However, for this specific endeavor, we are utilizing the AWS CDK as our conduit.AWS CDK is an Infrastructure as Code tool that empowers us to define and manage resources with code, offering a fresh perspective on serverless architecture deployments.Before we dive in, lets take a moment to visualize the architecture we are about to construct:Pre-requisites:AWS CLIAWS CDK v2PythonDockerCreating a CDK ProjectTo get started with creating a new CDK project, you'll need to enter the below commands:mkdir rss-lambda-ddb-socialshare cd rss-lambda-ddb-socialshare cdk init --language pythonEnter fullscreen modeExit fullscreen modecdk initOnce thecdk initcommand has executed, you'll need to activate the virtual environment with one of the below commands depending on your OS:source .venv/bin/activateEnter fullscreen modeExit fullscreen mode.venv\\Scripts\\activate.batEnter fullscreen modeExit fullscreen modeNext you will need to install Python packages and dependencies required for our stack and constructs, run the below command:pip install -r requirements.txtEnter fullscreen modeExit fullscreen modeFeel free to explore the project directory. You'll notice folders and files have been created for you.\u2523 \ud83d\udcc2.venv \u2503 \u2523 \ud83d\udcc2bin \u2503 \u2523 \ud83d\udcc2include \u2503 \u2523 \ud83d\udcc2lib \u2503 \u2517 \ud83d\udcdcpyvenv.cfg \u2523 \ud83d\udcc2rss_lambda_ddb_socialshare \u2503 \u2523 \ud83d\udcdc__init__.py \u2503 \u2517 \ud83d\udcdcrss_lambda_ddb_socialshare_stack.py \u2523 \ud83d\udcc2tests \u2503 \u2523 \ud83d\udcc2unit \u2503 \u2503 \u2523 \ud83d\udcdc__init__.py \u2503 \u2503 \u2517 \ud83d\udcdctest_rss_lambda_ddb_socialshare_stack.py \u2503 \u2517 \ud83d\udcdc__init__.py \u2523 \ud83d\udcdc.gitignore \u2523 \ud83d\udcdcapp.py \u2523 \ud83d\udcdccdk.json \u2523 \ud83d\udcdcREADME.md \u2523 \ud83d\udcdcrequirements-dev.txt \u2523 \ud83d\udcdcrequirements.txt \u2517 \ud83d\udcdcsource.batEnter fullscreen modeExit fullscreen modeThe app.py file is your app's entry point. The code in this file instantiates an instance of theRssLambdaDdbSocialshareStackclass from therss_lambda_ddb_socialshare/rss_lambda_ddb_socialshare_stack.pyfile.The most important file that we care about is therss_lambda_ddb_socialshare/rss_lambda_ddb_socialshare_stack.pyfile, this is where most of the code to define resources will be added.We will modify the stack to create the below resources:DynamoDB Table2 Lambda FunctionsEventBridge ruleThe first construct in our stack that we will add is a DynamoDB Table, you'll need to importRemovalPolicyandaws_dynamodb as dynamodb. Then modify the stack, I have included code snippets that you can use to overwrite what is currently in therss_lambda_ddb_socialshare/rss_lambda_ddb_socialshare_stack.pyfile.The removal policy we have defined in theDynamoDB Tableconstruct is destructive, for production environments one would use theRETAINattribute, e.g.removal_policy=RemovalPolicy.RETAINcdk synthThe next command to execute iscdk synth, this will generate a CloudFormation template for our current stack and be used for deployment.cdk bootstrapOnce synthesized, we need to prepare an environment (target AWS account and AWS Region) for deployment. This is done using thecdk bootstrapcommand. This will provision an S3 bucket to store files as well as create IAM roles that are required to perform deployments. This command only needs to be run once.cdk bootsrap aws://{AWS-ACCOUNT-NUMBER}/{REGION}Enter fullscreen modeExit fullscreen modecdk deployThecdk deploycommand will deploy our stack to AWS.We can verify the deployment was successful by viewing the terminal output, alternatively log into theAWS Management Console.ConclusionThat concludesPart 1, in an upcoming blog post, we will learn how to create constructs for Lambda functions, granting permissions for constructs within the stack and adding the actual code for our Lambda functions that performs the magic.ResourcesStacksConstructsRemovalPolicy"}
{"title": "DevOps with Guruu | Chapter 15 : Host Any Static Website with S3", "published_at": 1711757681, "tags": ["webdev", "devops", "aws"], "user": "Hoang Guruu", "url": "https://dev.to/aws-builders/devops-with-guruu-chapter-15-host-any-static-website-with-s3-gap", "details": "DevOps with Guruu | Chapter 15 : Host Any Static Website with S3Join me on this journey to mastering DevOps. Let's get hands-on and build a strong foundation together! Don't forget to like, share, and subscribe for more exciting content. Happy coding, future DevOps!\""}
{"title": "Invalid Grant error with AWS IAM Identity Center", "published_at": 1711702800, "tags": ["aws", "awscloud", "awsiam", "awssso"], "user": "Rich", "url": "https://dev.to/aws-builders/invalid-grant-error-with-aws-iam-identity-center-35fl", "details": "Recently I was setting up a new computer which involved configuring the AWS CLI to use IAM Identity Center (formerly AWS SSO) to access my accounts. Normally this is a prety straight forward proposition. After runningaws configure ssocommand you need to provide four pieces of information:Session NameStart URLRegionRegistration ScopesAWS then authenticates you, you select your account, answer some more questions and it's done.This time I keep getting an invalid_grant error after I authenticated myself.The problem and solution turned out to be really simple. I selected the wrong region for IAM Identity Center. In my defence I mostly work with IAM Identity Center in my closest region but this was an older account and it was setup in a different region. Once I had the correct region everything worked correctly."}
{"title": "DevOps with Guruu | Chapter 14 : Automated server shutdown and Slack messaging with AWS Lambda", "published_at": 1711672005, "tags": ["webdev", "devops", "aws", "automation"], "user": "Hoang Guruu", "url": "https://dev.to/aws-builders/devops-with-guruu-chapter-14-automated-server-shutdown-and-slack-messaging-with-aws-lambda-5b92", "details": "DevOps with Guruu | Chapter 14 : Automated server shutdown and Slack messaging with AWS Lambda0:00 Introduction2:00 Create VPC4:00 Create Security Group5:35 Create EC2 instance6:35 Incoming Web-hooks slack12:10 Create Tag for Instance13:20 Create Role for Lambda15:30 Create Lambda Function26:30 Check ResultClean up resourcesJoin me on this journey to mastering DevOps. Let's get hands-on and build a strong foundation together! Don't forget to like, share, and subscribe for more exciting content. Happy coding, future DevOps!\""}
{"title": "Composite Alarms in AWS Cloud: Advanced Notification System", "published_at": 1711658050, "tags": ["aws", "cloud", "monitoring", "devops"], "user": "Wojciech Lepczy\u0144ski", "url": "https://dev.to/aws-builders/composite-alarms-in-aws-cloud-advanced-notification-system-2omn", "details": "Monitoring the state and performance of various resources is a crucial element in maintaining stability and efficiency within IT systems. One of the most powerful monitoring tools inAWS cloudis theCloudWatchservice, which enables the collection and monitoring ofmetrics and logsfrom various AWS services and resources.Within the CloudWatch service, there exists the capability to create \"composite alarms,\" allowing the consolidation of multiple alarms into one logical expression. This enables the creation of more complex monitoring conditions, significantly enhancing the flexibility and capabilities of the monitoring system within AWS cloud.Consolidating Alarms into Composite AlarmsA composite alarm in AWS CloudWatch allows for the combination of alarms from other metrics or resources into one logical expression. This means that we can specify a condition that will be triggered when the conditions of all defined alarms within it are met. We can also apply different logical operators such asAND, OR, or NOT, providing us with a wide range of options for defining advanced monitoring rules.Operators Available for Use:AND: This logical operator triggers the composite alarm when all the defined alarms within it are active.OR: The OR operator activates the composite alarm when at least one of the defined alarms within it is active.NOT: The NOT operator allows for the definition of negation, activating the composite alarm when the defined alarm is not active.How Can This Be Helpful?Creating composite alarms in AWS CloudWatch can be incredibly helpful in monitoring complex infrastructures and applications. It allows for the definition of more advanced alarm rules, which can take into account multiple different factors simultaneously. For example, we can create a composite alarm that triggers only when CPU usage exceeds a certain threshold and there is also an issue with service availability.ALARM(RAMUtilizationTooHigh) AND ALARM(CPUUtilizationTooHigh)The expression specifies that the composite alarm goes into ALARM only if RAMUtilizationTooHigh and CPUUtilizationTooHighare in ALARM.(ALARM(RAMUtilizationTooHigh) OR ALARM(CPUUtilizationTooHigh)) AND OK(NetworkOutTooHigh)The expression specifies that the composite alarm goes into ALARM if (RAMUtilizationTooHigh) or (CPUUtilizationTooHigh) is in ALARM and (NetworkOutTooHigh) is in OK. This is an example of a composite alarm that reduces alarm noise by not sending you notifications when either of the underlying alarms aren\u2019t in ALARM while a network issue is occurring.Utilization in Complex Notification SystemsComposite alarms are invaluable in building advanced notification systems. With them, we can define more intelligent rules that notify about issues only when they are essential for the stability or performance of the system. For instance, we can configure a notification system to send an alert only when there is both high resource consumption and a decrease in application performance.SummaryComposite alarms in AWS CloudWatch are powerful tools that allow for the consolidation of multiple alarms into one logical expression. They enable the creation of more advanced monitoring rules, significantly increasing the flexibility and effectiveness of monitoring systems within the AWS cloud. Utilizing composite alarms is crucial for building complex notification systems that inform about significant issues in infrastructure or applications only when necessary.Additional Resource: YouTube ChannelFor a practical demonstration of creating simple composite alarms consisting of multiple CloudWatch alarms, you can visit myYouTube channel. There, you'll find a video describing this topic and see how the creation of composite alarms looks in a real cloud environment.AWS Composite Alarm | CloudWatch 2024 - YouTubeWelcome on my YouTube channel, my name is Wojciech Lepczy\u0144ski and today I will be talking about Composite Alarm in AWS Cloud! In today's world, monitoring an...youtube.comBlog:https://lepczynski.it/enAWS documentation:https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/AlarmThatSendsEmail.html"}
{"title": "Solving AWS Network Puzzles with Mathematics - Part 1", "published_at": 1711656333, "tags": ["security", "network", "aws", "formalmethods"], "user": "Cheshire Cat", "url": "https://dev.to/aws-builders/solving-aws-network-puzzles-with-mathematics-part-1-294n", "details": "IntroductionAs engineers utilising AWS, many readers have likely encountered network issues at some point. These situations may include instances where expected communication between EC2 Instances fails to reach its destination or, conversely, when communication that should be blocked succeeds in reaching its destination. In such cases, one may be at a loss regarding modifying the numerous, complexly related Security Groups. In these moments, it is an undeniable fact that an expert's advanced knowledge and experience can be invaluable in solving the puzzle. However, are these the only tools at our disposal?This article explores a solution that relies on established mathematical theories, an additional thought process beyond one's brain. Specifically, we aim to understand the research paper\"Reachability Analysis for AWS-Based Networks\"published by Backes et al. in 2019 and the subsequently implemented feature,VPC Reachability AnalyzerandVPC Network Access Analyzer.However, this paper is highly specialised, and understanding it without prior knowledge can be challenging. This series of articles will be divided into three parts to provide a comprehensive explanation.In this initial article, we will examine a concrete example of a network configuration on AWS and explore the functionality and positioning of VPC Reachability Analyzer.Configuration of a Sample NetworkLet's consider a compact, uncomplicated network that is robust enough to demonstrate the capabilities of VPC Reachability. The diagram below illustrates this network architecture.Examining the diagram shows that within the VPC are two Subnets labelled X and Y, hosting EC2 Instances A, B, and C. Additionally, Security Groups aiming to allow communication through port 22 have been attached to the Instances.Now, let's ask our readers an initial question: Is SSH communication from Instance A to C possible? The answer is clearly yes. The settings of Security Group 1 make this possible. On the other hand, it should be equally clear from the diagram that SSH communication from Instances B to C will be blocked by the misconfigured Security Group 2, limiting the destination to the local. Moreover, assigning a public IP address allows Instance C to access the Internet; it potentially violates your security compliance.This might seem like a simple matter. However, is network connectivity always straightforward? Certainly not. In real-world production environments, many EC2 Instances and other computational resources are closely interconnected with Security Groups and Route Tables involving routing to multiple networks. In such complex production environments, determining which Instances can communicate with others and which cannot is a task full of complexity.So, what are the potential consequences of this complexity? You carefully review the infrastructure, defined using CDK or Terraform, on GitHub. It looks perfect. You go ahead and deploy. At this point, you realise that your carefully deployed network isn't working as expected.Dual Approaches to TroubleshootingWhen faced with a problem, two main troubleshooting methods are available to us.The first approach involves actively sending packets and then checking connectivity. This is a well-established technique that predates our move to the cloud. From the essential ping, traceroute, and telnet commands to the more versatile netcat utility, an experienced infrastructure engineer's toolbox is filled with various tools. Alternatively, if the issue is within a database, one may investigate the application's behaviour and logs to determine the root cause.The second approach involves forgoing the sending of actual packets and carefully examining the configuration to identify the root cause. By logging into the AWS Management Console and navigating through numerous pages, combining the various pieces of information in one's mind, one may ultimately identify the incorrect configuration that caused the problem. This is an impressive accomplishment, a testament to the skill of an experienced engineer.The two approaches described above are, in a way, opposite. One involves active trial and error, while the other relies on the role of an armchair detective to identify the cause. However, a key characteristic unites these two methods: they are both stressful. A single misplaced entry in a Route Table with many configurations closely intertwined can block communication. Few engineers would voluntarily take on the task of solving this puzzle.Power of VPC Reachability AnalyzerWho, then, shall unravel this enigma if not you? The answer lies in the VPC Reachability Analyzer. This powerful tool can automatically resolve issues related to network behaviour examples without requiring human intervention.The role of humans is limited to simply specifying the conditions for communication. For example, one might define the source and destination EC2 Instances, protocol, and port number. At the cost of a small amount of time and usage fees, the Analyzer will provide a resolution, determining whether the communication is possible.What is particularly interesting is that during this operation, the Analyzer does not send any packets. Instead, it examines the configuration items rather than the actual communication results. Like an experienced engineer, the Analyzer possesses \"knowledge\" about the network.This leads to another significant advantage. If the Analyzer were to assess reachability by sending packets in a way similar to the traceroute command, the communication would be unable to proceed beyond the point where a component with a discontinuous configuration is encountered, thus failing to obtain information about the entire communication path.However, in reality, the Analyzer analyses the \"meaning\" of the configurations. As a result, it can provide a comprehensive answer, including which configurations are problematic and require fixing. This becomes clear when examining the actual results screen presented below.Upon initial inspection of the results, one is immediately drawn to the red icon, accompanied by the concerning message: \"None of the egress rules in the following security groups apply.\" This troubling finding reveals the root cause of the SSH connection's failure, originating from the previously discussed misconfiguration of the Security Group. Moreover, it is particularly significant that the Analyzer displays the entire path from the source to the destination, clearly identifying the point of misconfiguration. This clear visualisation offers valuable guidance on resolving the network connectivity problem.Syntax, Rules and SemanticsAs previously explained, VPC Reachability Analyzer represents a verification paradigm that understands the inherent \"meaning\" of the configurations. When aiming to verify infrastructure as code, one can divide the verification levels into three tiers.The first tier relates to syntactic verification, representing the most basic level. Imagine a scenario where you define your infrastructure using CloudFormation templates. In this context, syntactic verification is equivalent to examining the YAML file structure's compliance with the specified schema. As syntactic errors are directly related to the failure of the deployment itself, the necessity of this verification stands as an undeniable prerequisite. However, as previously noted, it is not a sufficient condition. The deployed resources operate according to your definitions rather than your intentions.The second tier includes rule-based verification. One of the most typical rule-based verifications in the networking domain is the prohibition of access from 0.0.0.0/0. This rule, which comes pre-configured in AWS Trusted Advisor, denies exposure to the Internet. However, it is simply an explicit formulation of the knowledge that \"allowing 0.0.0.0/0 is equivalent to granting access from the Internet.\"The third and highest tier of verification under consideration is semantic verification. At this level, by providing the configurations as input, one can perform a verification that considers the \"meaning\" of how these configurations will manifest their effects. Imagine a scenario involving numerous components, where the interaction of their respective rules ultimately determines the network's reachability. In semantic verification, the need for individual judgments is eliminated by proactively formalising the mutual influence of rules, enabling the verification to determine whether the desired outcome will be achieved as the final result.For the readers who have read this article thus far, it will be clear that this semantic verification is the distinguishing feature of VPC Reachability Analyzer. It is a decisive advantage for us engineers who are constantly challenged by network puzzles.ConclusionIn this article, we have presented the challenges of network troubleshooting on AWS and introduced the capabilities of VPC Reachability Analyzer as a solution. Moreover, we have defined three tiers of verification for codified infrastructure, positioning Reachability Analyzer as the third tier enabling semantic verification.In the subsequent article, we shall address how VPC Reachability Analyzer performs semantic verification, or in other words, what mechanisms it employs to comprehend the \"meaning\" of configurations. Specifically, we will introduce the foundational technologies of SAT and SMT solvers and elucidate their operating principles.AppendixYou can try out VPC Reachability Analyzer and VPC Network Access Analyzer using the provided CloudFormation template.Please note that I cannot be held responsible for any issues using this template. In particular, be aware that this template will launch actual EC2 instances, albeit small ones, and that both Analyzers incur costs (especially Reachability Analyzer, which is expensive at 0.1 USD per execution).---Description:'DemoforVPCAnalyzers'AWSTemplateFormatVersion:2010-09-09Mappings:RegionMap:ap-northeast-1:execution:ami-02892a4ea9bfa2192Resources:VPC:Type:AWS::EC2::VPCProperties:CidrBlock:172.0.0.0/16InternetGateway:Type:AWS::EC2::InternetGatewayInternetGatewayAttachement:Type:AWS::EC2::VPCGatewayAttachmentProperties:InternetGatewayId:!RefInternetGatewayVpcId:!RefVPCSubnetX:Type:AWS::EC2::SubnetProperties:VpcId:!RefVPCCidrBlock:172.0.1.0/24SubnetY:Type:AWS::EC2::SubnetDependsOn:InternetGatewayProperties:VpcId:!RefVPCCidrBlock:172.0.2.0/24MapPublicIpOnLaunch:truePublicRouteTable:Type:AWS::EC2::RouteTableProperties:VpcId:!RefVPCPublicRouteTableAssociation:Type:AWS::EC2::SubnetRouteTableAssociationProperties:RouteTableId:!RefPublicRouteTableSubnetId:!RefSubnetYPublicRoute:Type:AWS::EC2::RouteProperties:RouteTableId:!RefPublicRouteTableGatewayId:!RefInternetGatewayDestinationCidrBlock:0.0.0.0/0SecurityGroup1:Type:AWS::EC2::SecurityGroupProperties:GroupDescription:'SampleSG1'VpcId:!RefVPCSecurityGroupIngress:-CidrIp:!GetAttVPC.CidrBlockIpProtocol:'tcp'FromPort:22ToPort:22SecurityGroupEgress:-CidrIp:0.0.0.0/0IpProtocol:'-1'SecurityGroup2:Type:AWS::EC2::SecurityGroupProperties:GroupDescription:'SampleSG2'VpcId:!RefVPCSecurityGroupIngress:-CidrIp:!GetAttVPC.CidrBlockIpProtocol:'tcp'FromPort:22ToPort:22SecurityGroupEgress:-CidrIp:127.0.0.1/32IpProtocol:'-1'InstanceA:Type:AWS::EC2::InstanceProperties:ImageId:Fn::FindInMap:-RegionMap-!RefAWS::Region-executionInstanceType:'t3.nano'SubnetId:!RefSubnetXSecurityGroupIds:-!RefSecurityGroup1Tags:-Key:NameValue:InstanceAInstanceB:Type:AWS::EC2::InstanceProperties:ImageId:Fn::FindInMap:-RegionMap-!RefAWS::Region-executionInstanceType:'t3.nano'SubnetId:!RefSubnetXSecurityGroupIds:-!RefSecurityGroup2Tags:-Key:NameValue:InstanceBInstanceC:Type:AWS::EC2::InstanceProperties:ImageId:Fn::FindInMap:-RegionMap-!RefAWS::Region-executionInstanceType:'t3.nano'SubnetId:!RefSubnetYSecurityGroupIds:-!RefSecurityGroup1Tags:-Key:NameValue:InstanceCReachablePath:Type:AWS::EC2::NetworkInsightsPathProperties:Source:!RefInstanceADestination:!RefInstanceCDestinationPort:22Protocol:tcpTags:-Key:NameValue:'ReachablePath'BlockedPath:Type:AWS::EC2::NetworkInsightsPathProperties:Source:!RefInstanceBDestination:!RefInstanceCDestinationPort:22Protocol:tcpTags:-Key:NameValue:'BlockedPath'AccessToInternet:Type:AWS::EC2::NetworkInsightsAccessScopeProperties:MatchPaths:-Destination:ResourceStatement:ResourceTypes:-AWS::EC2::InternetGatewayTags:-Key:NameValue:'AllAccessToInternet'Enter fullscreen modeExit fullscreen mode"}
{"title": "Building a Serverless Newsletter: Your Guide to AWS and Amazon SES", "published_at": 1711630800, "tags": ["webdev", "aws", "serverless", "programming"], "user": "Matteo Depascale", "url": "https://dev.to/aws-builders/building-a-serverless-newsletter-your-guide-to-aws-and-amazon-ses-4emg", "details": "\u26a0\ufe0f Hey! This blog post was initially published on my own blog. Check it out from the source:https://cloudnature.net/blog/building-a-serverless-newsletter-your-guide-to-aws-and-amazon-sesDiscover the secrets of creating a serverless newsletter using AWS and Amazon SES. Dive into SST, Middy with Typescript, and master everything you need to know about SES for a seamless newsletter service.IntroductionHave you ever wrestled with Amazon SES, firing off emails left and right, dealing with bounces, and handling complaints? I sure have. Then, a lightbulb moment hit \ud83d\udca1, I want to build my own newsletter infrastructure! Sure, there are quicker routes out there, but where's the fun in that?So, buckle up for this blog post. We're delving into serverless setups and everything there is to know about Amazon SES. Of course, we're making things work at scale!And hold tight, because one section is titled \"Lambda? Nope, thank you!\". Can serverless exists without Lambda functions? \ud83e\udd14Project OverviewCreating a newsletter infrastructure is quite a puzzle. It needs to be quick, handle heavy traffic, and stay operational no matter what. So, why did I opt for a complete serverless approach? It's straightforward: serverless computing is the superhero of performance, resilience, and scalability \ud83e\uddb8.And hey, it's also budget-friendly. Newsletters don't need to be active 24/7; they wake up once a day, get the job done, and then rest until the next day. Let's break down the reasons behind choosing a serverless setup for this newsletter. But before we get there, let's tackle the elephant in the room by exploring three ways to create a newsletter.The Fastest WayDo I really have to spell it out? Numerous newsletter providers exist, so many choices, really. \ud83d\ude10The Simple WaySetting up a basic serverless newsletter infrastructure on AWS is a breeze, requiring just a few essential components:Validate your sender identity in Amazon SESImplement a straightforward AWS Step Function for sending emails with AWSSet up a single web service to publish the newsletterWith these in place, you can kick off your newsletter distribution. The beauty of this solution? In just 8-16 hours, you've built your custom newsletter infrastructure.But hold on, we're not aiming for a quick fix here, are we? We're on a mission to dedicate an entire weekend just to the HTML and CSS \ud83d\ude44. Don't worry, I'm not trying to spook anyone. I believe in your champion-level skills, capable of centering a div better than I ever could.Don't worry, I'll spare you the HTML and CSS details of the newsletter templates.Let's shift our focus to the complete AWS infrastructure, which is the reason we're all gathered here \ud83d\ude80.The Complete WayBuilding a serverless infrastructure doesn't have to be a complete mess. I'm not a fan of unnecessary complexity. Instead, let's keep it simple by assembling basic components, creating completeness, and minimizing confusion.So, what are these foundational bricks? Let's break them down:Setting up Amazon SESAdditional Amazon SES configurations for Bounces, Complaints, Clicks (don't worry, we'll dive into this shortly)API Web Services (AWS \ud83d\ude1c) for seamless newsletter operationsAWS Step function workflow for sending emailsThese bricks, when glued together, form the backbone of our newsletter.Now, let's delve into each element while keeping things clear and straightforward.Amazon SES SetupSetting up SES (Simple Email Service) is the starting point for our newsletter infrastructure. It's the AWS service that powers our email delivery. Let's look at the infrastructure needed to send emails with AWS.Domain ValidationBefore we jump into the technicalities, let's get our domain validated. Why? Email providers want to know who owns that email address. The drill? Domain validation through DNS (basically, adding a few CNAME records in your DNS provider). In my case, a quick validation ofhttps://cloudnature.netin Route53, and we're good to go.Find the full identity validation process in the Infrastructure As Code section at the bottom of the GitHub repository.But hold on, validation alone is not enough for email providers. To be top-tier among other email providers, we need a bit more from Amazon SES:DKIM (DomainKeys Identified Mail)Custom MAIL FROM (cue SPF authentication and DMARC compliance)BIMI complianceNow, let's look into all of them. The names might be a mouthful, but they're harmless, I promise \ud83d\ude1cDomainKeys Identified Mail (DKIM)Ensures email authenticity by adding a digital signature. This is your email's passport to a trustworthy inbox. DKIM is a security standard, ensures that an email claiming to be from a specific domain is genuinely authorized by the domain owner.Opting for Easy DKIM means Amazon SES takes the lead. It generates a public-private key pair and includes a DKIM signature in every message sent from that identity. Your job? Well, nothing much, Amazon SES has got it covered!Custom MAIL FROMEver wondered about the addresses behind the emails you send? There's the From address, visible to the recipient, and then there's the MAIL FROM address, indicating the message's origin. By default, Amazon SES assigns a MAIL FROM domain for your outgoing messages, using a subdomain ofamazonses.com.But what if you want to add a personal touch to your MAIL FROM address? As seen in the picture above, adding your own MAIL FROM means you get rid of that ugly\"via amazonses.com\"tag near your name!For this, you simply need to comply with the Sender Policy Framework (SPF), an email validation standard designed to prevent email spoofing. While at it, let's add another protocol: Domain-based Message Authentication, Reporting, and Conformance (DMARC). DMARC, fancy name, simple job: it helps detect email spoofing, allowing you to guide the email provider on how to act.To check your DMARC compliance, visit\ud83d\udd17 DMARC Inspector.Brand Indicators for Message Identification (BIMI)Give your emails a visual identity. Let's set up BIMI and become a recognized face in your recipient's inbox.At this point, we're almost there. But wouldn't it be nice to have a logo as your email profile image? That's where BIMI comes in. If you've followed the previous steps, your Amazon SES configuration is nearly ready.All we need are two things:Create an SVG for our profile image.Add the BIMI record to our DNS registrar.To convert your SVG logo (modern SVGs may not be sufficient for BIMI), download a simple yet efficient program in GitHub repositoryhttps://github.com/authindicators/svg-ps-converters/tree/master.Afterward, upload your logo and add a DNS record like so:default._bimi.dev.cloudnature.net TXT v=BIMI1;l=https://cloudnature.net/logos/logo.svg;Enter fullscreen modeExit fullscreen modeAnd there you have it! Check your BIMI compliance at\ud83d\udd17 Bimi Group.Email Sending ChallengesSending emails may seem like a straightforward task, but the journey from your server to your recipient's inbox can be a bumpy ride. In this section, we'll navigate through common challenges encountered while sending emails.We need to be careful because AWS policies are strict, so manual checking may not be enough. Here's how our automatism flows:Some triggering event arrivesAmazon SES triggers Amazon SNSAmazon SNS triggers AWS LambdaAWS Lambda run custom logic to decide if the email address needs to be removed from the newsletter list (for example if the email is unexistent), if so, it remove the record from the DynamoDB tableLet's see briefly one case at the time \ud83d\udc47.Handling BouncesBounces in the email world are annoying but more common than you could think. They happen when an email cannot be delivered to the recipient, either due to aninvalid email addressor a temporary issue. In our serverless setup, dealing with bounces is crucial for maintaining a clean and effective newsletter infrastructure.So, what's the drill when a bounce event pops up? There are different types of bounces, but for the hard ones, we remove that email address from our newsletter list.Addressing ComplaintsNot every email recipient is thrilled to receive newsletters, and complaints may arise. Complaints occur when a recipient marks your email asspamor unwanted. Managing complaints is key to maintaining a positive sender reputation and ensuring your emails land in the inbox.Now, what happen when a complaint event lands in our logs? Developers don't pay attention to warnings, so treat it like a yellow card and investigate why it got the spam label. If it becomes a recurring event, consider removing that email address from your subscribers list.To check if you are following every best practices, use this website:Mail TesterTroubleshooting ErrorsIn the intricate world of email sending, errors can pop up unexpectedly. From server glitches to misconfigurations, troubleshooting errors is an essential part of maintaining a reliable newsletter infrastructure.Always keep a watchful eye on error logging. It's like your troubleshooting toolkit, it helps you identify issues worth fixing.Tracking Clicks Per LinkKnowing how recipients engage with your newsletters is gold. Tracking clicks per links gives you the insight on the effectiveness of your email campaigns. While Amazon SES doesn't offer this out of the box, we've got to add to our serverless architecture an additional SNS Topic and an AWS Lambda. This duo saves records in our Amazon DynamoDB database, tracking every piece of information about a specific link.A big shoutout toGuillermo Ojedafor enlightening me on the significance of this metric during our conversation about must-have metrics for newsletters. Thanks Guillermo \ud83d\ude4f.Newsletter API Web ServicesBuilding the AWS serverless infrastructure was a piece of cache (sorry, I couldn't resist). Let's break down the stack:SST with CDK integrationTypescriptMiddyThese tools are the unsung heroes that took my developer experience to a whole new level. I won't dive too deep here; you can find the full repository with all the goodies at the bottom.Now, which APIs are we talking about?CRUD APIsfor admin operations on newsletter itemsNewsletterpublish and unpublishAPIsNewslettersubscribe and unsubscribeAPIsPretty standard stuff, right? For the Amazon DynamoDB connection, I opted for dynamodb-toolbox: a library that proved its worth and is likely to feature in my future projects too.Take a peek at a snippet of the Postman APIs developed for this project. One API in the mix serves as the documentation. If you're authenticated and give it a buzz, it'll hand you the JSON Postman collection, all decked out with documentation ready to use (just remember to include environment variables).AWS Lambda functions? No, Thank You!Alright, let's kick off this chapter by addressing the elephant in the serverless room: AWS Lambda functions. Now, you might wonder, \"Why didn't we use Lambda functions?\" Well, the answer is as simple as our serverless setup: a single AWSStepFunction took the reins and orchestrated the entire show. Say goodbye to the need for countless Lambda functions or a single AWS Lambda function doing all the dirty work. Interested? Let's explore why onenot-so-bigAWS Step function stole the spotlight.This overview divides our step function into three main parts:Workflow that sends the first email: a critical step to ensure everything is functioning as expected. An initial test to our own email address helps us validate the integrity of our setup.List every subscriber from Amazon DynamoDB: an essential task where we fetch the list of subscribers and organize them into batches of50 email addresseseach.For each batch, send out emails with Amazon SES: the final leg of our journey involves the systematic sending of emails in batches using Amazon SES.Now, let's dive into each step and explore the challenges that may arise when building this intricate workflow with AWS Step Functions.Sending Test EmailThis segment involves a straightforward process. We retrieve the newsletter item from Amazon DynamoDB, process and convert it into a valid JSON, and then proceed to send the email using the Amazon SES SendBulkEmail V2 API.SendBulkEmail stands out as the best practice, ensuring that each recipient sees only their own name and email address in the To header of the messages they receive.This step proves to be exceptionally useful to know how the newsletter will truly appear in our email inbox. The simplicity and effectiveness of this approach allow for quick identification and correction if any HTML issues arise. Following this, we can unpublish and reschedule the newsletter publish date.Processing SubscribersThis phase proved to be the most intricate to develop! Pagination for Amazon DynamoDB works differently compared to many NoSQL or SQL databases. You simply inform the database about the last item you retrieved. Using that as a starting point, we can list through every single subscriber.The objective here is to produce a list of subscribed email addresses. However, to create that list, we need to go through a few rounds of Pass states. In my case, this was the workflow:First Step: Query elements from Amazon DynamoDB (not scan)Second Step: Create an array of items that merges previous items and new ones: States.Array($.dynamodbConfig.items[ * ], $.subscribers.Items[ * ].email.S)Third Step: The result is pretty ugly:\"items\": [       [         \"emailaddress3@example.com\"             ],       [         \"emailaddress@example.com\",         \"emailaddress1@example.com\",         \"emailaddress2@example.com\"       ] ]Enter fullscreen modeExit fullscreen modeThis means we need to flatten this list like so: $.dynamodbConfig.items[ * ][ * ]Fourth Step: Check if there are more elements. If not, we are going to close this process. If, in fact, there are more subscribers, we save the items in another object and start all over again.Sending Out EmailsNow, we are almost ready to \"hit\" that send button and dispatch our newsletters. The workflow here is comparatively simpler than the previous one, but we need to make some adjustments to ensure we can call the Amazon SES SendBulkEmail at scale.The process begins with this input:[         \"emailaddress@example.com\",         \"emailaddress1@example.com\",         \"emailaddress2@example.com\",         \"emailaddress3@example.com\",                 ...     ]Enter fullscreen modeExit fullscreen modeHere's what we need to achieve:Create a valid object with parameters like so:[     {       \"Destination\": {         \"ToAddresses\": [           \"emailaddress@example.com\"         ]       }     },         ...   ]Enter fullscreen modeExit fullscreen modeWhile this task is relatively straightforward, we need to loop through each email address and add the payload as shown above.Divide the object containing email addresses into batches of 50 items (due to Amazon SES hard quotas). Even better, we can use the AWS Step Function intrinsic function like so States.ArrayPartition($.toAddressDestinations, 50)For each batch, call the Amazon SES SendBulkEmail with the correct parameter structure.One thing that proved super useful was getting the parameter from outside the Map state. This is possible usingItemSelectorlike so:{   \"ContextIndex.$\": \"$$.Map.Item.Index\",   \"ContextValue.$\": \"$$.Map.Item.Value\",   \"toAddressDestinations.$\": \"$.toAddressDestinations\",   \"templateData.$\": \"$.templateData\" }Enter fullscreen modeExit fullscreen modeFinally, our workflow is done. Before wrapping everything up, we need to set up a proper error flow. In this case, if there are any errors, my workflow stops, and I get alerted via email.See? A complete workflow without AWS Lambda functions!ChallengesDuring the process of building the serverless AWS infrastructure, I encountered several challenges, but I'm glad I was able to complete the AWS Step Function workflow without the need for AWS Lambda functions.Let's explore some of the challenges I faced (and no, I won't talk about the enormous time spent on HTML/CSS \ud83d\ude1c):Unfortunately, there is no built-in utility in AWS Step Functions to enable marshalling and unmarshalling results to and from Amazon DynamoDB. This means you need to handle .S or .N every time.You can identify errors from SendBulkEmail by examining this object: $.BulkEmailEntryResults[ * ].Error. In my case, I used this object to list every error. If any were found, I raised an error, stopping the entire workflow.Flattening the DynamoDB output requires two steps. Unfortunately, it's not possible to accomplish this in a single, straightforward step.It's not possible to use AWS Step Function intrinsic functions within the Choice state. Consequently, in many cases, a new Pass step needed to be added.MetricsVisualization of metrics is crucial, and SES offers two main capabilities:Metricsfor tracking Bounces, Complaints, Sends, and more.Virtual Deliverability Manager(VDM) for Amazon SES.The default metrics are fairly standard. Now, let's look into VDM functionality.Virtual Deliverability Manager for Amazon SESVDM provides a lot of metrics, including soft and hard bounce rates, open rates for emails, and more. While it lacks a \"click per link\" metric, which we had to build that ourselves. Apart from this, there aren't many fancy or custom metrics available; however, the standard insights provided are fundamental for any newsletter.Additionally, you can download a CSV file containing every single metric available in Amazon SES, which proves to be quite useful.Total PricingLet's quickly summarize the pricing for Amazon SES and AWS Step Functions. Here's a comparison:Amazon SES:1,000emails:~ 0.3$10,000emails:~ 3$1,000,000emails:~ 30$AWS Step Functions:1,000emails: ~ 2,150 executions =free10,000emails: ~ 21,500 executions =~ 0.5$1,000,000emails: ~ 2,150,000 executions =~ 55$\u26a0\ufe0f The price could potentially be reduced to 3$ for AWS Step Functions by incorporating the \"Compute to bulk destination\" Map into the DynamoDB processing. This could be a notable improvement for the future.ConclusionCreating a serverless newsletter with Amazon SES and AWS Step Functions is a potent and cost-efficient solution. By leveraging SES for reliable email delivery and Step Functions for orchestration, we've built a scalable system without traditional Lambda functions.Well now you know how to create a serverless newsletter on AWS, the only thing I didn't show you is how it turns out!Check the results \ud83d\udc49https://cloudnature.net/newsletters/subscribeI'll post monthly, covering the latest AWS trends, community blog posts, and Cloud-related news. If you're an AWS Cloud Architect, Cloud Engineer, DevOps, or AI/ML Developer, you'll find valuable content.You can find the repository here:https://github.com/Depaa/newsletter-manager-template\ud83d\ude09.If you enjoyed this article, please let me know in the comment section or send me a DM. I'm always happy to chat! \u270c\ufe0fThank you so much for reading! \ud83d\ude4f Keep an eye out for more AWS-related posts, and feel free to connect with me on LinkedIn \ud83d\udc49https://www.linkedin.com/in/matteo-depascale/.Referenceshttps://docs.aws.amazon.com/ses/https://middy.js.org/https://docs.sst.dev/https://www.mail-tester.com/Disclaimer: opinions expressed are solely my own and do not express the views or opinions of my employer."}
{"title": "Multi-region Deployments with CDK", "published_at": 1711625571, "tags": ["aws", "awscdk", "tutorial", "devops"], "user": "Dakota Lewallen", "url": "https://dev.to/aws-builders/multi-region-deployments-with-cdk-4kfk", "details": "IamFlowZ/multi-region-cdk-exampleMulti Region Stack ExampleThis is an example of one way to deploy multiple Cloudformation stacks to different regions, without having forks within the code.After deployment has completed, you should find function urls listed in your CLI corresponding to the regions you deployed to. Opening them should respond withHello from ${region}Thecdk.jsonfile tells the CDK Toolkit how to execute your app.CredentialsBy default this assumes you have an AWS profile configured, and will use the default account and region associated with it. The default is the account the credentials belong to and (typically) us-east-1.If you would like to target a specific account and environment, you can modifybin/multi-region-example.tsto use either of the commentedenvlines. One is for hard coding the other is for environment variables. You canlearn more about CDK environments here.Useful commandsnpm run buildcompile typescript to\u2026View on GitHubFrom the second you deploy your resources into the AWS Cloud, you have access to a global computing network. Something unimaginable in the past and grossly underutilized even by today\u2019s standards. In this article, I\u2019ll show you two simple ways you can begin leveraging the power of global deployments with CDK.PrerequisitesIf you plan on following along, make sure you have completed these tasks.Installed theAWS CLIInstalled theCDK CLIConfigured IAM credentialsBootstrapped at least two regions in an AWS account using thecdk bootstrap commandRoot Stack with SubstacksThis design implements a \u201cRoot\u201d stack that then deploys the subsequent region-specific stacks. The root stack will be deployed into the default region associated with the profile (us-east-1 unless specified). There are several things achieved with this.We develop a clear hierarchyThe main benefit of this is dependency management since we create a clear route by which we can pass information both up and back down the treeWe create a point of centralization that we can use to build dependencies that are used by many dependentsFor example, if we have an SQS queue that is used globally within the application, we can build it in the root stack and pass it through to the props of the application stacksWe have a \u201cnon-regional\u201d stack to house global resourcesBuilding on the last point, we have a stack that handles deploying for things that aren\u2019t regional in AWS (IAM, S3, etc.).The stack itself is tied to a region, but there\u2019s no reason that the resources the stack deploys also have to be tied to that region.Here is an example of the root stack taken fromhere in the repository.import*ascdkfrom'aws-cdk-lib';import{Construct}from'constructs';import{AppStack}from'./app-stack/app-stack';exportclassMultiRegionExampleStackextendscdk.Stack{constructor(scope:Construct,id:string,props?:cdk.StackProps){super(scope,id,props);constregions=['us-east-1','eu-west-1'];// store the regional stacks in an object for debuggingconstregionalStacks=regions.reduce((accu,region)=>{constregionalStack=newAppStack(this,region,{env:{region},});return{...accu,[region]:regionalStack};},{});}}Enter fullscreen modeExit fullscreen modeSome caveats:If you\u2019re familiar withthe NestedStack construct, you are probably thinking \u201cWhat a perfect use case!\u201d. Sadly at the time of writing this, it seems that NestedStacks have to be deployed in the same region that the source stack is deployed in. I don\u2019t believe this is a CDK limitation, but instead a CloudFormation one.There is an open issuewith some discussion around implementing this in CDK, however it does seem to be rather complicated. But hey if this is a blocker for you, get active and let them know!Another Cloudformation feature that is commonly used to solve this problem isStack Sets. At the time of writing this, there is no support out of the box in CDK. But there is aCDK Labs constructin the experimental phase you can try out. But per their readme, I would avoid getting overly tied to its implementation until it is generally released.In theory, you shouldn\u2019t need to reference values from an application stack deployed in Region A in an application stack deployed in Region B. As they are siblings, so dependencies should be managed via prop drilling. However reality is often finicky, so if you do end up in this situation, I would recommend storing the dependency value in ParameterStore and dynamically referencing it in the dependent stack. If you\u2019d like to learn more, [I wrote a deep-dive here] that you may find useful.Deploy the Stacks DirectlyFor a more traditional IaC experience, we can remove the root stack. Opting to instead deploy the stacks directly. We do lose some of the \u201csuperpowers\u201d the first option provides, with the benefit of having something simpler to reason about.There are two main options I\u2019ve observedInstantiate your region-specific stack however many times you need in the /bin/app.ts file. You can find a full examplehere in the CDK docs. As well asthis branchin the repository.Instantiate your region-specific stack once in the /bin/app.ts file, and use thecdk deploycommand to deploy it to the regions you desire.This branchhas one approach you might take.While this does supply you with a more traditional experience there are still benefits to using CDK, that you wouldn\u2019t have access to otherwise.Higher-level constructsIf like many others you are on a Serverless journey, you may be looking to migrate to Fargate. One of my personal favorites in the CDK library is the ApplicationLoadBalancedFargateService. It\u2019s a mouthful because it does a lot for very little. In ~30 lines (or less) of code, it will deploy an ECS cluster, service, and task definition. As well as provide one-line APIs for wiring in the cluster, load balancer, and any VPC configuration you may need to do. All of this would be hundreds of lines of Cloudformation templating.Component librariesIf the standard library can\u2019t meet your needs,constructs.devis an open-source marketplace that houses thousands of constructs. Ranging from monitoring tools, linters, and everything else you can imagine codifying in the cloud.Not only are there public libraries, but you can also create centralization within your organization through private ones. Making it possible to standardize higher-level patterns in the cloud.ConclusionWith CDK, it\u2019s never been easier to duplicate resources into multiple AWS regions. You can bring your code closer to your customers and increase your fault tolerance with just a few clicks.Find me onLinkedIn|Github|Mastodon"}
{"title": "DevOps with Guruu | Chapter 13 : Deploy Wordpress and Manage Blogs", "published_at": 1711625244, "tags": ["webdev", "devops", "aws"], "user": "Hoang Guruu", "url": "https://dev.to/aws-builders/devops-with-guruu-chapter-13-deploy-wordpress-and-manage-blogs-4opb", "details": "DevOps with Guruu | Chapter 13 : Deploy Wordpress and Manage BlogsWe will use wordpress to deploy my blog, self host my wordpress instead of use it on website wordpress.com. Because thanks to it, we can use more than feature, about plugin, theme, anything without pay money for any package.Join me on this journey to mastering DevOps. Let's get hands-on and build a strong foundation together! Don't forget to like, share, and subscribe for more exciting content. Happy coding, future DevOps!\""}
{"title": "Afraid of outgrowing AWS Rekognition? Try YOLO in Lambda.", "published_at": 1711618380, "tags": ["aws", "javascript", "machinelearning", "serverless"], "user": "Michal \u0160imon", "url": "https://dev.to/aws-builders/afraid-of-outgrowing-aws-rekognition-try-yolo-in-lambda-25lk", "details": "When it comes to machine learning, the cloud is a straightforward choice due to its robust computing power. Cloud can also provide a variety of services that can speed up your efforts significantly. For the purpose of this article, we will use computer vision as an example of a machine learning use case and since my cloud of choice is AWS, the most relevant service is AWS Rekognition. It offers an API where you can send a picture and it will respond with a list of keywords that have been identified there. This is perfect for when you are building a PoC of your new product, as these ready-made ML services can bring your idea to life in a cheap and fast manner.However, there\u2019s a catch. As your product gains traction and real-world customers, you might find yourself bumping against the limits of AWS Rekognition. Suddenly, what was once a smooth ride can become a bottleneck, both in terms of accuracy and cost. Fear not! Switching quite early to a more flexible solution can save you a lot of headaches - like running a pre-trained model in an AWS Lambda, harnessing the same serverless properties that AWS Rekognition provides.In this article, we\u2019ll explore the limitations of AWS Rekognition, dive into the world of YOLO, and guide you through implementing this more adaptable solution. Plus, we\u2019ll look at a cost comparison to help you make an informed decision.AWS RekognitionAWS Rekognitionis a great choice for many types of real-world projects or just for testing an idea on your images. The issue eventually comes with its cost, unfortunately, which we will see later in a specific example. Don\u2019t get me wrong, Rekognition is a great service and I love to use it for its simplicity and reliable performance on quite a few projects.Another downside is the inability to grow the model with your business. When embarking on a new project, requirements are often modest. AWS Rekognition shines here, catapulting you from zero to 80% completion in record time. Over time, however, you realise there are some specific cases that you need to optimise your application for and Rekognition is not working flawlessly in those situations anymore. You can bring your own dataset and useAWS Rekognition Custom Labelswhich is a fantastic feature. It is a bit of work to get it right but once it works it can get you quite far. Unfortunately, if you outgrow even this feature and you need to tweak your model even further, the only option is to start over with a custom model. This limitation can feel restrictive, hindering your model\u2019s evolution alongside your business needs which brings me to pre-trained models.Overall, Rekognition can get you started quite quickly and bring you a lot of added value. Yet, as your project matures, its constraints and pricing can become bottlenecks and you will need to dance around them.YOLO and Friends: A Versatile Approach to Object DetectionYou Only Look Once, commonly known asYOLO, is a well-established player in the world of computer vision. This pre-trained model boasts remarkable speed and accuracy, making it an excellent choice for detecting hundreds of object categories within images. Whether you\u2019re building a recommendation system, enhancing security, or analyzing satellite imagery, YOLO has your back. There are also many alternatives out there so do your research and pick the best model for your images.Hugging Faceis a great starting point.YOLO comes in various sizes, akin to a wardrobe of pre-trained outfits. Start with the smallest,  lightweight and nimble. As your business needs evolve, upgrade to larger variants. But here\u2019s the magic: YOLO\u2019s flexibility extends beyond off-the-rack sizes. You can fine-tune it with your own data, enhancing accuracy to suit your unique context. It needs a little bit of knowledge but by the time your business will need such accuracy, you will probably also have some time and budget for it. It is not a silver bullet but you will eventually not face a dead end with the only solution of starting over like with AWS Rekognition.ImplementationPython is these days the de-facto standard programming language for data analysis and machine learning. If you are familiar with Python, go ahead and leverage the YOLO documentation to implement a Python Lambda function for image classification. For educational purposes, I think it\u2019s beneficial to assume you are not necessarily running everything in Python. I like to use JavaScript/TypeScript and I will use it here as an example. If you prefer to write your e-commerce solution, FinTech startup, or CRM in a different language, Python can not stop you here.ONNX RuntimeDeveloping machine learning (ML) models in Python has become second nature for data scientists and engineers. Yet, the beauty of ML lies not in the language used during development, but in the ability to deploy and run those models seamlessly across diverse platforms. ONNX Runtime is a set of tools that allows you to achieve portability of models across different platforms, languages as well a variety of computing hardware.You can choose a model developed in several of the common frameworks and export it into a*.onnxformat that can be easily run on a specialized cloud processor or in your web browser. Let\u2019s draw a parallel. Think of ONNX as the bytecode of the ML world. When you compile Java code, it transforms into bytecode, a portable representation that can journey across platforms. Then the ONNX Runtime, our Java Virtual Machine (JVM) for ML, will take the intermediate format and run it specifically optimized for the hardware configuration of the device.ONNX Runtime is like JVM. Wherever it runs you can execute your models the same way. In our case, we will use the JavaScript version ofONNX Runtime for Node.jswhich is running in AWS Lambda as well. If you prefer a different language or a different infrastructure, feel free to leverage it as long as ONNX Runtime is available for your tech stack. Even though ONNX Runtime and JVM are quite different technologies, they share some common concepts at their core.PyTorch to ONNX ConversionONNX covers many languages and platforms, yet a touch of Python to convert the model first remains essential. Because YOLO is leveraging PyTorch, let\u2019s look at this variant. If you leverage TensorFlow or any other supported model format, the principle stays the same, just the specific commands will differ.importtorchmodel=torch.load(\"./yolov8n.pt\")torch.onnx.export(model,\"./yolov8n.onnx\")Enter fullscreen modeExit fullscreen modeFirst, you need to load the original model. In our case it isyolov8n.pt- the smallest of the shelf YOLO variants. Then we need to convert it toyolov8n.onnxformat. PyTorch already has an export method so we leverage it. Keep in mind that you can tweak the model even during the export and if you are planning to put a heavy load on the model, this can increase your performance and save you quite a lot of money. For simplification, we will just export it as it is.InferenceRunning the inference is almost as easy as the export. I will use JavaScript here to demonstrate a different language scenario which you might probably have. The Node.js ONNX Runtime library is quite lightweight and powerful.The inference process can be broken down into three main steps:Preprocessing - prepare the picture for the modelPrediction - ask the model to execute the classificationPostprocessing - convert the output into human-readable formatWe create anInferenceSessioninstance which loads the ONNX model. Then you can just run it with the input to classify.import{InferenceSession}from'onnxruntime-node';constsession=awaitInferenceSession.create('./yolov8n.onnx');constinput={...};constresults=awaitsession.run(input);Enter fullscreen modeExit fullscreen modeThe input for the inference is a picture converted into a specific format that YOLO requires. In simple words it needs to be an array of all pixels ordered by colour:[...red, ...green, ...blue]. Each pixel in a standard color image has a value of red, green and blue which together mix the final color. So if your picture has 2 pixels, you put the red value of the first pixel into the array and move to the next pixel. Once both reds are in the array, repeat the same process for greens and blues.Pixel1:[R1, G1, B1]Pixel2:[R2, G2, B2]YOLO Input:[R1, R2, G1, G2, B1, B2]For more details, explore the YOLO v8reference implementation.ResultsYOLO returns back three types of information. They all come in a bit ciphered format from the model but with a little bit of postprocessing (NMS) they look like this:Label - the model returns an identifier, which you can easily convert to a name of the object categoryBounding Box - coordinates, where in the picture was the label detectedProbability - a number on a range from 0 to 1 how confident is the model about the label here[{\"label\":\"person\",\"probability\":0.41142538189888,\"boundingBox\":[730.8682617187501,400.01552124023436,150.67451171875,180.06134033203125]},...]Enter fullscreen modeExit fullscreen modeIt\u2019s important to note that bounding box coordinates are normalized. You will therefore need to account for the original image width and height to calculate the specific coordinates if you want to draw rectangles around the detected objects like in my example.DeploymentMicroservices and event-driven architecture (EDA) are preferred choices in modern cloud architectures. The only step missing here is wrapping an API around the model and deploying it into a Lambda function that fits well into this architecture paradigm from several points of view.Lambda is a serverless compute service that allows quick scaling and handling quite a big load as well as scaling down to zero. This means that Lambda itself costs nothing if not used. It\u2019s an ideal candidate for an infrequent asynchronous load to annotate images. Scenarios like automatic content moderation, building an image search index, or improving your image ALT attributes because of SEO are perfect for this architecture.Ideally, we would like to leverage an asynchronous SQS queue or EventBridge as a source of the events. Furthermore, storing the actual image in S3 and the results in DynamoDB can be a great addition. These architectural decisions depend highly on your application though.AWS Rekognition vs. YOLO Lambda ComparisonI\u2019ve mentioned earlier that AWS Rekognition is not the cheapest service, especially at scale so let\u2019s compare the YOLO Lambda solution with Rekognition. Note that this is not a detailed benchmark, just a high-level comparison. Your implementation may vary.AWS RekognitionYOLOLambdaInference~300ms~1sCost per image$0.0010$0.0000166667Images per $1~1000~60KResources1024 MB RAMOtherus-east-1YOLO: 8nAs you can see, the price difference (~60x) is quite significant in favor of YOLO Lambda although the comparison is not strictly apples to apples. I used the smallest YOLO model which is not as accurate as Rekognition. For some use cases that might be enough though. On the other hand, you can leverage the flexibility of a custom image model and upgrade the accuracy as you go or even fine-tune the accuracy with your own dataset.With Rekognition, you get the simplicity of an API which is in many cases great to start with and in some cases even to stay with. YOLO Lambda is a bit more complicated to build and operate, however, it gives you great flexibility in terms of price, functionality, performance, and accuracy. Both variants can be further optimised for performance and cost so don\u2019t take this calculation for granted.ConclusionComputer Vision is a very interesting and helpful capability if you are working with pictures. It can help you moderate content, identify specific objects, or even improve the SEO of your e-commerce website (article coming soon).Achieving the best accuracy vs. price combination can sometimes be tricky with API-based services. Therefore we explored the possibilities of how to take advantage of pre-trained models like YOLO and run them in AWS Lambda with much more control and options to tweak for your specific use case. Finally, we compared the pros and cons of each solution from the cost perspective so you can pick the right one for you.For those interested in delving deeper into this topic, I recently spoke at a conference where I discussed these concepts in greater detail. I encourage you to watch the video of the talk for additional insights and practical examples.Are you leveraging computer vision in your application? Leave a comment below, I would love to hear your thoughts and experience."}
{"title": "Using Step Functions to handle feature flags", "published_at": 1711610855, "tags": ["aws", "serverless", "stepfunctions", "appconfig"], "user": "Arpad Toth", "url": "https://dev.to/aws-builders/using-step-functions-to-handle-feature-flags-4m31", "details": "We use feature flags to control how we release parts of a product. Instead of adding conditional statements in the code, we can use Step Functions to decide if the feature flag is enabled.1. The scenarioBob's company has a popular application and wants to release a new feature. They want to thoroughly test it in the development environment first. But it's a requirement to continually push changes to production, so the feature might not block the deployment pipeline.One way to manage this problem is to usefeature flags.AWS AppConfig, part of theSystems Managerecosystem, is a service that allows us to apply feature flags and configuration objects into our application.One way to incorporate them into the code is to use conditional statements that check whether we have enabled the feature flag for the given environment. But because Bob wants to minimize code changes and reduce code complexity (and because it's fun), he decided to useStep Functionsinstead ofifstatements.He created a separate Lambda function with the new feature (NewFeature), which exists parallel to the existing code (ExistingFeature).Let's see how this experiment worked out.2. AppConfig conceptsWhen getting a feature flag from AppConfig, we must provide three parameters.Anapplicationis a namespace or a folder that contains configurations, feature flags and environments for the given application.Theenvironmentis the target for the feature flag. We can name it as we like. In this example, we'll have two environments,devandprod. We enable the feature flag indev, which runs the new code. We keep the existing code inprod.The last element is theconfiguration profile, which can befeature flagor freeform configuration. This example will use a feature flag.I won't describe how to create applications, environments and configuration profiles in AppConfig. I'll provide a link that explains the process at the end of the post.3. Getting the feature flagFirst, we fetch the feature flag state (enabled or disabled) for the given environment from AppConfig.3.1. Lambda extensionLuckily, we (and Bob) build serverless applications and useLambdafunctions. AWS provides anextensionthat we can integrate with our function as alayer.If we useSAM templatesto create the resources, we can add the extension like this:AppConfigFunction:Type:AWS::Serverless::FunctionProperties:Runtime:nodejs20.xLayers:-'arn:aws:lambda:eu-central-1:066940009817:layer:AWS-AppConfig-Extension-Arm64:49'# more Lambda propertiesEnter fullscreen modeExit fullscreen modeThe URL is different for each region and Lambda function architecture, so you need tofindthe right one for your scenario.3.2. The codeWe can now call the extension from theGetFeatureFlagfunction. The code can look like this:importaxiosfrom'axios';const{AWS_APPCONFIG_EXTENSION_HTTP_PORT,APPCONFIG_APPLICATION_NAME,APPCONFIG_ENVIRONMENT_NAME,APPCONFIG_CONFIGURATION_NAME,}=process.env;constclient=axios.create({baseURL:`http://localhost:${AWS_APPCONFIG_EXTENSION_HTTP_PORT}`,timeout:5000,});exportconsthandler=async()=>{try{// 1. Fetch the feature flag from AppConfigconstconfig=awaitclient.get(`/applications/${APPCONFIG_APPLICATION_NAME}/environments/${APPCONFIG_ENVIRONMENT_NAME}/configurations/${APPCONFIG_CONFIGURATION_NAME}`,);// 2. Return the feature flag as the value of the config propertyreturn{config:config.data,};}catch(error){throwerror;}};Enter fullscreen modeExit fullscreen modeAWS_APPCONFIG_EXTENSION_HTTP_PORTdefaults to 2772, which we can leave as is.We can have an environment variable for each mandatory AppConfig parameter, application, environment (devorprodin this case) and configuration profile (1). This way, when we deploy the resources to multiple environments, the function will know the feature flag state for the given environment.The function's return value will be similar to the following:{\"isAllowed\":{\"enabled\":true}}Enter fullscreen modeExit fullscreen modeAs we can see, AppConfig returns an object of feature flag objects.isAllowedis the feature flag's very creative name. The presented value refers to thedevenvironment because the flag is enabled there. The value would beenabled: falseinprod. We encapsulate the feature flag value in theconfigproperty of the returned object (2).3.3. PermissionsThe function's execution role must allow theappconfig:StartConfigurationSessionandappconfig:GetLatestConfigurationpermissions.4. Using Step FunctionsGetFeatureFlagis part of thestate machine, so its return value (the feature flag name and its state) will be the input of the next state.In this case, it's aChoicestate, where we decide if we call the existing function or the one with the new feature.The state's definition can look like this:\"IsFeatureFlagEnabled\":{\"Type\":\"Choice\",\"Choices\":[{\"Variable\":\"$.config.isAllowed.enabled\",\"BooleanEquals\":true,\"Next\":\"NewFeature\"}],\"Default\":\"ExistingFeature\"}Enter fullscreen modeExit fullscreen modeWhen the feature flag's value isenabled: true, Step Functions will call theNewFeaturefunction. Otherwise, it will invokeExistingFeature. From this point, the flow can continue as usual.We have successfully eliminated theifblock from the code!5. AppConfig interactions with Step FunctionsWhat if we wanted to remove theGetFeatureFlagLambda function and make Step Functions directly interact with AppConfig? We can do that, but there are some considerations to take.5.1. What's going on in the background?With a few lines of code in the function handler (1), the Lambda AppConfig extension does a complex job in the background.First, it calls theStartConfigurationSessionAPI endpoint, which sends back anInitialConfigurationToken. Then, it invokes theGetLatestConfigurationendpoint, which returns the feature flag object seen above.It then callsGetLatestConfigurationat a configured interval (defaults to 60 seconds) and caches the result.5.2. Doing the same with Step FunctionsWe can remove this Lambda function from the architecture and delegate the AppConfig API calls to Step Functions. But in this case, we have to manage everything that the AppConfig extension does for us.The above workflow snippet shows the change only. TheChoicestate and everything after will remain the same.Step Functions integrates with 10,000+ AWS APIs, includingStartConfigurationSessionandGetLatestConfiguration.TheStartConfigurationSessionstate requires the mandatory AppConfig parameters we used in the HTTP call inside the Lambda handler. The state'sAPI parameterssection can look like this:{\"ApplicationIdentifier.$\":\"$.ApplicationIdentifier\",\"ConfigurationProfileIdentifier.$\":\"$.ConfigurationProfileIdentifier\",\"EnvironmentIdentifier.$\":\"$.EnvironmentIdentifier\"}Enter fullscreen modeExit fullscreen modeWe assume the state's input contains theApplicationIdentifier,ConfigurationProfileIdentifierandEnvironmentIdentifierproperties.The state's output (InitialConfigurationToken) will be the input of the following state,GetLatestConfiguration. This state needs one mandatory parameter calledConfigurationToken. The relevant part of the definition can look like this:{\"ConfigurationToken.$\":\"$.InitialConfigurationToken\"}Enter fullscreen modeExit fullscreen modeThe output will be similar to this:{\"Configuration\":\"{\\\"isAllowed\\\":{\\\"enabled\\\":true}}\",\"ContentType\":\"application/json\",\"NextPollConfigurationToken\":\"TOKEN\",\"NextPollIntervalInSeconds\":60}Enter fullscreen modeExit fullscreen modeAs we can see, theConfigurationproperty contains the feature flag as expected.5.3. It might not be a good ideaBut there's something else here.TheGetLatestConfigurationcall returns atokenin theNextPollConfigurationTokenproperty. AWS recommends that clients use it for subsequent calls to the endpoint.The documentation also recommends caching the feature flag instead of continually fetching it from AppConfig. We should take this advice because AWS charges after theGetLatestConfigurationcalls. So we want to reduce the number of invocations!It means that the client that calls the state machine should also provide the current token in the input. The first state could check if the request contains the token. In this case, the state machine could jump to theGetLatestConfigurationstate. If the client can't provide the token (for example, because it's the first call), the state machine could callStartConfigurationSession.Alternatively, the state machine could store the token somewhere externally, for example, in aDynamoDBtable. But this solution would add at least two extra API calls (read and update token) to the flow.All of these would increase complexity. For this reason, I would keep the Lambda function with the AppConfig extension.6. ConsiderationsIt's not only feature flags that we can configure in AppConfig. It's possible to store more complex configuration objects, too.As said above, we can have multiple feature flags for the same application and environment. If this is the case, we'll need a more complexChoicestate configuration, which can lead to harder-to-manage states. Alternatively,  Bob can write multipleifstatements in the code, one for each feature flag.7. SummaryAppConfig can store feature flags and other configurations we can use in our applications. With the help of the AppConfig Agent or the Lambda extension, we can fetch the feature flag from AppConfig. The extension follows the AWS-recommended flow of API calls and caches the feature flag.We can use Step Functions and incorporate different code versions based on the feature flag value into our application.8. Further readingCreating feature flags and free form configuration data in AWS AppConfig- Guide to create applications, environments and configuration profilesAWS AppConfig workshop- Get your hands dirtyGetting started with Lambda- How to create a Lambda functionInput and Output Processing in Step Functions- Data flow manipulation"}
{"title": "Vector Database solutions on AWS", "published_at": 1711592089, "tags": ["aws", "vectordatabase", "ia", "database"], "user": "Anderson Londo\u00f1o", "url": "https://dev.to/aws-builders/vector-database-solutions-on-aws-46f7", "details": "When talking about Vector Databases, in the market we can find the specialized ones and multi-model, most of the major database providers likeOracle,PostgreSQLorMongoDB, for mention some of them, have integrated a specific solution to retrieve vector data.The key concept isRetrieval Augmented Generation(RAG) and combined with Large Language Models (LLM), we can use models with data that is always changing. It is a use case to deploy a model trained with static data, for example the history of a store sales, but when the data is constantly changing you can give an external knowledge database to improve the response of the LLM.DB-Enginesoffers a complete index for databasesbased ontheir current popularity, mentions in social networks, frequency of search in Google Trends, frequency of discussion in technical foros and number of job offers, in which is mentioned.Why a Vector Database is needed ?The way the data is used for LLM's is in a vector representation. That means the prompt requested by the user goes to the vector database and search for the document with best similarity and return the results ranks to the LLM and personalize the prompt response.Which service can be used on AWS ?MongoDB from themarketplaceor directly fromAtlas Portal.RDS PostgreSQL with the extensionpgvector.Open Search with the new featureOpen Search Serverless.I am going to focus on pgvector and Open Search Services, this services are infrastructureless oriented, we don't need to care about a lot in administrative tasks with the infrastructure.Check this blog postBuilding AI-powered search in PostgreSQL using Amazon SageMaker and pgvector, if you want to use the extension inside RDS for PostgreSQL.And if you are looking to go Serverless, check this blog postVector engine for Amazon OpenSearch Serverless is now available"}
{"title": "Adicionando dom\u00ednio customizado a uma Lambda com o API Gateway", "published_at": 1711583007, "tags": ["lambda", "apigateway", "aws", "webdev"], "user": "Ricardo Mello", "url": "https://dev.to/aws-builders/adicionando-dominio-customizado-a-uma-lambda-com-o-api-gateway-1p5g", "details": "J\u00e1 tem tempos que eu n\u00e3o escrevo ent\u00e3o me desculpa porque perdi um pouco o jeito, mas hoje eu vou compartilhar algo super simples que eu nunca tinha feito: adicionar um dom\u00ednio customizado pra uma AWS Lambda.Se voc\u00ea nunca criou uma lambda, te recomendoesse post aquique eu acabei de criar.Agora, n\u00f3s vamos adicionar um dom\u00ednio customizado pra ela.Mas por que usar um dom\u00ednio customizado?E por que n\u00e3o usar? Parece que um gato sobe no teclado sempre que a gente gera um link de uma lambda. Links comohttps://dz4y7rvv2az4iopdqzvgneefhy0jdfii.lambda-url.us-east-1.on.awsouhttps://lh4j4sdpmljdcrcr34gqc6ddni0qvugg.lambda-url.us-east-1.on.awss\u00e3o gerados e ficam beeem feios quando usados no frontend.Plus, se voc\u00ea precisar apontar o seu link pra qualquer outro lugar (outra lambda, uma API, outra cloud...) ser\u00e1 muito mais tranquilo e voc\u00ea n\u00e3o vai ter que atualizar a url da lambda em milhares de lugares onde ela pode estar sendo utilizada.API GatewayPra atingir o nosso objetivo, vamos usar o API Gateway. Esse cara consegue organizar todas as nossas lambdas e criar mapeamentos pra elas definindo paths espec\u00edficos. Isso te permite usar a lambdanomesuperultragigante.lambda-url.us-east-1.on.awscomo um endpoint comumapi.meuwebsite.com/v1/lambda.Criando a API no API GatewayPra come\u00e7ar, vamos criar a API que vai mapear a lambda em um endpoint no api gateway. Para isso, ainda na tela do API Gateway, v\u00e1 emAPIs > Create API > REST API. Eu vou usar o nomefaker-apipra esse tutorial:Com a API criada, voc\u00ea deve ver essa tela:ResourcesNo API Gateway voc\u00ea pode realmente brincar de cria\u00e7\u00e3o de API. Em create resources voc\u00ea pode ir definindo os paths por exemplo/v1, depois/v1/domainat\u00e9 chegar em/v1/domain/endpoint. \u00c9 uma ferramenta muito poderosa que te permite mapear todas as rotas e integrar com lambdas, apis, ou qualquer outro servi\u00e7o.No nosso caso, eu vou tentar manter mais simples e criar um \u00fanico resource chamadopersons. Para isso, v\u00e1 emCreate Resource, d\u00ea o nomepersonspra ele, selecione a op\u00e7\u00e3o de CORS e clique emCreate Resource.Com o endpoint persons selecionado, v\u00e1 emCreate methode preencha as seguintes infos:Method type: GETIntegration type: Lambda functionEm lambda function: Selecione a nossa lambda de testeDepois de clicar em Create method, o resultado deve ser esse aqui:Clique emDeploy APIe crie um stage chamadodevelopment:Voc\u00ea deve ser redirecionado pra essa tela com o stage criado, e mais uma URL gigante:Criando o dom\u00ednioPra criar o nosso dom\u00ednio, vamos no painel doAPI Gateway ->  Custom domain names -> Create.Antes de come\u00e7ar, na configura\u00e7\u00e3o do endpoint, voc\u00ea vai precisar adicionar um certificado pro dom\u00ednio que voc\u00ea deseja criar. Clique emCreate a new ACM certificatee uma nova aba se abrir\u00e1.Na tela do Certificate Manager, clique emRequest a certificate -> Request a public certificate.Na tela que vai se abrir, insira o dom\u00ednio que voc\u00ea vai usar pra solicitar o certificado. No meu caso \u00e9 ofaker-api.ricmello.com, mas voc\u00ea pode adicionar mais de um dom\u00ednio ou um wildcard como*.ricmello.compra n\u00e3o precisar criar um certificado pra cada subdom\u00ednio.O m\u00e9todo de valida\u00e7\u00e3o recomendado pela pr\u00f3pria AWS \u00e9 o de DNS.Clique emrequeste depoisview certificate.Caso voc\u00ea j\u00e1 use o Route53, basta clicar emCreate records in Route 53e os registros ser\u00e3o criados automaticamente pra valida\u00e7\u00e3o do dom\u00ednio. Ou caso voc\u00ea use outro servi\u00e7o \u00e9 s\u00f3 criar um novo registro do tipo CNAME e colar os valores que v\u00e3o aparecer pra voc\u00ea como no print abaixo.Depois de configurar o DNS, o seu certificado vai estar dispon\u00edvel, mas isso pode demorar um pouco. Voc\u00ea pode usar a ferramentawhatsmydnspra ver se a sua configura\u00e7\u00e3o t\u00e1 certa e aguardar at\u00e9 o certificado ser emitido como esse aqui:Agora podemos voltar na cria\u00e7\u00e3o do dom\u00ednio no API Gateway e selecionar o certificado criado. No nome do dom\u00ednio eu preenchi o mesmofaker-api.ricmello.comque eu usei pro certificado:Clique emCreate domain namepara salvar e volte pra tela de configura\u00e7\u00e3o de dom\u00ednio. Voc\u00ea deve criar um CNAME apontando o seu dom\u00ednio pro API Gateway domain name, que no meu exemplo \u00e9 od-4ip5ycuy62.execute-api.us-east-1.amazonaws.com.Associando o dom\u00ednioNa tela de Custom domain names, v\u00e1 emAPI mappings > Configure API mappings > Add new mapping, e configure um mapeamento selecionando a sua API e o stage criado.Feito! O seu dom\u00ednio vai apontar pro api gateway e voc\u00ea vai conseguir utilizar a lambda pelo mapeamento como na imagem abaixo.E a\u00ed, curtiu? Tem alguma d\u00favida? Se tiver qualquer coisa que eu possa fazer pra tornar esse artigo melhor, pode reclamar, elogiar ou sugerir outro artigo. Manda ver nos coment\u00e1rios porque feedbacks s\u00e3o sempre super bem vindos."}
{"title": "Swift things with AWS Management Console", "published_at": 1711566310, "tags": ["aws", "services", "awsconsole", "cloudskills"], "user": "VijayaNirmalaGopal", "url": "https://dev.to/aws-builders/swift-things-with-aws-management-console-4bc2", "details": "When you have huge vision, goals, extensive dedication towards whatever your tasks/schedule, majority of the people, overlook into the minute details, which would be quite an essential & a life saver as well.Well, in this blog post, we will see few things from AWS Management Console, that are readily available for use but sometimes or some of them just realize the existence !!Swift Fact #1Let us say, you have AWS Management Console access and you access services as per your requirement. Either you get into the services' console via \"Recent\" or \"Typing the AWS Service\" name in the \"Search\" boxDid you know, we can mark as 'Favourite' and get them placed as icons on top of the AWS Management console and ready for us to access easily. Services, thus marked as 'Favourite' are also available under 'Favourites' menuSwift Fact #2Have you ever needed to do some clean up, general scripting, check our repo, a interface with basics installed(like npm, pip) or a platform to run CLI commands not just AWS CLI, but ECS CLI, SAM CLI, as well?Well, then use \"AWS Cloud Shell\" readily available for to be accessed & started from consoleBenefits of using AWS Cloud shell are1) No pricing attached for using AWS Cloud Shell2) No resources required to be up & running to use this shellSwift Fact #3There will be times when you need to talk to team in AWS, need some technical expertise or advice on your use case or so. In such cases, the support centre menu is the centralized location to access point of contacts for People & Resources. Let us see how !!Support CenterFrom the above screen shot, you can see \"Support Center\" options and in order to raise a request or concern regarding your accounts' billing or technical glitch, by selecting appropriate case, as in below screen shot.Team will reach out either online chat, via email or via phone as per your selection. Just keep watching this tab/space for further updates on the case/ticket raised with AWS.Reach out to AWS Experts/SMEsIf you have a project or use case, need assistance or expert opinion on that, then reach out to pool of AWS Certified pool of experts by posting the details of your question, requirement in AWS IQ platform link and get responses from SMEs & experts in any domain of interestTechnical GuidanceIrrespective of your cloud status, your code, service usage, application evolution will have errors unresolved, integration issues, services feature usage or implementation errors, reach out to team AWS for tested, validated answers to any of your questions, classified accordinglyAWS re:post is a platform to discuss technical or service related doubts/queries/integration issues/errors and resolution, which are authorized & validated responses that are reliableResources to look forYou can google & reach to AWS Documentation but yet can access Resources documentation, training or Resource centre from main \"Support\" MenuFeedback to AWSFinally, if you have feedback to AWS on issues/facts related to AWS feature, account billing or UI related suggestions, reviews, remarks, you can pass it on to TEAM AWS by filling the \"Feedback Form\" (refer below)Swift Fact #4Did you know that you can change \"AWS Management console\" text from English to a language of your choice?Visual settings to view in 'Dark' or 'Light' can also be set from above optionsSwift Fact #5Applications, users or usage will have to tracked & notified to teams/users, but there could be varied level of notifications delivered to chats or mobiles or emails. But have them all displayed under one window, under \"Notifications\" tabJust not notifications but cloud users can see \"Health of Services\" issues & updates, as well. How comforting is that to see notifications & health updates all in one tab of report?I believe that at least one among the above features will be of help to people using AWS Management Console. Good Luck !!"}
{"title": "Observability Maturity Model for AWS", "published_at": 1711563288, "tags": ["observability", "aws", "sre"], "user": "Indika_Wimalasuriya", "url": "https://dev.to/aws-builders/observability-maturity-model-for-aws-al3", "details": "In the fast-paced and complex landscape of Cloud-Native environments, achieving effective observability in AWS systems becomes challenging. The intricacies of distributed architectures often lead to undetected issues, hindering operational efficiency and compromising user experience.As part of this post, I will present to you an Observability maturity model that will take you on the transformative journey from \"Reactive to Autonomous Observability\" with the AWS-focused Maturity Model.Before we jump in, let's find out what the key pillars are for this model:Logs- Records of events and activities in your systems and applications. Useful for troubleshooting issues and auditing.Metrics- Quantitative data about performance and behavior over time. Help track trends and identify anomalies.Tracing- Follows a request as it flows through distributed systems. Used to analyze bottlenecks and errors.Alarms- Automated notifications when certain thresholds are breached. Help quickly identify and respond to issues.Dashboards- Visual representations of metrics, logs and other data. Provide at-a-glance views of system health.Canaries- Automated tests that run synthetic transactions to monitor availability and performanceReal User Monitoring- Captures performance from an end user perspective. Surfaces issues that affect users.Infrastructure Monitoring- Monitors the health and utilization of underlying resources like servers, databases, etc.Network Monitoring- Observes network connectivity and traffic to detect problems and optimize performance.Security Monitoring- Detection of security threats, anomalies and unauthorized activities.Cost Optimization- Tracking usage and spending to optimize costs.Based on above, i have created 4 stage of ObservabilityStage 1 -Reactive MonitoringStage 2 -Proactive ObservabilityStage 3 -Predictive ObservabilityStage 4 -Autonomous ObservabilityLet's delve into each stage in detail, comparing them with the pillars listed above.Pillars of AWS ObservabilityReactiveProactivePredictiveAutonomousLogs\u2022 Logs used for troubleshooting after incidents\u2022 Monitoring logs with alerts for abnormal patterns\u2022 Advanced analysis for trend prediction\u2022 Automated analysis, correlation, anomaly detectionMetrics\u2022 Basic collection, not actively monitored\u2022 Monitoring metrics with predefined thresholds\u2022 Advanced analytics for anomaly detection\u2022 Automated scaling, anomaly detection based on MLTracing\u2022 Tracing not implemented\u2022 Basic tracing for critical services\u2022 Distributed tracing for performance optimization\u2022 Automated tracing, root cause analysisCanaries\u2022 Canaries not utilized\u2022 Basic canaries for critical services\u2022 Advanced canaries for predictive insights\u2022 Self-adaptive canaries, automatic scalingReal User Monitoring (RUM)\u2022 RUM data not collected\u2022 Basic RUM data collection for user experience\u2022 Advanced analytics for predicting user behavior\u2022 Automated optimization based on RUM and analyticsInfrastructure Monitoring\u2022 Basic metrics collected, not actively monitored\u2022 Automated monitoring with alerts for deviations\u2022 Predictive maintenance and capacity planning\u2022 Self-healing infrastructure, automated scalingNetwork Monitoring\u2022 Network monitoring tools not implemented\u2022 Basic network monitoring for outages/performance\u2022 Advanced analytics for security threats\u2022 Self-adaptive network monitoring, dynamic configSecurity Monitoring\u2022 Security monitoring not implemented\u2022 Basic monitoring tools for known threats\u2022 Real-time threat detection, automated response\u2022 Autonomous security monitoring with AICost Optimization\u2022 Cost optimization not considered\u2022 Basic strategies based on manual analysis\u2022 Advanced optimization using automation/predictive\u2022 Fully automated cost optimization, CI/CD integratedNow that you have a framework in place, it's essential to start measuring overall improvements tied to business outcomes. Here are a few important recommendations:Define clear goals (e.g., reducing downtime, enhancing satisfaction).Track observability's impact over time.Focus on metrics like cost reduction, faster issue resolution.Set improvement targets for each maturity stage.Use data to quantify customer experience benefits.Relate maturity stages to enhanced customer service.Showcase how maturity accelerates innovation.Align observability with strategic customer-focused objectives.Secure executive buy-in by highlighting customer-centric results.There are a few best practices you cannot afford to ignore. Following these will make a big difference in your journey.Use CloudWatch for metrics, logs, and alarms. CloudWatch provides a centralized place for monitoring across AWS services.Enable enhanced monitoring for EC2, ELB, RDS, etc. The detailed metrics can help troubleshoot issues.Use X-Ray for distributed tracing. X-Ray helps trace requests across services and identify latency issues.Send application logs to CloudWatch Logs or third-party tools. Centralized logging is critical for debugging errors.Set up dashboards in CloudWatch. Pre-built and custom dashboards provide visibility into key metrics.This blog post is based on a presentation I delivered during Cloud Native 2024. If you're interested in listening to that with more details, please refer to the video below."}
{"title": "Monitor EC2 instance metrics with Datadog (step-by-step)", "published_at": 1711553264, "tags": ["datadog", "aws", "ec2", "beginners"], "user": "Esther Ninyo", "url": "https://dev.to/aws-builders/monitor-ec2-instance-metrics-with-datadog-step-by-step-3c35", "details": "Hi there,Ever thought of a straight forward way to monitor your EC2 instance metrics with Datadog but couldn't get a simplified solution? Look no further!The three phases to get this up and running are:Phase one: Enable the AWS integration in DatadogPhase two: Deploy the Datadog agent on your EC2 instancePhase three: Start creating your monitorsDatadog agent can be installed directly on your EC2 instances which gives you the ability to collect metrics such as memory, CPU, disk etc within a short period of time.To have a robust understanding of how this works, please visit theDatadog blog postfor more detail.Pre-requisite:To continue with this hands-on, make sure you have the following:EC2 instanceDatadog accountProject deep diveFor the scope of this project, we will be monitoring the following system-level EC2 metrics such as:High CPU UtilizationHigh Memory UtilizationHigh Disk UtilizationPHASE ONEThis phase consist of enabling the AWS integration in Datadog to allow monitoring of the EC2 instance.We will setup the Datadog integration using terraform. You can get ithere.Folder structure--> EC2 monitoring ------> provider.tf ------> main.tf ------> variables.tfEnter fullscreen modeExit fullscreen modemain.tfdata \"aws_caller_identity\" \"current\" {}  data \"aws_iam_policy_document\" \"datadog_aws_integration_assume_role\" {    statement {    actions = [\"sts:AssumeRole\"]     principals {       type = \"AWS\"       identifiers = [\"arn:aws:iam::${data.aws_caller_identity.current.account_id}:root\"]    }    condition {       test = \"StringEquals\"       variable = \"sts:ExternalId\"        values = [          \"${datadog_integration_aws.sandbox.external_id}\"       ]    }    } }  data \"aws_iam_policy_document\" \"datadog_aws_integration\" {    statement {         actions = [             \"ec2:Describe*\",             \"ec2:GetTransitGatewayPrefixListReferences\",             \"ec2:SearchTransitGatewayRoutes\"         ]     resources = [\"arn:aws:ec2:${var.region}:${data.aws_caller_identity.current.account_id}:instance/${var.instance_id}\"]    } }  resource \"aws_iam_policy\" \"datadog_aws_integration\" {    name = \"TutorialDatadogAWSIntegrationPolicy\"    policy = \"${data.aws_iam_policy_document.datadog_aws_integration.json}\" }  resource \"aws_iam_role\" \"datadog_aws_integration\" {    name = \"TutorialDatadogAWSIntegrationRole\"    description = \"Role for Datadog AWS Integration\"    assume_role_policy = \"${data.aws_iam_policy_document.datadog_aws_integration_assume_role.json}\" }  resource \"aws_iam_role_policy_attachment\" \"datadog_aws_integration\" {    role = \"${aws_iam_role.datadog_aws_integration.name}\"    policy_arn = \"${aws_iam_policy.datadog_aws_integration.arn}\" }  resource \"datadog_integration_aws\" \"sandbox\" {    account_id  = \"${data.aws_caller_identity.current.account_id}\"    role_name   = \"TutorialDatadogAWSIntegrationRole\" }Enter fullscreen modeExit fullscreen modevariable.tfvariable \"region\" {   type        = string   description = \"The AWS region to use.\"   default     = \"eu-west-1\" }  variable \"datadog_api_key\" {   type        = string   description = \"The Datadog API key.\"   default     = \"<REDACTED>\" }  variable \"datadog_app_key\" {   type        = string   description = \"The Datadog application key.\"     default     = \"<REDACTED>\" }  variable \"instance_id\" {   type        = string   description = \"EC2 instance ID.\"     default     = \"<REDACTED\" }Enter fullscreen modeExit fullscreen modeprovider.tfterraform {   required_version = \"~> 1.6\"   required_providers {     aws = {       source  = \"hashicorp/aws\"     }     datadog = {       source  = \"DataDog/datadog\"     }   } }  # Configure the AWS Provider provider \"aws\" {   region = var.region    default_tags {     tags = {       Environment = terraform.workspace,       ManagedBy   = \"Terraform\"     }   } }  # Configure the Datadog provider provider \"datadog\" {     api_key = var.datadog_api_key     app_key = var.datadog_app_key     api_url = \"https://api.datadoghq.eu\" }Enter fullscreen modeExit fullscreen modeGet datadog app key, api key and api urlGo to your datadog profile at the bottom left and click on organisation settings.Locate the navigation pane at the left (1), under access (2), click on application key (3) to create a new key. Also click on the api key (4) to create a new key to be used.Click onthis linkto access the api url depending the Datadog site you use. Replace app with api.WHAT NEXT?The next line of action is to initialise, plan and apply your terraform changes. To do this, use the command below in your folder home directory:terraform initterraform planterraform applyIf theterraform planis successful, you should see the resources that will be created after running terraform apply like the result below:terraform plan data.aws_iam_policy_document.datadog_aws_integration: Reading... data.aws_caller_identity.current: Reading... data.aws_iam_policy_document.datadog_aws_integration: Read complete after 0s [id=1400131043] data.aws_caller_identity.current: Read complete after 0s [id=134130342652]  Terraform used the selected providers to generate the following execution plan. Resource actions are indicated with the following symbols:   + create  <= read (data resources)  Terraform will perform the following actions:    # data.aws_iam_policy_document.datadog_aws_integration_assume_role will be read during apply   # (config refers to values not yet known)  <= data \"aws_iam_policy_document\" \"datadog_aws_integration_assume_role\" {       + id   = (known after apply)       + json = (known after apply)        + statement {           + actions = [               + \"sts:AssumeRole\",             ]            + condition {               + test     = \"StringEquals\"               + values   = [                   + (known after apply),                 ]               + variable = \"sts:ExternalId\"             }            + principals {               + identifiers = [                   + \"arn:aws:iam::<REDACTED>:root\",                 ]               + type        = \"AWS\"             }         }     }    # aws_iam_policy.datadog_aws_integration will be created   + resource \"aws_iam_policy\" \"datadog_aws_integration\" {       + arn         = (known after apply)       + id          = (known after apply)       + name        = \"TutorialDatadogAWSIntegrationPolicy\"       + name_prefix = (known after apply)       + path        = \"/\"       + policy      = jsonencode(             {               + Statement = [                   + {                       + Action   = [                           + \"ec2:SearchTransitGatewayRoutes\",                           + \"ec2:GetTransitGatewayPrefixListReferences\",                           + \"ec2:Describe*\",                         ]                       + Effect   = \"Allow\"                       + Resource = \"arn:aws:ec2:<REDACTED>:instance/<REDACTED>\"                     },                 ]               + Version   = \"2012-10-17\"             }         )       + policy_id   = (known after apply)       + tags_all    = {           + \"Environment\" = \"default\"           + \"ManagedBy\"   = \"Terraform\"         }     }    # aws_iam_role.datadog_aws_integration will be created   + resource \"aws_iam_role\" \"datadog_aws_integration\" {       + arn                   = (known after apply)       + assume_role_policy    = (known after apply)       + create_date           = (known after apply)       + description           = \"Role for Datadog AWS Integration\"       + force_detach_policies = false       + id                    = (known after apply)       + managed_policy_arns   = (known after apply)       + max_session_duration  = 3600       + name                  = \"TutorialDatadogAWSIntegrationRole\"       + name_prefix           = (known after apply)       + path                  = \"/\"       + tags_all              = {           + \"Environment\" = \"default\"           + \"ManagedBy\"   = \"Terraform\"         }       + unique_id             = (known after apply)     }    # aws_iam_role_policy_attachment.datadog_aws_integration will be created   + resource \"aws_iam_role_policy_attachment\" \"datadog_aws_integration\" {       + id         = (known after apply)       + policy_arn = (known after apply)       + role       = \"TutorialDatadogAWSIntegrationRole\"     }    # datadog_integration_aws.sandbox will be created   + resource \"datadog_integration_aws\" \"sandbox\" {       + account_id                       = \"<REDACTED>\"       + cspm_resource_collection_enabled = (known after apply)       + external_id                      = (known after apply)       + id                               = (known after apply)       + metrics_collection_enabled       = (known after apply)       + resource_collection_enabled      = (known after apply)       + role_name                        = \"TutorialDatadogAWSIntegrationRole\"     }  Plan: 4 to add, 0 to change, 0 to destroy.  \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500  Note: You didn't use the -out option to save this plan, so Terraform can't guarantee to take exactly these actions if you run \"terraform apply\" now.Enter fullscreen modeExit fullscreen modePHASE TWOThe second phase is to deploy the agent.Use the command below to install the agent on ubuntu server:DD_API_KEY=<API_KEY DD_SITE=<DATADOG_SITE> DD_APM_INSTRUMENTATION_ENABLED=host bash -c \"$(curl -L https://s3.amazonaws.com/dd-agent/scripts/install_script_agent7.sh)\"Enter fullscreen modeExit fullscreen modewhere:API_KEY = your Datadog api keyDATADOG_SITE = Datadog site. For this exercise, we use \"datadog.eu\".Depending on the operating system you use, navigate tothis siteto get the command for installing Datadog agent.After the Datadog agent agent has been installed, go to your Datadog account, navigate to metrics and you will start to see the reports of your EC2 metrics in Datadog like the image below:PHASE THREEIn this phase, we will create monitors for our EC2 instance for the metrics listed at the beginning of this tutorial.A. HIGH CPU UTILISATIONOn the monitors page in Datadog, on the top right, click on new monitorClick on metrics and configure your monitorThe image below shows the configuration needed to monitor your EC2 cpu utilisationYour monitor should look like this after creation:To understand each options used in creating the monitor, clickhereB. HIGH MEMORY UTILISATIONYour monitor should look like this after creation:C. HIGH DISK UTILISATIONConclusionI hope you are able to follow through and also are able to create the Datadog monitors for your metrics. Do you have any question? Please send it my way. Kindly follow me onLinkedIn."}
{"title": "Trigger AWS Lambda with TCP Traffic + Static IP Address", "published_at": 1711550818, "tags": [], "user": "Raphael Jambalos", "url": "https://dev.to/aws-builders/trigger-aws-lambda-with-tcp-traffic-static-ip-address-1bha", "details": "In this post, I will walk you through how to trigger AWS Lambda with TCP traffic. By the end of the post, we would have created a setup similar to the diagram below:First question is WHY?In my case, it is because the client application that I was working with can only send requests via TCP. I needed to supply a stable IP address and a port number. They couldn't really update the client to HTTP because it was a legacy system.Given a choice though, it is best to just use HTTP traffic so you can connect with Lambda via API Gateway. Using this approach comes with advantages:You can use API Gateway's routing capabilities to route traffic to small Lambda functionsThe setup is more durable because API Gateway is a managed service. As you'd see later, we would have to create a TCP server to route traffic to Lambda + APIGW. And that can be a single point of failure.FACT: Lambda cannot be directly triggered with TCP traffic.When sending TCP requests, you need the IP address and the port number of the target. Because Lambda functions are only spawned when they are triggered by an event, they are only given an ephemeral IP address and port number. Sending TCP requests to Lambda directly implies you are made aware of the IP address and port number of each invocation of Lambda. And so far, AWS has no feature that makes this possible.If you are looking to deploy an application in Lambda to receive and process TCP trafficdirectly, this post unfortunately cannot help you. At this time of writing, this is not technically feasible yet. I suggest you just use ECS, EC2 or EKS to do this.But if you are looking to find a way for a TCP client to communicate with Lambda by converting TCP traffic to HTTP, then this post is for you!Now on the solution(1)Following the diagram above, we first provision an EC2 instance. To follow with the examples here, I suggest you choose a Linux instance (i.e Ubuntu or Amazon Linux 2).(2)Install Python 3(3)Create a Python virtual environment and install the requests package.python3-mvenv venvsourcevenv/bin/activate  pipinstallrequests pip freeze>requirements.txtEnter fullscreen modeExit fullscreen mode(4)Create a file calledtcp_server.pyand copy paste the contents of thisGitHub Snippetto the file.(5) Set the environment variables and run the TCP server.exportTCP_IP_ADDRESS_BIND=0.0.0.0exportTCP_PORT_NUMBER=8108exportTCP_SERVER_TIMEOUT=20exportAPIGW_BASE_URL=\"<YOUR APIGW ROUTE>\"chmod+x tcp_server.py python tcp_server.pyEnter fullscreen modeExit fullscreen modeYou should see this in your terminal:(6) To test your setup, create another Python script by copying the contents ofthis second Github Gistto a file calledtcp_client.py. This gist sends a TCP request to the TCP web server we have started.(7) Next, execute these commands to ping the TCP server.exportTCP_SERVER_IP_ADDRESS=0.0.0.0exportTCP_PORT_NUMBER=8108  python tcp_client.pyEnter fullscreen modeExit fullscreen modeIn the screencap below, we see that we have received a response from our TCP web server.What's next?This solution is a POC that demonstrates that we can receive TCP traffic and route it to Lambda. The next step is to run this script inside an EC2 instance and expose the port and IP address of the EC2 instance.For low-traffic conditions, this setup is okay. But with increased traffic, we have to start thinking about how this would all scale automatically when there is an increased load. For this, we recommend that the TCP server is hosted in a Docker container, orchestrated by either ECS, EKS, or a custom Kubernetes cluster.The full repo is foundherePhoto byTaylor VickonUnsplash"}
{"title": "A Simple Way to Sign AWS Requests with Signature V4", "published_at": 1711544920, "tags": ["aws", "javascript", "typescript", "webdev"], "user": "Chris Cook", "url": "https://dev.to/aws-builders/a-simple-way-to-sign-aws-requests-with-signature-v4-9eo", "details": "In aprevious article, I explored the challenges of using Lambda Function URLs with IAM authorization and CloudFront custom domains. A key aspect of this setup involves signing HTTP requests with AWS Signature Version 4 (SigV4) to authenticate with IAM.While the AWS SDK provides utilities for SigV4 signing, the process can be somewhat cumbersome, especially when working with the barebone functions likefetchinstead of SDKs. To simplify this process, I've created theaws-sigv4-fetchpackage, which automatically signs fetch requests with SigV4 for a given AWS service and region.What is AWS Signature Version 4?AWS Signature Version 4 (SigV4) is a process for adding authentication information to AWS API requests sent over HTTP. For security reasons, most requests to AWS must be signed with an access key, which consists of an access key ID and a secret access key (your AWS credentials).The SigV4 signing process involves creating a canonical request based on the HTTP request details, calculating a signature using your AWS credentials, and adding this signature to the request as anAuthorizationheader. AWS then replicates this process and verifies the signature, granting or denying access accordingly.For a more detailed explanation of SigV4, refer to myprevious articleor theAWS documentation.Sign All RequestsTheaws-sigv4-fetchpackage aims to simplify the SigV4 signing process for modern JavaScript applications. It exports a single function,createSignedFetcher, which returns afetchfunction that automatically signs HTTP requests with SigV4 for the specified AWS service and region.Here's an example usage:import{createSignedFetcher}from'aws-sigv4-fetch';constsignedFetch=createSignedFetcher({service:'execute-api',region:'eu-west-1'});consturl='https://restapi.execute-api.eu-west-1.amazonaws.com/foo/bar';constresponse=awaitsignedFetch(url);constdata=awaitresponse.json();Enter fullscreen modeExit fullscreen modeIn this example, we create asignedFetchfunction that automatically signs requests to API Gateway in theeu-west-1region. We can then use this function like a regularfetch, passing in the URL and request options. Theaws-sigv4-fetchpackage will handle the SigV4 signing process behind the scenes, adding the necessaryAuthorizationheader to the request.ThecreateSignedFetcherfunction accepts an optionalfetchargument, allowing you to pass in a customfetchimplementation (e.g., a polyfill likecross-fetch). If nofetchis provided, it defaults to the nativefetchfunction which is available in Node.js since v18.ESM and CommonJS SupportTheaws-sigv4-fetchpackage is available on npm and can be installed with your preferred package manager:npminstallaws-sigv4-fetchEnter fullscreen modeExit fullscreen modeThe package supports both ES Modules and CommonJS, so you canimportorrequireit as needed:// ESMimport{createSignedFetcher}from'aws-sigv4-fetch';// CommonJSconst{createSignedFetcher}=require('aws-sigv4-fetch');constsignedFetch=createSignedFetcher({service:'appsync',region:'eu-west-1'});Enter fullscreen modeExit fullscreen modeIntegration with GraphQL LibrariesTheaws-sigv4-fetchpackage can be integrated into GraphQL libraries likegraphql-request. For example, you can pass thesignedFetchfunction as the customfetchoption:import{createSignedFetcher}from'aws-sigv4-fetch';import{GraphQLClient}from'graphql-request';constsignedFetch=createSignedFetcher({service:'appsync',region:'eu-west-1'});constclient=newGraphQLClient('https://graphqlapi.appsync-api.eu-west-1.amazonaws.com/graphql',{fetch:signedFetch,});constresult=awaitclient.request(query,variables);Enter fullscreen modeExit fullscreen modeWith this setup, all GraphQL requests made through theclientwill be automatically signed with SigV4.ContributionsI am in the process of adding E2E tests for AWS services. So far only API Gateway and IAM are covered by tests with real resources. If you are using a specific AWS service and want to make sure thataws-sigv4-fetchalways works with that service, I would greatly appreciate your contribution. Of course, feedback and improvements are always welcome.zirkelc/aws-sigv4-fetchAWS SignatureV4 fetchaws-sigv4-fetchAWS SignatureV4 fetch API function to automatically sign HTTP request with given AWS credentials. Built entirely on the newest version of the officialAWS SDK for JS.Signature Version 4Signature Version 4 (SigV4) is the process to add authentication information to AWS API requests sent by HTTP. For security, most requests to AWS must be signed with an access key. The access key consists of an access key ID and secret access key, which are commonly referred to as your security credentialsAWS documentation on Signature Version 4 signing process\u26a0\ufe0fESM SupportSince v3, this package ships with ESM and CommonJS support. That means you canimportorrequirethe package in your project.// ESMimport{createSignedFetcher}from'aws-sigv4-fetch';// CommonJSconst{createSignedFetcher}=require('aws-sigv4-fetch');Enter fullscreen modeExit fullscreen modeInstallnpm install --save aws-sigv4-fetch  yard add aws-sigv4-fetch  pnpm add aws-sigv4-fetchEnter fullscreen modeExit fullscreen mode\u2026View on GitHub"}
{"title": "DevOps with Guruu | Chapter 12 : Real Project DevOps 50 USD | Deploy Web on AWS | Upwork", "published_at": 1711544317, "tags": ["webdev", "devops", "aws"], "user": "Hoang Guruu", "url": "https://dev.to/aws-builders/devops-with-guruu-chapter-12-real-project-devops-50-usd-deploy-web-on-aws-upwork-l50", "details": "DevOps with Guruu | Chapter 12 : Real Project DevOps 50 USD | Deploy Web on AWS | Upwork\"Hello every future DevOps! Welcome to my series 'DevOps with Guruu. You can call me by Hoang Guruu or Guruu, and today, we will work with Real project DevOps, I will go to Upwork and find some jobs I feel suitable for our project today.  \"Join me on this journey to mastering DevOps. Let's get hands-on and build a strong foundation together! Don't forget to like, share, and subscribe for more exciting content. Happy coding, future DevOps!\""}
{"title": "Data Streaming Architecture", "published_at": 1711509597, "tags": ["aws", "bigdata", "architecture"], "user": "Jose Luis Sastoque Rey", "url": "https://dev.to/aws-builders/data-streaming-architecture-32j0", "details": "In a previous post, we studied thedata streaming architecture basics, now we are going to set up AWS services to enable speed layer capabilities to ingest, aggregate, and store the streaming data. Each AWS service belongs to one or many stages of the data pipeline depending on their capabilities.Kinesis Data StreamKinesis Data Stream is a serverless service with high throughput to ingest fast and continuous streaming data in real-time, it uses a shard to receive and store temporarily the data record in a unique sequence. A shard can support up to 1,000 RPS or 1 MB/sec writes and 2,000 RPS or 2 MB/sec read operations. The number of shards depends on the amount of data ingested and the level of throughput needed, more details about the capacity arehere.Go to Kinesis services and create a data stream using the on-demand capacity.awsmeter JMeter PluginTo produce streaming data we are going to useawsmeter, it is a JMeter plugin that uses AWS SDK + KPL (Kinesis Producer Library) to publish messaging on shards. You need an AWS access key and a data stream name. Find more detailshere.The structure of this example message is usingCloudEventsspecification:{\"specversion\":\"1.0\",\"type\":\"kinesis-data-stream\",\"source\":\"https://github.com/JoseLuisSR/awsmeter\",\"subject\":\"event-created\",\"id\":\"${eventId}\",\"time\":\"${time}\",\"datacontentcoding\":\"text/xml\",\"datacontenttype\":\"text/xml\",\"data\":\"<much wow=\\\"xml\\\"/>\"}Enter fullscreen modeExit fullscreen modeS3Use S3 to store streaming data and query the data with S3 Select. It is integrated natively with Kinesis Firehose and is a fully managed and regional service. Create a S3 bucket.Kinesis FirehoseIt is a streaming ETL solution to capture, transform, and load stream data into AWS data stores. Kinesis Firehose is a serverless service, fully managed by AWS, and automatically scales to support the throughput you need. To create a delivery stream you choose the source and destination.For source choose Kinesis Data Stream, in the source setting section search the kinesis data stream you created.For Destination choose S3, in the destination setting section search S3 bucket you created.Enable Dynamic partitioning for efficient query optimization.Enable Inline parsing for JSON to use the Kinesis Data Firehose built-in support mechanism, a jq parser, for extracting the keys from messages for partitioning data records that are in JSON format. Specify the key name and JQ expression as below:In the S3 bucket prefix section, chooseApply dynamic partitioning keysto generate partition key expressions. To enable Hive-compatible style partitioning by type and source, update the S3 bucket prefix default value with type= and source=.In the S3 bucket error output prefix box, enter kdferror/. It will contain all the records that the Kinesis Data Firehose is not able to deliver to the specified S3 destination.In the delivery stream setting section, expand the Buffer hints, compression, and encryption section.For Buffer size, enter 64.For Buffer interval, enter 60.Kinesis Data Firehose buffers incoming streaming data to a certain size and for a certain period before delivering it to the specified destinations (S3). For a delivery stream where data partitioning is enabled, the buffer size ranges from 64 to 128MB, with the default set to 128MB, and the buffer interval ranges from 60 seconds to 900 seconds.To Kinesis Firehose can access S3 need to use IAM roles with the permissions needed, on the advance settings choose existing IAM roleReview the stream configuration, and then choose Create delivery streamS3 SelectCheck your S3 Bucket to see if the data is stored on the folders of type and subject that are the dynamic partitions we configured. Choose one file, go to actions, and select theQuery with S3 Selectoption.ConclusionIn this post you have provisioned and set up aws services to enable a data streaming architecture solution, you have done the following:Created Kinesis Data Stream.Set up awsmeter to generate streaming messages.Created a Kinesis Data Firehose stream and connected the Kinesis data stream to it.Configured dynamic partitioning on the Kinesis Data Firehose delivery stream.Delivered data to Amazon S3."}
{"title": "Data Streaming Architecture Basics", "published_at": 1711509564, "tags": [], "user": "Jose Luis Sastoque Rey", "url": "https://dev.to/aws-builders/data-streaming-architecture-basics-52d7", "details": "In this post, we will understand the6'V of Big Data, review theData PipelineandLambda architectureto understand the complexity of getting, storing, and processing the data, and then set up AWS services to ingest and store streaming data to perform real-time analytics. Let's start.6'V of Big DataThe amount of data, the speed of produced data, and the diversity of data are common in the systems today, data is everywhere and we need to choose the right data sources to get the most accurate and valuable data. The 6V\u2019s of Big Data shows us the challenges we need to face when creating Data Streaming Architecture with cost optimization and performance efficiency:Volume: Gigabytes, Terabytes, Petabytes, and more are the amount of data we need to receive, store, and process.Velocity: The data is produced all day, Eg150Kvideos are uploaded to YouTube every minute, and66Kphotos are shared on Instagram every minute. The data is coming at scheduled hours (batch), real-time (streaming), or both.Variety: The data are files, fields, text, audio, video, and images. The data is structured, semi-structured, and unstructured.Veracity: With many data sources the data could be incomplete, outdated, repeated, or not real, the quality of the data is essential to get the right insights.Variability: The data could change, depending on the seasons, geopolitics events, or just add a new data source. You need to validate how often the structure or shape of your data changes and the side effect is the meaning the data changes also.Value: The main purpose is to give value, describing what is happening, what could happen, and the opportunities identified. The data insights enable organizations to become data-driven.Data PipelineIt is the building block for analytics solutions, it defines the layers and capabilities needed to optimize the ingestion, transformation, and storage of your analytics system. The layers are:Data sources:Include databases, system logs, IoT signals, tape disks, and other kinds of storage systems with data related to your business. The data is structured, semi-structured, and unstructured.Ingestion, move the data from external data sources to another location using tools like ETL, SDK, or middlewares, the tools depend on data type and workload requirements.Storage, the data is stored temporarily or persistently in databases or object storage. The data is stored with format, partitioning, and compression for efficient storage and optimized querying.Processing, to get performance efficiency and cost optimization, the data is cataloged for indexing and search, then processed to clean, complete, anonymization and enrich, and finally control access to enable confidentiality and integrity of the data.Analytics and visualizationprovide descriptive and predictive analytics for discovering patterns and insights in data. This stage provides business decision-makers with graphical representations of analysis, making it easier to see the implications of the data.Lambda architectureIt is not a lambda function, it is a data-processing architecture designed to handle massive quantities of data coming from batch and real-time streams, and serving the data for user queries. This is a layered architecture to distribute the responsibilities and load to get better latency, throughput, and fault tolerance, the layers are.Batch, all the data sources with historical data or transaction information with restrictions to get data in real-time are in this layer with ETL and execute a map-reduce programming model to filter and sort information and reduce (summarize) the data using a distributed and parallel system.Speed, most recent information like events that occurred, online transactions, and streaming data are in this layer with capabilities to aggregate, partition, and compress the data in near real-time. The data could not be accurate or complete as a batch layer but it is available almost immediately.Serving, the information from batch and speed layers are joined and stored on this layer to enable analytics tools to do predictive and prescriptive analytics executing queries over precomputed views.In the next post, we will set upKinesis Data Stream + Kinesis Firehose + S3to ingest and store streaming data to perform real-time analytics."}
{"title": "Journey for writing my second book about cloud security", "published_at": 1711479493, "tags": ["aws", "tutorial", "learning", "cloud"], "user": "Eyal Estrin", "url": "https://dev.to/aws-builders/journey-for-writing-my-second-book-about-cloud-security-968", "details": "My name is Eyal, and I am a cloud security architect.In my spare time, I promote cloud adoption, by sharing knowledge, writing blog posts, and from time to time, conducting lectures about various aspects of cloud services.In 2022, I published my first book \u2013Cloud Security HandbookbyPACKT Publishing, which provides knowledge about cloud security services, for anyone who uses AWS, Azure, or GCP.In May 2023, I began considering writing my second book.To write a book, you need to contact a book publisher and submit a book proposal, and if everyone agrees on the contract terms, you will sign a contract with the book publisher, and begin writing chapters, according to an agreed time frame.In June 2023, I was approached byBPB Publications, with an offer to write a book about \"Cloud Auditing\", and since this topic is not my specialty, we scheduled a meeting and I was offered another opportunity to write a book about security in cloud-native applications.Writing processFrom my personal view, cloud-native applications are considered one of the hottest topics in the IT and development domain.Although I come from an infrastructure security background, I considered the offer for writing a book, as my next professional challenge.It requires me to do research while writing the book, conduct experiments in different services from AWS, Azure, and GCP, read vendors' documentation, understand best practices, and more.It took me out of my comfort zone or things I did and experienced in the past, such as writing short samples of code, creating test labs for Kubernetes, etc.Writing blog posts, comes naturally for me, meaning, once I find a topic, I believe people are missing information about, combined with a muse for writing (something difficult to catch), I\u2019m able to complete 3-5 pages of a blog post, easily.Writing an entire book (200+ pages) is a challenging process.You should align to a predefined schedule, you need to think in advance about the number of chapters, topics, and headlines, and begin filling each chapter with content.In this book, I put a lot of effort into creating diagrams, to make the topics easy to understand by the readers.Another thing I did to make the book practical, was to find and embed code samples. Some of the code samples were full (and I had to test them, to make sure that are fully functional), and some were just partial code samples (which I added a note for the readers).My goal, same as I had in the first book, was to write a practical book \u2013 something that a reader can refer to, everything they stumble on a topic that they are looking for more information, best practices, etc.On average, my schedule was to deliver a new chapter every 2-3 weeks, depending on the length of the chapter, so most of my work was usually during weekends since I am working a full-time job.Working on my second book, was pretty much like the work I did on my first book.Usually, book publishers have their template for writing the chapters (in terms of fonts, headlines, etc.)You duplicate the original template for the first chapter you write, and then, when you move on to the next chapter, you can duplicate the previous chapter, adjust the title and headlines, clean everything else, and begin the hard work of writing a new chapter.When I prepared the book proposal, I had to imagine in my head what topics would appear in each chapter, so the next thing I usually do, is to begin researching the topic \u2013 investing time in searching for references in the vendor's documentation, blog post, etc.For this specific book, since it focuses on applications, I also took the time to search and adjust code samples, deploy test labs, try some open-source tools to be able to take some screenshots and make sure all code samples are fully functional, and if adjustments are required, I would add notes below the code samples.As always, when you write a book talking about the technical aspects of three cloud providers, you find out that even though in most cases they are trying to achieve the same goal, or to have the same capabilities, there are many examples that each cloud provider implements its services differently, or that their services' capabilities are different.Reviewal processOnce I completed writing each chapter, I used to submit my draft for review by a content development editor, who used to review my draft, make grammar and some other design adjustments, and send me a copy of the draft to accept or reject the changes.After I finish accepting the changes to the draft, the draft is sent to a technical reviewer, to add his comments and recommendations for fixing some of the content, or adding some additional content, until finally the work on each chapter is completed.Working with a technical reviewer requires attention to his comments. You need to fully understand the comments and make a decision whether you need to make adjustments or keep things the way you wrote them.Luckily, the technical reviewer was a colleague of mine, so I was able to chat with him directly, understand his comments, and make sure everything I write is correct and all code samples were working.One last thing about the review process \u2013 once I received the final drafts of each chapter, I wanted to make sure everything would look perfect \u2013 meaning, no spelling mistakes, and no paragraph or code samples break between pages, to make it easy for the readers.Making things practicalSince technology keeps evolving, during the technical review process, I found out that some of the cloud provider\u2019s service names were changed over time, some capabilities were deprecated and some of the commands I wrote in the book (after testing them on my environment), were not working, and I had to replace them and test them again before publishing the book.Since the book focuses on technical materials, and since the technology discussed in the book keeps evolving, whenever I get a new draft of my chapters, I try to make sure that if new features are released or if service names are changed, I will be able to update the content, before the final copy of each chapter is approved, and before the book is released.Side projectsOther than the actual work on research, practice, and writing content for the book chapters, I had another challenge \u2013 before committing to writing my second book, I volunteered to be a technical reviewer for three other books.During my writing process, one of the books I was a technical reviewer for, was released -Practical Cybersecurity Architecture.Another thing I didn\u2019t want to limit myself and my creativity, was finding spare time to write blog posts, in parallel to writing the book. It requires the time for research on a topic I wanted to write about. During the time of working on my book, I managed to find time (and muse) to write 16 blog posts on various topics, that I believed required writing about (since in many cases, they were not getting enough attention from my point of view).SummaryI am looking at writing a book as a journey or something I chose to do, and I enjoyed it.Sure, not all the time I have the muse to complete another chapter. Sometimes I had to research and deploy test labs to gain hands-on experience with the services I wrote about, and there were cases where I had to sit for several days and figure out why the vendor\u2019s documentation didn\u2019t work in my test lab, as I expected. At the end of the day, it was a good practice for me and a good educational experience.Now that the work on the book is completed, I will return to writing blog posts, and probably soon, once I find a topic I care about, I will begin another project of writing my next book.If you are interested in learning how to secure cloud-native applications, my book \"Security for Cloud Native Applications\" is available for purchase on Amazon bookstore:https://www.amazon.com/dp/B0CYYFNSSQAbout the AuthorEyal Estrin is a cloud and information security architect, and the author of the booksCloud Security Handbook, andSecurity for Cloud Native Applications, with more than 20 years in the IT industry. You can connect with him onTwitter.Opinions are his own and not the views of his employer."}
{"title": "Secure Proxy Server in AWS", "published_at": 1711478432, "tags": ["aws", "cdk", "squid", "proxy"], "user": "Felipe Malaquias", "url": "https://dev.to/aws-builders/secure-proxy-server-in-aws-3cfc", "details": "Securely access third-party content with whitelisted IP from wherever you are.If you want to jump directly to the solution using CDK, gohere.Proxy servers are used for several purposes, and in general they provide a gateway between users and the destination they want to access.In this example, we will tackle the scenario where you want to access third-party content protected by a firewall that can only be accessed from specific white-listed IPs wherever you are.Well, I\u2019m sure there are a couple of ways to solve it, but here I\u2019ll describe a solution that doesn\u2019t depend on increasing too much costs and complexity in your infrastructure, like dealing withVPN clientsorDirect Connectlinks, while not compromising security.Although in generalNAT Gatewaysshould be avoided, as they might incur additionalunnecessary costswhen outbound connectivity from private subnets could be solved differently (e.g., by usingVPC endpointsorIPv6 egress-only internet gateways), in this case, I opted to configure myVPC with a private subnet with NAT Gatewayso I would have enough flexibility at my side to control how I want to proxy the requests to the third party service while still maintaining a fixed small list of public IPs which would be whitelisted at the third part service (e.g.: 3elastic IPsfrom NAT Gateway, one for eachAZ).For the proxy server, I chose to useEC2withSquid Cacheas, in this case, I wanted to keep things simple while I also didn\u2019t need4 9s availabilityfor this server, and if I ever need it, I can restart it or quickly spin up a new one. Of course, if you want to go cheaper, you might also considerspot instancesor setfunctions for starting and stopping the instancewhen you need it, or even do it manually if it\u2019s a once-in-a-while usage.A couple of important things about the EC2 setup:Do not assign a key pair for login (it is not needed; generally, having long-living keys lying around is a security risk).Assign the EC2 instance to a private subnet to reduce exposure to the public internet.Use a security group with no INGRESS rules (we will useSSM VPC endpointsandinstance connectto access it instead).Ensure that EGRESS TCP is enabled for all (or restrict it to ephemeral ports used to communicate with AWS services and the services you want to allow your proxy access).Make sure you assign the \u2018AmazonSSMRoleForInstancesQuickSetup\u2019 IAM instance profile (or a custom one with the same permissions) to it.Use Amazon Linux 2 or newer AMI (check which AMIs support instance connect)Add a \u2018Name\u2019 tag with a value of \u2018proxy-server\u2019, for example, so you can easily automate the tunnel creation later with a script.Use the following user data for installing and starting squid-cache, instance connect and SSM agent:#!/bin/bash  yum update -y -q  sudo yum install ec2-instance-connect sudo systemctl enable amazon-ssm-agent sudo systemctl start amazon-ssm-agent  sudo yum -y install squid  sudo service squid restartEnter fullscreen modeExit fullscreen modeTo access the EC2 instance, we will use instance connect. As the EC2 is in a private subnet, we need to create the following VPC Endpoints to be able to access it:ssm.region.amazonaws.comssmmessages.region.amazonaws.comec2messages.region.amazonaws.comFinally, you need a role you will assume for creating a tunnel and port forwarding to your proxy server, through a temporary ssh session started by SSM. This role must have the following permissions:{   \"Version\": \"2012-10-17\",   \"Statement\": [     {       \"Sid\": \"\",       \"Effect\": \"Allow\",       \"Action\": [         \"ssm:StartSession\",         \"ec2-instance-connect:SendSSHPublicKey\"       ],       \"Resource\": [         \"arn:aws:ec2:*:*:instance/*\"       ],       \"Condition\": {         \"StringEquals\": { \"aws:ResourceTag/Name\": \"proxy-server\" }       }     },     {       \"Sid\": \"\",       \"Effect\": \"Allow\",       \"Action\": [         \"ssm:StartSession\"       ],       \"Resource\": [         \"arn:aws:ssm:*:*:document/AWS-StartSSHSession\"       ]     },     {       \"Sid\": \"\",       \"Effect\": \"Allow\",       \"Action\": [         \"ssm:TerminateSession\",         \"ssm:ResumeSession\"       ],       \"Resource\": [\"arn:aws:ssm:*:*:session/$${aws:username}-*\"]     },     {       \"Sid\": \"\",       \"Effect\": \"Allow\",       \"Action\": [         \"ec2:DescribeInstances\"       ],       \"Resource\": \"*\"     }   ] }Enter fullscreen modeExit fullscreen modeThe statements above give permissions for describing all EC2 instances in your account, sending SSH public keys, and starting and terminating sessions on EC2 instances with the \u2018proxy-server\u2019 name.On your computer, to start the session and create the tunnel, you need to install and configure the aws-cli and thesession manager plugin for aws-cli.Finally, you can create the tunnel with the following script:FORWARDED_PORT=3128 AWS_REGION=<<your AWS region>>  ec2_instance_id=$(aws ec2 describe-instances \\   --filters Name=tag:Name,Values=proxy-server Name=instance-state-name,Values=running \\   --output text --query 'Reservations[*].Instances[*].InstanceId')  ec2_az=$(aws ec2 describe-instances \\                 --filters Name=tag:Name,Values=proxy-server Name=instance-state-name,Values=running \\                 --output text --query 'Reservations[*].Instances[*].Placement.AvailabilityZone')   echo \"Generating temporary keys\"  TMP=$(mktemp -u key.XXXXXX)\".pem\"  ssh-keygen -t rsa -f \"$TMP\" -N \"\" -q -m PEM  aws ec2-instance-connect send-ssh-public-key \\   --region ${AWS_REGION} \\   --instance-id ${ec2_instance_id} \\   --availability-zone ${ec2_az} \\   --instance-os-user ec2-user \\   --ssh-public-key \"file://$TMP.pub\"  ssh -i $TMP \\       -Nf -M \\       -L ${FORWARDED_PORT}:localhost:${FORWARDED_PORT} \\       -o \"UserKnownHostsFile=/dev/null\" \\       -o \"StrictHostKeyChecking=no\" \\       -o IdentitiesOnly=yes \\       -o ProxyCommand=\"aws ssm start-session --target %h --document AWS-StartSSHSession --parameters portNumber=%p --region=eu-central-1\" \\       ec2-user@${ec2_instance_id}  rm $TMP \"$TMP.pub\"Enter fullscreen modeExit fullscreen modeBefore running the script above, ensure your aws-cli sso is properly configured to access the account you deployed your proxy server to.The script above will create a temporary key and upload it to the ec2 instance, enabling you temporary access to this instance with this key. The key is automatically removed after 60 seconds, and if you haven\u2019t accessed your ec2 instance with that key during that period, you\u2019d need to create and send a new one. After the key is sent, a port forwarding proxy is created on port 3128. Finally, you can access the third-party content through your proxy by using localhost:3128, as in the curl example below:curl -x \"127.0.0.1:3128\" \"http://httpbin.org/ip\"Enter fullscreen modeExit fullscreen modeTo destroy the tunnel, you may execute the following command:lsof -P | grep ':'${FORWARDED_PORT} | awk '{print $2}' | xargs kill -9Enter fullscreen modeExit fullscreen modeThat\u2019s it. Consider fine-tuning /etc/squid/squid.conf to secure it even further against misuse.Example with CDK is available ongithub."}
{"title": "Kubernetes All in One | Basic Necessary Skill for DevOps", "published_at": 1711467708, "tags": ["webdev", "devops", "kubernetes", "aws"], "user": "Hoang Guruu", "url": "https://dev.to/aws-builders/kubernetes-all-in-one-basic-necessary-skill-for-devops-16fd", "details": "Kubernetes All in One | Basic Necessary Skill for DevOpsJoin me on this journey to mastering DevOps. Let's get hands-on and build a strong foundation together! Don't forget to like, share, and subscribe for more exciting content. Happy coding, future DevOps!\""}
{"title": "Issue 37 of AWS Cloud Security Weekly", "published_at": 1711459587, "tags": ["security", "aws", "cybersecurity", "newsletter"], "user": "AJ", "url": "https://dev.to/aws-builders/issue-37-of-aws-cloud-security-weekly-4fdm", "details": "(Summary of Issue 37 of AWS Cloud Security weekly @https://aws-cloudsec.com/p/issue-37)What happened in AWS CloudSecurity & CyberSecurity last week March 18-25, 2024?Amazon WorkMail now offers Audit Logging functionality, enabling users to gain insights into their mailbox access behaviors- including logs for authentication, access control, and mailbox access via Amazon CloudWatch Logs, Amazon S3, and Amazon Data Firehose. Additionally, CloudWatch will furnish new mailbox metrics for WorkMail organizations. This feature empowers administrators to investigate instances where users encountered issues accessing their mailbox, pinpoint the IP addresses associated with specific mailbox accesses, and identify actions such as moving or deleting mailbox data. Administrators can establish alarms to notify them when authentication or access failures surpass predetermined thresholds, as well as tailor processing for the logs, which are delivered as JSON recordsAWS Secrets Manager now allows you to to generate and rotate credentials for Amazon Redshift Serverless- making it easier to establish and automate credential rotation for their Amazon Redshift Serverless data warehouse.In the AWS GovCloud (US) regions, Amazon Kinesis Data Streams now supports resource-based policies. This enables you to, for example, process data ingested into a stream in one account using an AWS Lambda function in another account.Amazon EMR Serverless has expanded its coverage to include FedRAMP Moderate compliance in the US East (Ohio), US East (N. Virginia), US West (N. California), and US West (Oregon) Regions. This means that you can utilize EMR Serverless to execute Apache Spark and Hive workloads while adhering to FedRAMP Moderate standards.Amazon DynamoDB has introduced support for resource-based policies, aiming to streamline access control for your DynamoDB resources. Through resource-based policies, you gain the ability to specify Identity and Access Management (IAM) principals and define their permitted actions on a resource. These policies can be attached to either a DynamoDB table or a stream. When attaching a resource-based policy to a table, you can encompass access permissions for its indexes. Similarly, attaching such a policy to a stream allows for access permissions specific to the stream. Furthermore, resource-based policies facilitate the simplification of cross-account access control, enabling the sharing of resources with IAM principals across different AWS accounts.Trending on the news & advisories:Bringing Access Back \u2014 Initial Access Brokers Exploit F5 BIG-IP (CVE-2023-46747) and ScreenConnect by Mandiant.CISA, FBI, and MS-ISAC Release Update to Joint Guidance on Distributed Denial-of-Service Techniques.CISA: PRC State-Sponsored Cyber Activity: Actions for Critical Infrastructure Leaders.Google security- Vulnerability Reward Program: 2023 Year in Review. Link."}
{"title": "How I - well, AWS WAF and CloudFront - saved the day for my client", "published_at": 1711439700, "tags": [], "user": "Paul SANTUS", "url": "https://dev.to/aws-builders/how-i-well-aws-waf-and-cloudfront-saved-the-day-for-my-client-9hk", "details": "I was working on the migration to AWS of my client, an e-retailer, when I received a phone call: \u201cPaul, we are in trouble, our site has been attacked by denial of service for a week; we are losing money! Can you help?\u201cRushing the migration project was not an option. The client had not yet containerized their app, we hadn't done any data migration test, nor any load test. But as I wrote ina previous blog post, Cloud services can also benefit on-premises infrastructure. Time to prove it!The first analyzes carried out revealed the attacker used multiple IPs (from the TOR network) and targeted the site's login page. This page makes database calls and the database was overloaded, causing first latency (then outages) throughout the system.I get to work right away. Thanks to Terraform, in half a day, I have a functional stack in a test environment, ready to be promoted to production.The tech stackHere is a diagram of the technical stack deployed to counter the attack my client faced:Here are the main additions to the existing stack:Instead of sending directly to my client's on-site infrastructure, DNS now sends requests to CloudFront.CloudFrontis a managed Content-Delivery Network (CDN). That is to say, it makes it possible to serve cached content (or not) from locations close to clients.During the incident, initially, it is not the cache functionality (which reduces the load on the servers, and the latency on the client side) that I was looking for, but rather the possibility of exposing an HTTPS endpoint as an proxy between visitors and my client's infrastructure.Before relaying requests to my \"origin\" (the existing infrastructure), CloudFront passes through AWS WAFWAFis a Web Application Firewall, which allows the inspection of HTTP requests.On AWS WAF, I configured rules based onAWS managed rule sets. Here are the rules that proved most useful in stopping the attack:TheAWSManagedRulesAnonymousIpListrule groupcontains a rulewhich precisely targets known exit IPs of the TOR network as well as the most frequently used VPN services, and another one listing hosting providers (who may zombie machines). This rule will do 95% of the job.The secondAWSManagedRulesATPRuleSetallowsprecisely to protect the login pages, by analyzing requests that are made: do they include all the expected login form fields? Is an IP responsible for multiple authentication failures?In addition to these rules, as a precautionary measure, we put in place the \"usual\" rules: SQL injections, PHP vulnerabilities, OWASP top10, etc.Finally, we added a rule allowing IPs to be whitelisted (the economic model of our e-retailer involves quite a lot of traffic from a few partners, whose IPs were caught by the aforementioned hosting provider list).Implementation and resultWe moved my clients main DNS zone to theRoute53 service(luckily, all the preparatory census work had been carried out before). This brings at least two benefits:The automation offered by Route53, in conjunction with Terraform, allowed me to quickly generate the DNS entries necessary for the Certificate Manager service to deliver SSL certificates authenticating my client's domain.The service makes it possible to define a dynamic \u201cA\u201d record (analias) at the root of the domain, while RFC 1034 does not allow a CNAME (which cannot co-exist with other records) to be positioned at the root.We createdorigin.mydomain.frtype records in this zone and my client did the required work on their webserver to process requests made to this address (including with a TLS certificate so that CloudFront - origin traffic is encrypted in-transit).Once this was tested, we switched the DNS entries formondomaine.frandapi.mondomaine.frto CloudFront.To avoid WAF bypass (in case the hacker discovers theoriginURLs or simply directly uses the IP of my client's server), CloudFront was configured to send a \"secret\" header with each origin request, making it easy for on-premise infrastructure to filter any bypassing traffic.The result is immediate: at 8pm. we made the switch. The site immediately became fully available again. At 9pm. the attacker stopped the attack (before waited for the next day for his next attempt)The image below shows allowed traffic in orange and blocked traffic in blue. We therefore had 6000 requests per minute, more than twice the usual traffic:A word on cost / FinOpsWAF costs $0.60 per million requests analyzed using basic managed rules (the group that includes all of our rulesexcept one). That's less than $5 per day to protect my client.Be careful though! Advanced rules like Account Takeover Protection are billed (after a free tier of 10,000 calls) $1 per 1000 (yes, 1000, not 1,000,000) calls.And at the beginning, our configuration looked like this this:In 24 hours, we burned $700 worth of WAF usage. Fortunately, I had set up cost anomaly alarms when designing the landing zone! It took just a support ticket (category \u201cdispute a charge\u201d) for AWS to gracefully clean our slate! [Nb: in my experience, AWS always clears high slates resulting from configuration errors; this very good commercial policy is one of the reasons, along with the quality of their support, which makes it my favorite cloud provider].In short, we corrected it by placing the ATP rule in last position in order of priority and, above all, by conditioning its execution on the presence of a label placed by another rule which tags requests on the/connectionpath.Relief all the same when we see the traffic passing by the ATP rule go down!An additional benefit of CloudfrontAfter a well-deserved rest, it was time to add an additional benefit for my client: activating Cloudfront cache for all the static resources served by the application.Thanks to Terraform, it's not very complicated: the following block allows you to hide all the gifs.ordered_cache_behavior {     path_pattern             = \"*.gif\"     allowed_methods          = [\"GET\", \"HEAD\", \"OPTIONS\", \"PUT\", \"POST\", \"PATCH\", \"DELETE\"]     cached_methods           = [\"GET\", \"HEAD\", \"OPTIONS\"]     target_origin_id         = local.origin_domain     viewer_protocol_policy   = \"redirect-to-https\"     cache_policy_id          = aws_cloudfront_cache_policy.cachingoptimizez_with_v_header.id     origin_request_policy_id = \"b689b0a8-53d0-40ab-baf2-68738e2966ac\" #Hard-Coded: Forward all headers EXCEPT HOST, cookies and query strings   }Enter fullscreen modeExit fullscreen modeHere too, the effect is immediate. A few minutes later, almost 90% of requests were served by CloudFront, relieving my client's infrastructure of quite a load and improving time-to-full-load for clients!Let's talk!If you need help migrating to the Cloud, helping your dev teams take advantage of the many services available, do not hesitate to contact me viaLinkedInormy website."}
{"title": "DevOps with Guruu | Chapter 11: Deploy CI/CD with Jenkins, SonarQube, Docker, MicroK8s, ArgoCD", "published_at": 1711419082, "tags": ["webdev", "devops", "aws"], "user": "Hoang Guruu", "url": "https://dev.to/aws-builders/devops-with-guruu-chapter-11-deploy-cicd-with-jenkins-sonarqube-docker-microk8s-argocd-4bgp", "details": "DevOps with Guruu | Chapter 11: Deploy CI/CD with Jenkins, SonarQube, Docker, MicroK8s, ArgoCD | P.1Jenkins CI/CD ConfigurationJenkins InstallationJenkins Plugin InstallationJenkins System ConfigurationMaven InstallationSonarQube InstallationDocker InstallationJenkins CI/CD PipelineSonarQube IntegrationJenkins Job ConfigurationTest the BuildJenkinsfile ConfigurationMicroK8s InstallationMicroK8s Add-onsArgoCD InstallationMonitoring with Prometheus and GrafanaPrometheusGrafanaJoin me on this journey to mastering DevOps. Let's get hands-on and build a strong foundation together! Don't forget to like, share, and subscribe for more exciting content. Happy coding, future DevOps!\""}
{"title": "How do I become an AWS Cloud Architect?", "published_at": 1711416684, "tags": ["aws", "solutionarchitect", "cloudcomputing", "devops"], "user": "Gabriel Olaleye", "url": "https://dev.to/aws-builders/how-do-i-become-an-aws-cloud-architect-3gkn", "details": "Becoming an AWS Cloud Architect involves a combination of education, skills development, hands-on experience, and industry knowledge. Here's a step-by-step guide to help you become an AWS Cloud Architect:Educational BackgroundObtain a bachelor's degree in a relevant field such as Computer Science, Information Technology, or Engineering. While not always mandatory, a degree can provide a strong foundation.Cloud FundamentalsStart by learning the fundamentals of cloud computing and AWS. There are many online resources, tutorials, and courses available to help you understand basic concepts.AWS CertificationsAWS offers a variety of certifications that validate your skills and knowledge in different aspects of cloud computing. The most relevant certifications for becoming an AWS Cloud Architect include:AWS Certified Solutions Architect \u2013 AssociateAWS Certified Solutions Architect \u2013 ProfessionalOther relevant AWS certifications based on your specialization (e.g., AWS Certified DevOps Engineer, AWS Certified Security \u2013 Specialty, etc.)Hands-On ExperiencePractical experience is crucial. Create AWS accounts and start experimenting with various services. Build small projects, deploy applications, and familiarize yourself with different AWS components.Project ExperienceWork on real-world projects that involve designing, deploying, and managing AWS-based solutions. This experience will demonstrate your practical skills to potential employers.Networking and LearningAttend tech conferences, workshops, webinars, and meetups related to cloud computing and AWS. Networking with professionals in the field can provide valuable insights and opportunities.Programming and ScriptingGain proficiency in programming languages commonly used for cloud solutions, such as Python, Java, or Node.js. Scripting languages like Bash or PowerShell can also be valuable.Automation and Infrastructure as Code (IaC)Learn about automation tools like AWS CloudFormation or Terraform. These tools help you define and deploy infrastructure as code, a critical skill for a Cloud Architect.Security KnowledgeDeepen your understanding of cloud security practices, encryption, access controls, and compliance standards. Security is a crucial aspect of cloud architecture.Soft SkillsDevelop strong communication, problem-solving, and teamwork skills. Cloud Architects need to collaborate effectively with various teams and translate business needs into technical solutions.Continuous LearningThe cloud landscape evolves rapidly. Stay updated with the latest AWS services, features, and best practices through blogs, documentation, and online courses.PortfolioBuild a portfolio showcasing your projects, certifications, and experience. This can be useful when applying for jobs or freelance opportunities.Job ExperienceSeek roles that allow you to work closely with AWS technologies, architecture design, and cloud solutions. Junior cloud roles, DevOps positions, or systems administration roles can provide valuable experience.Apply for Cloud Architect PositionsOnce you have gained the necessary skills and experience, start applying for AWS Cloud Architect positions. Tailor your resume to highlight your AWS certifications, relevant experience, and project work.Remember that becoming an AWS Cloud Architect is a journey that requires dedication, continuous learning, and hands-on practice. It's important to start with the basics and progressively build your expertise in cloud architecture and AWS services.Like, share and leave a commentKindly follow my Cloud JourneyHEREand subscribe to my channel"}
{"title": "What does AWS Cloud Architect do?", "published_at": 1711415238, "tags": ["aws", "architecture", "design", "softwareengineering"], "user": "Gabriel Olaleye", "url": "https://dev.to/aws-builders/what-does-aws-cloud-architect-do-5hdj", "details": "An AWS Cloud Architect is a professional responsible for designing and implementing effective cloud solutions using Amazon Web Services (AWS) infrastructure. Their role involves understanding business requirements, technical constraints, and best practices to create scalable, secure, and high-performing systems on the AWS platform. Here's a breakdown of what an AWS Cloud Architect typically does:Solution Design:Cloud Architects analyze business needs and technical requirements to design AWS-based solutions. They create architectural diagrams, select appropriate AWS services, and define the overall structure of the system.Infrastructure Provisioning:They configure and provision the necessary resources on AWS to support the designed architecture. This includes setting up virtual machines, storage, networking components, databases, and other services.Scalability and High Availability:Cloud Architects design systems that can scale dynamically to handle changes in demand while maintaining high availability. They implement techniques like auto-scaling, load balancing, and geographic redundancy to ensure system reliability.Security:Security is a crucial aspect of cloud architecture. AWS Cloud Architects implement best practices to secure data, applications, and infrastructure. They configure firewalls, encryption, access controls, and compliance measures to protect against unauthorized access and data breaches.Cost Optimization:Architects consider cost efficiency by selecting appropriate instance types, storage solutions, and services that align with the organization's budget. They also implement cost monitoring and management strategies to optimize spending.Performance Optimization:Cloud Architects fine-tune the system's performance by optimizing resource allocation, configuring caching mechanisms, and implementing content delivery networks (CDNs) to reduce latency and improve responsiveness.Migration Strategies:For organizations transitioning from on-premises infrastructure to the cloud, Cloud Architects devise migration strategies. They plan the migration process, assess application dependencies, and ensure minimal disruption during the migration.Automation and Orchestration:AWS Cloud Architects often leverage automation tools like AWS CloudFormation, Terraform, or AWS Lambda to automate the provisioning and management of resources. They create scripts and templates to streamline deployment and management processes.Collaboration:Cloud Architects collaborate with various teams, including developers, operations, and business stakeholders. Effective communication is essential for translating business needs into technical solutions and ensuring alignment with the organization's goals.Documentation:Cloud Architects document the architectural decisions, designs, and configurations. This documentation helps in knowledge sharing, troubleshooting, and future maintenance.Continuous Improvement:AWS services and best practices evolve over time. Cloud Architects stay up-to-date with the latest developments in the AWS ecosystem and continuously improve their architecture designs to incorporate new features and enhancements.In essence, an AWS Cloud Architect is responsible for designing and building cloud solutions that are robust, scalable, secure, and cost-effective, leveraging the wide array of services offered by AWS to meet the organization's technological and business objectives.Please like and leave your commentsKindly follow my Cloud JourneyHEREand subscribe to my channel"}
{"title": "When Should We Expand into a New AWS Region?", "published_at": 1711411355, "tags": ["aws", "cloud", "architecture", "startup"], "user": "Xabi Errotabehere", "url": "https://dev.to/aws-builders/when-should-we-expand-into-a-new-aws-region-3a9l", "details": "As the CTO of Hivelight, I've been grappling with the pivotal decision of when to expand into new AWS regions. This isn't just a technical matter; it's about ensuring our company's growth, resilience, and ability to serve our clients effectively. Drawing from my experiences and insights from the vibrant community, let's explore the nuances of this decision.Why expand?In today's interconnected world, where our users are spread across various regions (ANZ, North America, the Caribbean, and Europe), expanding into new AWS regions is no longer a luxury but a strategic necessity. By doing so, we unlock several benefits that directly impact our startup's performance and competitiveness:Reduced Latency for Enhanced User ExperienceImagine a lawyer using our platform to access critical matters or collaborate with colleagues. Reduced latency means quicker access and seamless interactions, leading to higher user satisfaction and retention.Improved Fault Tolerance and High AvailabilityAs a productivity platform, downtime is simply not an option. By duplicating our infrastructure across multiple AWS regions, we increase our fault tolerance and ensure that our services remain accessible even in the face of regional outages or disruptions.Compliance and Data Residency ConsiderationsIn the legal sector, where data privacy and regulatory compliance are paramount, operating in multiple AWS regions allows us to adhere to data residency requirements and navigate complex regulatory landscapes.Signs Pointing Towards ExpansionExpanding into new AWS regions shouldn't be a hasty decision but a well-thought-out strategy informed by data and market insights. Here are some key indicators that suggest it's time for us to broaden our AWS footprint:User Base Distribution AnalysisBy closely monitoring the distribution of our user base, we can identify regions where there's a concentration of users. If a significant portion of our users is clustered in a specific geographic area, it's a strong signal that we need to establish a local presence to optimize performance and responsiveness. As of today, most of our users are in the ANZ region, but our expansion plans are in North America.Latency Metrics and Performance MonitoringTracking latency metrics and performance indicators across different regions provides us with valuable insights into user experience. Consistently high latency for users in certain regions indicates a need for local infrastructure to mitigate delays and improve service quality. For instance, the roundtrip from New Zealand to us-east-1 (AWS) for a lambda function is 500ms, but can vary from 200ms to 3 seconds. No user has expressed that the system is slow, better yet, our users are happy with the responsiveness.Historical Downtime and Outage AnalysisAnalyzing past downtime incidents and regional outages helps us identify patterns and vulnerabilities. If we notice recurring issues or a particular region's susceptibility to disruptions, it would signal the importance of diversifying our AWS footprint to enhance reliability and resilience, so at least only a fraction of our customers would be impacted by such an event. So far, we haven't seen any outage that impacted our production environment, fingers crossed.Strategic Market Expansion PlansOur startup's growth trajectory and market expansion plans also influence our decision to expand into new AWS regions. If we're targeting new geographical markets or experiencing rapid growth in specific regions, establishing a local presence ensures that our infrastructure can scale seamlessly to meet growing demand. That is why we are using in us-east-1 (AWS) because this is where we see the biggest growth potential.To sum up, expanding into new AWS regions is a strategic move that requires careful consideration, data-driven insights, and collaboration. By assessing user distribution, latency metrics, downtime incidents, and market expansion plans, we can pinpoint opportune moments to diversify our AWS footprint and unlock new avenues for growth and innovation. We're not there yet however, we need to be ready for that.This isn't just about technical infrastructure; it's about empowering our company to thrive in an ever-changing landscape. We can chart a course towards success and ensure that our platform continues to provide the platform that every law practice needs to grow, do its best work, and help more people."}
{"title": "Chaos Engineering the AWS Way: 101", "published_at": 1711387756, "tags": ["chaos", "aws"], "user": "Simon Hanmer", "url": "https://dev.to/aws-builders/chaos-engineering-the-aws-way-101-1acb", "details": "Over the last decade or so, we\u2019ve seen a switch from the era of monolithic applications to a modern, more efficient, microservice-based development model. This had meant that we need to consider a wider landscape around testing to ensure our applications are resilient and perform as expected.Build me a monolith.In the \u2018good old days\u2019 of software development, applications were designed and written as monoliths. These large, self-contained codebases simplified initial development but led to challenges as the scope of the applications grew. Scaling was difficult as the application would often have to be duplicated, rather than being able to scale individual components as needed. Also deploying updates or new functionality was a complex process, often needing extended downtime for the entire application.Testing of these applications would often be carried out manually, by test teams who would concentrate on the functional requirements. If non-functional requirements were tested, often this would be limited to performance-related tasks such as ensuring that a defined hardware configuration could handle a specific level of user traffic and respond within a given timeframe.Transition to microservices.As the use of Cloud providers became more prevalent, new serverless functionality allowed us to change our approach to development, leading to the spread in the use of microservices. These allowed us to break down the monolithic functionality into smaller, independently developed and deployed services. Each microservice tended to focus on a specific piece of business functionality, allowing teams to work in parallel in a more agile manner.Applications now often have a distributed architecture with code running on servers, serverless or container-based systems or even client-side in browsers. We\u2019re using databases with multiple read (or even write) hosts, caching, load balancers and other components, all coming together to create what is known as a distributed system, typically communicating via network links. This enables us to scale individual services as needed, leading to a more efficient use of resources and increased fault-tolerance but introduces new challenges related to communications, consistency and the need for more observability.This new paradigm has also enabled us to improve our development and QA practices, through the use of automated deployments, often including automated testing covering unit tests, through to behavioural testing. But again, these have tended to concentrate on functional requirements \u2013 and this means that whilst the complexity of our application landscapes has grown, our non-functional testing hasn\u2019t kept pace with the now complex architectures.This challenge in managing distributed systems led to Peter Deutsch and James O. Coplient, articulating what are known as the \u20188 fallacies of distributed computing\u20191:The network is reliable.Latency is zero.Bandwidth is infinite.The network is secure.Topology doesn't change.There is one administrator.Transport cost is zero.The network is homogeneous.Acknowledging these fallacies is essential for designing robust, resilient distributed systems and their associated applications.Release the apes of chaos.To counter this lack of testing around complexity, a new discipline started to emerge in the early 2000s - chaos engineering.Chaos engineering can be considered as:the discipline of experimenting on a system in order to build confidence in the system's capability to withstand turbulent conditions in production.2Whilst there were some early, basic attempts by Amazon and Google, Netflix is generally considered as having defined this new approach by considering the 8 fallacies and trying to develop engineering solutions that would test against them to ensure reliability within their systems.In 2011, Netflix developed a tool known as \u2018Chaos Monkey\u2019, which intentionally disabled servers, databases and applications at random in the production environment to test the resilience of their worldwide video-streaming network. The name came from the idea that you could measure what might happen if a wild monkey ran through their data centres and Cloud environments with a weapon, smashing servers and chewing through cabling3.They quickly realised the value that this provided to their engineers, allowing them to design more highly available services, and expanded the tooling to create what is known as the \u2018Simian Army\u2019, with a variety of tools such asLatency Monkey \u2013 a tool that introduced artificial delays in the network layer between different services. It allowed Netflix to simulate a server becoming unavailable, or even losing an entire service.Conformity Monkey \u2013 looks for servers not configured according to their best practices and shuts them down. For example, it would look for servers that weren\u2019t part of an auto-scaling group, and so had limited resilience to unplanned shutdown.Chaos Gorilla \u2013 whilst Chaos Monkey looked for individual servers to target, Chaos Gorilla looked to test the outage of an entire availability zone.These, and many other, approaches to testing Netflix\u2019s resilience soon began to gain recognition within the wider engineering community, and others tried to re-use or re-engineer similar tools.AWS enter the Fray.As the major player in the Cloud engineering space, it\u2019s safe to say that Netflix\u2019s approach would have caught the eye of Amazon Web Services. After all, for many years, they were the biggest single customer using the Cloud provider\u2019s services.One of AWS\u2019s oft-quoted ideals is the aim of reducing \u2018undifferentiated heavy lifting\u2019, where they look for tasks that are being widely adopted by their customers, and which they could provide as a managed service, providing them with the opportunity to reduce workload and complexity (whilst at the same time, no doubt, providing an income stream). However, AWS\u2019s announcement4at their 2020 re:Invent conference, that they would provide a managed service providing chaos engineering tools still came as a surprise to some.Amazon\u2019s Fault Injection Service, or FIS5, offered a set of scenarios that could be deployed in their customers\u2019 accounts, initially allowing testing against Amazon\u2019s EC2, ECS, EKS and RDS services. Since that time, the offering has expanded and now includes cross-account testing, simulating errors at the control plane and network layers, allowing simulation of availability zone failures, API throttling and a wide range of other failure scenarios.FIS and associated AWS services, allow engineering teams to follow what is now seen as a standard set of practices within chaos engineering:Baseline performance \u2013 services such as AWS CloudWatch allow a deep understanding of applications\u2019 operating parameters, allowing them to measure and track interactions, dependencies and various service metrics.Hypothesise \u2013 once there is an understanding of the components of a service, architects and engineers can start to think about \u2018what if\u2019 \u2013 would their availability continue under increased network latency, API throttling or the unplanned termination of components?FIS enables these hypotheses to be codified using what are known as \u2018experiment templates\u2019, describing tests to be carried out along with which components should be stressed.Experiment \u2013 FIS allows the experiment templates to be deployed and executed in an AWS environment.Blast Radius \u2013 any chaos engineering tool should have the option to terminate an experiment if it starts to affect service in an unplanned way. FIS allows CloudWatch Alarms to be configured to halt experiments and roll back the effects that had been put in place.Measurement \u2013 once again, CloudWatch provides services such as metrics, logs and alarms, allowing application designers to understand how their services reacted to the experiments put in place.FIS also brings an important element to the world of chaos engineering \u2013 control. While it\u2019s good to test in development environments to understand how services will react to unplanned scenarios, one of chaos engineering\u2019s tenets is that the most valuable insights will be gained by testing against production services. However, AWS customers will want to control how and when these experiments are deployed \u2013 this is achieved using IAM permissions to control who can define and execute FIS scenarios.Conclusion.Organisations and engineers working with complex, distributed systems within Cloud environments should look to adopt the principles of chaos engineering, ensuring that it becomes not just a best practice but a strategic imperative.Amazon\u2019s FIS empowers engineering teams to proactively address the challenges of distributed systems, ensuring the robustness and resilience of applications in the dynamic and unpredictable cloud environment. Working to their principle of undifferentiated heavy lifting, AWS has positioned chaos engineering as a managed service, aligning with their commitment to reducing complexity and empowering customers to navigate the intricacies of modern cloud-based architectures.https://en.wikipedia.org/wiki/Fallacies_of_distributed_computing\u21a9https://principlesofchaos.org/\u21a9https://netflixtechblog.com/the-netflix-simian-army-16e57fbab116\u21a9https://www.youtube.com/watch?v=VndV2j8fulo\u21a9http://aws.amazon.com/fis\u21a9"}
{"title": "Spring Boot 3 application on AWS Lambda - Part 1 Introduction to the series", "published_at": 1711371155, "tags": ["aws", "serverless", "java", "springboot"], "user": "Vadym Kazulkin", "url": "https://dev.to/aws-builders/spring-boot-3-application-on-aws-lambda-part-1-introduction-to-the-series-2m5g", "details": "What we will explore and learn throughout the series?Let me introduce to you my new series. \"Spring Boot 3 application on AWS Lambda\".In the course of the series, we'll use Java 21 runtime to demonstrate multiple ways how to run and optimize Spring Boot 3 (and at the time of writing the latest major Spring Boot version is 3.2) application on AWS Lambda. You can easily adopt the examples to the newer Spring Boot version when it will be released.In the upcoming articles we'll explore the following ways to run Spring Boot 3 application on AWS Lambda using the following frameworks, technologies or concepts:AWS Serverless Java ContainerAWS Lambda Web AdapterSpring Cloud FunctionCustom Docker ImageFor each of this we'll introduce the concept first, then how to develop, deploy and run our application using each approach and also how to optimize the application with Lambda SnapStart (including various priming techniques) if available (currently not available on the Docker Container images). We'll also exploreGraalVM Native Imageusing Spring Cloud Function deployed as the AWS Lambda Custom Runtime as the optimization technique.Of course, we'll measure and the cold and warm start times of the Lambda function using all mentioned approaches and compare all introduced solutions.Last but not least we'll explore whether Spring Boot 3native supportofCRaCis also a valid approach. We'll also make a comparison of the all introduced frameworks and concepts.You can already find code samples for the entire series inAWSLambdaJavaWithSpringBoot repositoryof my GitHub account.In the next part of the series we'll introduce AWS Serverless Java Container."}
{"title": "Simple and Cost-Effective Testing Using Functions", "published_at": 1711362957, "tags": ["aws", "testing", "lambda", "postman"], "user": "Felipe Malaquias", "url": "https://dev.to/aws-builders/simple-and-cost-effective-testing-using-functions-42dm", "details": "Don\u2019t limit yourself to pipeline testsWhile tests in your pipeline are a must, you should not have the false idea that things are fine because your pipeline is green, and here are a couple of reasons why:Complexity in Distributed SystemsYou\u2019re not alone. Nowadays, a service is rarely an isolated system with no connections to other services, be it databases, Kafka clusters, other services of your team, other team\u2019s services, third-party services, you name it.Imagine the following system:You deploy a shiny new feature for your web login in the web frontend service, test it locally, create unit and integration tests, deploy, and make sure it works in production.After some time, you or someone else (of course, always someone else \ud83d\ude1b), wanted to provide an MFA feature for mobile apps, and, therefore, modified the account service to provide some additional context to the apps and ended up breaking the login for the web frontend. Let\u2019s say neither account service nor mobile app is your team's responsibility. How long would it take for you to know this feature is broken? Of course, you have metrics and alarms in place, but let\u2019s make it less obvious. Instead of breaking the feature completely, you only break it for a small subset of users, for example. Depending on your thresholds for your alarms, evaluation period, points to alarm, etc., you may take a very long time to detect it (and by very long, I consider it already 5 minutes or above). Or worse, if you don\u2019t have alarms and metrics in place (shame on you), it could detect it only during your next build or after a couple of customer complaints.Fail Fast Fix FasterAs mentioned in the previous section, it may take time to detect a failure, and as a consequence, even more time to detect the root cause, as you may not be able to find out so quickly when it started to happen (e.g., short log retention time, missing metrics, etc.). If you are constantly testing your system, you know exactly when something stopped working, making it easier to find the subset of changes during that timeframe that could have led to the issue.Hidden Intermittent FailuresThere are a few things that irritate me more than green peas and the act of restarting a failed build and if it works, proceeding as if everything is fine and it was just a \u2018glitch\u2019.There is no such thing as a \u2018glitch\u2019 in mathematics and, therefore, computer science. Behind everything, there is a reason, and you should always know the reason so you do not get caught off guard in the near future. If an issue can happen, it will happen. Did you get it? Are you sure?I\u2019ve seen teams run buggy software for days, months, and even years without fixing intermittent failures because they seemed just randomness, and no one could explain the reason because the frequency was relatively low that no one bothered to check the root cause, and at some point in time, this issue comes and bites you, because if you don\u2019t know the reason, you might make the same mistake again, in another scenario, service, or system that will lead to a higher impact on your business.Chasing the reason for things to happen should be the number one goal of software engineers because only then can we learn and improve.So, What\u2019s My Suggestion?Continuously Test Your ApplicationsBy continuously, I really mean continuously, and not only during deployments. Test it at a one-minute frequency, for example, so you have enough resolution to know when things started to go bad and can also know how frequently an issue occurs. Does it always occur? Every x requests? Only during the night quiet period? All these questions can help you find the root cause faster. Also, make sure those tests alarm you in case they are not working properly.A Possible Solution with FunctionsThere are a couple of companies out there that offer continuous testing services, such asUptrends. However, if you\u2019re looking to run some continuous integration tests, I believe you could have a much more cost-effective, simpler, and more useful solution if you build it on your own usingPostmanas a basis.Postman is a great tool that has been on the market for a very long time. It is very reliable, has very good features for end users, and has enough flexibility to adapt to your needs.More UsefulI realize occasionally that most developers are not very familiar with their APIs. By that, I mean that they often don\u2019t have a shared collection of API calls prepared for running on demand at each stage if needed, for example.Postman allows you to share collections of HTTP, GraphQL, gRPC, Websocket, Socket.IO, and MQTT requests and organize them into multiple environments, each with its variables (e.g., hostname, secrets, user names, etc.).By sharing these collections with the team, everyone can quickly understand your APIs by calling them whenever needed, at any stage, for example, and, with this, integrate them into their own systems.SimplerBefore implementing the solution mentioned in this article, I encountered integration test suites written in Java. Therefore, they had their own projects configured with Maven and had a lot of verbose and redundant code for performing and verifying HTTP calls. These projects were checked out during the build and executed for each stage. The execution also needed some spring boot bootstrap time, making the pipeline slower.By using Postman, creating new test cases is much quicker and simpler, as it can be created in a user-friendly UI by inserting the address, adding variables as you need for each environment, adding very straightforward individual assertions per test case, and running it with a click of a button for verifying it. See some exampleshere.Cost EffectiveYou can use Postman for free with some limitations if you like (you can share your collections with up to 3 people), and this would be enough to implement the solution I\u2019ll describe here. However, if you want to share the collections with your team, it\u2019s good to look at theirplans and pricing.Also, by building your infrastructure to run it, you may even be able to run these tests almost for free! The idea behind this infrastructure is to run the tests using functions through a Postman runner to run test collections exported from Postman. Lambda functions are avery affordableway of executing code for a short period of time.SolutionAs you can see in the above diagram,EventBridgeschedules alambda functionto be executed periodically. This lambda function retrieves the assets exported from Postman (test collection, environment, and global variables), injects secrets from the secrets manager, executes the tests using theNewmannpm package, and, in case of failures, updates metrics in CloudWatch and stores test results in the S3 bucket. An alarm is triggered if the metrics exceed a threshold (in this case, a count of 1).The complete solution withSAMis availablehere.The infrastructure is defined in thetemplate.yamlfile, and the lambda function handler with all testing logic is defined inapi-testing-handler.ts.This infrastructure can be reused for any Postman testing (HTTP, REST APIs, etc.). An example of an exported Postman collection is availablehere. Please notice that these files were not created manually butexportedfrom the UI. All these files must be placed inside the S3 bucket generated by the infrastructure in the folder defined by the*TestName*parameter input during the infrastructure deployment (in this case, \u2018MyService\u2019 by default).Also, notice that the*SecretId*secret must exist in order for the lambda function to inject any secret needed by the test collection.Have fun playing around with it."}
{"title": "AWS announces a 7-day window to return Savings Plans", "published_at": 1711361857, "tags": ["aws", "awscommunitybuilders", "costoptimization", "savingsplans"], "user": "Fady Nabil", "url": "https://dev.to/aws-builders/aws-announces-a-7-day-window-to-return-savings-plans-22kc", "details": "\ud83c\udd95 Amazon Web Services (AWS) announces a 7-day window to return Savings Plans \ud83d\udd04\ud83d\udcb2\ud83d\udcb0Savings Plans is a flexible pricing model that can help you reduce your bill by up to 72% compared to On-Demand prices, in exchange for a one- or three-year hourly spend commitment. Starting today, if you realize that the Savings Plan you recently purchased isn\u2019t optimal for your needs, you can return it and if needed, repurchase another Savings Plan that better matches your needs.\ud83d\udee0\ufe0f How to Return Your Savings Plan:Access the AWS Billing and Cost Management console.Navigate to 'Savings Plans' > 'Inventory'.Select the Savings Plan you wish to return and hit the 'Return Savings Plan' button.Confirm your return in the dialog box.Read more here:https://aws.amazon.com/about-aws/whats-new/2024/03/aws-7-day-window-return-savings-plans/aws #AWS #awscommunitybuilders #awscommunity #awsnews #SavingsPlans  #CloudComputing #SavingPlans #Cost #CostOptimize #EC2 #Compute #CloudCostOptimization #FinOps"}
{"title": "DevOps with Guruu | Chapter 10 : Deploy CI/CD with Jenkins integrating with GitHub | Docker | Node.js | npm", "published_at": 1711359390, "tags": ["webdev", "devops", "aws"], "user": "Hoang Guruu", "url": "https://dev.to/aws-builders/devops-with-guruu-chapter-10-deploy-cicd-with-jenkins-integrating-with-github-docker-nodejs-npm-2km5", "details": "DevOps with Guruu | Chapter 10 : Deploy CI/CD with Jenkins integrating with GitHub | Docker | Node.js | npmAWS EC2 Instance setupJenkins InstallationGitHub IntegrationApplication SetupDocker IntegrationGitHub Webhook ConfigurationJoin me on this journey to mastering DevOps. Let's get hands-on and build a strong foundation together! Don't forget to like, share, and subscribe for more exciting content. Happy coding, future DevOps!\""}
{"title": "The Intersection of DevOps, AWS, and Cloud Security: Best Practices for a Secure Environment", "published_at": 1711357163, "tags": ["aws", "devops", "cloudseurity", "career"], "user": "Frank Osasere Idugboe", "url": "https://dev.to/aws-builders/the-intersection-of-devops-aws-and-cloud-security-best-practices-for-a-secure-environment-348o", "details": "As a DevOps engineer, I have come to learn the importance of cloud security in my line of work. With the rise of cloud computing, it has become essential to have a secure environment to protect sensitive data, and this is where DevOps, AWS, and cloud security intersect. In this article, I will explain what DevOps is and how it intersects with AWS and cloud security. I will also discuss the importance of cloud security, best practices for cloud security, how DevOps and AWS can help with cloud security, the impact of cloud security on your career in DevOps and AWS, courses and certifications for DevOps, AWS, and cloud security, tools for DevOps, AWS, and cloud security, and finally, the future of DevOps, AWS, and cloud security.Introduction to DevOps, AWS, and Cloud SecurityDevOps is a software development approach that emphasises collaboration and communication between development and operations teams. The goal of DevOps is to create a culture of collaboration, integration, and automation, which helps teams to deliver software faster and more reliably. AWS, on the other hand, is a cloud computing platform that provides a wide range of services, including computing, storage, and databases, among others. It is a popular choice for businesses looking to move their applications to the cloud. Cloud security, on the other hand, refers to the set of policies, controls, and technologies that are put in place to protect cloud-based systems, data, and infrastructure.What is DevOps and how does it intersect with AWS and Cloud Security?DevOps is all about collaboration, integration, and automation. It brings together the development and operations teams to work together in a seamless manner. AWS provides a wide range of services that can be used to support DevOps, including compute, storage, and databases. AWS can be used to automate the deployment and management of applications, which is a key aspect of DevOps. Cloud security is an important consideration when using AWS, as the cloud platform can be vulnerable to security breaches. DevOps can help to improve cloud security by integrating security into the application development process.Understanding the Importance of Cloud SecurityCloud security is essential in today's environment, as more and more businesses move their applications to the cloud. Cloud security is important because it helps to protect sensitive data from breaches and unauthorised access. Cloud security also helps to ensure that cloud-based systems are available and reliable. Breaches can cause significant damage to businesses, including financial losses and damage to reputation. It is essential to have a comprehensive cloud security strategy in place to protect against these threats.Best Practices for Cloud SecurityThere are several best practices that can be used to ensure cloud security. These include:Implementing a comprehensive security policy:A security policy should be in place to guide the implementation of security controls and procedures.Implementing access controls: Access controls should be put in place to ensure that only authorised users can access cloud-based systems.Implementing encryption:Encryption should be used to protect sensitive data from unauthorised access.Regularly monitoring and auditing cloud-based systems: Regular monitoring and auditing can help to identify security issues before they become serious problems.Regularly updating software:Regularly updating software can help to ensure that systems are protected against known vulnerabilities.How DevOps and AWS can help with Cloud SecurityDevOps and AWS can help to improve cloud security by integrating security into the application development process. DevOps can help to identify security issues early on in the development process, which can help to prevent security breaches. AWS provides a wide range of security services, including identity and access management, network security, and encryption, among others. These services can be used to enhance cloud security and ensure that cloud-based systems are protected against threats.The Impact of Cloud Security on Your Career in DevOps and AWSCloud security is becoming increasingly important in the world of DevOps and AWS. As a DevOps engineer, having a solid understanding of cloud security can help to set you apart from other engineers. Understanding cloud security can also help you to identify potential security issues and implement solutions to address them. Cloud security certifications can help to demonstrate your knowledge and expertise in this area and can help to advance your career in DevOps and AWS.Courses and Certifications for DevOps, AWS, and Cloud SecurityThere are several courses and certifications available for DevOps, AWS, and cloud security. These include:AWS Certified DevOps Engineer:This certification validates your understanding of DevOps practices and how they can be implemented using AWS services.Certified Cloud Security Professional (CCSP):This certification validates your knowledge of cloud security best practices and how they can be applied to different cloud platforms.DevOps Foundation Certification:This certification validates your understanding of DevOps practices and principles.Tools for DevOps, AWS, and Cloud SecurityThere are several tools available for DevOps, AWS, and cloud security. These include:AWS CloudFormation:This tool can be used to automate the deployment and management of AWS resources.AWS Config: This tool can be used to monitor and manage AWS resources.AWS Identity and Access Management (IAM):This tool can be used to manage access to AWS resources.DevOps, AWS, and Cloud Security ServicesThere are several services available for DevOps, AWS, and cloud security. These include:AWS Security Hub:This service provides a centralized view of security alerts and compliance status across AWS accounts.AWS Shield:This service provides protection against DDoS attacks.AWS WAF:This service provides protection against web-based attacks.ConclusionDevOps, AWS, and cloud security are all interconnected. DevOps can help to improve cloud security by integrating security into the application development process. AWS provides a wide range of security services that can be used to enhance cloud security. Cloud security is becoming increasingly important in the world of DevOps and AWS, and having a solid understanding of cloud security can help to advance your career in this field. There are several courses, certifications, tools, and services available for DevOps, AWS, and cloud security, and it is essential to stay up-to-date with the latest developments in this area."}
{"title": "AWS Cost Optimization Hub", "published_at": 1711355678, "tags": ["aws", "costmanagement", "costoptimizationhub"], "user": "Olawale Adepoju", "url": "https://dev.to/aws-builders/aws-cost-optimization-hub-8io", "details": "Overview of Cost Optimization HubCost Optimization Hub is an AWS Billing and Cost Management feature that helps you consolidate and prioritize cost optimization recommendations across your AWS accounts and AWS Regions, so that you can get the most out of your AWS spend.Cost Optimization Hub provides the following main benefits:Automatically identify and consolidate your AWS cost optimization opportunities.Quantify estimated savings that incorporate your AWS pricing and discounts.Aggregate and deduplicate savings across related cost optimization opportunities.Prioritize your cost optimization recommendations with filtering, sorting, and grouping.Measure and benchmark your cost efficiency.Accounts supported by Cost Optimization HubThe following AWS account types can opt in to Cost Optimization Hub:Standalone AWS accountA standalone AWS account that doesn't have AWS Organizations enabled. For example, if you opt in to Cost Optimization Hub while signed in to a standalone account, Cost Optimization Hub identifies cost optimization opportunities and consolidates recommendations.Member account of an organizationAn AWS account that's a member of an organization. If you opt in to Cost Optimization Hub while signed in to a member account of an organization, Cost Optimization Hub identifies cost optimization opportunities and consolidates recommendations.Management account of an organizationAn AWS account that administers an organization. If you opt in to Cost Optimization Hub while signed in to a management account of an organization, Cost Optimization Hub gives you the option to opt in the management account only, or the management account and all member accounts of the organization.Note:To opt in all member accounts for an organization, make sure that the organization has all features enabled. For more information, see\u00a0Enabling All Features in Your OrganizationGetting started with Cost Optimization HubWhen you access Cost Optimization Hub for the first time, you're asked to opt in using the account that you\u2019re signed in with.Before you can use the feature, you must opt in. In addition, you can also opt in using the Cost Optimization Hub API, AWS Command Line Interface (AWS CLI), or SDKs.By opting in, you authorize Cost Optimization Hub to import cost optimization recommendations generated by multiple AWS services in your account and all member accounts of your organization. These include rightsizing recommendations from AWS Compute Optimizer and Savings Plans recommendations from AWS Billing and Cost Management. These recommendations are saved in the US East (N. Virginia) Region.Enabling Cost Optimization HubTo enable Cost Optimization HubSign in to the AWS Management Console.In the navigation pane, choose\u00a0Cost Optimization Hub.On the\u00a0Cost Optimization Hub\u00a0page, choose your relevant organization and member account settings:Enable Cost Optimization Hub for this account and all member accounts: Recommendations in this account and all member accounts will be imported into Cost Optimization Hub.Choose Enable.After you enable Cost Optimization Hub, AWS starts to import cost optimization recommendations from various AWS products, such as AWS Compute Optimizer. It can take as long as 24 hours for Cost Optimization Hub to import recommendations for all supported AWS resources.Accessing the consoleWhen your setup is complete, access Cost Optimization Hub.To access Cost Optimization HubSign in to the AWS Management ConsoleIn the navigation pane, choose Cost Optimization Hub.Opting out of Cost Optimization HubYou can opt out of Cost Optimization Hub at any time. However, the organization account can't opt out all member accounts. Each member needs to opt out at account level.To opt out of Cost Optimization HubSign in to the AWS Management Console.In the navigation pane, choose Cost Management Preferences.In Preferences, choose Cost Optimization Hub.On the Cost Optimization Hub tab, clear Enable Cost Optimization Hub.Choose Save preferences.Viewing your cost optimization opportunitiesCost optimization findings for your resources are displayed on the Cost Optimization Hub dashboard. You can use this dashboard to filter cost optimization opportunities and aggregate estimated savings. You can compare your total savings opportunities against your previous month's AWS spend.Use the dashboard to group your savings opportunities by AWS account, AWS Region, resource types, and tags. View the distribution of your savings opportunities, explore the recommended actions, and identify the areas with the most savings opportunities. The dashboard is refreshed daily and all costs reflect your usage up to the previous day. For example, if today is December 2, the data includes your usage through December 1.Viewing the dashboardUse the following procedure to view the dashboard and your cost optimization opportunities.Sign in to the AWS Management Console and open the AWS Billing and Cost Management.In the navigation pane, choose Cost Optimization Hub. By default, the dashboard displays an overview of cost optimization opportunities for AWS resources across all AWS Regions in the account that you're currently signed in to.You can perform the following actions on the dashboard:To view the cost optimization findings for a particular AWS Region in the account, choose the Region in the chart.To view the cost optimization findings for resources in a particular account, under Aggregate estimated savings by, choose AWS account, and then choose an account ID in the chart.To view cost optimization findings by resource type, under Aggregate estimated savings by, choose Resource type.To view recommended actions, under Aggregate estimated savings by, choose Recommended action.To filter findings on the dashboard, under Filter, choose from the filter options.To go to the list of resources available for optimization, choose View opportunities.Switching the dashboard viewThe Cost Optimization Hub dashboard provides you two styles for viewing your cost optimization opportunities:Chart viewTable viewYou can set the style by choosing one of the views on the top right corner of the chart or table.ReferenceYou can also referencehere"}
{"title": "Redefining AWS EKS networking with Gateway API", "published_at": 1711351327, "tags": ["aws", "eks", "gatewayapi", "vpclattice"], "user": "saifeddine Rajhi", "url": "https://dev.to/aws-builders/redefining-aws-eks-networking-with-gateway-api-2in9", "details": "Introduction:Building upon our exploration of Kubernetes networking evolution with the Gateway API, Part 2 tackles AWS Application Networking.This implementation of the Kubernetes Gateway API operates within Kubernetes clusters to manage AWS VPC Lattice resources.By using Kubernetes Custom Resource Definitions such as Gateway and HTTPRoute, AWS Application Networking orchestrates networking operations, enhancing the efficiency and security of networking within Kubernetes environments.Amazon VPC\u00a0Lattice:Amazon VPC Lattice, a new feature of Amazon Virtual Private Cloud (Amazon VPC), is generally available, offering a unified approach to connect, secure, and monitor service communication.With the launch of Amazon VPC Lattice comes the introduction of theAWS Gateway API controller, an implementation of the Kubernetes Gateway API. This open-source standard interface enhances Kubernetes application networking through flexible, extensible, and role-oriented interfaces.The AWS Gateway API controller extends custom resources defined by the Gateway API, enabling the creation of VPC Lattice resources using Kubernetes APIs.As organizations transition from monolithic applications to microservices architecture with Kubernetes, they gain agility and enable more frequent deployments. However, this shift introduces challenges related to application traffic management, security, monitoring, and routing.To address these concerns across ingress (north-south) and service-to-service communication (east-west), customers often rely on various tools like load balancers, service discovery mechanisms, service meshes, and monitoring agents.Amazon VPC Lattice is a fully managed service built directly into the AWS network infrastructure. Amazon VPC Lattice allows you to:handle network connectivity seamlessly between services across VPCs and accountsdiscover these services spanning multiple Kubernetes clustersimplement defense-in-depth strategy to secure communication between those servicesobserve the request/response traffic across the servicesTo implement the features just described, you aren't required to develop custom code or required to manage Kubernetes sidecar proxies.AWS Gateway API Controller:Gateway API is an open-source project managed by the Kubernetes networking community. It is a collection of resources that model application networking in Kubernetes. Gateway API supports resources such as GatewayClass, Gateway, and Route that have been implemented by many vendors and have broad industry support.Originally conceived as a successor to the well-known Ingress API, the benefits of the Gateway API include (but are not limited to) explicit support for many commonly used networking protocols, as well as tightly integrated support for Transport Layer Security (TLS).When installed in your cluster, the controller watches for the creation of Gateway API resources such as gateways and routes and provisions corresponding Amazon VPC Lattice objects according to the mapping in the image below.The AWS Gateway API Controller is an open-source project and fully supported by Amazon.As shown in the figure, there are different personas associated with different levels of control in the Kubernetes Gateway API:Infrastructure provider: Creates the Kubernetes GatewayClass to identify VPC Lattice as the GatewayClass.Cluster operator: Creates the Kubernetes Gateway, which gets information from VPC Lattice related to the service networks.Application developer: Creates HTTPRoute objects that specify how the traffic is redirected from the gateway to backend Kubernetes services.AWS Kubernetes Gateway API resources and VPC Lattice resources correspond as follows:Deploying the AWS Gateway API Controller:Follow these instructions to create a cluster and deploy the AWS Gateway API Controller.First, configure security group to receive traffic from the VPC Lattice network. You must set up security groups so that they allow all Pods communicating with VPC Lattice to allow traffic from the VPC Lattice managed prefix lists.Lattice has both an IPv4 and IPv6 prefix lists available.CLUSTER_SG=$(aws eks describe-cluster --name $EKS_CLUSTER_NAME --output json| jq -r '.cluster.resourcesVpcConfig.clusterSecurityGroupId') PREFIX_LIST_ID=$(aws ec2 describe-managed-prefix-lists --query \"PrefixLists[?PrefixListName==\"\\'com.amazonaws.$AWS_REGION.vpc-lattice\\'\"].PrefixListId\" | jq -r '.[]') aws ec2 authorize-security-group-ingress --group-id $CLUSTER_SG --ip-permissions \"PrefixListIds=[{PrefixListId=${PREFIX_LIST_ID}}],IpProtocol=-1\" PREFIX_LIST_ID_IPV6=$(aws ec2 describe-managed-prefix-lists --query \"PrefixLists[?PrefixListName==\"\\'com.amazonaws.$AWS_REGION.ipv6.vpc-lattice\\'\"].PrefixListId\" | jq -r '.[]') aws ec2 authorize-security-group-ingress --group-id $CLUSTER_SG --ip-permissions \"PrefixListIds=[{PrefixListId=${PREFIX_LIST_ID_IPV6}}],IpProtocol=-1\"Enter fullscreen modeExit fullscreen modeThis step will install the controller and the CRDs (Custom Resource Definitions) required to interact with the Kubernetes Gateway API.aws ecr-public get-login-password --region us-east-1 \\   | helm registry login --username AWS --password-stdin public.ecr.aws helm install gateway-api-controller \\     oci://public.ecr.aws/aws-application-networking-k8s/aws-gateway-controller-chart \\     --version=v1.0.1 \\     --create-namespace \\     --set=aws.region=${AWS_REGION} \\     --set serviceAccount.annotations.\"eks\\.amazonaws\\.com/role-arn\"=\"$LATTICE_IAM_ROLE\" \\     --set=defaultServiceNetwork=${EKS_CLUSTER_NAME} \\     --namespace gateway-api-controller \\     --waitEnter fullscreen modeExit fullscreen modeThe controller will now be running as a deployment:kubectl get deployment -n gateway-api-controller NAME                                                  READY   UP-TO-DATE   AVAILABLE   AGE gateway-api-controller-aws-gateway-controller-chart   2/2     2            2           24sEnter fullscreen modeExit fullscreen modeService network:The Gateway API controller has been configured to create a VPC Lattice service network and associate a Kubernetes cluster VPC with it automatically. A service network is a logical boundary that's used to automatically implement service discovery and connectivity as well as apply access and observability policies to a collection of services.It offers inter-application connectivity over HTTP, HTTPS, and gRPC protocols within a VPC. As of today, the controller supports HTTP and HTTPS.Before creating a Gateway, we need to formalize the types of load balancing implementations that are available via the Kubernetes resource model with a GatewayClass.The controller that listens to the Gateway API relies on an associated GatewayClass resource that the user can reference from their Gateway:# Create a new Gateway Class for AWS VPC lattice provider apiVersion: gateway.networking.k8s.io/v1 kind: GatewayClass metadata:   name: amazon-vpc-lattice spec:   controllerName: application-networking.k8s.aws/gateway-api-controllerEnter fullscreen modeExit fullscreen modeThe following YAML will create a Kubernetes Gateway resource which is associated with a VPC Lattice Service Network.apiVersion: gateway.networking.k8s.io/v1 kind: Gateway metadata:   name: ${EKS_CLUSTER_NAME}   namespace: checkout spec:   gatewayClassName: amazon-vpc-lattice   listeners:   - name: http     protocol: HTTP     port: 80Enter fullscreen modeExit fullscreen modeApply it and verify that eks-workshop gateway is created:$ kubectl get gateway -n checkout NAME                CLASS                ADDRESS   PROGRAMMED   AGE eks-workshop        amazon-vpc-lattice             True         29sEnter fullscreen modeExit fullscreen modeOnce the gateway is created, find the VPC Lattice service network. Wait until the status is Reconciled (this could take about five minutes).$ kubectl describe gateway ${EKS_CLUSTER_NAME} -n checkout apiVersion: gateway.networking.k8s.io/v1 kind: Gateway status:    conditions:       message: 'aws-gateway-arn: arn:aws:vpc-lattice:us-west-2:1234567890:servicenetwork/sn-03015ffef38fdc005'       reason: Programmed       status: \"True\"  $ kubectl wait --for=condition=Programmed gateway/${EKS_CLUSTER_NAME} -n checkoutEnter fullscreen modeExit fullscreen modeNow you can see the associated Service Network created in the VPC console under the Lattice resources in the AWS console.Configuring routes:In this section, we will show how to use Amazon VPC Lattice for advanced traffic management with weighted routing for blue/green and canary-style deployments.Let's deploy a modified version of the checkout microservice with the added prefix \"Lattice\" in the shipping options. Let's deploy this new version in a new namespace (checkoutv2) using Kustomize.git clone https://github.com/seifrajhi/gateaway-api-vpc-lattice.git kubectl apply -f gateaway-api-vpc-lattice/abtesting/Enter fullscreen modeExit fullscreen modeThe checkoutv2 namespace now contains a second version of the application, while using the same redis instance in the checkout namespace.$ kubectl get pods -n checkoutv2 NAME                        READY   STATUS    RESTARTS   AGE checkout-854cd7cd66-s2blp   1/1     Running   0          26sEnter fullscreen modeExit fullscreen modeNow let's demonstrate how weighted routing works by creating HTTPRoute resources.First we'll create a TargetGroupPolicy that tells Lattice how to properly perform health checks on our checkout service:kubectl apply -f gateaway-api-vpc-lattice/target-group-policy/target-group-policy.yamlEnter fullscreen modeExit fullscreen modeNow create the Kubernetes HTTPRoute route that distributes 75% traffic to checkoutv2 and remaining 25% traffic to checkout:kubectl apply -f gateaway-api-vpc-lattice/routes/checkout-route.yamlEnter fullscreen modeExit fullscreen modeThis creation of the associated resources may take 2\u20133 minutes, run the following command to wait for it to complete:$kubectl wait -n checkout - timeout=3m \\  - for=jsonpath='{.status.parents[-1:].conditions[-1:].reason}'=ResolvedRefs httproute/checkoutrouteEnter fullscreen modeExit fullscreen modeOnce completed you will find the HTTPRoute's DNS name from HTTPRoute status (highlighted here on the message line):$ kubectl describe httproute checkoutroute -n checkout Name:         checkoutroute Namespace:    checkout Labels:       <none> Annotations:  application-networking.k8s.aws/lattice-assigned-domain-name:                 checkoutroute-checkout-0d8e3f4604a069e36.7d67968.vpc-lattice-svcs.eu-west-1.on.aws API Version:  gateway.networking.k8s.io/v1beta1 Kind:         HTTPRoute ... Status:   Parents:     Conditions:       Last Transition Time:  2023-06-12T16:42:08Z       Message:               DNS Name: checkoutroute-checkout-0d8e3f4604a069e36.7d67968.vpc-lattice-svcs.us-east-2.on.aws       Reason:                ResolvedRefs       Status:                True       Type:                  ResolvedRefs ...Enter fullscreen modeExit fullscreen modeNow you can see the associated Service created in the VPC Lattice console under the Lattice resources.\ud83d\ude80 Summary:In conclusion, the integration of Gateway API with AWS EKS and VPC Lattice has demonstrated significant improvements in networking efficiency and scalability.By optimizing configurations, this integration enhances the performance of AWS EKS clusters and simplifies networking tasks.Overall, it represents a promising advancement in AWS EKS networking, paving the way for smoother operations and increased resilience."}
{"title": "How To Implement AWS SSB Controls in Terraform - Part 3", "published_at": 1711337140, "tags": ["aws", "terraform", "security"], "user": "Anthony Wat", "url": "https://dev.to/aws-builders/how-to-implement-aws-ssb-controls-in-terraform-part-3-2p6", "details": "IntroductionTheAWS Startup Security Baseline (SSB)defines a set of controls that comprise a lean but solid foundation for the security posture of your AWS accounts. Inpart 1andpart 2of ourblog series, we examined how to implement account controls using Terraform. In this installment, we will look at the workload controls that focus on access to your workload infrastructure and protection of your data in AWS. Let's start with WKLD.01, which is about using IAM roles to provide AWS resources with access to other AWS services.WKLD.01 \u2013 Use IAM Roles for PermissionsThe workload controlWKLD.01requires using IAM roles with all supported compute environments to grant them appropriate permissions to access other AWS services and resources.The use oftemporary or short-term credentialsvia IAM roles and identity federation is significantly more secure than long-term credentials such as IAM users and access keys, which could cause serious harm if they are compromised. In the case of AWS compute services, the instance typically assumes an IAM role using AWS Security Token Service (AWS STS) to generate temporary credentials to gain access as defined by the role permissions. The following is the list of common AWS compute services and the feature that supports IAM role assumption:ServiceFeatureTerraform resource and argumentAmazon EC2Instance profileaws_iam_instance_profileresourceandiam_instance_profileinaws_instanceAmazon ECSTask IAM roletask_role_arninaws_ecs_task_defintionAmazon EKSIAM roles for service accounts (IRSA)iam-role-for-service-accounts-eksmoduleAmazon EKSEKS Pod Identitiesaws_eks_pod_identity_associationresourceAWS App RunnerInstance roleinstance_role_arninaws_apprunner_serviceAWS LambdaLambda execution roleroleinaws_lambda_functionInACCT.04 in part 1 of the blog series, we have already looked at an example that assigns an execution role that allows fetching CloudWatch metrics and sending emails via SES to a Lambda function. It works very similarly for ECS tasks and App Runner services, so we won't provide more examples for brevity.For EC2 instances, there is an additional step of creating aninstance profilethat is associated with the target IAM role and attaching it to the EC2 instances. Here is an example that enables SSM for an EC2 instance:data\"aws_ami\"\"ubuntu\"{most_recent=truefilter{name=\"name\"values=[\"ubuntu/images/hvm-ssd/ubuntu-jammy-22.04-amd64-server-*\"]}filter{name=\"virtualization-type\"values=[\"hvm\"]}owners=[\"099720109477\"]# Canonical}data\"aws_iam_policy\"\"ssm_managed_instance_core\"{name=\"AmazonSSMManagedInstanceCore\"}resource\"aws_iam_role\"\"ssm\"{name=\"SSMDomainJoinRoleForEC2\"assume_role_policy=jsonencode({Version=\"2012-10-17\"Statement=[{Action=\"sts:AssumeRole\"Effect=\"Allow\"Sid=\"\"Principal={Service=\"ec2.amazonaws.com\"}}]})managed_policy_arns=[data.aws_iam_policy.ssm_managed_instance_core.arn]}resource\"aws_iam_instance_profile\"\"ssm\"{name=aws_iam_role.ssm.namerole=aws_iam_role.ssm.name}resource\"aws_instance\"\"web\"{ami=data.aws_ami.ubuntu.idiam_instance_profile=aws_iam_instance_profile.ssm.nameinstance_type=\"t3.micro\"subnet_id=data.aws_subnet.private.id}Enter fullscreen modeExit fullscreen modeFor EKS with IRSA, theiam-role-for-service-accounts-ekssubmoduleof theterraform-aws-iammodulecan be useful especially if you use theterraform-aws-eks moduleto manage your EKS resources. Refer to the module documentation as linked above for details.For EKS with Pod Identities, the blog postAWS EKS: From IRSA to Pod Identity With Terraformby Marco Sciatta provides a decent walkthrough on how to configure it in Terraform.Don't forget to apply least privilege permissions as a best practice!WKLD.02 \u2013 Use Resource-Based PoliciesThe workload controlWKLD.02recommends usingresource-based policiesto provide additional access control at the resource level.In the situation where both types of policies are set, thepermission evaluation logictakes the union of allow permissions from the identity-based permissions of a user and the resource-based permission of the resource being access to determine the effective access level. Explicit deny permissions from either policies will take precedence to prevent access as usual.Resource-based policies are especially effective when combined withpolicy conditionswithglobal condition context keyssuch asaws:PrincipalOrgID, which specifies the organization ID of the principal.Many AWS resources support resource-based policies - here are a few common examples:ServiceFeatureTerraform resourceAmazon S3Bucket policyaws_s3_bucket_policyAmazon SQSQueue policyaws_sqs_queue_policyAWS KMSKey policyaws_kms_key_policyFor the full list of AWS services that support resource-based policies, refer to the table inAWS services that work with IAM.To define resource-based policies in Terraform, let's look at the following example that provisions an S3 bucket that uses a KMS customer-managed key (CMK) with both an S3 bucket policy and a KMS key policy:data\"aws_caller_identity\"\"this\"{}locals{account_id=data.aws_caller_identity.this.account_id}resource\"aws_iam_role\"\"top_secret_reader\"{name=\"TopSecretReaderRole\"assume_role_policy=jsonencode({Version=\"2012-10-17\"Statement=[{Action=\"sts:AssumeRole\"Effect=\"Allow\"Principal={AWS=\"arn:aws:iam::${local.account_id}:root\"}}]})inline_policy{name=\"ListAllMyBuckets\"policy=jsonencode({Statement=[{Version=\"2012-10-17\"Action=\"s3:ListAllMyBuckets\"Effect=\"Allow\"Resource=\"*\"}]})}}resource\"aws_kms_key\"\"this\"{description=\"KMS key for S3 SSE\"}resource\"aws_kms_alias\"\"this\"{name=\"alias/s3_sse_key\"target_key_id=aws_kms_key.this.key_id}resource\"aws_kms_key_policy\"\"this\"{key_id=aws_kms_key.this.idpolicy=jsonencode({Statement=[{Sid=\"Enable IAM user and AWS service permissions\"Action=\"kms:*\"Effect=\"Allow\"Principal={AWS=\"arn:aws:iam::${local.account_id}:root\"}Resource=\"*\"},{Sid=\"Allow use of the key\"Action=[\"kms:Encrypt\",\"kms:Decrypt\",\"kms:ReEncrypt*\",\"kms:GenerateDataKey*\",\"kms:DescribeKey\"]Effect=\"Allow\"Principal={AWS=\"arn:aws:iam::${local.account_id}:role/TopSecretReaderRole\"}Resource=\"*\"}]Version=\"2012-10-17\"})}resource\"aws_s3_bucket\"\"this\"{bucket=\"top-secret-bucket-${local.account_id}\"}resource\"aws_s3_bucket_server_side_encryption_configuration\"\"this\"{bucket=aws_s3_bucket.this.idrule{apply_server_side_encryption_by_default{kms_master_key_id=aws_kms_key.this.key_idsse_algorithm=\"aws:kms\"}}}resource\"aws_s3_bucket_policy\"\"this\"{bucket=aws_s3_bucket.this.idpolicy=jsonencode({Statement=[{Sid=\"Allow listing of the bucket\"Action=[\"s3:ListBucket\",\"s3:GetBucketLocation\"]Effect=\"Allow\"Principal={AWS=\"arn:aws:iam::${local.account_id}:role/TopSecretReaderRole\"}Resource=aws_s3_bucket.this.arn},{Sid=\"Allow read of objects in the bucket\"Action=[\"s3:GetObject\",\"s3:GetObjectVersion\"]Effect=\"Allow\"Principal={AWS=\"arn:aws:iam::${local.account_id}:role/TopSecretReaderRole\"}Resource=\"${aws_s3_bucket.this.arn}/*\"}]Version=\"2012-10-17\"})}Enter fullscreen modeExit fullscreen modeA role calledTopSecretReaderRolehas minimal permission assigned via an identity-based policy, so that we can defer access control via resource-based policy. The KMS key policy provides access to use the key for decryption, while the S3 bucket policy provides read-only access to the bucket and its objects that are encrypted with the same KMS key. To test the access, you can first upload a file to the S3 bucket with an IAM user/role that has access, then assumeTopSecretReaderRoleand verify that you can download the file from the S3 bucket.Once again, ensure that you practice the least privilege principle when defining your resource-based policies.WKLD.03 \u2013 Use Ephemeral Secrets or a Secrets-Management ServiceThe workload controlWKLD.03recommends using either ephemeral secrets or a secrets-management service for applications in AWS.There are two main AWS services that support secret management:AWS Secrets ManagerAWS Systems Manager Parameter StoreSecrets Manager is a comprehensive secrets management service that provides features such as secrets rotation, monitoring, and auditing for compliance. Many AWS services integrate with Secrets Manager to store and retrieve credentials and sensitive data.Meanwhile, Parameter Store offers a simple option to store secrets alongside other related parameters. While it lacks features such as secrets rotation and have smaller size limit, Parameter Store is free to use and is a great option for storing application and service settings.\ud83d\udca1It is imperative that you use theS3 backendorTerraform Cloudwith appropriate security configuration to store the Terraform state remotely. Remote states are loaded into memory when Terraform runs, so thesensitive datathat are stored in the state in plain text would not be persisted locally and risks exposure.Personally I don't find it natural to manage secrets in Terraform and generally avoid it. I would instead create secrets outside Terraform and use theaws_secretsmanager_secretdata sourceand theaws_secretsmanager_secret_versiondata sourceto retrieve secrets for use in resource arguments. For example:data\"aws_secretsmanager_secret\"\"fsx_init_admin_pwd\"{name=\"aws/fsx/my-ontap-fs/initial-admin-password\"}data\"aws_secretsmanager_secret_version\"\"fsx_init_admin_pwd\"{secret_id=data.aws_secretsmanager_secret.fsx_init_admin_pwd.id}locals{# Use local.fsx_init_admin_pwd to set the fsx_admin_password arg of the aws_fsx_ontap_file_system resourcefsx_init_admin_pwd=jsondecode(data.aws_secretsmanager_secret_version.fsx_init_admin_pwd.secret_string)[\"password\"]}Enter fullscreen modeExit fullscreen modeAs an additional reference, the AWS prescriptive guidanceSecuring sensitive data by using AWS Secrets Manager and HashiCorp Terraformprovides some general best practices and considerations.As for Parameter Store, I also take the same approach and would prefer managing secure parameters outside of Terraform and use theaws_ssm_parameterdata sourceto retrieve them for use. Here is an example:data\"aws_ssm_parameter\"\"fsx_init_admin_pwd\"{name=\"/fsx/my-ontap-fs/initial-admin-password\"}locals{# Use local.fsx_init_admin_pwd to set the fsx_admin_password arg of the aws_fsx_ontap_file_system resourcefsx_init_admin_pwd=data.aws_ssm_parameter.slack_token.insecure_value}Enter fullscreen modeExit fullscreen modeWKLD.04 \u2013 Protect Application SecretsThe workload controlWKLD.04implores that you incorporate checks for exposed secrets as part of your commit and code review processes. This is outside the scope of Terraform, so we will move on to the next control.WKLD.05 \u2013 Detect and Remediate Exposed SecretsThe workload controlWKLD.05recommends deploying a solution to detect application secrets in source code.Amazon CodeGuru Security, a feature ofAmazon CodeGuru, is a static application security tool that uses machine learning to detect security policy violations and vulnerabilities. In particular, it can detect unprotected secrets. The service is \"enabled\" by configuring a CI pipeline for supported platforms, including GitHub, BitBucket, GitLab, and AWS CodePipeline. The third-party solutions require an OIDC provider and an IAM role to be created, which CodeGuru Security provides CloudFormation templates for. So you can either convert them into Terraform for deployment, or deploy them directly in Terraform using theaws_cloudformation_stackresource. Here is an example for GitHub integration:resource\"aws_cloudformation_stack\"\"codeguru_security_github\"{name=\"codeguru-security-github\"# The template URL is obtainedtemplate_url=\"https://codeguru-security-371921485547.s3-accesspoint.us-east-1.amazonaws.com/setup-github.yml\"parameters={Repository=\"my-org/my-repo\"}capabilities=[\"CAPABILITY_NAMED_IAM\"]}Enter fullscreen modeExit fullscreen modeThe template URL is obtained in the AWS Management Console by clicking on theOpen template in CloudFormationbutton once you select an integration, as shown in the screenshots below:WKLD.06 \u2013 Use Systems Manager Instead of SSH or RDPThe workload controlWKLD.06recommends the use ofAWS Systems Manager Session Managerto securely access EC2 instances instead of placing them in a public subnet, or using a jump box or a bastion host.With Session Manager, a user with the necessary IAM permissions can connect to an EC2 instance (to be precise, the SSM Agent running on the instance) using the browser-based shell in the AWS Management Console or the AWS CLI with the Session Manager plugin. This method only requires outbound traffic to the SSM service endpoints (either through the internet or VPC endpoints), and does not require enabling SSH and RDP traffic from the internet or from a jump box or bastion host. This results in better security and governance.To enable Sessions Manager on an EC2 instance in Terraform, you need to create an instance profile with the AWS-managed policyAmazonSSMManagedInstanceCoreattached and then attach it to the EC2 instance. A basic example is already provided in theWKLD.01 sectionabove, so please refer to that. Note that this example assumes that the EC2 instance is deployed to a private subnet that routes outbound internet traffic to a NAT gateway in the same VPC.If you prefer that EC2 instances communicate with SSM only within the AWS network, you candefine the VPC endpoints required by SSMin the VPC and subnet where the EC2 instances reside. This can be achieved in Terraform using theaws_vpc_endpointresourceas follows:# Example assumes that data.aws_vpc.this and data.aws_subnet.private are already defineddata\"aws_region\"\"this\"{}locals{region=data.aws_region.this.name}resource\"aws_security_group\"\"ssm_sg\"{name=\"ssm-sg\"description=\"Allow TLS inbound To AWS Systems Manager Session Manager\"vpc_id=data.aws_vpc.this.idingress{description=\"HTTPS from VPC\"from_port=443to_port=443protocol=\"tcp\"cidr_blocks=[data.aws_vpc.this.cidr_block]}egress{description=\"Allow All Egress\"from_port=0to_port=0protocol=\"-1\"cidr_blocks=[\"0.0.0.0/0\"]}}resource\"aws_vpc_endpoint\"\"ssm\"{vpc_id=data.aws_vpc.this.idsubnet_ids=[data.aws_subnet.private.id]service_name=\"com.amazonaws.${local.region}.ssm\"vpc_endpoint_type=\"Interface\"security_group_ids=[aws_security_group.ssm_sg.id]private_dns_enabled=true}resource\"aws_vpc_endpoint\"\"ec2messages\"{vpc_id=data.aws_vpc.this.idsubnet_ids=[data.aws_subnet.private.id]service_name=\"com.amazonaws.${local.region}.ec2messages\"vpc_endpoint_type=\"Interface\"security_group_ids=[aws_security_group.ssm_sg.id,]private_dns_enabled=true}resource\"aws_vpc_endpoint\"\"ssmmessages\"{vpc_id=data.aws_vpc.this.idsubnet_ids=[data.aws_subnet.private.id]service_name=\"com.amazonaws.${local.region}.ssmmessages\"vpc_endpoint_type=\"Interface\"security_group_ids=[aws_security_group.ssm_sg.id,]private_dns_enabled=true}Enter fullscreen modeExit fullscreen modeWKLD.07 \u2013 Log Data Events for Select S3 BucketsThe workload controlWKLD.07recommends logging data events for S3 buckets that contain sensitive data in CloudTrail.By default, data events are not captured in a CloudTrail trail and must be explicitly enabled. Since the volume of data events can be high depending on the access pattern of the resource, logging data events can get expensive quickly. It is therefore recommended that you log data events only for resources that contain sensitive data that warrants more scrutiny. Note that CloudTrail can also capture data events for other AWS services as listed in theAWS CloudTrail User Guide.To demonstrate how to enable CloudTrail data event logging in Terraform, we will extend the basic example fromACCT.07 in part 2 of the blog seriesand capture data events from an S3 bucket using anadvanced event selector. Here is the Terraform configuration:data\"aws_caller_identity\"\"this\"{}data\"aws_region\"\"this\"{}locals{account_id=data.aws_caller_identity.current.account_idregion=data.aws_region.this.name}resource\"aws_s3_bucket\"\"top_secret\"{bucket=\"top-secret-${local.account_id}-${local.region}\"}# Note: Bucket versioning and server-side encryption are not shown for brevityresource\"aws_s3_bucket\"\"cloudtrail\"{bucket=\"aws-cloudtrail-logs-${local.account_id}-${local.region}\"}resource\"aws_s3_bucket_policy\"\"cloudtrail\"{bucket=aws_s3_bucket.cloudtrail.idpolicy=<<-EOT{   \"Version\": \"2012-10-17\",   \"Statement\": [     {       \"Sid\": \"AWSCloudTrailAclCheck\",       \"Effect\": \"Allow\",       \"Principal\": {         \"Service\": \"cloudtrail.amazonaws.com\"       },       \"Action\": \"s3:GetBucketAcl\",       \"Resource\": \"${aws_s3_bucket.cloudtrail.arn}\"     },     {       \"Sid\": \"AWSCloudTrailWrite\",       \"Effect\": \"Allow\",       \"Principal\": {         \"Service\": \"cloudtrail.amazonaws.com\"       },       \"Action\": \"s3:PutObject\",       \"Resource\": \"${aws_s3_bucket.cloudtrail.arn}/AWSLogs/${local.account_id}/*\",       \"Condition\": {         \"StringEquals\": {           \"s3:x-amz-acl\": \"bucket-owner-full-control\"         }       }     }   ] }EOT}resource\"aws_cloudtrail\"\"this\"{name=\"aws-cloudtrail-logs-${local.account_id}-${local.region}\"s3_bucket_name=aws_s3_bucket.cloudtrail.idenable_log_file_validation=trueis_multi_region_trail=trueadvanced_event_selector{field_selector{field=\"eventCategory\"equals=[\"Management\"]}}advanced_event_selector{name=\"Log all S3 objects events for the top secret bucket\"field_selector{field=\"eventCategory\"equals=[\"Data\"]}field_selector{field=\"resources.ARN\"starts_with=[\"${aws_s3_bucket.top_secret.arn}/\"]}field_selector{field=\"resources.type\"equals=[\"AWS::S3::Object\"]}}}Enter fullscreen modeExit fullscreen modeWKLD.08 \u2013 Encrypt Amazon EBS VolumesThe workload controlWKLD.08encrypting all Amazon EBS volumes.AWS provides server-side encryption to many services that store data, so that end-users can practice encryption at rest with minimal effort. For EBS volumes, there are two methods toencrypt EBS volumes:Enable EBS encryption by default at the regional level.Enable EBS volume encryption when the EBS volume is created either individually or as part of provisioning an EC2 instance.In both cases, you can either use the AWS-managed keyaws/ebsor provide your own KMS CMK. The latter supportsseamless key rotationfor added security and compliance.To enable EBS encryption by default using Terraform, use theaws_ebs_encryption_by_defaultresource, and optionally theebs_default_kms_keyresourceif you wish to use a KMS CMK, as follows:\ud83d\udca1 The KMS key policy is not defined in the Terraform configuration for brevity. Ensure that you define one as per theWKLD.02 sectionabove.resource\"aws_kms_key\"\"ebs\"{description=\"CMK for EBS encryption\"enable_key_rotation=true}resource\"aws_ebs_encryption_by_default\"\"this\"{enabled=true}# Optional - the AWS-managed key will be used if this resource is not usedresource\"aws_ebs_default_kms_key\"\"this\"{key_arn=aws_kms_key.ebs.arn}Enter fullscreen modeExit fullscreen modeTo encrypt the EBS volumes of an EC2 instance, you can set theencryptedargumentand optionally thekms_key_idargumentin the*_block_deviceconfiguration blocks in theaws_instanceresource. Likewise, these arguments are also applicable to theaws_ebs_volumeresourceif you are provisioning EBS volumes individually. Here is a basic example that uses the default AWS-managed key for the root block device and an additional volume:data\"aws_ami\"\"ubuntu\"{most_recent=truefilter{name=\"name\"values=[\"ubuntu/images/hvm-ssd/ubuntu-jammy-22.04-amd64-server-*\"]}filter{name=\"virtualization-type\"values=[\"hvm\"]}owners=[\"099720109477\"]# Canonical}resource\"aws_instance\"\"app_server\"{ami=data.aws_ami.ubuntu.idinstance_type=\"t3.large\"subnet_id=data.aws_subnet.private.idroot_block_device{volume_type=\"gp3\"volume_size=20delete_on_termination=trueencrypted=true}}resource\"aws_ebs_volume\"\"app_server_data\"{availability_zone=\"us-east-1a\"type=\"gp3\"size=50encrypted=true}resource\"aws_volume_attachment\"\"app_server_data\"{device_name=\"xvdb\"volume_id=aws_ebs_volume.app_server_data.idinstance_id=aws_instance.app_server.id}Enter fullscreen modeExit fullscreen mode\u26a0 Note that you cannot update the Terraform configuration directly to encrypt an unencrypted EBS volume. Doing so will force a replacement of the resource and will destroy your data. You must follow a process such as what is described inthis AWS re:Post KB articleor implement anautomated solutionto first enable encryption, then reconcile your Terraform configuration.WKLD.09 \u2013 Encrypt Amazon RDS DatabasesThe workload controlWKLD.09requires encrypting all Amazon RDS databases.Encrypting an RDS DB instanceis very similar to the encrypting EBS volumes. You can either use the default AWS-managed KMS keyaws/rdsor supply a KMS CMK. In Terraform, you can set thestorage_encryptedargumentand optionally thekms_key_idargumentin theaws_db_instanceresource. Here is a basic example that uses the default AWS-managed key:resource\"aws_db_instance\"\"app_db\"{allocated_storage=50db_name=\"appdb\"engine=\"mysql\"engine_version=\"8.0\"instance_class=\"db.t3.large\"manage_master_user_password=trueparameter_group_name=\"default.mysql8.0\"skip_final_snapshot=truestorage_encrypted=trueusername=\"mysqladm\"}Enter fullscreen modeExit fullscreen mode\u26a0 You can only enable storage encryption of an RDS DB instance at the time of creation. To encrypt it afterwards, you must manually create a snapshot, create an encrypted copy of the snapshot, and restore it as a new DB instance. Then you can reconcile your Terraform configuration.SummaryIf you have followed this blog post this far, great job! This was a lot of information, but it is important to fully understand the best practices in controlling access and safeguarding your workload and their data. In thenext installmentof theblog series, we will wrap up with the remaining workload-level controls that focus on network security. Please look forward to it and check out other posts in theAvangards Blog."}
{"title": "Como criar um backend pro seu frontend com AWS Lambdas", "published_at": 1711336811, "tags": ["frontend", "webdev", "aws", "javascript"], "user": "Ricardo Mello", "url": "https://dev.to/aws-builders/como-criar-um-backend-pro-seu-frontend-com-aws-lambdas-5fk9", "details": "Embora eu n\u00e3o seja o cara do backend, eu sempre tive muito contato com o desenvolvimento de APIs mesmo enquanto front, mas Lambdas foram algo que eu sempre ignorei.Primeiro eu achava complicado, depois desnecess\u00e1rio porque eu achava mais f\u00e1cil subir uma aplica\u00e7\u00e3o, at\u00e9 o dia que eu resolvi dar uma chance pro neg\u00f3cio e cara, vale a pena.No youtube tem v\u00eddeo antigo falando sobre isso, mas se voc\u00ea \u00e9 velho que nem eu e gosta de ler, vamo l\u00e1!Ah, e pra esse exemplo eu vou criar uma lambda que busca os dados da Faker Api. \u00c9 um exemplo b\u00e1sico, mas mais pra frente eu vou explicar alguns casos de uso reais onde ele pode ser \u00fatil.Criando a lambdaA real \u00e9 que voc\u00ea n\u00e3o precisa ser o bich\u00e3o do backend pra trabalhar com lambdas. Criar uma \u00e9 bem simples e pode ser criada direto do painel da AWS. Pra come\u00e7ar, acesse opainel das lambdase clique emCreate Function.Ao criar, voc\u00ea pode usar um blueprint que j\u00e1 cont\u00e9m alguns exemplos como conectar ao DynamoDB ou ao S3, mas pro nosso caso n\u00f3s vamos deAuthor from Scratchmesmo.D\u00ea um nome pra function, que eu vou chamar defaker-api, e altere o campo architecture pra arm64 que tem um custo menor do que a x86. Voc\u00ea pode conferir o meu setup aqui:Implementando a Faker ApiFunction criada, agora vamos implementar o nosso c\u00f3digo pra buscar os dados da faker api. Voc\u00ea deve substituir o c\u00f3digo padr\u00e3o do index.mjs por esse aqui:import*ashttpsfrom'https';functiongetData(){constapiUrl=`https://fakerapi.it/api/v1/persons?_quantity=1&_gender=male&_birthday_start=2005-01-01`;returnnewPromise((resolve,reject)=>{constreq=https.get(apiUrl,{},res=>{letrawData='';res.on('data',chunk=>{rawData+=chunk;});res.on('end',()=>{try{resolve(JSON.parse(rawData));}catch(err){reject(newError(err));}});});req.on('error',err=>{reject(newError(err));});});}exportconsthandler=async(body)=>{constres=awaitgetData();constresponse={statusCode:200,body:res,};returnresponse;};Enter fullscreen modeExit fullscreen modeExplicando o c\u00f3digo acima:N\u00f3s temos a arrow functionhandler, que \u00e9 respons\u00e1vel por executar a lambda. Dentro dela, eu t\u00f4 chamando a fun\u00e7\u00e3ogetDataque por sua vez faz uma requisi\u00e7\u00e3o pra Faker Api utilizando a apihttpsdo Node. Em seguida, faz o parser da response e retorna os dados.A essa altura a sua lambda deve estar assim:Clique emdeploypara publicar \ud83d\ude42Criando uma URL p\u00fablicaPor \u00faltimo, v\u00e1 na abaConfiguration, clique emFunction urleCreate function url.Vamos deixar oAuth TypecomoNONEe em Additional Settings, marque a caixaConfigure cross-origin resource sharing (CORS). Depois, \u00e9 s\u00f3 clicar emSave. Voc\u00ea vai ver uma URL criada similar a essa aqui:Basta abrir o link no seu navegador que voc\u00ea vai conseguir ver os resultados. E com isso voc\u00ea tem um endpoint seu publicado pra utilizar no frontend. \u00c9 s\u00f3 consumir com a fun\u00e7\u00e3ofetch.constgetData=async()=>{fetch('https://dz4y7rvv2az4iopdqzvgneefhy0jdfif.lambda-url.us-east-1.on.aws').then((response)=>response.json()).then((data)=>{console.log('success:',data)}).catch((error)=>{console.error('error:',error);});};Enter fullscreen modeExit fullscreen modeT\u00e1, agora vamos aos esclarecimentosVoc\u00ea deve estar se perguntando: pra que diabos eu preciso de uma lambda pra chamar outra API p\u00fablica se eu posso chamar ela diretamente? E a realidade \u00e9 que voc\u00ea n\u00e3o precisa. Por\u00e9m, existem alguns casos onde esse tipo b\u00e1sico de lambda vai te ajudar bastante:Proxy de APINem sempre voc\u00ea vai querer que o frontend se conecte no seu endpoint ou possa manipular os resultados da pesquisa. No nosso exemplo, a lambda est\u00e1 filtrando os resultados da Faker Api pra sempre mostrar uma pessoa do g\u00eanero masculino nascida a partir de 01/01/2005, e o frontend (ou um usu\u00e1rio mal intencionado) n\u00e3o consegue mudar os filtros aplicados.Em um caso de uso real, eu subi um servidor do Strapi e usei a lambda pra expor algumas collections sem expor a API inteira. Nesse caso, a minha lambda tinha a apiKey pra consultar o Strapi e s\u00f3 retornava o que era estritamente necess\u00e1rio pro frontend funcionar. Praticamente um BFF.MockSe voc\u00ea precisar subir um servi\u00e7o pra simular uma API que ainda n\u00e3o est\u00e1 constru\u00edda, voc\u00ea pode usar essa mesma lambda retornando dados est\u00e1ticos respeitando o contrato da API que ainda ser\u00e1 feita. Em minutos voc\u00ea vai ter um endpoint pra trabalhar que vai estar acess\u00edvel de qualquer lugar.\u00c9 isso! E a\u00ed, curtiu? Tem alguma d\u00favida? Se tiver qualquer coisa que eu possa fazer pra tornar esse artigo melhor, seja reclamar, elogiar ou sugerir outro artigo, manda ver nos coment\u00e1rios. Feedbacks s\u00e3o sempre super bem vindos."}
{"title": "AWS EC2 IMDS(Instance Metadata Service ) all that you need to know", "published_at": 1711320986, "tags": ["aws", "security", "awsec2", "awsiam"], "user": "Armando Contreras", "url": "https://dev.to/aws-builders/aws-ec2-imdsinstance-metadata-service-all-that-you-need-to-know-2144", "details": "In my detailed study of the AWS EC2 service, I came across a common configuration called IMDS, which stands for Instance Metadata Service. If you\u2019ve launched an EC2 instance or created a launch template for an Autoscaling group, you\u2019ve likely seen this option.So, what is it? IMDS is a local service endpoint that your services, scripts, or applications within your EC2 instances can connect to in order to acquire instance metadata, such as hostname, events, security groups, orAWS credentials. It is important to note thatyou can only access this endpoint from the instance itself.The data shared by this service is not protected by authentication or any cryptographic method.This means that anyone who has access to the instance from the inside can access this endpoint. Therefore, you should never store sensitive data like passwords in the data used by your launch template.the service is exposed in two ip\u2019s addresses: for ipv4169.254.169.254or over the IPv6 protocol, it\u2019s address is[fd00:ec2::254]. The IPv6 address of the IMDS is compatible with IMDSv2 commands. The IPv6 address is only accessible on instancias powered by nitro systemHow to use it?this service comes in two flavors.IMDSv1\u2014 a request/response methodIMDSv2 \u2014 a session-oriented methodyou can have enabled both or just one. The PUT or GET headers are unique to IMDSv2. If these headers are present in the request, then the request is intended for IMDSv2. If no headers are present, it is assumed the request is intended for IMDSv1. my recomendation is just to use the v2 due to its security imporvements that i will comment next.Add defense in depth against open firewalls, reverse proxies, and SSRF vulnerabilities with enhancements to the EC2 Instance Metadata Service | AWS Security Blogaws.amazon.comThis blog provides a detailed explanation of the security improvements in V2 compared to V1, which I recommend reading. To provide an overview, IMDSv2 requires an authenticated session for each request. A session can be requested via an HTTP PUT request, with a header that sets the session token duration(with a maximum of 6 hours 21600 seconds). There is no limit on the number of requests made by a session or the number of sessions.This token is only usable within the same EC2 instance.TOKEN=(curl-XPUT\"http://$IP_ADDRESS_IMDSV2/latest/api/token\"-H\"X-aws-ec2-metadata-token-ttl-seconds: 3600\")curl\"http://$IP_ADDRESS_IMDSV2/latest/meta-data/profile\"-H\"X-aws-ec2-metadata-token:$TOKEN\"curl\"http://$IP_ADDRESS_IMDSV2/latest/user-data\"-H\"X-aws-ec2-metadata-token:$TOKEN\"curl\"http://$IP_ADDRESS_IMDSV2/latest/dynamic\"-H\"X-aws-ec2-metadata-token:$TOKEN\"Enter fullscreen modeExit fullscreen modeThe use of a PUT request adds an extra layer of security to IMDSv2 against misconfigured third-party firewalls. By default, most of these firewalls do not support PUT requests. This helps avoid unauthorized access via a misconfigured Web Application Firewall (WAF) or reverse proxy.GET and HEAD methods are allowed in IMDSv2 instance metadata requests.PUT requests are rejected if they contain an X-Forwarded-For header.knowing all this my advice is to configure your instances to use the IMDSv2 if you need it or your underlying applicaion running inside your insntace will use the isntance profile attached to the instace, a good practice with containerized workloads on eks is to restrict this endpoints to containers. One way is to block pod IMDS access is to apply a network policy, enforced by the Amazon VPC CNI or an add-on,to ensure pods are unable to reach the Instance Metadata Service.To do this, configure your network policy to block egress traffic to 169.254.0.0/16. Another way to block pod IMDS access isto require IMDSv2 to be used, and to set the maximum hop count to 1. Configuring IMDS this way will cause requests to IMDS from pods to be rejected, provided those pods do not use host networking, instead to provide iam permissions to your pods useservice accounts with OIDC via iam ROLES. this will prvent pod iam permissions escalation and ensure least-minimum privilege in your end.you can alsoprevent the launch of instance with IMDSv2 not enabled with a IAM policieslike this{\"Version\":\"2012-10-17\",\"Statement\":[{\"Sid\":\"RequireImdsV2\",\"Effect\":\"Deny\",\"Action\":\"ec2:RunInstances\",\"Resource\":\"arn:aws:ec2:*:*:instance/*\",\"Condition\":{\"StringNotEquals\":{\"ec2:MetadataHttpTokens\":\"required\"}}}]}Enter fullscreen modeExit fullscreen modeor prevent the hop size limit to exceed a fixed value{\"Version\":\"2012-10-17\",\"Statement\":[{\"Sid\":\"MaxImdsHopLimit\",\"Effect\":\"Deny\",\"Action\":\"ec2:RunInstances\",\"Resource\":\"arn:aws:ec2:*:*:instance/*\",\"Condition\":{\"NumericGreaterThan\":{\"ec2:MetadataHttpPutResponseHopLimit\":\"2\"}}}]}Enter fullscreen modeExit fullscreen modeother endpoinds available to retrieve data from the IMDS endpointscheck other iam polies related to the IMDSlastly to avoid a confussion there is an important difference between theinstance identity roleand theinstance profile. AWS services and features that are integrated to use theinstance identity rolecan use it to identify the instance to the service. The instance identity role credentials are accessible from the Instance Metadata Service (IMDS) at/identity-credentials/ec2/security-credentials/ec2-instance. and the instance profile is an iam role that allows the isntance to assume this role and then send requests to aws services.i hope this post could be useful for you. this was a feature that i didn\u2019t know until today and that is a very important security consideration for any workload on EC2 instances. : )"}
{"title": "Unlocking Next-Gen Serverless Performance: A Deep Dive into AWS LLRT", "published_at": 1711311469, "tags": ["serverless", "lambda", "javascript", "aws"], "user": "Dixit R Jain", "url": "https://dev.to/aws-builders/unlocking-next-gen-serverless-performance-a-deep-dive-into-aws-llrt-5f3f", "details": "Table of ContentsIntroductionKey FeaturesLLRT vs. Node.jsBenchmarkingAWS SDK V3 with LLRTUse CasesConfigure Lambda functions to use LLRTCompatibilityLimitations and ConsiderationsConclusionReferencesIntroductionThe transition to serverless computing signifies a pivotal shift in application deployment, focusing more on code development than on managing infrastructure. This shift underscores the need for innovations that not only reduce latency but also enhance performance.AWS LLRTemerges as a significant advancement in this domain, offering a lightweight JavaScript runtime that outperforms traditional runtimes like Node.js in efficiency and reduced resource utilization. Designed specifically to address the 'cold start' problem in serverless computing, LLRT ensures rapid startup times, thus enhancing user experience and aligning cost-effectiveness with superior performance. This development marks a paradigm shift in application responsiveness to operational demands, prioritizing speed and efficiency in serverless architecturesKey FeaturesSwift Cold-Starts: LLRT provides a significant boost in performance, offering over 10x faster startup times, which is crucial for serverless applications where cold starts can impact the user experience.Efficiency: By optimizing for low latency and efficient memory usage, LLRT ensures that applications are not only fast but also cost-effective, with up to 2x lower overall costs.Specialized for Serverless Workload: LLRT is designed specifically for serverless computing, in contrast to general-purpose runtimes such as Node.js, Bun, or Deno.LLRT vs. Node.jsThe primary distinction between LLRT and Node.js lies in their architecture and design principles. Built on QuickJS, LLRT is significantly lighter than Node.js, contributing to its quicker cold start times and reduced initialization periods. This optimization benefits serverless applications in terms of both performance and cost.LLRT's design focuses on supporting key Node APIs without aiming for full Node.js API parity. It prioritizes APIs crucial for serverless functions while offering full or partial support for built-in Node libraries, such as buffer, stream, and fetch.Technically, LLRT distinguishes itself by employing Rust for its performance and safety features and leveraging Tokio for efficient, event-driven, non-blocking I/O operations. This unique combination allows for concurrent task management without the complexities of multithreading, thereby enhancing system resource efficiency.At its core, LLRT utilizes QuickJS\u2014a compact and fast JavaScript engine that eschews complex JIT compilation in favor of immediate script execution. This characteristic is especially beneficial for serverless functions, which necessitate fast, sporadic execution times.BenchmarkingCold-Start Comparison: LLRT demonstrates significantly faster startup times compared to Node.js, indicating a 12x to 18x speed improvement. Below benchmark is captured for Lambda Function doing Dynamo PUT Operation.Warm-Start Comparison: LLRT also shows a faster warm-start time compared to Node.js, with up to a 2x to 3x speed improvement. Again, below benchmark is captured for the same Lambda Function doing Dynamo PUT Operation.These benchmarks suggest that LLRT offers a promising alternative to NodeJS for specific use cases requiring fast startup times and efficient handling of HTTP requests, especially in cost-sensitive environments due to its lower resource consumption and potentially lower operational costs on AWS Lambda.For a detailed exploration of the benchmarks and their implications for AWS Lambda users, you can refer tothisarticle on learnaws.io.AWS SDK V3 with LLRTLLRT incorporates many AWS SDK clients and utilities, finely tuned for optimal performance. These included SDK clients are engineered for best performance without compromising compatibility, replacing some JavaScript dependencies with native implementations for tasks like Hash calculations and XML parsing. For SDK packages not bundled with LLRT, they should be included with your source code, marking specific packages as external.Hereis the list AWS SDK packages bundled within the LLRT Runtime.Use CasesAWS Low Latency Runtime (LLRT) enhances serverless applications by offering rapid startup times and efficient execution, critical for performance-sensitive environments. It's ideal for:Microservices Architecture: Facilitates scalable, flexible cloud applications by enabling individual Lambda functions with quick startup times for each microservice.Real-Time Data Processing: Essential for platforms like financial trading or live gaming, where LLRT minimizes latency in streaming data processing.IoT and Edge Computing: Supports environments with strict resource and response time requirements, allowing for immediate data processing from sensors or device control.API Endpoints: Improves responsiveness of client applications by reducing latency in serverless API requests and responses, enhancing user experience.Batch Processing: Enables timely execution of time-sensitive batch jobs, such as media processing tasks, by ensuring serverless functions start and complete quickly without significant delays.Configure Lambda functions to use LLRTWe have 5 ways to configure LLRT as runtime for the lambda function:Custom runtime (recommended)ChooseCustom Runtime on Amazon Linux 2023and package the LLRTbootstrapbinary together with your JS code.For most use cases, utilizing LLRT as a custom runtime provides the optimal balance between performance and flexibility. Here's how to set it up:Download the Latest LLRT Release: Download the last LLRT release fromherePackage Your Function with LLRT: Create a deployment package by including your JavaScript code and the LLRT binary. Ensure that your project structure adheres to the AWS Lambda requirements for custom runtimes.Deploy Your Lambda Function: Use the AWS Management Console, AWS CLI, or AWS SDKs to create a new Lambda function. Specify the runtime as Custom Runtime on Amazon Linux 2023, and upload your deployment package. Set the handler information according to your function\u2019s entry point.Using a layerChooseCustom Runtime on Amazon Linux 2023, uploadllrt-lambda-arm64.ziporllrt-lambda-x64.zipas a layer and add to your functionBundle LLRT in a container imageYou can package the LLRT runtime into a container image using below DockerfileFROM--platform=arm64 busyboxWORKDIR/var/task/COPYapp.mjs ./ADDhttps://github.com/awslabs/llrt/releases/latest/download/llrt-container-arm64 /usr/bin/llrtRUNchmod+x /usr/bin/llrtENVLAMBDA_HANDLER \"app.handler\"CMD[ \"llrt\" ]Enter fullscreen modeExit fullscreen modeUsing AWS SAMYou can referexample projectto set up a lambdainstrumented with a layer containing the llrt runtime using AWS SAM.Using AWS CDKYou can usecdk-lambda-llrtconstruct libraryto deploy LLRT Lambda functions with AWS CDK.import{LlrtFunction}from\"cdk-lambda-llrt\";consthandler=newLlrtFunction(this,\"Handler\",{entry:\"lambda/index.ts\",});Enter fullscreen modeExit fullscreen modeSeeConstruct Hubandits examplesfor more details.CompatibilityLLRT supports ES2020, making it a modern runtime capable of running contemporary JavaScript code.NoteAlthough LLRT supportsES2020it'sNOTa drop in replacement for Node.js. ConsultCompatibility matrixandAPIfor more details.All dependencies should be bundled for abrowserplatform and mark included@aws-sdkpackages as external.Limitations and ConsiderationsWhile LLRT offers numerous advantages, it's important to recognize its limitations. LLRT is most effective for smaller serverless functions focused on tasks like data transformation, real-time processing, and AWS service integrations. It's designed to complement existing components rather than serve as a comprehensive replacement for all use cases. Additionally, LLRT's experimental status means it is subject to change and is intended for evaluation purposes.ConclusionAWS LLRT represents a significant step forward in serverless computing, offering developers a powerful tool for building high-performance, efficient serverless applications. By addressing the specific needs of serverless environments, LLRT enables faster application startups and lower operational costs, making it an attractive option for developers looking to optimize their serverless applications.If you liked this blog and found it helpful, do check out my other blogs on AWS:Lost in AWS CDK? Here's Your Map for Debugging Like a ProAWS Step Functions: The Conductor of Your Microservices OrchestraBuilding Event-Driven Serverless Architecture with AWS EventBridge and Serverless FrameworkUnlocking DynamoDB's Hidden Potential: Elevate Your Serverless Game with Batch Operations MasteryDive into AWS DynamoDB: A NoSQL Database for Scalable and High-Performance ApplicationsFor those interested in exploring LLRT further, the AWS Labs GitHub repository provides detailed documentation, examples, and resources to get started.See you until next time. Happy coding!.ReferencesLLRT GitHub RepositoryLLRT Lambda TuturialLLRT Benchmarking"}
{"title": "DevOps with Guruu | Chapter 9 : Deploy CI/CD with Code Pipeline | Code Build | Code Commit | Code Deploy", "published_at": 1711293492, "tags": ["webdev", "devops", "aws"], "user": "Hoang Guruu", "url": "https://dev.to/aws-builders/devops-with-guruu-chapter-9-deploy-cicd-with-code-pipeline-code-build-code-commit-code-deploy-2g8", "details": "DevOps with Guruu | Chapter 9 : Deploy CI/CD with Code Pipeline | Code Build | Code Commit | Code DeployIntroduction to DevOps and AWS CodePipeline.CI/CD pipeline for a web application.Process from code commit to build and deployment.Join me on this journey to mastering DevOps. Let's get hands-on and build a strong foundation together! Don't forget to like, share, and subscribe for more exciting content. Happy coding, future DevOps!\""}
{"title": "GenAI-Powered Digital Threads - AI Security Under the Hood, Part II", "published_at": 1711291818, "tags": ["aws", "security", "cloud", "devsecops"], "user": "David Melamed", "url": "https://dev.to/aws-builders/genai-powered-digital-threads-ai-security-under-the-hood-part-ii-5gk1", "details": "In our previous blog post onAI Security, we spoke about borrowing the concept of Digital Threads from the manufacturing world, in order to aggregate disparate company data into a single source\u2013\u2013a knowledge graph.\u00a0 This knowledge graph can provide us with important security context through transparency and visibility of our organization's many different data sources, driving greater AI-powered security, risk management, and mitigation.When we understand the source of risk, such as which repositories are actually running in production, or whether machines are exposed to the public web, we are better equipped to prioritize security risk and remediation for our organization. With alert fatigue growing across all engineering disciplines from DevOps to QA to Security, we need to work towards minimizing the noise and focusing on what really brings our organization value. This is the exact benefit that a human-language queryable graph database can deliver to our already bogged-down engineering teams.In this post, we\u2019ll dive into the technical specifics under the hood of the graph database, including:Graph database architectureTools that helped launch the applicationNotebooks used to build the knowledge graphGenAI model that enabled the querying through human language that also outputs results in human languageThis technical example was built upon an AWS AI service suite to test its capabilities, and it was pretty impressive, with minimal learning curve for the AI enthusiast. This example leveragesNeptuneas the graph database,Bedrock\u2019s Claude v3for our GenAI model and LLM, along with out-of-the-box security notebooks, to populate the data. This coupled with excellent docs and some tinkering helped wire the example into common open-source tools likeLangchain, and programming languages likeopenCypherto test out GenAI-powered context-based security in action.The Graph Database ArchitectureIn this example, we start with aGithub repositorywith two folders. You\u2019ll find aTerraformfile in one folder that enables you to bring up an end-to-end Neptune environment, which is particularly useful for those just getting started (as I had to learn to configure this from scratch).\u00a0 This Terraform will spin up a cluster, and an EC2 proxy to connect locally. Neptune comes packaged with several notebooks that also include data sources, where one such notebook is called \u201csecurity graph\u201d, which includes a dataset that was leveraged for this example.The notebook is the basis from which the data is ingested into the graph. This enables you to query for all of the specific data available in the graph. A nifty feature is that inside the notebook you can also have the option to visualize the data as a graph, so you can understand the relationships between the data sources.In this sample app and demo, one additional step was added, which was to add an EC2 instance into the same subnet, in order to be able to communicate with Neptune remotely, because by default it is inside the VPC and it is not otherwise accessible remotely. Once we have these tools and our cluster set up, it\u2019s time to connect all of this to GenAI and see what happens.Querying our Knowledge GraphOpen an SSH tunnel to connect to your Neptune cluster locally. For that use the address from the Terraform output and make sure to add it to your /etc/hosts file pointing to 127.0.0.1, using the following command:ssh -i neptune.pem -L 8182:<db-neptune-instance-id>.us-east-1.neptune.amazonaws.com:8182 <EC2 Proxy IP>Now it is possible to run the application. The application starts with a default question \u201cWhich machines do I have running in this data set?\u201dBehind the scenes, it runs the following Cypher queryMATCH m: ec2:instanceRETURN m.\u00a0 But how does it know to run this specific query? The database schema is included in the question, which is then converted into a Cypher query. Not only will you see the data output as JSON in the CLI, but if you return to the application, you will also see the generated response in natural human language.And this is just one example.Eventually it is possible to query the application and receive a relevant response to any data points available in the knowledge graph.\u00a0 So if we were to take a security example, and how we can achieve better AI-based security for our systems, we can find out if any of these machines have any public IPs or get a list of all the types of cloud resources we have in our stack (an inventory), which it will be able to aggregate based on the node labels \u2013\u2013 which is essentially a list of all of the different object types we have in our graph.A Word on Limitations & Good Practices for Optimal ResultsWhen I first started playing around with this stack testing different AI models, it worked well until one of the queries was too large (meaning the prompt that includes the graph database schema exceeded the context window), and the library threw an error.\u00a0 It also happened to me with some other questions I threw to the engine and the resulting OpenCypher query was incorrect (syntax error). This brings up the question of which model should we use?There are several models, but not all work in the same way. In this example, we leveraged Bedrock and compared several models. Claude 3 delivered the best results for this example, as did GPT-4 from OpenAI. So if you want to get started and play around, these are the two recommended models.Benchmarking and fine-tuning the AI models is a must as some will perform better than others based on your use case.In the library itself, it\u2019s possible to play around with a diversity of parameters, including the type of prompts sent to the engine, where a certain amount of fine-tuning will certainly impact the quality of the results.In addition, the more you enrich the graph with data, the richer queries you\u2019ll be able to get, and the more value you\u2019ll receive from each query. By connecting the graph to third-party resources, you can receive additional business context. For example, being able to query an HR system about active employees, and see who still has access to different cloud resources.AWS Neptune + Bedrock for Better AI SecurityIn this example, we demonstrated how with a simple AWS-based stack of Neptune and Bedrock, leveraging common OSS libraries like Langchain and Streamlit, it was possible to build a knowledge graph that simulates the same value as a digital thread in manufacturing. By consolidating disparate organizational data into a single graph database, we can then leverage the power of GenAI to query this data, receive accurate results based on this important organizational and system context, and all in human language to minimize the learning curve of acquiring a new syntax or analyzing complex raw JSON results.The more we converge data, and are able to visualize the relationship between the different data sources, the more we can have a rapid understanding of issues when they arise - from breaches to outages. This has immense benefits for engineering, security, and operations alike, enabling us to have a richer context when it comes to the root cause with greater data and context-driven visibility into our systems."}
{"title": "Understanding Security Group and Network Access Control List (NACL) in AWS", "published_at": 1711272409, "tags": ["security", "aws", "network", "devops"], "user": "Wojciech Lepczy\u0144ski", "url": "https://dev.to/aws-builders/understanding-security-group-and-network-access-control-list-nacl-in-aws-38ei", "details": "AWS provides robust security measures to protect resources within its cloud infrastructure. Two fundamental components of AWS security are Security Groups and Network Access Control Lists (NACLs). While both serve similar purposes \u2013 controlling traffic to and from AWS resources \u2013 they operate at different layers of the network stack and offer distinct features.Security GroupsSecurity Groups act as virtual firewalls for EC2 instances and other AWS resources. They regulate inbound and outbound traffic by defining rules that specify which type of traffic is allowed or denied. Here are some key points about Security Groups:Security Groups operate at the instance level and are stateful, meaning if you allow inbound traffic, the return traffic is automatically allowed regardless of outbound rules. This simplifies the management of security policies.You define inbound and outbound rules to permit specific types of traffic. For example, you can allow SSH (port 22) for administration or HTTP (port 80) for web traffic. If a rule doesn't explicitly allow traffic, it's implicitly denied.Security Groups are easy to configure through the AWS Management Console, CLI, or SDKs. You can modify rules dynamically without restarting instances.A single Security Group can be attached to multiple instances, enabling consistent security policies across resources.Network Access Control Lists (NACLs)NACLs are another layer of defense for controlling traffic at the subnet level. Unlike Security Groups, which operate at the instance level, NACLs function at the subnet level. Here's what you need to know about NACLs:Unlike Security Groups, NACLs are stateless. This means that if you allow inbound traffic, you must explicitly allow the corresponding outbound traffic and vice versa.NACLs have numbered rules that are evaluated in ascending order. Once a rule is matched, subsequent rules are not processed. This order matters when defining complex access control policies.NACLs support both allow and deny rules. However, the order of rules and the stateless nature of NACLs make it essential to carefully plan and configure rules.NACLs are associated with subnets, allowing you to control traffic entering and leaving the subnet. Each subnet in a VPC must be associated with a NACL, and by default, it allows all traffic.Best PracticesTo effectively secure your AWS infrastructure using Security Groups and NACLs, consider the following best practices:*Follow the principle of least privilege by only permitting necessary traffic. Restrict access to ports and protocols that are required for the application to function.Implement multiple layers of security using both Security Groups and NACLs. While Security Groups provide instance-level security, NACLs offer subnet-level control, adding an extra layer of defense.Regularly review and audit your security rules to ensure they align with your organization's security policies. Remove any unnecessary rules or overly permissive configurations.Utilize AWS APIs, CLI, or Infrastructure as Code (IaC) tools like AWS CloudFormation or Terraform to automate the configuration and management of Security Groups and NACLs, ensuring consistency and scalability.SummaryIn conclusion, Security Groups and Network Access Control Lists are essential components of AWS security, offering different levels of control over inbound and outbound traffic within your VPC. By understanding their differences and best practices, you can effectively secure your AWS resources and protect them from unauthorized access and malicious activity.You can see a video tutorial about NACL and SG describing the strengths, weaknesses, usage example, and a hands-on demo showing how you can create a NACL and SG and even more -YouTube videoNACL and Security Group in AWS | Demo Tutorial 2024 SG & Network Access Control List - YouTubeExploring Cloud Network Security: A Comparative Analysis of Security Groups and Network Access Control Lists (NACLs)In this informative video, I take you thr...youtube.comRelated articles on myblog https://lepczynski.it/en/aws_en/a-comparison-of-security-groups-and-network-access-control-lists-in-aws/:A Comparison of Security Groups and Network Access Control Lists in AWSEnsuring proper security is extremely important in the cloud... Security Groups and Network Access Control Lists in the AWS cloud...lepczynski.it"}
{"title": "A Ride Through Optimising Legacy Spring Boot Services For High Throughput", "published_at": 1711269622, "tags": ["springboot", "performance", "testing", "profiling"], "user": "Felipe Malaquias", "url": "https://dev.to/aws-builders/a-ride-through-optimising-legacy-spring-boot-services-for-high-throughput-477n", "details": "Oops! Did I fix it or screw it up for real?Even though we could easily scale this system up vertically and/or horizontally as desired, and the load tested was 20x the expected peak, the rate of failed responses on our load tests before my quest was about 8%.8% for me is a lot!8% of \u20ac1.000.000.000.000,00 is a lot of money for me (maybe not for Elon Musk).8% of the world\u2019s population is a good number of people8% of a gold bar \u2014 I\u2019d love to own it!8% of Charlie Sheen\u2019s ex-girlfriends, damn\u2026 That must be tough to handle!Why is that? Because of previous alarms and metrics I\u2019ve set along the way, I knew something was off, and our throughput was suboptimal, even considering the fairly small number of pods we were running for these services, even if we are talking about outdated libs/technology. And worst\u2026 we are only talking about a few hundred requests per second\u2014it should just work, and at a higher scale!As you will see at the end of this article, performing such load tests in your services can reveal a lot of real issues hidden in your architecture, code, and/or configuration. The smell that \u201csomething is off\u201d here indicated that something was indeed off, also for regular usage of those services. Chasing the root cause of problems is always worth it \u2014 never ignore errors, considering it\u2019s a \u201chiccup\u201d. There\u2019s no such thing as a \u201chiccup\u201d in software. The least that can happen is that you learn more about the software you wrote, the frameworks you use, and the infrastructure that hosts it.Tech StackAs there are so many variables in software development (pun intended), I think context is important in this case, and we will limit talking about optimizations on the following pretty common legacy tech stack (concepts apply to others as well \u2014 yes, including the latest shit):Springboot 2.3 + ThymeleafMongoDBJavaArchitectureArchitectural changes are not the focus of this article, so I assume some basic understanding of resilient architectures and I won\u2019t write about it besides giving a few notes below on what is expected you are aware of when talking about highly available systems (but again, not limited to):The network is protected (e.g., divided intopublic and privateor equivalent subnets)The network is resilient (e.g.: redundant subnets are distributed across differentavailability zones)Use clusters and multiple nodes in distributed locations when applicable (e.g.Redis cache clusters,db clusters, service instances deployed in multiple availability zones, and so on)Useload balancersand distribute load accordingly to your redundant spring boot services.Haveautoscalingin place based on common metrics (e.g.: CPU, memory, latency)Add cache and edge servers to avoid unnecessary service load when possible (e.g.:Cloudfront)Add a firewall and other mechanisms for protecting your endpoints against malicious traffic and bots before it hits your workload and consume those precious worker threads (e.g.:WAF)Health checks are setup.The minimum number of desired instances/pods is set according to your normal loadFor simplification purposes, I\u2019m reducing the context of this article (and therefore the diagram below) to study only the SpringBoot fine-tuning part of it, in a system similar to the following one:First things firstAs mentioned, the progress I\u2019m describing here was only possible due to measurements and monitoring introduced before the changes. How can you improve something if you don\u2019t know where you are and have no idea where you want to go? Set the f*cking monitoring and alarms up before you proceed with implementing that useless feature that won\u2019t work properly anyway if you don\u2019t build it right and monitor it.A few indicators you may want to monitor in advance (at least, but not limited to):SpringBoot Service:api response codes (5xx and 4xx)latency per endpointrequests per second per endpointtomcat metrics (servlet errors, connections, current threads)CPUmemoryDB Cluster:top queriesreplication latencyread/write latencyslow queriesdocument lockssystem locksCPUcurrent sessionsFor SpringBoot, this is easily measurable by enabling management endpoints and collecting the metrics using Prometheus and shipping metrics to Grafana or Cloudwatch, for example. After the metrics are shipped, set alarms on reasonable thresholds.For the database, it depends on the technology, and you should monitor it at both the client (spring boot db metrics) and server sides. Monitoring on the client side is important to see if any proxy or firewall is blocking any of your commands from time to time. Believe me, these connection drops may happen even if you test it and it seems to work just fine, in case something is not properly configured and you want to catch it! For example, a misconfiguration of outbound traffic on the DB port on your proxy sidecar may lead to dirty HTTP connections at the spring boot side that were already closed on the server side.Alright, let\u2019s crash it.It\u2019s time to set your load test based on your most crucial processes that you know will be under high pressure during peak periods (or maybe that\u2019s already your normal case, and that\u2019s what we should aim for anyway\u2026 maximum efficiency with the least amount of resources possible).In this case, we chose to use thissolution from AWSfor load testing just because the setup is very simple and we already had compatible JMeter scripts ready for use, but I\u2019d rather suggest usingdistributed Locustinstead for better reporting and flexibility.In our case, we started simulating load with 5 instances and 50 threads each, with a ramp-up period of 5 minutes. This simulates something like 250 clients accessing the system at the same time. Well, this is way above the normal load we have on those particular services anyway, but we should know the limits of our services\u2026 in this case, it was pretty much low, and we reached it quite fast \u2014 shame on us!Those metrics above were extracted from our API gateways'automatically generated Cloudwatch dashboard.There, you can see a couple of things:1- The load test starts around 15:55 and ends around 16:102- The load is only applied to one of the services (see \u201ccount\u201d metric)3- The load applied to one upstream service caused latency in three services to increase (the service the load was applied to + 2 downstream services)4- A high rate of requests failed with 500s on the service we applied the load toTherefore, we can conclude that the increased request rate caused bottlenecks in downstream services, which caused latency and probably caused upstream services to timeout, and the errors were not handled properly, resulting in 500 errors to the client (load test). As autoscaling was set up, we can also conclude another important observation: autoscaling did not help, and our services would become unavailable due to bottlenecks!Connection Pools and HTTP ClientsThe first thing I checked was the connection pools and HTTP clients set up so I could get an idea of the maximum parallel connections those services could open, how fast they would start rejecting new connections after all the current connections were busy, and how long they would wait for responses until they started to time out.In our case, we were not usingWebFlux, so I didn\u2019t want to start refactoring services and deal with breaking changes. I was more interested in first checking what I could optimize with minimal changes, preferably configuration only, and only performing larger changes if really needed. Think about the \u201cPareto rule\u201d, \u201cchoose your battles wisely\u201d, \u201ctime is money\u201d, and so on. In this case, we were using theRest Template.Let\u2019s review what a request flow would look like at a high level:So, you can see the incoming request is handled by one of the servlet container\u2019s (tomcat) threads, dispatched by Spring\u2019s DispatcherServlet to the right controller, which calls a service containing some business logic. This service then calls a downstream remote service using anHTTP clientthrough a traditional Rest template.The downstream service handles the request in a similar manner, but in this case, it interacts with MongoDB, which also uses a connection pool managed byMongo Java DriverbehindSpring Data MongoDB.The first thing I noticed was that the setup was inconsistent, sometimes with missing timeout configurations, using default connection managers, and so on, making it a bit tricky to predict consistent behavior.TimeoutsAs a good practice, one should always check timeouts. Why? Not every application is the same; hence, the defaults might not fit your use cases. For example, database drivers tend to always have absurdly long timeouts by default for queries (such as infinite), as most simple applications might do some background task for performing a query once in a while and return the result to some task, job, or something similar. However, when we are talking about high-scalable and high-throughput systems, one must not wait forever for a query to complete; otherwise, if you have any issue with some specific collection, query, index, or anything like that blocks your DB instances, you will end up piling up requests and overloading all your systems very quickly.Think of it like a very long supermarket line with a very slow cashier, where the line keeps growing indefinitely and you are at the very end of the line. You can either wait forever and maybe get to the front of the line before the shop closes (and other people will keep queueing behind you), or you (and all the others) can decide after 3s to get out of that crowded place and come back later.Timeout is a mechanism to give the clients a quick response, avoiding keeping upstream services waiting and blocking them from accepting new requests.Circuit breakers, on the other hand, are safeguards to avoid overloading your downstream services in case of trouble (connection drops, CPU overload, etc.). Circuit breakers are, for example, those waiters or waitresses who send customers back home without a chance to wait for a table when the restaurant is full.Connection PoolRemote connections, such as the ones used for communicating with databases or Rest APIs, are expensive resources to create for each request. It requires opening a connection, establishing ahandshake, verifying certificates, and so on.Connection pools allow us to reuse connections to optimize performance and increase concurrency in our applications by maintaining multiple parallel connections, each in its own thread. Given certain configurations, they also give us the flexibility to queue requests for a certain amount of time if all connections from the pool are busy so they are not immediately rejected, giving our services more chances to serve all requests successfully within a certain period.You might have a quick read of this article for more information about connectionpools.So that\u2019s more or less how it looks after the changes:@Bean HttpClient httpClient(PoolingHttpClientConnectionManager connectionManager) {     return HttpClientBuilder.create()             .setConnectionManager( connectionManager )             .build(); }  @Bean PoolingHttpClientConnectionManager connectionManager() {     PoolingHttpClientConnectionManager connectionManager = new PoolingHttpClientConnectionManager();     connectionManager.setMaxTotal( POOL_MAX_TOTAL );     connectionManager.setDefaultMaxPerRoute( POOL_DEFAULT_MAX_PER_ROUTE );     return connectionManager; }  @Bean ClientHttpRequestFactory clientHttpRequestFactory(HttpClient httpClient) {     HttpComponentsClientHttpRequestFactory factory = new HttpComponentsClientHttpRequestFactory(httpClient);     factory.setConnectTimeout(CONNECT_TIMEOUT_IN_MILLISECONDS);     factory.setReadTimeout(READ_TIMEOUT_IN_MILLISECONDS);     factory.setConnectionRequestTimeout(CONNECTION_REQUEST_TIMEOUT);     return factory; }  @Bean public RestTemplate restTemplate(ClientHttpRequestFactory clientHttpRequestFactory) {     RestTemplate restTemplate = new RestTemplateBuilder()         .requestFactory(() -> clientHttpRequestFactory)         .build();     return restTemplate; }Enter fullscreen modeExit fullscreen modeThe beans above will ensure the HTTP client used by the Rest template uses a connection manager with a reasonable amount of max connections per route and max connections in total. If there are more incoming requests than we are able to serve with those settings, they will be queued by the connection manager until the connection request timeout is reached. If no attempt to connect is performed after the connection request timeout because the request is still in the queue, the request will fail. Read more about the different types of HTTP client timeoutshere.Make sure to adjust the constants according to your needs and server resources. Be aware that one thread is open for each connection, and threads are limited by OS resources. Therefore, you can\u2019t simply increase those limits to unreasonable values.Let\u2019s try it again!So there I was. Looking forward to another try after increasing the number of parallel requests handled by the HTTP clients and seeing a better overall performance of all services!But to my surprise, this happened:So now our average latency has increased almost 8 times, and the number of errors has also increased! How come?!MongoDBLuckily, I also had a monitoring setup for our MongoDB cluster, and there, it was easy to spot the culprit! A document was locked up by several concurrent write attempts. So, the changes indeed increased throughput, and now our DB was overloaded with so many parallel writes in the same document, which caused a huge amount of time waiting for it to be unlocked for the next query to update.You may want to read more about MongoDB concurrency and lockshere.).As a consequence, the DB connection pool was busy queueing requests, and therefore, the upstream services also started to get their thread pools busy handling incoming requests due to the sync nature of rest templates waiting for a response. This increased CPU consumption in upstream services and caused higher processing times and failures, as we observed in previous graphics!As MongoDB monitoring pointed me to the exact collection containing the document that was locked and I had CPU profiling enabled (which I\u2019ll describe in the next section), I could easily find the line code causing the lock through an unnecessary save() call in the same document at each service execution for updating one single field, which, to my surprise, never changed its value.Document locks are necessary for concurrency but are no good as they can easily start blocking your DB connections, and they usually indicate problems with either your code or collections design, so always make sure to review it in case you see some indication your documents are being locked.After removing the unnecessary save() call, things started looking better \u2014 but still not good.In comparison to the initial measures, the latency is higher, though the error rate dropped to almost 1/3 of the initial amount. Also, in comparison to the first try, it seems the errors are popping up slower than before.Before proceeding to fix the next bottleneck, let\u2019s review one more thing. Ok, we had an issue in the code that caused the locks, but why did we let queries run for so long? Remember what I wrote initially at connection pools, HTTP clients, and timeout sections. The same applies here: remember to always review default values for your connections and timeouts. MongoDB allows you to overwrite defaults through itsconnection options.Connections will be created based on both minPoolSize and maxPoolSize. If queries take longer to be executed and new queries come in, new connections will be created until maxPoolSize is reached. From there, we can also define how long a query can wait to be executed with waitQueueTimeoutMS. If we are talking about DB writes, which was our case here, you should also review wtimeoutMS, which, by default, keeps the connection busy until the DB finishes the write. If setting a value different than the default (never timeout), you may also set a circuit breaker around the DB to ensure you don\u2019t overload it with additional requests. If your DB cluster contains multiple nodes, distribute the load with reads by setting readPreference=secondaryPreffered. Be aware ofconsistency, read isolation, and recency.CPU ProfilingIf you are working on performance issues, the first thing you should care about is profiling your application. This can be done locally using your favorite IDE or remotely attaching the profiler agent to your JVM process.Application profiling enables you to see which frames of your application consume the most processing time or memory.You can read more about Java profilershere. I used theCodeGuru profilerfrom AWS in this case.See below an example of an application containing performance issues profiled with CodeGuru.The large frames indicate a large amount of processing time, and the blue color indicates namespaces recognized as your code. On top of that, sometimes you may have some recommendations based on a detected issue. However, don\u2019t expect it to always point you precisely to the issues in your code. Focus on the large frames and use them to detect parts of the code that normally should not consume so much processing time.In the example above, one of the main issues seems to be creating SQS clients in the Main class. After fixing it, come back and check what the profiling results look like after some period of time monitoring the new code.In our case, the profiler indicated a couple of problematic frames in different applications, which caused bottlenecks and, as a consequence, the 500 errors and long latency in the previous graphics.In general, this either indicates low-performant code (e.g., strong encryption algorithms executed repeatedly) or leaks in general (e.g., the creation of a new object mapper in each request). In our case, it pointed to some namespaces, and after analyzing them, I could find opportunities for caching expensive operations, for example.Thymeleaf CacheThis was a funny one. A cache is always supposed to speed up our code execution, as we don\u2019t need to obtain a resource for the source again, right? Right\u2026?Yes, if configured properly!Thymeleafserves frontend resources in this service, and it has cache enabled for static resources based on content. Something like the following properties:spring.resources.chain.enabled=true spring.resources.chain.strategy.content.enabled=true spring.resources.chain.strategy.content.paths=/**Enter fullscreen modeExit fullscreen modeHowever, there are two issues introduced with these three lines.1- Caching is enabled based on resource content. However, with each request, the content is read from the disk over and over again so its hash can be recalculated, as the result of the hash calculation for the cache itself is not cached. To solve this, don\u2019t forget to add the following property:spring.resources.chain.cache=trueEnter fullscreen modeExit fullscreen mode2- Unfortunately, the service is not using any base path for unifying the resolution of our static resources, so basically, Thymeleaf would try by default to load every link as a static resource from disk, even though they were just controller paths, for example. Keep in mind that disk operations are, in general, expensive.As I didn\u2019t want to introduce an incompatible change by moving all static resources to a new directory within the resources folder, as it would cause link changes, and I had very well-defined paths for the static resources, I could simply solve it withsetOptimizeLocations()from ResourceHandlerRegistration.Disabling Expensive Debug LogsAnother common mistake is to enable excessive logging, especially logs that print too much too often (e.g., often full stack trace logging). If you have high throughput on your systems, make sure to set up an appropriate log level and log only the necessary information. Review your logs frequently and evaluate if you want to be alerted to warnings and errors when you have your logs clean (e.g., no wrong log levels for debug/trace info).In this specific case, we had one log line logging a full stack trace for common scenarios. I disabled it as it was supposed to be enabled just for a short period of time for debugging purposes and disabled afterward but it was probably just forgotten.Auto Scaling TuningAuto-scaling settings are easy to get working, but it can be tricky to get them working optimally. The basic thing you can do is enable auto-scaling based on CPU and Memory metrics. However, knowing your services in terms of how many requests per second they are able to handle can help you scale horizontally before your services start to degrade performance.Check possible different metrics you may want to observe for scaling, set reasonable thresholds, fine-tunescale-in and scale-out cooldown periods, define minimum desired instances according to your expected load, and define a maximum number of instances to avoid unexpectedly high costs. Know your infrastructure and your service implementations inside out.Giving Another TryPerforming the same load test one more time yielded the following results in comparison with the initial results:Count: SumWe are now handling more than double the number of requests within the same 15 minutes of testing.Integration Latency: AverageThe average integration latency in the service, which had load applied, was reduced more than twice compared to before. Meanwhile, downstream services remained with almost constant latency during the tests compared to before, so no more domino effect was observed.5XXError: SumMore importantly, errors were gone. The remaining errors we see on the graphic on the right are unrelated to the load test, as we can see in the following report.Finally, we can see that auto-scaling changes helped us reduce the average response time to the normal state and keep it stable after about 10 minutes of the test.ConclusionAre we done? Of course not.These optimisations took me about 24 hours of work in total, but they should be performed regularly in multiple systems and different parts of them. However, when considering a large enterprise, such work can quickly become very expensive.Choosing a good balance between keeping it as it is and becoming obsessed with optimizing every millisecond is tricky, and you should keep in mind that such optimizations bring against opportunity costs.Do not forget that it\u2019s not only about tuning services to be performant under high load but also making sure your services can produce consistent and correct results under normal conditions as well (e.g., such issues as higher I/O dependencies can lead to \u201crandom\u201d unexpectedly longer response times if some jobs are being performed in the background on the operational system of your service instance, for example).Finally, I often see developers tend to use frameworks and infrastructure without knowing their internals, and this behavior introduces several issues without being noticed. Ensure you understand how your systems behave, what bottlenecks they create, what possible security issues could be exploited, and which settings are available to optimize them to your needs.I hope this article helps you set the mindset of caring about such aspects of your systems. Good luck!"}
{"title": "Bedrock Jumpstart Series: Foundational Models", "published_at": 1711214964, "tags": ["aws", "bedrock", "ai", "machinelearning"], "user": "Juan Taylor", "url": "https://dev.to/aws-builders/bedrock-jumpstart-series-bedrock-overview2-nb0", "details": "What is a Foundational Model?Foundational Models (FMs) form the backbone of Generative AI. They are large machine learning models trained on labeled and unlabeled data. Unlike traditional machine learning models usually trained on just labeled data. They have a less specific target than traditional models as well. There are two types of FMs: The gigantic models like GPTs that take a long time to build and millions of dollars or smaller models a developer can perhaps build by themselves from scratch on and use on SageMaker.However, the developer couldn\u2019t put the small FM they built on Bedrock because that\u2019s only for models especially chosen by Amazon. Bedrock is an abstraction above SageMaker specifically for Foundational Models but one\u2019s that are selectively chosen.Three types of Foundational ModelsBelow are the three types of FMs. Embeddings are the numerical translations of information so that FMs can process them. Developers can work with raw embeddings when doing specific things like similarity searches.Text to TextText to EmbeddingsMultimodal (text to another data modality)Foundational Models in BedrockBedrock has a number of popular FMs. Each model is put out by a company and may have different versions. Amazon Titan is Amazon\u2019s very own FM building on 25 years of dealing with AI in their own systems. These are the different models of Titan.Titan Multimodal EmbeddingsMultimodal search and recommandationsTitan Text EmbeddingsBasic semantic similarityTitan Text ExpressUsed for RAG and a host of textual capabilities.Titan Text Litecost-effective and very customizable, good for summarization and copywritingTitan Image GeneratorImage generation and editingTokens in FMsFoundational model tokens are the fundamental units of information the model uses for processing which depending on context can be a word, a subword or an entire sentence. Bedrock pricing is largely based on FM tokens so it\u2019s something to always keep in mind as you\u2019re working in Bedrock. There are two types of tokens:Input Tokens = Tokens in text you send to the model for processing.Output tokens = tokens the model generates in the response.Inference OptionsThere are two types of Inference options: On-demand and Provisioned throughput. As seen in many AWS services, the flexibility of these options is especially for pricing considerations.On-demandThis is usually for non-production workloads. For prototyping, POCs (Proof of Concepts) and small production workloads.Provisioned ThroughputThis is for production workloads. Stable throughput at a fixed monthly cost with higher throughput available.Customizing Foundational ModelsWhen working with FMs, you will want to customize them for your business goals. Here are four ways to customize your FMs beginning with the least maximized type of customization.Prompt Engineering (Customizes FM response.)RAG (Retrieval Augmented Generation):  (Customizes FM responses.)Fine-tuning (A form of full customization.)Train FM from Scratch (A form of full customization.)Code Example: Bedrock & LambdaFinally let\u2019s see Bedrock FMs in action in a simple code example using a prompt. Here are steps and code to quickly get Bedrock running on Lambda with a query to a foundational model.1. Activate chosen FMs in BedrockEach foundational model has to be activated before use.2. Create Lambda Layer for latest Boto3 SDK for BedrockLambda in your AWS account may not have the latest version of Boto3 SDK for python which enables Bedrock. If that\u2019s the case, you can add the latest version of Boto3 as a Lambda layer. Here is a way to do this. Thanks to Mike Chambers for his tutorials on this (see references).1: From the AWS Console, open Cloud Shell (It should be a button on the upper bar)2: Type in the following.mkdir ./bedrock-layer cd ./bedrock-layer/ mkdir ./python pip3 install -t ./python/ Boto3 zip -r bedrock-layer.zip . aws lambda publish-layer-version --layer-name bedrock-layer --zip-file fileb://bedrock-layer.zipEnter fullscreen modeExit fullscreen mode3: Go to \u2018Lambda > Layers\u2019 and click on the layer just created and copy the ARN shown.4: Click on \u2018Layers\u2019 in the \u2018Function Overview\u2019 diagram.5: Add a Layer to your function you will code Bedrock in by specifying the ARN.3. Get the Model IDFor your code to work on a specific FMs you\u2019ll need the Model ID which Amazon lists on this webpage:Amazon Bedrock Model IDs4. Get the Inference Parameter for your FMYou will need the correct set and format of the inference parameters of your chosen model. They may differ in different FMs. You can consult the documentation below or use the Playground in the console and click on the \u2018view API Request\u2019.Inference parameters for foundation models5. Code a Simple Bedrock Lamba FunctionThis is code for a simple Lambda function using Amazon\u2019s \u2018Titan Text G1 - Express\u2019 model. You can change the prompt to change the answer the model gives.import boto3 import json  bedrock_client = boto3.client(      service_name='bedrock-runtime',       region_name='us-east-1'     )  def lambda_handler(event, context):     # Input text for the chosen Bedrock model     prompt = \"What is the Capital of France?\"      body = json.dumps(        {           \"inputText\": \"what is the Capital of the Bahamas?\"        }      )      # Specify the model you want to use     model_name = \"amazon.titan-text-express-v1\"       # Invoke the Bedrock model     response = bedrock_client.invoke_model(         body=body,         accept='application/json',          contentType='application/json',         modelId=model_name          )      response_body = json.loads(response.get('body').read())     outputText = response_body.get('results')[0].get('outputText')      return {         'statusCode': 200,         'body': json.dumps(outputText)        }Enter fullscreen modeExit fullscreen modeFurther ReferencesMike Chambers tutorial on creating a Lambda layer to load the lastest Boto3 SDKServerless Generative AI: Amazon Bedrock Running in Lambdahttps://www.youtube.com/watch?v=7PK4zdUgAt0Mike Chambers Quick Tip: \u2018about a Lambda layer to load the lastest Boto3 SDK for Amazon Web Services (AWS)\u2019https://www.linkedin.com/posts/mikegchambers_serverless-python-activity-7154258975926964224-IL4G/Amazon Bedrock model IDshttps://docs.aws.amazon.com/bedrock/latest/userguide/model-ids.htmlInference parameters for foundation modelshttps://docs.aws.amazon.com/bedrock/latest/userguide/model-parameters.htmlAmazon Bedrock Pricinghttps://aws.amazon.com/bedrock/pricing/"}
{"title": "Bedrock Jumpstart Series: Bedrock Overview", "published_at": 1711214763, "tags": ["aws", "bedrock", "ai", "machinelearning"], "user": "Juan Taylor", "url": "https://dev.to/aws-builders/bedrock-jumpstart-series-bedrock-overview-1h0", "details": "## Bedrock SeriesThe Bedrock Jumpstart Series will go through the fundamentals of Bedrock for the AWS developer who already knows the fundamentals of AWS. Each article besides this one will come with a Python code example in which will be contained in an AWS Lambda function.Bedrock OverviewFoundational ModelsKnowledge Bases (RAG)Langchain and BedrockBedrock AgentsFine-tuning ModelsSo in this article we will go over the basics in a quick overview of Bedrock and then take a close look at Foundational Models. Then onto Knowledge bases and RAG which are innovations on top of generative AI and are seeing significant development in the world of AI. Then onto Langchain, a python-based framework that simplifies foundational model app development, especially LLMs (Large Language Models), and that can be used in conjunction with the Amazon Bedrock API. This will enable us to go onto building a Bedrock Agent in a better way. Agents will also likely have very significant development in AI. Lastly we will go into the mechanics of fine-tuning of models in Bedrock in which one deeply customizes the model for specific use cases.What is Generative AI?In 2017, the Transformer architecture revolutionized NLP (Natural Language Process) and led to a new subset of deep learning called Generative AI. Generative AI (GenAI) is being adopted quickly and many are heralding it as a disruptive force across industries. It is indeed a subset of deep learning which in turn is a subset of Machine Learning, which is the ability of computers to see and remember patterns in data by learning without explicit programming. So it\u2019s important to conceptualize a bit how it enriches traditional deep learning and machine learning.In GenAI the Transformer Architecture through its Attention Mechanism allows us to see patterns in data in a greater way than traditional deep learning by creating interconnections between discrete parts of the data through vectors of numbers. It's like instead of examining pairs of words it examines the relationship of each word to every other word. In that way, through vectors of numbers, it can in some way understand the semantics of the words through fine-grained comparisons along with a very large body of data. And it can do this dynamically, re-examining a set of new data to make even better predictions, generate more creative text outputs, or provide deeper insights than ever before.Generative AI is called Generative AI because in understanding so much of the semantics of language (or other data) through comparisons of vectors it's able to not just predict but to create, it facilitates not just simple predictions but complex predictions.What is Bedrock?Here is the Webster definition of \u2018Bedrock\u2019:\u201c:the solid rock underlying unconsolidated surface materials (such as soil)\u201dSimilar to the natural bedrock, AWS Bedrock provides a foundational layer in which surface materials can be integrated with. A natural bedrock provides stability but also variety  and in the same way AWS Bedrock provides the stability of FMs with not just one type but a variety of choices.Bedrock vs. SageMakerBedrock is an abstraction above SageMaker for foundational models. Those FMs don\u2019t need to deal with Generative AI but Bedrock is mostly targeted for FMs that cater to Generative AI.Bedrock Security & GovernanceIn Bedrock there is comprehensive monitoring and logging capabilities. CloudWatch can track usage metrics and build customized dashboards for enhanced visibility. CloudTrail to monitor API activity to help to identify and troubleshoot issues.Data is always encrypted at transit and at rest and you can use your own keys to encrypt. You can use PrivateLink to connect Bedrock to on-prem networks without wandering through the internet. Your content is never shared with Third-party providers and is never used to better the models. With its many AWS tools and configurations, AWS Bedrock allows you to maintain compliance with standards like GDPR compliance and HIPPA eligibility.Bedrock GuardrailsAs part of \u2018Responsible AI\u2019, AWS Bedrock implements Guardrails which allow certain restrictions that could be harmful to the user experience. These guardrails can be customized for user queries and responses and even integrated into agents. You can have fine-grained control having multiple guardrails for an application with each guardrail having unique control combinations. Most FMs already have built-in protections so these Bedrock Guardrails allow even finer safeguards over queries and responses.Bedrock Guardrails Abilitiesrestrict topicsfilter harmful content(Soon) Redact PII in inputs and responsesBoto3 Bedrock APIBoto3 is the AWS SDK for Python which is one implementation of the Amazon Bedrock API. With it, you can interact with different AWS services including Bedrock, at least in newer versions. And of course python is the most used programming language for ML/AI.There are two bedrock clients:Bedrock = creating and managing modelsBedrock-runtime = for inference requestsBoto3 Bedrock ModulesBelow are the Boto3 Bedrock Modules containing python classes. The modules \u2018Bedrock\u2019 and \u2018AgentsforBedrockRuntime\u2019 contain classes to deal with creating and managing models and agents respectively. The \u2018Runtime\u2019 modules contain classes for invoking and querying models and running the agents.BedrockAgentsforBedrockAgentsforBedrockRuntimeBedrockRuntimePlaygroundsAmazon Bedrock playgrounds provide a way to experiment with models before deciding to use them in your apps. There are three types of playgrounds:Chat ModelsText ModelsImage ModelsYou can enter prompts and adjust the inference parameters, and also view the API request in order to use the same correct set and formatting of inference parameters in your code.Further ReferencesAmazon Bedrock (Official)https://aws.amazon.com/bedrock/Getting started with Lambdahttps://docs.aws.amazon.com/lambda/latest/dg/getting-started.htmlBoto3 documentationhttps://boto3.amazonaws.com/v1/documentation/api/latest/index.html"}
{"title": "Monitoring and Troubleshooting on AWS: CloudWatch, X-Ray, and Beyond", "published_at": 1711211719, "tags": ["aws", "cloud", "devops", "monitoring"], "user": "Guille Ojeda", "url": "https://dev.to/aws-builders/monitoring-and-troubleshooting-on-aws-cloudwatch-x-ray-and-beyond-3oa3", "details": "As an AWS user, I'm sure you know that monitoring and troubleshooting are essential for keeping your applications running smoothly. After all, you can't fix what you can't see. But with the sheer number of services and tools available on AWS, it can be overwhelming to know where to start.That's where this article comes in. We'll dive into AWS monitoring and troubleshooting, with some key services like CloudWatch and X-Ray, along with other tools and best practices. By the end, you'll have a better understanding of how to effectively monitor and troubleshoot your AWS applications, so you can spend less time fighting fires and more time building cool stuff.Understanding AWS CloudWatchAt the heart of AWS monitoring is CloudWatch, a powerful service that collects monitoring and operational data in the form of logs, metrics, and events. Think of it as the central nervous system of your AWS environment, constantly keeping track of everything that's going on.CloudWatch MetricsOne of the core components of CloudWatch is metrics. CloudWatch Metrics are data points that represent the performance and health of your AWS resources over time. AWS services automatically send metrics to CloudWatch, and you can also publish your own custom metrics.For example, EC2 instances automatically send metrics like CPU utilization, network traffic, and disk I/O to CloudWatch. RDS databases send metrics like database connections, read/write latency, and free storage space. By monitoring these metrics, you can get a clear picture of how your resources are performing and identify potential issues before they impact your users.CloudWatch LogsAnother key feature of CloudWatch is logs. CloudWatch Logs allows you to collect, monitor, and store log files from various sources, including EC2 instances, Lambda functions, and on-premises servers. You can use CloudWatch Logs to troubleshoot issues, analyze application behavior, and gain insights into user activity.One of the most powerful features of CloudWatch Logs is the ability to filter and search log data. You can use simple text searches or complex query syntax to find specific log events, making it easy to identify errors, exceptions, or other issues. With CloudWatch Logs Insights, you can even perform real-time log analytics, allowing you to quickly investigate and resolve problems.CloudWatch AlarmsOf course, collecting metrics and logs is only half the battle. You also need a way to proactively detect and respond to issues. That's where CloudWatch Alarms come in.CloudWatch Alarms allow you to set thresholds for your metrics and receive notifications when those thresholds are breached. For example, you could create an alarm that triggers when the CPU utilization of an EC2 instance exceeds 80% for more than 5 minutes. When the alarm is triggered, you can have CloudWatch send an email, SMS message, or push notification to your team, or even perform automated actions like scaling up your instances or triggering a Lambda function.When setting up alarms, it's important to strike a balance between being proactive and being spammed with notifications. A good rule of thumb is to focus on metrics that directly impact the user experience or the stability of your application. You should also carefully consider the thresholds and time periods for your alarms to avoid false positives.CloudWatch DashboardsFinally, CloudWatch Dashboards provide a way to visualize your metrics and logs in a single, customizable view. Dashboards allow you to create graphs, tables, and other widgets based on your CloudWatch data, giving you a real-time overview of your application's health and performance.When creating dashboards, it's important to focus on the metrics and logs that are most relevant to your team and your users. You should also use clear and concise labels and annotations to help your team quickly understand the data being presented. And don't forget to share your dashboards with your team members, so everyone has access to the same information.Stop copying cloud solutions, startunderstandingthem. Join over 4000 devs, tech leads, and experts learning how to architect cloud solutions, not pass exams, with theSimple AWS newsletter.AWS X-Ray: Distributed Tracing for MicroservicesWhile CloudWatch is great for monitoring individual resources and services, it doesn't provide a complete picture of how requests flow through your application. That's where AWS X-Ray comes in.X-Ray is a distributed tracing service that allows you to track requests as they move through your application, helping you identify performance bottlenecks, errors, and other issues. X-Ray is especially useful for troubleshooting microservices architectures, where requests often span multiple services and resources.Instrumenting Applications for X-RayTo use X-Ray, you first need to instrument your application code to send tracing data to the X-Ray service. AWS provides X-Ray SDKs for popular programming languages like Java, Node.js, Python, and .NET, which make it easy to add tracing to your code.When instrumenting your code, it's important to follow best practices like using meaningful segment names, adding annotations and metadata to your traces, and handling errors gracefully. You should also be careful not to over-instrument your code, as this can add unnecessary overhead and complexity.Tracing Requests with X-RayOnce your application is instrumented, X-Ray will automatically capture and visualize traces as requests flow through your system. The X-Ray service map provides a high-level overview of your application architecture, showing how services and resources are connected and how requests are routed between them.By drilling down into individual traces, you can see detailed information about each segment of the request, including response times, errors, and other metadata. This makes it easy to identify performance bottlenecks, such as slow database queries or high network latency, and pinpoint the root cause of issues.X-Ray also integrates with other AWS services, allowing you to trace requests as they move between services like API Gateway, Lambda, and DynamoDB. This provides a complete end-to-end view of your application, making it easier to troubleshoot issues that span multiple services.Analyzing and Visualizing TracesThe X-Ray console provides a powerful interface for analyzing and visualizing your tracing data. You can use the console to view the service map, examine individual traces, and filter and group traces based on various attributes like response time, error rate, or user agent.One of the most useful features of the X-Ray console is the ability to create custom trace views and dashboards. This allows you to focus on the metrics and traces that are most important to your team, and share those views with other team members.You can also integrate X-Ray with CloudWatch, allowing you to create alarms based on X-Ray metrics and visualize X-Ray data alongside other CloudWatch metrics. This provides a more comprehensive view of your application's health and performance, making it easier to identify and resolve issues.Monitoring Serverless Applications on AWSServerless architectures, such as those based on AWS Lambda and Step Functions, present unique challenges when it comes to monitoring and troubleshooting. Because serverless functions are ephemeral and can scale rapidly, traditional monitoring approaches may not be effective.Monitoring AWS Lambda with CloudWatchOne of the key tools for monitoring AWS Lambda is CloudWatch Logs. By default, Lambda sends log output to CloudWatch Logs, allowing you to view and search log data in real-time. You can use CloudWatch Logs to troubleshoot issues, analyze function behavior, and gain insights into performance and usage patterns.In addition to logs, Lambda also sends metrics to CloudWatch, including invocations, duration, errors, and throttles. By monitoring these metrics, you can identify performance issues, detect anomalies, and set up alarms to proactively notify you of problems.When monitoring Lambda functions, it's important to correlate logs and metrics to get a complete picture of function behavior. For example, if you notice a spike in function duration, you can use CloudWatch Logs to investigate the root cause, such as a slow database query or a network issue.Monitoring AWS Step Functions with X-RayFor more complex serverless workflows, such as those based on AWS Step Functions, X-Ray can be a powerful tool for monitoring and troubleshooting. By enabling X-Ray tracing for your Step Functions, you can visualize the execution flow of your state machines, identify performance bottlenecks, and pinpoint the root cause of errors.X-Ray integrates seamlessly with Step Functions, automatically capturing traces as executions move through the state machine. You can use the X-Ray console to view the service map, examine individual executions, and filter and group traces based on various attributes.One of the most useful features of X-Ray for Step Functions is the ability to correlate traces across Lambda functions and other AWS services. This allows you to see how data flows through your application, identify performance issues, and troubleshoot errors that span multiple services.Other AWS Monitoring and Troubleshooting ToolsWhile CloudWatch and X-Ray are the core tools for monitoring and troubleshooting on AWS, there are many other services and features that can help you keep your applications running smoothly. Here are a few worth mentioning:Amazon EventBridgeEventBridge is a serverless event bus that makes it easy to build event-driven architectures on AWS. With EventBridge, you can monitor events from a wide range of sources, including AWS services, SaaS applications, and custom applications, and trigger automated actions based on those events.For example, you could use EventBridge to monitor EC2 instance state changes, capture S3 bucket events, or detect changes to your AWS resources using CloudTrail. You can then use EventBridge rules to trigger Lambda functions, send SNS notifications, or perform other actions in response to those events.AWS ConfigAWS Config is a service that helps you assess, audit, and evaluate the configurations of your AWS resources. With Config, you can continuously monitor and record resource configurations, and receive notifications when those configurations change.Config is particularly useful for troubleshooting issues related to resource misconfigurations or compliance violations. For example, you could use Config to detect when an S3 bucket is made publicly accessible, or when an EC2 instance is launched without the required security group.VPC Flow LogsVPC Flow Logs is a feature that allows you to capture information about the IP traffic going to and from your VPC. With Flow Logs, you can monitor network traffic at the interface or subnet level, and gain insights into traffic patterns, security issues, and performance bottlenecks.Flow Logs can be particularly useful for troubleshooting connectivity issues, detecting unusual traffic patterns, and investigating security incidents. You can use tools like Amazon Athena or Amazon CloudWatch Logs Insights to analyze Flow Log data and identify issues.Best Practices for Monitoring and Troubleshooting on AWSEffective monitoring and troubleshooting on AWS requires more than just the right tools and services. It also requires a well-defined strategy, clear objectives, and a commitment to continuous improvement. Here are some best practices to keep in mind:Establish clear monitoring and troubleshooting objectives. What are the key metrics and logs that matter most to your application and your users? What are your target response times and error rates? By setting clear objectives upfront, you can focus your monitoring and troubleshooting efforts where they'll have the biggest impact.Create a comprehensive monitoring strategy. Your monitoring strategy should cover all aspects of your application, from infrastructure and application metrics to logs and traces. It should also define clear roles and responsibilities for your team, as well as processes for incident response and escalation.Implement proactive and reactive troubleshooting processes. Proactive troubleshooting involves using monitoring data to identify and resolve issues before they impact users. Reactive troubleshooting involves quickly identifying and resolving issues when they do occur. Both approaches are essential for maintaining a reliable and performant application.Leverage automation and Infrastructure as Code. Automation and Infrastructure as Code (IaC) can help you ensure consistency and reliability across your monitoring and troubleshooting processes. By defining your monitoring configuration as code, you can version control your settings, test changes before applying them, and quickly roll back if needed.Continuously optimize your approach. Monitoring and troubleshooting is an ongoing process, not a one-time setup. As your application evolves and your usage patterns change, you'll need to continuously optimize your monitoring and troubleshooting approach to ensure it remains effective. This may involve adding new metrics and logs, adjusting alarm thresholds, or refining your troubleshooting processes.ConclusionMonitoring and troubleshooting are essential skills for any AWS user, whether you're running a simple web application or a complex microservices architecture. By using tools like CloudWatch and X-Ray, plus other AWS services and best practices, you can gain deep visibility into your application's behavior and quickly resolve issues when they occur.But effective monitoring and troubleshooting is about more than just tools and technology. It's also about having a clear strategy, well-defined processes, and a culture of continuous improvement. By setting clear objectives, implementing proactive and reactive troubleshooting approaches, and continuously optimizing your monitoring and troubleshooting practices, you can build more reliable, performant, and resilient applications on AWS.So don't wait until something breaks to start thinking about monitoring and troubleshooting. Start implementing these best practices today, and you'll be well on your way to building better applications on AWS.Stop copying cloud solutions, startunderstandingthem. Join over 4000 devs, tech leads, and experts learning how to architect cloud solutions, not pass exams, with theSimple AWS newsletter.Realscenarios and solutionsThewhybehind the solutionsBest practicesto improve themSubscribe for freeIf you'd like to know more about me, you can find meon LinkedInor atwww.guilleojeda.com"}
{"title": "An Allow List Lambda Function in Rust is 1 Guaranteed Way to Improve CORS", "published_at": 1711206701, "tags": ["aws", "serverless", "rust", "api"], "user": "Benjamen Pyle", "url": "https://dev.to/aws-builders/an-allow-list-lambda-function-in-rust-is-1-guaranteed-way-to-improve-cors-3oe2", "details": "Some time ago I wrote an article aboutCross-Origin Resource Sharingwith API Gateway that talks about custom allow lists.  I wanted to revisit that implementation not because the code doesn't work, but because I wanted to see what it would look like in Rust.  Remember, I believe that more developers would be choosing Rust with Serverless if more content and examples existed.  Let's dive into building a Lambda Function in Rust for CORS.ArchitectureWorking with CORS is something that many developers sort of take for granted. I mean, Cross-Origin Resource Sharing isn\u2019t something you need to pay attention to, is it? The fact is though, it\u2019s something you use every day while using the internet or in the applications you are building, but it\u2019s usually already in place when working on a project. However, let\u2019s pretend it isn\u2019t.In my career, I\u2019ve seen so many instances where developers respond to an OPTIONS request with Access-Control-Allow-Origin: *. This article\u2019s purpose isn\u2019t to explain when and why you should or shouldn\u2019t do that. But there are times when you are building an API with authorization that you will have to make provisions for the Authorization header. Two of the ways to do that are by using Access-Control-Allow-Headers or Access-Control-Allow-Credentials. And when using allow credentials, you lose the ability to return * to the allow origin header.What this means for us as developers is that we need to return the matching origin for the supplied request.Giant caution here: Do not reflect the incoming origin to simply bypass a check of allowed origins. You will be sharing with bad actors that you have a flaw in your implementation and will give them a reason to take advantage of this.But what we can do is use API Gateway to trigger a Lambda function in Rust that responds to our CORS request and verifies the origin is in an allow list.The Lambda Function in RustLet's just right into our Lambda Function in Rust CORS implementation.  For a quick aside, the sample repository at the bottom of the article has CDK code in TypeScript so you can deploy this to your AWS account and get going.Main FnAll Lambda Functions in Rust have amainfunction entry point.  It's the first function that is called and helps initialize defaults or items that'll be used throughout the lifecycle of the request.The key thing to note is that I'm requiring a variable calledALLOWED_ORIGINSwhich is a comma-separated list of acceptable domains and allowed by this CORS function.  Imagine though that you have a larger list of allowed domains?  This could be pivoted to a DynamoDB table or perhaps even a SET in aMomentocache.#[tokio::main]asyncfnmain()->Result<(),Error>{tracing_subscriber::fmt().with_max_level(tracing::Level::INFO).with_target(false).json().init();letorigins=env::var(\"ALLOWED_ORIGINS\").expect(\"ALLOWED_ORIGINS must be set\");letallowed_origins=&origins;run(service_fn(move|payload:Request|asyncmove{function_handler(allowed_origins,payload).await})).await}Enter fullscreen modeExit fullscreen modeHandler FnMost API Gateway OPTIONS request implementations I've seen are MOCK requests that return a stack response.  Custom domain checks might be complicated or slow and developers sometimes might not feel the overhead is worth the check.  This is where implementing this Lambda Function in Rust makes so much sense.  I'vewritten about this topicquite a bit, Rust's performance with Lambda is blazing fast.  So using Rust in this space would be a great first Lambda Function in Rust I'd look to deploy if you are just starting out with Rust.  It's a great use case.The handler takes a pointer to the allow list string and the incoming request that will have a header HeaderMap.  I then pair it with aget_originfunction that checks the allow list for the value in the Origin header.fnget_origin(headers:&HeaderMap,allowed_origins:&str)->Option<String>{returnmatchheaders.get(\"origin\"){Some(origin)=>{lets=allowed_origins.split(',');foroins{ifo==origin{returnSome(o.to_string());}}None}None=>{None}};}asyncfnfunction_handler(allowed_origins:&str,event:Request,)->Result<implIntoResponse,Error>{matchget_origin(event.headers(),allowed_origins){Some(origin)=>{letresponse=Response::builder().status(StatusCode::OK).header(\"Access-Control-Allow-Origin\",origin).header(\"Access-Control-Allow-Headers\",\"Content-Type\").header(\"Access-Control-Allow-Methods\",\"GET, PUT, DELETE, POST, OPTIONS, PATCH\").body(\"\".to_string()).map_err(Box::new)?;Ok(response)}None=>{letresponse=Response::builder().status(StatusCode::BAD_REQUEST).body(\"\".to_string()).map_err(Box::new)?;Ok(response)}}}Enter fullscreen modeExit fullscreen modeNotice that I'm making use of Rust's Option enumeration and the match construct so that I can validate that I've received an allowed value from the origin header. In the case of matching, I can return anything that I want in the response headers.  These values are 100% up to your use case.In the scenario where I'm not finding a match to the allow list, I just return a 400 BAD_REQUEST.That's all there is to it.  A non-mirrored allow list performed by a Lambda Function in Rust can then be connected to an API Gateway.API GatewayAt this point, I can deploy the infrastructure up to AWS which will create my API Gateway, and Lambda Function in Rust and connect the two. Connected, the API Gateway OPTIONS endpoint will look like this:)Quick Note on PerformanceAgain, I'm back to performance because it is such a compelling argument for Rust.  Quickly though, it's not the only argument that I've said numerous times, but any chance I can get to demonstrate this substainability aspect of the language, I'm going to.I have written before about theLambda Power Tuningproject.  You need to be using this if you are deploying Lambda Functions in production. For this article, I ran this Lambda Function in Rust through the tooling and the output is below..  The Power Tuning Tool takes a payload a configured list of memory options that it runs against your Lambda Function.   The graph then shows the memory size, duration, and the cost associated with the execution.  What I like about this tool is that I don't have to guess the size of my Lambda Function.  He helps me make that optimal choice.But back to the Lambda Function in Rust for this CORS allow list.  At 128MB of memory, the average execution is < 1 ms.  Nothing I can say about that, so I'll just leave you with it and let it sync in.Wrapping UpCORS can be hard or it can be ignored and then the problems that come from not doing it right can be extremely hard. However, dealing with multiple domain origins doesn't have to be difficult. By using a Lambda Function in Rust to build a CORS allow list, you can add a layer of security while also not sacrificing performance.  To tie it all back together, this is a 100% serverless solution that could be mixed in with an existing serverless or serverful API.  Start small and innovate.  Serverless doesn't have to be the strategy, but it 100% should be a part of your strategy.For reference,here is the GitHub repositorythat you can clone, adjust, and deploy!Thanks so much for reading and happy building!"}
{"title": "DocumentDB Vacuum Locks", "published_at": 1711196741, "tags": ["documentdb", "mvcc", "aws", "database"], "user": "Felipe Malaquias", "url": "https://dev.to/aws-builders/documentdb-vacuum-locks-57fn", "details": "Beware possible locks on large updates/deletionsHistorically, traditional databases dealt with writes with pessimistic locks on records during writes to avoid inconsistency, which had the obvious drawback of being unable to handle concurrency properly, as transactions could fail.This is solved byMVCC(Multiversion Concurrency Control), by creating a new version of a record on every update, circumventing the need to lock records, and allowing concurrency (seethisvideo from Cameron McKenzie for a nice and simple illustrated explanation).However, to clean up the old versions, a vacuum process must run in the background, which may cause locks in your complete collection, bottlenecks, and possibly unexpected downtimes in your application.There is no permanent fix for this at the moment, but if you need to perform such updates, you may contact support and ask them to disable the process that reclaims unused storage space. This will not negatively impact your workload, and space reclaimed by the garbage collector will continue to be recycled. However, the size of your collections will never decrease, even if a significant amount of data has been deleted.The good newsAWS is currently working on a fix for it, which may be available at any time, so you should keep an eye on theDocumentDB release notes.This is how we experienced itOn a lovely Tuesday morning, we reset one of our Kafka topics (~71 GB) to re-consume all our data for a particular domain to aggregate it with new fields in our database. All messages were successfully consumed and written in the primary DB instance in a few minutes as expected:What we did not expect, though, are those waves of latency increase in our workload hours after the records were consumed and initially without much of a pattern until approx. 5 pm:Those were all caused by locks on a particular collection in the read replicas as shown by the pink bars in theDocument performance insightsmetrics below:As you see, the locks were gone after around 11 pm, matching the end of the DocumentDB freeable memory metrics changes below:Reaching out to AWS support, they investigated the issue. They confirmed it was caused by the process of reclaiming unused space during the garbage collection on the vacuum process. This process must be synchronized between the writer and readers because the readers might still have in-flight transactions that can see the deleted data. In some rare circumstances and the presence of a large amount of reclaimable data, this synchronization can adversely affect the workload on the replicas.ConclusionBe aware of the MVCC strategy and check how it may affect your database (not only DocumentDB) in case of large updates as described above, and probably most importantly, always test it in staging first ;)Also, be aware of the known issue with the DocumentDB vacuum process and watch therelease notes."}
{"title": "Using Custom Authorization - Request based for AWS Lambda", "published_at": 1711183484, "tags": ["awslambda", "apisecurity", "customauthorization", "aws"], "user": "\ud83c\udd77\ud83c\udd70\ud83c\udd81\ud83c\udd73\ud83c\udd78\ud83c\udd7a \ud83c\udd79\ud83c\udd7e\ud83c\udd82\ud83c\udd77\ud83c\udd78", "url": "https://dev.to/aws-builders/using-custom-authorization-request-based-for-aws-lambda-2d04", "details": "Many a time we face a challenge for our Lambda function endpoints where a customer autorization is required.Below is a post which shows how to implement a Request based Custom Authorization for AWS Lambda.ALambda authorizer, formerly known as custom authorizer, controls API access using a Lambda function. It's beneficial for custom authorization with bearer token authentication like OAuth or SAML OR request parameter based authentications. It processes the caller's identity and returns an IAM policy. There are two types:TOKEN authorizerfor bearer tokens andREQUEST authorizerfor parameters. WebSocket APIs only support REQUEST authorizers. You can use Lambda functions across AWS accounts for authorizers.In this part, I will show you how to create a request based token.It will cover 3 high level stepsCreate a sample Lambda REST APICreate a trigger on API GatewayCreate the customer Authorizer and attach it to API and test it.*Step 1 *- Go to AWS Console, select Lambda and click on 'Create Function'Step 2- Author from scratch and API, Give it any name and create a new role for Lambda executionStep 3- Customize the output message with your favourite Hello world message. My sample as belowexport const handler = async (event) => {   // TODO implement   const response = {     statusCode: 200,     body: JSON.stringify('You have called My Rest API!'),   };   return response; };Enter fullscreen modeExit fullscreen modeStep 4- Once the Lambda is created we will create an API Gateway trigger which will basically call this Lambda on the endpoint invocationa. Click on add triggerb. Select the source as API Gatewayc. Fill in the required Trigger parameters to expose it as a REST API and click on Create. Note the security parameter is kept asOPEN. This is specifically to test our scenariosStep 5- On the API, click on Configuration tab and click on Triggers, Note the API Endpoint and click on it to open it.Step 6- You will be able to view the result from the endpoint.Step 7- Now we will create the authorizer function, Go back to Lambda function, and click on 'Create Function'. Fill in the details as belowStep 8- Now under the code section, fill in the below code.This is a sample code from AWS blogs. Importantly look for the Header, QueryParam and StageVariable validation conditions.This will be the one which will be request-based authorizer.// A simple request-based authorizer example to demonstrate how to use request      // parameters to allow or deny a request. In this example, a request is       // authorized if the client-supplied headerauth1 header, QueryString1     // query parameter, and stage variable of StageVar1 all match     // specified values of 'headerValue1', 'queryValue1', and 'stageValue1',     // respectively.  export const handler = function(event, context, callback) {     console.log('Received event:', JSON.stringify(event, null, 2));      // Retrieve request parameters from the Lambda function input:     var headers = event.headers;     var queryStringParameters = event.queryStringParameters;     var pathParameters = event.pathParameters;     var stageVariables = event.stageVariables;      // Parse the input for the parameter values     var tmp = event.methodArn.split(':');     var apiGatewayArnTmp = tmp[5].split('/');     var awsAccountId = tmp[4];     var region = tmp[3];     var restApiId = apiGatewayArnTmp[0];     var stage = apiGatewayArnTmp[1];     var method = apiGatewayArnTmp[2];     var resource = '/'; // root resource     if (apiGatewayArnTmp[3]) {         resource += apiGatewayArnTmp[3];     }      // Perform authorization to return the Allow policy for correct parameters and      // the 'Unauthorized' error, otherwise.     var authResponse = {};     var condition = {};     condition.IpAddress = {};      if (headers.headerauth1 === \"H1\"         || queryStringParameters.QueryString1 === \"Q1\"         || stageVariables.StageVar1 === \"S1\") {         callback(null, generateAllow('me', event.methodArn));     }  else {         callback(\"Unauthorized\");     } }  // Help function to generate an IAM policy var generatePolicy = function(principalId, effect, resource) {     // Required output:     var authResponse = {};     authResponse.principalId = principalId;     if (effect && resource) {         var policyDocument = {};         policyDocument.Version = '2012-10-17'; // default version         policyDocument.Statement = [];         var statementOne = {};         statementOne.Action = 'execute-api:Invoke'; // default action         statementOne.Effect = effect;         statementOne.Resource = resource;         policyDocument.Statement[0] = statementOne;         authResponse.policyDocument = policyDocument;     }     // Optional output with custom properties of the String, Number or Boolean type.     authResponse.context = {         \"stringKey\": \"stringval\",         \"numberKey\": 123,         \"booleanKey\": true     };     return authResponse; }  var generateAllow = function(principalId, resource) {     return generatePolicy(principalId, 'Allow', resource); }  var generateDeny = function(principalId, resource) {     return generatePolicy(principalId, 'Deny', resource); }Enter fullscreen modeExit fullscreen modeStep 9- Add the code and now click 'Deploy'Step 10- Now, go toAPI Gatewayand click on the API Name. NOTE:- you need to go to API Gateway not AWS Lambda.Step 11- Now go toAuthorizers _and click on _Create authorizerStep 12- Next up, we need to create the autorizer with the right Identify source type and the key values.One which we are doing here are Header, Query String and StageVariable. There are few others also based on which you can perform the authorizationStep 13- Now go back to the authorizer that is created and click on its detailsStep 14- Next screen will give you the option to test the authorization,a. if you give any values that are acceptable then it will return a 200b. if you give any values that are not acceptable then it will return a 401So in this way you can achieve custom authorization for your code very quickly.Hope this was useful to you. And in the next blog I will show creation of a token based authorizer."}
{"title": "AWS Amplify : Website Migration", "published_at": 1711166809, "tags": ["awsamplify", "webdev", "aws"], "user": "\ud83c\udd77\ud83c\udd70\ud83c\udd81\ud83c\udd73\ud83c\udd78\ud83c\udd7a \ud83c\udd79\ud83c\udd7e\ud83c\udd82\ud83c\udd77\ud83c\udd78", "url": "https://dev.to/aws-builders/aws-amplify-website-migration-4jc6", "details": "I have a custom web app which would help me track buses around my area and to also give me a clear view on where they current GPS location, letting me leave out at a precise time.This Website was earlier hosted on Heroku and was working fine. However, recently Heroku has taken away the free tier and I was exploring alternate web hosting options for my site.I gave a go with AWS Amplify web hosting and I am very impressed with the ease of getting it done. Took me less than 5 mins to complete the whole process.I will document here the full journey on AWS Amplify Web hosting.Firstly,what is AWS Amplify?AWS Amplify is a set of products and tools that enable mobile and front-end web developers to build and deploy secure, scalable full-stack applications, powered by AWS.AWS Amplify provides two componentsAmplify StudioBuild an app backend with auth, data, and storage, and create custom UI components. Then integrate them in your app with just a few steps.Amplify HostingConnect your Git repository to continuously deploy your frontend and backend. Host it on a globally available CDN.Pre-requisites:For this you will need a repo with a bitbucket URL (and optionally may be already deployed earlier or at least locally tested).Steps:Step 1- Open the AWS Amplify homepageeg:-https://ap-southeast-console.aws.amazon.com/amplify/home?region=ap-southeast-2#/This will open the Amplify Homepage.Step 2- Scroll down to Amplify Hosting and click 'Get Started'. This will start the webhosting process.Step 3-  Choose an existing repo. In my case the repo was hosted on bitbucket. In my case, its currently in GitHub (planning to migrate it to AWS CodeCommit).This step will initiate the steps required for linking the repo.Step 4- Authorize this access request by AWS Amplify on Github bya. signing in with the GitHub credentials,b. entering the 2FAc. authorizing the accessThis step will build a connection with your dev workStep 5- Next provide the permissions to access all repos or selective repos.Click 'Install & authorize'Step 6- Now under recently updated repos, select the repo & branch that you want to get published.a. select the repob. select the branchStep 7- Next select the build setting. Here you can define the app name and additionally you can configure the advanced settings if required. I didnt had that much need so went with the default one.Step 8- Now, review all the settings and click 'Save and Deploy'Step 9- Monitor the deployment for Provision - Build and then Deploy.That's it.You will get the deployment URL and once the deploy step is successful you will be able to access it.It was really easy and straightforward to migrate my website on AWS Amplify.Hope you had a good read and can try this out now."}
{"title": "Terraform Import & Demo", "published_at": 1711151949, "tags": ["terraform", "import", "iac"], "user": "Srinivasulu Paranduru", "url": "https://dev.to/aws-builders/terraform-import-304l", "details": "Use the import block to import existing infrastructure resources into Terraform, bringing them under Terraform's management.Once imported, Terraform tracks the resource in your state file. You can then manage the imported resource like any other, updated /destroy it as part your requirement.Terraform older version < 1.5 : It will import state file information but the configuration has to be written.Example1:provider \"aws\" {   region     = \"eu-west-1\" }  resource \"aws_instance\" \"srinivm\" {  ami           = \"unknown\"  instance_type = \"unknown\" }Enter fullscreen modeExit fullscreen modeRun the terraform commands :terraform initEnter fullscreen modeExit fullscreen modeTerraform import :Think of it as if the cloud resource (EC2 instance) and its corresponding configuration were available in our files. All that\u2019s left to do is to map the two into our state file. We do that by running the import command as follows.terraform import aws_instance.srinivm <Instance ID>Enter fullscreen modeExit fullscreen modeSuccessfull import looks likeaws_instance.srinivm: Importing from ID \"i-0ad9e2e3d0f1a35a9\"... aws_instance.srinivm: Import prepared!   Prepared aws_instance for import aws_instance.srinivm: Refreshing state... [id=i-0ad9e2e3d0f1a35a9]  Import successful!  The resources that were imported are shown above. These resources are now in your Terraform state and will henceforth be managed by Terraform.Enter fullscreen modeExit fullscreen modeWhile we do not expect to make backwards-incompatible changes to syntax, the-generate-config-outflag andhow Terraform processes imports during the plan stageand generates configuration may change in future releases.Example 2:Step1: Create AWS Security group through AWS Console with one ingress ruleStep2: Make sure you are having latest terraform version downloadedthen run the commandterraform versionversion should be greater than 1.5Step3: create a file import.tf then copy the below mentioned codeprovider \"aws\" {   region     = \"eu-west-1\" }   import {   to = aws_security_group.mysg   id = \"sg-0edc915507cfcd57a\" #this is the security group which i have created from step 1 }Enter fullscreen modeExit fullscreen modeStep4 : Run the terraform commandterraform initterraform plan -generate-config-out sg.tfsg.tf configuration will be created after the above command ran with the below message\u2502 Warning: Config generation is experimental\u2502\u2502 Generating configuration during import is currently experimental, and the generated configuration format may\u2502 change in future versions.\u2575Step 5: sg.tf created with the below mentioned code# __generated__ by Terraform # Please review these resources and move them into your main configuration files.  # __generated__ by Terraform from \"sg-0edc915507cfcd57a\" resource \"aws_security_group\" \"mysg\" {   description = \"Allow SSH and RDP\"   egress = [{     cidr_blocks      = [\"0.0.0.0/0\"]     description      = \"\"     from_port        = 0     ipv6_cidr_blocks = []     prefix_list_ids  = []     protocol         = \"-1\"     security_groups  = []     self             = false     to_port          = 0   }]   ingress = [{     cidr_blocks      = [\"0.0.0.0/0\"]     description      = \"\"     from_port        = -1     ipv6_cidr_blocks = []     prefix_list_ids  = []     protocol         = \"icmp\"     security_groups  = []     self             = false     to_port          = -1     }]   name                   = \"sgej\"   name_prefix            = null   revoke_rules_on_delete = null   vpc_id = \"vpc-***\" }Enter fullscreen modeExit fullscreen modeStep6 : Run the below command for terraform apply> terraform apply --auto-approveBelow response is generated :aws_security_group.mysg: Importing... [id=sg-0edc915507cfcd57a]aws_security_group.mysg: Import complete [id=sg-0edc915507cfcd57a]Apply complete! Resources: 1 imported, 0 added, 0 changed, 0 destroyed.References:https://developer.hashicorp.com/terraform/language/importConclusion : Discussed about terraform import and created aws security group manually through aws console and imported the configuration using terraform import command.\ud83d\udcac If you enjoyed reading this blog post and found it informative, please take a moment to share your thoughts by leaving a review and liking it \ud83d\ude00 and follow me indev.to,linkedinandbuy me a coffee"}
{"title": "AWS GuardDuty for ATP (Advanced Threat Detection)", "published_at": 1711127527, "tags": ["threatdetection", "guardduty", "awssecurity", "cybersecurity"], "user": "Isaac Oppong-Amoah", "url": "https://dev.to/aws-builders/aws-guardduty-for-atp-advanced-threat-detection-24lg", "details": "In today's cybersecurity landscape, proactive threat detection is paramount. AWS GuardDuty, a managed threat detection service, offers an effective solution for identifying and prioritizing potential security threats in your AWS environment. Let's explore how to leverage GuardDuty with practical examples and CloudFormation code.1. Enable GuardDuty:Start by enabling GuardDuty in your AWS account. You can do this through the AWS Management Console or by using CloudFormation. Here's a CloudFormation snippet to enable GuardDuty:Resources:MyGuardDutyDetector:Type:AWS::GuardDuty::DetectorProperties:{}Enter fullscreen modeExit fullscreen mode2. Configure GuardDuty:Customize GuardDuty settings to suit your security requirements. This includes specifying which AWS regions to monitor, setting up threat intelligence feeds, and defining alert thresholds.Resources:MyGuardDutySettings:Type:AWS::GuardDuty::DetectorProperties:FindingPublishingFrequency:FIFTEEN_MINUTESEnableThreatIntelSets:true...Enter fullscreen modeExit fullscreen mode3. Analyze Findings:GuardDuty continuously analyzes logs from various AWS data sources, such as CloudTrail, VPC Flow Logs, and DNS logs. It then generates findings based on identified threats, anomalies, or suspicious activities.Resources:MyGuardDutyCloudTrail:Type:AWS::GuardDuty::FilterProperties:DetectorId:!RefMyGuardDutyDetectorAction:ARCHIVEFindingCriteria:Criterion:-Field:typeEq:UnauthorizedAccess:EC2/MaliciousIPCaller.CustomEnter fullscreen modeExit fullscreen mode4. Respond to Threats:Once GuardDuty identifies a potential threat, it generates findings that you can investigate further. You can integrate GuardDuty with AWS Lambda to automate response actions, such as isolating compromised instances or updating security group rules.Resources:MyGuardDutyLambdaFunction:Type:AWS::Lambda::FunctionProperties:...MyGuardDutyInvocator:Type:AWS::Lambda::PermissionProperties:...Enter fullscreen modeExit fullscreen mode5. Monitor and Fine-Tune:Regularly review GuardDuty findings and adjust settings as needed. Monitor GuardDuty metrics and alarms in Amazon CloudWatch to ensure effective threat detection and response.6. Benefits:By incorporating AWS GuardDuty into your AWS security strategy, you can strengthen your defenses against evolving cyber threats. With its comprehensive threat detection capabilities and seamless integration with AWS services, GuardDuty empowers you to safeguard your cloud infrastructure with confidence."}
{"title": "EDA for AWS operations", "published_at": 1711120532, "tags": ["ansible", "eventdriven", "aws", "redhat"], "user": "Jakub Wo\u0142ynko", "url": "https://dev.to/aws-builders/eda-for-aws-operations-2fpe", "details": "WelcomeGreat that you're here. Today, I will write a bit about Event-Driven Ansible and its possibilities for improving the operational efficiency of our AWS environment. But wait, what does it mean by Event-Driven Ansible? My favorite sentence from the documentation is \"EDA provides a way of codifying operational logic,\" so in short words, we can code logic for responding to different events. For example, we can trigger playbook execution with Lambdas make our self-service process a bit more ops-oriented, or introduce ChatOps.Before we beginNative AWS services?Seems that could be a question here. Why not use System Manager, as it is a dedicated service for operations? Or OpsWorks? Or even Lambdas. Let\u2019s break it down now.System ManagerAs AWS has many complementary solutions with quite similar functionality, we will start with the most well-known, and easy to use. System Manager is agent agent-based solution for managing a whole fleet of instances, no matter whether cloud or on-prem, if an agent is installed System Manager can be used. Additionally, with the usage ofAutomation, we can use more than 370 pre-defined tasks, written with PowerShell, Bash, or Python, and that is great. If we wanted to schedule servers update, we can do it, and execute this operation on the whole fleet at the same time(which is not the best idea), or on part based on tags and groups.So what\u2019s the challenge? System Manager isn\u2019t declarative, If we would like to add a custom ssm-document for creating a new folder, we need to write for example:---schemaVersion:'2.2'description:\"StateManagermkdirexample\"parameters:{}mainSteps:-action:aws:runShellScriptname:make /opt/testinputs:runCommand:-sudo mkdir /opt/test && chmod -R 0440 && chown admin. /opt/testEnter fullscreen modeExit fullscreen modeIt\u2019s fine, but it's like taking a step back. For service restart, it\u2019s a great solution, but refreshing our system or using complex functions becomes even more complex.OpsWorksThe solution was designed to serve managed Chef/Puppet to customers, unfortunately, all of them will reach End of Life withe the end of May 2024. During the time of writing this article (1-half of March), you can read about it on the public service page.OpsWorks.So as a summary, nice solution unfortunately based on Chef/Puppet, not a SaltStack, also the idea of stacks could be a blocker for a multi-cloud environment.LambdasLambdas and the whole serverless ecosystem were called a \u201cnew big thing\u201d, a lot of people advocate for using function as much as possible and everywhere. After a while, we realized, that Lambdas-based architecture is very hard to keep clean and simple, also can easily make our bill much higher. That is why we started using them when it makes sense. For example image processing, data processing, etc, but most importantly, functions can act as a glue, for everything, connecting services that aren\u2019t natively integrated, executing tasks, or evenstopping our instance, when some criteria weren\u2019t met. The thing is that it will be quite hard and not very time-efficient to use Lambdas for service restart tasks. The main reason is that functions operate on different logic layers, let\u2019s say hypervisor level. However, server-level operation could be a challenge.Initial infrastructureFirst, we need to set up our base infrastructure and install the needed packages.Let\u2019s assume, that our infrastructure will be very simple. We will use just for simplification three small VMs, all of which will be placed in a private subnet, one will be acting as a bastion host, for communication with our ansible-runner, just to keep the whole setup a bit more \u201creal world\u201d and secure.As it\u2019s quite a straightforward task I will use AWS CDK for that, please take a look at the user-data part, which is a place where we\u2019re installing EDA components:userData.addCommands(`dnf --assumeyes install git maven gcc java-17-openjdk python3-pip python3-devel`,`echo 'export JAVA_HOME=/usr/lib/jvm/jre-17-openjdk' >> /etc/profile`,`pip3 install wheel ansible ansible-rulebook ansible-runner`,`echo 'export PATH=$PATH:/usr/local/bin' >> /etc/profile`,`mkdir /opt/ansible`,`git clone https://github.com/3sky/eda-example-ansible /opt/ansible/eda-example-ansible`,`cd /opt/ansible/eda-example-ansible && /usr/local/bin/ansible-galaxy collection install -r collections/requirements.yml -p collections`,`chown -R ec2-user:ec2-user /opt/ansible/*`,);Enter fullscreen modeExit fullscreen modeWhen the stack is ready we can connect with a regular.ssh/configfile.Host jump   PreferredAuthentications publickeyIdentitiesOnly=yesIdentityFile ~/.ssh/id_ed25519_eda   User ec2-user# this one could change# as its public IPHostname 3.126.251.7  Host runner   PreferredAuthentications publickeyIdentitiesOnly=yesProxyJump jump   IdentityFile ~/.ssh/id_ed25519_eda   User ec2-user   Hostname 10.192.0.99  Host compute   PreferredAuthentications publickeyIdentitiesOnly=yesProxyJump jump   IdentityFile ~/.ssh/id_ed25519_eda   User ec2-user   Hostname 10.192.0.86Enter fullscreen modeExit fullscreen modeWith that being done, we can focus on popular use cases.EDA configurationAs today\u2019s article is an EDA introduction, we will focus on basic http listener and basic service configuration, next article will be focused on more complex event sources likeMSKor watchdogs.Server configurationFirst let\u2019s use a basic server setup, based on documentation:----name:Listen for events on a webhookhosts:all## Define our source for eventssources:# use Python range function-ansible.eda.range:limit:5# use webhook with path /endpoint-ansible.eda.webhook:host:0.0.0.0port:5000# use kafka-ansible.eda.kafka:topic:edahost:localhostport:9092group_id:testingEnter fullscreen modeExit fullscreen modeSeems rather straight, right? First, we\u2019re using the keywordsources, then with typical Ansible syntax, we\u2019re providing the FQDN of the module. Also as you can see there are multiple event sources, all of which are describedhere. In the case of introduction, I will be using web-hook endpoint.Rules configurationNow let\u2019s define our first rules:----name:Listen for events on a webhookhosts:allsources:# use webhook with path /endpoint-ansible.eda.webhook:host:0.0.0.0port:5000# define the conditions we are looking forrules:-name:Rule1condition:event.payload.message == \"Hello\"# define the action after meeting the conditionaction:run_playbook:name:what.yml# As it's a list of dicts, we can add another rule-name:Rule2condition:event.payload.message == \"Ahoj\"action:run_playbook:name:co.ymlEnter fullscreen modeExit fullscreen modeExecution phaseGreat! Let\u2019s assume, that both of ourrun_playbooklooks very similar, and are simple:----hosts:localhostconnection:localtasks:-debug:msg:\"World\"Enter fullscreen modeExit fullscreen modeNow, we\u2019re good to go, and execute ouransible-rulebookcommand:ansible-rulebook--rulebookeda-example.yml-iinventory.yml--verboseEnter fullscreen modeExit fullscreen modeThen let\u2019s open another terminal window(or login intobastionhost) and execute the testing http request:curl-H'Content-Type: application/json'-d\"{\\\"message\\\":\\\"Test\\\"}\"10.192.0.92:5000/endpointEnter fullscreen modeExit fullscreen modeInside the log we can see:2024-03-20 20:42:26,859 - aiohttp.access - INFO - 10.192.0.92 [20/Mar/2024:20:42:26 +0000] \"POST /endpoint HTTP/1.1\" 200 158 \"-\" \"curl/7.76.1\"Enter fullscreen modeExit fullscreen modeThen, let\u2019s try met out conditional, by changing our message to \u201cHello\u201d, and next to \u201cAhoj\u201d.2024-03-20 20:44:39,199 - aiohttp.access - INFO - 10.192.0.92[20/Mar/2024:20:44:39 +0000]\"POST /endpoint HTTP/1.1\"200 158\"-\"\"curl/7.76.1\"PLAY[localhost]***************************************************************TASK[Gathering Facts]*********************************************************ok:[localhost]  TASK[debug]*******************************************************************ok:[localhost]=>{\"msg\":\"World\"}PLAY RECAP*********************************************************************localhost                  :ok=2changed=0unreachable=0failed=0skipped=0rescued=0ignored=0 2024-03-20 20:44:41,923 - ansible_rulebook.action.runner - INFO - Ansible runner Queue task cancelled 2024-03-20 20:44:41,924 - ansible_rulebook.action.run_playbook - INFO - Ansible runner rc: 0, status: successful 2024-03-20 20:46:14,240 - aiohttp.access - INFO - 10.192.0.92[20/Mar/2024:20:46:14 +0000]\"POST /endpoint HTTP/1.1\"200 158\"-\"\"curl/7.76.1\"PLAY[localhost]***************************************************************TASK[Gathering Facts]*********************************************************ok:[localhost]  TASK[debug]*******************************************************************ok:[localhost]=>{\"msg\":\"Svet\"}PLAY RECAP*********************************************************************localhost                  :ok=2changed=0unreachable=0failed=0skipped=0rescued=0ignored=0 2024-03-20 20:46:16,620 - ansible_rulebook.action.runner - INFO - Ansible runner Queue task cancelled 2024-03-20 20:46:16,621 - ansible_rulebook.action.run_playbook - INFO - Ansible runner rc: 0, status: successfulEnter fullscreen modeExit fullscreen modeNow we have a working example, of working EDA. Let\u2019s focus on something more useful.Example use casesAs you probably notice already it\u2019s more like REST API to Ansible, and you\u2019re right. It could do that as well, as web-hook is an event as well. So let\u2019s focus on basic administration tasks.----name:Listen for events on a webhookhosts:allsources:-ansible.eda.webhook:host:0.0.0.0port:5000-name:Restart Servercondition:event.payload.task == \"restart\"action:run_playbook:name:reboot.yml-name:Reload Nginxcondition:event.payload.task == \"reload-nginx\"action:run_playbook:name:reload-nginx.ymlEnter fullscreen modeExit fullscreen modeWith the following rulebook, we\u2019re able to restart the server if needed or reload nginx with the usage of REST API. That can be easily connected with ITSM systems like Jira or ServiceNow. However please note that there is no filter or tags implemented - based ondocumentation, which means, we need a dedicated playbook for a particular group of hosts, or issues.SummaryEDA seems to be a more and more popular topic among many organizations, which is why in my opinion is a good time to take a look at it. To make it even simpler to start with I prepared a whole ready-to-use project that contains:playground based on AWSall the playbooks and rules included in this articlestep-by-step README fileThat is why please use this article just like an introduction, the whole magic should take place on your AWS account. Please play with rules and use-cases, the thing about possibilities which this pattern can deliver in your organization, or which aspect can improve. If you have any questions or need help don't hesitate and contact me.Link to GitHub repository"}
{"title": "\ud83d\ude80\"Streamlining Infrastructure Deployment: \ud83d\udcbb Seamless CloudFormation Stack Deployment with Git Sync\ud83d\udd04\"", "published_at": 1711092959, "tags": ["aws", "cloudformation", "git", "infrastructureascode"], "user": "Sarvar Nadaf", "url": "https://dev.to/aws-builders/streamlining-infrastructure-deployment-seamless-cloudformation-stack-deployment-with-git-sync-2hg5", "details": "Hello There!!!Called Sarvar, I am an Enterprise Architect, Currently working at Deloitte. With years of experience working on cutting-edge technologies, I have honed my expertise in Cloud Operations (Azure and AWS), Data Operations, Data Analytics, and DevOps. Throughout my career, I\u2019ve worked with clients from all around the world, delivering excellent results, and going above and beyond expectations. I am passionate about learning the latest and treading technologies.Folks, today we are looking into,\u00a0Git sync is an exciting new feature from CloudFormation that enables customers to easily connect Git, a popular version control system, with their cloud resource management. We are going to see the implementation of a real-time connection with an actual example. We'll make the changes that are needed, establish a few AWS resources, and investigate our options for leveraging CloudFormation to enable Git Sync in the future.So let's begin......!!!What is Amazon CloudFormation Git Sync?The Amazon CloudFormation Git Sync is a solution that integrates the popular version control system Git with AWS CloudFormation. Thanks to this integration, developers may store CloudFormation templates directly in a Git repository, where they can define and configure cloud resources. Git Sync makes sure that cloud infrastructure is always up to date with the most recent code changes by automatically updating the related CloudFormation stack with each commit to the repository. This feature streamlines processes, permits automatic updates, and lets you provide dynamic values using a YAML deployment file, making cloud resource management easier.The ability for Git sync to automatically update is one of its main advantages. Once set up, AWS will make sure that your cloud stack always reflects the most recent modifications by automatically syncing the deployment file and CloudFormation template with each Git commit. This increases development speed and creates an environment that is more responsive and flexible. Moreover, testing CloudFormation modifications prior to deploying them to production is made possible by this functionality. Teams can guarantee the dependability and durability of their cloud settings by setting up various stacks to sync from Git branches, such as a production branch for live deployment and a staging branch for testing.Let's See How We Can Leverage the New Feature of Amazon CloudFormation:We will view the entire tutorial in a step-by-step manner. Before we begin this tutorial, there are a few items that are necessary.Prerequisites:Need GitHub account.Need CloudFormation templates. (You will get this in my GitHub URL)IAM Role and PolicyLinkGitHub Repository URL -LinkStep 1: Create IAM Policy and RoleHere we are just creating a IAM policy & Role required for this demo. If you want detailed information please follow link in above prerequisites section.Use below JSON to create a Policy required for this demo.{     \"Version\": \"2012-10-17\",     \"Statement\": [         {             \"Sid\": \"SyncToCloudFormation\",             \"Effect\": \"Allow\",             \"Action\": [                 \"cloudformation:CreateChangeSet\",                 \"cloudformation:DeleteChangeSet\",                 \"cloudformation:DescribeChangeSet\",                 \"cloudformation:DescribeStackEvents\",                 \"cloudformation:DescribeStacks\",                 \"cloudformation:ExecuteChangeSet\",                 \"cloudformation:GetTemplate\",                 \"cloudformation:ListChangeSets\",                 \"cloudformation:ListStacks\",                 \"cloudformation:ValidateTemplate\"             ],             \"Resource\": \"*\"         },         {             \"Sid\": \"PolicyForManagedRules\",             \"Effect\": \"Allow\",             \"Action\": [                 \"events:PutRule\",                 \"events:PutTargets\"             ],             \"Resource\": \"*\",             \"Condition\": {                 \"StringEquals\": {                 \"events:ManagedBy\": [\"cloudformation.sync.codeconnections.amazonaws.com\"]                 }             }         },         {             \"Sid\": \"PolicyForDescribingRule\",             \"Effect\": \"Allow\",             \"Action\": \"events:DescribeRule\",             \"Resource\": \"*\"         }     ] }Enter fullscreen modeExit fullscreen modeCreate IAM Role with Trust Policy.{   \"Version\": \"2012-10-17\",   \"Statement\": [     {       \"Sid\": \"CfnGitSyncTrustPolicy\",       \"Effect\": \"Allow\",       \"Principal\": {         \"Service\": \"cloudformation.sync.codeconnections.amazonaws.com\"       },       \"Action\": \"sts:AssumeRole\"     }   ] }Enter fullscreen modeExit fullscreen modeCreate Role with this Highlighted Option and Attached above created policy to it.Step 2: Setup a Connection with GitHub Account.There are two methods we can use to create this connection: using the\u00a0Developer\u00a0tool service which know as CodePipeline Service\u00a0or the\u00a0CloudFormation\u00a0console. Since you will ultimately be using the developer tool service only let me demonstrate how to establish a connection using the development tool.Login to AWS ConsoleSearch Any of this Service - CodePipeline, CodeBuild, CodeCommit, CodeDeploy or CodeArtifacts.You will see the console of Developers Tool.Now click on Settings which is located at the left sidebar.Here you can see the connections. Here we are established the new GitHub connection.Create New connection by click on Create Connections.Once you select GitHub as your provider just click on connect GitHub and it will reroute you to authorization page and it will look like below. You should pre login to you GitHub account.Once you click on Authorize option it will direct you to GitHub Connection Setting were you need to click on install new app option. At last just click on Connect.If Everything goes well you will able to see the connection string like below and you are done with Connection.Step 3: Let's Perform GitSync feature in CloudFormationI want to share an important note with you before we get started. Here, a VPC will be created, and an EC2 instance with the auto scaling option is being created inside of that VPC. in order for us to scale up and down the EC2 instances. The deployment file is the second crucial point. A deployment file contains a configuration file needed to manage a CloudFormation stack. These files are usually in YAML or JSON format. It has values and options that specify how the resources in the stack should be set up. This file allows you to customize the stack's parameters and tags and provides the path to the CloudFormation template. Automatic updates are performed to the corresponding CloudFormation stack when modifications are made to the deployment file and committed to the repository.GitHub Repo Files:Here we are having only 2 files one is deployment file which we will use in this demo to run our cloudformation template and second one is cloudformation template as shown below.Deployment File Format:This file will use to deploy the Cloudformation Stack. I Highly recommend you to go through my GitHub repo to see the deployment file and actual cloudformation template.template-file-path: ./my-stack-template.yaml  parameters:      image: public.ecr.aws/lts/nginx:latest     task_size: size     max_capacity: 11     port: 80     env: prod tags:     cost-center: '123456'     org: 'AWS'Enter fullscreen modeExit fullscreen modeLet's Create CloudFormation Stack: Login to the AWS Console and Go to Cloudformation Service and Click on Create New Stack you will able to the interface as below and just go ahead and select GitSync option has highlighted below.Give Suitable name to Stack and Select Link a Git Repository.Once you select the Link a Git Repository Select Git Repository Provider and Connection which we have created at very 1st stage.Once you select the Connection then it will able to fetch all the repositories and you can choose the repo, branches and deployment file. Lastly select the role we have prepared for this demo in above section.If all of your selections are correct, let's proceed to the next phase, where you will choose the role that will assist Cloudformation in creating the necessary resources. This role is different from the one we have previously created, hence it is a straightforward role with full resource creation access. Rest keep everything default hit click on create option and here you go....As you can see below the stack has been created successfully and you can able to the progress here. GitSync is Enable and Git Repository Sync is Successful. Only provision will take sometime because its actual stage were all the deployment will happens.Finally all the Resource provision has been completed successfully. Now you can validate the resources.Navigate to VPC Service and click on VPC and you will able to see the new VPC Namely gitsync-cft-vpc as shown below.Now Navigate to EC2 Service and See you have one EC2 Instance running nemaly gitsync-cft-auto-scaling-group as shown below.Step 4: Let's Perform Some Chaos Testing to See How Well the Cloudformation GitSync Feature Works.Here, we modify the Cloudformation template, and we'll see how Gitsync detects and applies the modifications to our Cloudformation stack in terms of resource creation automatically. As you can see, we set the autoscaling group for our EC2 instance to 1, as you can see below. We are now changing it to 2, and we'll see how the modifications take effect. For your reference, kindly refer to the images below.The cloudformation will automatically detect the changes as you make them and commit them to Git as you can see below, we have two EC2 instances ready. This feature's attractiveness lies in it.Step 5: CleanUpAll you have to do is delete the stack that you prepared for this demo\u00a0the stack will be automatically removed all resource we have created using cloudformation, and there won't be additional costs.Conclusion: With the release of Amazon CloudFormation Git Sync, version control for cloud resources has advanced significantly as Git's powerful features are smoothly integrated with AWS CloudFormation. By enabling developers to instantly synchronize CloudFormation templates with their Git repositories, this ground-breaking tool streamlines the development process and promotes a more effective workflow. Git Sync shortens development timelines and promotes a more responsive and agile development environment by enabling automatic updates with every Git contribution,  guaranteeing that cloud stacks are always up to date.\u2014 \u2014 \u2014 \u2014 \u2014 \u2014 \u2014 \u2014Here is the End!Thank you for taking the time to read my article. I hope you found this article informative and helpful. As I continue to explore the latest developments in technology, I look forward to sharing my insights with you. Stay tuned for more articles like this one that break down complex concepts and make them easier to understand.Remember, learning is a lifelong journey, and it\u2019s important to keep up with the latest trends and developments to stay ahead of the curve. Thank you again for reading, and I hope to see you in the next article!Happy Learning!"}
{"title": "Brief Notes on AWS CodeDeploy", "published_at": 1711047844, "tags": ["aws", "devops", "cicd"], "user": "Amit Kayal", "url": "https://dev.to/aws-builders/brief-notes-on-aws-codedeploy-2731", "details": "Service that automates code deployments to any instance, including Amazon EC2 instances and instances running on-premises.Supported Platforms/Deployment Types:EC2/On-Premises: In-Place or Blue/Green DeploymentsDescribes instances of physical servers that can be Amazon EC2 cloud instances, on-premises servers, or both. Applications created using the EC2/On-Premises compute platform can be composed of executable files, configuration files, images, and more. o   -   - Deployments that use the EC2/On-Premises compute platform manage the way in which traffic is directed to instances by using an in-place or blue/green deployment type.AWS Lambda: Canary, Linear, All-At-Once DeploymentsApplications created using the AWS Lambda compute platform can manage the way in which traffic is directed to the updated Lambda function versions during a deployment by choosing a canary, linear, or all-at-once configuration.Amazon ECS: Blue/Green DeploymentUsed to deploy an Amazon ECS containerized application as a task set.CodeDeploy performs a blue/green deployment by installing an updated version of the containerized application as a new replacement task set. CodeDeploy reroutes production traffic from the original application, or task set, to the replacement task set. The original task set is terminated after a successful deployment.Deployment approach for EC2Deploys a revision to a set of instances.Deploys a new revision that consists of an application and AppSpec file. The AppSpec specifies how to deploy the application to the instances in a deployment group.Deployment approach for LambdaDeploys a new version of a serverless Lambda function on a high-availability compute infrastructure.Shifts production traffic from one version of a Lambda function to a new version of the same function. The AppSpec file specifies which Lambda function version to deploy.Deployment approach for ECSDeploys an updated version of an Amazon ECS containerized application as a new, replacement task set. CodeDeploy reroutes production traffic from the task set with the original version to the new replacement task set with the updated version. When the deployment completes, the original task set is terminated.App Spec FileThe application specification file (AppSpec file) is a YAML-formatted or JSON-formatted file used by CodeDeploy to manage a deployment. Note: the name of the AppSpec file for an EC2/On-Premises deployment must be appspec.yml. The name of the AppSpec file for an Amazon ECS or AWS Lambda deployment must be appspec.yml.For ECSThe container and port in replacement task set where your Application Load Balancer or Network Load Balancer reroutes traffic during a deployment. This is specified with the LoadBalancerInfo instruction in the AppSpec file.Amazon ECS task definition file. This is specified with its ARN in the TaskDefinition instruction in the AppSpec file.For LambdaLambda function version to deploy.Lambda functions to use as validation tests.For EC2Which lifecycle event hooks to run in response to deployment lifecycle events."}
{"title": "Data Streaming Hands-On: High-scale data streaming with AWS Kinesis Data Streams", "published_at": 1711047520, "tags": ["aws", "data", "kinesis", "lambda"], "user": "Anita Andonoska", "url": "https://dev.to/aws-builders/data-streaming-hands-on-high-scale-data-streaming-with-aws-kinesis-data-streams-326p", "details": "In my previous post, we saw how to create a data streaming app on AWS, with Kinesis Data Streams and Lambda. Here we\u2019ll see high-scale data streaming in action by exploring Kinesis Data Streams on-demand scaling.Kinesis Data Streams ScalingA shard serves as a base throughput unit of Kinesis Data Streams. A shard supports 1 MB/s and 1,000 records/s for writes and 2 MB/s for reads. By default, new data streams created with the on-demand capacity mode have 4 MB/s of write and 8 MB/s of read throughput. That means we have 4 shards, each one allowing 1MB/second and 1000 records per second for writes, and 2MB/seconds for reads.By default, On-Demand capacity mode can automatically scale up to 200MB/s of write and 400MB/s of read throughput. That is the default limit per account but you can request a limit increase through AWS Support to enable your On-Demand stream to scale up to 1 GB/s write and 2 GB/s read throughput.According to the information provided in thisAWS blog, scaling in on-demand capacity mode occurs at the individual shard level. When the average utilization of an ingest shard reaches 50% (0.5 MB/s or 500 records/s) within a minute, the shard is split into two. If random values are used as partition keys, all stream shards will have event traffic distribution, and scaling will occur simultaneously across all shards. Depending on the number of shards being scaled, it can take up to 15 minutes to split the shards.As the load increases, Kinesis Data Streams adjusts the number of shards in the stream by monitoring the ingest throughput at the shard level. Nevertheless, under certain circumstances, producers might encounter \u201cWriteThroughputExceeded\u201d and \u201cRate Exceeded\u201d errors, even when operating in on-demand capacity mode. According toAWS documentation, you may encounter \"ProvisionedThroughputExceeded\" exceptions if your traffic increases by more than double the previous peak within a 15-minute window. Further information on various scaling scenarios can be foundhere.Load Generator appI aimed to replicate the scenario of multiple producers sending data to the stream, and to achieve this, I developed a data streaming application for load generation. This application is built with Lambda functions serving as producers, utilizing Kinesis Data Streams with on-demand capacity, and Lambda functions as consumers. For a deeper understanding of the setup, you can refer to the specifics outlined in my earlierpublication.To generate the load, an additional Lambda function called the Producer Orchestrator has been introduced. This Orchestrator function is configured to accept three parameters: the duration of the load in milliseconds, the number of producers, and the number of records each producer will push to the Data Streams service. Depending on the specified number of producers, multiple instances of Lambda producers will be created. To ensure  the data to be evenly distributed across the shards, a unique partition key is assigned to each record.The code for the app can be foundhere.TestingAs previously mentioned, scaling occurs when the average ingest shard utilization reaches 50% (equivalent to 0.5 MB/s or 500 records/s) within a minute. To initiate the testing, I began with a small load to align with the initial capacity of the stream, set at 4MB/s of write throughput. Initially, I started with 4 producers, each generating 1K records and then reached 80 producers with 6K records each. As the load increased, the number of shards scaled rapidly, reaching 12, 28, 60, 96, and ultimately 100 shards. This scaling was triggered by a brief surge in load lasting only a minute or two. My objective was to observe the scaling process, hence the load increase was not gradual, resulting in some records being throttled.SummaryThe testing conducted on scaling scenarios in Kinesis Data Streams revealed that scaling occurs swiftly when the average ingest shard utilization reaches the predefined threshold. The stream's capacity quickly adapted, with the number of shards increasing significantly in response to increased load. Even a brief (around 1 minute) surge in load triggered scaling actions, demonstrating the auto scaling capability of Kinesis Data Streams."}
{"title": "DevSecOps with AWS- IaC at scale - Building your own platform - Part 1", "published_at": 1711035460, "tags": ["devops", "devsecops", "iac", "aws"], "user": "Alejandro Velez", "url": "https://dev.to/aws-builders/devsecops-with-aws-iac-at-scale-building-your-own-platform-part-1-1j87", "details": "level 300Today, the things change rapidly, the landscape and ecosystem are more complex, according to the infrastructure maturity path theapplication centricapproach is closer and concepts as Infrastructure from code, composition as Code emerging and show a next generation for cloud automation. Tools based on generative IA, code companions, and the construction of custom platform based on custom\"code warehouse\"are the normal in many organizations, create the self-service capabilities and construct platform engineering teams is a trend for the next mounts and years. Wherever, the energy should be concentrated in supply the business requirements through the developer\u2019s enablement securely, without lose control, flexibility, and scalability.For other hand, many workloads could be deployed in a sequential way due to the hierarchy resources, typically first each layer of the infrastructure is deployed and after the application, however themodern cloud automation the application and infrastructure are interwovenand serverless deployments, event driven architectures, data analytics architectures, and others translate de responsibility to the builder and convert the infrastructure resources provisioning in the less relevant part.The following image depicts this situation and a high level model guide where the integration of tools as Amazon Codewhisperer, Amazon Q improve and enablement the practice adoption and productivity.According tothe Well architected framework- Operational Excellent Pillarandthe previous blogthe operational approach in this series consists inSeparated Application Engineering and Operations (AEO) and Infrastructure Engineering and Operations (IEO) with centralized governance and an internal service provider consulting partner.In consonance with the first part, it\u2019s time to create the pipeline based on the artifacts and guidelines supplied by the Cloud Operations and Platform Enablement Team (COPE).There are many approaches for building a custom platform usingopensource toolsor commercial services likeAWS CodeCatalyst, an integrated DevOps service for software development teams adopting continuous integration and deployment practices into their software development process. On the other hand, you can build a complete suite for custom deployments based on services such asCodeBuild, CodeCommit, Codepipeline, CodeDeploy, Service Catalog, AWS Proton, Cloud9 and other tools and services. For this post you will provide a DevSecOps as a Service to the builders\u2019 team based on these tools, to wit, the hard way \ud83d\udc7d.Solution OverviewRequirementscdk >= 2.133.0AWS CLI >= 2.7.0Python >= 3.10.4AWS ServicesAWS Cloud Development Kit (CDK): is an open-source software development framework to define your cloud application resources using familiar programming languages.AWS Identity and Access Management (IAM): Securely manage identities and access to AWS services and resources.AWS IAM Identity Center (Successor to AWS Single Sign-On): helps you securely create or connect your workforce identities and manage their access centrally across AWS accounts and applications.AWS CodeBuild: fully managed continuous integration service that compiles source code, runs tests, and produces software packages that are ready to deploy.AWS CodePipeline: fully managed continuous delivery service that helps you automate your release pipelines for fast and reliable application and infrastructure updates.AWS Key Management Service (AWS KMS): lets you create, manage, and control cryptographic keys across your applications and more than 100 AWS services.AWS CloudFormation: Speed up cloud provisioning with infrastructure as code.AWS Resource Access Manager: Simply and securely share your AWS resources across multiple accounts.Here the summary of the main steps.First, create a custom CodeBuild Image for CodePipeline projects and publish into ECR in a shared account. It contains the predefine versions, tools, and requirements for your deployments according to the use case: IaC or any other Application.Second, create a SaaS model to deploy your standard pipelines in other DevSecOps accounts. Using CDK Pipelines for this approach. In the future post you can enable the self-service capabilities for now just focusing on the platform\u2019s bases.Here you could be thinking,why should you use CDK to define a terraform pipeline or another kind of pipeline in AWS?Because in this model you can take advantage of scalability, control and delivery model like internal SaaS department, this approach is util because the scenario is a devsecops in a product team with many microservices and builders\u2019 team. In contrast with other approach the pipeline here is define as a product, not just like a central library definition in yaml, o code library.Third, test your pipeline in a sandbox account with an infrastructure pattern or common use case in your organization.Fourth, define the base for storage tfstate, single or many buckets per project, consider the number of microservices or infrastructure components and layers, mono repo, or multiples repos, perhaps, stacks or micro stacks for manage the infrastructure resources.Fifth, Certified and approve your changes for deploy pipelines into production accounts.Sixth, deploy a production workload with many stages and environments.Seventh, the product pipeline runs, and the services are deployed in the specified environment.Now, hands on with the first step, create a custom CodeBuild image for IaC deployments.Based on public repository forCodebuild Image,  the image base will be the Ubuntu standard 7.0.Here the Dockerfile is modified removing runtimes unnecessary and installing the custom tools for your deployments. For example, installing terraform specific version, terragrunt, tfswitch,  terramate and other tools and package....  #************************** Terraform *************************************ARGTERRAFORM_VERSION=1.7.3RUNset-ex\\&&curl-Ohttps://releases.hashicorp.com/terraform/${TERRAFORM_VERSION}/terraform_${TERRAFORM_VERSION}_linux_amd64.zip&&unzip terraform_${TERRAFORM_VERSION}_linux_amd64.zip-d/usr/local/bin/RUNset-ex\\&&mkdir-p$HOME/.terraform.d/plugin-cache&&echo'plugin_cache_dir   = \"$HOME/.terraform.d/plugin-cache\"'>~/.terraformrc#************************* Terragrunt *************************************ARGTERRAGRUNT_VERSION=0.55.1RUNset-ex\\&&wget https://github.com/gruntwork-io/terragrunt/releases/download/v${TERRAGRUNT_VERSION}/terragrunt_linux_amd64-q\\&&mvterragrunt_linux_amd64 /usr/local/bin/terragrunt\\&&chmod+x /usr/local/bin/terragrunt#*********************** Terramate ****************************************ARGTERRAMATE_VERSION=0.4.5RUNset-ex\\&&wget https://github.com/mineiros-io/terramate/releases/download/v${TERRAMATE_VERSION}/terramate_${TERRAMATE_VERSION}_linux_x86_64.tar.gz\\&&tar-xzfterramate_${TERRAMATE_VERSION}_linux_x86_64.tar.gz\\&&mvterramate /usr/local/bin/terramate\\&&chmod+x /usr/local/bin/terramate#*********************** tfsec ********************************************ARGTFSEC_VERSION=1.28.5RUNset-ex\\&&wget https://github.com/aquasecurity/tfsec/releases/download/v${TFSEC_VERSION}/tfsec-linux-amd64\\&&mvtfsec-linux-amd64 /usr/local/bin/tfsec\\&&chmod+x /usr/local/bin/tfsec\\&&terragrunt--version#**********************Terraform docs ************************************ARGTERRRAFORM_DOCS_VERSION=0.17.0RUNset-ex\\&&curl-sSLo./terraform-docs.tar.gz https://terraform-docs.io/dl/v${TERRRAFORM_DOCS_VERSION}/terraform-docs-v${TERRRAFORM_DOCS_VERSION}-$(uname)-amd64.tar.gz\\&&tar-xzfterraform-docs.tar.gz\\&&chmod+x terraform-docs\\&&mvterraform-docs /usr/local/bin/terraform-docs#********************* ShellCheck *****************************************ARGSHELLCHECK_VERSION=\"stable\"RUNset-ex\\&&wget-qO-\"https://github.com/koalaman/shellcheck/releases/download/${SHELLCHECK_VERSION?}/shellcheck-${SHELLCHECK_VERSION?}.linux.x86_64.tar.xz\"|tar-xJv\\&&cp\"shellcheck-${SHELLCHECK_VERSION}/shellcheck\"/usr/bin/\\&&shellcheck--version...Enter fullscreen modeExit fullscreen mode\u2139\ufe0f For this case the image could be robust and with biggest size in comparison to the application image due the specific use case.\u2139\ufe0f Also can take long time for building.Finally, according to theprevious postsyou can use the same approach to deploy in multienvironment as is depicted in the following image.Also, it shows a cross account pipeline using AWS CodePipeline, AWS CodeCommit, AWS Codebuild and AWS CloudFormation. This pipeline is created with CDK Pipelines.1- The changes are detected and activate the pipeline. For this demo the branch main is the default branch.2-The CDK project is synthesized if is aligned with AWS Security Best practices.3-The pipeline run self-update action.4-The unit test runs, and its report is published in codebuild reports group.5-The SAST runs, and its report is published in codebuild reports group.6-The Cloudformation stack is prepared for shared environment.7-The Cloudformation stack is deployed for shared environment after success result from Cloudformation hooks, this create ECR repository and put the custom resources policies for different scenarios (Codebuild projects, organization units and individual accounts).8-To move to other step a manual approval step is added, the notification is sent to slack channel.9-The image is created and publish to ECR Repository and shared according to parameters in environment_options.yaml.A shared parameter store publishes the stable version of the base image for other accounts or organizational units. This invitation could be accepted with AWS CLI o with another IaC stack in destination account.Get the code here - \u2b07\ufe0f\u2b07\ufe0fvelez94/cdkv2_devsecops_iac_codebuild_imageCodebuild image for devsecops IaC deployments using tools like terraform, terragrunt, terramate and moreTable of ContentsCodeBuild DevSecOps Container Image for IaC DeploymentsAWS ServicesReference Architecture DiagramCode Diagram structureHow ToModify project optionsIntegrate with slackTo configure a Slack clientTo configure a Slack channelDeploy manual projectCreate commit changes and pushUseful commandsSome utils commandsCodeBuild DevSecOps Container Image for IaC DeploymentsAWS ServicesAWS Cloud Development Kit (CDK): is an open-source software development framework to define your cloud application resources using familiar programming languages.AWS Identity and Access Management (IAM): Securely manage identities and access to AWS services and resources.AWS IAM Identity Center (Successor to AWS Single Sign-On): helps you securely create or connect your workforce identities and manage their access centrally across AWS accounts and applications.AWS CodeBuild: fully managed continuous integration service that compiles source code, runs tests, and produces software packages that are ready to deploy.AWS\u2026View on GitHubThanks for reading and sharing! \ud83d\ude03"}
{"title": "Take IT Shipping with Serverless Technology", "published_at": 1711024559, "tags": ["aws", "serverless", "dynamodb", "lambda"], "user": "Mohamed Radwan", "url": "https://dev.to/aws-builders/take-it-shipping-with-serverless-technology-ehi", "details": "In this article, I'll take you through the journey of creating a mobile application utilizing serverless architecture.The app idea is a platform that facilitates and supports peer-to-peer shipping services, connecting requesters and travelers.You candownloadthe app on Android.The App Architecture:Frontend Framework (Flutter):Flutter serves as the frontend framework for developing the mobile application.Flutter allows for cross-platform development, enabling the app to run on both iOS and Android devices from a single codebase.Backend Services (Lambda):The backend service is built using Lambda, a serverless computing platform.Lambda functions are deployed in response to events triggered by user actions, ensuring scalability and cost-efficiency.\"The Take IT\" app utilizes Python Flask within the Lambda functionsAuthentication (Cognito):Cognito, a serverless authentication service, manages the sign-up/sign-in process in the Take IT app. Additionally, it seamlessly integrates with identity providers such as Google or Facebook. Authentication is typically managed using JWT (JSON Web Tokens) for secure and efficient user authentication.API Gateway:Integrating AWS API Gateway with Cognito offers enhanced security and scalability for the Take IT app.API Gateway ensures only authenticated users can access backend services.Database (DynamoDB):DynamoDB serves as the serverless database utilized to store all data in the Take IT app. It is leveraged to manage user preferences, trip details, contacts, as well as the status of sent and received requests (including accepted, pending, and declined requests).Adding a TripThe function looks like this:def add_trip(item):     return query_table.put_item(Item=item)Enter fullscreen modeExit fullscreen modeBelow is an example of how a trip is saved into DynamoDB in JSON format:{  \"username\": \"4fb1dce7-6a50-41a8-8d7d\",  \"created\": \"2024-10-04-19-34-18-736061\",  \"acceptfrom\": \"2024-10-05\",  \"acceptto\": \"2024-10-30\",  \"allowed\": {   \"Clothes\": {    \"cost\": \"3.0\",    \"kg\": \"10.0\"   },   \"Electronics\": {    \"cost\": \"20.0\",    \"kg\": \"5.0\"   }  },  \"currency\": \"EUR\",  \"fromcity\": \"Berlin-Germany\",  \"fromto\": \"Berlin-Germany_Cairo-Egypt\",  \"tocity\": \"Cairo-Egypt\",  \"trdate\": \"2024-10-31\",  \"tstamp\": 1732961762 }Enter fullscreen modeExit fullscreen modeBy utilizing the TTL (Time to Live) feature in DynamoDB, you can automatically delete records after a specified time period. For instance, in the trip record, there is an attribute called \"tstamp\" that determines the deletion time of the record.Find available tripsThe attributes in the trip record, such as \"fromCity,\" \"toCity,\" or \"fromTo,\" are utilized for search functionality when users seek trips. I employ a global secondary index in DynamoDB to retrieve trips based on the originating city, destination city, or the combination of both.The function looks like this:Index: This refers to the name of the global secondary index.Key: This represents the DynamoDB attribute used for indexing, typically referring to \"fromCity\" or \"toCity.\"City: Denotes the name of the city being referenced.Limit: Specifies the maximum number of records to retrieve.Today_date: This indicates the current date, used to filter and display only trips available from today onwards.LastKey: Utilized for pagination purposes, facilitating the retrieval of subsequent sets of records beyond the initial limit.def get_global_index(index,key,city,limit,today_date,lastkey=None):      if lastkey:         return query_table.query(         IndexName=index,KeyConditionExpression=Key(             key).eq(city),Limit=int(limit),FilterExpression=Attr('acceptto').gte(today_date),ExclusiveStartKey=json.loads(lastkey))      return query_table.query(         IndexName=index,KeyConditionExpression=Key(             key).eq(city),Limit=int(limit),FilterExpression=Attr('acceptto').gte(today_date))Enter fullscreen modeExit fullscreen modeUsers Sent/Received RequestsDynamoDB supports querying data with a key that begins with a specific value. When users send or receive requests in the Take IT app, these requests are stored in DynamoDB with statuses such as \"pending,\" \"accepted,\" \"request,\" or \"declined.\" This allows for efficient querying and retrieval of requests based on their status, enabling seamless management and tracking of request statuses within the application.The function looks like this:item: This refers to \"pending,\" \"accepted,\" \"request,\" or \"declined.\"query_table.query(KeyConditionExpression=Key(\"username\").eq(username) & Key(\"created\").begins_with(item))Enter fullscreen modeExit fullscreen modeHere's an example of how a user request is saved into DynamoDB in JSON format:{  \"username\": \"user2-466b-a4b9-94f90\",  \"created\":  \"request_2022-10-15-22-35-18-213147_user1-4fb1dce7-6a50\",  \"dtime\": \"2022-10-15-22-42-41-599518\",  \"tripid\": \"2022-10-15-22-35-18-213147\",  \"tstamp\": 1669766400 }, {  \"username\": \"user1-4fb1dce7-6a50\",  \"created\": \"pending_2022-10-15-22-35-18-213147_user2-466b-a4b9-94f90\",  \"dtime\": \"2022-10-15-22-42-41-599518\",  \"tripid\": \"2022-10-15-22-35-18-213147\",  \"tstamp\": 1669766400 }Enter fullscreen modeExit fullscreen modeRemove AccountWhen a user decides to delete their account in the Take IT app, several steps are initiated:Querying User Data: All data associated with the user is queried from DynamoDB, including trip history, pending requests, and any other relevant information.Deleting User Data: Each record associated with the user's account is deleted from DynamoDB.This includes trip records, request records (both sent and received), and any other user-specific data stored in the database.Removing User from Cognito: The user is removed from the Cognito user pool, deleting their account and associated authentication tokens.The Cognito delete function looks like this:cognito = boto3.client('cognito-idp',region_name = region_name, verify=True) cognito.admin_delete_user(UserPoolId= userpoolid, Username= username)Enter fullscreen modeExit fullscreen modeThe delete function looks like this:def delete_records(username):    items= query_table.query(        KeyConditionExpression=Key(\"username\").eq(username)    )    for i in range(len(items['Items'])):        query_table.delete_item(        Key={            'username': username,            'created' : items['Items'][i]['created']            }            )    return TrueEnter fullscreen modeExit fullscreen mode"}
{"title": "AWS RE: INFORCE 2024 ABW GRANT", "published_at": 1711023522, "tags": ["aws", "reinforce2024", "abwgrant"], "user": "Isaac Oppong-Amoah", "url": "https://dev.to/aws-builders/aws-re-inforce-2024-abw-grant-385g", "details": "\ud83d\ude80 Exciting news...I am so thrilled to finally share that we are expanding the ABW Grant program to AWS re:Inforce 2024!\ud83d\udd12 AWS re:Inforce is our annual cloud security learning conference focused exclusively on AWS security solutions, cloud security, compliance, and identity.This year, the conference will take place in \ud83d\udd14 Philadelphia, PA from June 10 - 12, 2024\u270d\ufe0f Applications for the re:Inforce ABW Grant are open now and will close on Monday, April 1, 2024 at 5:00 PM ESTApply here\ud83c\udfafEligibility requirements include:First five years in cloud security21 years or olderNot an employee, intern, contractor, etc. of Amazon, AWS, or affiliatesIdentify as being part of a group that is underrepresented in tech or have faced barriers or disadvantages in your technology career due to discrimination actions against underrepresented groupsMost notable for this group \ud83d\udce3 All re:Invent Grant Alumni ARE eligible to apply if you meet the other requirements above. We are allowing folks to receive the grant more than once if its for a different event, so apply asap if you are in the cloud securi ty field!:speaking_head_in_silhouette: Please share with your networks and help us get the word out!AWS Social Posts to share:LinkedInTwitter/XInstagramFacebookPlease also share out with any relevant professional networks, internal affinity groups at your company, or anywhere else that will reach folks breaking into the cloud security field!"}
{"title": "Speed up and save cost for Terraform Deployments with AWS CodeBuild's Lambda Compute", "published_at": 1710993877, "tags": ["terraform", "aws", "codebuild", "finops"], "user": "Jatin Mehrotra", "url": "https://dev.to/aws-builders/accelerate-and-save-cost-for-terraform-deployments-with-aws-codebuilds-lambda-compute-5814", "details": "This article is all about how you can make yourterraform deployments fasterandreduce your AWS billingusing AWSCodeBuild's Lambda Compute.This article will help those who are using Terraform for their IaC on AWS and are aiming to optimise the AWS infrastructure provisioning process.I wanted to go ahead for my readers so I have shared a strategy using the concepts of this blog which will help them to make their CI/CD pipelines better.MotivationAWS has recently releasedtwo new updatesrelated to CodeBuild and these updates were the reason for coming with this blog.[ Nov 6, 2023 ]AWS CodeBuild now supports AWS Lambda computeWhat does that mean:AWS CodeBuild users now have the option to employ AWS Lambda for building and testing their software packages.This new compute mode choice offersquicker builds thanks to Lambda's almost instantaneous start-up times. Additionally, customers can benefit fromcost savings since Lambda compute is billed per second of usage.Until now, The only compute option was to use Ec2 as compute but with this update we can use LAMBDA as compute with CodeBuild.[ Mar 19, 2024 ]AWS CodeBuild now supports custom images for AWS Lambda computeWhat does that mean:AWS CodeBuild has enhanced its capabilities by enabling the utilization of container images stored in Amazon ECR repositories for projects configured to operate on Lambda compute.Previously, users were restricted to leveraging managed container images supplied by AWS CodeBuild. These AWS-managed container images come equipped with support for tools like AWS CLI, AWS SAM CLI, and a range of programming language runtimes.Until now, With LAMBDA as compute there was only the option to use AWS-managed images but now we can use custom images for LAMBDA as compute with CodeBuild.PrerequisitesKnowledge of TerraformBasic working of CodeBuildUnderstanding of core AWS and its operationsLet's get started.Create ECR repositoryAs mentioned in the update we need to create an ECR repository to host custom images. I will name it asterraform.Pull the Terraform docker imageThere is anofficial terraform docker imagewhich I use as a custom image and push to ecr repository.docker pull hashicorp/terraformEnter fullscreen modeExit fullscreen modePush image to ECR repositoryRetrieve an authentication token and authenticate your Docker client to your registry.(change account id as per your aws account id)aws ecr get-login-password--regioneu-west-1--profilecicd | docker login--usernameAWS--password-stdin123456789.dkr.ecr.eu-west-1.amazonaws.comEnter fullscreen modeExit fullscreen modeTag your image so you can push the image to this repository:docker tag hashicorp/terraform 123456789.dkr.ecr.eu-west-1.amazonaws.com/terraform:latestEnter fullscreen modeExit fullscreen modePush terraform(custom) imagedocker push 123456789.dkr.ecr.eu-west-1.amazonaws.com/terraform:latestEnter fullscreen modeExit fullscreen modeCreate s3 buckets for terraform code and backendIn this step, will create twoversioned s3 buckets; one for storing terraform code and another for terraform backend for storing state files with all the default settingsexcept versioning. I will enable versioning for both buckets.I will call themterraform-deployment-test-lambda-computeandterraform-deployment-test-lambda-compute-backendCreate a CodePipelineTo automate and share the strategy I will create a small pipeline which will be crucial for automation.Add source stage as s3.Selectterraform-deployment-test-lambda-compute.Give an arbitrary name for the file name. Since I will be uploading a zip file, I will usetf.zipCreate CodeBuild Project with Lambda as ComputeAfter creating the source stage for the pipeline, you will see the next screen to add the build stage. SelectCodeBuildand selectcreate new projector choose existing if you already have CodeBuild project.While creating the CodeBuild Project, use the Project name asterraform-deployment-test-lambda-compute.Codepipeline's source stage will provide the input artifact which istf.zipfor CodeBuild.For Environment, Environment Image asCustom. Compute asLambda, Environment type asLinux Lambda.Choose Image registry asECR. Selectterraformas ecr repository created in first step.Choose ECR Image aslatest.Create Service role, choose default setting ascreate new service role.For buidspec, chooseinsert build commandsand add the following yaml.All it does is initialise the terraform and apply the terraform configuration.version:0.2phases:build:commands:-terraform init-terraform apply -auto-approve -no-color -input=falseEnter fullscreen modeExit fullscreen modeRest choose everything as default andselect create build projectat the bottom. After creation, you will return to the CodePipline screen with CodeBuild project name auto-filled. Select NextAs next step Skip Deploy StageAfter Reviewing, selectCreateto create the Pipeline.Modify service role for Codebuild projectGo to the Codebuild project, find theterraform-deployment-test-lambda-computeand click on the service role link created automatically by CodeBuild.Since CodeBuild will deploy terraform resources across the account in this case we need to giveAdministrator access policyso that it can deploy any kind of resources.It will make more sense to use this method for cross- account deployment as in that case you can restrict permissions by permitting only to assume to role of the account terraform wants to deploy resources.Terraform codeterraform{backend\"s3\"{bucket=\"terraform-deployment-test-lambda-compute-backend\"region=\"eu-west-1\"key=\"terraform-deployment.tfstate\"}}resource\"aws_s3_bucket\"\"deploy-bucket\"{bucket=\"test-lambda-compute-backend-test-bucket\"tags={Project=\"For CI CD deploy test\"Environment=\"Dev\"}}resource\"aws_s3_bucket\"\"deploy-bucket-two\"{bucket=\"test-lambda-compute-backend-test-bucket-two\"tags={Project=\"For CI CD deploy test\"Environment=\"Dev\"}}Enter fullscreen modeExit fullscreen modeHere I am using backend bucket for storing terraform state.In this terraform code, I am creating two s3 buckets.You can of course divide your code into modules just like any other regular terraform project.zip-rtf.zip ./Enter fullscreen modeExit fullscreen modeThen I am converting this terraform file to a zip file called tf.zip and upload to the s3 bucket. This upload will trigger CodePipline and runterraform applyinside Codebuild using Lambda to create two s3 buckets as per configuration.Pipeline in ActionAfter uploadingtf.zip, the pipeline gets automatically triggered and runs.We can also see the action details of codebuild where it initialises s3 backend for the terraform state and creates two s3 buckets.We can confirm the created buckets in the console too.EC2 compute v/s Lambda computeDocumentation sayslambda compute promotes faster builds and saves cost so I wanted to test this same operation with the Ec2 Compute CodeBuild project.Buildspec commands forCodeBuild project with Compute as EC2will be as follows. It's not the same because with LAMBDA our docker image is from Terraform which already has Terraform command but in the EC2 compute case we need to install Terraform command.version:0.2env:variables:TERRAFORM_VERSION:1.5.4phases:install:commands:-echo\"======== install terraform ========\"-wget--quiethttps://releases.hashicorp.com/terraform/${TERRAFORM_VERSION}/terraform_${TERRAFORM_VERSION}_linux_amd64.zip-unzipterraform_${TERRAFORM_VERSION}_linux_amd64.zip-mvterraform/usr/local/bin/build:commands:-terraforminit-no-color-input=false-terraformapply-auto-approve-no-color-input=falseEnter fullscreen modeExit fullscreen modeEdit the Ec2 codebuild project service role and add Administrator policy as it will be used to create resources.All the other steps remain the same(except buildspec file), just create another CodeBuild project with Compute as EC2 and replace the build action in the pipeline with Lambda compute codeBuild Project.Delete the buckets by commenting out in the terraform code, regeneratetf.zipand upload to s3.Once buckets are deleted, uncomment the terraform regeneratetf.zipand upload to s3. Pipeline will Runwe can see that even though Lambda's default memory configured is 2GB whereas EC2 compute is configured with a default memory of 3GB still** LAMBDA-based deployment(below image) is faster in every sense**.Limitations to LAMBDA as computeAccording to Docs, there are some use cases where EC2 as compute should be used.AWS Lambda lacks support for tools needing root permissions,  Docker, writing outside /tmp, lambda timeout error after 15 minutes, LINUX_GPU_CONTAINER environment, and features like caching, VPC connectivity, EFS, or SSH access.Bonus: StrategyAs you have seen,If your infrastructure deployment workload is something less than 15 minutes codeBuild's LAMBDA as compute with CodePipeline can be used to create a completely secure serverless CD pipeline.A pipeline which can be used to deploy infrastructure cross-account. You can go crazier by adding alerts on failure and success, adding manual approval actions and handling approvals with lambda functions, and integrating with Slack using CodePipeline features.If your workload increases by more than 15 minutes then you can strategise to split codeBuild steps into multiple stages(subsections) within CodePipeline.From DevOps PerspectiveWith this feature, the possibility of doing operations forcode-based eventsbecomes endless (of course considering the limitations of LAMBDA).You can not only build the application coder artifact but also deploy infrastructure cost-effectively and more quickly thanks to AWS LAMBDA.I would like to hear how other members are planning to use this amazing update and in what way. Please try this out, feel free to post your feedback or any questions you didn't understand."}
{"title": "Deploying Amazon Managed Service for Apache Kafka (Amazon MSK) with CloudFormation", "published_at": 1710990960, "tags": ["aws", "kafka", "infrastructureascode", "devops"], "user": "Ehi Enabs", "url": "https://dev.to/aws-builders/deploying-amazon-managed-service-for-apache-kafka-amazon-msk-with-cloudformation-463o", "details": "Amazon Managed Service for Apache Kafka (Amazon MSK) helps developers easily build scalable and resilient streaming applications by abstracting the complexities of Kafka infrastructure management.Developed initially by LinkedIn, Apache Kafka has become a significant technology for real-time data processing and event streaming. Kafka is a distributed streaming platform which uses a publish-subscribe messaging model. Known for its scalability, reliability, fault tolerance, and durable storage capabilities, Kafka facilitates the seamless ingestion, processing, and delivery of massive volumes of data in real-time.Whether it's processing website clickstreams, tracking sensor data from IoT devices, or powering real-time analytics, Kafka's versatility makes it a go-to solution for building robust streaming architectures.Deploying Amazon Managed Service forApache Kafka (Amazon MSK)with CloudFormationAWS CloudFormation is a great tool designed to streamline infrastructure provisioning through code. With CloudFormation, developers can describe their AWS infrastructure using a simple, declarative template format, encompassing everything from EC2 instances to S3 buckets and IAM roles. This template is a blueprint for resource creation and configuration, ensuring consistency and reproducibility across environments. CloudFormation templates, typically written in JSON or YAML, articulate the desired state of the infrastructure, abstracting away the procedural steps needed to achieve that state.The following is a CloudFormation Template for deploying Apache Kafka (Amazon MSK)AWSTemplateFormatVersion:\"2024-09-09\"Description:\"AmazonMSKDeploymentExample\"Resources:KafkaCluster:Type:AWS::MSK::ClusterProperties:ClusterName:MyKafkaClusterKafkaVersion:\"2.8.0\"NumberOfBrokerNodes:3BrokerNodeGroupInfo:InstanceType:kafka.m5.largeClientSubnets:-!RefSubnetIdsSecurityGroups:-!RefSecurityGroupIdEncryptionInfo:EncryptionInTransit:InCluster:trueEncryptionAtRest:DataVolumeKMSKeyId:!RefKmsKeyIdLoggingInfo:BrokerLogs:CloudWatchLogs:LogGroup:!RefLogGroupNameParameters:SubnetIds:Description:\"SubnetIDsforKafkacluster\"Type:List<AWS::EC2::Subnet::Id>SecurityGroupId:Description:\"SecurityGroupIDforKafkacluster\"Type:AWS::EC2::SecurityGroup::IdKmsKeyId:Description:\"KMSKeyIDfordataencryptionatrest\"Type:AWS::KMS::Key::IdLogGroupName:Description:\"LogGroupNameforKafkaclusterlogs\"Type:StringEnter fullscreen modeExit fullscreen modeAfter creating the template, you can deploy your Amazon MSK with was cli by using the following command;aws cloudformation create-stack--stack-nameMyKafkaStack--template-bodyfile://msk-deployment.yaml\\--parametersParameterKey=SubnetIds,ParameterValue=\"subnet-12345678,subnet-23456789\"\\ParameterKey=SecurityGroupId,ParameterValue=\"sg-12345678\"\\ParameterKey=KmsKeyId,ParameterValue=\"arn:aws:kms:us-east-1:123456789012:key/abcd1234-abcd-1234-abcd-123456789012\"\\ParameterKey=LogGroupName,ParameterValue=\"/aws/kafka/mykafkacluster\"Enter fullscreen modeExit fullscreen modeConclusion:Deploying Amazon Managed Service for Apache Kafka (Amazon MSK) with AWS CloudFormation streamlines the process of setting up a Kafka cluster for real-time data streaming. By abstracting the complexities of infrastructure management, Amazon MSK enables teams to focus on building innovative streaming applications with ease."}
{"title": "Bedrock Agent & Tools - Tracing Best practises", "published_at": 1710957532, "tags": ["sagemaker", "aws", "bedrock"], "user": "Amit Kayal", "url": "https://dev.to/aws-builders/bedrock-agent-tools-tracing-best-practises-4217", "details": "I understand most of bedrock agent userss will have a use case where you have implemented multiple Lambda functions with a Bedrock Agent to perform different tasks and are looking for guidance in Debugging the API calls and responses from the Agent and lambda functions.Here are some of the approaches that we have been using and found quite effective to track and trace agents and usage of their toolsEnable Tracing for the Agent: When invoking the agent, set thedebugparameter totrue. This will enable detailed tracing for the agent's execution, including the tools (Lambda functions) invoked and their responses. The trace will be printed to the console or returned as part of the agent's response, depending on how you invoke the agent. [1] Example (Python):python result = agent.run(query, debug=True)Log Within Lambda Functions: Within each of your Lambda functions (tools), add logging statements to capture relevant information and events. You can use AWS Lambda's built-in logging capabilities or integrate with a centralized logging service like Amazon CloudWatch Logs. [2] Example (Python):python import logginglogger = logging.getLogger(__name__)def lambda_handler(event, context):http://logger.info (f\"Received event: {event}\") # Your Lambda function's logic here http://logger.info (f\"Returning result: {result}\") return resultCorrelate Logs Using Request IDs or Tracing IDs: To correlate logs across multiple Lambda functions and the agent, you can use request IDs or tracing IDs. Pass a unique ID as part of the event or context to your Lambda functions and include it in your log statements. This will allow you to trace the flow of events across different components of your system.import loggingimport uuiddef lambda_handler(event, context):request_id = event.get(\"request_id\", str(uuid.uuid4()))logger = logging.getLogger(__name__)logger = logging.LoggerAdapter(logger, {\"request_id\": request_id})logger.info(f\"Received event: {event}\")logger.info(f\"Returning result: {result}\")return resultUse AWS X-Ray for Distributed Tracing: AWS X-Ray is a service that can help you analyze and debug distributed applications, including Lambda functions. By integrating X-Ray with your Bedrock application, you can trace requests as they travel through your Lambda functions and gain insights into their performance and potential issues. [3] - Enable X-Ray tracing for your Lambda functions by adding the necessary configuration. - Instrument your Lambda functions with X-Ray tracing code to capture relevant information and events. - Use the X-Ray console or integrate with other monitoring tools to analyze the traces and identify potential bottlenecks or issues.Implement Advanced prompts : By using advanced prompts, you can enhance your agent's accuracy through modifying these prompt templates to provide detailed configurations. You can also provide hand-curated examples for few-shot prompting, in which you improve model performance by providing labeled examples for a specific task. [4] By combining the built-in tracing mechanism, custom logging within your Lambda functions, and distributed tracing with AWS X-Ray, you can gain better visibility into the API calls, events, and interactions happening within your Bedrock agent and its associated tools. This can help you debug issues more effectively and trace errors back to their source across multiple Lambda functions.ReferenceTrace events in Amazon Bedrock - Amazon BedrockBest practices for your debugging environment - AWS LambdaWhat is AWS X-Ray? - AWS X-RayAdvanced prompts in Amazon Bedrock - Amazon Bedrock"}
{"title": "DevOps with Guruu | Chapter 8 : Connect more VPC without internet \u2013 Best Pratice | Network Service with AWS", "published_at": 1710939955, "tags": ["webdev", "devops", "aws"], "user": "Hoang Guruu", "url": "https://dev.to/aws-builders/devops-with-guruu-chapter-8-connect-more-vpc-without-internet-best-pratice-network-service-with-aws-3khh", "details": "DevOps with Guruu | Chapter 8 : Connect more VPC without internet \u2013 Best Pratice | Network Service with AWS0:00 Welcome to my Series DevOps with GuruuGenerate Key Pair1:50 Initialize CloudFormation Template10:34 Create Transit Gateway11:00 Create Transit Gateway Attachments14:00 Create Transit Gateway Route Tables16:00 Add Transit Gateway Routes to VPC Route Tables22:00 Clean up resourcesJoin me on this journey to mastering DevOps. Let's get hands-on and build a strong foundation together! Don't forget to like, share, and subscribe for more exciting content. Happy coding, future DevOps!\""}
{"title": "Failover Mechanism in Amazon Route 53 Private Hosted Zones", "published_at": 1710933110, "tags": ["aws", "awscommunity", "route53", "cloud"], "user": "Kunal Shah", "url": "https://dev.to/aws-builders/failover-mechanism-in-amazon-route-53-private-hosted-zones-lb", "details": "Failover Mechanism in Amazon Route 53 Private Hosted ZonesAWS Cloud Hands-on Lab Practice SeriesProject Overview \u2014The AWS Route 53 Failover project aims to architect a secure, cost-efficient, Fault tolerant and Highly available cloud environment. The project\u2019s primary goal is to provide how to configure DNS entries on Amazon Route 53 to do dynamic routing between resources in different AWS regions while adhering to AWS best practices and compliance requirements.SOLUTIONS ARCHITECTURE OVERVIEW -First Let\u2019s understand the real world use case :1.Disaster Recovery:In a disaster recovery setup, where you have primary and secondary data centers or regions, failover policies can automatically redirect traffic from the primary to the secondary site in case of a failure.Example:let\u2019s say you have a primary data center in US-East (Virginia) and a secondary data center in US-West (Oregon). If the primary data center experiences an outage, Route 53 failover policies can redirect traffic to the secondary data center.2.Multi-Region Redundancy:For global applications that require high availability, you may deploy identical application stacks in multiple AWS regions. Failover policies can route traffic to the closest healthy region or distribute traffic evenly across regions based on health checks.Example:If your application is deployed in US-East (Virginia) and EU-West (Ireland), Route 53 can route traffic to the region with the lowest latency or the region that passes health checks.3.Blue/Green Deployments:During software updates or deployments, you can use failover policies to perform blue/green deployments.Example :For instance, suppose you have a production environment (blue) and a staging environment (green). You can update the staging environment, run health checks, and then switch traffic from the production environment to the staging environment using failover policies once the update is successful.4.Highly Available Database Replication: Failover policies can be used to manage database failover scenarios where you have active-passive or active-active database replication setups.Example:For instance, if you have a primary database in one availability zone and a standby database in another availability zone, Route 53 can automatically redirect traffic to the standby database if the primary database becomes unavailable.5.Content Delivery Networks (CDNs):In CDN setups, failover policies can be employed to route traffic to alternate CDN endpoints in case of CDN node failures or performance degradation.Example:If you have CDN endpoints in different regions or with different providers, Route 53 can direct traffic to the next best endpoint if the primary endpoint experiences issues.Also when considering the use of failover policies in Amazon Route 53 Private Hosted Zones within the context of the Well-Architected Framework (AWS WAR), it primarily aligns with the \u201cReliability\u201d and \u201cOperational Excellence\u201d pillars.Prerequisite \u2014AWS Account with Admin Access.AWS CLI user with Access key & Secret Key.AWS Services Usage \u2014AWS Route53, VPC, EC2, SSM, S3, CloudFormation and IAMSTEP BY STEP GUIDE -STEP 1 : Clone the GitHub RepoNavigate to following GitHub RepositoryAWS-Route53-FailoverClone the repo to download the CloudFormation Template & code used for this lab.CloudFormation template name \u2014route53-vpc-cfn-template.yamlSTEP 2 : Creating AWS resources through CloudFormation service.Login to AWS account, Navigate to AWS CloudFormation Service.Head over & change the region of the aws console where you want to deploy the Primary region resources. (SITE-A)Click on Create Stack & upload the template downloaded in the step 1.VPC CIDR (SITE-A) : 10.0.0.0/16PublicSubnet : 10.0.1.0/24This stack will create one VPC and create a public subnet with two EC2 instances, an internet gateway, with a default route on the public subnet in regions you deployed.NOW Head over & change the region of the aws console where you want to deploy the Secondary region resources. (SITE-B)Click on Create Stack & upload the template downloaded in the step 1.VPC CIDR (SITE-B) : 10.1.0.0/16PublicSubnet : 10.1.1.0/24This stack will create one VPC and create a public subnet with two EC2 instances, an internet gateway, with a default route on the public subnet in regions you deployed.STEP 3 : Creating a VPC Peering to connect both regionsOption 1: Use AWS CloudShell to run the commands.Option 2: From your local machine where you have aws cli working with appropriate access key & secret key.Make sure to edit the regions before running the commandsRun the commands given inVPC env variable script.txtfile.Get the VPC ID of the respective VPC\u2019s created.Now Run the commands givenVPC Peering Script.txtfile.This will create VPC peering between two VPCs from different AWS Regions (SITE A & SITE B).STEP 4 : Creating Private DNS entries:Create a Route 53 \u2014 Private Hosted Zone for theacloudguy.internalDNS entries by associating the Site A.You can change the hosted zone name according to your wish.Run the commands fromprivate DNS entries.txtfile.STEP 5 : Creating Route 53 Health Check:Get thepublicIP of both Web Servers. This is because Route 53 health checkers are public and they can only monitor hosts with IP addresses that are publicly routable on the internet.Run the command fromPublic-IP-For-Route53-HealthChecks.txtfile.You need to create a health check policy file:health check policy.txtNow, Let\u2019s create the health check for our primary endpoint that is in SITE A region.Run the command fromPrimary-HealthCheck-Record.txtfile.The health check will be active in 30\u201360 seconds.Go to Route53 Console & check the health check section.STEP 6 : Creating Route 53 Failover Policy:Get theprivateIP of both Web Servers.Run the command fromPrivate-IP-For-Route53-HealthChecks.txtfile.Run the Failover routing policy fromFailover policy.txtfile.TIP :It\u2019s being used Private IP to keep the communication through VPC Peering.Now Associate the traffic policy to Route53 Private Hosted Zone.Run the command fromAssociate Failover Policy.txtfile.We just created a policy similar to the image below: (your public IP & Private IP will be different)STEP 7 : Test the failover policy:Connect to EC2 Instance through EC2 Connect from SITE A.Try to access the website using \u201cservice.acloudguy.internal\u201ddig +short service.acloudguy.internalcurl service.acloudguy.internalYou will below responses from it.Now to test the failover, remove Security Group Inbound rules of Primary Site A EC2 Instance from AWS Console.Once there is no Inbound rule to EC2 Instance.After 2 minutes, the Health Check will mark thePRIMARY EC2 InstanceasUNHEALTHYand will trigger the failover mechanism.Health Check will fail & route the traffic to Secondary Site B EC2 Instance.We can run the commands againdig +short service.acloudguy.internalcurl service.acloudguy.internalSTEP 8 : Decommission:Delete all the resources created during the lab.aws ec2 delete-vpc-peering-connection \u2014 vpc-peering-connection-id $PEERING_IDDelete the CloudFormation Stacks from SITE A & SITE B Regions.Delete Route53 Health Checks from AWS Console.Delete Route53 Traffic Policy and Private Hosted Zone from ConsoleDelete files health-check-config.json & failover-policy.jsonCongrats ! We have successfully completed lab for How to Implement Failover Policies in Amazon Route 53 Private Hosted Zones.I am Kunal Shah, AWS Certified Solutions Architect, helping clients to achieve optimal solutions on the Cloud. Cloud Enabler by choice, DevOps Practitioner having 8+ Years of overall experience in the IT industry.I love to talk about Cloud Technology, DevOps, Digital Transformation, Analytics, Infrastructure, Dev Tools, Operational efficiency, Serverless, Cost Optimization, Cloud Networking & Security.aws #community #builders #VPC #route53 #failover #mechanism #cloudformation #disaster #recovery #network #security #hybrid #network #peering #isolated #solution #war #reliability #operations #Excellence #infrastructure #scalable #highly #available #blue #green #deployment #private #secure #design #acloudguyYou can reach out to me @acloudguy.in"}
{"title": "Terraform - Modules & Demos", "published_at": 1710933108, "tags": ["terraform", "modules", "iac", "registry"], "user": "Srinivasulu Paranduru", "url": "https://dev.to/aws-builders/terraform-modules-demos-209h", "details": "Use case why we using Terraform Modules :We do repeat multiple times various terraform resources for multiple projects, create once and reuse it when ever requiredUnderstanding the DRY principleIn software engineering, Don't repeat yourself (DRY) is a principle of software development aimed at reducing repetition of software patterns.Sample EC2 Resourceresource \"aws_instance\" \"myweb\" {      ami = \"ami-bf5540df\"     instance_type = \"t2.micro\"   }Enter fullscreen modeExit fullscreen modeInstead of repeating a resource block multiple times, we can make use of a centralized structure.Centralized Structure:We can centralize the terraform resources and can call out from TF files whenever required. So we are going to use Terraform modules for this approach.Modules: Understanding the DRY principleIn software engineering, Don't repeat yourself (DRY) is a principle of software development aimed at reducing repetition of software patterns.A module is a container for multiple resources that are used together.Every Terraform configuration has at least one module, known as its root module, which consists of the resources defined in the .tf files in the main working directory.A module can call other modules, which lets you include the child module's resources into the configuration in a concise way. Modules can also be called multiple times, either within the same configuration or in separate configurations, allowing resource configurations to be packaged and re-used.Calling a Child ModuleTo call a module means to include the contents of that module into the configuration with specific values for its input variables.Modules are called from within other modules using module blocksmodule \"CT-Server\"{     source =   \"./Modules/CT-Server\"     instance_type = \"t2.nano\"     ami_id = \"ami-bf5540df\"    # ami id changes based on the region }Enter fullscreen modeExit fullscreen modeA module that includes a module block like this is the calling module of the child module.The label immediately after the module keyword is a local name (CT-Server is the local name), which the calling module can use to refer to this instance of the module.Within the block body (between { and }) are the arguments for the module. Module calls use the following kinds of arguments:The source argument is mandatory for all modules.The version argument is recommended for modules from a registry.Terraform defines a few other meta-arguments that can be used with all modules, including for_each and depends_on.Version: Use the version argument in the module block to specify versionsmodule \"consul\" {   source  = \"hashicorp/consul/aws\"   version = \"0.0.5\"    servers = 3 }Enter fullscreen modeExit fullscreen modeMeta-argumentsAlong with source and version, Terraform defines a few more optional meta-arguments that have special meaning across all modulescountfor_eachprovidersdepends_onAccessing Module Output Values: The resources defined in a module are encapsulated, so the calling module cannot access their attributes directly. However, the child module can declare output values to selectively export certain values to be accessed by the calling module.resource \"aws_elb\" \"example\" {   # ...    instances = module.servers.instance_ids }Enter fullscreen modeExit fullscreen modeReferences:https://developer.hashicorp.com/terraform/language/moduleshttps://developer.hashicorp.com/terraform/tutorials/modules?utm_source=WEBSITE&utm_medium=WEB_IO&utm_offer=ARTICLE_PAGE&utm_content=DOCSTerraform Registry :The terraform registry is a repository of modules written by the terraform community.The terraform can help you get started with Terraform more quickly.https://registry.terraform.io/browse/modulesExample:Creating an AWS EC2 Module and trying to call the terraform module.My folder structure and files looks like thisTerraformTraining  ( Base folder)   Modules  (sub folder)     - CT-Server (sub folder)       - main.tf       - variable.tf   main.tf   provider.tf1.Create a folder  TerraformTraining as Base folder and create a subfolder Modules inside TerraformTraining folder then create one more folderCT-Serverunder Modules folder.2.Create a file main.tf and place the content belowresource \"aws_instance\" \"ec2-training\" {     ami = var.ami_id     instance_type = var.instance_type }Enter fullscreen modeExit fullscreen mode3.Create a file variable.tf and place the content belowvariable \"ami_id\"{    type = string    default = \"\" }  variable \"instance_type\"{   type = string   default = \"t2.micro\" }Enter fullscreen modeExit fullscreen mode4.Go back to the base folder TerraformTraining and create a file main.tf and copy the below mentioned codedata \"aws_ami\" \"app_ami\" {    most_recent =true    owners = [\"amazon\"]     filter{      name = \"name\"      values = [\"amzn2-ami-hvm*\"]    } }  ## Creating EC2 using Terraform module module \"CT-Server\"{     source =   \"./Modules/CT-Server\"     instance_type = \"t2.nano\"     ami_id = data.aws_ami.app_ami.id }Enter fullscreen modeExit fullscreen mode5.Go back to the base folder TerraformTraining and create a file provider.tf and copy the below mentioned codeprovider \"aws\" {   region = \"eu-west-1\" }Enter fullscreen modeExit fullscreen modeNote : Already i have configured aws credentials using aws configureConclusion : Discussed about terraform modules, create your own terraform modules and use it or use terraform modules from terraform registry.\ud83d\udcac If you enjoyed reading this blog post and found it informative, please take a moment to share your thoughts by leaving a review and liking it \ud83d\ude00 and follow me inlinkedin"}
{"title": "Adding Basic Authentication to the Serverless Dash App", "published_at": 1710924243, "tags": ["aws", "dash", "apigateway", "authentication"], "user": "Maurice Borgmeier", "url": "https://dev.to/aws-builders/adding-basic-authentication-to-the-serverless-dash-app-na9", "details": "In a recentpostexplained how to set up a Dash app in a completely serverless manner. If you haven't read that one yet, you should - otherwise this won't make much sense because we'll continue where we left off.At the end of that post we successfully hosted a Dash App in a Lambda function behind an API Gateway. That's already a good basis but unless you want your webapp to be open to the public, you probably want to add some form of authentication. Today I'm going to explain how to add HTTP Basic Auth, in a future post I'll dive into Cognito.HTTP Basic Auth has been around for a very long time (RFC 7235). The general idea is pretty simple. If a website requires credentials, you send them in the authorization header. The authorization string starts withBasicand is followed by the base64 encoded version of concatenating username and password with a colon as the separator. It looks something like this.Authorization: Basic base64(<username>:<password>)Enter fullscreen modeExit fullscreen modeObviously this should be only used when communicating over an encrypted HTTPS connection, otherwise the credentials are sent in clear text over the internet. Given that the API Gateway supports exclusively HTTPS, we can take that for granted.But how does a user enter the credentials when accessing the website?The first option is through the url, like this:https://<username>:<password>@website.com/Enter fullscreen modeExit fullscreen modeThe browser will then automatically encode this correctly as basic auth information. But that's not very user friendly and potentially logs the credentials in the browser history so it should be avoided. The other option is that the server asks for authentication by sending a response with the HTTP status code 401 and this header:WWW-Authenticate: Basic realm=\"mywebsite\"Enter fullscreen modeExit fullscreen modeIf the server sends this header, the browser will prompt for the credentials. Therealm=...part is optional and only interpreted by some browsers. A browser that supports it will display the realm info as part of the login prompt. This Flow-Diagram from themdn web docs about HTTP Basic Authenticationvisualizes this quite well.Here's what the login prompt looks like:Granted, this login window isn't very pretty but it gets the job done and protects our website from unauthorized access.Now that we understand this flow, let's start building it. If you're already familiar with Dash, you may know that it has abasic auth extensionthat seems like it would be useful here. Unfortunately it isn't. The API Gateway willrename theWWWW-Authenticateheaderif it's returned from the backend to resolvepotential ambiguity, which means the browser won't prompt for credentials. This means the straightforward solution won't work.Instead, we have to rely on a custom authorizer and a gateway response. This approach is inspired by amedium articlethat I adapted and extended. The custom authorizer is a Lambda function that we'll write to decode the Authorization header and check the credentials. It will support multiple credential backends: hardcoded credentials, the SSM Parameter Store and a Secretsmanager Secret. But let's not get ahead of ourselves.In addition to the Lambda function, we need to configure agateway response. These gateway responses allow us to send a response with theWWW-Authenticateheader that we need for the prompt. Gateway responses are employed if the backend or an authorizer don't send a response. It uses a simple pattern matching based on the HTTP status code.To implement our solution, we add a few more resources to our SAM app. The authorizer function, an SSM parameter for the credentials, and the gateway response.# template.yamlResources:#...AuthorizerFunction:Type:AWS::Serverless::FunctionProperties:CodeUri:basic_auth_authorizer/Handler:basic_auth.lambda_handlerEnvironment:Variables:CREDENTIAL_PROVIDER_NAME:SSMSSM_CREDENTIAL_PARAMETER_NAME:!RefSsmParameterWithCredentials# ...Policies:-SSMParameterReadPolicy:ParameterName:!RefSsmParameterWithCredentials# ...SsmParameterWithCredentials:Type:AWS::SSM::ParameterProperties:Type:String# JSON-Object, e.g. {\"username\": \"password\"}Value:\"{}\"BasicAuthPrompt:Type:AWS::ApiGateway::GatewayResponseProperties:ResponseType:UNAUTHORIZEDRestApiId:!RefServerlessRestApiStatusCode:\"401\"ResponseParameters:gatewayresponse.header.WWW-Authenticate:!Sub'''Basicrealm=\"${AuthenticationPrompt}\"'''Enter fullscreen modeExit fullscreen modeThe code for this setup isavailable on Github again. I'm not going to walk through all of the authorizer code here, feel free to check out the fullimplementation, it should be relatively easy to read.# basic_auth_authorizer/basic_auth.pydeflambda_handler(event,_context):credential_provider=CREDENTIAL_PROVIDER_NAME_TO_CREDENTIAL_PROVIDER.get(os.environ.get(ENV_CREDENTIAL_PROVIDER_NAME,\"HARDCODED\"))valid_credentials=credential_provider()try:username,password=get_username_and_password_from_header(event)correct_password=valid_credentials.get(username)ifpassword==correct_password:prefix,stage,*_=event[\"methodArn\"].split(\"/\")all_resources_arn=f\"{prefix}/{stage}/*\"policy={\"principalId\":username,\"policyDocument\":{\"Version\":\"2012-10-17\",\"Statement\":[{\"Action\":\"execute-api:Invoke\",\"Effect\":\"Allow\",\"Resource\":all_resources_arn,}],},}returnpolicyraiseUnauthorizedException()exceptException:return\"Unauthorized\"Enter fullscreen modeExit fullscreen modeAs you can see, thelambda_handlerfunction first selects the credential provider based on an environment variable and looks up the supported credentials. Next, it extracts the authorization string from the event and parses it into the supplied username and password. If anything goes wrong here or the header is missing, an exception is raised and the function returnsUnauthorized. This will then trigger the login prompt through the gateway response.If we were able to fetch valid credentials and the supplied credentials match, we create an IAM policy that grants access to the API gateway. This policy document is then returned and the API gateway caches the response for a period of time (5 minutes by default). The policy I implemented here is very broad and grants full read/write access to that stage of the API gateway. If you need a more granular policy, you can achieve that through a more detailed resource section.I chose to make the credential provider plugable and currently the code supports hard-coded credentials, an SSM parameter or a SecretsManager secret. More details on their respective configurations can be found in theGithub repo.# basic_auth_authorizer/basic_auth.pyCREDENTIAL_PROVIDER_NAME_TO_CREDENTIAL_PROVIDER:dict[str,Callable[[],dict[str,str]]]={# \"HARDCODED\": lambda: {\"avoid\": \"using_me\"},  # You really shouldn't use these.\"SSM\":get_credentials_from_ssm_parameter,\"SECRETS_MANAGER\":get_credentials_from_secrets_manager,}Enter fullscreen modeExit fullscreen modeAs the last step, we just need to tell our API gateway to use the new authorizer function, which I'm doing in theGlobalsection of the SAM template.# template.yamlGlobals:# ...Api:Auth:Authorizers:BasicAuth:FunctionArn:!GetAttAuthorizerFunction.ArnDefaultAuthorizer:BasicAuthEnter fullscreen modeExit fullscreen modeAfter runningsam build && sam deploywe can test it. When you access the website you should be prompted to login and after to enter the credentials you'll be allowed to see the webapp.This setup ensures that only authenticated access to the webapp is possible. There are however a few drawbacks. Username and passwords are stored in plain text (albeit encrypted at rest depending on the backend), which is not ideal for a production system. It would be safer to salt and hash the saved password in order to store a derivative of the original to check against without storing the password itself.This could be implemented, but there's a much better solution if you need something more serious. It's possible to integrate the serverless Dash app with Cognito and I'll show you how to do that in the future. The other problem is the lack of proper session management. The app only knows that someone is logged in and not many more details, which makes authorization difficult.Now that I've outlined the weaknesses that you should be aware of, let's talk about what this is intended to be used for. If you want to build a quick PoC or just avoid exposing the app to unauthenticated users, this setup is a good start. It has a limited complexity, is easy to integrate and in addition to that not really Dash-specific, i.e. we didn't touch the Dash app once. Sure it's not a fancy solution, but that's not always necessary.Hopefully you learned something from this post, stay tuned for the implementation with Cognito based authentication.\u2014 MauriceTitle Photo byKutan UralonUnsplash"}
{"title": "Rapid Development of Agents for Amazon Bedrock Using AWS Lambda Web Adapter", "published_at": 1710896977, "tags": ["bedrock", "lambda", "fastapi", "serverless"], "user": "moritalous | Kazuaki Morita", "url": "https://dev.to/aws-builders/rapid-development-of-agents-for-amazon-bedrock-using-aws-lambda-web-adapter-3f5e", "details": "Hello everyone. This is my first post. I work as a system integrator in Japan. My work involves system architecture using services like Bedrock and serverless. I plan to continue posting here regularly, so please stay tuned. Thank you.\ud83d\udca1Original Japanese Version is here.https://qiita.com/moritalous/items/f828c5d7d2d116884f9aThere is a feature called Agents for Amazon Bedrock that allows you to build agents on Amazon Bedrock. This feature is built on Lambda, but requires the following:A Lambda function containing the business logic for the actions the agent will executeAn OpenAPI schema containing the API description, structure, and parametersAdditionally, since the event JSON is in a dedicated format, you need to be mindful of what kind of events will arrive while developing.AWS's AWS Lambda Web Adapter now supports Agents for Amazon Bedrock, so we'll introduce how to use it.What is AWS Lambda Web Adapter?It's a tool for running web applications on AWS Lambda.With AWS Lambda Web Adaptor, developers can build web apps (HTTP APIs) using their familiar frameworks (Express.js, Next.js, Flask, SpringBoot, ASP.NET, Laravel, etc., anything that uses HTTP 1.1/1.0) and run them on AWS Lambda. You can also run the same Docker image on AWS Lambda, Amazon EC2, AWS Fargate, and your local computer.https://github.com/awslabs/aws-lambda-web-adapterHere's an image showing the configuration (from the official site):Until now, only HTTP events from API Gateway or ALB were supported, butNon-HTTP Eventshave now been supported! \ud83c\udf89\ud83c\udf89\ud83c\udf89Here's an image for Non-HTTP events:Non-HTTP events such as SNS or SQS can now be received. The event type is automatically identified, with HTTP events forwarded to their respective path, and Non-HTTP events forwarded to/events(which can be changed).For Agents for Amazon Bedrock, the following JSON will be sent to/events. If the JSON received at/eventscan be parsed and routed to the desired path, it will work as intended.Reference:Lambda input event from Amazon Bedrock{\"messageVersion\":\"1.0\",\"agent\":{\"name\":\"string\",\"id\":\"string\",\"alias\":\"string\",\"version\":\"string\"},\"inputText\":\"string\",\"sessionId\":\"string\",\"actionGroup\":\"string\",\"apiPath\":\"string\",\"httpMethod\":\"string\",\"parameters\":[{\"name\":\"string\",\"type\":\"string\",\"value\":\"string\"},...],\"requestBody\":{\"content\":{\"<content_type>\":{\"properties\":[{\"name\":\"string\",\"type\":\"string\",\"value\":\"string\"},...]}}},\"sessionAttributes\":{\"string\":\"string\",},\"promptSessionAttributes\":{\"string\":\"string\"}}Enter fullscreen modeExit fullscreen modeThe routing process is common, so I'vepublished a libraryusing FastAPI's Middleware mechanism.With this library, you can develop APIs with FastAPI without worrying about/events.https://github.com/moritalous/lwa-fastapi-middleware-bedrock-agent\ud83d\udca1NoteI requested support for Agents for Amazon Bedrock for Lambda Web Adapter, which led to its implementation. I was also asked to create a sample and a general library.I hope support Agents for Amazon Bedrock #317It was a very valuable experience.Sample App ExplanationHere's a sample app using this configuration:https://github.com/moritalous/lwa-fastapi-middleware-bedrock-agent/tree/main/example/bedrock-agent-fastapiDirectory structurebedrock-agent-fastapi \u251c\u2500\u2500 README.md \u251c\u2500\u2500 app \u2502   \u251c\u2500\u2500 Dockerfile \u2502   \u251c\u2500\u2500 __init__.py \u2502   \u251c\u2500\u2500 main.py \u2502   \u2514\u2500\u2500 requirements.txt \u251c\u2500\u2500 events \u2502   \u251c\u2500\u2500 s3_bucket_count.json \u2502   \u251c\u2500\u2500 s3_object.json \u2502   \u2514\u2500\u2500 s3_object_count.json \u2514\u2500\u2500 template.yamlLibraries usedMain Python application libraries used:FastAPIlwa-fastapi-middleware-bedrock-agent(FastAPI Middleware for \"Agents for Amazon Bedrock\")Integrating Lambda Web AdapterThe deployment type for Lambda is containers. In the Dockerfile's second line, we copy the contents of Lambda Web Adapter. This single line is the minimum required to use Lambda Web Adapter.FROMpublic.ecr.aws/docker/library/python:3.12.0-slimCOPY--from=public.ecr.aws/awsguru/aws-lambda-adapter:0.8.1 /lambda-adapter /opt/extensions/lambda-adapterENVPORT=8000 AWS_LWA_READINESS_CHECK_PROTOCOL=tcpWORKDIR/var/taskCOPYrequirements.txt ./RUNpython-mpipinstall-rrequirements.txtCOPY*.py ./CMDexec uvicorn --port=$PORT main:appFastAPI API DevelopmentThe program code is only inmain.py. Except for specifying the Middleware, it's the same as a general FastAPI application.This sample includes the following APIs:PathMethodDescription/s3_bucket_countGETReturns the number of S3 buckets/s3_object_countGETReturns the number of objects in the specified S3 bucket/s3_objectGETReturns the last modified date of the specified S3 bucket and object keyimportdatetimeimportloggingimportboto3fromfastapiimportFastAPI,QueryfrompydanticimportBaseModel,Fieldfrombedrock_agent.middlewareimportBedrockAgentMiddlewareapp=FastAPI(description=\"This agent allows you to query the S3 information in your AWS account.\",)app.openapi_version=\"3.0.2\"app.add_middleware(BedrockAgentMiddleware)middleware_logger=logging.getLogger(\"bedrockagent-middleware\")middleware_logger.setLevel(level=logging.DEBUG)s3=boto3.resource(\"s3\")classS3BucketCountResponse(BaseModel):count:int=Field(description=\"the number of S3 buckets\")@app.get(\"/s3_bucket_count\")asyncdefget_s3_bucket_count()->S3BucketCountResponse:\"\"\"This method returns the number of S3 buckets in your AWS account.      Return:         S3BucketCountResponse: A json object containing the number of S3 buckets in your AWS account.\"\"\"count=len(list(s3.buckets.all()))returnS3BucketCountResponse(count=count)classS3ObjectCountResponse(BaseModel):count:int=Field(description=\"the number of S3 objects\")@app.get(\"/s3_object_count\")asyncdefget_s3_object_count(bucket_name:str=Query(description=\"Bucket name\"),)->S3ObjectCountResponse:\"\"\"This method returns the number of S3 objects in your specified bucket.      Return:         S3ObjectCountResponse: A json object containing the number of S3 objects in your specified bucket.\"\"\"count=len(list(s3.Bucket(bucket_name).objects.all()))returnS3ObjectCountResponse(count=count)classS3GetObjectRequest(BaseModel):bucket_name:str=Field(description=\"Bucket name\")object_key:str=Field(description=\"Object key\")classS3GetObjectResponse(BaseModel):last_modified:datetime.datetime=Field(description=\"the last modified date\")@app.post(\"/s3_object\")asyncdefget_s3_object(request:S3GetObjectRequest):\"\"\"This method returns the last modified date of S3 object.      Return:         S3GetObjectResponse: A json object containing the last modified date of S3 objects.\"\"\"object=s3.Object(request.bucket_name,request.object_key)last_modified=object.get()[\"LastModified\"]returnS3GetObjectResponse(last_modified=last_modified)The Middleware specification is in this part. This single specification routes events received at/eventsappropriately.frombedrock_agent.middlewareimportBedrockAgentMiddlewareapp.add_middleware(BedrockAgentMiddleware)Running FastAPI LocallyInstall librarypipinstallfastapi uvicornpydantic==1.10.13 lwa-fastapi-middleware-bedrock-agent boto3Since it's a FastAPI app, you can run it as is.uvicorn main:app--reload(console log)INFO:     Uvicorn running on http://127.0.0.1:8000 (Press CTRL+C to quit) INFO:     Started reloader process [4999] using StatReload INFO:     Started server process [5001] INFO:     Waiting for application startup. INFO:     pass_through_path: /events INFO:     Application startup complete.Accessinghttp://127.0.0.1:8000/docswill display the OpenAPI documentation via Swagger UI.You can proceed with agent development as FastAPI APIs.Generating OpenAPI SchemaThe OpenAPI schema can be output using FastAPI's functionality. This output can be used as-is for Agents for Amazon Bedrock.(Run in the app directory)python-c\"import main;import json; print(json.dumps(main.app.openapi()))\">openapi.jsonBuild and DeploySince it's a SAM project, you can build and deploy using standard commands.sam build  sam deploy--guidedLocal TestingYou can also test locally with thesam local invokecommand.samlocalinvoke--eventevents/s3_bucket_count.jsonCreating Agents for Amazon BedrockWith the Lambda and OpenAPI schema ready, you can create the agent using the management console.SummaryThank you for reading until the end. Since you can focus solely on API definition for agent development, please make use of it."}
{"title": "Security in AWS: IAM Best Practices and Advanced Techniques", "published_at": 1710895339, "tags": ["aws", "cloud", "security"], "user": "Guille Ojeda", "url": "https://dev.to/aws-builders/security-in-aws-iam-best-practices-and-advanced-techniques-25ac", "details": "AWS IAM (Identity and Access Management) is the backbone of any AWS security strategy. It's the service that controls who can access your AWS resources and what actions they can perform. Get IAM right, and you're well on your way to a secure cloud deployment. Mess it up, and you're leaving the door wide open for all sorts of security nightmares.In this article, we'll dive deep into IAM best practices and advanced techniques to help you lock down your AWS environment like a pro. We'll start with the fundamentals, then move on to more advanced topics like granular access control, cross-account access, and automating IAM with Infrastructure as Code. By the end, you'll have a solid understanding of how to use IAM to secure your AWS resources and protect your sensitive data.Understanding IAM FundamentalsBefore we jump into the best practices and advanced techniques, let's make sure we're all on the same page with the IAM basics. Understanding these foundational concepts is crucial for designing and implementing an effective IAM strategy.IAM Users, groups, and rolesAt the core of IAM are three main identity types: users, groups, and roles. IAM users represent individual people or applications that need access to your AWS resources. IAM groups are collections of IAM users, making it easier to manage permissions for multiple users at once. IAM roles are a bit different: they're not associated with a specific user, but rather are used by AWS services or external identities that need temporary access to your resources.IAM policies and permissionsIAM policies are JSON documents that define permissions for IAM identities. They specify what actions an identity can perform on which AWS resources. Policies can be attached to IAM users, groups, or roles, or even directly to AWS resources (more on that later).Resource-based policies vs. identity-based policiesThere are two main types of IAM policies: identity-based policies and resource-based policies. Identity-based policies are attached to IAM identities (users, groups, or roles) and define what actions those identities can perform on which resources. Resource-based policies, on the other hand, are attached directly to AWS resources (like S3 buckets or KMS keys) and define who can access those resources and what actions they can perform.How IAM interacts with other AWS servicesIAM is deeply integrated with other AWS services. It's used to control access to virtually every AWS resource, from EC2 instances to S3 buckets to Lambda functions. Many AWS services also have their own resource-based policies that work in conjunction with IAM policies to provide fine-grained access control.IAM Best PracticesNow that we've got the fundamentals down, let's dive into some IAM best practices that every AWS user should follow.Principle of least privilegeThe principle of least privilege is the golden rule of IAM. It means only granting users the permissions they need to perform their job duties; no more, no less. This helps minimize the blast radius if a user's credentials are compromised, and makes it easier to audit and manage permissions over time.Proper IAM user and role managementManaging IAM users and roles can get complex, especially in large organizations. Some key best practices include:Create individual IAM users for each person who needs access to AWS, rather than sharing credentialsUse IAM roles for applications and services that need access to AWS resourcesRegularly review and remove unused IAM users and rolesUsing IAM groups for better organizationIAM groups make it easier to manage permissions for multiple users at once. By creating groups for different job functions or teams, you can assign permissions at the group level rather than individually. This makes it easier to onboard new users and ensure consistent permissions across your organization.Password policies and MFA enforcementStrong password policies and multi-factor authentication (MFA) are critical for protecting your IAM users. AWS allows you to set password policies that enforce minimum length, complexity, and rotation requirements. You should also require MFA for all IAM users, especially those with administrative privileges.Regularly reviewing and rotating IAM credentialsOver time, IAM users can accumulate unnecessary permissions, and credentials can become stale or compromised. That's why it's important to regularly review IAM users and their permissions, and rotate access keys and passwords on a regular basis. AWS recommends rotating access keys every 90 days, and immediately revoking credentials for users who leave your organization.Avoiding use of root user accountThe root user account has unrestricted access to all AWS resources in your account, making it a prime target for attackers. Best practice is to avoid using the root user account for day-to-day tasks, and instead create individual IAM users with specific permissions. You should also enable MFA on the root user account and use it only for tasks that absolutely require root privileges.Implementing Granular Access ControlOne of the most powerful features of IAM is the ability to create fine-grained policies that precisely control access to your AWS resources. Here are some techniques for implementing granular access control:Creating fine-grained IAM policiesWhen creating IAM policies, it's important to be as specific as possible. Instead of granting broad permissions likes3:*, grant only the specific actions needed, likes3:GetObjectors3:PutObject. You can also restrict access to specific resources using ARNs (Amazon Resource Names), and limit permissions to specific IP ranges or VPC endpoints.Using policy conditions for more precise controlIAM policy conditions allow you to further refine permissions based on specific criteria. For example, you can use conditions to allow access only during certain time windows, from specific IP ranges, or for requests that include certain headers or parameters.Leveraging IAM policy variablesIAM policy variables allow you to create dynamic policies that adapt to your environment. For example, you can use theaws:usernamevariable to grant users access to their own home directory in an S3 bucket, or theaws:SourceIpvariable to restrict access based on the requester's IP address.Combining multiple policies for complex permissionsIn some cases, you may need to combine multiple policies to achieve the desired level of access control. For example, you might use an identity-based policy to grant broad permissions to a group of users, then use a resource-based policy to further restrict access to specific resources.Real-world examples of granular access control in AWSLet's look at a couple real-world examples of granular access control in action:Granting read-only access to an S3 bucket for a specific IAM user{\"Version\":\"2012-10-17\",\"Statement\":[{\"Sid\":\"ReadOnlyAccess\",\"Effect\":\"Allow\",\"Action\":[\"s3:GetObject\",\"s3:ListBucket\"],\"Resource\":[\"arn:aws:s3:::my-bucket\",\"arn:aws:s3:::my-bucket/*\"]}]}Enter fullscreen modeExit fullscreen modeAllowing an EC2 instance to access S3, but only from a specific VPC endpoint{\"Version\":\"2012-10-17\",\"Statement\":[{\"Sid\":\"AccessFromVPCEndpoint\",\"Effect\":\"Allow\",\"Action\":\"s3:*\",\"Resource\":\"*\",\"Condition\":{\"StringEquals\":{\"aws:sourceVpce\":\"vpce-1a2b3c4d\"}}}]}Enter fullscreen modeExit fullscreen modeCross-Account Access and IAM RolesIn many organizations, you'll need to grant access to AWS resources across multiple accounts. That's where IAM roles and cross-account access come in.Understanding cross-account accessCross-account access allows IAM users or roles in one AWS account to access resources in another account. This is useful for scenarios like granting developers access to a production account, or allowing a central security team to monitor multiple accounts.Using IAM roles for secure access delegationIAM roles are the preferred way to grant cross-account access. Instead of sharing access keys or passwords, you create an IAM role in the target account and grant permissions to the trusted entity (user or role) in the source account. The trusted entity can then assume the role and access resources in the target account.Assuming roles vs. using access keysWhen accessing resources across accounts, it's best to assume an IAM role rather than using access keys. Access keys are long-term credentials that can be easily leaked or compromised, while IAM roles provide temporary, short-lived credentials that automatically expire.Best practices for managing cross-account accessSome best practices for managing cross-account access include:Use IAM roles for cross-account access instead of sharing long-term access keysLimit the permissions granted to cross-account roles to the minimum necessaryRegularly review and audit cross-account accessUse external ID's to prevent the confused deputy problemSecuring Access to AWS ResourcesIn addition to identity-based policies, AWS also supports resource-based policies that allow you to control access to specific resources like S3 buckets, KMS keys, and Lambda functions.Using resource-based policies (e.g., S3 bucket policies)Resource-based policies are attached directly to an AWS resource and define who can access that resource and what actions they can perform. For example, an S3 bucket policy can allow read access to objects from a specific IP range, or deny all public access to the bucket.Combining resource-based and identity-based policiesResource-based policies work in conjunction with identity-based policies to provide comprehensive access control. When an IAM user or role tries to access a resource, AWS evaluates both the identity-based policies attached to the user/role and the resource-based policy attached to the resource. Access is granted only if both policies allow it.VPC endpoints and IAM policiesVPC endpoints allow you to securely access AWS services from within your VPC, without traversing the public internet. You can use IAM policies to control access to VPC endpoints, ensuring that only authorized users or roles can access the services behind the endpoint.Securing access to API Gateway and LambdaAPI Gateway and Lambda are powerful tools for building serverless applications, but they also introduce new security challenges. Best practices for securing access to these services include:Use IAM roles to grant Lambda functions access to other AWS servicesImplement OAuth or JWT authentication for APIsUse API keys and usage plans to control access to APIsEnable AWS WAF to protect against common web exploitsProtecting sensitive data with KMS and IAMAWS Key Management Service (KMS) allows you to encrypt your sensitive data using centrally managed keys. IAM policies can be used to control access to KMS keys, ensuring that only authorized users or roles can encrypt or decrypt data.Stop copying cloud solutions, startunderstandingthem. Join over 4000 devs, tech leads, and experts learning how to architect cloud solutions, not pass exams, with theSimple AWS newsletter.Centralized IAM Management with AWS OrganizationsFor organizations with multiple AWS accounts, managing IAM across all those accounts can be a challenge. That's where AWS Organizations comes in.Benefits of using AWS OrganizationsAWS Organizations allows you to centrally manage access across multiple accounts. You can create an organization, invite accounts to join, and then use Service Control Policies (SCPs) to enforce IAM policies across all accounts in the organization.Setting up an organization and creating member accountsTo get started with AWS Organizations, you create an organization and invite existing accounts to join, or create new accounts directly within the organization. You can organize accounts into Organizational Units (OUs) to apply policies hierarchically.Implementing Service Control Policies (SCPs)Service Control Policies are a powerful feature of AWS Organizations that allow you to centrally control what actions can be performed by IAM users and roles across all accounts in your organization. SCPs are similar to IAM policies, but they apply at the account level and can be used to enforce security best practices and compliance requirements.Delegating access across accounts with IAM rolesIn addition to SCPs, AWS Organizations also simplifies cross-account access using IAM roles. You can create a role in a central account and grant access to users or roles in other accounts within the organization. This allows you to centrally manage permissions while still enabling teams to access the resources they need.Best practices for AWS OrganizationsSome best practices for using AWS Organizations include:Use SCPs to enforce security best practices and compliance requirementsImplement a least privilege model, granting only the permissions necessary for each accountUse AWS CloudTrail to monitor IAM activity across all accountsRegularly review and audit IAM policies and rolesUse automation tools like AWS CloudFormation to manage IAM resources consistently across accountsMonitoring and Auditing IAM Activity with AWS CloudTrailMonitoring and auditing IAM activity is critical for detecting and responding to security incidents. AWS CloudTrail is a powerful tool for tracking IAM activity across your AWS accounts.Importance of monitoring IAM eventsBy monitoring IAM events, you can detect suspicious activity like unauthorized access attempts, changes to IAM policies, or creation of new IAM users or roles. This allows you to quickly investigate and respond to potential security breaches.Using AWS CloudTrail to track IAM actionsAWS CloudTrail logs all API calls made to IAM, including who made the call, what actions were performed, and what resources were affected. You can use CloudTrail to create a complete audit trail of IAM activity in your account.Monitoring IAM events with Amazon CloudWatchIn addition to CloudTrail, you can use Amazon CloudWatch to monitor IAM events in real-time. CloudWatch allows you to create alarms based on specific IAM events, like failed login attempts or changes to sensitive policies.Detecting and alerting on suspicious IAM activityBy combining CloudTrail and CloudWatch, you can create a comprehensive monitoring and alerting system for IAM. Some best practices include:Create alarms for high-risk events like IAM policy changes or root account usageUse CloudTrail Insights to detect unusual activity patternsIntegrate with SIEM tools like Splunk or AWS Security Hub for centralized monitoringConducting regular IAM audits and compliance checksIn addition to real-time monitoring, it's important to conduct regular IAM audits to ensure your policies and permissions are configured correctly and comply with your security and compliance requirements. Tools like AWS IAM Access Analyzer and AWS Config can help automate this process.Advanced IAM Security FeaturesThese are some more advanced features of AWS IAM, or some related services that will help you secure your AWS accounts and workloads.IAM Access AnalyzerAWS IAM Access Analyzer is a powerful tool for identifying unintended access to your AWS resources. It analyzes your IAM policies and resource-based policies to determine who has access to your resources and whether that access is intended.IAM Access Analyzer can help you identify scenarios like:Public access to S3 buckets or other resourcesAccess granted to external AWS accountsOverly permissive IAM policiesBy identifying these issues early, you can take corrective action before they lead to a security breach.IAM Permission BoundariesIAM Permission Boundaries are a way to limit the maximum permissions that can be granted to an IAM user or role. They're useful for scenarios like allowing developers to create their own IAM policies, but ensuring they can't grant themselves excessive permissions.To implement a permission boundary, you create an IAM policy that defines the maximum permissions allowed, then attach that policy as a permission boundary to an IAM user or role. Any policies attached to the user or role are evaluated within the constraints of the permission boundary.IAM Policy ConditionsIAM Policy Conditions allow you to create more fine-grained access control policies based on specific attributes of a request, like the source IP address, time of day, or presence of multi-factor authentication.Some examples of using IAM policy conditions include:Allowing access only during business hoursRequiring multi-factor authentication for sensitive actionsRestricting access to specific IP ranges or VPC endpointsIAM Identity Center for AWS SSOIAM Identity Center (formerly AWS Single Sign-On) is a centralized access management service that allows users to sign in once and access multiple AWS accounts and cloud applications.With IAM Identity Center, you can create and manage user identities in a central directory, then assign permissions to those users across multiple AWS accounts. Users sign in once to the IAM Identity Center portal, then access their assigned accounts and applications without needing to manage separate credentials.Integrating IAM Identity Center with third-party identity providersIAM Identity Center also allows you to integrate with third-party identity providers like Azure AD, Okta, or Ping Identity. This allows you to use your existing identity management system to control access to AWS, without needing to recreate user identities in IAM.Automating IAM with Infrastructure as Code ToolsAs your AWS environment grows, managing IAM policies and roles manually becomes increasingly difficult. That's where Infrastructure as Code (IaC) tools like AWS CloudFormation, Terraform, and the AWS CDK come in.Benefits of using Infrastructure as Code (IaC) for IAMBy defining your IAM resources as code, you can:Version control your IAM policies and rolesAutomate the creation and updates of IAM resourcesEnsure consistency across multiple AWS accounts and regionsEasily roll back changes if neededUsing AWS CloudFormation to manage IAM resourcesAWS CloudFormation is a native AWS service that allows you to define your infrastructure as code using JSON or YAML templates. You can use CloudFormation to create and manage IAM users, groups, roles, and policies across multiple accounts and regions.Terraform and AWS CDK for IAM automationTerraform and the AWS Cloud Development Kit (CDK) are popular third-party IaC tools that support IAM resource management. Terraform uses a declarative language called HCL (HashiCorp Configuration Language) to define infrastructure resources, while the AWS CDK allows you to define infrastructure using familiar programming languages like JavaScript, TypeScript, Python, or Java.Best practices for IAM automation and version controlWhen automating IAM with IaC tools, it's important to follow best practices like:Storing your IaC templates in a version control system like GitUsing separate AWS accounts for development, staging, and production environmentsImplementing a code review process for IAM changesUsing tools like AWS CloudTrail and AWS Config to monitor and audit IAM changesBy treating your IAM resources as code and following these best practices, you can ensure consistency, maintainability, and auditability of your IAM configuration.ConclusionIAM is a critical component of securing your AWS environment, but it can be really complex and challenging to manage at scale. By following best practices like theprinciple of least privilege,using IAM roles for cross-account access, and implementingstrong password policies and MFA, you can lay a solid foundation for your IAM strategy.But to truly secure your accounts and environments, you need to go beyond the basics. Techniques likegranular access control with policy conditions,resource-based policies, andpermission boundariesallow you to implement fine-grained security policies that precisely control access to your resources.Centralized management with AWS Organizationsandmonitoring with CloudTrail and CloudWatchprovide visibility and actionable data across your entire AWS environment.As your AWS usage grows,automating IAM with Infrastructure as Codetools like CloudFormation, Terraform, and the AWS CDK becomes increasingly important. By defining your IAM resources as code and following best practices for version control and testing, you can ensure consistency and maintainability of your IAM configuration.Securing your AWS environment is an ongoing process, not a one-time task. As you adopt new AWS services and your application requirements evolve, it's important to continually review and update your IAM policies to ensure they align with your security goals. Regular audits and compliance checks, along with automated monitoring and alerting, can help you stay on top of your IAM configuration and quickly detect and respond to potential issues.By following the best practices and techniques outlined in this article, you can build a robust and secure IAM strategy that helps you protect your critical AWS resources and data. But don't stop here! Continue to explore and adopt new security services and features like AWS GuardDuty, AWS Security Hub, and AWS Secrets Manager to further strengthen your security posture.Remember, security is a shared responsibility between AWS and you, the customer. By taking a proactive and layered approach to IAM and security, you can ensure that your AWS environment is protected against evolving threats and ready to support your business needs for years to come.Stop copying cloud solutions, startunderstandingthem. Join over 4000 devs, tech leads, and experts learning how to architect cloud solutions, not pass exams, with theSimple AWS newsletter.Realscenarios and solutionsThewhybehind the solutionsBest practicesto improve themSubscribe for freeIf you'd like to know more about me, you can find meon LinkedInor atwww.guilleojeda.com"}
{"title": "Terraform - Workspaces & Demos", "published_at": 1710890804, "tags": ["terraform", "workspace", "iac"], "user": "Srinivasulu Paranduru", "url": "https://dev.to/aws-builders/terraform-workspaces-laf", "details": "Workspacesin the Terraform CLI refer to separate instances of state data inside the same Terraform working directory.Terraform relies on state to associate resources with real-world objects.When you run the same configuration multiple times with separate state data, Terraform can manage multiple sets of non-overlapping resources.Under a folder TF_AWS having below mentioned .tf filesTF_AWS  (folder)data.tfmain.tfprovider.tfvariable.tfterraform.tfvars1.Create a folder TF_AWS2.Create a new file main.tf under  the folder TF_AWS and copy the below mentioned coderesource \"aws_instance\" \"myfirstec2\" {   ami           =  data.aws_ami.app_ami.id    instance_type =  \"t2.micro\" }Enter fullscreen modeExit fullscreen mode3.Create a new file provider.tf under  the folder TF_AWS and copy the below mentioned codeprovider \"aws\" {   region     = \"eu-west-1\" }Enter fullscreen modeExit fullscreen mode4.Create a new file data.tf under  the folder TF_AWS and copy the  below mentioned codedata \"aws_ami\" \"app_ami\" {    most_recent =true    owners = [\"amazon\"]     filter{      name = \"name\"      values = [\"amzn2-ami-hvm*\"]    } }Enter fullscreen modeExit fullscreen mode5.Create a new file variable.tf under  the folder TF_AWS and copy the below mentioned codevariable \"instance_type\" {   type = string   description = \"instance type\" }Enter fullscreen modeExit fullscreen mode6.Create a file terraform.tfvars under the folder TF_AWS and copy the below mentioned codeinstance_type = \"t2.micro\"Enter fullscreen modeExit fullscreen modeRun the terraform commandterraform workspacethen will get the following messageUsage: terraform workspacenew, list, show, select and delete Terraform workspaces.terraform workspace list*defaultNote :Initially with out creating any workspaces, default is selected and its highlighted with * symbolCreating a new workspacedevby the running the below commandterraform workspace new devNewly created workspace will be selected by defaultWhen a workspace is created initially then following folders will be created under the base folderterraform.tfstate.ddevSimilarly create new  test,prod workspacestest and prod folders will be created under terraform.tfstate.d folderterraform.tfstate.ddevtestprodNote :If any workspaces related any environment deleted using  the below command, then the following environment folder will be deleted under the folder terraform.tfstate.dCant delete the workspace selected, if still wanted to deleted the current workspace then switch to other workspace and delete the sameterraform workspace delete prodEnter fullscreen modeExit fullscreen modeSelect the dev workspaceterraform workspace select devThen the following the terraform commandsterraform initterraform planterraform approve --auto-approveSeparate terraform.tfstate file will be created under the dev folderterraform.tfstate.d\\dev\\terraform.tfstateSelect the dev workspaceterraform workspace select testThen the following the terraform commandsterraform initterraform planterraform approve --auto-approve`PS C:\\Terraform_Training\\TF_AWS> terraform workspace select testSwitched to workspace \"test\".PS C:\\Terraform_Training\\TF_AWS> terraform apply --auto-approvePS C:\\Terraform_Training\\TF_AWS> terraform apply --auto-approveaws_instance.myfirstec2: Creating...aws_instance.myfirstec2: Still creating... [10s elapsed]aws_instance.myfirstec2: Still creating... [20s elapsed]aws_instance.myfirstec2: Still creating... [30s elapsed]aws_instance.myfirstec2: Creation complete after 31s [id=i-0274011109b4a4e04]`Separate terraform.tfstate file will be created under the test folder  `terraform.tfstate.d\\test\\terraform.tfstateIf you are trying to delete the current selected workspace 'test' ` PS C:\\Terraform_Training\\TF_AWS> terraform workspace delete test Workspace \"test\" is your active workspace.You cannot delete the currently active workspace. Please switchto another workspace and try again.`Switch to workspace 'dev'terraform workspace select devNow try to delete 'test' workspaceterraform workspace select testPS C:\\Terraform_Training\\TF_AWS> terraform workspace delete testWorkspace \"test\" is not empty.Deleting \"test\" can result in dangling resources: resources thatexist but are no longer manageable by Terraform. Please destroythese resources first.  If you want to delete this workspaceanyway and risk dangling resources, use the '-force' flag.Note :Cant able to delete a workspace having resources and best way is to destroy the resources for the selected workspace then destroy the workspaceIf you try with the below option for the 'test' workspace and for which resources existsterraform workspace delete -force testDeleted workspace \"test\"!WARNING: \"test\" was non-empty.The resources managed by the deleted workspace may still exist,but are no longer manageable by Terraform since the state hasbeen deleted.It has deleted the 'test' workspace but the resources still existing and those resources needs to be deleted manually through aws consoleNote: Destroy the resources under dev and prod if any existing then delete the workspacesPS C:\\Terraform_Training\\TF_AWS> terraform workspace listdefaultdev prodPS C:\\Terraform_Training\\TF_AWS> terraform destroy --auto-approveaws_instance.myfirstec2: Destroying... [id=i-0546cf22debe05002]aws_instance.myfirstec2: Still destroying... [id=i-0546cf22debe05002, 10s elapsed]aws_instance.myfirstec2: Still destroying... [id=i-0546cf22debe05002, 20s elapsed]aws_instance.myfirstec2: Still destroying... [id=i-0546cf22debe05002, 30s elapsed]aws_instance.myfirstec2: Still destroying... [id=i-0546cf22debe05002, 40s elapsed]aws_instance.myfirstec2: Destruction complete after 40sPS C:\\Terraform_Training\\TF_AWS> terraform workspace select prodSwitched to workspace \"prod\".PS C:\\Terraform_Training\\TF_AWS> terraform workspace delete devDeleted workspace \"dev\"!PS C:\\Terraform_Training\\TF_AWS> terraform workspace select defaultSwitched to workspace \"default\".PS C:\\Terraform_Training\\TF_AWS> terraform workspace delete prodDeleted workspace \"prod\"!PS C:\\Terraform_Training\\TF_AWS> terraform workspace listdefaultNow all the workspaces created by me are destroyedRecheck the folders inside terraform.tfstate.d are deletedCommandDescriptionUsuageworkspace listThe terraform workspace list command is used to list all existing workspaces.terraform workspace list [DIR]workspace selectThe terraform workspace select command is used to choose a different workspace to use for further operations.terraform workspace select NAME [DIR]workspace newThe terraform workspace new command is used to create a new workspace.terraform workspace new [OPTIONS] NAME [DIR]workspace deleteThe terraform workspace delete command is used to delete an existing workspace.terraform workspace delete [OPTIONS] NAME [DIR]workspace showThe terraform workspace show command is used to output the current workspace.terraform workspace showConclusion : Discussed about terraform workspace and created multiple workspaces and deleted\ud83d\udcac If you enjoyed reading this blog post and found it informative, please take a moment to share your thoughts by leaving a review and liking it \ud83d\ude00 and follow me indev.to,linkedinandbuy me a coffee"}
{"title": "How to Cloud: Containerization", "published_at": 1710871870, "tags": ["aws", "devops", "docker", "containers"], "user": "Joseph", "url": "https://dev.to/aws-builders/how-to-cloud-containerization-1k56", "details": "I have kind of a standard approach on building out a cloud presence that I\u2019ve developed over the last decade or so. I\u2019ve used it extensively with multiple teams to create a highly mature DevOps culture within groups again and again. Today I\u2019m beginning a series that explains some of the basics. Hopefully it can be of some use to those trying to understand good ways to leverage cloud technologies.Any Cloud Will WorkI\u2019m going to start this series off with examples of how to deploy the concepts I\u2019m talking about in AWS, but the approach I use works with a lot of cloud solutions out there, including Azure and GCP. I will do my best to call out how the approach would work in the Big 3 where it\u2019s appropriate, but the topic here is not about showing you a detailed solution of a tech stack, but rather the strategy behind the build out and how to make the best decisions when it comes to rolling things out. And speaking of decisions, when it comes to adopting a cloud approach, the very first thing your teams need to understand is containerization. The need to learn Docker.Learn DockerDocker is the gateway to understanding what containerization is all about. Not only is it a containerization solution itself, but the concepts your team learns from understanding it will help them navigate the massive forest of different compute options they will have across any cloud vendor. Without starting here you will never be able to see the forest through all the trees being thrown at you. Options range from AWS ECS, to Fargate, or EKS, or Lambdas, and that\u2019s just one cloud vendor, so trust me and start here. I\u2019m not going to get into the specifics of all things Docker, but I do want to highlight some of the learnings I\u2019ve made over the years.Organize your imagesWhen you are just beginning to learn what containers are this won\u2019t make a lot of sense, but after you have your first image working and you actually have a system working inside a container, you should stop and think about organizing your image builds. What this means is not only keeping them logically organized so your team can understand how to use them efficiently, but also layering them together so you can reduce your build times significantly where you have a lot of commonality across systems. Let me show you a simple example.The Starting PointOnce the team knows how to build docker images and can reliably run them, I start introducing a pattern that allows them to create layered images that use each other instead of creating separate duplicate builds that make your CI/CD process take forever to finish. This begins by identifying your base starting point and branching off from there. Let\u2019s say your team needs to build a web app and a background service, but they plan to use nodejs for both. Start by making a docker compose yml that will simply define the common base both of them will use. For example:docker-compose.ymlversion:\"2\"services:how-to-cloud-base:build:.image:how-to-cloud:baseEnter fullscreen modeExit fullscreen modeDockerfileFROMnode:alpineEnter fullscreen modeExit fullscreen modeThe base image right now is mind numbingly simple. The docker compose yml just builds the Dockerfile below and then tags the image with a base tag. The Dockerfile doesn\u2019t really do anything other than grab the alpine distro of node. That\u2019s how it starts. Later your team will realize there are commonalities that both systems share and they can \u201cbubble up\u201d those things into the base image when they realize it. Now that you have the base the team can use it to create the web app and the background service.Let\u2019s build out the web version first\u2026docker-compose.web.ymlversion:\"2\"services:how-to-cloud-web:build:context:../dockerfile:./docker/Dockerfile.webimage:how-to-cloud-web:latestEnter fullscreen modeExit fullscreen modeDockerfile.webFROMhow-to-cloud:baseCOPY./web /appWORKDIR/appCMDnpm startEnter fullscreen modeExit fullscreen modeand now the background service, which for now will look very similar to the web\u2026docker-compose.background.ymlversion:\"2\"services:how-to-cloud-background:build:context:../dockerfile:./docker/Dockerfile.backgroundimage:how-to-cloud-background:latestEnter fullscreen modeExit fullscreen modeDockerfile.backgroundFROMhow-to-cloud:baseCOPY./background /appWORKDIR/appCMDnpm startEnter fullscreen modeExit fullscreen modeClarifying PointsOne thing that\u2019s worth pointing out here is that the web app and the background service function completely independently. Meaning one system does not in any way depend on the other from a deployment perspective. If they did you there are other ways to deal with that.Another call out I want to make is that when I use this organizational method I really like to make an LTS build that has kind of a default set of what my team normally uses in the cases where they just wanna use \u201cthe normal\u201d build and not have to have something specialized. It looks something like this.docker-compose.lts.ymlversion:\"2\"services:how-to-cloud-lts:build:context:../dockerfile:./docker/Dockerfile.ltsimage:how-to-cloud:ltsEnter fullscreen modeExit fullscreen modeDockerfile.ltsFROMhow-to-cloud:baseCOPY./lts /appWORKDIR/appCMDnpm startEnter fullscreen modeExit fullscreen modeDeploymentSo with all this set up we can talk about how this all ties together in a CI/CD pipeline and why you can get some pretty significant performance gains for your build when you follow this approach. The process is separated into three main stages: build, deploy, clean. It looks something like this.Build#!/bin/bashset-e# build imagedocker compose build docker compose-fdocker-compose.web.yml build docker compose-fdocker-compose.background.yml build docker compose-fdocker-compose.lts.yml buildEnter fullscreen modeExit fullscreen modeThe important thing here is that you build the base docker image first, and then build the images that depend on that base image. You can get much more elaborate with the layering of the images if you want to, but the main point here is that the more commonalities that you bring up into higher layers the faster you\u2019ll make your build, and the easier it will be to maintain your systems across all your teams because they\u2019ll be sharing and collaborating together instead of duplicating the same concepts in silos.Deploy#!/bin/bashset-e# login to ECRversion=$(aws--version|awk-F'[/.]''{print $2}')if[$version-eq\"1\"];thenlogin=$(aws ecr get-login--no-include-email)&&eval\"$login\"elseaws ecr get-login-password | docker login--usernameAWS--password-stdin$ecrfi# push image to ECR repodocker compose-fdocker-compose.lts.yml push docker compose-fdocker-compose.web.yml push docker compose-fdocker-compose.background.yml pushEnter fullscreen modeExit fullscreen modeAfter the images are built you can deploy them to your container registry. In AWS it looks basically like this. The AWS CLI has two versions, and both of them are pretty widely used from what I\u2019ve seen, so I still have a check for which version the team has installed so it uses the right command, but if you know your team is going to only use version 2, you can clean that up a little bit and only use the second command to log into the ECR.Aside from that once you have logged into your ECR it\u2019s a simple matter of pushing all the images up. You don\u2019t need to publish the base image because you should be building that every time your CI/CD pipeline runs, unless you have a really good reason to need some kind of available redundancy for it. From my experience that is usually over kill.Also, in the example above we\u2019re deploying to AWS ECR, but every cloud vendor has their own container registry. The point here is how you design the process, not the specifics of how you get the image into the registry. The process works for most if not all cloud vendors.CleanAt the end of your build process you might need to do some cleaning up, so we have a third stage to handle that, but a lot of the time it\u2019s empty because the CI/CD process is ephemeral anyway. However, if you are using external dependencies during your build and you need to clean them up after the fact, then you can do that here, or if your CI/CD process is not ephemeral then you\u2019ll probably want to do some clean up before you call the build done.In this set up our clean up process is empty because there\u2019s nothing really to clean, but I\u2019ve used this in the past under certain situations. For example, we\u2019ve built github self hosted runner farms that required us to pre-emptively download and extract binaries because the docker build was actually failing to sign certificates, so the host had binaries we needed to clean up after we were finished with the build.Show me the CodeI went ahead and made a repository to showcase all the lessons I talked about here, as well as any future topics surrounding How to Cloud. If you want to see the full set up I described here head over tohttps://github.com/josephbulger/how-to-cloud"}
{"title": "DevOps with Guruu | Chapter 7 : Connect 2 VPC without internet \u2013 Security | Network Service with AWS", "published_at": 1710868319, "tags": ["webdev", "devops", "aws"], "user": "Hoang Guruu", "url": "https://dev.to/aws-builders/devops-with-guruu-chapter-7-connect-2-vpc-without-internet-security-network-service-with-aws-33g2", "details": "DevOps with Guruu | Chapter 7 : Connect 2 VPC without internet \u2013 Security | Network Service with AWS0:00 Welcome to my Series DevOps with Guruu1:43 My mistake21:31 Initialize CloudFormation Template Again24:00 Create Security Group26:30 Create EC2 instance29:30 Update Network ACL35:00 Create connect Peering37:00 Config Route tables38:00 Delete ResourcesJoin me on this journey to mastering DevOps. Let's get hands-on and build a strong foundation together! Don't forget to like, share, and subscribe for more exciting content. Happy coding, future DevOps!\""}
{"title": "Fixing Issues in AWS VPC with AI Help", "published_at": 1710867331, "tags": ["vpc", "ai", "networking", "aws"], "user": "Andrii Melashchenko", "url": "https://dev.to/aws-builders/fixing-issues-in-aws-vpc-with-ai-help-5c8p", "details": "IntroductionMy story begins with AWS Advance Networking Certification. I will start learning all possible network configurations and buy a notebook to note all VPC, Subnets, Route Tables, ENI, and other entity IDes to debug misconfigurations. I have an idea: All this information is available via AWS CLI. Maybe Generative AI can help me with this?!So, I built a simple react AI Agent to help with some simple stuff. Maybe this article will inspire you ;)ResultIf you, my honoured reader, want to understand whether this article is for you, here is the result of the small eight-operator code.Question: I cannot access web server at ec2 instance ID i-xxxxxx.Answer: Initially, you were unable to access the web server on the EC2 instance i-xxxxxx because the associated security group did not have an inbound rule permitting web traffic. Specifically, it lacked rules allowing HTTP (port 80) or HTTPS (port 443) inbound connections.!!!To resolve this issue, an inbound rule has been added to the security group (sg-xxxxxx) to permit traffic on port 80 from any IP address (0.0.0.0/0).!!!!With this change, you should now be able to access the web server using the public IP address xxx.xxx.xxx.xxx.However, if you continue experiencing difficulties accessing the web server after adding the security group rule, you may need to investigate further. Potential areas to review include the web server configuration on the EC2 instance, ensuring the web server software is running, and verifying that the instance is located in a public subnet with an Internet Gateway attached to the VPC's route table.PrerequisitesYou should be comfortable with AWS Console, Cloudformation, VPCYou should configure access to theAWS Bedrock Calude 3 Sonnet modelYou should haveProject Jupyter | HomeinstalledAlso, I'm usingaws sso loginto secure access toAWS EnviromentDisclaimer!!!!!!!!!!!!!!!All the words in these sections are big WARNING that the code is using bash shell and AWS credentials to do its \"magic\". LLM access to shell on its own is high risk ;)So please think about questions and experiments!You were warned.!!!!!!!!!!!!!!!The code provided in this article is offered as-is, without any warranties or guarantees of any kind.The author does not assume any responsibility for any errors, omissions, or damages that may arise from the use or misuse of the code. The user assumes all risks associated with the implementation and usage of the code.The author shall not be held liable for any direct, indirect, incidental, special, or consequential damages that may result from the use or inability to use the code. This includes, but is not limited to, loss of data, loss of profits, business interruption, or any other commercial damages or losses.The code is provided for educational and informational purposes only and should not be used in production environments without proper testing, validation, and security considerations.The user is solely responsible for ensuring the code's suitability, security, and compliance with applicable laws and regulations.The author does not endorse or recommend any specific use of the code and does not guarantee its compatibility with any particular software, hardware, or operating system. The user assumes full responsibility for any modifications made to the code and any consequences that may arise from such modifications.By using the code, the user acknowledges and agrees to these terms and conditions. If the user does not agree with these terms, they should refrain from using or implementing the code.!!!!!!!!!!!!!!!ProblemOne of the common challenges AWS newcomers face is misconfigured security group rules, which can lead to denied access to resources like virtual machines (VMs) or web servers running on Elastic Compute Cloud (EC2) instances. This issue often arises due to the \"deny all\" principle by default in AWS, where no inbound or outbound traffic is allowed unless explicitly permitted through security group rules.In this use case, we created a virtual cloud network (VPC) with a misconfigured firewall (security group) that prevented access to a web server running on an EC2 instance on port 80 (HTTP). This scenario is a typical newbie mistake and highlights the importance of properly configuring security group rules to allow intended traffic.To address this issue, we leveraged the power of an AI assistant specifically designed for VPC troubleshooting. This AI agent was equipped with access to a bash shell and the AWS Command Line Interface (CLI) toolkit, enabling it to interact with AWS resources and perform necessary actions.The AI assistant first analyzed the current configuration of the VPC, including the security groups associated with the EC2 instance hosting the web server. By identifying the missing inbound rule for HTTP traffic on port 80, the AI agent was able to add the required rule to the appropriate security group, allowing inbound traffic from any IP address (0.0.0.0/0) on port 80.Once the security group rule was updated, the AI assistant verified that the web server could be accessed using the public IP address of the EC2 instance.If access was still denied, the AI agent had the capability to further investigate potential issues, such as the web server configuration on the instance, ensuring the web server software was running, and verifying that the instance was in a public subnet with an Internet Gateway attached to the VPC's route table.This use case highlights the power of an AI assistant specifically designed for VPC troubleshooting. By leveraging its knowledge and capabilities, the AI agent was able to quickly identify and resolve a common misconfiguration issue, saving valuable time and effort for AWS users, especially those new to the platform.ImplementationCloudFormation template to simulate VPC misconfiguration:AWSTemplateFormatVersion:'2010-09-09'Description:VPC with EC2 Instance and Security GroupResources:VPC:Type:AWS::EC2::VPCProperties:CidrBlock:10.0.0.0/16EnableDnsHostnames:trueEnableDnsSupport:trueInstanceTenancy:defaultTags:-Key:NameValue:MyVPCInternetGateway:Type:AWS::EC2::InternetGatewayVPCGatewayAttachment:Type:AWS::EC2::VPCGatewayAttachmentProperties:VpcId:!RefVPCInternetGatewayId:!RefInternetGatewayPublicSubnet:Type:AWS::EC2::SubnetProperties:VpcId:!RefVPCCidrBlock:10.0.0.0/24MapPublicIpOnLaunch:trueAvailabilityZone:!Select-0-!GetAZs''Tags:-Key:NameValue:Public SubnetRouteTable:Type:AWS::EC2::RouteTableProperties:VpcId:!RefVPCTags:-Key:NameValue:Public Route TableRoute:Type:AWS::EC2::RouteDependsOn:VPCGatewayAttachmentProperties:RouteTableId:!RefRouteTableDestinationCidrBlock:0.0.0.0/0GatewayId:!RefInternetGatewaySubnetRouteTableAssociation:Type:AWS::EC2::SubnetRouteTableAssociationProperties:SubnetId:!RefPublicSubnetRouteTableId:!RefRouteTableSecurityGroup:Type:AWS::EC2::SecurityGroupProperties:GroupDescription:Allow SSHSecurityGroupIngress:-IpProtocol:tcpFromPort:22ToPort:22CidrIp:0.0.0.0/0VpcId:!RefVPCTags:-Key:NameValue:SSH AccessEC2Instance:Type:AWS::EC2::InstanceProperties:ImageId:ami-0cff7528ff583bf9aInstanceType:t2.microKeyName:test-vpcNetworkInterfaces:-AssociatePublicIpAddress:'true'DeviceIndex:'0'GroupSet:-!RefSecurityGroupSubnetId:!RefPublicSubnetUserData:!Base64|#!/bin/bashyum update -yyum install -y httpdsystemctl start httpdsystemctl enable httpdTags:-Key:NameValue:DummyHTTPServerOutputs:VPC:Description:A reference to the created VPCValue:!RefVPCExport:Name:!Sub${AWS::StackName}-VPCIDPublicSubnet:Description:A reference to the public SubnetValue:!RefPublicSubnetExport:Name:!Sub${AWS::StackName}-SubnetIDEC2Instance:Description:A reference to the EC2 instanceValue:!RefEC2InstanceExport:Name:!Sub${AWS::StackName}-EC2InstanceIDEnter fullscreen modeExit fullscreen modeThis CloudFormation template creates the following resources:A VPC with DNS settings and a custom tag.An Internet Gateway is attached to the VPC.A public subnet with public IP auto-assignment enabled.A route table associated with the public subnet, including a route to the Internet Gateway.A security group allowing SSH access from any IP address.An EC2 instance in the public subnet, running an Apache HTTP server.AI AgentHere is a code for AI Agent build using theLangChainframework.# Notebook to build AI Agent to fix VPC issues# Access to Claude-3 Sonnet LLM using aws sso login to get credentialsfromlangchain_community.chat_modelsimportBedrockChatimportboto3boto3_bedrock=boto3.client(\"bedrock-runtime\",'us-east-1')model=BedrockChat(model_id=\"anthropic.claude-3-sonnet-20240229-v1:0\",model_kwargs={\"temperature\":0,\"top_k\":250,\"top_p\":1,},client=boto3_bedrock,)# configure react agentfromlangchain.agentsimportAgentExecutor,create_react_agentfromlangchain_community.toolsimportShellToolfromlangchain_core.promptsimportPromptTemplate# Initialize the language modelllm=model# Initialize the tools # !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!! # !!!!!!!!!!!!!!!!!!!!!!!!!!!!!! WARNING RAW SHELL EXPOSED !!!!!!!!!!!!!!!!!!!!!!!tools=[ShellTool()]# !!!!!!!!!!!!!!!!!!!!!!!!!!!!!! WARNING RAW SHELL EXPOSED !!!!!!!!!!!!!!!!!!!!!!! # !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!# Initialize prompttemplate=\"\"\"You are an AWS VPC troubleshooting assistant, and your task is to identify and resolve any misconfigurations in an AWS Virtual Private Cloud (VPC) and its associated resources.  You have access to the bash shell, which allows you to run AWS CLI commands to retrieve information about AWS resources, make changes, and perform various operations. List of tools: {tools} {tool_names}  First, gather information about the existing VPC setup by running the following AWS CLI commands:  1. `aws ec2 describe-vpcs`: This command will list all available VPCs in the current region, along with their CIDR blocks, VPC IDs, and other details. Review the output to ensure the VPC you want to troubleshoot exists and has the correct CIDR block.  2. `aws ec2 describe-subnets --filters Name=vpc-id,Values=<vpc-id>`: Replace `<vpc-id>` with the ID of the VPC you want to troubleshoot. This command will list all subnets associated with the specified VPC, their Availability Zones, and CIDR blocks. Verify that the subnets have the correct CIDR blocks and are associated with the intended Availability Zones.  3. `aws ec2 describe-route-tables --filters Name=vpc-id,Values=<vpc-id>`: This command will list all route tables associated with the specified VPC. Review the routes in each route table to ensure they are configured correctly and match your expectations. For example, check if there are routes to an Internet Gateway for public subnets, and routes to a NAT Gateway or VPN for private subnets.  4. `aws ec2 describe-security-groups --filters Name=vpc-id,Values=<vpc-id>`: This command will list all security groups associated with the specified VPC. Review the inbound and outbound rules for each security group to ensure they are configured correctly and allow or deny the intended traffic.  5. `aws ec2 describe-network-acls --filters Name=vpc-id,Values=<vpc-id>`: This command will list all network ACLs associated with the specified VPC. Review the inbound and outbound rules for each network ACL to ensure they are configured correctly and allow or deny the intended traffic.  After gathering this information, analyze the output and identify any potential misconfigurations or deviations from the expected setup. If you find any issues, provide step-by-step instructions on how to fix them using the appropriate AWS CLI commands.  Remember to double-check your commands before executing them, as some commands may make changes to the AWS environment.  Use the following format:  Question: the input question you must answer Thought: you should always think about what to do Action: the action to take, should be one of [{tool_names}] Action Input: the input to the action Observation: the result of the action...(thisThought/Action/ActionInput/ObservationcanrepeatNtimes)Thought:InowknowthefinalanswerFinalAnswer:thefinalanswertotheoriginalinputquestionBegin!  Question: {input} Thought:{agent_scratchpad}\"\"\"prompt=PromptTemplate.from_template(template)# Initialize the agentagent=create_react_agent(llm,tools,prompt)# Run the agentagent_executor=AgentExecutor(agent=agent,tools=tools,verbose=True,handle_parsing_errors=True)agent_executor.invoke({\"input\":\"I cannot access web server at ec2 instance ID i-xxxxxx\"})Enter fullscreen modeExit fullscreen modeHere's a section about the AI Agent based on the provided code:The provided code demonstrates how to build an AI Agent for troubleshooting AWS Virtual Private Cloud (VPC) issues using the AWS Bedrock runtime and the Claude-3 Sonnet language model from Anthropic. This agent leverages the power of large language models (LLMs) and the AWS Command Line Interface (CLI) to identify and resolve misconfigurations in VPC setups.Setting up the EnvironmentThe code begins by importing the necessary libraries and establishing a connection to the AWS Bedrock runtime using theboto3library. TheBedrockChatmodel from thelangchain_communitypackage is used to interface with the Claude-3 Sonnet language model.fromlangchain_community.chat_modelsimportBedrockChatimportboto3boto3_bedrock=boto3.client(\"bedrock-runtime\",'us-east-1')model=BedrockChat(model_id=\"anthropic.claude-3-sonnet-20240229-v1:0\",model_kwargs={\"temperature\":0,\"top_k\":250,\"top_p\":1,},client=boto3_bedrock,)Enter fullscreen modeExit fullscreen modeConfiguring the AgentThe code then configures the AI agent using thecreate_react_agentfunction from thelangchain.agentsmodule. The agent is equipped with aShellToolfrom thelangchain_community.toolsmodule, which allows it to execute shell commands, including AWS CLI commands.fromlangchain.agentsimportAgentExecutor,create_react_agentfromlangchain_community.toolsimportShellToolfromlangchain_core.promptsimportPromptTemplatellm=modeltools=[ShellTool()]Enter fullscreen modeExit fullscreen modeDefining the PromptA prompt template guides the agent's behaviour and provides instructions and context for the troubleshooting task. The template includes a list of AWS CLI commands that the agent can execute to gather information about the VPC setup, such as describing VPCs, subnets, route tables, security groups, and network ACLs.template=\"\"\"# ... (prompt template omitted for brevity)\"\"\"prompt=PromptTemplate.from_template(template)Enter fullscreen modeExit fullscreen modeRunning the AgentFinally, the code initializes the agent using thecreate_react_agentfunction and runs it using theAgentExecutor. The agent can be invoked with a specific input question or problem statement related to VPC troubleshooting.agent=create_react_agent(llm,tools,prompt)agent_executor=AgentExecutor(agent=agent,tools=tools,verbose=True,handle_parsing_errors=True)agent_executor.invoke({\"input\":\"I cannot access web server at ec2 instance ID i-xxxxxx\"})Enter fullscreen modeExit fullscreen modeWhen invoked, the agent will follow the instructions in the prompt, gathering information about the VPC setup using the AWS CLI commands. It will analyze the output, identify potential misconfigurations, and provide step-by-step instructions on how to fix any issues using the appropriate AWS CLI commands.This AI Agent can be a valuable tool for AWS administrators and developers who are responsible for managing and troubleshooting VPC setups. By leveraging the power of large language models and the AWS CLI, the agent can quickly identify and resolve complex VPC issues, streamlining the troubleshooting process and improving overall operational efficiency.The result is a new oneThis time I removed route to Internet Gateway in public subnet.Question: I cannot access web server at ec2 instance ID i-xxxxxx.Answer: To allow internet access to the web server on the EC2 instance (i-xxxxxx), follow these steps:Create an Internet Gateway:aws ec2 create-internet-gatewayAttach the Internet Gateway to the VPC:aws ec2 attach-internet-gateway --vpc-id vpc-03xxxxx --internet-gateway-id <internet-gateway-id>Replace<internet-gateway-id>with the ID of the Internet Gateway created in step 1.Find the route table associated with the public subnet (subnet-03xxxxxx):aws ec2 describe-route-tables --filters Name=association.subnet-id,Values=subnet-03xxxxxx --query 'RouteTables[*].RouteTableId' --output textCreate a route in the route table to the Internet Gateway:aws ec2 create-route --route-table-id <route-table-id> --destination-cidr-block 0.0.0.0/0 --gateway-id <internet-gateway-id>Replace<route-table-id>with the ID of the route table associated with the public subnet, and<internet-gateway-id>with the ID of the Internet Gateway created in step 1.After completing these steps, instances launched in the public subnet (subnet-03xxxxxx) should be able to access the internet and be accessible from the internet. You should now be able to access the web server on the EC2 instance (i-xxxx) using its public IP address (xxx.xxx.xxx.xxx).ConclusionIn conclusion, leveraging Generative AI and AWS CLI to troubleshoot AWS VPC issues offers a groundbreaking approach to network management. This methodology streamlines the process of identifying and resolving misconfigurations and significantly reduces the time and effort required for these tasks. By employing a simple React AI Agent integrated with the Claude-3 Sonnet model, we demonstrated a practical application of AI in automating the process of fixing common security group misconfigurations, thereby enhancing access to EC2 instances. This exploration underscores the potential of AI in simplifying cloud networking challenges, making it an invaluable tool for AWS users seeking to optimize their networking configurations efficiently."}
{"title": "Serverless statistics solution with Lambda@Edge", "published_at": 1710842862, "tags": ["aws", "serverless"], "user": "Jimmy Dahlqvist", "url": "https://dev.to/aws-builders/serverless-statistics-solution-with-lambdaedge-41j6", "details": "For a very long time I have been using Google Analytics to understand how many readers I have on my blogs. However, Google Analytics is super advanced and can do so much more than I need, you need to do proper data engineering on the data to get the full picture. I started to look into Open Source alternatives that I could host my self. Every solution use some form of client side setup for sending data, so I started thinking if I could be doing this as a server-side solution.My blog is static HTML served from S3 and CloudFront, could Lambda@Edge be used in some way? Could I send the data I needed to a central location?This blog explores the setup I created to create a serverless statistics service using Lambda@Edge, StepFunctions, Glue, Athena, Managed Grafana and a couple of more services.Serverless HandbookThe solution in this post is available as a fully deployable solution onMy Serverless HandbookArchitecture OverviewThe entire architecture setup involve several serverless and managed services from AWS and creates a great foundation for analytics service. The solution ingestion and analytics parts are decoupled using EventBridge, which creates an fantastic opportunity for extending it. It's based on ingestion of data using Lambda@Edge and StepFunctions. Data store and analytics with FireHose, Glue, and Athena. Then an optional visualization using Managed Grafana.CloudFront setupSince I need to call other AWS services, I needed to use Lambda@Edge over CloudFront Functions. I wanted data for all page-views to be ingested, so I thought I would use the Viewer-Response integration point.Viewer-Response would run just before the page was sent to the viewer, which would be an great integration point. However, I needed to rethink this. I use a CloudFront function in response to the Viewer-request, I use this function to re-write parts of the URL to ensure content can be loaded properly. Since I use a CloudFront function for Viewer-Request you can't use a Lambda@Edge for Viewer-Response, that combination is not supported.Instead I opted for using the Origin-Request integration point, this will run after the CloudFront cache, jsu before the origin is called. I use a no-cache policy in my CloudFront distribution therefor this integration point will always run.I wanted to understand what page was viewed, time of day, what country the reader is from, and what type of device the reader is using (mobile, desktop, tablet). Page is available by default in the uri field of the event. Example event{\"Records\":[{\"cf\":{\"config\":{\"distributionDomainName\":\"d111111abcdef8.cloudfront.net\",\"distributionId\":\"EDFDVBD6EXAMPLE\",\"eventType\":\"origin-request\",\"requestId\":\"4TyzHTaYWb1GX1qTfsHhEqV6HUDd_BzoBZnwfnvQc_1oF26ClkoUSEQ==\"},\"request\":{\"headers\":{},\"method\":\"GET\",\"origin\":{\"custom\":{}},\"querystring\":\"\",\"uri\":\"/\"}}}]}Enter fullscreen modeExit fullscreen modeCountry and type of device is not, but CloudFront have support foradding this, and several other headers, into the event. To do that an OriginRequestPolicy, that include the headers, must be created and added to the distribution.HeadersEnabledOriginRequestPolicy:Type:AWS::CloudFront::OriginRequestPolicyProperties:OriginRequestPolicyConfig:Name:HeadersEnabledOriginRequestPolicyComment:Headers added OriginRequestPolicyCookiesConfig:CookieBehavior:noneQueryStringsConfig:QueryStringBehavior:noneHeadersConfig:HeaderBehavior:whitelistHeaders:-CloudFront-Viewer-Country-CloudFront-Viewer-Country-Name-CloudFront-Is-Mobile-Viewer-CloudFront-Is-Desktop-Viewer-CloudFront-Is-Tablet-ViewerCloudFrontDistribution:Type:AWS::CloudFront::DistributionProperties:DefaultCacheBehavior:...OriginRequestPolicyId:!RefHeadersEnabledOriginRequestPolicy...Enter fullscreen modeExit fullscreen modeAccess time, is not available in the event at all, so for that data I needed to use a different trick.Data ingestion overviewI decided early that I wanted to separate data ingestion and processing, that way I could create a decoupled solution where I could change the processing if needed. In the first solution I tried I posted an event from Lambda@Edge to an custom EventBridge event-bus.This setup worked OK, I got the data I needed and could process it. But, when I loaded a page on the blog it felt slow, it was not crazy but I could clearly feel a delay. With some tracing I discovered that posting an event cross region, which is an synchronous operation, could easily reach 200ms, enough to make the page feel sluggish.Instead, I opted for a solution where the Lambda@Edge function invokes a StepFunction, which can be done asynchronous, and gone was the sluggish feeling. With StepFunctions I could also do more advanced operations, and since it was invoked asynchronously time was not an issue.The first thing to resolve was the access time, it is not available in the CloudFront event, instead I need to get it from somewhere else. In the Context object for the StepFunction invocation there is a start time, this would be perfect to use as the exact time is not that important.I could also convert the three boolean values for the viewer headers, e.g CloudFront-Is-Mobile-Viewer, I can convert this into a string field instead.This would create an data-event that is posted to the EventBridge event-bus.{\"DataVersion\":\"2\",\"CloudFront-Is-Mobile-Viewer\":\"CloudFront-Is-Mobile-Viewer\",\"CloudFront-Is-Tablet-Viewer\":\"CloudFront-Is-Tablet-Viewer\",\"CloudFront-Is-Desktop-Viewer\":\"CloudFront-Is-Desktop-Viewer\",\"Cloudfront-Viewer-Country\":\"Cloudfront-Viewer-Country\",\"Cloudfront-Viewer-Country-Name\":\"Cloudfront-Viewer-Country-Name\",\"Page\":\"Page\",\"AccessTime\":\"AccessTime\",\"Userhash\":\"Userhash\",\"CountryCode\":\"Cloudfront-Viewer-Country\",\"CountryName\":\"Cloudfront-Viewer-Country-Name\",\"Viewer\":\"ParsedViewerType.Viewer\",\"UTCViewDateTime\":\"States.Format('{} {} UTC', ParsedTimeAndDate.Date, ParsedTimeAndDate.Time)\",\"ViewDate\":\"ParsedTimeAndDate.Date\",\"ViewTime\":\"ParsedTimeAndDate.Time\"}Enter fullscreen modeExit fullscreen modeData storeAs seen, Lambda@Edge will invoke a StepFunctions that will send an data-event to the EventBridge event-bus. Next would be to pick up the data-event, process and store it. As I plan to keep the solution serverless and at low cost, S3 is great data-store. To send data to S3 FireHose is a great service, as it also support buffering. Data will be stored in larger files, and data will be stored in date based partitions, which is good for Glue and Athena performance, that I plan to use.When Firehose write data to S3 it will add several Json objects to the file, each object will be written one after another. it would be something like this.{\"DataVersion\":\"2\",....}{\"DataVersion\":\"2\",....}{\"DataVersion\":\"2\",....}Enter fullscreen modeExit fullscreen modeBut, when Glue Crawler index the data, the crawler expect each object to be separated by a new line. This is not the default behaviour in Firehose. So to be able to accomplish a file that can be indexed, looking like this.{\"DataVersion\":\"2\",....}{\"DataVersion\":\"2\",....}{\"DataVersion\":\"2\",....}Enter fullscreen modeExit fullscreen modeThe stream must be configured with an ProcessingConfiguration that will add the delimiter. To do that a Processor of AppendDelimiterToRecord type must be added. If not, a Glue Crawler will not be able to index the data.StatisticsDataFirehose:Type:AWS::KinesisFirehose::DeliveryStreamProperties:ProcessingConfiguration:Enabled:TrueProcessors:-Type:AppendDelimiterToRecordEnter fullscreen modeExit fullscreen modeData analyticsWith data stored in S3 I could point a Glue Crawler at the data and have it indexed in a Glue Data Catalog. Athena can be used to query the data and a Managed Grafana can be used for graphs and visual analytics. Now, Managed Grafana is not a fully serverless service. You will be paying a monthly cost based on the number of editors and administrators. It would be possible to leave out Managed Grafana from the solution and just use Athena for queries.To use Athena in Grafana the first thing that needed to be done is to set a Tag on the workgroup. I was going to use the primary workgroup. The tag GrafanaDataSource must be set to true.Next I could create the Managed Grafana Workspace, navigate to the Managed Grafana Console and fill in the needed information. First set a name and version to use, I'm using version 9.4 of Grafana.In the second step I set authentication to identity Center, make sure you tick the Turn on plugin management, this is needed to configure Athena as a source.Finally add Athena as one of the Data sources.From Managed Grafana console it's possible to jump directly into Grafana, where we can turn on Athena plugin and then configure the Athena data source.With everything created it's possible to run queries likeselect\"page\",count(*)asvisitsfromstatistics_datawhere\"viewdate\"isnotnulland\"viewdate\"between$__rawTimeFrom()and$__rawTimeTo()groupby\"page\"orderby\"visits\"DESCEnter fullscreen modeExit fullscreen modeAnd setup dashboards for tracking everything.Final WordsIn this post I described an approached I used to create a serverless statistics service for a static website hosted from S3 and CloudFront. For a full deployable solution, visitServerless HandbookDon't forget to follow me onLinkedInandXfor more content, and read rest of myBlogsAs Werner says! Now Go Build!"}
{"title": "Remotely Run Commands On An EC2 Instance With AWS Systems Manager", "published_at": 1710840978, "tags": [], "user": "Ashish Gajjar", "url": "https://dev.to/aws-builders/remotely-run-commands-on-an-ec2-instance-with-aws-systems-manager-5m9", "details": "AWS Systems Manager provides configuration management, which helps you maintain consistent configuration of your Amazon EC2 or on-premises instances.If you are a System administrator and assigned a task to upgrade the packages for one application running on an EC2 instance, but due to some security restrictions, you are not permitted to access production instances via SSH or bastion host. In this situation, you can use AWS Systems Manager to remotely run shell scripts or certain commands to update packages on EC2 instances.In this blog, we will cover everything you need to know about AWS Systems Manager and how to use it!What is AWS System Manager and how does it work?Benefits of System ManagerWho can use AWS Systems Manager?Hands-on \u2013 Run commands remotely on an EC2 Instance using AWS Systems ManagerConclusionHow does it work?AWS Systems Manager provides its users visibility and control of their infrastructure on AWS. It has a unified user interface so one can view operational data from multiple AWS services and lets the user automate operational tasks across AWS resources.What are its benefits?Quick problem detectionHybrid Environment ManagementEasy AutomationSecurity and Compliance MaintenanceImprove Visibility and ControlWho can use AWS Systems Manager?The key feature of System Manager is to make multiple roles can be performed easily. Hence, this service can be used by:System administratorsSoftware developersSecurity architectsCloud architectsIT professionals who would like to manage AWS resources.Hands-OnIn this a scenario wherein you are assigned tasks by your team to upgrade the packages for your application running on your EC2 instances. Due to some security restrictions, you are not permitted to directly access your production instances via SSH and are not even allowed to use the bastion hosts. In this situation let\u2019s use Amazon Systems Manager to remotely run your shell scripts or certain commands to update packages on your EC2 instances.Step 1: Create an Identity and Access Management (IAM) role.Step 2: Create an EC2 instance.Step 3: Update the Systems Manager Agent.Step 4: Upgradation process via the Fleet Manager dashboard.Step 5: Run a Remote Shell Script. Login to your AWS account on the AWS console and navigate to the IAM console to get started. Click on \u201cRoles\u201d under the \u201cAccess management\u201d section on the left navigation pane.Click on \u201cCreate role\u201d to create a new role. You will use this role to give Amazon Systems Manager permission to perform actions on your instances.Search for the \u201cAmazonEC2RoleForSSM\u201d policy and click on the checkbox to add the policy to the role.Once done, click on \u201cNext: Review\u201d and enter a name for the newly created role and descriptionOn creation of the role, you can type in the role name in the search bar on the Roles dashboard to verify if the role has been created successfully. Choose and Amazon Machine Image (AMI), select \u201cAmazon Linux 2 AMI (64-bit)\u201d and click on \u201cSelect\u201dWe will create an EC2 instance using the role that we created above. This will help us create a managed EC2 instance that will be managed by the Amazon Systems Manager. Navigate to the Amazon EC2 console and ensure that the preferred region is selected in which you want to create your instance.EC2 Dashboard console and click on \u201cLaunch instance\u201d to launch a new managed instance in your preferred region.Choose and Amazon Machine Image (AMI), select \u201cAmazon Linux 2 AMI (64-bit)\u201d and click on \u201cSelect\u201dSelect Instance Type and Key Pair.Next, you need to ensure that your have select a subnet has the \u201cEnable auto-assign public IPv4 address\u201d enabled. This is to be ensured since you will have to connect to your EC2 instance. Without the public IPv4 address, you will not be allowed to connect to your instance.Note: Make sure the SSM agent is installed on your EC2 instance.Scroll down and for the \u201cIAM role\u201d, select the role you createdOnce done, click on \u201cLaunch Instances\u201d.You will see the newly created instance in the list shown below.Once the \u201cInstance State\u201d changes to the \u201cRunning\u201d state, select the newly created instance and click on \u201cActions\u201d. Select \u201cConnect\u201d from the dropdown.click on \u201cConnect\u201d to connect to your EC2 instance.A new terminal console will open in a new tab as shown below.Check the status of SSM Agent \"sudo systemctl status amazon-ssm-agent\"Once you have an EC2 instance running the Systems Manager agent,Navigate to the Amazon Systems Manager console on AWS.Click on \u201cFleet Manager\u201d under the \u201cNode Management\u201d section in the left navigation pane.To automate the upgradation, click on \u201cAccount Management\u201d and then, click on \u201cAuto-update SSM agent\u201d.Click on \u201cAuto-update SSM agent\u201d and after a few minutes, the update will be automated for any existing or new instances you create.Click on \u201cRun Command\u201d under the \u201cNode Management\u201d section in the left navigation pane.Now, click on \u201cRun command\u201d to upgrade the SSM-agent manually.Now, click on the radio button on the left of \u201cAWS-UpdateSSMAgent\u201d. This is known as the document and this will upgrade the Systems Management agent on the selected instance.Once done, scroll down to the \u201cTargets\u201d section on the same page and select the radio button on the left of \u201cChoose instances manually\u201dScroll down and click on \u201cRun\u201d to execute the document.You will see the \u201cOverall Status\u201d as \u201cIn Progress\u201d.After a few minutes, hit refresh and the status will change to \u201cSuccess\u201d.After a few minutes, hit refresh and the status will be updated to \u201cSuccess\u201d on successful completion of execution of the command.Now, to run a remote shell script for upgrading any packages on your EC2 instance, navigate back to the \u201cRun Command\u201d dashboard in Amazon Systems Manager and click on \u201cRun Command\u201d.select the radio button on the left of \u201cChoose instances manually\u201d enter command parameters#!/bin/bash yum update -y  yum install httpd -y systemctl enable --now httpd.serviceEnter fullscreen modeExit fullscreen modeAfter a few minutes, hit refresh and the status will be updated to \u201cSuccess\u201d on successful completion of execution of the command.Verify httpd package installed or not.Conclusion:In this blog, we have explored that AWS Systems Manager has the ability to automate tasks and helps in keeping all our EC2 instances healthy, and applications managed, secure, and updated."}
{"title": "Migrating repository from github to gitlab", "published_at": 1710825316, "tags": ["github", "gitlab", "git"], "user": "Olawale Adepoju", "url": "https://dev.to/aws-builders/migrating-repository-from-github-to-gitlab-3ape", "details": "I will be showing how I migrated a repository from GitHub to GitLab. There are two methods;Option 1Firstly you can do it on the console by going to GitLab and authenticating your GitHub credential there and you will be able to import all the repositories you want on GitLab.Log in to your GitLab, and create a new projectChoose the import project optionChoose the GitHub optionIt will show the option of Authorization from GitHub.you can import all repositories or choose the repository to be migrated. Also, there is an option of importing issues pull request events.Option 2The second option, let's say your GitHub is in a private network and you want to take one repository and push it to GitLab, where GitLab is on some other network or hosted in some other cloud environment. How do you migrate in this case?Clone the GitHub repository you want to migrate. usegit clone --bare <GitHub URL>Enter fullscreen modeExit fullscreen modeCreate a new project in GitLab, and choose the create the blank project optionGive a project name and click create project.Push the cloned repository to GitLab.git push --mirror <GitLab URL>Enter fullscreen modeExit fullscreen modeError Alert:while I push to GitLab I encountered an errorHow did I solve it?Go to project: \"Settings\" \u2192 \"Repository\" \u2192 \"Expand\" on \"Protected branches\" and click on unprotect.Now the GitHub repository is migrated successfully to GitLab"}
{"title": "How To Implement AWS SSB Controls in Terraform - Part 2", "published_at": 1710802748, "tags": ["aws", "terraform", "security"], "user": "Anthony Wat", "url": "https://dev.to/aws-builders/how-to-implement-aws-ssb-controls-in-terraform-part-2-359k", "details": "IntroductionTheAWS Startup Security Baseline (SSB)defines a set of controls that comprises a lean but solid foundation for the security posture of your AWS accounts. Inpart 1of ourblog series, we examined how to implement account controls related to account-level and identity settings using Terraform. In this installment, we will look at the remaining account controls that focus on both proactive and preventive security and governance measures. Let's begin with ACCT.07, which mandates the CloudTrail log delivery to a protected S3 bucket.ACCT.07 \u2013 Log EventsThe account controlACCT.07requires that actions taken by users, roles, and services in your AWS account be recorded usingAWS CloudTrail.CloudTrail enables auditing, security monitoring, and operational troubleshooting by tracking user activity and API usage. Any API event that CloudTrail records can be used as anevent sourcein Amazon EventBridge to trigger various automations. AWS does not charge for the first trail that records management events, making it a cost-effective choice to adopt.You can create a CloudTrail trail using Terraform with theaws_cloudtrailresource. Since CloudTrail writes events to an S3 bucket, you also need to create one with the appropriate bucket policy. Here is a basic example:data\"aws_caller_identity\"\"this\"{}data\"aws_region\"\"this\"{}locals{account_id=data.aws_caller_identity.current.account_idregion=data.aws_region.this.name}# Note: Bucket versioning and server-side encryption are not shown for brevityresource\"aws_s3_bucket\"\"cloudtrail\"{bucket=\"aws-cloudtrail-logs-${local.account_id}-${local.region}\"}resource\"aws_s3_bucket_policy\"\"cloudtrail\"{bucket=aws_s3_bucket.cloudtrail.idpolicy=<<-EOT{   \"Version\": \"2012-10-17\",   \"Statement\": [     {       \"Sid\": \"AWSCloudTrailAclCheck\",       \"Effect\": \"Allow\",       \"Principal\": {         \"Service\": \"cloudtrail.amazonaws.com\"       },       \"Action\": \"s3:GetBucketAcl\",       \"Resource\": \"${aws_s3_bucket.cloudtrail.arn}\"     },     {       \"Sid\": \"AWSCloudTrailWrite\",       \"Effect\": \"Allow\",       \"Principal\": {         \"Service\": \"cloudtrail.amazonaws.com\"       },       \"Action\": \"s3:PutObject\",       \"Resource\": \"${aws_s3_bucket.cloudtrail.arn}/AWSLogs/${local.account_id}/*\",       \"Condition\": {         \"StringEquals\": {           \"s3:x-amz-acl\": \"bucket-owner-full-control\"         }       }     }   ] }EOT}resource\"aws_cloudtrail\"\"this\"{name=\"aws-cloudtrail-logs-${local.account_id}-${local.region}\"s3_bucket_name=aws_s3_bucket.cloudtrail.idenable_log_file_validation=trueis_multi_region_trail=trueadvanced_event_selector{field_selector{field=\"eventCategory\"equals=[\"Management\"]}}}Enter fullscreen modeExit fullscreen modeIf you are using AWS Organizations, you can create anorganization trailin the management account to logs events for all accounts. In Terraform, an organization trail can be created by setting theis_organization_trailargument totruefor theaws_cloudtrailresource. If you are using AWS Control Tower, a standard organization trail is created automatically when you launch your landing zone. You can import and manage it using Terraform thereafter.ACCT.08 \u2013 Prevent Public Access To Private S3 BucketsThe account controlACCT.08requires theS3 Block Public Access featureto be enabled if public access is not required.To ensure security by default, AWS enables Block Public Access by default for S3 bucketscreated on or after April 28, 2023. For S3 buckets that are created earlier, you may still need to enable the feature by yourself. In Terraform, you can use theaws_s3_bucket_public_access_blockresourceto configure the settings as appropriate. Here is an example that enables block public access for an S3 bucket:data\"aws_caller_identity\"\"this\"{}locals{account_id=data.aws_caller_identity.current.account_id}resource\"aws_s3_bucket\"\"alb_access_logs\"{bucket=\"alb-access-logs-${local.account_id}\"}resource\"aws_s3_bucket_public_access_block\"\"alb_access_logs\"{bucket=aws_s3_bucket.alb_access_logs.idblock_public_acls=trueblock_public_policy=trueignore_public_acls=truerestrict_public_buckets=true}Enter fullscreen modeExit fullscreen modeIf you are confident that all S3 buckets in your account do not require public access, you can also use theaws_s3_account_public_access_blockresourceto enable public public access at the account level as follows:resource\"aws_account_public_access_block\"\"alb_access_logs\"{block_public_acls=trueblock_public_policy=trueignore_public_acls=truerestrict_public_buckets=true}Enter fullscreen modeExit fullscreen modeACCT.09 \u2013 Delete Unused ResourcesThe account controlACCT.09requires that unused resources be deleted or disabled to reduce the opportunity for security issues.In particular, thedefault VPCthat is automatically created in each AWS account and enabled region should be considered for deletion. Default VPCs are created with public subnets that automatically assign IPv4 addresses, so novice AWS users could inadvertently expose private workloads to the internet. A multi-VPC environment with peering requirements also ought to use a well-defined CIDR allocation scheme other than the default172.31.0.0/16range. It is therefore recommended that you delete the default VPCs and create ones that are more thought out as necessary.In Terraform, there are resources such asaws_default_vpcandaws_default_subnetwhich can technically be used to delete the default VPC resources with theforce_destroyargument set totrue. However, you would first have to define these resources in your Terraform configuration to \"bring them in\" before you can destroy them, making it a two-step process.Alternatively, you can use theawsutilsmodule from cloudposseto remove the default VPC resources more efficiently. Theawsutilsmodule provides theawsutils_default_vpc_deletionresource, which when defined in your Terraform configuration will deletes the default VPC along with the child resources of the VPC in the configured region, for example:terraform{required_providers{awsutils={source=\"cloudposse/awsutils\"}}}provider\"awsutils\"{region=\"us-east-1\"}resource\"awsutils_default_vpc_deletion\"\"default\"{}Enter fullscreen modeExit fullscreen modeHowever, it might be more efficient to simply write a shell script to delete the default VPC from all regions instead of using Terraform.In the case where your AWS environment is created usingAWS Control Tower's Account Factory, you canuncheck all regionsso that the default VPC is not created in any of them. Account Factory for Terraform (AFT) also hasan option to delete the default VPCif you are using that feature to dispense new accounts.ACCT.10 \u2013 Monitor CostsThe account controlACCT.10requires cost monitoring and notification using services such asAWS Budgets.AWS Budgets allows users to set custom budgets for AWS resource usage and sends notifications when actual or forecasted usage exceeds the budgeted amounts. A budget can be created in Terraform using theaws_budgets_budgetresource. Here is an example configuration that creates budgets similar to theMonthly cost budget template:resource\"aws_budgets_budget\"\"this\"{name=\"My Monthly Cost Budget\"budget_type=\"COST\"limit_amount=\"1000\"limit_unit=\"USD\"time_unit=\"MONTHLY\"notification{comparison_operator=\"GREATER_THAN\"threshold=85threshold_type=\"PERCENTAGE\"notification_type=\"ACTUAL\"subscriber_email_addresses=[\"finance@example.com\"]}notification{comparison_operator=\"GREATER_THAN\"threshold=100threshold_type=\"PERCENTAGE\"notification_type=\"ACTUAL\"subscriber_email_addresses=[\"finance@example.com\"]}notification{comparison_operator=\"GREATER_THAN\"threshold=100threshold_type=\"PERCENTAGE\"notification_type=\"FORECASTED\"subscriber_email_addresses=[\"finance@example.com\"]}}Enter fullscreen modeExit fullscreen modeAlthough it is not mentioned in the documentation for this control, I would also recommend that you also configureAWS Cost Anomaly Detectionas an additional safeguard for cost overruns. This feature uses machine learning models to detect and alert on anomalous spending patterns in your deployed AWS services. You can create a cost monitor using theaws_ce_anomaly_monitorresourceand subscriptions using theaws_ce_anomaly_subscriptionresourcein Terraform. The following is an example that sets up a cost monitor and a daily summary alert when the cost is 50% above the expected spend.resource\"aws_ce_anomaly_monitor\"\"service\"{name=\"AWSServiceMonitor\"monitor_type=\"DIMENSIONAL\"monitor_dimension=\"SERVICE\"}resource\"aws_ce_anomaly_subscription\"\"service_daily\"{name=\"DAILYSUBSCRIPTION\"frequency=\"DAILY\"monitor_arn_list=[aws_ce_anomaly_monitor.service.arn]subscriber{type=\"EMAIL\"address=\"finance@example.com\"}threshold_expression{dimension{key=\"ANOMALY_TOTAL_IMPACT_PERCENTAGE\"match_options=[\"GREATER_THAN_OR_EQUAL\"]values=[\"50\"]}}}Enter fullscreen modeExit fullscreen modeACCT.11 \u2013 Enable GuardDutyThe account controlACCT.11recommends enablingAmazon GuardDutyto continuously monitor for malicious and unauthorized behavior to help protect against threats.To enable GuardDuty in Terraform, use theaws_guardduty_detectorresourceto enable the service and theaws_guardduty_detector_featureresourceto enable individual features. The following is a full example that enables all available protection features:\u26a0 GuardDuty is a regional service and thus must be enabled in each region that you are using.resource\"aws_guardduty_detector\"\"this\"{enable=truefinding_publishing_frequency=\"SIX_HOURS\"}resource\"aws_guardduty_detector_feature\"\"s3\"{detector_id=aws_guardduty_detector.this.idname=\"S3_DATA_EVENTS\"status=\"ENABLED\"}resource\"aws_guardduty_detector_feature\"\"eks\"{detector_id=aws_guardduty_detector.this.idname=\"EKS_AUDIT_LOGS\"status=\"ENABLED\"}resource\"aws_guardduty_detector_feature\"\"runtime\"{detector_id=aws_guardduty_detector.this.idname=\"RUNTIME_MONITORING\"status=\"ENABLED\"additional_configuration{name=\"EKS_ADDON_MANAGEMENT\"status=\"ENABLED\"}additional_configuration{name=\"ECS_FARGATE_AGENT_MANAGEMENT\"status=\"ENABLED\"}}resource\"aws_guardduty_detector_feature\"\"malware\"{detector_id=aws_guardduty_detector.this.idname=\"EBS_MALWARE_PROTECTION\"status=\"ENABLED\"}resource\"aws_guardduty_detector_feature\"\"rds\"{detector_id=aws_guardduty_detector.this.idname=\"RDS_LOGIN_EVENTS\"status=\"ENABLED\"}resource\"aws_guardduty_detector_feature\"\"lambda\"{detector_id=aws_guardduty_detector.this.idname=\"LAMBDA_NETWORK_LOGS\"status=\"ENABLED\"}Enter fullscreen modeExit fullscreen modeIf you have a multi-account landing zone that uses AWS Organizations or AWS Control Tower, you can use theaws_guardduty_organization_admin_accountresource, theaws_guardduty_organization_configurationresource, and theaws_guardduty_organization_configuration_featureresource to configure GuardDuty at the organization level. Here is an example that configures GuardDuty with a delegated administrator to the Audit account (which you generally find in a Control Tower landing zone) and auto-enable GuardDuty for all member accounts in the organization:provider\"aws\"{alias=\"management\"profile=\"management\"}provider\"aws\"{alias=\"audit\"profile=\"audit\"}data\"aws_caller_identity\"\"audit\"{provider=aws.audit}locals{audit_account_id=data.aws_caller_identity.audit.account_id}resource\"aws_guardduty_organization_admin_account\"\"this\"{admin_account_id=local.audit_account_id}resource\"aws_guardduty_detector\"\"audit\"{provider=aws.auditenable=true}resource\"aws_guardduty_organization_configuration\"\"audit\"{provider=aws.auditauto_enable_organization_members=\"ALL\"detector_id=aws_guardduty_detector.audit.id}resource\"aws_guardduty_organization_configuration_feature\"\"audit_s3\"{provider=aws.auditauto_enable=\"ALL\"detector_id=aws_guardduty_detector.audit.idname=\"S3_DATA_EVENTS\"}Enter fullscreen modeExit fullscreen mode\ud83d\udca1 Based on the feedback I received, I have written a new blog postHow To Manage Amazon GuardDuty in AWS Organizations Using Terraformwith more details on this topic. Feel free to read it if you are interested.As an alternative to auto enablement, you can use theaws_guardduty_memberresourceto add GuardDuty members individually and use theaws_guardduty_invite_accepterresourceat the member account to accept the invitation. Since the fully automated method is preferred, we won't go through an example for these resources.\u26a0 GuardDuty is a costly service, so make sure that you review thepricingand understand how much each feature costs. You shouldleverage the 30-day free trial period to estimate the GuardDuty cost. This allows you to weigh the cost against the risks and requirements before deciding whether to enable GuardDuty and which protections to enable.ACCT.12 \u2013 Monitor High-Risk IssuesThe account controlACCT.12recommends usingAWS Trusted Advisorto scan for high-risk or high-impact issues related to security, performance, cost, and reliability.\u26a0 If you do not have a Business Support Plan or higher, you are only eligible for some basic security checks and service limit checks in Trusted Advisor. The Trusted Advisor API also cannot be used to enable automation such as refreshing check results and custom notification. So Trusted Advisor is restrictive and frankly not very useful at the free tier.There is no Terraform resource for interacting with Trusted Advisor, so you need toconfigure notification in the AWS Management Console. There is however atrusted-advisor-refreshmodulethat helps refresh the Trusted Advisor check results more often than theautomatic one-week scheduleif you have access to the Trusted Advisor API with a higher-tier Support Plan.If you are using AWS Organizations or AWS Control Tower, you canenable organizational viewin the management account. However the Support Plan requirement still applies toeach member account, that is, you will not receive any additional checks in member accounts that do not have Business Support Plan or higher.SummaryIn this second blog post of the seriesHow to implement the AWS Startup Security Baseline (SSB) using Terraform, we examined the remaining account-level controls and explained how you can implement them using Terraform. In the next installment, we will focus on the the workload-level controls for complete coverage. Please continue to follow the series and check out other posts in theAvangards Blog."}
{"title": "Build an AI image catalogue! - Claude 3 Haiku", "published_at": 1710800786, "tags": ["serverless", "machinelearning", "aws"], "user": "Alan Blockley", "url": "https://dev.to/aws-builders/build-an-ai-image-catalogue-claude-3-haiku-34d7", "details": "Building an Intelligent Photo Album with Amazon BedrockIf you\u2019re into your Machine Learning/Artificial Intelligence/Generative AI and have been living under a rock, it may be news to you that Anthropic have now released their latest version of the Claude Large Language Model (LLM), which they have predictively named, Claude 3.   But what\u2019s not so predictable is that not only is this the third generation of Claude, but there are also three different variations of Claude.In this post, we\u2019re going to explore the creation of a basic, serverless, image cataloguing application using Claude 3 and explore how it enhances the functionality of our AI photo album application. Powered by Amazon Bedrock, this application leverages the poetic prowess of Claude 3 Haiku to provide insightful summaries of uploaded images.In today's digital era, our memories are often immortalized through the lens of a camera, capturing moments that evoke a myriad of emotions. As our photo collections grow, organizing and making sense of these images can become a daunting task. However, with the emergence of artificial intelligence (AI) and cloud computing, we now have the opportunity to create intelligent solutions that automate and enrich the photo management process.Announcing Claude 3\u201cThe next generation of Claude: A new standard for intelligence\u201d -https://www.anthropic.com/news/claude-3-familyWhen Claude 3 was announced they came up with not just one new model, but three:Haiku (Available on Amazon Bedrock)Sonnet (Available on Amazon Bedrock)Opus (Coming soon to your favourite AI provider starting with B\u2026 and ending in rock)Now whilst I write this in celebration of the Claude 3 family, I\u2019m not going to go into all the differing benchmark charts as I\u2019m sure you\u2019ve already seen, however, the links and resources in this post should provide the benchmarks you seek if this is something you\u2019re interested in.What really got my attention was the overall impact that the whole Claude 3 family has on the LLM landscape.  Bold strokes of improvement across all three variances including;Near-instant resultsStrong vision capabilitiesFewer refusalsImproved accuracyLong context and near-perfect recallResponsible designEasier to useSource:https://www.anthropic.com/news/claude-3-familyWhat got my attention the most was the claim of \u201cStrong vision capabilities\u201d.  How strong?  That\u2019s something I wanted to test out.I originally started writing this little project about 2 weeks before the release of Claude 3 Haiku,  using Claude 3 Sonnet.  The appeal was that Claude 3 Sonnet was offering unmatched vision capabilities.\u201cThe Claude 3 models have sophisticated vision capabilities on par with other leading models. They can process a wide range of visual formats, including photos, charts, graphs and technical diagrams. We\u2019re particularly excited to provide this new modality to our enterprise customers, some of whom have up to 50% of their knowledge bases encoded in various formats such as PDFs, flowcharts, or presentation slides.\u201d(OK I caved in and put a benchmark chart in - worth it though!)Exploring Claude 3 Haiku\u201cClaude 3 Haiku: our fastest model yet\u201d -https://www.anthropic.com/news/claude-3-haikuThe moment Haiku was released, my colleagues at the other end of the work meeting I was attending at the time were greeted with a physical whoop of joy.  I was already excited by Claude 3 sonnet so having the next in the generation arrive was like Christmas had come early.I also knew this meant that I could change my large language model from Sonnet to Haiku.The benefits it provided to my proof of concept were:The fastest model in the Claude 3 familyThe most cost-effective model in the Claude 3 family.This means a lot, especially in a proof of concept like this.  This gives us the speed of execution to develop fast, fail fast and iterate on solutions a lot quicker.  It also means that we\u2019re able to control the costs of the application.\u201cSpeed is essential for our enterprise users who need to quickly analyze large datasets and generate timely output for tasks like customer support. Claude 3 Haiku is three times faster than its peers for the vast majority of workloads, processing 21K tokens (~30 pages) per second for prompts under 32K tokens [1]. It also generates swift output, enabling responsive, engaging chat experiences and the execution of many small tasks in tandem.\u201dThis speed also means the ability to analyze documents at a much faster rate than ever with a faster recall of information.In our use case, we want to visualize our images by using descriptive language to help create a catalogue of images with a summary and to also have it provide a category for the image.This can create powerful use cases such as making art more accessible for the blind or improve operational overhead in managing catalogs of stock images in a creative studio.Let\u2019s start building!By the end of this post, I hope to have demonstrated:Seamlessly integrate Claude 3 Haiku into a photo album appCalling Amazon Bedrock from a serverless applicationBasic Architecture that is needed to achieve this.Some lessons learned with Prompt EngineeringThe \u201cLego Bricks\u201dThe basic architecture that we\u2019re going to build looks something like this:I liken AWS Services to Lego bricks.  There are many of them and when put together in varying combinations, they can make many unique different builds.  In this build, we\u2019ll be using:Amazon S3To receive images and send them to be processed using S3 EventsTo store and host processed images and a static-hosted HTML pageAPI GatewayTo accept a GET request from a static-hosted HTML pageAWS LambdaTo receive the S3 event and process the imageTo accept a GET request and provide the user with the image dataAmazon BedrockTo leverage Calude 3 and provide an AI-generated summary and categoryAmazon DynamoDBTo store the information generated by Claude 3 / Amazon BedrockI\u2019ve utilised the AWS Serverless Application Model (SAM) framework to help build and deploy this as well to keep things as easy as possible.\u201cSAM and Bedrock? Surely not?!?\u201d- Remember, Amazon Bedrock is a serverless service and can be called by an API or an SDK.  The power is yours to call Bedrock any way you want to.And last but not least, our language of choice.  Python for our AWS Lambda functions and there\u2019s a small sprinkling of old-school Javascript to assist the front-end HTML.If you\u2019re keen to get this setup yourself feel free to browse the repo and get started yourself -https://github.com/alanblockley/bedrock-haiku-image-catalogOtherwise, for more explanation, keep reading.Prompt Engineering - No such thing as too muchWhen writing this little experiment I started with a very typical MVP style approach.  Get something working.  We\u2019ve all been there, right?  But in this process, I fell victim to also shirking on my prompt for Claude 3.  My original prompt was something like\u201cProvide me a summary of this image and give it a short category\u201d.In my defence, it worked.  But as any good builder/coder/developer, I read the documentation\u2026 actually, no I didn\u2019t\u2026 Have you read IKEA instructions, I\u2019ve been burnt for life!  But instead, I sought out a peer review from a very good friend of mine.  He swiftly explained that this was a bad prompt.  But I didn\u2019t get it.  Yes, I\u2019ve done ML/AI/PartyRock/Gen AI stuff but I\u2019ve never really had to write anything in anger until now.The explanation of why this is a bad prompt can be complex.  To make it easier to understand, let\u2019s think of it in human terms, after all this is an emulation of human intelligence.  In your prompt you need to allow for:Time for the model to think about what it\u2019s doing - When we are thinking about something or trying to work on something, we generally think it through 3 or 4 times maybe, sometimes we\u2019ll jot down some notes as well.Structure your output - Claude likes XML tags, so maybe have Claude structure it\u2019s output.  Ask it to put it\u2019s thinking into XML tags such as<SCRATCHPAD>or<NOTES>, closing them respectively.  Then have it put the output of value, your output, into another XML tag such as<OUTPUT></OUTPUT>The more detail the better.  Assume you\u2019re explaining this task for the first time to a trainee.  If a trainee runs this task for the first time and gets it wrong, you didn\u2019t explain it well enough and Claude definitely won\u2019t get it right.  Give it a purpose or a role.  It\u2019s not good to expect a toaster to dispense ice.  So ensure that your model is given a contextual purpose to work to.  * This will help make your answers more accurate.Whilst all of these actions will help find the accuracy you need in your AI tasks, they\u2019ll also make things more consistent as well.  With this advice, my prompt ended up taking up 34 lines and told a story to the model, not just a quick 1 line action that could be interpreted vaguely.  Have a play with it and see what you think.And don\u2019t forget a little bit of logic in your code to help extract the data from the XML tags.  I\u2019ve included a Python example in my repo.The Lambda FunctionsIn our architecture, we\u2019ve detailed two functions.  One that processes images and one that retrieves the stored data.For the sake of simplicity, there\u2019s a third function, not detailed in the diagram, which takes the image and copies it to the image store but also renames the image to a random, unique UUID.  This is to help indexing and also removes any bias from the AI generation of image data.  The model then has no concept of image name.  You could also add MIME checking, AV scanning and other checks here and also utilise this bucket to receive from anywhere such as AWS Transfer Family or a web front end via Presigned URLs.I\u2019ve included the functions in a public repository for you to play with -https://github.com/alanblockley/bedrock-haiku-image-catalogFor the sake of making this an interesting read, I\u2019ve only included a copy/paste of the function that calls Amazon Bedrock.   This function performs the following:Decode Image Name: Extracts the name of the uploaded image from the S3 event and removes any obfuscation caused by URL formatting of the name.Retrieve Image Type: Determines the MIME type of the uploaded image using the S3 object key.Retrieve Image Data: Fetches the image data from the designated S3 bucket.Encode Image Data: Converts the image data to a base64-encoded string for processing by the Claude 3 Haiku model.Generate Summary: Utilizes the Claude 3 Haiku model to produce a summary of the image content.Extract Summary: Parses the response from the Claude 3 Haiku model to extract the generated summary.Store Summary in DynamoDB: Stores the extracted summary along with relevant image details (e.g., object ID) in a DynamoDB table for future retrieval.This lambda function will also handle the streaming output that Bedrock provides and format it in such a way that it can be inserted into a DynamoDB table.Also note, that I asked for structured data from the model.  So when Claude presents me with this data, it\u2019s creating a JSON string within the XML tags which I then use to continue processing the data.Summarise Image - Codeimport os import json import boto3 import base64 import urllib.parse from botocore.exceptions import ClientError   IMAGE_TABLE = os.environ['IMAGE_TABLE'] IMAGE_BUCKET = os.environ['IMAGE_BUCKET']   s3 = boto3.client('s3') dynamodb = boto3.resource('dynamodb') bedrock_runtime = boto3.client('bedrock-runtime')   def extract_substring(text, trigger_str, end_str):    # Find string between two values (Thanks for this Mike!)    last_trigger_index = text.rfind(trigger_str)    if last_trigger_index == -1:        return \"\"    next_end_index = text.find(end_str, last_trigger_index)    if next_end_index == -1:        return \"\"    substring = text[last_trigger_index + len(trigger_str):next_end_index]    return substring   def decode_object_name(object_name):    # Decode the object name from the URL-encoded format    return urllib.parse.unquote_plus(object_name)   # Function to get the image and turn it into a base64 string def get_image_base64(bucket, key):    try:        response = s3.get_object(Bucket=bucket, Key=key)    except ClientError as e:        print(e)        return False    else:        image = response['Body'].read()        return image   def get_image_type(bucket, key):    # Get the Mime type using object key and head_object    # Must use head_object      try:        response = s3.head_object(Bucket=bucket, Key=key)    except ClientError as e:        print(e)        return False    else:        content_type = response['ContentType']        print(content_type)        return content_type   def generate_summary(image_base64, content_type):    # Generate a summary of the input image using the Bedrock Runtime and claude3 model    # Must usee invoke_model      prompt = \"\"\"Your purpose is to catalog images based upon common categories. Create a structured set of data in json providing a summary of the image and a very short, generalised, image category.  Do not return any narrative language. Before you provide any output, show your working in <scratchpad> XML tags. JSON fields must be labelled image_summary and image_category.   Example json structure is:   <json> {    \"image_summary\": SUMMARY OF THE IMAGE,    \"image_category\": SHORT CATEGORY OF THE IMAGE, } </json>   Examples of categorie are:   Animals Nature People Travel Food Technology Business Education Health Sports Arts Fashion Backgrounds Concepts Holidays  Output the json structure as a string in <json> XML tags.  Do not return any narrative language.   Look at the images in detail, looking for people, animals, landmarks or features and where possible try to identify them. \"\"\"     response = bedrock_runtime.invoke_model(        modelId='anthropic.claude-3-haiku-20240307-v1:0',        contentType='application/json',        accept='application/json',        body=json.dumps({            \"anthropic_version\": \"bedrock-2023-05-31\",            \"max_tokens\": 1000,            \"system\": prompt,            \"messages\": [                {                    \"role\": \"user\",                    \"content\": [                        {                            \"type\": \"image\",                            \"source\": {                                \"type\": \"base64\",                                \"media_type\": content_type,                                \"data\": image_base64                            }                        },                        # {                        #     \"type\": \"text\",                        #     \"text\": prompt                        # }                    ]                }            ]        })    )      print(response)    return json.loads(response.get('body').read())   def store_summary(object_id, summary, category):    # Store the summary in DynamoDB    # Must use put_item     table = dynamodb.Table(IMAGE_TABLE)    table.put_item(        Item={            'id': object_id,            'summary': summary,            'category': category        }    )   def lambda_handler(event, context):      print(json.dumps(event, indent=4))    object_name = decode_object_name(event['Records'][0]['s3']['object']['key'])      image_type = get_image_type(IMAGE_BUCKET, object_name)    image_base64 = get_image_base64(IMAGE_BUCKET, object_name)      if not image_base64:        return {            'statusCode': 500,            'body': json.dumps('Error getting image')        }    else:        image_base64 = base64.b64encode(image_base64).decode('utf-8')        response_body = generate_summary(image_base64, image_type)         print(response_body)         summary = response_body['content'][0]['text']        json_summary = json.loads(extract_substring(summary, \"<json>\", \"</json>\"))         image_summary = json_summary['image_summary']        image_category = json_summary['image_category']        store_summary(object_name, image_summary, image_category)      return {        'statusCode': 200,        'body': json.dumps('Summary stored in DynamoDB')    }Enter fullscreen modeExit fullscreen modeNOTE: To call a bedrock-friendly version of boto3 we\u2019re making use of a Lambda Layer.  This must be built into your account before deploying this SAM template.  You will then need to include the ARN of the layer as a Parameter as you deploy.  To build the boto3 lambda layer:Open Cloud Shell from the AWS Console and type the following commandsmkdir ./bedrock-layer   cd ./bedrock-layer   mkdir ./python   pip3 install -t ./python/ boto3   zip -r ../bedrock-layer.zip .   cd ..    aws lambda publish-layer-version \\   --layer-name bedrock-layer \\   --zip-file fileb://bedrock-layer.zipEnter fullscreen modeExit fullscreen modeSource:-https://www.linkedin.com/posts/mikegchambers_serverless-python-activity-7154258975926964224-IL4GThe Front EndTo use the information we\u2019ve generated in a useful way I created a very simple front end.  This just lists any data in the dynamoDB and gives a thumbnail.I\u2019ve kept this as basic as possible.  The hero of the story here after all is Claude\u2026 not my horrific javascript skills.As you can see, Claude has done pretty well at creating a summary and categorising these images.  This could then lead to further features being added such as search engines in large collections or services like Amazon Polly to explain what the image is.Imagine how good this would be at Pictionary!The SAM TemplateThe glue that holds this all together is a SAM template defining the resources in a declarative language that we\u2019re all familiar with.I\u2019ve kept this template as simple as possible, including a SimpleTable for the DynamoDB table and separating resources between the frontend and the backend.AWSTemplateFormatVersion: '2010-09-09' Transform: AWS::Serverless-2016-10-31 Description: >  AI Photo album   Globals:  Api:    Cors:      AllowMethods: \"'*'\"      AllowHeaders: \"'*'\"      AllowOrigin: \"'*'\"      AllowCredentials: \"'*'\"  Function:    Timeout: 30    MemorySize: 256    Runtime: python3.11    Environment:      Variables:        INCOMING_BUCKET: !Sub '${Prefix}-${Workload}-incoming'        IMAGE_BUCKET: !Sub '${Prefix}-${Workload}-images'        IMAGE_TABLE: !Ref ImageTable    Architectures:      - arm64   Parameters:  Prefix:    Type: String    Description: Prefix for all resources    Default: my-ml     Workload:    Type: String    Description: Workload of the application    Default: photo-album    BedrockLayerArn:    Type: String    Description: ARN of the Bedrock layer    Default: arn:aws:lambda:us-west-2:168420111683:layer:bedrock-layer:1   Resources: ### Backend resources  ImageTable:    Type: AWS::Serverless::SimpleTable    Properties:      PrimaryKey:        Name: id        Type: String      TableName: !Sub '${Prefix}-${Workload}-image-table'    IncomingBucket:    Type: AWS::S3::Bucket    Properties:      BucketName: !Sub '${Prefix}-${Workload}-incoming'      AccessControl: Private      PublicAccessBlockConfiguration:        BlockPublicAcls: true        BlockPublicPolicy: true        IgnorePublicAcls: true        RestrictPublicBuckets: true    ImageBucket:    Type: AWS::S3::Bucket    Properties:      BucketName: !Sub '${Prefix}-${Workload}-images'      AccessControl: Private      PublicAccessBlockConfiguration:        BlockPublicAcls: true        BlockPublicPolicy: true        IgnorePublicAcls: true        RestrictPublicBuckets: true    RenameImageFunction:    Type: AWS::Serverless::Function    Properties:      CodeUri: rename_image/      Handler: app.lambda_handler      Layers:        - !Ref BedrockLayerArn      Policies:        - S3ReadPolicy:            BucketName: !Sub '${Prefix}-${Workload}-incoming'        - S3WritePolicy:            BucketName: !Sub '${Prefix}-${Workload}-images'      Events:        S3ObjectCreated:          Type: S3          Properties:            Bucket:              Ref: IncomingBucket            Events: s3:ObjectCreated:*    SummariseImageFunction:    Type: AWS::Serverless::Function    Properties:      CodeUri: summarise_image/      Handler: app.lambda_handler      Layers:        - !Ref BedrockLayerArn      Policies:        - DynamoDBWritePolicy:            TableName: !Ref ImageTable        - S3ReadPolicy:            BucketName: !Sub '${Prefix}-${Workload}-images'        - Statement:            - Sid: Bedrock              Effect: Allow              Action:                - bedrock:InvokeModel              Resource: !Sub 'arn:aws:bedrock:${AWS::Region}::foundation-model/*'      Events:        S3ObjectCreated:          Type: S3          Properties:            Bucket:              Ref: ImageBucket            Events: s3:ObjectCreated:*    GetImagesFunction:    Type: AWS::Serverless::Function    Properties:      CodeUri: get_images/      Handler: app.lambda_handler      Policies:        - DynamoDBReadPolicy:            TableName: !Ref ImageTable      Events:        Api:          Type: Api          Properties:            Path: /images            Method: get   ### Front end resources  ImageHostingOriginaccessidentity:      Type: AWS::CloudFront::CloudFrontOriginAccessIdentity      Properties:        CloudFrontOriginAccessIdentityConfig:          Comment: !Sub ${Prefix}-${Workload}-originaccessidentity    ImageHostingBucketPolicy:    Type: AWS::S3::BucketPolicy    Properties:      Bucket: !Ref ImageBucket      PolicyDocument:        Statement:          -            Action:              - \"s3:GetObject\"            Effect: \"Allow\"            Resource:              Fn::Join:                - \"\"                -                  - \"arn:aws:s3:::\"                  -                    Ref: ImageBucket                  - \"/*\"            Principal:              CanonicalUser: !GetAtt ImageHostingOriginaccessidentity.S3CanonicalUserId    ImageHostingCloudFront:    Type: AWS::CloudFront::Distribution    Properties:      DistributionConfig:        Enabled: true        Comment: !Sub ${Prefix}-${Workload}-distribution        DefaultRootObject: index.html        CustomErrorResponses:          - ErrorCode: 400            ResponseCode: 200            ResponsePagePath: \"/error.html\"          - ErrorCode: 403            ResponseCode: 200            ResponsePagePath: \"/error.html\"          - ErrorCode: 404            ResponseCode: 200            ResponsePagePath: \"/error.html\"               Origins:        - Id: ImageBucket          DomainName: !Sub ${ImageBucket}.s3.${AWS::Region}.amazonaws.com               S3OriginConfig:            OriginAccessIdentity: !Join [ \"\", [ \"origin-access-identity/cloudfront/\", !Ref ImageHostingOriginaccessidentity ] ]         DefaultCacheBehavior:                             TargetOriginId: ImageBucket          ViewerProtocolPolicy: redirect-to-https          Compress: false          CachePolicyId: \"4135ea2d-6df8-44a3-9df3-4b5a84be39ad\"          ResponseHeadersPolicyId: \"5cc3b908-e619-4b99-88e5-2cf7f45965bd\"          OriginRequestPolicyId: \"88a5eaf4-2fd4-4709-b370-b4c650ea3fcf\"Enter fullscreen modeExit fullscreen modeSummarySome of the most important lessons I learned in this experience were not about Amazon Bedrock.  Whilst I utilized Bedrock to call our new friend, Sir Claude the third of Haiku, I spent more time working on my prompt than I did calling Bedrock.  Many iterations of this prompt happened and also varying changes between system prompts and multi-modal prompts (The ability to use images and text in a prompt).As we conclude our exploration of Haiku and its role in our photo album application, we're reminded of the boundless possibilities that AI and cloud computing offer in reshaping our digital landscape.  I see this being used in many use cases but I can\u2019t escape the ability to use this to make life easier who may be vision impaired:A guided tour of an art museumAssistive technology when going through daily life such as shopping for groceriesAdvanced screen reading.Some of these use cases can even lead to the betterment of social inclusion for people with disabilities and vision impairment.There are many commercial use cases for Claude 3\u2019s vision capabilities including and not limited to marketing, hospitality, healthcare/wellness and many more but I\u2019ll leave these to your imagination, as the opportunities are endless.What next?Curious to explore the world of Claude 3 Haiku and its applications beyond photo management? Dive deeper into AWS services and AI technologies, and discover the endless possibilities of AI models in enriching our digital experiences. Share your thoughts and insights in the comments on my LinkedIn or below, and let's embark on an AI journey together!https://aws.amazon.com/bedrock/https://www.anthropic.com/news/claude-3-familyhttps://github.com/alanblockley/bedrock-haiku-image-catalog"}
{"title": "GenAI-Powered Digital Threads - A Novel Approach to AI Security, Part I", "published_at": 1710787560, "tags": ["aws", "genai", "security", "ai"], "user": "David Melamed", "url": "https://dev.to/aws-builders/genai-powered-digital-threads-a-novel-approach-to-ai-security-part-i-560g", "details": "Engineering organizations today are becoming increasingly data-reliant.\u00a0 All of our tools and stacks accrue large amounts of data that are distributed among tools and platforms\u2013\u2013from our code and our repos, to our specs and requirements, CI/CD workflows, governance and policies, configurations across clouds, environments, and everything else. This growing amount of data is continuously used throughout our software development lifecycle (SDLC).While organizations become greater data producers and consumers, organizations increasingly are becoming data-driven to make educated decisions with real business impact.\u00a0 However, much like Conway\u2019s Law, as our teams grow more distributed, so do our systems, and ultimately as a byproduct, the data these systems produce. We have data scattered across our organization and stacks.Why does this matter?If our data were able to become more consolidated, with greater communication and visibility we could understand a lot more about cause and effect in our systems, and make decisions that apply to real problems and challenges we\u2019re facing.\u00a0 For example, if we know which repositories are actually running in production, from a security perspective we can know which systems are actually exposed and pose risk to our organization. We can understand what is actually happening in our environments and make the right decisions at the right time.This is because eventually as evolved as security tools have become over the years, these tools still treat all the parts they are intended to scan or monitor the same. This means that all repos are treated equally, all parts of the code, infrastructure, and anything else. This causes these tools to not only be very noisy adding a lot of complexity without having sufficient context, having a hard time distinguishing between real threats and alerts we can really just ignore (or not receive them at all, to begin with). This also creates a lot of cognitive load associated with managing security at scale \u2013\u2013 and where it's valuable to invest effort, and what we can skip.The data however is all there.\u00a0 It\u2019s just a matter of connecting these distributed and scattered data points, to provide us with more helpful and context-based insights about our systems.Borrowing from the World of Manufacturing (Again)In the same way that DevOps borrowed assembly line concepts to streamline development through operations, by removing friction in workflows and pipelines, and adding much-needed automation, there is plenty more to learn from manufacturing to apply to technology concepts.\u00a0 Another concept popular in the world of manufacturing is digital threads.This can basically be summarized as \u200b\u200ba closed loop between digital and physical worlds in order to help optimize products, people, processes, and places. (You can read more about thishere).\u00a0 Connecting these different \u201cendpoints\u201d or threads enables you to have a much more holistic and comprehensive view of your business. This helps to answer the right questions.Let\u2019s take the example of a product defect. In order to track down why this product is being produced with a defect, by connecting data from different departments, disciplines, machines, and resources, it\u2019s possible to understand whether the defect originated in the requirements and design, the Engineering, or in the execution and production. (Starting to see the similarities with our technology stacks?)Knowledge Graphs for Technology ContextIf we take a look at technology stacks, the place that would connect all of our disparate worlds of data is called a knowledge graph, which can be built into a graph database. A graph database is a tool that is very helpful in aggregating data from multiple data sources into a single unified place.Below is an example of what an engineering knowledge graph would look like.This is an example model of a service application that gives us a good understanding of the essential parts of our systems, and the data they produce, consume, and store.\u00a0 In this model, you can see that there is a Github team that owns several repositories. One of the repositories deploys a lambda function through a GitHub Action, where there is a workflow that sits in the repository, and is exposed to the internet because there\u2019s an API gateway in the middle that exposes some endpoints.With such a knowledge graph and diagram, it\u2019s quite easy to distinguish that one repository is exposed to the internet and has a production impact, while the other does not. The hard part is to actually build this graph, particularly because as the graph grows, it\u2019s harder to control the data and queries.GenAI + Graph Database for Human Language Data ManagementWith generative AI (GenAI) becoming all the rage, with a diversity of applications across organizations, it\u2019s no surprise then that another useful application for GenAI is querying useful digital threads compiled in a consolidated knowledge graph. If we\u2019d like to be able to leverage the knowledge graph, without having to learn the entire syntax or language of the graph, as unfortunately there isn't yet one single standard, enter GenAI.Why is GenAI interesting in this context?Well, for starters, GenAI is smart.\u00a0 It\u2019s particularly useful with being taught specific tasks, and evolving this capability. This means we can teach GenAI how to create queries for our graph database.The knowledge graph provides us with the foundations to teach GenAI how to query it in human language. GenAI is known to have hallucinations and sometimes be creative with its answers. However, when coupled with a knowledge graph using structured data, it can have nearly 100% data accuracy.Without this kind of accurate data together with relevant information for example about the type of environment (i.e. dev vs. prod), responses are rarely context-based, and largely vague or generic \u2013\u2013 making understanding risk and mitigation much more difficult. This approach therefore basically brings the benefit of both worlds together, where we can have a way higher confidence in the data, but also ask and receive answers in human language, related to our very own stacks.We can have GenAI orchestrate the querying of the model, which then, in turn, queries the graph, and ultimately can translate the intent from the knowledge graph created to useful and human language output. So, how do we do this without receiving raw JSON?In this example we leveragedAWS Neptunecoupled with GenAI \u2013\u2013 this can be done with OpenAI or one of the models available inAmazon Bedrock(in our example, we picked the brand newClaude 3), where the value add of leveraging the AWS stack is that it comes with several models out of the box. The core of the graph database is built upon the open-source library Langchain, a library containing a module dedicated to query graph databases.We\u2019re Just Getting StartedThis example was built and run pretty simply with a few out-of-the-box tools made available through the newly minted AWS suite of AI services, from Neptune to Bedrock, built to work natively together, and with the open-source AI ecosystem. These worked pretty well to help create a first sample app, and queryable knowledge graph in a digital threads approach, enabling the extraction of important data points in human language for greater context-driven security powered by AI.In our next post, we\u2019ll dive into the architecture and technical resources used to make this possible, walking through an example built upon AWS Neptune with Bedrock, and the open-source tools Langchain and Streamlit.\u00a0 This is a replicable example that will enable you to get started and test drive how you can and should do this at home (just mind the cost!), and gain better insights into your organizational security.Stay tuned for the second more technical post in this two-part series."}
{"title": "Data API for Amazon Aurora Serverless v2 with AWS SDK for Java - Part 4 Working with database transactions", "published_at": 1710779101, "tags": ["aws", "serverless", "java", "database"], "user": "Vadym Kazulkin", "url": "https://dev.to/aws-builders/data-api-for-amazon-aurora-serverless-v2-with-aws-sdk-for-java-part-4-working-with-database-transactions-555m", "details": "IntroductionIn thefirst partof the series we set up our sample application which has API Gateway in front of Lambda functions which communicate with Aurora Serverless v2 PostgreSQL database via Data API to create the products and retrieve them (by id). In thesecond partwe dove dive deeper into the new Data API for Aurora Serverless v2 itself and its capabilities like executing SQL Statements and used AWS SDK for Java for it. In thethird partof the series we explored Data API capabilities to batch SQL statement over an array of data for bulk update and insert operations. In this part of the series we'll look at how to use database transactions with Data API.Working with Data API and its database transactions  capabilitiesTo show the database transaction capability I used the samesample applicationbut added the capabilities to create the user and user address with HTTP PUT /user request (see the CreateUserDataApiFunction Lambda definition in theSAM template). The correspondingCreateUserViaAuroraServerlessV2DataApiHandlerLambda function expects the JSON as HTTP body like{   \"first_name\": \"Vadym\",   \"last_name\":  \"Kazulkin\",   \"email\":  \"blabla@email.com\",   \"address\": {      \"street\": \"Alexandra Platz\",      \"city\": \"Berlin\",      \"country\": \"Germany\",      \"zip\": \"53334\"    } }Enter fullscreen modeExit fullscreen modewhich contains the information about theuseranduser addresswhich will be stored within one transaction in the 2 corresponding separate tables of the PostgreSQL database. In the \"Transactional Example\" in theREADmewhere I described the creation of those tables and sequences.Now let's explore how we can implement this use case using the transactional capabilities of the Data API. For the complete implementation please visit thecreateUserAndAddressTransactionalmethod of theAuroraServerlessV2DataApiDao.java.In order to begin the transaction we need to create BeginTransactionRequest and invokebeginTransactionmethod on the RdsDataClient.final BeginTransactionRequest transactionBeginRequest =  BeginTransactionRequest.builder().database(\"\"). resourceArn(dbClusterArn).secretArn(dbSecretStoreArn).build();  final BeginTransactionResponse transactionBeginResponse =  rdsDataClient.beginTransaction(transactionBeginRequest);Enter fullscreen modeExit fullscreen modeAfter it we need to get thetransaction idfrom the BeginTransactionResponse.String transactionId = transactionBeginResponse.transactionId();Enter fullscreen modeExit fullscreen modeWhen creating all subsequent ExecuteStatementRequests which are part of the same transaction like creating user (as in the example below) and user address in our case we extra set the transaction id by invokingtransactionIdmethod.final ExecuteStatementRequest createUserRequest = ExecuteStatementRequest.builder(). database(\"\").resourceArn(dbClusterArn). secretArn(dbSecretStoreArn).sql(CREATE_USER_SQL). parameters(userIdParam, firstNameParam, lastNameParam, emailParam). transactionId(transactionId).build();Enter fullscreen modeExit fullscreen modeIf the creation of the user and user address within the transaction was successful (no error was thrown) we need to commit the transaction by creating CommitTransactionRequest using the transaction id and invokecommitTransactionmethod on the RdsDataClient.final CommitTransactionRequest transactionCommitRequest = CommitTransactionRequest.builder(). resourceArn(dbClusterArn).secretArn(dbSecretStoreArn). transactionId(transactionId).build();  final CommitTransactionResponse transactionCommitResponse = rdsDataClient.commitTransaction(transactionCommitRequest);Enter fullscreen modeExit fullscreen modeWe can check the commit transaction status by calling.transactionCommitResponse.transactionStatus();Enter fullscreen modeExit fullscreen modeIf the creation of the user or user address within the transaction caused the error we need to rollback the transaction by creating RollbackTransactionRequest using the transaction id and invokerollbackTransactionmethod on the RdsDataClient.final RollbackTransactionRequest transactionRollbackRequest = RollbackTransactionRequest.builder(). resourceArn(dbClusterArn).secretArn(dbSecretStoreArn). transactionId(transactionId).build();  final RollbackTransactionResponse transactionRollbackResponse = rdsDataClient.rollbackTransaction(transactionRollbackRequest);Enter fullscreen modeExit fullscreen modeWe can then check the rollback transaction status by calling.transactionRollbackResponse.transactionStatus();Enter fullscreen modeExit fullscreen modeTo test both scenarios you can do the following :for the successfully executed transaction use the /user path of the created API Gateway and pass the following JSON:{   \"first_name\": \"Vadym\",   \"last_name\":  \"Kazulkin\",   \"email\":  \"blabla@email.com\",   \"address\": {      \"street\": \"Alexandra Platz\",      \"city\": \"Berlin\",      \"country\": \"Germany\",      \"zip\": \"53334\"    } }Enter fullscreen modeExit fullscreen modefor the transaction which won't be successful and will be rollbacked use the /user path of the created API Gateway and pass the following JSON:{   \"first_name\": \"Vadym\",   \"email\":  \"blabla@email.com\",   \"address\": {      \"street\": \"Alexandra Platz\",      \"city\": \"Berlin\",      \"country\": \"Germany\",      \"zip\": \"53334\"    } }Enter fullscreen modeExit fullscreen modeas the last_name is missing as the User property and thelast_namecolumn of thetbl_usertable can't be null, creating the user in the database will cause and error and the transaction will be rollbacked.ConclusionIn this part of the series, we looked at how to use database transactions with Data API. We learned how to use Data API capabilities to begin the transaction and get the transaction id which we use in the subsequent executeStatement(s) requests and then to commit or rollback this transaction.In the next part of the series we'll make some performance measurements of the Data API for the Aurora Serverless v2."}
{"title": "DevOps with Guruu | Chapter 6 : Build a VPC for Work Space | Network Service with AWS", "published_at": 1710776336, "tags": ["webdev", "devops", "aws"], "user": "Hoang Guruu", "url": "https://dev.to/aws-builders/devops-with-guruu-chapter-6-build-a-vpc-for-work-space-network-service-with-aws-54dh", "details": "DevOps with Guruu | Chapter 6 : Build a VPC for Work Space | Network Service with AWS0:00 Welcome to my Series DevOps with Guruu2:25 Create VPC3:28 Create Subnet6:05 Create Internet Gateway7:00 Create Route Table9:00 Create Security Group11:00 Create EC2 Server13:00 Test Connection17:55 Create NAT GatewayJoin me on this journey to mastering DevOps. Let's get hands-on and build a strong foundation together! Don't forget to like, share, and subscribe for more exciting content. Happy coding, future DevOps!\""}
{"title": "Unleash Your Creativity with AWS Bedrock: Afterparty (Day 3)", "published_at": 1710772179, "tags": ["bedrock"], "user": "Danny Chan", "url": "https://dev.to/aws-builders/unleash-your-creativity-with-aws-bedrock-afterparty-day-3-1mmh", "details": "The limitation of Titan model:fake fingerunbalanced eyesoversized sunglassesIf you try to create a generative image, pay attention to those key points."}
{"title": "Compliant infrastructure using infrastructure as code", "published_at": 1710765408, "tags": ["aws", "cloud", "security", "technology"], "user": "Joris Conijn", "url": "https://dev.to/aws-builders/compliant-infrastructure-using-infrastructure-as-code-ghi", "details": "When you are using compute you have a lot of options. One of these options isAmazon EC2. In a world where more and more workloads become serverless. You might still have this use-case that is better off on EC2. But, how do you combine EC2 with compliance and security? In this blog post we will explore how we can build a compliant and secure EC2 stack.Compliance in AWSWhen we talk about compliance we are actually saying AWS Config. AWS Config is the service that enables you to confirm if the infrastructure is compliant. I wrote a blog in the past on how you can writeyour own rules. But out of the box AWS has the AWS Foundational Security Best Practices v1.0.0. This standard comes with a few config rules for EC2:ec2-instance-multiple-eni-check, this control checks if Amazon EC2 instance uses multiple ENI/EFA. This control will pass if single network adapters is used.ec2-paravirtual-instance-check, this control checks if the EC2 virtualization type is paravirtual. The control fails for an EC2 instance ifvirtualizationTypeis set toparavirtual.ec2-instance-no-public-ip, this control checks whether EC2 instances have a public IP address. The control fails if the publicIp field is present in the instance configuration. This control applies to IPv4 addresses only.ec2-imdsv2-check, this control checks if your EC2 instances have IMDSv2 configured. The control passes if HttpTokens is set to required for IMDSv2. The control fails if HttpTokens is set to optional.ec2-instance-managed-by-ssm, this control checks if the EC2 instances are managed by SSMThe first 3 are easy, with a proper VPC design and using sane defaults you will be compliant. But for the last 2 you need to perform some extra actions.Security in AWSSecurity is a broad topic and can include a lot of different things. In this blog I will only focus on the EC2 instance itself. This means that patch management and hardening is in scope. The service that AWS has for this is called AWS Inspector. This tool will scan the EC2 instance for known vulnerabilities. Inspector will report the findings through the AWS Console. It also integrates with Security Hub. Here you can manage all your security related findings from a single place.Inspector can also scan your instance against the CIS hardening baseline.Why should I use infrastructure as code?When you use IaC (infrastructure as code), you create repeatable and predictable infrastructure. Now why is this important? Assume you create an EC2 instance, and you will use the console to launch it. Now AWS Config will start reporting that IMDSv2 is not enforced. And that the instance is not managed by SSM (AWS Systems Manager). This is because you forgot all those extra steps. Humans are not designed to think about all the options that we need to include. IaC is designed to do this for you, you define a template. You deploy the template and the end result is what is defined in the template.If you design your templates to be compliant and secure. The infrastructure that you deploy will be compliant and secure.Let\u2019s build a compliant and secure template!Now my IaC technology of choice is AWS CloudFormation. So the examples that I will present are built in CloudFormation. But, this can also be achieved in any other IaC technology that you want to use.Setting up an instance profileWe want the EC2 instance to be managed by SSM. For this we need to create an IAM Role and an Instance Profile. We will attach 2 managed policies:AmazonSSMManagedInstanceCore, enables the instance to be managed by SSM.AmazonInspector2ManagedCispolicy, enables the ability to perform the CIS baseline scan.Role:    Type: AWS::IAM::Role    Properties:      AssumeRolePolicyDocument:        Version: 2012-10-17        Statement:          - Effect: Allow            Action: sts:AssumeRole            Principal:              Service: ec2.amazonaws.com      ManagedPolicyArns:        - arn:aws:iam::aws:policy/AmazonSSMManagedInstanceCore        - arn:aws:iam::aws:policy/AmazonInspector2ManagedCispolicy   InstanceProfile:    Type: AWS::IAM::InstanceProfile    Properties:      Roles:        - !Ref RoleEnter fullscreen modeExit fullscreen modeSetting up the EC2 InstanceWhen you launch an EC2 instance we will be needing a security group. We also want our EC2 instance to be managed by SSM. To SSM Agent comes pre-installed in Amazon Linux 2. Needs to be able to connect outbound via HTTPS, for this reason we add 0.0.0.0/0 on port 443 on the egress traffic.We also want to make IMDSv2 required. We can configure this in the launch templateMetadataOptions. To ensure that all security updates are installed during the first boot. We include theyum update -y --securityoption in the user-data.You can now use the launch template to create an EC2 instance. Or, use an auto scaling group to launch one or more EC2 instances.InstanceSecurityGroup:    Type: AWS::EC2::SecurityGroup    Properties:      GroupDescription: Security group for the test instance      VpcId: \"{{resolve:ssm:/landingzone/vpc/vpc-id}}\"      SecurityGroupEgress:        - Description: Allow outbound connectivity to port 443.          IpProtocol: tcp          FromPort: 443          ToPort: 443          CidrIp: 0.0.0.0/0   LaunchTemplate:    Type: AWS::EC2::LaunchTemplate    Properties:      LaunchTemplateData:        IamInstanceProfile:          Arn: !GetAtt InstanceProfile.Arn        ImageId: \"{{resolve:ssm:/aws/service/ami-amazon-linux-latest/amzn2-ami-hvm-x86_64-gp2}}\"        InstanceType: t3.micro        SecurityGroupIds:          - !Ref InstanceSecurityGroup        MetadataOptions:          HttpTokens: required        UserData:          Fn::Base64: !Sub |-            #!/bin/bash -x            yum update -y --security   Instance:    Type: AWS::EC2::Instance    Properties:      LaunchTemplate:        LaunchTemplateId: !GetAtt LaunchTemplate.LaunchTemplateId        Version: !GetAtt LaunchTemplate.LatestVersionNumber      SubnetId: \"{{resolve:ssm:/landingzone/vpc/private-subnet-1-id}}\"Enter fullscreen modeExit fullscreen modeSpinning up the instanceIf you combine the two snippets you can spin-up a compliant EC2 instance. This instance has all the latest security patches installed. But when you look into Inspector you will notice that there are still some findings reported.The findings that popped up during the time of writing were kernel related. So for me to fix this I need to update the kernel. When you lookup your EC2 instance in the console you will notice theConnectbutton. This will create a shell session to your instance:Updating the kernelFirst, let\u2019s see what kernel we have running:s\u200b\u200bh-4.2$ uname -r 4.14.336-253.554.amzn2.x86_64 sh-4.2$Enter fullscreen modeExit fullscreen modeNow let\u2019s see what kernels are available:sh-4.2$ amazon-linux-extras | grep kernel 49 kernel-5.4 available [=stable] 55 kernel-5.10 available [=stable] 62 kernel-5.15 available [=stable] sh-4.2$Enter fullscreen modeExit fullscreen modeWe will install the latest version of the kernel 5.15 with the following command:sh-4.2$ sudo amazon-linux-extras install kernel-5.15Enter fullscreen modeExit fullscreen modeBecause a kernel update requires a reboot we need to reboot the instance:sudo rebootEnter fullscreen modeExit fullscreen modeAll done!Avoiding the kernel updateThe problem with the kernel update is the required reboot. You only have two options to remove this step:Create your own up to date base AMI.Use the Amazon Linux 2023 AMI with a newer kernel version.Maintaining a base AMI comes with a lot of extra overhead. So my recommendation is to always use the newer version that uses the newer kernel versions.But when you need to harden your images using the CIS benchmarks. You might need to start maintaining your own images anyway. There areCIS Hardened imagesavailable through the marketplace for an extra charge.ConclusionIaC (Infrastructure as Code) can help you spin up compliant and secure infrastructure. It removes the human aspect of forgetting to configure certain options. Using IaC you will have a predictable outcome every time you spin up the same template. This makes it reliable and is less time consuming. As a bonus you are not triggering any config rules to be marked as non compliant.Photo byHitesh ChoudharyThe postCompliant infrastructure using infrastructure as codeappeared first onXebia."}
{"title": "Three Tips for Writing CDK Tests", "published_at": 1710764722, "tags": ["aws", "awscdk", "testing", "learning"], "user": "Dakota Lewallen", "url": "https://dev.to/aws-builders/three-tips-for-writing-cdk-tests-53fk", "details": "1. Use the assertions moduleSomething of a no-brainer, but use the tools you\u2019re given! One of the best parts of the CDK library is the assertions module. It contains two sections. One for \u201cfine-grained\u201d assertions, like what you would write during unit testing. The other is for performing \u201csnapshot\u201d tests, in which you compare the new template against one that is already deployed. It\u2019s a section of the library that can be easily missed but provides so much value when used correctly.2. Make any Nested Stacks easily accessibleOne of the benefits of using CDK is the ability to create declarative dependency trees. But, testing the implemented Nested Stacks can be tricky. One of the best ways I\u2019ve found to simplify this is to add Nested Stacks as properties of the Stack class that handles deploying them. For example\u2026importSubStackfrom'substack-stack.ts';exportclassTopStackextendsStack{constructor(scope,id,props){constsubStack=newSubStack(this,'substack',props);}}Enter fullscreen modeExit fullscreen modeIs pretty standard. While it is possible to test this, you can simplify the process by \u201cstashing\u201d a reference.importSubStackfrom'substack-stack.ts';exportclassTopStackextendsStack{subStack:SubStack;constructor(scope,id,props){this.subStack=newSubStack(this,'substack',props);}}Enter fullscreen modeExit fullscreen modeNow accessing this for fine-grained assertions can look like\u2026import{Capture,Match,Template}from\"aws-cdk-lib/assertions\";import*ascdkfrom\"aws-cdk-lib\";import{TopStack}from\"top-stack\";describe(\"TopStack\",()=>{test(\"has the substack\",()=>{constapp=newcdk.App();consttopStack=newTopStack(app,\"topStack\",{});const{subStack}=topStack;// we pull the substack through the property// Pass the subStack to template.consttemplate=Template.fromStack(subStack);}}Enter fullscreen modeExit fullscreen mode3.  Write your template to a file (sometimes)While writing assertions, you can find situations where the tests are failing for reasons that may not make sense. When this happens, one of the first things I will do is write the CloudFormation template out into a file. This way you can get a cold hard copy of what it is you are building in your hand. Rather than trying to parse through complex logs or mysterious error messages. Not a guarantee but often getting to see the complete output can solve many common testing pitfalls.ConclusionTesting is great! Testing your infrastructure\u2026 Even better! If you\u2019re just getting started with CDK or if you\u2019ve been around since the early days, I hope these tips can help you along your journey! If you have any tips you'd like to add, share them in the comments. Always looking to learn more!Find me onLinkedIn|Github"}
{"title": "Prot\u00e9gez vos apps des attaques avec AWS CloudFront et AWS WAF", "published_at": 1710748800, "tags": [], "user": "Paul SANTUS", "url": "https://dev.to/aws-builders/protegez-votre-infrastructure-sur-site-avec-aws-cloudfront-et-aws-waf-2f5h", "details": "Je travaille sur la migration vers AWS de mon client, e-commer\u00e7ant, quand je re\u00e7ois un coup de fil : \u00ab Paul, on est mal, \u00e7a va fait une semaine que notre site est attaqu\u00e9 par d\u00e9ni de service ; on perd du CA ! Est-ce que tu as une solution ? \u00bbPas question de b\u00e2cler la fin de la migration. Le client n'a pas encore conteneuris\u00e9 son appli, on n'a pas fait de test de migration de donn\u00e9es, et encore moins de test de charge. Maiscomme je l'avais indiqu\u00e9 dans un pr\u00e9c\u00e9dent blog post, le Cloud peut aussi rendre des services \u00e0 l'infrastructure on-premise. C'est le moment de le prouver !Les premi\u00e8res analyses faites r\u00e9v\u00e8lent que l'attaque vient de multiples IPs (points de sortie du r\u00e9seau TOR) et cible la page de connexion. Pas de chance, cette page fait des appels en base de donn\u00e9es et celle-ci est satur\u00e9e, entra\u00eenant de la latence (puis une absence de r\u00e9ponse) sur l'ensemble du syst\u00e8me.Ni une, ni deux, je me mets au travail. Gr\u00e2ce \u00e0 Terraform, en une demi-journ\u00e9e, j'ai une stack fonctionnelle en test.La stack techniqueVoici un sch\u00e9ma de la stack technique d\u00e9ploy\u00e9e pour contrer l'attaque que subit mon client :Les principales modifications par rapport \u00e0 l'existant sont les suivantes :Au lieu de renvoyer directement sur l'infra sur site de mon client, le DNS va d\u00e9sormais renvoyer les requ\u00eates vers CloudFront.CloudFrontest un Content-Delivery Network (CDN) manag\u00e9. C'est \u00e0 dire qu'il permet de servir du contenu mis en cache (ou pas) depuis des localisations proches des clientsPendant l'incident, dans un premier temps, ce n'est pas la fonctionnalit\u00e9 de cache (qui permet de r\u00e9duire la charge sur les serveurs, et la latence c\u00f4t\u00e9 client) que la possibilit\u00e9 d'exposer un frontal HTTPS qui va s'interposer entre les clients et l'infrastructure de mon client.Avant de relayer les requ\u00eates vers mon \"origine\" (l'infra existante), CloudFront va les passer \u00e0 AWS WAFWAFest un Web Application Firewall, qui permet l'inspection des requ\u00eates HTTP.Sur le WAF, nous avons param\u00e9tr\u00e9 un certain nombre de r\u00e8gles en nous appuyant surles jeux de r\u00e8gles g\u00e9r\u00e9es d'AWS. Voici celles qui ont \u00e9t\u00e9 le plus utiles pour stopper l'attaque :Le groupe de r\u00e8glesAWSManagedRulesAnonymousIpListcontient une r\u00e8glequi recense pr\u00e9cis\u00e9ment les IPs de sortie connues du r\u00e9seau TOR ainsi que des VPNs les plus fr\u00e9quemment utilis\u00e9s, et une seconde recensant les h\u00e9bergeurs (dont les clients ont pu se faire corrompre une machine, la transformant en zombie). Cette r\u00e8gle va faire 95% du job.La secondeAWSManagedRulesATPRuleSetpermetpr\u00e9cis\u00e9ment de prot\u00e9ger les pagesde connexion, en analysant les requ\u00eates qui sont faites : comportent-elles l'ensemble des champs de formulaires attendus ? une IP qui a d\u00e9j\u00e0 \u00e9chou\u00e9 \u00e0 l'authentification persiste-t-elle de fa\u00e7on \u00e9trange ?En compl\u00e9ment de ces r\u00e8gles, par mesure de prudence, on mets en place les r\u00e8gles classiques : pr\u00e9vention des injections SQL, d'attaques sur faille PHP, un anti-top10 OWASP etc.Enfin, on ajoute une r\u00e8gle permet de whitelister des IPs (le mod\u00e8le \u00e9conomique de notre e-commer\u00e7ant impliquant un trafic assez important depuis quelques partenaires).Mise en oeuvre et r\u00e9sultatNous avons bascul\u00e9 c\u00f4t\u00e9 AWS, sur leservice Route53, la zone DNS principale du domaine de mon client (coup de chance, tout le travail pr\u00e9paratoire de recensement avait \u00e9t\u00e9 effectu\u00e9 en amont). Cela apporte au moins deux b\u00e9n\u00e9fices :L'automatisation qu'offre Route53, en lien avec Terraform, permet de g\u00e9n\u00e9rer rapidement les entr\u00e9es DNS n\u00e9cessaires pour que le service Certificate Manager accepte de g\u00e9n\u00e9rer des certificats SSL au nom du domaine de mon client.Le service permet de d\u00e9finir un enregistrement \u00ab A \u00bb dynamique (unalias) \u00e0 la racine du domaine, alors que la RFC 1034 ne permet pas de positionner un CNAME (qui ne peut co-exister avec d'autres enregistrements) \u00e0 la racine.Nous avons cr\u00e9\u00e9 dans cette zone des enregistrements de typeorigine.mondomaine.fret mon client a fait le n\u00e9cessaire pour que son serveur web traite les requ\u00eates sur cette adresse en servant son application (y compris avec un certificat TLS).Une fois cela test\u00e9, nous avons bascul\u00e9 le traffic associ\u00e9 aux urlsmondomaine.fretapi.mondomaine.frvers CloudFront.Pour \u00e9viter le contournement (au cas o\u00f9 le pirate d\u00e9couvrirait les urls enorigineou simplement utiliserait directement l'IP du serveur de mon client), CloudFront est param\u00e9tr\u00e9 pour envoyer un header \"secret\" \u00e0 chaque appel de l'infra de mon client. Il devient beaucoup plus simple pour celui-ci de filtrer quelqu'un qui contournerait le CDN.Le r\u00e9sultat est imm\u00e9diat : \u00e0 20h nous faisons la bascule. Le site redevient pleinement disponible. A 21h le pirate cesse l'attaque (avant de retenter sa chance le lendemain)Sur l'image ci-dessous on peut voir en orange le trafic passant, l\u00e9gitime, et en bleu le trafic bloqu\u00e9. Nous avions donc 6000 requ\u00eates par minute, soit plus de deux fois le trafic habituel :Co\u00fbt : un petit point d'attention !WAF co\u00fbte $0,60 par million de requ\u00eates analys\u00e9es \u00e0 l'aide des r\u00e8gles manag\u00e9es de base (groupe dont font partie toutes nos r\u00e8glessauf une) soit moins de $5 par jour pour mon client.Attention cependant, les r\u00e8gles avanc\u00e9es comme l'Account Takeover Protection sont factur\u00e9es (apr\u00e8s un tier gratuit de 10000 appels) $1 pour 1000 (oui, 1000, pas 1 000 000) d'appels.Et au d\u00e9but, notre param\u00e9trage donnait cela :En 24h, on a br\u00fbl\u00e9 pour $700 de WAF. Heureusement, votre serviteur avait mis en place une alarme d'anomalie de co\u00fbt ! Un ticket au support (cat\u00e9gorie \u00abdispute a charge\u00bb) et AWS nous a fait gr\u00e2ce de la facture ! [Nb : dans mon exp\u00e9rience, AWS efface toujours les ardoises \u00e9lev\u00e9es provenant d'erreurs de param\u00e9trage ; cette tr\u00e8s bonne politique commerciale est l'une des raisons, avec la qualit\u00e9 de leur support, qui en font mon cloud pr\u00e9f\u00e9r\u00e9].Bref, on a corrig\u00e9 en pla\u00e7ant la r\u00e8gle d'ATP en derni\u00e8re position par ordre de priorit\u00e9 et, surtout, en conditionnant son ex\u00e9cution \u00e0 la pr\u00e9sence d'une \u00e9tiquette pos\u00e9e par une autre r\u00e8gle qui identifie les requ\u00eates sur le chemin/connexionSoulagement tout de m\u00eame quand on voit le trafic passant par la r\u00e8gle ATP descendre !Un b\u00e9n\u00e9fice suppl\u00e9mentaire de CloudfrontApr\u00e8s le repos du guerrier heureux d'avoir bloqu\u00e9 son ennemi, il est l'heure d'ajouter un b\u00e9n\u00e9fice suppl\u00e9mentaire pour mon client : l'activation du cache pour toutes les ressource statiques servies par l'application.Gr\u00e2ce \u00e0 Terraform, \u00e7a n'est pas tr\u00e8s compliqu\u00e9 : le bloc suivant permet de cacher tous les gifs.ordered_cache_behavior {     path_pattern             = \"*.gif\"     allowed_methods          = [\"GET\", \"HEAD\", \"OPTIONS\", \"PUT\", \"POST\", \"PATCH\", \"DELETE\"]     cached_methods           = [\"GET\", \"HEAD\", \"OPTIONS\"]     target_origin_id         = local.origin_domain     viewer_protocol_policy   = \"redirect-to-https\"     cache_policy_id          = aws_cloudfront_cache_policy.cachingoptimizez_with_v_header.id     origin_request_policy_id = \"b689b0a8-53d0-40ab-baf2-68738e2966ac\" #Hard-Coded: Forward all headers EXCEPT HOST, cookies and query strings   }Enter fullscreen modeExit fullscreen modeL\u00e0 aussi, l'effet est imm\u00e9diat. Quelques minutes apr\u00e8s c'est pr\u00e8s de 90% des requ\u00eates qui seront servies par CloudFront, all\u00e9geant ainsi l'infra de mon client d'une charge certaine et am\u00e9liorant le temps de chargement pour les clients !Avant de nous quitterSi vous avez besoin d'aide pour migrer sur le Cloud, aider vos \u00e9quipes de dev \u00e0 s'approprier les nombreux services disponibles, n'h\u00e9sitez pas \u00e0 me contacter parLinkedInoumon site web."}
{"title": "Amazon Managed Service for Prometheus (AMP) with CloudFormation", "published_at": 1710735764, "tags": ["devops", "aws", "tutorial", "cloud"], "user": "Ehi Enabs", "url": "https://dev.to/aws-builders/amazon-managed-service-for-prometheus-amp-with-cloudformation-d7h", "details": "Deploying Amazon Managed Service for Prometheus (AMP) with CloudFormation.Amazon Managed Service for Prometheus (AMP) simplifies Prometheus's deployment, management, and scaling.Prometheus is an open-source monitoring and alerting toolkit. It has a robust querying language, a powerful data model, and extensive integrations. Prometheus helps engineers gain insights into the health and performance of their applications and systems by offering a thorough solution for monitoring infrastructure, applications, and services. From collecting metrics and generating alerts based on predefined thresholds, to its scalability, reliability, and community support, Prometheus is a significant pillar of observability.Why Amazon Managed Service for Prometheus (AMP)?AMP helps reduce the hassle of managing Prometheus infrastructure by handling the heavy lifting. Tasks such as provisioning network, storage, and computing resources for the deployment of Prometheus, scaling, upgrades and patches, high availability, etc., are all managed by AWS.That means you and your team can use those valuable insights to improve your applications. Plus, AMP plays super nicely with other AWS services and offers a pay-as-you-go setup, making it the go-to for organizations wanting top-notch Prometheus monitoring on AWS.Deploying Amazon Managed Service for Prometheus (AMP) with CloudFormationAWS CloudFormation, is a tool that enables developers to define their infrastructure as code (IaC). With CloudFormation, you can describe your AWS resources in a simple, declarative template, specifying everything you need, from EC2 instances to S3 buckets and IAM roles. Once defined, CloudFormation provides and configures the resources, ensuring consistency and repeatability across environments.A CloudFormation template is a JSON or YAML file that defines the AWS resources and their configurations needed to deploy an application or infrastructure stack.CloudFormation templates are written in a declarative format, specifying the desired state of the infrastructure rather than the steps required to achieve that state.The following is a CloudFormation Template for deploying AMPAWSTemplateFormatVersion:\"2024-09-09\"Description:\"AmazonManagedServiceforPrometheus(AMP)DeploymentExample\"Parameters:WorkspaceName:Description:\"NameforthePrometheusworkspace\"Type:StringDefault:\"MyPrometheusWorkspace\"Resources:PrometheusWorkspace:Type:AWS::Prometheus::WorkspaceProperties:WorkspaceName:!RefWorkspaceNameRetention:30# Retention period for metrics (in days)DataSources:-Type:\"CloudWatch\"Region:!RefAWS::RegionAssumeRoleArn:!GetAttIAMRole.ArnWorkspaceDescription:\"ManagedPrometheusworkspaceformonitoringapplications\"IAMRole:Type:AWS::IAM::RoleProperties:AssumeRolePolicyDocument:Version:\"2024-10-17\"Statement:-Effect:AllowPrincipal:Service:[prometheus.amazonaws.com]Action:[\"sts:AssumeRole\"]Policies:-PolicyName:\"PrometheusDataAccessPolicy\"PolicyDocument:Version:\"2024-10-17\"Statement:-Effect:AllowAction:-\"cloudwatch:GetMetricData\"-\"cloudwatch:GetMetricStatistics\"-\"cloudwatch:ListMetrics\"Resource:\"*\"Outputs:PrometheusWorkspaceName:Description:\"NameofthecreatedPrometheusworkspace\"Value:!RefPrometheusWorkspaceIAMRoleArn:Description:\"ARNoftheIAMroleusedbyPrometheusworkspace\"Value:!GetAttIAMRole.ArnEnter fullscreen modeExit fullscreen modeAfter creating the template, you can deploy the AMP stack with aws cli by using the following command;aws cloudformation create-stack--stack-nameMyAMPStack--template-bodyfile://amp-deployment.yaml--parametersParameterKey=WorkspaceName,ParameterValue=MyPrometheusWorkspaceEnter fullscreen modeExit fullscreen modeBenefits of Deploying AMP with CloudFormationDeploying AMP with CloudFormation helps simplify the process and reduces the manual labour required. Other benefits includesInfrastructure as Code (IaC): CloudFormation allows you to define your infrastructure in a declarative template, enabling you to treat infrastructure as code. This approach enhances consistency, repeatability, and version control, as infrastructure changes can be tracked along with application code.Automated Provisioning: With CloudFormation, you can automate the provisioning of AMP resources, including Prometheus workspaces and IAM roles. This eliminates manual intervention and reduces the risk of errors during deployment.Simplified Deployment: CloudFormation abstracts the complexity of infrastructure management, providing a simple and standardized way to deploy AMP. You can define all the necessary resources and configurations in a single template, making deployment straightforward and repeatable.Integration with AWS Ecosystem: CloudFormation seamlessly integrates with other AWS services, allowing you to incorporate AMP into your existing AWS environment effortlessly. You can leverage CloudFormation features like parameterization and resource dependencies to create highly customizable and scalable deploymentsResource Management: CloudFormation provides centralized management and tracking of AMP resources, making it easy to monitor, update, and delete resources as needed. You can track changes, view stack history, and roll back to previous configurations if necessary, providing greater control and visibility over your infrastructure.ConclusionDeploying Amazon Managed Service for Prometheus (AMP) with AWS CloudFormation streamlines the process of setting up Prometheus monitoring on AWS. By abstracting the complexities of infrastructure management, AMP enables teams to leverage monitoring insights to optimize application performance and reliability while reducing the manual labour and time."}
{"title": "Python: Part 2 - Dictionaries with demos", "published_at": 1710720835, "tags": ["python", "dictionary"], "user": "Srinivasulu Paranduru", "url": "https://dev.to/aws-builders/part-2-python-programming-python-102-139", "details": "Dictionaries in Python:1. DictionariesTry the commands in IDLE ShellmyCat = {'size':'fat','color':'gray','disposition':'loud'} myCat['size'] #Output : fatEnter fullscreen modeExit fullscreen mode'My cat has ' + myCat['color'] + ' fur.'output : 'My cat has gray fur.'Comparing the lists[1,2,3] == [3,2,1] #FalseEnter fullscreen modeExit fullscreen modeComparing the dictionaries with same key value pairs in different orderinfo = {'size':'fat','color':'gray','disposition':'loud'} info1 = {'color':'gray','disposition':'loud','size':'fat'} info == info1 >Output:  TrueEnter fullscreen modeExit fullscreen modecheck the keys existing in a dictionary'color' in info #True 'color' not in info #FalseEnter fullscreen modeExit fullscreen mode1.1 Dictionary methods - keys(),values() and items()1.1.1 Try the command:list(info.keys())Enter fullscreen modeExit fullscreen modeOutput:['size', 'color', 'disposition']1.1.2 Try the command:info.values()Enter fullscreen modeExit fullscreen modeOutput:dict_values(['fat', 'gray', 'loud'])1.1.3 Try the command:list(info.values())Enter fullscreen modeExit fullscreen modeOutput['fat', 'gray', 'loud']1.1.4 Try the command:info.items()Enter fullscreen modeExit fullscreen modeOutput:dict_items([('size', 'fat'), ('color', 'gray'), ('disposition', 'loud')])1.1.5 Try the command:list(info.items())Enter fullscreen modeExit fullscreen modeOutput:[('size', 'fat'), ('color', 'gray'), ('disposition', 'loud')]Dictionary with for loop1.1.6 Try the command -for k in info.keys() :   print(k)Enter fullscreen modeExit fullscreen modeoutput:sizecolordisposition1.1.7 Try the command -for v in info.values() :    print(v)Enter fullscreen modeExit fullscreen modeoutput:fatgrayloud1.1.8 Try the command -for k,v in info.items():     print(k,v)Enter fullscreen modeExit fullscreen modeoutput:size fatcolor graydisposition loud1.1.9 Try the command -for i in info.items():    print(i)Enter fullscreen modeExit fullscreen modeoutput:('size', 'fat')('color', 'gray')('disposition', 'loud')1.1.10 Try the command -'fat' in info.values()Enter fullscreen modeExit fullscreen modeoutput: True1.2 The get() Dictionary MethodPre-requisties : Set the variable info with the dictionary valueinfo = {'size':'fat','color':'gray','disposition':'loud'}Enter fullscreen modeExit fullscreen mode1.2.1if 'color' in info:       print(info['color'])   #output:  grayEnter fullscreen modeExit fullscreen mode1.2.2if 'dress' in info :       print(info['dress']) #output:  ''Enter fullscreen modeExit fullscreen mode1.2.3 : If the key - size exists it returns its value else empty string will be returnedinfo.get('size','')Enter fullscreen modeExit fullscreen modeoutput : 'fat'1.2.4: If the key - size it returns its value else empty string will be returnedinfo.get('size','medium')Enter fullscreen modeExit fullscreen modeoutput  : ''1.2.5picnicItems = {'apples':5,'cups':2}  print('I am bringing ' + str(picnicItems.get('napkins',0)) +' to the picnic.')Enter fullscreen modeExit fullscreen modeoutput: I am bringing 0 to the picnic.1.2.6picnicItems = {'apples':5,'cups':2}  print('I am bringing ' + str(picnicItems['napkins']) +' to the picnic.')Enter fullscreen modeExit fullscreen modeoutput: Error messageTraceback (most recent call last):File \"\", line 1, inprint('I am bringing ' + str(picnicItems['napkins']) +' to the picnic.')KeyError: 'napkins'1.2.7 : setdefault()eggs = {'name':'Zophie','species':'cat','age':8} eggs.setdefault('color','black') #output: 'black' eggs #output: {'name': 'Zophie', 'species': 'cat', 'age': 8, 'color': 'black'}  #if the default existing for the color key then the value won't be changed. eggs.setdefault('color','orange') #output: 'black' eggs {'name': 'Zophie', 'species': 'cat', 'age': 8, 'color': 'black'}Enter fullscreen modeExit fullscreen mode1.2.8: character count program1.2.8.1message = 'It was a bright cold day in April, and the clocks were striking thirteen.' count = {}  for character in message :     count.setdefault(character,0)     count[character]=count[character]+1  print(count)Enter fullscreen modeExit fullscreen modeoutput:{'I': 1, 't': 6, ' ': 13, 'w': 2, 'a': 4, 's': 3, 'b': 1, 'r': 5, 'i': 6, 'g': 2, 'h': 3, 'c': 3, 'o': 2, 'l': 3, 'd': 3, 'q': 1, 'y': 1, 'n': 4, 'A': 1, 'p': 1, ',': 1, 'e': 5, 'k': 2, '.': 1}1.2.8.2 : convert the message to upper case and print the countmessage = 'It was a bright cold day in April, and the clocks were striking thirteen.' count = {}  for character in message.upper() :     count.setdefault(character,0)     count[character]=count[character]+1  print(count)Enter fullscreen modeExit fullscreen modeoutput:{'I': 7, 'T': 6, ' ': 13, 'W': 2, 'A': 5, 'S': 3, 'B': 1, 'R': 5, 'G': 2, 'H': 3, 'C': 3, 'O': 2, 'L': 3, 'D': 3, 'Q': 1, 'Y': 1, 'N': 4, 'P': 1, ',': 1, 'E': 5, 'K': 2, '.': 1}1.2.8.3 : comment the line count.setdefault(character,0) and check the response.message = 'It was a bright cold day in April, and the clocks were striking thirteen.' count = {}  for character in message.upper() :     # count.setdefault(character,0)      count[character]=count[character]+1  print(count)Enter fullscreen modeExit fullscreen modeoutput:Traceback (most recent call last):File \"C:/Srinivas/3 Code/PythonPrograms/charactercount.py\", line 6, incount[character]=count[character]+1KeyError: 'I'count.setdefault(character,0)This line will set the count of the character to 0 for the first time, if the variable does not exists.1.2.8.4: Big message is assigned to the variable message and its using ''' at the start and end of the message.message = '''Ensure your post has a cover image set to make the most of the home feed and social media platforms. Share your post on social media platforms or with your co-workers or local communities. Ask people to leave questions for you in the comments. It's a great way to spark additional discussion describing personally why you wrote it or why people might find it helpful.'''  count = {}  for character in message.upper() :     count.setdefault(character,0)     count[character]=count[character]+1  print(count)Enter fullscreen modeExit fullscreen modeoutput:{'E': 30, 'N': 12, 'S': 24, 'U': 10, 'R': 19, ' ': 62, 'Y': 9, 'O': 35, 'P': 11, 'T': 23, 'H': 11, 'A': 22, 'C': 9, 'V': 2, 'I': 21, 'M': 13, 'G': 4, 'K': 4, 'F': 7, 'D': 9, 'L': 14, '.': 4, '\\n': 2, 'W': 6, '-': 1, 'Q': 1, \"'\": 1, 'B': 1}1.2.8.5 : Import pprint moduleimport pprint message = '''Ensure your post has a cover image set to make the most of the home feed and social media platforms. Share your post on social media platforms or with your co-workers or local communities. Ask people to leave questions for you in the comments. It's a great way to spark additional discussion describing personally why you wrote it or why people might find it helpful.'''  count = {}  for character in message.upper() :     count.setdefault(character,0)     count[character]=count[character]+1  pprint.pprint(count)Enter fullscreen modeExit fullscreen modeoutput:=============================={'\\n': 2,' ': 62,\"'\": 1,'-': 1,'.': 4,'A': 22,'B': 1,'C': 9,'D': 9,'E': 30,'F': 7,'G': 4,'H': 11,'I': 21,'K': 4,'L': 14,'M': 13,'N': 12,'O': 35,'P': 11,'Q': 1,'R': 19,'S': 24,'T': 23,'U': 10,'V': 2,'W': 6,'Y': 9}1.2.8.5 : using pprint module and using pformatimport pprint message = '''Ensure your post has a cover image set to make the most of the home feed and social media platforms. Share your post on social media platforms or with your co-workers or local communities. Ask people to leave questions for you in the comments. It's a great way to spark additional discussion describing personally why you wrote it or why people might find it helpful.'''  count = {}  for character in message.upper() :     count.setdefault(character,0)     count[character]=count[character]+1  stext=pprint.pformat(count) print(stext)Enter fullscreen modeExit fullscreen modeoutput:{'\\n': 2,' ': 62,\"'\": 1,'-': 1,'.': 4,'A': 22,'B': 1,'C': 9,'D': 9,'E': 30,'F': 7,'G': 4,'H': 11,'I': 21,'K': 4,'L': 14,'M': 13,'N': 12,'O': 35,'P': 11,'Q': 1,'R': 19,'S': 24,'T': 23,'U': 10,'V': 2,'W': 6,'Y': 9}1.2.9 : List of dictionaries>>> allCats=[] >>> allCats.append({'name':'Zophie','age':7,'color':'gray'}) >>> allCats.append({'name':'Pooka','age':5,'color':'gray'}) >>> allCats.append({'name':'Fat-tail','age':4,'color':'red'}) >>> allCats [{'name': 'Zophie', 'age': 7, 'color': 'gray'}, {'name': 'Pooka', 'age': 5, 'color': 'gray'}, {'name': 'Fat-tail', 'age': 4, 'color': 'red'}]Enter fullscreen modeExit fullscreen modeRecap :Dictionaries contain key-value pairs. Keys are like a list's indexes.Dictionaries are mutable. Variables hold references to dictionary values, not the dictionary value itself.Dictionaries are unordered. There is no \"first\" key-value pair in a dictionary.The keys(), values(), and items() methods will return list-like values of a dictionary's keys, vaues, and both keys and values, respectively.The get() method can return a default value if a key doesn't exist.The setdefault() method can set a value if a key doesn't exist.The pprint module's pprint() \"pretty print\" function can display a dictionary value cleanly. The pformat() function returns a string value of this output.Conclusion : Discussed about python - dictionaries used IDLE shell command for running the python code\ud83d\udcac If you enjoyed reading this blog post and found it informative, please take a moment to share your thoughts by leaving a review and liking it \ud83d\ude00 and share this blog with ur friends and follow me inlinkedin"}
{"title": "How To Implement AWS SSB Controls in Terraform - Part 1", "published_at": 1710714237, "tags": ["aws", "terraform", "security"], "user": "Anthony Wat", "url": "https://dev.to/aws-builders/how-to-implement-aws-ssb-controls-in-terraform-part-1-3ggm", "details": "IntroductionIn my AWS consultant role, I have helped numerous organizations with building a landing zone or reviewing their existing AWS environment against best practices and recommend remediations, with a focus on security. Many of them happen to be in the early stage of adopting AWS due to a recent migration or starting a new business. These companies look to us in guiding them towards best practices when they have limited time and resources to learn on their own.As a big fan of frameworks and prescriptive guidance from AWS, I often spend hours browsing through them for inspirations to improve my deliverables. Early on, I came across theAWS Security Baseline (SSB)which I found immensely suitable for the profile of customers I work with. I then started incorporating it into my toolset, particularly the Terraform templates that I deploy landing zones with. Given the effectiveness of the framework, I wanted to share my experience with the AWS community and explain how you too can add these security controls into your Terraform configuration.Since I will be going through each control in details, I organized the write-up into ablog seriesconsisting of four blog posts so that it is easier to follow. The first two will cover the account controls, while the latter two will cover the workload controls. With that said, let's first look at what the AWS SSB is about.What is the AWS Startup Security Baseline (SSB)?TheAWS Startup Security Baseline (SSB)comprises a collection of controls designed to establish a foundational level of security for startups and organizations in early phase of adopting AWS, while maintaining their agility on AWS. There are two types of controls in the AWS SSB:Account controls, which help keep your AWS account secure.Workload controls, which help secure your workloads and data in the cloud.These controls can be considered \"low-hanging fruits\" that are easy to implement but provides a decent level of security, which is perfect for organizations that are still navigating through the intricacy of managing an AWS environment and figuring out an operational model. These controls also carry over to a multi-account model as your organization's AWS usage matures.If you practice DevOps and leverage Infrastructure-as-Code (IaC), you'd be please to learn that these controls can easily be implemented in IaC including Terraform which this blog series focuses on. We will dive into each control and how it can be defined in Terraform configurations. Let's start with the first account control ACCT.01, which covers setting account-level contacts to valid email distribution lists.ACCT.01 \u2013 Set Account-Level ContactsThe account controlACCT.01requires that account-level primary and alternate contacts be set, ideally with email distribution lists.Setting contacts is generally a one-time task, so it is often done in the AWS Management Console. However, it can also be done easily in Terraform using theaws_account_primary_contactresourceand theaws_account_alternate_contactresource. The following example demonstrates how to set the primary contact and the alternate billing contact:# Please don't fact check the contact info!resource\"aws_account_primary_contact\"\"this\"{address_line_1=\"742 Evergreen Terrace\"city=\"Springfield\"company_name=\"Mr. Plow\"country_code=\"US\"district_or_county=\"Pressboard Estates\"full_name=\"Homer Simpson\"phone_number=\"+16365553226\"postal_code=\"49007\"state_or_region=\"NT\"website_url=\"https://www.mrplow.com\"}resource\"aws_account_alternate_contact\"\"billing\"{alternate_contact_type=\"BILLING\"name=\"Marge Simpson\"title=\"CFO\"email_address=\"finance@mrplow.com\"phone_number=\"+16365553226\"}Enter fullscreen modeExit fullscreen mode\ud83d\udca1For a multi-account landing zone with many member accounts, consider using plus addressing (a.k.a. sub-addressing) to reduce the number of required email distribution list. You can either define plus addressing by account (for example,awsaccount1+billing@example.com,awsaccount1+security@example.com, etc.) or by function (for example,awsbilling+account1@example.com,awsbilling+account2@example.com, etc.)ACCT.02 \u2013 Restrict Use of the Root UserThe account controlACCT.02requires that the root user be put away from further use after all initial account setup activities are completed.Root user configuration is typically done in the AWS Management Console, therefore we won't be using Terraform. You can refer toRoot user best practices for your AWS accountin the IAM User guide for more information. You may create additional administrative users for day-to-day use while following the controls that are described below.ACCT.03 \u2013 Configure Console AccessThe account controlACCT.03recommends using temporary credentials to grant access to AWS accounts and resources.Most AWS users will just use IAM initially to manage access to their AWS environment. While there are resources to manage IAM users in Terraform, it is certainly not the best approach. IAM users arestatefulresources which end-users will modify themselves (for example, changing password or adding an virtual MFA device). These out-of-band changes will cause perpetual changes in Terraform unless you leverage theignore_changeslifecycle meta-argument, which is a chore to configure. As well, Terraform storessensitive state datain plain text, which as you know is not very secure. For these reasons, it is recommended to leverage identity federation and single sign-on to manage users to your AWS environment.\ud83d\udca1 As your organization grows, you will find a multi-account architecture and identity federation to be increasingly valuable for better workload and environment separation. Consider usingAWS OrganizationsandAWS IAM Identity Centereven for your single account environment. You can create a new organizations and enroll your existing account as the member, then enable single sign-on (SSO) with IAM Identity Center. You can also start withAWS Control Toweror convert your organization into an AWS Control Tower landing zone later.With that said, you can use theaws_iam_userresourceto create the user if you must. For example:resource\"aws_iam_user\"\"john_doe\"{name=\"john_doe\"}Enter fullscreen modeExit fullscreen modeYou can then use theaws_iam_user_login_profileresourceto create theinitialpassword:resource\"aws_iam_user_login_profile\"\"john_doe\"{user=aws_iam_user.john_doe.name}output\"john_doe_password\"{# encrypted_password is the base64-encoded password# It provides a tad more security than the password attributevalue=aws_iam_user_login_profile.john_doe.encrypted_password}Enter fullscreen modeExit fullscreen modeYou will also need to assign IAM policies to the user (or better yet, to a group to which the user is then assigned) as described in the next control.ACCT.04 \u2013 Assign PermissionsThe account controlACCT.04requires user permissions to be configured by assigning policies to their IAM identity following the least privilege principle.IAM policiesare fundamental constructs that define what a principal can access within an AWS environment. There are two types of IAM policies - AWS-managed and user-managed. In Terraform, AWS-managed policies can be used via theaws_iam_policydata source, for instance:data\"aws_iam_policy\"\"lambda_basic_exec_role\"{name=\"AWSLambdaBasicExecutionRole\"}Enter fullscreen modeExit fullscreen modeYou can also use this data source to retrieve IAM policies that are created outside of your Terraform configuration. Meanwhile, managed policies can be created with theaws_iam_policyresource. The following is an example of a policy for a hypothetical Lambda function that processes some CloudWatch metrics and sends an email report via SES:resource\"aws_iam_role_policy\"\"cw_stats_email_lambda_exec\"{name=\"CWStatsEmailLambdaExecutionPolicy\"description=\"Grants permissions to the CWStatsEmail Lambda function.\"policy=jsonencode({Version=\"2012-10-17\"Statement=[{Action=[\"cloudwatch:GetMetricStatistics\"]Effect=\"Allow\"\"Resource\"=\"*\"},{Action=[\"ses:SendEmail\",\"ses:SendRawEmail\"]Effect=\"Allow\"\"Resource\"=\"*\"}]})}Enter fullscreen modeExit fullscreen modeHow you use these IAM policies thereafter depends on the IdP. If you are using IAM directly, the best practice is to assign the policies to IAM groups or IAM roles as mentioned earlier. The assignment can be done in Terraform with theaws_iam_group_policy_attachmentresourceand theaws_iam_role_policy_attachmentresourcerespectively. Using the same example above, the Terraform configuration below creates the Lambda execution role and attaches both the AWS-managed policyAWSLambdaBasicExecutionRoleand the user-managed policyCWStatsEmailLambdaExecutionPolicyto the role:data\"aws_caller_identity\"\"this\"{}data\"aws_region\"\"this\"{}locals{account_id=data.aws_caller_identity.current.account_idregion=data.aws_region.this.name}resource\"aws_iam_role\"\"cw_stats_email_lambda_exec\"{name=\"CWStatsEmailLambdaExecutionRole\"description=\"Execution role for the CWStatsEmail Lambda function.\"assume_role_policy=jsonencode({Version=\"2012-10-17\"aStatement=[{Action=\"sts:AssumeRole\"Effect=\"Allow\"Principal={Service=\"lambda.amazonaws.com\"}Condition={StringEquals={\"aws:SourceAccount\"=\"${local.account_id}\"}ArnLike={\"aws:SourceArn\"=\"arn:aws:lambda:${local.region}:${local.account_id}:function:*\"}}}]})managed_policy_arns=[data.aws_iam_policy.lambda_basic_exec_role.arn]}resource\"aws_iam_role_policy_attachment\"\"cw_stats_email_lambda_exec\"{policy_arn=aws_iam_policy.cw_stats_email_lambda_exec.arnrole=aws_iam_role.cw_stats_email_lambda_exec.name}Enter fullscreen modeExit fullscreen modeIf you are integrating an external OIDC or SAML IdP to IAM directly, the federated principal will also use IAM roles to access the AWS environment. The process to create IAM roles for this scenario is similar to the above, but you would need to adjust the trust policy (theassume_role_policyattribute) accordingly.If you have set up identity federation using IAM Identity Center, permissions are assigned as IAM policies topermissions sets, which are then associated with users and groups for accounts in the organization. Permissions can be created in Terraform using theaws_ssoadmin_permission_setresourcetypically in the management account of the organization. Here is an example for setting up a permission set for network administrators:data\"aws_ssoadmin_instances\"\"\"{}data\"aws_iam_policy\"\"network_admin\"{name=\"NetworkAdministrator \"}resource\"aws_ssoadmin_permission_set\"\"network_admin\"{name=\"MyOrgNetworkAdministrator\"description=\"Grants full access permissions to AWS services and actions required to set up and configure AWS network resources.\"instance_arn=tolist(data.aws_ssoadmin_instances.this.arns)[0]}resource\"aws_ssoadmin_managed_policy_attachment\"\"network_admin\"{instance_arn=tolist(data.aws_ssoadmin_instances.this.arns)[0]managed_policy_arn=aws_iam_policy.network_admin.arnpermission_set_arn=aws_ssoadmin_permission_set.network_admin.arn}Enter fullscreen modeExit fullscreen modeThe permission set can then be assigned to an IAM Identity Center user or group for an account in the organization usingaws_ssoadmin_account_assignmentresource.ACCT.05 \u2013 Require MFAThe account controlACCT.05requires MFA to be enabled for AWS account access especially for long-term user credentials as a security best practice.IAM users can enable MFA devices via the AWS Management Console given the appropriate permissions. For more information seeUsing multi-factor authentication (MFA) in AWSin the IAM User Guide.In terms of Terraform, although theaws_iam_virtual_mfa_deviceresourcecan be used to provision an IAM virtual MFA device, user still needs to associate an actual device to their IAM user with the provided attributesbase_32_string_seedorqr_code_png. Terraform is also not a particularly appropriate choice for managing IAM users due to the aforementioned security and procedural implications, so it's best left for separate management with a process that fits the IT security requirements of your organization.If you have set up identity federation, MFA should be managed in the centralized IdP instead. For example, AWS IAM Identity Provider hasMFA support, and you can expect enterprise-grade IdP such as Microsoft Entra ID to have advancedMFA capabilitiesand additional features such asConditional Access.ACCT.06 \u2013 Enforce a Password PolicyThe account controlACCT.06requires that passwords adhere to a strong password policy, ideally one that aligns with theCenter for Internet Security (CIS) Password Policy Guide, to help prevent discovery through brute force or social engineering.Thepassword policyfor IAM users are set on the account. The following is a mapping of the CIS Password Policy Guide recommendations to the IAM password policy settings. You may tune them as you see fit.Password OnlyWith MFAPassword minimum length148Require at least one uppercase letter from the Latin alphabet (A-Z)YesYesRequire at least one lowercase letter from the Latin alphabet (a-z)YesYesRequire at least one numberYesYesRequire at least one non-alphanumeric characterYesYesPassword expires innday(s)365365Allow users to change their own passwordYesYesPrevent password reuse from the pastnchanges55\ud83d\udca1 The CIS Password Policy Guide argues that password composition requirements are not effective because it leads to users choosing predictable patterns that are prone to dictionary attacks for convenience. However, there is also no common standard. Since this is an opinionated, I chose to follow the default settings in IAM.To configure the account password policy in Terraform, use theaws_iam_account_password_policyresourceas follows:resource\"aws_iam_account_password_policy\"\"this\"{allow_users_to_change_password=truemax_password_age=365minimum_password_length=8password_reuse_prevention=5require_lowercase_characters=truerequire_numbers=truerequire_symbols=truerequire_uppercase_characters=true}Enter fullscreen modeExit fullscreen modeIf you have set up identity federation, the password policy should primarily be managed in the centralized IdP. That being said, it never hurts to update the IAM account password policy as above just in case.\ud83d\udca1 As for enforcing MFA, I would recommend adopting a detective approach using services such asAWS Trusted Advisor(forroot account MFA) orAmazon Security Hubwith a standard that includes rules such as[IAM.5] MFA should be enabled for all IAM users that have a console passwordand[IAM.9] MFA should be enabled for the root user.SummaryIn this first blog post of the seriesHow to implement the AWS Startup Security Baseline (SSB) using Terraform, we examined the account-level controls that pertain to account and identity and explained how you can implement them using Terraform. In the next installment, we will focus on the remaining account-level controls. Please continue to follow the series and check out other posts in theAvangards Blog."}
{"title": "High Level Machine Learning services on AWS", "published_at": 1710707460, "tags": [], "user": "\u0639\u0628\u062f\u0627\u0644\u0644\u0647 \u0639\u064a\u0627\u062f | Abdullah Ayad", "url": "https://dev.to/aws-builders/machine-learning-services-on-aws-4jpk", "details": "Recognitionis a way for you to do face detections, to do labeling, and celebrity recognition.Transcribeis for you to get, a way to get subtitles, for example, to convert your audio into text.Pollyis the opposite. It allows you to get, to use your text and create audio out of it.Translateis for you to get translations.Lexis to build conversational robots or chatbots.Connectif you bundleLexwithConnect, then you can create a cloud contact center.Comprehendis a way for you to do natural language processing.SageMakeris a fully featured machine learning service that is accessible to developer and data scientist.Forecastallows you to build highly accurate forecast.Kendrais going to be an ML-powered document search engine.Personalizeis used for real-time personalized recommendations for your customers.Textractis used to detect text and data, and extract them from various documents.GitHubLinkedInFacebookMedium"}
{"title": "Deploy of Application in Nodes for Amazon Elastic Kubernetes Service", "published_at": 1710684664, "tags": ["amazoneks", "cloudwatch", "ec2instance", "aws"], "user": "GargeeBhatnagar", "url": "https://dev.to/aws-builders/deploy-of-application-in-nodes-for-amazon-elastic-kubernetes-service-5acm", "details": "\u201c I have checked the documents of AWS to get the solution for deploy of application in nodes for amazon elastic kubernetes service, in such scenario it is possible to deploy application after creation of amazon eks cluster and node with proper iam permissions. In terms of cost, need to pay for services integrated with the scenario.\u201dAmazon Elastic Kubernetes Service is a managed kubernetes service to run kubernetes in the AWS cloud and on-premises data centers. In the cloud, Amazon EKS automatically manages the availability and scalability of the kubernetes control plane nodes responsible for scheduling containers, managing application availability, storing cluster data and other key tasks. You can take advantage of all the performance, scale, reliability and availability of AWS infrastructure as well as integrations with AWS networking and security services. On-premises, EKS provides a consistent, fully supported kubernetes solution with integrated tooling and simple deployment to AWS Outposts, VM or bare metal servers.In this post, you will experience how to deploy of application in nodes for amazon elastic kubernetes service. Here I have created a cluster and node group in amazon eks with iam roles.Architecture OverviewThe architecture diagram shows the overall deployment architecture with data flow, amazon eks, cluster, node group, ec2 instance and cloudwatch logs.Solution overviewThe blog post consists of the following phases:Create of IAM Cluster Role and Cluster in Amazon Elastic Kubernetes ServiceCreate of IAM Node Role and Add of Node Group in Cluster CreatedDeploy and Test a Nginx Application in Nodes of ClusterPhase 1: Create of IAM Cluster Role and Cluster in Amazon Elastic Kubernetes ServiceOpen the console of IAM, create a role for the cluster with required aws managed policy. Also open the console of Amazon EKS, create a cluster in required vpc with configurations and add-ons.Phase 2: Create of IAM Node Role and Add of Node Group in Cluster CreatedOpen the console of IAM, create a role for node with required aws managed policies. Also open the cluster created in eks, add a node group with required configurations.Phase 3: Deploy and Test a Nginx Application in Nodes of ClusterClean-upDelete of Amazon EKS(Node Group and Cluster), IAM role and Cloudwatch log group.PricingI review the pricing and estimated cost of this example.Cost of Elastic Container Service for Kubernetes = $0.58Cost of CloudWatch = $0.139Cost of Elastic Compute Cloud = $1.21Total Cost = $1.929SummaryIn this post, I showed \u201chow to deploy of application in nodes for amazon elastic kubernetes service\u201d.For more details on Amazon EKS, Checkout Get started Amazon EKS, open theAmazon EKS console. To learn more, read theAmazon EKS documentation.Thanks for reading!Connect with me:Linkedin"}
{"title": "Unlocking High-Quality Realistic Pictures: Tips and Tricks with AWS Bedrock", "published_at": 1710681503, "tags": ["bedrock"], "user": "Danny Chan", "url": "https://dev.to/aws-builders/unlocking-high-quality-realistic-pictures-tips-and-tricks-with-aws-bedrock-28mn", "details": "Why you should be familiar with Bedrock:70% of enterprises leverage AI services for business growth and communication.68% of marketing and event organizations use Generative AI to enhance user experience and engagement.90% of customers believe Generative AI enhances existing sales services both online and in-store.If your business isn't utilizing Generative AI yet, it's time to explore AWS Bedrock.What Generative AI enables you to do:Generate ideas, concepts, and drafts to boost human creativity.Tailor personalized content based on individual preferences, improving user experience.Automate content generation at scale, facilitating the production of large volumes.Create realistic simulations.What is AWS Bedrock?A fully managed service.Offers a range of high-performing foundation models (FMs) for building generative AI applications.playgroundsProvides interactive playgrounds for text, chat, and image exploration.Orchestration knowledge baseAllows customization and fine-tuning of FMs using your own data while ensuring privacy, security, and compliance with standards like GDPR and HIPAA.Single agent APIOffers a user-friendly Single agent API for easy application inference.Serverless architecturepay as you go, no long-term commitments, no infrastructure management, and automatic scalability. Focus on what you want to accomplish without worrying about connecting FMs with code.Available Foundation Models:AI21 Labs: JurassicAnthropic: ClaudeCohere: Command & EmbedMeta: Llama 2Mistral AI: Mixtral 8x7B and Mistral 7BStability AI: Stable Diffusion XLAmazon: Amazon TitanLatest Supported Model: Claude 3My Role:Assisting financial service companies in adopting generative AI for streamlined development workflows and production-ready systems.Focusing on foundation models Titan v1 (lite & express) and Claude 3.Leveraging the Free Bedrock Environment:China AWS provides a free bedrock environment for exploring Generative AI capabilities.Generate countless pictures at no cost to showcase the performance of bedrock.Creating Realistic Conceptual Pictures:Developed a two-day Hong Kong on-site conference project for proof of concept.Picture optimized for social media sharing to engage and captivate audiences.Utilizing FMs Titan v1 and Claude 3 on AWS bedrock.Enhancing Accuracy Tips and tricks:Here are some methods to improve picture accuracy according to your expectations.1 Bedrock's Limited Background Understanding2 Bedrock's Hand Gesture Representation3 Bedrock's Eye Contact and QualityIssue: Bedrock's Limited Background UnderstandingBedrock often generates images with default, less exciting backgrounds, like crowded environment: Meetup, summit, conference.Lack of comprehension about the venue's activities leads to less engaging visuals.Creating Interesting Backgrounds:Use vivid descriptions to help Bedrock capture your desired atmosphere and emotions.Incorporate more generic environment details like outdoor, indoor, night, or sunny settings.Desired Image Types:Cloudy joyful after-parties, outdoor meetups, nightclubs, and restaurants.Bedrock excels at producing amazing pictures with a joyful, chilly atmosphere and cloudy backgrounds.Tips for Bedrock Usage:Utilize exciting descriptions to convey your imagination and emotions effectively.Avoid using generic or unexciting descriptions that may result in boring default images.Advanced Tip for Background Fine-Tuning:To further customize the background, include additional generic environment descriptions like outdoor, indoor, night, or sunshine settings.Example: After-partyIssue: Bedrock's Hand Gesture RepresentationBedrock often produces images with strange hand gestures and unrealistic figures when holding props like cups and bags.Understanding Bedrock's Limitations:Hand gestures pose a challenge for many foundation models, not just Titan.Changing the model won't resolve this issue.Tips for Better Hand Gestures:Avoid complex hand positions and actions like gripping a cup or holding a bag.Foundation models excel at drawing fists or open palms without specific gestures.Design tasks that involve props requiring a fist or open palm, such as controlling a joystick or holding a book.Observe daily life to understand common hand gestures for more accurate representations.Good News:Foundation models can accurately depict hands forming a fist or showing the palm without any specific gesture.Examples of Avoiding Hand Gestures:Controlling a joystick (making a fist)Holding a book (finger not interacting with the object, simple task)Giving a thumbs-up (similar to making a fist)DJ playing music on a panel (showing the back of the palm without any gesture)Portrait shooting (crossing arms)Tips for Realistic Hand Gestures:Remember that foundation models lack human understanding and familiarity with human body structure.Daily life observations can provide insights into natural hand gestures during various activities.Advanced Tip for Hand Gestures:To fine-tune hand gestures, consider using a backbone system that addresses both body and hand gestures directly.Issue: Bedrock's Eye Contact and QualityBedrock often generates portrait images where models don't look at the camera or have unbalanced eye positions.This results in odd and unprofessional pictures.Understanding the Eye Contact Challenge:Some foundation models struggle with portrait generation, and human sensitivity to eye contact exacerbates the issue.Improving Image Quality:Enhancing eye contact quality requires substantial effort and may not be suitable for proof of concept purposes.Utilizing Titan's Advantage:Titan produces better overall quality, including facial expressions and eye balance, especially for models wearing glasses.Take advantage of this by incorporating \"sunglasses\" and specifying \"under sunshine\" to avoid strong eye contact.This approach results in stylish and interesting images perfect for social media and conceptual art.Making It More Engaging:Titan performs exceptionally well with specific topics like Formula 1 and engineering uniforms.Utilize this expertise by creating images related to realistic engineering, leveraging the AWS community's engineering background.Tip for Random Results:Generate images in batches of five and adjust the seed or introduce more random patterns to increase the chance of obtaining a masterpiece.Consider Singular vs. Plural:Titan may overlook plural references, resulting in single individuals instead of groups.To ensure accuracy, use phrases like \"a group of models\" to obtain the desired outcome.Understanding Object Relationships:Titan struggles to comprehend object relationships in images.Avoid complex tasks involving multiple objects, as the results may be humorous or unexpected.Simplify the tasks to ensure more accurate and reliable image generationWhy you need to learn prompt engineering:Importance of Prompt Engineering:Prompt engineering is an accessible skill to learn but requires effort to master.It demands patience for trial and error, as well as luck to generate captivating images.Leveraging Model Advantages:Understand the model's strengths to generate high-quality images.For example, Titan excels at depicting uniforms, sunglasses, and models in sunny environments.Avoiding Model Limitations:Identify the model's weaknesses and avoid utilizing them.For instance, Titan struggles with hand gestures when holding cups or depicting closely grouped individuals.Bedrock's Benefits: What bedrock can help you:Gaming Industry:Quickly create conceptual art sets and demo videos for proof of concept projects.Entertainment Companies or Small Businesses:Generate venue visuals for floor plan collaboration with co-hosted parties.Multichannel Market Selling:Create visually appealing graphics and context for upselling and cross-selling on platforms like Facebook, LinkedIn, and Twitter."}
{"title": "Unleash Your Creativity with AWS Bedrock: Formula 1 Championship (Day 2)", "published_at": 1710679212, "tags": ["bedrock"], "user": "Danny Chan", "url": "https://dev.to/aws-builders/unleash-your-creativity-with-aws-bedrock-formula-1-championship-day-2-m1f", "details": "The champion of the Formula 1 machine learning competition in Hong Kong, the \"Who Knows the Lonely Girl's Heart\" team from the inaugural champion class, completed the most difficult level in 6 minutes and 37 seconds.The runner-up of the Formula 1 machine learning competition in Hong Kong, the Korean group \"I Love John Deere\", is celebrating their 10th global award.Mateo, the cloud service data center operations engineer, is showcasing a special gift for the machine learning hackers of the Hong Kong Formula 1 Championship.Oliver, the cloud service data center on-site power support security expert, is showcasing another special gift for the machine learning hackers of the Hong Kong Formula 1 Championship.Isla, the cloud service data center on-site network data encryption expert, is showcasing another special gift for the machine learning hackers of the Hong Kong Formula 1 Championship.Darcy, the cloud service data center communication pin scheduler engineer, is showing another special gift to the machine learning hackers of the Hong Kong Formula 1 Championship.Luna and Abigail, the million-dollar power equipment experts, are showcasing a special trophy for the machine learning hackers of the Hong Kong Formula 1 Championship.Hank, the user group leader and container cloud service provider, is a co-organizer of the machine learning portion of the Hong Kong Formula 1 Championship.Chloe, a machine learning engineer working for the government, is establishing an autonomous driving system on a construction site.Korean Formula 1 esports player Hannah (also known as Naughty Taffy Candy) is showcasing her self-made joystick for controlling a remote-controlled car system.Thai Formula 1 esports player Athena is adjusting her self-made micro-adjustment control micro-board electric gloves at the Hong Kong championship final.An AWS security expert is transporting the Hong Kong Formula 1 machine learning champion trophy to the venue.The local Hong Kong team \"Big Dragon in the Mouth\", the machine learning Hong Kong Formula 1 champion, is preparing for the final battle.The team \"Little Car Becomes Big Girl\", the 5,000 km truck group champion of the Hong Kong machine learning Formula 1 championship, is celebrating the big moment.The former Thai military engineering information systems unit \"Capital One\" is a supporting partner of the Hong Kong Formula 1 champion machine learning.The team \"Bull Head Power Boat\", the 1,000 km motor group champion of the Hong Kong machine learning Formula 1 championship, is celebrating the big event.The mentor team from the Greater China Institute of Machine Learning Systems Engineering is a supporting partner of the Hong Kong Formula 1 Championship, leading the team to upgrade the tracking system.The engineering support units from Thailand and Korea joined forces to upgrade the on-site machine learning and edge device performance through the \"Graviton4\" chip.The engineering units supported by the Greater China cloud computing collaborated with the Hong Kong Formula 1 Championship. They provided high-voltage power for the battery swap system carefully planned for the Korean team's vehicle system upgrade.The 7,800 km high-speed oil tanker group, the Korean team \"Hate My Girl\", of the Hong Kong machine learning Formula 1 champion, finally tested their system before the final battle.Julia, the edge device electronic chip exchange information engineer, is celebrating at the champion's afterparty.Charlie, an edge device machine learning advocate, is also the team mentor of the Thai former military engineering information systems unit \"Capital One\".Melanie, a graphic designer and user experience engineer, is showcasing 3D printing materials for swags.Melanie, a graphic designer and user experience engineer, is showcasing 3D printed champion badges.Colt, a system design solution engineer, is dedicated to high-throughput disaster recovery solutions in the financial industry.Bryce, a machine learning in-memory database solution provider, a supporting partner of the Hong Kong Formula 1 Championship, is also the mentor of the Thai team's 5,000 km second place \"Focused Fleet\".User experience engineer Maximiliano from the 5,000 km oil tanker group first place \"Capital One\" team is receiving swags on the main stage.Power engineer Kayson helped the engineering unit establish on-site machine learning infrastructure.Isabelle, the electronic chip information exchange engineer, helped the engineers adjust the sensitivity speed of the Formula 1 control panel.The 5,000 km oil tanker group champion team of the Hong Kong machine learning Formula 1 championship. They are celebrating the victory.The materials engineering team is showcasing next-generation military-grade elastic materials on the main stage.The machine learning team is showcasing a tracking system for a logistics industry client.Formula 1 esports champion team members Alaia and Amara. They helped the machine learning team test and fine-tune the edge device control performance.The former Formula 1 esports champion team \"Sweet Little Dad\" is a technical support partner. They helped the champion team adjust the site-to-device integration system.Cecilia, a 3D printing engineer, is researching adjustments to the material system for electronic chips.The Greater China Guangzhou user group. They are cloud service consulting partners, helping clients adopt site-to-cloud infrastructure.Magnolia, a machine learning analyst, is developing information exchange and high-throughput exchange deployments on edge devices.Ashley, a machine learning analyst, is testing the next-generation tracking and video capture system.Oakley, the previous 5,000 km high-speed truck champion of the Hong Kong machine learning Formula 1 championship, is helping to adjust the new control system panel.The swag delivery team on the main conference stage. They randomly distribute swags to participants.The Thai machine learning edge device engineering unit. They are the AI assistant for the control panel developed for the champion team.Brian, a machine learning analyst, is a supporting partner and mentor for the Hong Kong Formula 1 champion machine learning.The Japanese champion team engineering unit. They helped upgrade the existing track for the Hong Kong Formula 1 champion race.The Japanese champion team engineering \"Flowery Old Man\" won the second place in the Hong Kong Formula 1 electric truck competition.The Japanese champion team engineering \"Playboy Dad\" is adjusting the control panel sensitivity for the final battle.The Thai Formula 1 esports team \"Escort Force\" is preparing the 5,000 km high-speed motor device.Brianna and Sara. They are researching the next-generation ink for 3D printing materials.Adalyn is showcasing the final proof-of-concept demo of the next-generation ink for 3D printing materials.The swag ambassadors on the main conference stage.The swag ambassadors on the machine learning demonstration conference stage.Bailey and Aiden. They are jointly upgrading the Formula 1 high-speed driving control system.The Thai Formula 1 engineering unit \"Sunny Smile\" is showcasing an edge device machine learning demonstration car.Callie. Cloud supply engineer. The Greater China Formula 1 engineering unit \"Big Boy Naughty Girl\".The 15,000 km high-speed electric vehicle group, the Thai team \"Sweet Angel\", won the Formula 1 champion in the Hong Kong machine learning championship.The Greater China esports team \"Incredible Architecture\" in the Hong Kong machine learning Formula 1 championship.The second team of the Greater China esports team \"Incredible Architecture\" in the Hong Kong machine learning Formula 1 championship.Faith, Amy, Kamila, Vera. Cloud computing support engineers. They developed the infrastructure for the Hong Kong machine learning Formula 1 championship.The SWAG booth at the main conference.Diana, Rachel, Adaline, Leia are machine learning analysts developing a video capture system for the Hong Kong Formula 1 champion.The Indonesian cloud service user group.The Indonesian engineering team is a supporting partner of the Hong Kong Formula 1 champion race. They developed an electric track for autonomous driving cars.The Greater China engineering unit is a supporting partner of the Hong Kong Formula 1 champion race. They developed an electronic chip for autonomous driving cars.The Greater China engineering department, they developed the machine learning remote control panel and information exchange system for autonomous driving cars.The Greater China engineering team, adjusting the control panel, preparing for the final battle.The Greater China esports team \"Savage Girl\" won the first place in the autonomous driving car group of the Hong Kong Formula 1 champion race."}
{"title": "AWS DEV OPS Professional Exam short notes", "published_at": 1710654958, "tags": ["aws", "devops", "serverless"], "user": "Amit Kayal", "url": "https://dev.to/aws-builders/aws-dev-ops-professional-exam-short-notes-4b47", "details": "Last few weeks I have been preparing for this exam and have summarized below key notes for further quick reference.Key NotesYou can use CloudWatch Logs to monitor applications and systems using log data. For example, CloudWatch Logs can track the number of errors that occur in your application logs and send you a notification whenever the rate of errors exceeds a threshold you specify. CloudWatch Logs uses your log data for monitoring; so, no code changes are required. For more information on Cloudwatch logs , please refer to the below link:http://docs.aws.amazon.com/AmazonCloudWatch/latest/logs/WhatIsCloudWatchLogs.htmlThe correct answer is: Install the CloudWatch Logs Agent on your AMI, and configure CloudWatch Logs Agent to stream your logs.You can add another layer of protection by enabling MFA Delete on a versioned bucket. Once you do so, you must provide your AWS account\u0092s access keys and a valid code from the account\u0092s MFA device in order to permanently delete an object version or suspend or reactivate versioning on the bucket. For more information on MFA please refer to the below link:https://aws.amazon.com/blogs/security/securing-access-to-aws-using-mfa-part-3/IAM roles are designed so that your applications can securely make API requests from your instances, without requiring you to manage the security credentials that the applications use. Instead of creating and distributing your AWS credentials, you can delegate permission to make API requests using IAM roles For more information on Roles for EC2 please refer to the below link:http://docs.aws.amazon.com/AWSEC2/latest/UserGuide/iam-roles-for-amazon-ec2.htmlAs your infrastructure grows, common patterns can emerge in which you declare the same components in each of your templates. You can separate out these common components and create dedicated templates for them. That way, you can mix and match different templates but use nested stacks to create a single, unified stack. Nested stacks are stacks that create other stacks. To create nested stacks, use the AWS::CloudFormation::Stackresource in your template to reference other templates. For more information on best practices for Cloudformation please refer to the below link:http://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/best-practices.htmlThe correct answer is: Separate the AWS CloudFormation template into a nested structure that has individual templates for the resources that are to be governed by different departments, and use the outputs from the networking and security stacks for the application template that you control.You can use Amazon CloudWatch Logs to monitor, store, and access your log files from Amazon Elastic Compute Cloud (Amazon EC2) instances, AWS CloudTrail, and other sources. You can then retrieve the associated log data from CloudWatch Logs. For more information on Cloudwatch logs please refer to the below link:http://docs.aws.amazon.com/AmazonCloudWatch/latest/logs/WhatIsCloudWatchLogs.htmlYou can the use Kinesis to process those logs For more information on Amazon Kinesis please refer to the below link:http://docs.aws.amazon.com/streams/latest/dev/introduction.htmlThe correct answers are: Using AWS CloudFormation, create a CloudWatch Logs LogGroup and send the operating system and application logs of interest using the CloudWatch Logs Agent., Using configuration management, set up remote logging to send events to Amazon Kinesis and insert these into Amazon CloudSearch or Amazon Redshift, depending on available analytic tools.IAM roles are designed so that your applications can securely make API requests from your instances, without requiring you to manage the security credentials that the applications use. Instead of creating and distributing your AWS credentials For more information on IAM Roles please refer to the below link:http://docs.aws.amazon.com/AWSEC2/latest/UserGuide/iam-roles-for-amazon-ec2.htmlThe AWS Security Token Service (STS) is a web service that enables you to request temporary, limited-privilege credentials for AWS Identity and Access Management (IAM) users or for users that you authenticate (federated users). The token can then be used to grant access to the objects in S3. You can then provides access to the objects based on the key values generated via the user idAs your infrastructure grows, common patterns can emerge in which you declare the same components in each of your templates. You can separate out these common components and create dedicated templates for them. That way, you can mix and match different templates but use nested stacks to create a single, unified stack. Nested stacks are stacks that create other stacks. To create nested stacks, use the AWS::CloudFormation::Stackresource in your template to reference other templates. For more information on Cloudformation best practises please refer to the below link:http://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/best-practices.htmlThe correct answer is: Create separate templates based on functionality, create nested stacks with CloudFormation.The default autosclae termination policy is designed to help ensure that your network architecture spans Availability Zones evenly. When using the default termination policy, Auto Scaling selects an instance to terminate as follows: Auto Scaling determines whether there are instances in multiple Availability Zones. If so, it selects the Availability Zone with the most instances and at least one instance that is not protected from scale in. If there is more than one Availability Zone with this number of instances, Auto Scaling selects the Availability Zone with the instances that use the oldest launch configuration. For more information on Autoscaling instance termination please refer to the below link:http://docs.aws.amazon.com/autoscaling/latest/userguide/as-instance-termination.htmlThe correct answer is: Auto Scaling will select the AZ with 4 EC2 instances and terminate an instance.Amazon RDS Read Replicas provide enhanced performance and durability for database (DB) instances. This replication feature makes it easy to elastically scale out beyond the capacity constraints of a single DB Instance for read-heavy database workloads. You can create one or more replicas of a given source DB Instance and serve high-volume application read traffic from multiple copies of your data, thereby increasing aggregate read throughput. Sharding is a common concept to split data across multiple tables in a database. Shard your data set among multiple Amazon RDS DB instances.Amazon ElastiCache is a web service that makes it easy to deploy, operate, and scale an in-memory data store or cache in the cloud. The service improves the performance of web applications by allowing you to retrieve information from fast, managed, in-memory data stores, instead of relying entirely on slower disk-based databases.Continuous Integration (CI) is a development practice that requires developers to integrate code into a shared repository several times a day. Each check-in is then verified by an automated build, allowing teams to detect problems early.Elastic Beanstalk simplifies this process by managing the Amazon SQS queue and running a daemon process on each instance that reads from the queue for you. When the daemon pulls an item from the queue, it sends an HTTP POST request locally tohttp://localhost/with the contents of the queue message in the body. All that your application needs to do is perform the long-running task in response to the POST. For more information Elastic Beanstalk managing worker environments, please visit the below URL:http://docs.aws.amazon.com/elasticbeanstalk/latest/dg/using-features-managing-env-tiers.htmlIf you suspend AddToLoadBalancer, Auto Scaling launches the instances but does not add them to the load balancer or target group. If you resume the AddToLoadBalancer process, Auto Scaling resumes adding instances to the load balancer or target group when they are launched. However, Auto Scaling does not add the instances that were launched while this process was suspended. You must register those instances manually. For more information on the Suspension and Resumption process, please visit the below URL:http://docs.aws.amazon.com/autoscaling/latest/userguide/as-suspend-resume-processes.htmlYou can use the container_commands key of elastic beanstalk to execute commands that affect your application source code. Container commands run after the application and web server have been set up and the application version archive has been extracted, but before the application version is deployed. Non-container commands and other customization operations are performed prior to the application source code being extracted. You can use leader_only to only run the command on a single instance, or configure a test to only run the command when a test command evaluates to true. Leader-only container commands are only executed during environment creation and deployments, while other commands and server customization operations are performed every time an instance is provisioned or updated. Leader-only container commands are not executed due to launch configuration changes, such as a change in the AMI Id or instance type. For more information on customizing containers, please visit the below URL:http://docs.aws.amazon.com/elasticbeanstalk/latest/dg/customize-containers-ec2.htmlThe correct answer is: Use a \u0093Container command\u0094 within an Elastic Beanstalk configuration file to execute the script, ensuring that the \u0093leader only\u0094 flag is set to true.A Dockerrun.aws.json file is an Elastic Beanstalk\u0096specific JSON file that describes how to deploy a set of Docker containers as an Elastic Beanstalk application. You can use aDockerrun.aws.json file for a multicontainer Docker environment. Dockerrun.aws.json describes the containers to deploy to each container instance in the environment as well as the data volumes to create on the host instance for the containers to mount.http://docs.aws.amazon.com/elasticbeanstalk/latest/dg/create_deploy_docker_v2config.htmlElastic Beanstalk supports the deployment of web applications from Docker containers. With Docker containers, you can define your own runtime environment. You can choose your own platform, programming language, and any application dependencies (such as package managers or tools), that aren\u2019t supported by other platforms. Docker containers are self-contained and include all the configuration information and software your web application requires to run.When you see Amazon Kinesis as an option, this becomes the ideal option to process data in real time. Amazon Kinesis makes it easy to collect, process, and analyze real-time, streaming data so you can get timely insights and react quickly to new information. Amazon Kinesis offers key capabilities to cost effectively process streaming data at any scale, along with the flexibility to choose the tools that best suit the requirements of your application. With Amazon Kinesis, you can ingest real-time data such as application logs, website clickstreams, IoT telemetry data, and more into your databases, data lakes and data warehouses, or build your own real-time applications using this data. For more information on Amazon Kinesis, please visit the below URL:https://aws.amazon.com/kinesisYou can use CloudWatch Logs to monitor applications and systems using log data CloudWatch Logs uses your log data for monitoring; so, no code changes are required. For example, you can monitor application logs for specific literal terms (such as \u201cNullReferenceException\u201d) or count the number of occurrences of a literal term at a particular position in log data (such as \u201c404\u201d status codes in an Apache access log). When the term you are searching for is found, CloudWatch Logs reports the data to a CloudWatch metric that you specify. Log data is encrypted while in transit and while it is at rest For more information on Cloudwatch logs please refer to the below link:http://docs.aws.amazon.com/AmazonCloudWatch/latest/logs/WhatIsCloudWatchLogs.htmlAmazon CloudWatch uses Amazon SNS to send email. First, create and subscribe to an SNS topic. When you create a CloudWatch alarm, you can add this SNS topic to send an email notification when the alarm changes state. For more information on SNS and Cloudwatch logs please refer to the below link:  The correct answers are: Install a CloudWatch Logs Agent on your servers to stream web application logs to CloudWatch., Create a CloudWatch Logs group and define metric filters that capture 500 Internal Server Errors. Set a CloudWatch alarm on that metric., Use Amazon Simple Notification Service to notify an on-call engineer when a CloudWatch alarm is triggeredWhen you provision an Amazon EC2 instance in an AWS CloudFormation stack, you might specify additional actions to configure the instance, such as install software packages or bootstrap applications. Normally, CloudFormation proceeds with stack creation after the instance has been successfully created. However, you can use a CreationPolicy so that CloudFormation proceeds with stack creation only after your configuration actions are done. That way you\u0092ll know your applications are ready to go after stack creation succeeds.Auto Scaling periodically performs health checks on the instances in your Auto Scaling group and identifies any instances that are unhealthy. You can configure Auto Scaling to determine the health status of an instance using Amazon EC2 status checks, Elastic Load Balancing health checks, or custom health checks By default, Auto Scaling health checks use the results of the EC2 status checks to determine the health status of an instance. Auto Scaling marks an instance as unhealthy if its instance fails one or more of the status checks. For more information monitoring in Autoscaling , please visit the below URL:http://docs.aws.amazon.com/autoscaling/latest/userguide/as-monitoring-features.htmlYou need to have a custom health check which will evaluate the application functionality. Its not enough using the normal health checks. If the application functionality does not work and if you don\u0092t have custom health checks , the instances will still be deemed as healthy. If you have custom health checks, you can send the information from your health checks to Auto Scaling so that Auto Scaling can use this information. For example, if you determine that an instance is not functioning as expected, you can set the health status of the instance to Unhealthy. The next time that Auto Scaling performs a health check on the instance, it will determine that the instance is unhealthy and then launch a replacement instance For more information on Autoscaling health checks , please refer to the below document link: from AWShttp://docs.aws.amazon.com/autoscaling/latest/userguide/healthcheck.htmlA blue group carries the production load while a green group is staged and deployed with the new code. When it\u0092s time to deploy, you simply attach the green group to the existing load balancer to introduce traffic to the new environment. For HTTP/HTTPS listeners, the load balancer favors the green Auto Scaling group because it uses a least outstanding requests routing algorithm As you scale up the green Auto Scaling group, you can take blue Auto Scaling group instances out of service by either terminating them or putting them in Standby state, For more information on Blue Green Deployments , please refer to the below document link: from AWShttps://d0.awsstatic.com/whitepapers/AWS_Blue_Green_Deployments.pdfEnsure first that the cloudformation template is updated with the new instance type. The AWS::AutoScaling::AutoScalingGroup resource supports an UpdatePolicy attribute. This is used to define how an Auto Scaling group resource is updated when an update to the CloudFormation stack occurs. A common approach to updating an Auto Scaling group is to perform a rolling update, which is done by specifying the AutoScalingRollingUpdate policy. This retains the same Auto Scaling group and replaces old instances with new ones, according to the parameters specifiedWith web identity federation, you don\u2019t need to create custom sign-in code or manage your own user identities. Instead, users of your app can sign in using a well-known identity provider (IdP) \u0097such as Login with Amazon, Facebook, Google, or any other OpenID Connect (OIDC)-compatible IdP, receive an authentication token, and then exchange that token for temporary security credentials in AWS that map to an IAM role with permissions to use the resources in your AWS account. Using an IdP helps you keep your AWS account secure, because you don\u2019t have to embed and distribute long-term security credentials with your application.The optional Conditions section includes statements that define when a resource is created or when a property is defined. For example, you can compare whether a value is equal to another value. Based on the result of that condition, you can conditionally create resources. If you have multiple conditions, separate them with commas. You might use conditions when you want to reuse a template that can create resources in different contexts, such as a test environment versus a production environment. In your template, you can add an EnvironmentType input parameter, which accepts either prod or test as inputs. For the production environment, you might include Amazon EC2 instances with certain capabilities; however, for the test environment, you want to use reduced capabilities to save money. With conditions, you can define which resources are created and how they\u2019re configured for each environment type. For more information on Cloudformation conditions please refer to the below link:http://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/conditions-section-structure.htmlElastic Beanstalk already has the facility to manage various versions and you don\u0092t need to use S3 separately for this.AWS beanstalk is the perfect solution for developers to maintain application versions. With AWS Elastic Beanstalk, you can quickly deploy and manage applications in the AWS Cloud without worrying about the infrastructure that runs those applications. AWS Elastic Beanstalk reduces management complexity without restricting choice or control. You simply upload your application, and AWS Elastic Beanstalk automatically handles the details of capacity provisioning, load balancing, scaling, and application health monitoring.The first step in using Elastic Beanstalk is to create an application, which represents your web application in AWS. In Elastic Beanstalk an application serves as a container for the environments that run your web app, and versions of your web app\u2019s source code, saved configurations, logs and other artifacts that you create while using Elastic Beanstalk. For more information on Applications, please refer to the below link:http://docs.aws.amazon.com/elasticbeanstalk/latest/dg/applications.htmlDeploying a new version of your application to an environment is typically a fairly quick process. The new source bundle is deployed to an instance and extracted, and then the web container or application server picks up the new version and restarts if necessary. During deployment, your application might still become unavailable to users for a few seconds. You can prevent this by configuring your environment to use rolling deployments to deploy the new version to instances in batches. For more information on deployment, please refer to the below link:http://docs.aws.amazon.com/elasticbeanstalk/latest/dg/using-features.deploy-existing-version.htmlWeighted routing lets you associate multiple resources with a single domain name (example.com) or subdomain name (acme.example.com) and choose how much traffic is routed to each resource. This can be useful for a variety of purposes, including load balancing and testing new versions of software. For more information on the Routing policy please refer to the below link:http://docs.aws.amazon.com/Route53/latest/DeveloperGuide/routing-policy.htmlAmazon Elasticsearch Service makes it easy to deploy, operate, and scale Elasticsearch for log analytics, full text search, application monitoring, and more. Amazon Elasticsearch Service is a fully managed service that delivers Elasticsearch\u0092s easy-to-use APIs and real-time capabilities along with the availability, scalability, and security required by production workloads. The service offers built-in integrations with Kibana, Logstash, and AWS services including Amazon Kinesis Firehose, AWS Lambda, and Amazon CloudWatch so that you can go from raw data to actionable insights quickly.You can use CloudWatch Logs to monitor applications and systems using log data. For example, CloudWatch Logs can track the number of errors that occur in your application logs and send you a notification whenever the rate of errors exceeds a threshold you specify. CloudWatch Logs uses your log data for monitoring; so, no code changes are required. For example, you can monitor application logs for specific literal terms (such as \u201cNullReferenceException\u201d) or count the number of occurrences of a literal term at a particular position in log data (such as \u201c404\u201d status codes in an Apache access log). When the term you are searching for is found, CloudWatch Logs reports the data to a CloudWatch metric that you specify. For more information on Cloudwatch Logs please refer to the below link:http://docs.aws.amazon.com/AmazonCloudWatch/latest/logs/WhatIsCloudWatchLogs.htmlAmazon CloudWatch uses Amazon SNS to send email. First, create and subscribe to an SNS topic. When you create a CloudWatch alarm, you can add this SNS topic to send an email notification when the alarm changes state. For more information on Cloudwatch and SNS please refer to the below link:http://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/US_SetupSNS.htmlAWS OpsWorks is a configuration management service that uses Chef, an automation platform that treats server configurations as code. OpsWorks uses Chef to automate how servers are configured, deployed, and managed across your Amazon Elastic Compute Cloud (Amazon EC2) instances or on-premises compute environments. OpsWorks has two offerings, AWS Opsworks for Chef Automate, and AWS OpsWorks Stacks. For more information on Opswork and SNS please refer to the below link:https://aws.amazon.com/opsworks/You can use Kinesis Streams for rapid and continuous data intake and aggregation. The type of data used includes IT infrastructure log data, application logs, social media, market data feeds, and web clickstream data. Because the response time for the data intake and processing is in real time, the processing is typically lightweight. The following are typical scenarios for using Kinesis Streams: Accelerated log and data feed intake and processing \u2013 You can have producers push data directly into a stream. For example, push system and application logs and they\u2019ll be available for processing in seconds. This prevents the log data from being lost if the front end or application server fails. Kinesis Streams provides accelerated data feed intake because you don\u2019t batch the data on the servers before you submit it for intake. Real-time metrics and reporting \u2013 You can use data collected into Kinesis Streams for simple data analysis and reporting in real time. For example, your data-processing application can work on metrics and reporting for system and application logs as the data is streaming in, rather than wait to receive batches of data. For more information on Amazon Kinesis and SNS please refer to the below link:http://docs.aws.amazon.com/streams/latest/dev/introduction.htmlWith Elastic Beanstalk, you can quickly deploy and manage applications in the AWS Cloud without worrying about the infrastructure that runs those applications. AWS Elastic Beanstalk reduces management complexity without restricting choice or control. You simply upload your application, and Elastic Beanstalk automatically handles the details of capacity provisioning, load balancing, scaling, and application health monitoring For more information on Elastic beanstalk please refer to the below link:http://docs.aws.amazon.com/elasticbeanstalk/latest/dg/Welcome.htmlYou can use intrinsic functions, such as Fn::If, Fn::Equals, and Fn::Not, to conditionally create stack resources. These conditions are evaluated based on input parameters that you declare when you create or update a stack. After you define all your conditions, you can associate them with resources or resource properties in the Resources and Outputs sections of a template.Amazon RDS Multi-AZ deployments provide enhanced availability and durability for Database (DB) Instances, making them a natural fit for production database workloads. When you provision a Multi-AZ DB Instance, Amazon RDS automatically creates a primary DB Instance and synchronously replicates the data to a standby instance in a different Availability Zone (AZ). Each AZ runs on its own physically distinct, independent infrastructure, and is engineered to be highly reliable. In case of an infrastructure failure, Amazon RDS performs an automatic failover to the standby (or to a read replica in the case of Amazon Aurora), so that you can resume database operations as soonYou can use AWS CloudTrail to get a history of AWS API calls and related events for your account. This history includes calls made with the AWS Management Console, AWS Command Line Interface, AWS SDKs, and other AWS services. For more information on Cloudtrail, please visit the below URL:http://docs.aws.amazon.com/awscloudtrail/latest/userguide/cloudtrail-user-guide.htmlAmazon CloudWatch Events delivers a near real-time stream of system events that describe changes in Amazon Web Services (AWS) resources. Using simple rules that you can quickly set up, you can match events and route them to one or more target functions or streams. CloudWatch Events becomes aware of operational changes as they occur. CloudWatch Events responds to these operational changes and takes corrective action as necessary, by sending messages to respond to the environment, activating functions, making changes, and capturing state informationBy default, all AWS accounts are limited to 5 Elastic IP addresses per region, because public (IPv4) Internet addresses are a scarce public resource. We strongly encourage you to use an Elastic IP address primarily for the ability to remap the address to another instance in the case of instance failure, and to use DNS hostnames for all other inter-node communicationYou can manage Amazon SQS messages with Amazon S3. This is especially useful for storing and consuming messages with a message size of up to 2 GB. To manage Amazon SQS messages with Amazon S3, use the Amazon SQS Extended Client Library for Java. Specifically, you use this library to: Specify whether messages are always stored in Amazon S3 or only when a message\u2019s size exceeds 256 KB. Send a message that references a single message object stored in an Amazon S3 bucket. Get the corresponding message object from an Amazon S3 bucket. Delete the corresponding message object from an Amazon S3 bucket. For more information on processing large messages for SQS, please visit the below URL:http://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/sqs-s3-messages.htmlAWS CloudFormation provisions and configures resources by making calls to the AWS services that are described in your template. After all the resources have been created, AWS CloudFormation reports that your stack has been created. You can then start using the resources in your stack. If stack creation fails, AWS CloudFormation rolls back your changes by deleting the resources that it created. The below snapshot from Cloudformation shows what happens when there is an error in the stack creation. For more information on how CloudFormation works , please refer to the below link:http://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/cfn-whatis-howdoesitwork.htmlBecause Elastic Beanstalk performs an in-place update when you update your application versions, your application may become unavailable to users for a short period of time. It is possible to avoid this downtime by performing a blue/green deployment, where you deploy the new version to a separate environment, and then swap CNAMEs of the two environments to redirect traffic to the new version instantly. Blue/green deployments require that your environment runs independently of your production database, if your application uses one. If your environment has an Amazon RDS DB instance attached to it, the data will not transfer over to your second environment, and will be lost if you terminate the original environment. For more information on Blue Green deployments with Elastic beanstalk , please refer to the below link:http://docs.aws.amazon.com/elasticbeanstalk/latest/dg/using-features.CNAMESwap.htmlAmazon RDS Read Replicas provide enhanced performance and durability for database (DB) instances. This replication feature makes it easy to elastically scale out beyond the capacity constraints of a single DB Instance for read-heavy database workloads. You can create one or more replicas of a given source DB Instance and serve high-volume application read traffic from multiple copies of your data, thereby increasing aggregate read throughput. Read replicas can also be promoted when needed to become standalone DB instances.Amazon Route 53 health checks monitor the health and performance of your web applications, web servers, and other resources.If you use SSL termination, your servers will always get non-secure connections and will never know whether users used a more secure channel or not. If you are using Elastic beanstalk to configure the ELB, you can use the below article to ensure end to end encryption.http://docs.aws.amazon.com/elasticbeanstalk/latest/dg/configuring-https-endtoend.htmlAmazon Kinesis Firehose is the easiest way to load streaming data into AWS. It can capture, transform, and load streaming data into Amazon Kinesis Analytics, Amazon S3, Amazon Redshift, and Amazon Elasticsearch Service, enabling near real-time analytics with existing business intelligence tools and dashboards you\u0092re already using today. It is a fully managed service that automatically scales to match the throughput of your data and requires no ongoing administration. It can also batch, compress, and encrypt the data before loading it, minimizing the amount of storage used at the destination and increasing security. For more information on Kinesis firehose, please visit the below URL:https://aws.amazon.com/kinesis/firehose/Amazon Redshift is a fully managed, petabyte-scale data warehouse service in the cloud. You can start with just a few hundred gigabytes of data and scale to a petabyte or more. This enables you to use your data to acquire new insights for your business and customers.Use Cloudfront distribution for distributing the heavy reads for your application. You can create a zone apex record to point to the Cloudfront distribution. You can control how long your objects stay in a CloudFront cache before CloudFront forwards another request to your origin. Reducing the duration allows you to serve dynamic content. Increasing the duration means your users get better performance because your objects are more likely to be served directly from the edge cache. A longer duration also reduces the load on your origin.Amazon EBS encryption offers you a simple encryption solution for your EBS volumes without the need for you to build, maintain, and secure your own key management infrastructure. When you create an encrypted EBS volume and attach it to a supported instance type, the following types of data are encrypted: Data at rest inside the volume All data moving between the volume and the instance All snapshots created from the volume Snapshots that are taken from encrypted volumes are automatically encrypted. Volumes that are created from encrypted snapshots are also automatically encrypted.A tag is a label that you or AWS assigns to an AWS resource. Each tag consists of a key and a value. A key can have more than one value. You can use tags to organize your resources, and cost allocation tags to track your AWS costs on a detailed level. After you activate cost allocation tags, AWS uses the cost allocation tags to organize your resource costs on your cost allocation report, to make it easier for you to categorize and track your AWS costs. AWS provides two types of cost allocation tags, an AWS-generated tag and user-defined tags. AWS defines, creates, and applies the AWS-generated tag for you, and you define, create, and apply user-defined tags. You must activate both types of tags separately before they can appear in Cost Explorer or on a cost allocation report.You can monitor the progress of a stack update by viewing the stack\u2019s events. The console\u2019s Events tab displays each major step in the creation and update of the stack sorted by the time of each event with latest events on top. The start of the stack update process is marked with an UPDATE_IN_PROGRESS event for the stack For more information on Monitoring your stack, please visit the below URL:http://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/using-cfn-updating-stacks-monitor-stack.htmlA placement group is a logical grouping of instances within a single Availability Zone. Placement groups are recommended for applications that benefit from low network latency, high network throughput, or both. To provide the lowest latency, and the highest packet-per-second network performance for your placement group, choose an instance type that supports enhanced networking. For more information on Placement Groups, please visit the below URL:http://docs.aws.amazon.com/AWSEC2/latest/UserGuide/placement-groups.htmlAWS CloudTrail is an AWS service that helps you enable governance, compliance, and operational and risk auditing of your AWS account. Actions taken by a user, role, or an AWS service are recorded as events in CloudTrail. Events include actions taken in the AWS Management Console, AWS Command Line Interface, and AWS SDKs and APIs. Visibility into your AWS account activity is a key aspect of security and operational best practices. You can use CloudTrail to view, search, download, archive, analyze, and respond to account activity across your AWS infrastructure. You can identify who or what took which action, what resources were acted upon, when the event occurred, and other details to help you analyze and respond to activity in your AWS account. For more information on Cloudtrail, please visit the below URL:http://docs.aws.amazon.com/awscloudtrail/latest/userguide/cloudtrail-user-guide.htmlCustom resources enable you to write custom provisioning logic in templates that AWS CloudFormation runs anytime you create, update (if you changed the custom resource), or delete stacks. For example, you might want to include resources that aren\u2019t available as AWS CloudFormation resource types. You can include those resources by using custom resources. That way you can still manage all your related resources in a single stack. Use the AWS::CloudFormation::CustomResource or Custom::String resource type to define custom resources in your templates. Custom resources require one property: the service token, which specifies where AWS CloudFormation sends requests to, such as an Amazon SNS topic.Failover routing lets you route traffic to a resource when the resource is healthy or to a different resource when the first resource is unhealthy. The primary and secondary resource record sets can route traffic to anything from an Amazon S3 bucket that is configured as a website to a complex tree of records. For more information on Route53 Failover Routing, please visit the below URL:http://docs.aws.amazon.com/Route53/latest/DeveloperGuide/routing-policy.htmlDeployment Types:Single Target Deployment - small dev projects, legacy or non-HA infrastructure; outage occurs in case of failure, testing opportunity is limited.All-at-Once Deployment - deployment happens on multiple targets, requires Orchestration tools, suitable for non critical apps in 5-10 range.Minimum in-service Deployment - keeps min in-service targets and deploy in multiple stages, suitable for large environments, allow automated testing, no downtimeRolling Deployments - x targets per stage, happens in multiple stages, after completion of stage 1, next stage begins, orchestration and health check required, can be least efficient if x is smaller, allow automated testing, no downtime if x is not large to impact application, can be paused, allowing multi-version testing.Blue Green Deployment - Deploy to seperate Green environment, update the code on Green, extra cost due to duplicate env during deployment, Deployment is rapid, cutover and migration is clean(DNS Change), Rollback easy(DNS regression), can be fully automates using CFN etc. Binary, No Traffic Split, not used to feature testA/B Testing - distribution traffic between blue/green, allows gradual performance/stability/health analysis, allows new feature testing, rollback is quick, end goal of A/B testing is not migration, Uses Route 53 for DNS resolution, 2 records one pointing A, other pointing B, weighted/round robin.Intrinsic & Conditional FunctionsIntrinsic Fn - inbuilt function provided by AWS to help manage, reference, and condtionally act upon resources, situation & inputs to a stack.Fn::Base64 - Base64 encoding for User DataFn::FindInMap - Mapping lookupFn::GetAtt - Advanced reference look upFn::GetAZs - retrieve list of AZs in a regionFn::Join - construct complex strings; concatenate stringsFn::Select - value selection from list (0, 1)Ref - default value of resourceConditional Functions - Fn::And, Fn::Equals, Fn::If, Fn::Not, Fn::OrCFN Resource Deletion PoliciesA policy/setting which is associated with each resource in a template; A way to control what happens to each resource when a stack is deleted.Policy value - Delete (Default), Retain, SnapshotDelete - Useful for testing environment, CI/CD/QA workflows,Presales, Short Lifecycle/Immutable env.Retain - live beyond lifcycle of stack; Windows Server Platform (AD), Servers with state, SQL, Exchange, File Servers,Non immutable architectures.Snapshot - restricted policy type only available for EBS volumes; takes snapshot before deleting for recovering data.Immutable Architecture - Replace infra instead of upgrading or repairing faulty components, treat servers as unchangeable objects, don't diagnose and fix, throw away and re-create, Nothing bootstraped except AMI.CFN Stack updatesstack policy is checked, updates can be prevented; absence ofstack policy allow all updates; stack policy cannot be deleted once applied. Once stack policy applied ALL objects are protected, Update is denied; to remove default DENY, explicit allow is required; can be applied to a single resource(id)/Wild card/NotResource; Has Principal and Action; Condition element (resource type) can also be used.Stack updates: 4 Types - Update with No Interrupion, Some Interruption, Replacement, Delete"}
{"title": "A Proven and Comprehensive Pattern for Building an API with Rust and Lambda", "published_at": 1710615491, "tags": ["aws", "serverless", "rust", "tutorial"], "user": "Benjamen Pyle", "url": "https://dev.to/aws-builders/a-proven-and-comprehensive-pattern-for-building-an-api-with-rust-and-lambda-2aol", "details": "I've been encouraged lately by all of the Rust and Serverless content that has been posted on various platforms.  I've also been public about the fact that I believe that Rust adoption with Serverless would be further along if there was more quality content written on the topic.  I know for certain that there is interest from developers about whether they should and how would they introduce Rust into their builds and while I've tackled pieces of the puzzle, I haven't taken on a fully working CRUD API.  This article looks to change that.  Let's dive into building an API with Rust and Lambda.API With Rust and LambdaBefore diving into the code, let's take a look at the designed implementation. This design is probably similar to the ones you've implemented yourself in other languages.  For a quick aside, this article isn't intended to start or continue any debates on Lambda per endpoint (verb) vs monolithic Lambdas but to walkthrough an approach for designing your API with Rust and Lambda.With any REST-based Create, Read, Update, and Delete API, there will be paths for each operation that support the different verbs.  The API in this article will cover the following endpoints./POST - create new Items/GET - fetches all items (supports pagination)=/{id}- GET - fetches an item by id/{id}- PUT - updates an item by id/{id}- DELETE - deletes an item by idWhy Rust AgainI've shared these in serval places but for a recap on why I'm so focused and interested in seeing Rust more in the Serverless ecosystem.The languageEnums are so powerfulFunctional and OOP concepts are easy to reach forCrates are maybe the best package management setup I've worked withType safety and no garbage collection (once you get over the borrow checker)Performance - not much to say here, Rust is fastConsumption of resources - I strongly believe in sustainability, and Rust helps with that as it uses so little compared to so many othersThe Serverless ExperienceAnd if you still want tolook aroundat more of my reasons, you can find them at that link.Project SetupBefore digging into the Lambdas, I want to share the project setup and the Rust-specific callouts.  This structure is a great way to build an API with Rust and Lambda.CDKBreaking down that layout, I prefer to organize my CDK code in theinfradirectory.  I like to put mycontructsin one place as well under thelibdirectory.  And then I put my stack building code inbin.  I find that just separating my infra from my application source code allows me to collapse my brain when I'm not working on that part of the application.LambdasWhen building an API with Rust and Lambda you can either package everything up in one Lambda or break the endpoints up into separate functions.  For this article, I'm going with the latter which means I've got separate executables for each Lambda function.The notion of shared code seems to come up all the time.  With compiled languages like Rust, layers don't make much sense unless it's an extension.  So for sharing code, I'm using Rust's crate system to build alibthat can be referenced by each Lambda. All of this is accomplished by using Cargo'sWorkspace concept.In the root of the project, there is a top-levelCargo.tomlfile that looks like this:[workspace]members=[\"lambdas/shared\",\"lambdas/post\",\"lambdas/get-by-id\",\"lambdas/delete-by-id\",\"lambdas/put-by-id\",\"lambdas/get-all\",]resolver=\"2\"Enter fullscreen modeExit fullscreen modeBy having a Cargo file that defines the workspace, I can have different binaries share some of the same code while also isolating those same binaries with their specific builds and dependencies.Working through the CodeThe way that I generally think about building APIs is this.  I like to think about the data model and access patterns first.  This is true even if I'm using an RDBMS.  In this article, to build an API with Rust and Lambda I'm using DynamoDB but the same principles apply.  Once I've got a model and some access patterns, I like to work back up to the top and think about my endpoints and the contracts.  My first endpoint is usually a POST so I can create items.  And that's where I'll begin.Creating ItemsThe create item Lambda is located in thelambdas/postdirectory and is marked in Cargo as abinor binary project.  Each Lambda in the API has the opportunity to define its own dependencies and package-specific settings.  I mentioned above that I have a shared library crate, it's defined in the Cargo file like this:[dependencies]shared={path=\"../shared\"}Enter fullscreen modeExit fullscreen modeI tend to put models like entities and data transfer objects in shared.  Other things that go in there are client creation, response building, and errors.  The things that get reused and cut down on code duplication without creating unnecessary dependencies are perfect for this library.Back to the create item Lambda.I'm only going to walk through themainfunction once because it looks almost identical in each Lambda.Main FunctionIn a language like Rust,mainis the entry point for the compiled program.  As I mentioned earlier, this API with Rust and Lambda contains a binary for each endpoint.  My function definition sets up defaults, builds the DyanmoDB client, and sets up the handler to be executed.#[tokio::main]asyncfnmain()->Result<(),Error>{// Logging and trace outputletfiltered_layer=tracing_subscriber::fmt::layer().pretty().json().with_target(true).with_file(true).with_filter(LevelFilter::INFO);tracing_subscriber::registry().with(filtered_layer).init();// helps direct how to build the DynamoDB clientletis_local=std::env::var(\"IS_LOCAL\").unwrap_or(\"false\".to_string());letclient=shared::clients::lambda_ddb_client::new_client(is_local).await;lettable_name=&std::env::var(\"TABLE_NAME\").expect(\"TABLE_NAME must be set\");letshared_client=&client;// the handler coderun(service_fn(move|event:Request|asyncmove{function_handler(table_name,shared_client,event).await})).await}Enter fullscreen modeExit fullscreen modeThe things to pay attention to in this function are that I'm using an IS_LOCAL variable and expecting a TABLE_NAME variable so that I can build the shared DynamoDB client.  By building a shared client, I get reuse out of that object and don't have to go through the latency of building it up each time.Create HandlerI'm going to do the same thing on this handler as I did with the main function in that I'll share more details and then in subsequent handlers just highlight the important pieces.  The full source code is available at the bottom of the article.asyncfnfunction_handler(table_name:&str,client:&Client,event:Request,)->Result<implIntoResponse,Error>{letbody=event.payload::<BasicEntityCreateDto>()?;letmutreturn_body=json!(\"\").to_string();letmutstatus_code=StatusCode::OK;matchbody{Some(v)=>{lete:BasicEntity=v.into();letr=create_item(client,table_name,e).await;matchr{Ok(v)=>{letdto=BasicEntityViewDto::from(v);return_body=serde_json::to_string(&dto).unwrap();}Err(e)=>{error!(\"Error saving entity: {}\",e);status_code=StatusCode::BAD_REQUEST;return_body=serde_json::to_string(\"Error saving entity\").unwrap()}}}None=>{status_code=StatusCode::BAD_REQUEST;}}letresponse=Response::builder().status(status_code).header(\"Content-Type\",\"application/json\").body(return_body).map_err(Box::new)?;Ok(response)}Enter fullscreen modeExit fullscreen modeBreaking down the handlerThe first item I want you to pay attention to islet body = event.payload::<BasicEntityCreateDto>()?;.  This code converts the incoming request into my custom data transfer object that is in the shared library.  That struct is defined with a few fields.  Notice the macros for Debug and Deserialize that help with tracing and automatic conversion from JSON to this struct.#[derive(Debug,Deserialize)]pubstructBasicEntityCreateDto{pubname:String,pubdescription:String,}Enter fullscreen modeExit fullscreen modeAuto conversionThe next block of code that will play itself out in the other handlers is the conversion of structs and the operations with the database.  I tend to work with my domain models and DynamoDB, not my DTOs.lete:BasicEntity=v.into();letr=create_item(client,table_name,e).await;Enter fullscreen modeExit fullscreen modeThis auto-conversion is handled by the DTO implementing the into trait.implInto<BasicEntity>forBasicEntityCreateDto{fninto(self)->BasicEntity{letksuid=Ksuid::new(None,None);letdt=Utc::now();lettimestamp:i64=dt.timestamp();BasicEntity::new(ksuid.to_string(),self.name,self.description,\"BasicEntity\".to_string(),timestamp,timestamp,)}}Enter fullscreen modeExit fullscreen modeWorking with DynamoDBI wouldn't have an API with Rust and Lambda without storing the entities that I'm working with.  The AWS SDK for Rust is a breeze to work with and makes interacting with DynamoDB efficient and predictable.  Theput_itemfunction creates the DynamoDB PutItemRequest and then executes it.client.put_item().item(\"id\".to_string(),AttributeValue::S(item.get_id())).item(\"name\".to_string(),AttributeValue::S(item.get_name())).item(\"description\".to_string(),AttributeValue::S(item.get_description()),).item(\"entity_type\".to_string(),AttributeValue::S(item.get_entity_type()),).item(\"updated_at\".to_string(),AttributeValue::N(item.get_updated_at().to_string()),).item(\"created_at\".to_string(),AttributeValue::N(item.get_created_at().to_string()),).table_name(table_name).send().await{Ok(_)=>Ok(item),Err(e)=>Err(e.into()),}Enter fullscreen modeExit fullscreen modeOne of the things you'll see in theErrblock of this put is that I've also made a call tointo().  In the repository under the shared library, I have an enum with these values.  And for each of those values I provide a conversion so that I can take an error from say Serde or the AWS SDK and convert it into something that I want. This is a technique for building custom errors with Rust.#[derive(Error,Debug)]pubenumQueryError{#[error(\"failed to parse response into a user: {0}\")]SerdeError(serde_dynamo::Error),#[error(\"aws_sdk_dynamodb error: {0}\")]DynamoError(aws_sdk_dynamodb::Error),#[error(\"aws_sdk_dynamodb::error:: error: {0}\")]DynamoSdkError(String),#[error(\"item not found\")]NotFound,}Enter fullscreen modeExit fullscreen modeGet and Put by IDThe GET and PUT by ID functions in an API with Rust and Lambda don't have a great deal of nuance to them but I do want to highlight how to access path variables which are common to both of these handlers.Accessing path variablesThroughout this project, I'm using thelambda_httpcrate that is provided by the AWS Labs team.  There are several useful functions that this crate provides and one of them is working with the path and query string parameters.Any Lambda in the API that works with the path will need to be able to fetch out those key elements.  In my case, I have an{id}in the path that I need to fetch.letpath_id=request.path_parameters_ref().and_then(|params|params.first(\"id\")).unwrap();Enter fullscreen modeExit fullscreen modeThe above code will return an&strthat I can then further use in the handler.  I generally don't want tounwrap()something but rather evaluate theOptionbut in this case, API Gateway won't call this function if it's missing a path variable.  So it's safe in that regard.Working with DynamoDBI highlighted above how to execute a PutItem request but I haven't shown the Query request.  I use this query in the Get and Put to make sure I have the right item and can then apply any updated values on theBasicEntity.letoutput=client.get_item().key(\"id\".to_string(),AttributeValue::S(id.to_string())).table_name(table_name).send().await?;matchoutput.item{Some(item)=>{leti:BasicEntity=serde_dynamo::from_item(item)?;Ok(i)}None=>Err(QueryError::NotFound),}Enter fullscreen modeExit fullscreen modeAgain, working with DynamoDB via the SDK is straightforward.  I'm issuing aget_itemwith thekeydefined with the nameidand the typeAttributeValue::S. In the case that I don't find an item, I'm using my custom QueryError to bring back a NotFound result.Deleting an ItemMy API with Rust and Lambda wouldn't be complete without a DELETE endpoint.  Deleting in DynamoDB with the SDK is just as well supported as the other operations.  My handler also doesn't differ that much other than I'm returning 204 NO_CONTENT vs the 200 or 201 I'm returning in other handlers.letmutstatus_code=StatusCode::NO_CONTENT;matchpath_id{Some(id)=>{leti:Result<(),QueryError>=delete_item(client,table_name,id).await;matchi{Ok(_)=>{}Err(_)=>{status_code=StatusCode::NOT_FOUND;}}}None=>{status_code=StatusCode::NOT_FOUND;}}Enter fullscreen modeExit fullscreen modeThe part that is worth mentioning is how I know that the DynamoDB API returned that the was not found.  By requesting the old values of the item and then checking for attributes.// the builder is above here.return_values(aws_sdk_dynamodb::types::ReturnValue::AllOld)// checking attributesmatchoutput.attributes(){Some(_)=>Ok(()),None=>Err(QueryError::NotFound),}Enter fullscreen modeExit fullscreen modeGet items with paginationThe last piece of this API with Rust and Lambda that I want to walk through is how to build a paginated API with the SDK.My handler code looks a lot like other handlers, but instead of using the path, I'm looking forlkin the query string.  Thislkequals the last key that was evaluated in the query I'm about to walk through.  With the last key, I can tell the DynamoDB API where to start in the records.  I'm using a scan for this because I'm looking at all of the records in the table but limiting the number I'm pulling at a time.letlast_key=request.query_string_parameters_ref().and_then(|params|params.first(\"lk\")).unwrap_or_else(||\"\").to_string();Enter fullscreen modeExit fullscreen modeUnlike other operations where I'm unwrapping the value in the path parameters, I'm usingunwrap_or_elseso that I don't experience a panic from accessing something that has no value.  If nothing is in thelkparameter, I'm just using\"\".The queryQuerying all of the items in the table for our API with Rust and Lambda follows the same patterns that are in the other DynamoDB SDK functions.However, I first want to build the last key expression. Since my table has one key, I only have one value in the Hashmap that ultimately must be anOption.letmutkey=None;iflast_key!=\"\"{letmutevaluated_key:HashMap<String,aws_sdk_dynamodb::types::AttributeValue>=HashMap::new();evaluated_key.insert(\"id\".to_string(),aws_sdk_dynamodb::types::AttributeValue::S(last_key),);key=Some(evaluated_key);}Enter fullscreen modeExit fullscreen modeI can then set the start key position by including it in the Scan Fluent Builder.letoutput=client.scan().set_exclusive_start_key(key).limit(limit).table_name(table_name).send().await?;Enter fullscreen modeExit fullscreen modeLastly, if it's present, make sure I send it back to the client so that they can supply the last key in subsequent requests to page through the records.ifoutput.last_evaluated_key.is_some(){letkey=output.last_evaluated_key.unwrap();letkey_value=key.get(\"id\").unwrap();letstring_value=key_value.as_s().unwrap().to_string();last_key=string_value;}Enter fullscreen modeExit fullscreen modeDeploying and runningBefore I wrap up the article, I want to demonstrate how to get the repository deployed and give you something to work with.  I tried to find a nice balance of diving into code while also explaining the why.  This approach and structure to building an API with Rust and Lambda have served me well when shipping to production and providing customer value.  Using Cargo and a shared library crate also makes reuse a breeze.DeployingBeing that this is a CDK project, deploying the API with Rust and Lambda requires running this command from the terminal.cdk deployEnter fullscreen modeExit fullscreen modeI've built this with a single stack that'll deploy the Lambdas, DynamoDB Table, and API Gateway.Running the APIOnce deployed, visit the AWS Console and find the AWS-assigned URL to your new API Gateway.  With that value, you can load Postman and launch the included Postman collection.  It has a variable namedAPI_ENDPOINTwhich is where the assigned URL needs to go.My recommendation is you start with the POST endpoint, create some new items, and then explore from there.Clean upThe last piece of this is that when you are done, just runcdk destroyin the project directory and everything will clean up.Wrapping upI struggled to put this piece together for a few weeks.  I haven't tackled something this big that shows this much of an application's build.  What spurred this on though is that I'm going to be using this piece in a couple of talks this spring.  There has been a tremendous amount of interest in Rust and Serverless so I'm feeling grateful to have the opportunity to share how amazing the Rust and Lambda experience is.  And I know that if something like this existed when I was starting my API with Rust and Lambda journey, I would have been thankful.I've mentioned a few times the repository,and here is the linkto all of the cod above and more.  I'd recommend you clone the repository and start playing around.  Learning Rust can be tough without a purpose, but learning Rust while building an API should help solidify some concepts.The reasons for Rust in the Serverless ecosystem are numerous.  It's a solid language that is strongly typed, has a great library system, is highly-performant, and is a great choice if ecological sustainability matters to you.  And with all of the amazing new content that seems to show up weekly, learning Rust and Serverless has never been easier.Thanks so much for reading this piece and happy building!"}
{"title": "DevOps with Guruu | Chapter 5 : Complete CI/CD Real Project | Build Full your CI/CD Pipeline", "published_at": 1710602747, "tags": ["webdev", "devops", "aws", "cicd"], "user": "Hoang Guruu", "url": "https://dev.to/aws-builders/devops-with-guruu-chapter-5-complete-cicd-real-project-build-full-your-cicd-pipeline-1fn4", "details": "DevOps with Guruu | Chapter 5 : Complete CI/CD Real Project | Build Full your CI/CD Pipeline0:00 Welcome to my Series DevOps with Guruu2:00 Setup Cloud9 Environment4:30 Install eksctl \u2013 helm \u2013 kubectl, necessary tools7:30 Configure access key for Admin user15:00 Install Jenkins46:00 Setup EKS Cluster with AWS CLI ( Chapter 4 )46:30 Setup EKS NodeGroup with AWS CLI ( Chapter 4 )55:40 Install Argocd1:09:00 Config Secret Pull Image1:26:00 Build complete CI/CD1:35:00 Work with Domain1:38:00 Install Grafana PrometheusJoin me on this journey to mastering DevOps. Let's get hands-on and build a strong foundation together! Don't forget to like, share, and subscribe for more exciting content. Happy coding, future DevOps!\""}
{"title": "How to set up AWS Budget Alerts to prevent surprises", "published_at": 1710517006, "tags": ["aws", "cloud", "budget", "notifications"], "user": "Rashwan Lazkani", "url": "https://dev.to/aws-builders/how-to-set-up-aws-budget-alerts-to-prevent-surprises-3l38", "details": "In this post, I'll explain how to set up an AWS Budget Alert and why it's important.Why is it good to set up a budget alert?You can use AWS Budgets to track and take action on your AWS costs and usage. You can use AWS Budgets to monitor your aggregate utilization and coverage metrics for your Reserved Instances (RIs), or Savings Plans.You can monitor and receive notifications on your budgets free of charge. In addition to budget monitoring, you can add actions to your budgets to control IAM and Service Control Policy permissions as well as AWS resources when thresholds are exceeded (or forecasted to exceed).How to set up AWS Budget AlertsBegin by clicking on your logged-in name located in the top-right corner, then select \"Billing and Cost Management\" from the dropdown menu.If you encounter an error message stating \"You Need Permissions,\" it indicates that you are using an IAM user without access to the Billing and Cost Management dashboard.To address this issue, follow these steps:Login as the Root userClick on you Root account on the top-right corner and select \"My Account\"Scroll down to \"IAM User and Role Access to Billing Information\"Click \"Edit\"Check the \"Activate IAM Access\" checkboxClick \"Update\"Now you should be able to set up budgets using your IAM user. Follow these steps to set up a budget:Go to the \"Billing and Cost Management\"Click \"Budgets\" on the left paneClick \"Create a Budget\"Here select a budget type, I have selected a monthly budget of 100$Monitor the created budgetYou can always navigate to \"Billing and Cost Management\" > \"Budgets\" and select your budget to monitor it. And also:Compare current spending to budgeted amountCompare forecasted spending to budgeted amount (Month-to-date)See your budget historyChange the budget periodUpdate the budgeted amount ($)Set the budget scope by adding filtering and use advanced options to narrow the set of cost information tracked as part of this budgetUpdate the email recipientsSetup Amazon SNS alertsSetup AWS Chatbot Alerts - AWS Chatbot lets you send budget alerts to the Amazon Chime, Slack or Microsoft Teams chat rooms of your choice which can by handySet alert thresholds - When should this alert be triggered? And how should this alert be triggered?ConclusionSetting up budget alerts is essential to avoid surprises. Additionally, I recommend configuring AWS Chatbot Alerts to receive notifications in your preferred channel. If you have any questions or thoughts, please feel free to comment!"}
{"title": "Amazon QuickSight", "published_at": 1710516978, "tags": [], "user": "Jasper Samuel J", "url": "https://dev.to/aws-builders/amazon-quicksight-5ebi", "details": "It's often hard to analyze the data and learn from it  for those who are not familiar with programming languages, that's where QuickSight comes in.QuickSight is business analytics and intelligence  service from AWS, business users can collect meaningful insights.Unlike PowerBI this is not a huge application to download and play around, QuickSight is a web based application which makes it easy for us to work on our data without worrying about storage and security.FeaturesData IntegrationThe biggest problem with most of the analytics and visualization applications are data integration. In QuickSight we can connect data from various sources which includes all the AWS services and third party services like MySQL, SQL Server etc.ML InsightsQuickSight will deliver deeper insights and trends without manual analysis based on the Machine learning algorithm which powers Amazon.com for the past two decades.Data VisualizationVisualization is paramount for any analytics application as it facilitates comprehension of data for business stakeholders, empowering them to make informed decisions effortlessly, QuickSight have the option to create wide range of options to create graphs, charts, heatmap etc.CollaborationQuickSight reports are easy to share and embed on your reports without worrying about the security concerns.SecurityQuickSight is secured by Role based access control of AWS's IAM this will help you to save your business data, reports from others without your permission.PricingQuickSight offers an unique pricing models where users charged on each sessions initiated, this will help you to save tons of money.Login into the AWS Console and search for QuickSightClick on New analysis and then new datasetYou can choose the data sources to import the dataFor time being I'll navigate you around to quicksight using a sample data.You can add new data by clicking data and add dataset.You can drag and drop the fields from left to the graph to insights.You can publish your report by clicking on publish"}
{"title": "[How to] Implementar Ephemeral environments para Pull Request en GitHub", "published_at": 1710515706, "tags": ["cloudnative", "productivity", "developmentcycle", "development"], "user": "Hector Fernandez CloudparaTodo", "url": "https://dev.to/aws-builders/how-to-implementar-ephemeral-environments-para-pull-request-en-github-1g9c", "details": "Hoy te quiero contar c\u00f3mo puedes crear tus propios ambientes ef\u00edmeros usando las herramientas deIaC(Infrastructure as Code) que ya tienes. En mi caso estar\u00e9 utilizandoAWS CDKpara el despliegue en la nube, pero el tutorial sirve para cualquierIaC.Tener la posibilidad de usar Ephemeral environments (ambientes ef\u00edmeros) al momento de crear una Pull Request es sin dudas una de las mejores herramientas para aumentar la productividad en el desarrollo de software nativo usando la nube. Ya que te permite tener un ambiente listo para hacer tus pruebas de integraci\u00f3n (E2E) justo al momento de terminar de codear los cambios.\u00bfQu\u00e9 es un Ephemeral environments o Preview environment?Recordemos que los ambientes \u201cephemeral\u201d tienen un ciclo de vida corto, fueron creados para una finalidad en particular (en nuestro caso validar el c\u00f3digo al momento de la creaci\u00f3n de una Pull Request), deben ser lo m\u00e1s parecido al ambiente de producci\u00f3n y estar disponibles cuando el programador los necesite. Muy importante luego de ser utilizados deben ser destruidos con herramientas automatizadas.Te dejo un post de mi blog con todos los detalles para que puedas profundizar m\u00e1s...https://blog.hectorfernandez.dev/ephemeral-environments-como-crearlos\u00bfQu\u00e9 debes de tener?Repositorio de c\u00f3digo (GIT)Cuenta de AWSIaC para el despliegue de tus recursosAWS CLIconfigurado en tu maquinaLos siguientes pasos est\u00e1n dentro de lacapa gratuita de AWS, pero seg\u00fan tu caso de uso puedes incurrir en costos.1) Identificar nuestra infraestructura (en el c\u00f3digo)El primer paso que debemos de debemos hacer es identificar nuestra infraestructura, por ejemplo\u00bfusamos contenedores?, usamos servicios de almacenamiento persistentes (S3)?\u00bfC\u00f3mo estos servicios est\u00e1n referenciados en nuestro c\u00f3digo?.\u00bfPero saber en qu\u00e9 parte del c\u00f3digo se hace referencia a servicios externos en qu\u00e9 me ayuda?por ahora te dir\u00eda much\u00edsimo.Al conocer c\u00f3mo interact\u00faa nuestra aplicaci\u00f3n nos permite identificar cu\u00e1les par\u00e1metros pueden ser pasados por ejemplo mediante variables de entorno. Cuando creamos ambientes de\u201cpreview\u201dc\u00f3mo tambi\u00e9n se le suele llamar, debemos saber que los nombres de cada entidad son autogenerados, por lo tanto no podremos decidir el nombre o dejar ese nombre de forma est\u00e1tica en el c\u00f3digo.Te muestro un ejemplo, para entender mejor el punto anterior.const msg =     process.env.BUCKET_NAME === \"prod\"       ? \"Production Mode \"       : \"Preview Enviroment\";    return {     statusCode: 200,     headers: { \"Content-Type\": \"application/json\" },     body: JSON.stringify({       message: `Hello from Lambda ${msg} using CDK`,     }),   };Enter fullscreen modeExit fullscreen modeEste c\u00f3digo es parte de la funci\u00f3nserverlessde lambda, d\u00f3nde dependiendo de la variable de entorno pasada se comporta de una u otra manera(esto puede ser tan complejo c\u00f3mo tu soluci\u00f3n lo necesite)es meramente para ilustrar la importancia deidentificarla parte que el c\u00f3digo hace uso de la infraestructura creada.2) High-level architectureEn este template podremos ver la idea general de la implementaci\u00f3n y c\u00f3mo quedar\u00eda nuestra cuenta luego de tener m\u00e1s de 1 ambiente temporal desplegado.Como podemos observar nuestra arquitectura es bastante simple para ense\u00f1arles la integraci\u00f3n que puede tener. La idea es que un ambiente productivo real siga la misma idea pero escalado a la cantidad de recursos que ya tienes.3) Stack de AWS CDK4) Github Actions que se encargar\u00e1 de hacer todo esto posible5) Puntos importantesSintetizar nuestra plantilla de cloudformation:40% m\u00e1s rapido la creaci\u00f3n de stacks cloudfromation(Si usas otra herramienta de IaC, sigue los pasos que te indica esa otra herramienta)Hacer el deploy del stack:Un punto aca super importante es que le pasamos como par\u00e1metro el ID del pull request que estamos creando, de esta forma se crea un ID \u00fanico.Validar los cambios que hicimosYa con esto tendr\u00edamos nuestroambiente desplegado en una cuenta compartidaentre todos los desarrolladores pero a su vez cada quien con su stack propio, permitiendo que las pruebas se puedan hacer de forma aislada.6)Revisando nuestros recursos en AWSSi vamos a cloudformation desde la consola tendremos algo similar a esto. Tendremos nuestrasURL listaspara utilizar \u00a1Lo logramos!ya con esto vamos a poder hacer nuestras pruebas e2e o cualquier prueba de humo sin mayor esfuerzo.Tips para mejorar esta soluci\u00f3n:Tener una colecci\u00f3n de Postman que pueda correr mediante CI / CD cada vez que se cree una Pull Request, de tal manera de asegurar la integridad de la aplicaci\u00f3n.Implementar comentarios autom\u00e1ticos en Github, de esta forma la URL del ambiente creado queda disponible para todos los reviewers.Solo hacer despliegue de los componente que sean necesarios seg\u00fan el c\u00f3digo que se toc\u00f3Definir un tiempo de vida para el stack, ya que si alguien deja esa PR por mucho tiempo pueda tener un ciclo de vida.\u00bfHabias utilizado algo similar antes? Me gustar\u00eda saber tu opini\u00f3n de estipo de ambientes en un ciclo de desarrollo continuo.Connect with meLinkedIn :https://www.linkedin.com/in/hectorfernandezdev/GitHub :https://github.com/hectorfernandezdevPodcast:https://podcast.hectorfernandez.dev/Let's keep building"}
{"title": "Como utilizar solu\u00e7\u00e3o PACS Fujifilm Storage para AWS", "published_at": 1710511490, "tags": ["awscommunitybuilders", "s3", "pacs", "fujifilm"], "user": "Carlos Filho", "url": "https://dev.to/aws-builders/como-utilizar-solucao-pacs-fujifilm-storage-para-aws-91d", "details": "A ideia deste post \u00e9 mostrar como podemos utilizar alguns servi\u00e7os de Storage da AWS para migrarmos imagens de exames m\u00e9dicos de um storage local para AWS.Sabemos que existem diversos servi\u00e7os de PACS (Picture Archiving and Communication System) que s\u00e3o utilizados para armazenar todas as imagens dos exames realizados em cl\u00ednicas ou hospitais.Ent\u00e3o sigamos em frente com o post com o cen\u00e1rio abaixo:Imaginemos o seguinte ponto:O cliente relatou que possui um arquivamento de 8TB mensal, tamb\u00e9m possui em torno de 450TB em fitas de armazenamentos que devido \u00e0 uma lei brasileira, \u00e9 necess\u00e1rio manter os exames armazenados por 20 anos.O cliente utiliza a solu\u00e7\u00e3o de PACS da Fujifilm que possui uma particularidade para se integrar com a AWS, pois n\u00e3o h\u00e1 campo de integra\u00e7\u00e3o de se conectar diretamente via S3 atrav\u00e9s de um usu\u00e1rio program\u00e1tico comAccess e Secret Keyse com permiss\u00f5es espec\u00edficas. Dessa forma seguiremos com um Storage Gateway para servir de intermedi\u00e1rio de envio e recebimento de dados.Em torno de 250 GB ser\u00e3o coletados da AWS (m\u00e9dia/m\u00eas) para consulta de exames.Como premissa o cliente exigiu um Direct Connect por conta da velocidade do link de internet local ser de baixa banda larga.Para isso propomos o seguinte cen\u00e1rio ao cliente:Entendendo a arquitetura de refer\u00eancia:Data Center CorporativoStorage representa os sistemas de armazenamento de dados do data center corporativo, onde os dados ativos s\u00e3o mantidos.Tape Storage representa o sistema de armazenamento em fitas, utilizado para backup e arquivamento de dados a longo prazo.AWS CloudRegi\u00e3o depender\u00e1 de onde voc\u00ea ir\u00e1 provisionar os recursos, geralmente o cliente precisa, por conta da LGPD (GPDR), implementar o ambiente em seu pa\u00eds local.Storage Gateway uma ponte que conecta o ambiente local \u00e0 nuvem AWS. Permitindo que os dados do storage local sejam facilmente transferidos e armazenados na AWS.AWS Storage Gateway Tape Gateway tamb\u00e9m foi atrelado no projeto porque envolve a convers\u00e3o de dados de fitas f\u00edsicas para a nuvem, o Tape Gateway \u00e9 a solu\u00e7\u00e3o se adequa, pois facilita a migra\u00e7\u00e3o desses dados para a AWS.O Tape Gateway simula uma infraestrutura de fita virtual que pode ser integrada com o sistema de backup existente, permitindo que sejam armazenados os dados de fita virtualmente no Amazon S3 e no Amazon Glacier ou Glacier Deep Archive para custos de armazenamento mais baixos.Este servi\u00e7o ser\u00e1 interligado com o Direct Connect para acelerar o envio dos dados.Com o S3 Standard o destino na AWS para armazenamento de dados transferidos atrav\u00e9s do Storage Gateway. Indicado para dados acessados frequentemente, que ap\u00f3s X per\u00edodo de tempo ser\u00e3o movidos via ciclo de vida de dados para uma camada de armazenamento mais fria denominada S3 Glacier Deep Archive para otimiza\u00e7\u00e3o de custos.Utilizando o S3 Glacier Deep Archive como uma op\u00e7\u00e3o de armazenamento de custo extremamente baixo para arquivamento de dados a longo prazo. Destinado para dados transferidos do Tape Storage via AWS Direct Connect.Seguindo com o Direct Connect que \u00e9 o servi\u00e7o que estabelecer\u00e1 uma conex\u00e3o de rede dedicada do data center corporativo para a AWS. Utilizado para transfer\u00eancias de dados de grande volume de maneira r\u00e1pida e segura, especialmente do Tape Gateway para o S3 Glacier Deep Archive.Fluxo de dadosDados s\u00e3o continuamente transferidos do Storage Corporativo para o AWS S3 Standard atrav\u00e9s do Storage Gateway para necessidades de recupera\u00e7\u00e3o r\u00e1pida e eficiente.Dados hist\u00f3ricos ou de longo prazo armazenados em Tape Storage s\u00e3o transferidos para o AWS S3 Glacier Deep Archive para arquivamento seguro e de baixo custo, utilizando AWS Direct Connect para garantir uma transfer\u00eancia de dados eficiente e com alta largura de banda.Quest\u00f5es levantadas pelo cliente:\u00c9 poss\u00edvel utilizar o Storage Gateway atrelado ao Direct Connect?Sim, voc\u00ea pode usar o AWS Direct Connect em conjunto com o AWS Storage Gateway na modalidade de Tape Gateway para acelerar o envio dos dados.Se o Direct Connect ser\u00e1 o canal que far\u00e1 a liga\u00e7\u00e3o entre o ambiente local das fitas para o armazenamento do S3 na AWS. Se faz necess\u00e1rio usar o Storage Gateway Tape Gateway?Basicamente, se voc\u00ea s\u00f3 precisa jogar um monte de dados para a AWS r\u00e1pido e n\u00e3o h\u00e1 import\u00e2ncia para o lance das fitas, o Direct Connect pode resolver seu problema. No entanto, se voc\u00ea estiver migrando de um sistema de backup baseado em fita, ou se precisa manter um fluxo de trabalho de backup baseado em fita, o Tape Gateway ser\u00e1 necess\u00e1rio, e o Direct Connect pode ser usado em conjunto para acelerar a transfer\u00eancia de dados para a nuvem.No mais, essa migra\u00e7\u00e3o \u00e9 bastante interessante para que possamos encontrar alternativas para poder atender o cliente e que ele possa executar e utilizar normalmente seu ambiente em nuvem com as melhores pr\u00e1ticas existentes.At\u00e9 a pr\u00f3xima."}
{"title": "Enhance Your AppSync API Development: Easy Steps to Auto-Generate Postman Collections", "published_at": 1710502321, "tags": ["aws", "graphql", "appsync", "serverless"], "user": "Lorenzo Hidalgo Gadea", "url": "https://dev.to/aws-builders/enhance-your-appsync-api-development-easy-steps-to-auto-generate-postman-collections-5f3i", "details": "TL;DR;This article covers how developers can take advantage of theserverless-gql-generatorplugin for Serverless Framework to automatically generate new Postman Collections or Raw Requests for an AppSync API.IntroductionA few weeks ago, I announced the release of a new Serverless Framework Plugin that I had been working on calledserverless-gql-generator.The idea for this plugin came from the difficulties faced during one of my projects where it was too time-consuming for developers to keep the sample requests and Postman Collection for every new schema update. This pain point was further intensified when we started to develop multiple interdependent AppSync APIs simultaneously, requiring multiple Postman Collections to be updated and shared across teams.In this article, we will cover how to integrate the Serverless Framework plugin into your development flow and CICD pipelines.Main Plugin FeaturesAs of writing this article, the current version1.2.1of the plugin has the following features:Automatic GraphQL Request Generation- The plugin will generate new requests on demand or during deployment, ensuring they are always up to date with the latest schema version.Automatic URL & API Key Retrieval- The URL and API Key will be automatically populated with the latest AppSync configuration.Choose between Inline or using variable file input- Developers can configure the plugin to generate requests with Inline input or with a separate file for variable input.Exports requests to independent Files and Postman Collections- Depending on the use case, developers can configure the plugin to export the generated requests to independent.graphqlfiles, a Postman Collect or both.Upload the generated files to S3- As the icing on the cake, this plugin also automates the upload of the generated files to the configured S3 bucket.PrerequisitesIn order to be able to implement this plugin into your workflow you will at least need:Node JS installationAWS Account to deploy the APIA project that uses the Serverless Framework to deploy an AppSync API - examples will be shown usingthis repositoryPostman,GraphBoltor any other tool to send requests and test the generated requestsInstallation ProcessInstalling and adding the plugin to your project is fairly straightforward to accomplish by following two simple steps.Install the plugin using NPMUse the following command to install the plugin and save it as adevDependencyin your project:npminstall-Dserverless-gql-generatorEnter fullscreen modeExit fullscreen modeAdd the Plugin to your projectAdd the plugin under thepluginslist in yourserverless.ymlfileservice:my-appplugins:-serverless-gql-generatorEnter fullscreen modeExit fullscreen modeUsing the pluginAfter adding the plugin to thepluginslist, it will start generating the Postman Collections every time the service is deployed.The experience can be improved further by configuring the plugin to match your needs better and by using the CLI commands to enable the request generation without the need to redeploy your service.Overriding the default behaviourThe plugin's default behaviour is to generate (and save locally under the./output/folder) a Postman Collection.This behaviour can be configured by overriding the defaults and adding any of the configuration attributes:service:my-appplugins:-serverless-gql-generatorgql-generator:schema:path:./schema.graphql# Overrides default schema pathencoding:utf-8assumeValidSDL:trueoutput:directory:./output# Output directorys3:# Enables Upload to AWS S3bucketName:gql-output-bucket# Mandatory Bucket namefolderPath:s3folder/path# Override Folder Path inside s3, defaults to `${service}/${stage}`skipLocalSaving:false# if the files should also be saved locally or notuseVariables:true# use variables or have the input inlinemaxDepth:10# max depth for schema recursionrawRequests:false# set to true to generate raw requestspostman:collectionName:test-name# Overrides colection name, defaults to `${service}-${stage}`url:https://test.com/graphql# Overrides url for postman collectionapiKey:abc-123# Overrides default API Key if anyEnter fullscreen modeExit fullscreen modeDefault Schema optionsThe plugin expects the schema to be in the root folder of the project and be called./schema.graphql, Developers can update the configuration undergql-generator.schemain order to update thepathorencodingof their schema.gql-generator:schema:path:./schema.graphqlencoding:utf-8assumeValidSDL:trueEnter fullscreen modeExit fullscreen modeDefault Request Generation OptionsOverriding the configuration on how the requests are being generated can be done by updating the following attributes undergql-generator.output.gql-generator:output:directory:./outputuseVariables:truemaxDepth:10rawRequests:falseEnter fullscreen modeExit fullscreen modeoutput.directory- path to the directory where the generated files should be stored atoutput.useVariables- flag to decide if the generated requests should have inline input or dedicated variable filesoutput.maxDepth- maximum allowed depth for the generated requests, used to avoid infinite recursions in the requestsoutput.rawRequests- flag to indicate if the raw requests (.graphqlfiles) should also be stored. This flag has to be set totrueifoutput.postmanis set tofalseDefault Postman Collection ConfigurationThe plugin will, by default, generate a Postman Collection that will be called based on${service}-${stage}and fetch the URL and API Key from the deployed API.gql-generator:output:postman:collectionName:test-nameurl:https://test.com/graphqlapiKey:abc-123# ORgql-generator:output:postman:falseEnter fullscreen modeExit fullscreen modeDevelopers can override this configuration by updating the following attributes:output.postman- can be set tofalseto avoid the Postman Collection being generated, for that scenariooutput.rawRequestswill be required to betrueoutput.postman.collectionName- used to override the name to be used for the generated collectionoutput.postman.url- used to override the API endpoint, especially useful if the API has a custom domain configured or you want to use the path to the CDN or Proxyoutput.postman.apiKey- used to override the API KEY to be used to authenticate the requestsUploading files to S3\u26a0Developers or CI/CD pipelines will require credentials with write access to the desired bucket.The output is only saved locally by default on the machine that generates the files, but there is the option to upload the files to S3 at the same time, making it even easier to share the results with other developers or teams.gql-generator:output:s3:bucketName:gql-output-bucketfolderPath:s3folder/pathskipLocalSaving:falseEnter fullscreen modeExit fullscreen modeIn order to enable the export to S3 features, developers will need to update the following configuration undergql-generator.output.s3:output.s3.bucketName-mandatoryfield, has to contain the name of anexisting bucketoutput.s3.folderPath- used to override the folder path in S3 where the files will be stored, by default it will create the following folder structure to store the files${service}/${stage}output.s3.skipLocalSaving- flag to indicate if the files should be stored locally or only uploaded to S3CLI commandsDevelopers might want to trigger the request generation without wanting to redeploy the API again, the plugin exposes some CLI commands to allow for that scenario.Schema ValidationWhen creating or updating a Schema developers might want to validate that it's formatted appropriately.The plugin exposes the following command to allow developers to validate the provided schema:serverless gql-generator validate-schemaEnter fullscreen modeExit fullscreen modeOnce the above line has been executed on the terminal, it will prompt any possible issues or confirm that the schema is valid and ready to be used.Requests generationBy default, the plugin generates all requests on every deployment, but there could be a scenario where one would like to generate them without deploying the API again.Developers might use the following command to trigger the Request generation:serverless gql-generator generateEnter fullscreen modeExit fullscreen modeOnce executed, the plugin will generate all requests based on the configuration.This command can be especially useful to confirm that the plugin is configured as expected or to generate the requests again in case the output folder has been added to.gitignore.CI/CD integrationThis plugin is easily integrated into one's CI/CD pipeline as it will be triggered automatically during the deployment process.To access the generated files, developers might choose between:Workflow Artifacts- Configuring the pipeline toexport the artifactsgenerated during the process would be equivalent to generating the files locally, but the developers would need to download them from the workflow execution.This approach is preferred for example for feature branches, where the output might change multiple times during the development.S3 Export- Output files will be uploaded automatically to S3 for easier access and sharing.This is the preferred approach for stable or shared branches, such asdevormain.ConclusionsAdding the serverless-gql-generator plugin to your stack can help you and your team save time by automating the process of generating and sharing GraphQL requests.I would love to hear about your experience using the plugin. Please useGitHub Issuesto report any issues while using the plugin or requesting new features."}
{"title": "Disaster Recovery Strategies on AWS: Ensuring Business Continuity", "published_at": 1710464886, "tags": ["aws", "devops", "cloud"], "user": "Guille Ojeda", "url": "https://dev.to/aws-builders/disaster-recovery-strategies-on-aws-ensuring-business-continuity-1lgh", "details": "We're now living in the world of immediate and always-on stuff, where even a few minutes of downtime can be a disaster for businesses. Customers expect 24/7 availability, and any interruption in service can lead to lost revenue, damaged reputation, and even legal consequences. That's where disaster recovery (DR) and business continuity planning come into play.Disaster recovery is all about preparing for the worst-case scenarios\u2014those unexpected events that can bring your systems to a halt. Whether it's a natural disaster, human error, or a cyber-attack (which is often also caused by human error), having a solid DR plan in place can make the difference between a minor hiccup and a catastrophic failure.Amazon Web Services (AWS) offers a wide variety of services and features to help you build robust, resilient architectures that can continue operating in the event of a disaster. In this article, we'll explore the key concepts and strategies for implementing effective disaster recovery on AWS.Understanding RTO and RPOBefore we dive into specific DR strategies, let's take a moment to define two critical metrics:Recovery Time Objective (RTO)andRecovery Point Objective (RPO).RTO is the maximum acceptable amount of time your systems can be down when a disaster occurs. In other words, it's the timeframe within which you need to restore your applications and data to avoid unacceptable consequences. For example, a financial trading platform might have an RTO of just a few minutes, while a less critical internal tool might have an RTO of several hours.RPO, on the other hand, refers to the maximum acceptable amount of data loss your business can tolerate. It's determined by how frequently you take backups and how much data you're willing to lose in the event of a disaster. For instance, an e-commerce site might have an RPO of just a few seconds, meaning they can only afford to lose a very small amount of data, while a blog might be okay with losing a day's worth of content.Your RTO and RPO will heavily influence your choice of DR strategies. The tighter your objectives, the more robust (and expensive) your DR solution will need to be. It's all about finding the right balance between cost and risk.Designing a Highly Available Architecture on AWSThe first step before even thinking about disaster recovery is a highly available architecture. On AWS, that means leveraging multiple Availability Zones (AZs) to build redundancy and fault tolerance into your applications.AWS operates a global network of data centers, grouped into regions and further subdivided into AZs. Each AZ is a fully isolated partition of the AWS infrastructure, with independent power, cooling, and networking. By deploying your applications across multiple AZs within a region, you can protect against failures at the data center level.Of course, building a highly available architecture involves more than just spreading your resources across AZs. You'll also need to implement load balancing and auto-scaling to distribute traffic evenly and automatically adjust capacity based on demand. Services like Amazon EC2 Auto Scaling and Elastic Load Balancing make this easy to achieve.But what if an entire region goes down? That's where multi-region architectures come into play. By replicating your data and applications across multiple AWS regions, you can ensure that even if an entire region becomes unavailable, your business can continue to operate from another location.That is what we call Disaster Recovery.Stop copying cloud solutions, startunderstandingthem. Join over 4000 devs, tech leads, and experts learning how to architect cloud solutions, not pass exams, with theSimple AWS newsletter.Disaster Recovery Strategies on AWSNow that we've covered the basics of high availability, let's explorefour common DR strategiesyou can implement on AWS: backup and restore, pilot light, warm standby, and multi-site active-active.Backup and Restore StrategyThe backup and restore strategy is the most basic and cost-effective approach to DR on AWS. It involves taking regular backups of your data and storing them in a secure, durable location like Amazon S3. In the event of a disaster, you can restore your systems from the most recent backup. While simple, this strategy typically involves significant downtime, as you'll need to provision new infrastructure and restore your data before your applications can be brought back online. It's best suited for non-critical workloads with lenient RTO and RPO requirements.Pilot Light StrategyThe pilot light strategy involves keeping a minimal version of your environment running in a secondary region, ready to scale up quickly in the event of a disaster. Core components, like your database servers, are always on, but application servers are kept in a stopped state to minimize costs. When disaster strikes, you can quickly start up your application servers, scale them out to handle the full production load, and redirect traffic to the secondary region. This approach offers faster recovery times than the backup and restore strategy, but still involves some downtime.Warm Standby StrategyThe warm standby strategy takes the pilot light approach a step further. Instead of keeping your secondary environment in a minimal state, you maintain a scaled-down version of your full production environment in the secondary region, with all components running. In the event of a disaster, you can rapidly scale up the secondary environment to handle the full production load. This strategy provides even faster recovery times than the pilot light approach, but comes with higher ongoing costs.Multi-Site Active-Active StrategyThe multi-site active-active strategy is the most comprehensive and expensive DR approach. It involves running your full production environment in multiple regions simultaneously, with each region serving traffic and replicating data in real-time. If one region fails, traffic is automatically routed to the other active region(s) without any interruption in service. This strategy provides the highest level of availability and the fastest recovery times, but also incurs the highest costs, as you're essentially running multiple copies of your entire infrastructure.How to Create Backups on AWSRegardless of which DR strategy you choose, creating regular backups is a critical component of any DR plan. AWS offers several backup services and features to help you protect your data.For Amazon EC2 instances, you can create point-in-time snapshots of your EBS volumes, which can be used to restore your instances to a previous state. You can automate the creation and management of EBS snapshots using AWS Backup, a fully managed backup service that simplifies the process of backing up your AWS resources.For managed database services like Amazon RDS and Amazon DynamoDB, automated backups are typically enabled by default. You can also create manual snapshots for longer-term retention or to copy your backups to another region for DR purposes.It's important to regularly test your backups to ensure they can be successfully restored in the event of a disaster. You should also consider implementing a backup retention policy to ensure you have the right balance of short-term and long-term backups to meet your RPO requirements.Replication and Failover StrategiesIn addition to backups, replication and failover are key components of many DR strategies on AWS. By replicating your data and applications across multiple regions, you can ensure that even if an entire region becomes unavailable, your business can continue to operate from another location.AWS offers several services and features to help you implement cross-region replication and failover. For example, you can use Amazon S3 Cross-Region Replication to automatically replicate objects across S3 buckets in different AWS regions. For databases, you can use Amazon RDS Read Replicas or Amazon Aurora Global Database to create cross-region read replicas that can be quickly promoted to standalone instances in the event of a disaster.When it comes to failover, you'll need to consider both application-level and DNS-level strategies. At the application level, you can use services like Amazon Route 53 Application Recovery Controller to continuously monitor your application's health and automatically route traffic to healthy resources in the event of a failure.For DNS failover, Amazon Route 53 offers a variety of routing policies that can help you direct traffic to the appropriate region based on factors like latency, geography, and resource health. By combining these strategies, you can create a robust, automated failover solution that minimizes downtime and ensures your applications remain available even in the face of a regional outage.Disaster Recovery Automation and TestingAutomation is key to implementing an effective DR strategy on AWS. By declaring your infrastructure as code using tools like AWS CloudFormation and Terraform, you can ensure that your DR environment can be quickly and consistently provisioned in the event of a disaster.Infrastructure as Code (IaC)not only speeds up the recovery process, but also reduces the risk of human error and ensures that your DR environment is always in a known, consistent state. You can use IaC templates to define everything from your network topology to your application configurations, making it easy to spin up an exact replica of your production environment in a secondary region.Regular testing is also essential to ensuring the viability of your DR plan. You should schedule periodic DR drills to simulate different failure scenarios and validate that your recovery processes work as expected. These drills can help you identify gaps in your plan and areas for improvement, ensuring that you're always prepared for a real-world disaster.Chaos Engineering on AWSIn addition to traditional DR testing, you may also want to consider implementing chaos engineering practices to proactively identify weaknesses in your systems. Chaos engineering involves intentionally injecting failures into your environment to test its resilience and uncover hidden vulnerabilities.AWS offers a service called AWS Fault Injection Simulator (FIS) that makes it easy to perform controlled chaos experiments on your AWS workloads. With FIS, you can simulate a variety of failure scenarios, like EC2 instance terminations, API throttling, and network latency, and observe how your applications respond.By regularly performing chaos experiments, you can build confidence in your systems' ability to withstand failures and identify opportunities for improvement before a real disaster strikes.Monitoring and Alerting for Disaster RecoveryEffective monitoring and alerting are critical components of any DR strategy. You need to be able to quickly detect and respond to issues before they escalate into full-blown disasters.AWS offers a range of monitoring and logging services, like Amazon CloudWatch and AWS X-Ray, that can help you gain visibility into the health and performance of your applications. CloudWatch allows you to collect and track metrics, collect and monitor log files, and set alarms that notify you when thresholds are breached. X-Ray helps you analyze and debug distributed applications, providing insights into how your services are interacting and performing.In addition to these services, you should also consider implementing a robust alerting strategy using Amazon Simple Notification Service (SNS). With SNS, you can send notifications via email, SMS, or even trigger automated remediation actions when specific events occur or thresholds are crossed.By combining comprehensive monitoring with proactive alerting, you can ensure that you're always aware of the state of your environment and can quickly respond to any issues that arise.Cost Optimization for Disaster RecoveryImplementing a comprehensive DR strategy can be expensive, especially if you're maintaining a fully replicated environment in a secondary region. However, there are several strategies you can use to optimize your costs without compromising on your DR objectives.One approach is to leverage AWS cost-saving features like Reserved Instances and Spot Instances for your DR environment. By purchasing Reserved Instances, you can significantly reduce your EC2 costs compared to On-Demand pricing. Spot Instances allow you to bid on spare EC2 capacity at steep discounts, which can be ideal for non-critical DR workloads.Another strategy is to tiered approach to DR, using different strategies for different parts of your application stack based on their criticality and recovery requirements. For example, you might use a multi-site active-active approach for your most critical databases, but a pilot light approach for less critical application tiers.Continuously monitoring and optimizing your DR costs is also important. You should regularly review your DR environment to identify any underutilized or unnecessary resources, and adjust your strategy accordingly. Tools like AWS Cost Explorer and AWS Budgets can help you track your spending and set alerts when you're approaching your budget limits.ConclusionImplementing an effective disaster recovery strategy on AWS requires careful planning, robust architecture, and regular testing and optimization. By leveraging the right mix of AWS services and features, you can create a DR solution that meets your business's unique requirements for availability, recovery time, and data protection.To recap, the four main DR strategies you can implement on AWS are:Backup and restore:Periodically backing up your data and resources, and restoring them in the event of a disaster.Pilot light:Maintaining a minimal version of your environment in a secondary region, ready to scale up when needed.Warm standby:Running a scaled-down version of your full environment in a secondary region, with the ability to quickly scale up to handle the full production load.Multi-site active-active:Running your full production environment simultaneously in multiple regions, with automatic failover between regions.Regardless of which strategy you choose, it's critical to regularly test and refine your DR plan to ensure it remains effective as your business evolves. By combining comprehensive monitoring, automated failover, and regular chaos engineering practices, you can build a resilient, highly available application that can weather any storm.Remember, disaster recovery planning isn't a one-time exercise\u2014it's an ongoing process that requires continuous improvement and optimization. By staying proactive and prepared, you can ensure that your business can continue to operate and thrive, no matter what challenges come your way.Stop copying cloud solutions, startunderstandingthem. Join over 4000 devs, tech leads, and experts learning how to architect cloud solutions, not pass exams, with theSimple AWS newsletter.Realscenarios and solutionsThewhybehind the solutionsBest practicesto improve themSubscribe for freeIf you'd like to know more about me, you can find meon LinkedInor atwww.guilleojeda.com"}
{"title": "Lambda Layers: The Secret Weapon of Savvy Serverless Developers", "published_at": 1710461068, "tags": ["aws", "lambda", "serverless"], "user": "Kasun de Silva", "url": "https://dev.to/aws-builders/lambda-layers-the-secret-weapon-of-savvy-serverless-developers-5fcd", "details": "When it comes to serverless development with AWS Lambda, keeping your code clean and efficient is essential. Lambda functions themselves have size limitations, and including all your dependencies within the function code can quickly bloat them. This is where Lambda layers come in \u2014 a powerful tool for streamlining your development process.What are Lambda Layers?Imagine a Lambda layer as a self-contained zip archive. This archive can hold various things, including:Library dependencies:Instead of bundling libraries with your function code, store them in a layer. This keeps your deployment packages leaner and easier to manage.Custom runtimes:Need a specific runtime environment for your function? Layers can provide that too.Configuration files:Separate your function\u2019s core logic from configuration details by storing them in a layer.Benefits of Using Lambda LayersSmaller Deployment Packages:By offloading dependencies to layers, you keep your function code concise and reduce deployment times.Improved Code Maintainability:Separating function code from dependencies makes it easier to update and manage each independently.Reusability:Layers can be attached to multiple functions, promoting code reuse and consistency across your serverless application.Standardized Runtimes:Manage custom runtimes through layers, continuing a consistent environment for your functions.Simple Layer ExampleLet\u2019s say you have a Lambda function that needs to make API requests. You can create a layer containing the requests library. Here's a basic example:Create a directory:Create a folder namedpythonfor your layer.Install the library:Inside the python directory, runpip install requests -t. This installs therequestslibrary into the current directory.Zip the directory:Use thezipcommand (or your OS's equivalent) to create a zip archive of the python directory. Name it something descriptive, likemy_requests_layer.zip.Now you have a layer containing therequestslibrary! You can upload this zip file to AWS Lambda and reference it in your function code.Using the Layer in Your Lambda FunctionOnce you have uploaded your layer to AWS Lambda, follow these steps to use it in your Lambda function:Navigate to Lambda functions in the AWS console.Click on the \u201cLayers\u201d section from the left menu.Click \u201cCreate layer\u201d, upload your zip file and hit \u201cCreate\u201d.Let\u2019s Create a Lambda functions that using therequestslibrary from a layer:importjsonimportrequestsdeflambda_handler(event,context):url=\"https://api.example.com/data\"# Using the requests library from the layerresponse=requests.get(url)data=json.loads(response.text)return{\"statusCode\":200,\"body\":json.dumps(data)}Enter fullscreen modeExit fullscreen modeCreate a new Lambda function using above code example.In the \u201ccode\u201d tab scroll all the way to bottom and find the Layers section.Click on \u201cAdd Layer\u201dSelect \u201cCustom layers\u201d and chose the \u201cpython_requests\u201d layer you just uploaded.Click \u201cAdd\u201dThen the custom layer you just created will be attached to your function.In this example, the function imports requests without needing it installed within the function code itself. This keeps the deployment package lean and leverages the reusable layer.Getting Started with AWS Lambda LayersUsing Lambda layers is straightforward. You create a zip archive containing your desired dependencies or configurations, upload it to AWS, and then reference it in your Lambda function code. Layers are versioned, so you can control exactly which version of the layer your function uses.Here are some additional resources to get you started:AWS Lambda Layers Documentation:https://docs.aws.amazon.com/lambda/latest/dg/chapter-layers.htmlUsing Lambda layers to simplify your development process:https://docs.aws.amazon.com/lambda/latest/dg/chapter-layers.htmlLambda layers are a valuable asset for any serverless developer on AWS. By leveraging layers, you can create cleaner, more manageable codebases, improve deployment efficiency, and promote code reuse across your serverless applications. So next time you\u2019re building serverless functions with AWS Lambda, consider using layers to simplify your workflow and streamline your development process."}
{"title": "Wecoded ! My own story lines 2024", "published_at": 1710452303, "tags": ["wecoded", "womenintech"], "user": "GargeeBhatnagar", "url": "https://dev.to/aws-builders/wecoded-my-own-story-lines-2024-50l1", "details": "It's all about ME and MY LIFE \ud83e\ude9cYou would have heard \"Opportunities don\u2019t happen, you have to create them\". This is exactly what I have implemented in my career. Starting a career as an Solution Architect is not easy but having a craziness to learn more about new things, gather learning opportunities, grab knowledge from wherever i can.And had also participated in various events, implemented the same in my daily work life. Few events that have really helped me to earn confidence in my work are AWS Summit, AWS Builder Series, AWS Sambhav, AWS re:Invent, AWS Public Sector Submit and many more. A point came in my life where I can say finally I have earned something huge as when I was \"Selected as AWS Ambassador from India\".Being a part of AWS Community Builder i had hosted various events, published blogs and conducted webinars.Also, being a part of AWS re:Skill community i had completed various challenges and earn badges.As being the first child from the family who has stepped out and built up a career, I have motivated my siblings to do something in life. Also in my college tenure, I have guided my juniors in taking various career related decisions. Also there are few people who have made me their inspiration and followed me throughout their career.My role model is my mother the way she has raised us and managed both of her professional and personal life and has never given up. I have truly followed her with a positive attitude of never giving up on whatever the situation is.Yes, there were various challenges i have gone through in my career as people talk about gender equality but they just talk, reality is still there are some points where people think no women can\u2019t do this or she should not do that. Still there are times where some men really don\u2019t want to work under a female boss or sometimes they don't trust your decision.I still remember the time when I had decided to start my career as an engineer and decided to step out. There were various people around me and even my relatives who had objected. She is a girl, the only work she can do is related to household and so on. People said to my parents that they are making a big mistake by sending a girl to study outside in a new city. But finally I proved them wrong and have tried to grab all the opportunities I can.I had taken my first step by getting admission in IIST Indore, enrolled for Bachelor of Engineering in Computer Science and then have chosen to complete Masters from Vellore Institute of Technology (M.Tech) in Computer Science with Specialization in Cloud Computing and then have started my professional career by working as an network engineer and then i have switched to a new company as a Cloud engineer. And then I joined Electromech Corporation as a Solution Architect.I have achieved certificates as AWS Solution Architect Associate and started writing blogs. In 2021, I have achieved my 2nd certification as AWS Certified DevOps Engineer Professional. And then I became an AWS Community Builder and with it I got an opportunity to publish blogs on the Dev community platform. I have also participated in various events to generate skills like AWS re:Invent, AWS Builder Series, AWS Sambhav for experiencing challenges. I have achieved my 3rd certification that is AWS Certified Data Analytics Speciality. Have conducted webinars and hosted various events. And finally I have received the greatest achievement to be selected as an AWS Ambassador from India.As we say life never ends giving you back if you constantly work hard and be back on your goal, I have received a great opportunity by Amazon to attend AWS re:Invent event in Las Vegas US. My trip is sponsored by AWS and I feel a lucky one to get a chance. But as every journey has a obstacles, I have tried hard to get visa for USA and got succeeded with it.I am continuously contributing in AWS Community as a Speaker, MOC Anchor and Volunteer.If you want to succeed in your life, you need to work hard and bridge the gap between where you are and where you want to be. People will definitely try to stop you but it\u2019s you who trust yourself, trust your own decisions, analyze and make the best out of it. Being a women is an honour and a pride, don't let it down for the people.If we really try to get something, goal is far from us but we definitely get it if we work hard with smart work\ud83d\ude00"}
{"title": "Access your AWS database using local port-forwarding on your ECS/Fargate container", "published_at": 1710409643, "tags": [], "user": "Paul SANTUS", "url": "https://dev.to/aws-builders/access-your-aws-database-using-local-port-forwarding-on-your-ecsfargate-container-4nk4", "details": "If your infrastructure is well-designed, your database is accessible only from the app it\u2019s supposed to serve. And, yes, ECS Exec enables to SSH on your containers, but you can\u2019t use it for port forwarding. How good it would be if it was possible to use your local tooling, like DBeaver or SQL Developer, to connect to the database in a secure way? Well it is, and this blog posts show you how.AWS-documented bastion solutions for ECS and EC2: a summaryAWS offers two distinct bastion solutions for your workloads. Both solutions rely on IAM for authentication (removing the need for static/shared credentials) and allow logging of all shell commands entered by the operator:On ECS, AWS offersECS Execfor workload debugging. The CLI command looks like this:aws ecs execute-command \\--profile  \\--region  \\--cluster  \\--task  \\--container  \\--interactive --command \"/bin/sh\"There a few pre-requisites:The task should be part of an ECS Service, not a standalone one\u201cEnable execute command\u201d should be set to true when creating the ECS serviceThe container process should have write access to a few directories (use mounts if you\u2019re otherwise following the good practice of hardening by not granting the container process write access to the host filesystem), namely/managed-agents,/var/lib/amazon/ssmand/var/log/amazon/ssm.The operator should have the Session Manager plugin for AWS CLI installed.2. On EC2, theSystems Manager (SSM) Start-Sessioncommand makes it possible open an interactive shell and, via the use of SSM Documents, namely the \u201cAWS-StartPortForwardingSessionToRemoteHost\u201d document, to enable port forwarding to a remote host.SSM start-session requires a target that is an EC2 instance ID.The trick: how to use SSM on a Fargate-hosted containerThe problem on a a Fargate-hosted container is that, though there may be an EC2 instance supporting the container, you don\u2019t know it and don\u2019t manage it.Here is the trick: ECS tasks have an identifier calledruntimeIdwhich you can use to run SSM start-session commands, instead of execute-command command.Retrieve the runtimeId for your container:aws ecs describe-tasks \\--cluster   \\--task  \\--profile \\--region  \\--query 'tasks[].containers[?name== ].runtimeId | [0] | [0]'(The --query clause above post-processes the CLI output so it returns only the requested identifier, not the full describe-tasks output. Seeherefor usage.)Start a session using SSM\u2019s start-session:aws ssm start-session \\--target ecs:\\--document-name AWS-StartPortForwardingSessionToRemoteHost \\--parameters '{\"host\":[\"\"],\"portNumber\":[\"\"], \"localPortNumber\":[\"\"]}' \\--profile  \\--regionThe trick here lies in the--targetparameter: we appendecs:then the name of the cluster, the the ID for the task, then the runtimeID retrieved from the previous step.(note that the taskID is included in runtimeId so it effectively appears twice in the--targetparameter).A demoNeed help to run your container workloads on AWS? Pleaseget in touch!"}
{"title": "Extend Amazon Connect capabilities with Lambda and LexV2", "published_at": 1710409592, "tags": [], "user": "Paul SANTUS", "url": "https://dev.to/aws-builders/extend-amazon-connect-capabilities-with-lambda-and-lexv2-50fa", "details": "Amazon Connect comes with a reach set of features, from call recordings to live sentiment analysis or the ability to prompt agents with step-by-step guides to solve customer requests.But no product has all features built-in. The beautiful thing about Connect is it is fully integrable with other AWS services that will help you customize your customer experience.In this post, I show how I extended Connect\u2019s capabilities with Lambda and Lex to add some features a customer requested: the ability to reach a person directly using a phone extension and to look up a name in the company directory.What is Amazon Connect?Amazon Connect is AWS\u2019 Contact-Center-as-a-Service (CCaaS) product. It means you can run a call center with no telecom skills, without the burden of managing IPBX or SBG infrastructure.Following Amazon Web Services \u201cpay-as-you-go\u201d approach to product pricing, there is no licence or fixed fee. This means you pay only $0,018/minute.AWS made huge investment in the product. In myblog post about last re:Invent, I stressed that, apart from EC2 silicon, it was in 2002 AWS\u2019 first service in terms of new features announcements (and it has kept on since).In the public references, you\u2019ll find big names, such as the Accor hospitality group or the Sixt car rental company, and much smaller companies such as Margaritas, a family-owned restaurant in Texas.Adding support for phone extensions with AWS LambdaGiven Amazon Connect primary use case (run a contact center, not a telephone switchboard), there is no built-in feature to assign each agent an extension number for direct contact. (You can still natively direct a call to a specific agent, for instance one that is assigned on the customer case, or one that is the customer single point of contact)Fortunately, Amazon Connect (i) comes with a complete API, (ii) support adding agent custom information using Tags and (iii) can trigger a Lambda function.AWS Lambda is a service where you basically put code and AWS takes care about all the underlying infrastructure (from machine to runtime). Invocations cost $0,0000133334 per Gb.second (which means my function that runs on a 128Mb Lambda for 130ms can run about 5 million times before it costs me one single dollar).First, let\u2019s hear how it sounds like!Here is how I did it:First, I assigned a custom tag called \u201cExtension\u201d to every agent in the directory.I then designed a flow that first requests customer to dial the Extension (this prompt is interruptible, so customers can save a contact +12345678910#777 in their phone). Here is what is looks like:The second box persists user input in the contact session. The third box checks whether customer entered an extension. If they didn\u2019t, then we\u2019ll skip to the Directory part of this post.The Lambda is invoked with an event that contains information on the call and uses theAmazon Connect SearchUsers APIto find the agent associated with the extension number.Et voil\u00e0 !We now have in the Amazon Connect session all the information we need to forward the call to the agents personal queue.Adding Support for Interactive Directory Look Up using LexMaybe the customer doesn\u2019t know the agent\u2019s extension number.. but only their name. And you can\u2019t dial a name, can you? Here comes Lex!Amazon Lex is a chat bot; unlike ChatGPT, its role is not to make lengthy conversations but to elicit a user\u2019s intent (among a number of predetermined intents) and collect information (called \u201cslots\u201d) in order to fulfill that intent.Lex can work as a standalone service and is also able, at any point during the conversation, invoke a Lambda in order to perform some processing and decide on the next step.Here we\u2019ll make it possible to reach sales and support teams, as well as mention we\u2019d like to talk to a specific person.First, let\u2019s hear how it sounds like!In the first part of the conversation, Lex determines that the customer want to perform a Directory look-up. To do that I only had to give Lex some phrases customer would utter, such as \u201cFind someone by name\u201d, or \u201cSpeak to a specific person\u201d. When I provided 8 of them, the bot was quite able to interpret any request that comes close.Then Lex tries to elicit the callee\u2019s family name. Since my name is not a usual English one, the first Directory search fails (it tries to find \u201cSantos\u201d instead of \u201cSantus\u201d \u2014 yes I could make it more clever).I wrote my Lambda code so that Lambda asks Lex to re-elicit the name, this time using spelling instead of just natural language.{     \"sessionState\": {         \"dialogAction\": {             \"type\": \"ElicitSlot\",             \"slotToElicit\": \"Name\",             \"slotElicitationStyle\": \"SpellByWord\"         },         \"sessionAttributes\": {             \"attemptNumber\": 2         },         \"intent\": {             \"name\": \"SpeakToSpecificPerson\"         }     },     \"messages\": [         {             \"contentType\": \"PlainText\",             \"content\": \"Sorry, we did not find any contact with this family name. Could you please spell it for me?\"         }     ] }Enter fullscreen modeExit fullscreen modeAs you can see, even with my Frenchie accent, Lex can then capture the right name, then Directory lookup returns the same data as previously, enabling us to transfer the call to the agent and also gracefully provided the Extension number to make the customer\u2019s life simpler next time.Need some help?In this post, I demonstrated how I integrated Amazon Connect, Lambda and Lex to create a powerful customer experience. With these features, you can turn add the telephone switchboard features Amazon Connect needs to address the needs of some smaller companies.Like always, if you need help to get started on AWS cloud services or help you boost your customer experience using Amazon Connect, please get in touch onhttps://www.terracloud.fror follow meon LinkedIn!"}
{"title": "Deploy a simple data storage API with very little code using Amazon API Gateway and DynamoDB", "published_at": 1710404845, "tags": [], "user": "Paul SANTUS", "url": "https://dev.to/aws-builders/deploy-a-simple-data-storage-api-with-very-little-code-using-amazon-api-gateway-and-dynamodb-d95", "details": "The AWS serverless ecosystem includes multiples services that architects and developers can leverage to build micro-services applications: storage (S3, DynamoDB), integration components (API Gateway, Eventbridge, SQS\u2026) and, of course, the ability to execute custom code in Lambda functions.But in some cases,you might not even need a Lambda function to process queries! In this blog post, I\u2019ll demonstrate how to use Amazon API Gateway direct integrationto cut costs (and reduce the amount of glue code you need to write) and still perform transformation between data storage and presentation. You\u2019ll find a fully-functional code sample at the end of this blog post.A real life use caseThis blog post actually comes from a real-life use case. In France, since this year,energy suppliers are legally required to offer their needy customers a device that provides real-time access to 5s-sampled power measurement dataof the previous 24 hours.To energy suppliers, this was quite a shock, since power metering used to be the thing of Distribution System Operators and skills in IoT were scarce, or even non-existent. We were faced with the obligation to ingest massive amount of data (6.3m samples per year per meter), most of which had basically zero value (most of the data would never ever be read by customers). At our own cost.When faced with such requirements,FinOps becomes key to designing your system. I came up with a design that relies on API Gateway to receive the data and DynamoDB to store it (and flush it, thanks toDynamoDB\u2019s Time-to-Live feature).I won\u2019t go into the specifics of that design, since it is proprietary to my company. Let\u2019s just assert here thatit is both cost-effective and - a nice quality for such a system - infinitely scalable with linear costs.Instead, you\u2019ll find below a fully-functional example of a \u201cPets API\u201d (create, then retrieve pets by name, gender, race etc.)Direct integration, the VTL magicOne quality of the system is also that, even though we process the incoming data, there is no Lambda function involved. The additional cost of Lambda would have made the total cost of ownership twice as high.So, how do you perform data transformation between an API Gateway and a DynamoDB, with no Lambda function? The answer is:Apache Velocity Template Language(aka. VTL).API Gateway processing works in 5 steps:Method Request handles authentication and input checks,then the query is passed on to the backend service through an Integration Request,then backend service performs it own logicthen the service response is sent back throught the Integration Response componentand eventually a response is rendered to the client via the Method Response component.Both theIntegration Request **and **Integration Responsecomponents allow to perform quite advanced data transformation using the Apache Velocity Template language. In this language, you can set variables and store data structures, loop/iterate over the elements of a list, etc.Below, you\u2019ll find the code that transforms the body of the HTTP POST call that creates a Pet.This transforms an input like[{\"owner\":\"Paul\", \"name\":\"milou\"},{\"owner\":\"Peter\", \"name\":\"rex\", \"gender\":\"M\", \"age\":13}]tothe body that the DynamoDB API\u2019s BatchWriteItems operation expects.A fully functional exampleInmy Github repository, you will find the Terraform code that will enable you to deploy a fully-functional \u201cPets API\u201d in a matter of seconds. In particular, you might be interested inapigateway-vtl-mappings.tfwhere I centralized the VTL for all the request/response processing.models.tfwhere I centralized all the Data model that API Gateway uses to perform input and output checks. Those usethe JSON-schema specification.GitHub - psantus/serverless.api-gateway-dynamodb-integration.terraformTheAWS full documentation to perform data transformation using VTL is available here."}
{"title": "PowerBI: d\u00e9ployer une passerelle sur AWS pour $0.12/j", "published_at": 1710404759, "tags": [], "user": "Paul SANTUS", "url": "https://dev.to/aws-builders/powerbi-deployer-une-passerelle-sur-aws-pour-012j-46n", "details": "Aujourd\u2019hui, je vais expliquer comment d\u00e9ployer une passerelle PowerBI fiable et fonctionnelle. En cadeau, des extraits de code et un repo GitHub pour d\u00e9ployer le tout en 5min avec Terraform :)Qu\u2019est-ce qu\u2019une passerelle de donn\u00e9es locale PowerBI ? A quoi sert-elle ?Pourquoi, me demandez-vous, auriez vous m\u00eame besoin de PowerBI si vous avez \u00e0 dispo le cloud AWS et son offre compl\u00e8te de services autour de ladata(pour en nommer quelques-uns: Glue pour l\u2019ETL, S3 pour le stockage, Athena pour requ\u00eater en SQL, Redshift pour l\u2019entrep\u00f4t et Quicksight pour la data visualisation) ?Bon.. vous savez que tout le monde aime Excel? Eh bien, PowerBI est le nouvel Excel :) Sa capacit\u00e9 \u00e0 fournir de beaux graphiques (si vous avez boss\u00e9 chez un \u00e9diteur de logiciel, vous savez d\u2019exp\u00e9rience qu\u2019aucun soft ne se vend sans dashboard, m\u00eame si personne ne les utilise \u00e0 la fin)\u00e0 partir de fichiers .xlsx locaux et de base de donn\u00e9es fait croire aux utilisateurs que le data management est chose simple (en mettant sous le tapis les difficult\u00e9s li\u00e9es au data lineage, au cataloging et des difficutl\u00e9s \u201cmineures\u201d comme la scalabilit\u00e9, la securit\u00e9 et la gouvernance).Donc vos donn\u00e9es sont dans le cloud, et quelqu\u2019un veut les analyser avec Power BI. Bon professionnel que vous \u00eates, vous ne les laissez pas ex\u00e9cuter de requ\u00eates en direct sur la BDD de prod (que celui qui n\u2019a jamais p\u00e9ch\u00e9\u2026). Voici donc \u00e0 quoi sert la \u201cPasserelles de donn\u00e9es locale PowerBI\u201d !Lapasserelle sert de proxysortant entre votre BDD et le service PowerBI, ce qui limite le besoin de monter un lien r\u00e9seau entre le service/Azure (ou les utilisateurs) et la base de donn\u00e9esSi vous ne controllez pas ce que les utilisateurs vont faire dans PowerBI, je vous recommande d\u2019\u00e9viter le modeDirect Query *et de param\u00e9trer une synchronisation p\u00e9riodique des donn\u00e9es dans ce que PowerBI appelle un *Dataflow(c\u2019est un stockage local au service que les utilisateurs vont attaquer).Voici ce que nous allons d\u00e9ployerLe code dans le repo \u00e0 la fin de cet article d\u00e9ploie ce qui suit :Pourquoi d\u00e9ployer la Data gateway dans un AutoscalingGroup EC2 ?Le co\u00fbt **(et l\u2019empreinte environnementale) : l\u2019ASG permet de **planifier la cr\u00e9ation/suppression **de notre instance EC2. Pour faire une synchro incr\u00e9mentale, nous avons juste besoin de booter l\u2019instance quelques minutes avant le d\u00e9clenchement du job de synchro et de l\u2019\u00e9teindre apr\u00e8s. Dans notre cas, j\u2019utilise aussi une **requ\u00eate spotqui permet encore de r\u00e9duire le co\u00fbt de ~30%.. je vais peut-\u00eatre louper un cycle de synchro si AWS n\u2019a pas de ressource dispo (REX : en 4 mois, j\u2019ai eu 2 cycles manqu\u00e9s). Une m5a.large (type d\u2019instance qui correspond aux exigences assez \u00e9lev\u00e9es de Microsoft pour ce cas d\u2019usage) qui tourne 3 fois par jour pendant 20 minutes me co\u00fbtera $0.12/j soit moins de $50.Fiabilit\u00e9 sans maintenance: l\u2019instance d\u00e9marre chaque fois de la m\u00eame image. De cette fa\u00e7on, je n\u2019aurai jamais de souci de type disque plein, fuite m\u00e9moire, Windows qui ralentit (oui, la passerelledoittourner sous Windows server). Un des principes devops (Pets vs. Cattle) est de consid\u00e9rer les machines comme dispensables!Puisque les experts PowerBI m\u2019ont indiqu\u00e9 que la Passerelle aura besoin demises \u00e0 jour r\u00e9guli\u00e8respour suivre le cycle de release de PowerBI, mon archi initiale impliquait un pipelineEC2 Image Builderpour g\u00e9n\u00e9rer une image machine en combinant *a. *la derni\u00e8re image (AMI) Windows *b. *la derni\u00e8re version du package d\u2019installation de passarelle PowerBI, et *c. *un script d\u2019installation-configuration de la passerelle.Pauvre fou! Ce serait esp\u00e9rer que Microsoft fournisse un produit fini, installable de fa\u00e7on silencieuse. Au risque de vous d\u00e9cevoir :M\u00eame siMicrosoft a publi\u00e9 un module Powershellpour automatiser le setup, les cmdlets qu\u2019il propose permettent uniquement de cr\u00e9er un nouveau cluster mais **pas d\u2019enregistrer une nouvelle machine comme participante d\u2019un cluster existant. **Autant pour l\u2019idempotence.La passerelle d\u00e9pend de drivers tiers. Le driver pour PostgreSQL est npgsql (dans une version assez obsol\u00e8te, la 4.0.10) qui doit \u00eatre install\u00e9e avec une option non active par default (Global Assembly Cache, ou GAC)qui ne peut l\u2019\u00eatre via une installation silencieuse.Au temps pour l\u2019automatisation.Pour ces raisons, j\u2019ai d\u00fb me r\u00e9soudre \u00e0 devoir g\u00e9n\u00e9rer l\u2019image Windows manuellement.D\u00e9ployer l\u2019infrastructureL\u2019infra elle-m\u00eame est relativement simple. Dans le moduel Autoscaling :On d\u00e9finit le planning de cr\u00e9ation/destructionOn dit qu\u2019on prend des instances au prix spotOn donne \u00e0 l\u2019instance un r\u00f4le qui permet de s\u2019y connecter en RDP via AWS Systems ManagerOn s\u2019assure que l\u2019instance peut joindre tant la BDD que le service PowerBI.Et voil\u00e0 !Un code Terraform pleinement fonctionnel est dispo sur mon compte Github :https://github.com/psantus/powerbi-onpremises-data-gateway.terraformD\u00e9ployez le dans un premier temps avec l\u2019AMI Windows standard. Puis apr\u00e8s avoir fait le setup initial d\u00e9crit ci-dessous, faites un snapshot de la VM et dites \u00e0 l\u2019AutoScaling Group de l\u2019utiliser pour les d\u00e9ploiments suivants.Installation de la passerelle de donn\u00e9es PowerBIApr\u00e8s que vous avez d\u00e9ploy\u00e9 l\u2019image Windows standard, vous pouvez suivre les \u00e9tapes suivantes pour installer les paquets de la passerelle PowerBI Gateway et les configurer ainsi que le service PowerBI.Connectez vous \u00e0 la machine via AWS Systems ManagerInstallez Powershell 7 (seule version compatible avec le module DataGateway) :msiexec.exe /packagehttps://github.com/PowerShell/PowerShell/releases/download/v7.2.6/PowerShell-7.2.6-win-x64.msi/quiet ADD_EXPLORER_CONTEXT_MENU_OPENPOWERSHELL=1 ADD_FILE_CONTEXT_MENU_RUNPOWERSHELL=1 ENABLE_PSREMOTING=1 REGISTER_MANIFEST=1 USE_MU=1 ENABLE_MU=1 ADD_PATH=1Lancez une session PowerShell 7 puis installez le module DataGatewaypwshInstall-Module DataGateway -ForceImport-Module DataGateway -ForceCr\u00e9ez une application sur l\u2019Azure AD. Pour ce faire vous devez \u00eatre admin Azure AD. L\u2019App doit avoir les permissions Tenant.ReadWrite.All sur le service PowerBI.G\u00e9n\u00e9rez un secret. Notez les \u00e9l\u00e9ments suivants pour la suite:App secretClientIDTenantIDConnectez vous sur le service PowerBI avec l\u2019App que vous venez de cr\u00e9er : (ici je suppose que le secret a \u00e9t\u00e9 coll\u00e9 dans un fichier nomm\u00e9 \u201csecret.txt\u201d sur le Bureau \u2014 n\u2019oubliez pas de le supprimer apr\u00e8s !)Set secret in a secure string$secureClientSecret = (cat .\\Desktop\\secret.txt | ConvertTo-SecureString -AsPlainText -Force)Connect to the PowerBI ServiceConnect-DataGatewayServiceAccount -ApplicationId $AppId -ClientSecret $secureClientSecret -Tenant $TenantIdInstallez le paquet de la passerelle PowerBIInstall-DataGateway -AcceptConditionsRestart the service after installation (removes some random errors)net stop PBIEgwServicenet start PBIEgwServiceInstallez le driver PostgresNpgsql v4.0.10. Seule cette version fonctionne (elle doit s\u2019appuyer sur la m\u00eame version de .NET que l\u2019application PowerBI Gateway, cf.Microsoft site). Quand vous lancez le MSI, assurez-vous de cocher \u201cNpgsql GAC Installation\u201d.Cr\u00e9ez une Passerelle de donn\u00e9es sur le service PowerBI et configurez l\u2019application pour cr\u00e9er un cluster:$GateWayDetails = Add-DataGatewayCluster -GatewayName \"My Gateway\" -RecoveryKey $secureClientSecret -OverwriteExistingGatewayGet gateways and find the one you just createdGet-DataGatewayClusterPut its detail in a variable$GateWayDetails = Get-DataGatewayCluster -Id \"xxxxxx-xxxxxx-xxxxxx-xxxxx\"Donnez des droits d\u2019admin sur la passerelle \u00e0 un utilisateur (ou un groupe)Add-DataGatewayClusterUser -GatewayClusterId $GateWayDetails.Id -PrincipalObjectId \"Azure AD User or Group ID here\" -AllowedDataSourceTypes $null -Role AdminD\u00e9sormais, vous devriez voir la passerelle dans l\u2019interface web PowerBI Gatewayhttps://app.powerbi.com/groups/me/gateways(Nb : vous pouvez donner des permissions plus limit\u00e9es sur la gateway, par exemple juste la possibilit\u00e9 de se connecter \u00e0 une BDD PostgreSQL mais pas d\u2019autres sources de donn\u00e9es)$dsTypes = New-Object 'System.Collections.Generic.List[Microsoft.PowerBI.ServiceContracts.Api.DatasourceType]'$dsTypes.Add([Microsoft.PowerBI.ServiceContracts.Api.DataSourceType]::PostgreSql)Add-DataGatewayClusterUser -GatewayClusterId $GateWayDetails.Id -PrincipalObjectId \"Azure AD User or Group ID here\" -AllowedDataSourceTypes $dsTypes -Role ConnectionCreatorRessourcesle code Terraform d\u00e9montr\u00e9 dans ce post est dispo dans un repo sur mon compte Githubhttps://github.com/psantus/powerbi-onpremises-data-gateway.terraformSi vous avez trouv\u00e9 ce post utile, ou que vous avez la bont\u00e9 de sugg\u00e9rer des am\u00e9liorations (c\u2019est mon premier post de blog, j\u2019imagine donc qu\u2019il est largement perfectible),n\u2019h\u00e9sitez pas \u00e0 laisser un commentaire!"}
{"title": "Running Power BI On-Premises Data Gateway on AWS for $0.12 a day", "published_at": 1710404679, "tags": [], "user": "Paul SANTUS", "url": "https://dev.to/aws-builders/running-power-bi-on-premises-data-gateway-on-aws-for-012-a-day-5bef", "details": "Today, I\u2019d like to discuss in detail how to deploy a fully functional (and reliable) Power BI Data Gateway, and the difficulties to overcome in doing so. As a Christmas present, you\u2019ll get code snippets and a fully functional GitHub repository to clone with Terraform code to deploy :)What the h\u2026 is Power BI On-Premises Data Gateway and why on earth would you use it?Why would you even need Power BI when you have the AWS cloud and its extensive range of data management and analytics service (just to name a few: Glue ETL, S3 storage, Athena in-place querying, Redshift warehousing and Quicksight data visualisation) ?Hum.. remember how business users love Excel? Well, Power BI is the new Excel :) Its ability to pop a beautiful dashboard (if you\u2019ve been in the software industry, you probably know from experience that no software sells well without a dashboard feature, even though no one will ever use it for real) from both a local xlsx file and a database makes them believe that data management is a piece of cake (sweeping under the rug the hardships of data lineage, cataloging and \u201cminor\u201d issues like scalability, security, governance).So your app\u2019s data is in the cloud, and someone wants to analyze it using Power BI. Being a good professional, you won\u2019t let them run queries on your production database. Here comes the \u201cPower BI On-Premise Data Gateway\u201d!TheGateway essentially proxiesyour app database for the Power BI service, removing the need for network connectivity between the service (or its users) and the database.If you have no control on what users will do in Power BI, I advise you to avoid theDirect Query *mode and set up a schedule data refresh in what Power BI names a *Dataflow(a blob storage that users will then query).Here\u2019s what we\u2019ll deployThe code in the repository at the end of this article deploys the following set up:The reasons for setting up the Data gateway as an EC2 instance within an AWS AutoScaling Group are the following:Cost **(and environmental footprint): the ASG is a nice way to **schedule the creation and terminationof our EC2 instance. In order to perform a scheduled refresh, we just need the instance to boot a couple of minutes before the refresh is triggered and we can kill it right afterwards. In this case, I also usespot requestto cut cost by ~30%.. I might skip one refresh if AWS has no resources to spare. An m5a.large (the instance type that matches Microsoft rather greedy requirements), run 3 times a day, 20 minutes every time, will cost me $0.12 a day, that is less than $50 a year.Reliability without any maintenance: the instance boots from the same image every time. This way, we\u2019ll never face issues like a full disk, memory leaks or Windows getting slow (yeah, the Gatewayhasto run on a Windows server). Cattle, not pets!Since Power BI experts told me the Gateway might needregular updatesto follow Power BI\u2019s release cycle, my initial design involved anEC2 Image Builder pipelineto generate a brand new machine image, combining *a. *the latest Windows AMI, *b. *the latest version of the Power BI Gateway package, and *c. *a scripted installation and configuration of the gateway.Alas! The gateway is unfortunately a rather unfinished product, with major drawbacks when it comes to devops / including it in a continuous integration pipeline :Even thoughMicrosoft engineers came up with a Powershell moduleto automate set up tasks, the provided cmdlets can only create a full gateway cluster, butcan\u2019t register an installation of the gateway package as an additional member of an existing cluster. So much for idempotence.The gateway app relies on external drivers. The driver for PostgreSQL is npgsql (in a rather old version, 4.0.10) and needs to be installed with a non-default option (install it in the Global Assembly Cache, or GAC) that isunavailable with the unattended / quiet install. So much for automation.For these reasons I had to come to terms with the fact that I\u2019ll have to update the windows image manually.Deploying the infrastructureThe infrastructure itself is quite simple. In the Autoscaling module :We define schedules block that tell when to scal up or down.We make sure we procure instances at spot priceWe give the instance a role that enables to connect using Remote Desktop Protocol via AWS Systems ManagerWe make sure that the instance can reach both the database and the Power BI serviceEt voil\u00e0 !The fully functional Terraform code is available in this repository in my Github account:https://github.com/psantus/powerbi-onpremises-data-gateway.terraformFirst, deploy it with AWS\u2019s standard Windows AMI. Then, once you done the initial set-up described below, make a VM Snapshot and tell the ASG to use it for subsequent machine creation.Initial set-up of the Power BI gatewayAfter you\u2019ve deployed the standard Windows image, you can follow these steps to install the Power BI Gateway packages and configure them as well as the Power BI service.Connect to the machine via AWS Systems ManagerInstall Powershell 7 (yes, the DataGateway module is only compatible with that version) :msiexec.exe /packagehttps://github.com/PowerShell/PowerShell/releases/download/v7.2.6/PowerShell-7.2.6-win-x64.msi/quiet ADD_EXPLORER_CONTEXT_MENU_OPENPOWERSHELL=1 ADD_FILE_CONTEXT_MENU_RUNPOWERSHELL=1 ENABLE_PSREMOTING=1 REGISTER_MANIFEST=1 USE_MU=1 ENABLE_MU=1 ADD_PATH=1Launch a PowerShell 7 session then install the DataGateway modulepwshInstall-Module DataGateway -ForceImport-Module DataGateway -ForceCreate an Azure AD registered App. For that part you need to be Azure AD admin. The App needs to have Tenant.ReadWrite.All access on the PowerBI Service.Create a secret. Note the following for later use:App secretClientIDTenantIDLogin on the Power BI service with the aforementioned app. Here, I assume the secret has been paste in a file named \u201csecret.txt\u201d on the machine Desktop (don\u2019t forget to delete it!)Set secret in a secure string$secureClientSecret = (cat .\\Desktop\\secret.txt | ConvertTo-SecureString -AsPlainText -Force)Connect to the PowerBI ServiceConnect-DataGatewayServiceAccount -ApplicationId $AppId -ClientSecret $secureClientSecret -Tenant $TenantIdInstall the Power BI Gateway packageInstall-DataGateway -AcceptConditionsRestart the service after installation (removes some random errors)net stop PBIEgwServicenet start PBIEgwServiceInstall theNpgsql v4.0.10PostgreSQL driver. Only this version works (we need a version of Npgsql that relies on the same .NET framework version as the Power BI Gateway app cf.Microsoft site). When running the MSI, make sure you check the \u201cNpgsql GAC Installation\u201d checkbox.Create a DataGateway on the Power BI service and configure the app to register as this cluster:$GateWayDetails = Add-DataGatewayCluster -GatewayName \"My Gateway\" -RecoveryKey $secureClientSecret -OverwriteExistingGatewayGet gateways and find the one you just createdGet-DataGatewayClusterPut its detail in a variable$GateWayDetails = Get-DataGatewayCluster -Id \"xxxxxx-xxxxxx-xxxxxx-xxxxx\"Grant admin permissions on the gateway to a user (or rather a group of users)Add-DataGatewayClusterUser -GatewayClusterId $GateWayDetails.Id -PrincipalObjectId \"Azure AD User or Group ID here\" -AllowedDataSourceTypes $null -Role AdminFrom there you should see your gateway appear in Power BI web interfacehttps://app.powerbi.com/groups/me/gatewaysYou can also grant more limited permissions to your gateway, for instance just the ability to connect the gateway to a PostgreSQL DB.$dsTypes = New-Object 'System.Collections.Generic.List[Microsoft.PowerBI.ServiceContracts.Api.DatasourceType]'$dsTypes.Add([Microsoft.PowerBI.ServiceContracts.Api.DataSourceType]::PostgreSql)Add-DataGatewayClusterUser -GatewayClusterId $GateWayDetails.Id -PrincipalObjectId \"Azure AD User or Group ID here\" -AllowedDataSourceTypes $dsTypes -Role ConnectionCreatorResourcesThe fully functional Terraform code is available in this repository in my Github account:https://github.com/psantus/powerbi-onpremises-data-gateway.terraformIf you found this blog post useful, or are graceful enough to suggest improvements (that\u2019s my first post, so I\u2019m sure there\u2019s room for some!), or have questions, just leave a comment"}
{"title": "How to retrieve DynamoDB items using secrets stored in AWS Secrets Manager with AWS Lambda - 2", "published_at": 1710363951, "tags": ["dynamodb", "secretsmanager", "lambda", "accesskeys"], "user": "Revathi Joshi", "url": "https://dev.to/aws-builders/how-to-retrieve-dynamodb-items-using-secrets-stored-in-aws-secrets-manager-with-aws-lambda-2-1nnm", "details": "Please read my previous article -How to retrieve DynamoDB items using secrets stored in AWS Secrets Manager with AWS Lambda - 1Let\u2019s get started!Please visit myGitHub Repository for DynamoDB articleson various topics being updated on constant basis.Pre-requisites:AWS user account with admin access, not a root account.Create an IAM roleResources Used:What is Amazon DynamoDB?What is AWS Secrets Manager?What is AWS Lambda?Steps for implementation to this project:Part 26. Create a Secret Manager to Store Access key and Secret Access keys1234NextNextStore567. Write a Lambda code to create DynamoDB Items by retrieving the access keys from Secrets Manager.1Click onFunctionsat the left side and select the Function you created.Select theCodetab under the lambdamyFunctionCopy thefile3, replace it with the existing code and and change theSecret ARNin file3import boto3 import json import base64  def lambda_handler(event, context):     secret_name = \"arn:aws:secretsmanager:us-east-1:xxxxxxxxxxxx:secret:mySecret-q7slUY\"     # Create a Secrets Manager client     secretClient = boto3.client(         service_name = 'secretsmanager',         region_name = 'us-east-1'     )      get_secret_value_response = secretClient.get_secret_value(         SecretId=secret_name     )     secret = get_secret_value_response['SecretString']     Table_name = 'myTable2'      print('DynamoDB Table creation started.')      dynamodb = boto3.resource(         'dynamodb',         aws_access_key_id = json.loads(secret).get('Access Key'),         aws_secret_access_key = json.loads(secret).get('Secret Access Key'),         region_name = 'us-east-1'     )      student_table = dynamodb.create_table(         TableName = Table_name,         KeySchema = [             {                 'KeyType': 'HASH',                 'AttributeName': 'StudId'             }         ],         AttributeDefinitions=[             {                 'AttributeName': 'StudId',                 'AttributeType': 'N'             }         ],         ProvisionedThroughput={             'ReadCapacityUnits': 2,             'WriteCapacityUnits': 2         }     )      # Wait until the Table gets created     student_table.meta.client.get_waiter('table_exists').wait(TableName = Table_name)     print('DynamoDB Table Creation Completed.')      print('Insert Student data to table started.')     # Insert 1st item into DynamoDB table     table = dynamodb.Table(Table_name)     table.put_item(     Item = {             'StudId': 100,             'FirstName': 'Rev1',             'LastName': 'Joshi1',             'Dept': 'Science',             'Age': 11         }     )        # Insert 2nd item into DynamoDB table     table.put_item(     Item = {             'StudId': 200,             'FirstName': 'Rev2',             'LastName': 'Joshi2',             'Dept': 'Science',             'Age': 22         }     )        # Insert 3rd item into DynamoDB table     table.put_item(     Item = {             'StudId': 300,             'FirstName': 'Rev3',             'LastName': 'Joshi3',             'Dept': 'Science',             'Age': 33         }     )     print('Insert Student data to table Completed.')Enter fullscreen modeExit fullscreen modeDeployTestOutput28. View DynamoDB Table created in console.12Select the tablemyTable2and click onExplore table itemsButton in the right side9. Write a lambda code to view the table items using a secret manager.1Click onFunctionsat the left side and select the Function you created.Select theCodetab under the lambdamyFunctionCopy thefile4, replace it with the existing code and change theSecret ARNinfile4import boto3 import json import base64  def lambda_handler(event, context):     secret_name = \"arn:aws:secretsmanager:us-east-1:xxxxxxxxxxxx:secret:mySecret-q7slUY\"     # Create a Secrets Manager client     secretClient = boto3.client(         service_name = 'secretsmanager',         region_name = 'us-east-1'     )      get_secret_value_response = secretClient.get_secret_value(         SecretId=secret_name     )      secret = get_secret_value_response['SecretString']     Table_name = 'myTable2'      # Create a DynamoDB table     print('DynamoDB Table creation started.')      dynamodb = boto3.resource(         'dynamodb',         aws_access_key_id = json.loads(secret).get('Access Key'),         aws_secret_access_key = json.loads(secret).get('Secret Access Key'),         region_name = 'us-east-1'     )      # Connect to table & Scan the entire table     table = dynamodb.Table(Table_name)     response = table.scan()      print('---------------------------------------')     print('------------STUDENT DETAILS------------')     print('---------------------------------------')     for item in response['Items']:         print('Student Id : ', item['StudId'])         print('Student Name : ', item['FirstName'], ' ', item['LastName'])         print('Student Department : ', item['Dept'])         print('Student Age : ', item['Age'])         print('_______________________________')     print('---------------------------------------')Enter fullscreen modeExit fullscreen modeDeployTestOutputCleanupDelete Lambda FunctionDelete DynamoDB tablesDelete SecretsWhat we have done so farSuccessfully retrieved items from DynamoDB tables using secrets stored in AWS Secrets Manager with AWS Lambda function."}
{"title": "Grilled Cheese and Service Control Policies (SCPs)", "published_at": 1710353963, "tags": ["aws"], "user": "Marty Henderson", "url": "https://dev.to/aws-builders/grilled-cheese-and-service-control-policies-scps-3mkh", "details": "Today I made, quite possibly, the worst grilled cheese of my life. The cheese was cold, I didn't wait for the pan to get warm before throwing the butter and bread in, and it all just became a too-toasted-bread-and-unmelted-cheese mess. It's quite disappointing that I did this because I am usually proficient at making grilled cheese. Perhaps I should've shown more patience but I was hungry.Needless to say, I still ate half of it. I was hungry, after all.But while munching on the sad substitute for a sandwich, I realized that it kind of reminds me of how service control policies often go in half baked, come out bad, and then are hated when, really, a bit more preparedness can help quite a bit. So, let's discuss how Service Control Policies work and some common tips and tricks.Before Service Control Policies can be enabledService Control Policies, or SCPs, are done based on your organization, so AWS Organizations need to be set first. If you don't have an org, you can't even start on SCPs. Plus, if you don't have control of the main organization, whether through console or through automation - the better choice - you can't apply them.I highly recommend using AWS Orgs, even on personal accounts, so you can divide your projects up and create a \"blast radius\" 0r places you can test and destroy things without impacting your long-term or more serious projects. My accounts are setup with an org main account, a couple of testing accounts, my \"production\" account, and my DeepRacer account (mostly for budgeting reasons on that one). It also helps that I control which of my friends can use certain accounts through SSO (using Identity Center and an IdP called Jumpcloud).Alright, with all the prework done, let's jump into SCPs.What are SCPs?SCPs, at their core, are policies that allow or deny certain functions or entire services on a given account. For instance, you can deny any DeepRacer functions on an account with an appropriate policy, prevent removal of GuardDuty, or even enforce tagging.They feel a lot like IAM policies, which I've written on before. They control down to granular functions if you desire, but they impact an entire account instead of a role or user. They can attach to an account or an entire Organizations Unit (OU) - something out of scope for this discussion, but still applies to this.{\"Version\":\"2012-10-17\",\"Statement\":[{\"Effect\":\"Deny\",\"Action\":[\"cloudwatch:DeleteAlarms\",\"cloudwatch:DeleteDashboards\",\"cloudwatch:DisableAlarmActions\",\"cloudwatch:PutDashboard\",\"cloudwatch:PutMetricAlarm\",\"cloudwatch:SetAlarmState\"],\"Resource\":\"*\"}]}Enter fullscreen modeExit fullscreen modeAn SCP contains an allow or deny statement - like IAM policies - but attaches to the account. One of AWS' examples is the \"Prevent users from disabling CloudWatch or altering its configuration\", which, as expected, prevents users from disabling CloudWatch or altering it's alarms and similar. It looks like this:As you can see from this build, it denies, on all resources, the ability to delete or create alarms, delete or create dashboards, or snooze an alarm. Applied to an account, this impacts all non-root users - even AdministratorAccess. In fact, an SCP deny overrides IAM allow policies.The fact it overrides an IAM policy (with denies) is important. A common issue I have seen is administrators and engineers believing they can grant the ability to delete alarms or expecting the AdministratorAccess policy to override it. This is an important consideration when applying a deny SCP as you cannot override it within the account.You can also layer SCPs and, like everything in AWS, a deny will always override an allow. If you have another SCP like{\"Version\":\"2012-10-17\",\"Statement\":[{\"Effect\":\"Allow\",\"Action\":[\"cloudwatch:PutDashboard\",\"cloudwatch:PutMetricAlarm\",],\"Resource\":\"*\"}]}Enter fullscreen modeExit fullscreen modeThe deny policy above would still now allow anyone on the account to create new alarms or dashboards.Let's discuss some tactics to create better SCPs to use.Limit by UserSome policies can be applied directly to certain users from the SCP. For example, if you only wanted to let the \"IAMAdmin\" role to edit IAM policies, you could apply something like{\"Version\":\"2012-10-17\",\"Statement\":[{\"Sid\":\"DenyAccessWithException\",\"Effect\":\"Deny\",\"Action\":[\"iam:DeleteRole\",\"iam:DeleteRolePermissionsBoundary\",\"iam:DeleteRolePolicy\",\"iam:PutRolePermissionsBoundary\",\"iam:PutRolePolicy\",\"iam:UpdateAssumeRolePolicy\",\"iam:UpdateRole\",\"iam:UpdateRoleDescription\"],\"Resource\":[\"*\"],\"Condition\":{\"StringNotLike\":{\"aws:PrincipalARN\":\"arn:aws:iam::*:role/IAMAdmin\"}}}]}Enter fullscreen modeExit fullscreen modeThis would prevent all changes to IAM roles (except attachments) unless you're current role is the IAMAdmin role. However, you can apply an even more narrow policy.A good example of this is if you have a deployer role policy that you don't want people to grant more abilities to, you can tie it directly to that role, such as{       \"Version\": \"2012-10-17\",   \"Statement\": [     {       \"Sid\": \"DenyAccessWithException\",       \"Effect\": \"Deny\",       \"Action\": [         \"iam:AttachRolePolicy\",         \"iam:DeleteRole\",         \"iam:DeleteRolePermissionsBoundary\",         \"iam:DeleteRolePolicy\",         \"iam:DetachRolePolicy\",         \"iam:PutRolePermissionsBoundary\",         \"iam:PutRolePolicy\",         \"iam:UpdateAssumeRolePolicy\",         \"iam:UpdateRole\",         \"iam:UpdateRoleDescription\"       ],       \"Resource\": [         \"arn:aws:iam::*:role/deployer-role\"       ],       \"Condition\": {         \"StringNotLike\": {           \"aws:PrincipalARN\":\"arn:aws:iam::*:role/IAMAdmin\"         }       }     }   ] }Enter fullscreen modeExit fullscreen modeThis is an example of a more nuanced policy that likely won't need to be removed in the future. It allows only a specific role, IAMAdmin, to edit the deployer role. This means you can prevent other account access, even AdminAccess from editing the deployer role. These kind of policies prevent what are known as \"supply chain attacks\" that by gaining access to one thing, they can gain access to other things.Limit insecure practicesA common use of SCP is to limit practices that can risk security or are otherwise best practices.A common best practice is secure objects that go into your S3 buckets. The policy recommended by AWS is{   \"Effect\": \"Deny\",   \"Action\": \"s3:PutObject\",   \"Resource\": \"*\",   \"Condition\": {     \"Null\": {       \"s3:x-amz-server-side-encryption\": \"true\"     }   } }Enter fullscreen modeExit fullscreen modeWhat this does is prevent anyone from putting an object in that is not encrypted by server side policies.However, where this can be a problem is when you have existing accounts and functions that use unencrypted buckets from legacy events. I've come across applications using buckets that don't enforce server-side encryption from 4+ years ago. In this case, this SCP could break your application - yikes!When applying SCPs, especially hard denials with no exceptions, it's best to test them in lower environments to see if they will stop your legacy applications from running.TaggingAnother favorite SCP to have is to force tagging. However, it's important to realize what the default tagging policy requires.The default tagging policy forces two tags - Project and CostCenter - to create EC2 instances or SecretsManager secrets.{   \"Version\": \"2012-10-17\",   \"Statement\": [     {       \"Sid\": \"DenyCreateSecretWithNoProjectTag\",       \"Effect\": \"Deny\",       \"Action\": \"secretsmanager:CreateSecret\",       \"Resource\": \"*\",       \"Condition\": {         \"Null\": {           \"aws:RequestTag/Project\": \"true\"         }       }     },     {       \"Sid\": \"DenyRunInstanceWithNoProjectTag\",       \"Effect\": \"Deny\",       \"Action\": \"ec2:RunInstances\",       \"Resource\": [         \"arn:aws:ec2:*:*:instance/*\",         \"arn:aws:ec2:*:*:volume/*\"       ],       \"Condition\": {         \"Null\": {           \"aws:RequestTag/Project\": \"true\"         }       }     },     {       \"Sid\": \"DenyCreateSecretWithNoCostCenterTag\",       \"Effect\": \"Deny\",       \"Action\": \"secretsmanager:CreateSecret\",       \"Resource\": \"*\",       \"Condition\": {         \"Null\": {           \"aws:RequestTag/CostCenter\": \"true\"         }       }     },     {       \"Sid\": \"DenyRunInstanceWithNoCostCenterTag\",       \"Effect\": \"Deny\",       \"Action\": \"ec2:RunInstances\",       \"Resource\": [         \"arn:aws:ec2:*:*:instance/*\",         \"arn:aws:ec2:*:*:volume/*\"       ],       \"Condition\": {         \"Null\": {           \"aws:RequestTag/CostCenter\": \"true\"         }       }     }   ] }Enter fullscreen modeExit fullscreen modeYou can see that it's also Case Sensitive - a tag of costcenter will fail when a tag of CostCenter will work. It can also apply to an entire deploy - if you are deploying with Terraform/OpenTofu without tags, it'll deny the creation of secrets and EC2s and fail! (though using default_tags and amap of tags will help with that)SummationMake sure your pan is hot and ready - test your policies in lower environmentsMake sure your cheese is room temperature - be patient and craft good policies, not quick policies you'll regret/roll backOverall, SCPs are a good, powerful tool, but take time to craft them well before throwing them in front of your accounts and make sure to test them before stopping your applications!"}
{"title": "DevOps with Guruu | Chapter 4 : Setup and use Kubernetes with EKS | Build your First CI/CD Pipeline", "published_at": 1710341011, "tags": ["webdev", "devops", "aws"], "user": "Hoang Guruu", "url": "https://dev.to/aws-builders/devops-with-guruu-chapter-4-setup-and-use-kubernetes-with-eks-build-your-first-cicd-pipeline-1kie", "details": "DevOps with Guruu | Chapter 4 : Setup and use Kubernetes with EKS | Build your First CI/CD Pipeline0:00 Welcome to my Series DevOps with Guruu1:34 Setup Cloud9 Environment4:27 Install eksctl \u2013 helm \u2013 kubectl, necessary tools8:25 Configure access key for Admin user14:05 Setup EKS Cluster with AWS CLI27:55 Setup EKS NodeGroup with AWS CLI40:20 Work with kubectl to run Nginx pod47:00 Work with kubectl to run A Sample APP pod50:00 Delete resourceJoin me on this journey to mastering DevOps. Let's get hands-on and build a strong foundation together! Don't forget to like, share, and subscribe for more exciting content. Happy coding, future DevOps!\""}
{"title": "Level-Up Your AWS CDK Game: Shift Left Security Unveiled!", "published_at": 1710327623, "tags": ["aws", "infrastructureascode", "awscdk", "devsecops"], "user": "Jatin Mehrotra", "url": "https://dev.to/aws-builders/level-up-your-aws-cdk-game-shift-left-security-unveiled-5f54", "details": "Infrastructure as Code (IaC) benefitsare known to everyone and one of the major benefits is to offerrapid infrastructure deployment as a major benefit, yet paradoxically, it can also contribute to slower deployment cycles.Infrastructure Security and compliance issues will become that reason which will lead to slower deployment cycles using IaC.In this blog I have tried to showhow we can implement security and compliance i.e. Shift Left DevSecOps practices during the infrastructure development phaseto achieve security by design, reducing the risks and issues of cloud infrastructure misconfigurations leading to the faster deployment phase of CDK applicationsusing CDK and policy validation plugin called as Keeping Infrastructure as Code Secure(KICS).PrerequisitesAccess to AWS account.Prior experience of working with CDK.Understanding how CDK works.Tools usedAWS CDKas IaC tool.CDK KICS plugin.Programming languageJavascript/TypescriptBefore jumping right into the usage of the plugin I would like to explain how did this approach to Shift Left with CDK came into existence.About CDK KICS pluginAn amazing plugin which reads the synthesized cloudFormation template to security issues and infrastructure misconfigurations.I like it because it tells the exact location of the resource in the CDK code, and gives ahow-to-fixlink which saves so much time.Super easy to set up in the CDK application.How is it possible to achieve validation using CDK?Using static code analysis tool against Cloudformation templates is possible afterApril 3rd, 2023 with this update.In very simple terms this updates means the following:The AWS Cloud Development Kit (CDK) now facilitates developers in validating Infrastructure as Code (IaC) templates against policy-as-code tools throughout the development process.This integration ensures prompt detection and resolution of security or configuration issues aligned with organizational policies.Once the CDK application synthesizes the template, the plugin automatically triggers validation against policies, presenting a detailed report with compliance status and actionable insights for any detected misconfigurations.Let's see some security with CDK in actionTo maintain the simplicity of the blog, I will be creating a CDK application( infrastructure) usingCDK workshop by AWSwhich can be easily replicated and followed along even by first-time users of CDK.Create a new Typescript CDK projectcdk init sample-app--languagetypescriptEnter fullscreen modeExit fullscreen modeNote:I won't be explaining the project structure as thisinformation is provided in the workshop.Install the KICS plugin to your CDK applicationnpminstall@checkmarx/cdk-validator-kicsEnter fullscreen modeExit fullscreen modeThis will automatically modify thepackage.jsonfile.Enable the KICS plugin in your CDK applicationTo use the plugin we need to add it to the CDK app.Under the/bin/<directory-name>.tsdirectory of this project modify the App construct for the CDK application.constapp=newcdk.App({policyValidationBeta1:[newKicsValidator()],});Enter fullscreen modeExit fullscreen modeThe final code for the entry point of the CDK application will look like the following:#!/usr/bin/env nodeimport*ascdkfrom'aws-cdk-lib';import{CdkLeftShiftStack}from'../lib/cdk-left-shift-stack';import{KicsValidator,QueryCategory,Severity}from\"@checkmarx/cdk-validator-kics/lib/plugin\";constapp=newcdk.App({policyValidationBeta1:[newKicsValidator()],});newCdkLeftShiftStack(app,'CdkLeftShiftStack');Enter fullscreen modeExit fullscreen modeImportant NoteWhen I was first trying to test this plugin fromAWS blog (How to Shift Left Security in Infrastructure as Code Using AWS CDK and Checkmarx KICS),I couldn't test it successfully as there is an error in the code provided in the blog.import{KicsValidator}from'@checkmarx/cdk-validator-kics/lib/plugin';constapp=newApp({validationPluginsBeta1:[newKicsValidator(),],});Enter fullscreen modeExit fullscreen modeIf you try the above code you will not able to test this asproperty(validationPluginsBeta1) mentioned in the above code does not exist for the APP class in the current version of the CDK 2.132.1Objectliteralmayonlyspecifyknownproperties,and'validationPluginsBeta1'doesnotexistintype'AppProps'.Enter fullscreen modeExit fullscreen modeThe propertypolicyValidationBeta1existwhich is used to in this blog to illustrate the use of KICS plugin.I have mentioned the relevant authors of the blog about this and hopefully, it will fixed soon.Add a s3 bucket to the StackAdd s3 bucket to the stack to get observe more findings from KICS.news3.Bucket(this,'MyFirstBucket');Enter fullscreen modeExit fullscreen modeThe final code would look like the following for the stack of the CDK applicationimport{Duration,Stack,StackProps}from'aws-cdk-lib';import*assnsfrom'aws-cdk-lib/aws-sns';import*assubsfrom'aws-cdk-lib/aws-sns-subscriptions';import*assqsfrom'aws-cdk-lib/aws-sqs';import*ass3from'aws-cdk-lib/aws-s3';import{Construct}from'constructs';exportclassCdkLeftShiftStackextendsStack{constructor(scope:Construct,id:string,props?:StackProps){super(scope,id,props);news3.Bucket(this,'MyFirstBucket');constqueue=newsqs.Queue(this,'CdkLeftShiftQueue',{visibilityTimeout:Duration.seconds(300)});consttopic=newsns.Topic(this,'CdkLeftShiftTopic');topic.addSubscription(newsubs.SqsSubscription(queue));}}Enter fullscreen modeExit fullscreen modeRun cdk synthcdk deploy\u3000\u30fcprofile cicdEnter fullscreen modeExit fullscreen modeUpon running CDK synth the plugin will be triggered and it will run its checks against the generated cloudFormation template.We can easily KICS plugin has identified configuration and security issues and segregated them intoHIGH, MEDIUM, LOWseverity.We can also see because its a failure to KICS it didn't allow CDK to deploy the resources with these security issues and configurations hence perfectly displaying the Shift left principles in action leading to a secure cloud infrastructure deployment process.Validation Report-----------------\u2554\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2557 \u2551             Plugin Report             \u2551 \u2551   Plugin: kics-cdk-validator-plugin   \u2551 \u2551   Version: N/A                        \u2551 \u2551   Status: failure                     \u2551 \u255a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u255d(Violations)S3 Bucket Without SSL In Write Actions(1 occurrences)Severity: HIGH    Occurrences:      - Construct Path: CdkLeftShiftStack/MyFirstBucket/Resource     - Template Path: cdk.out/CdkLeftShiftStack.template.json     - Creation Stack:         \u2514\u2500\u2500  CdkLeftShiftStack(CdkLeftShiftStack)\u2502 Construct: aws-cdk-lib.Stack              \u2502 Library Version: 2.132.1              \u2502 Location: Run with'--debug'to include location info              \u2514\u2500\u2500  MyFirstBucket(CdkLeftShiftStack/MyFirstBucket)\u2502 Construct: aws-cdk-lib.aws_s3.Bucket                   \u2502 Library Version: 2.132.1                   \u2502 Location: Run with'--debug'to include location info                   \u2514\u2500\u2500  Resource(CdkLeftShiftStack/MyFirstBucket/Resource)\u2502 Construct: aws-cdk-lib.aws_s3.CfnBucket                        \u2502 Library Version: 2.132.1                        \u2502 Location: Run with'--debug'to include location info     - Resource ID: MyFirstBucketB8884501     - Template Locations:>Resources.MyFirstBucketB8884501    Description: S3 Buckets should enforce encryption of data transfers using Secure Sockets Layer(SSL)How to fix: https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-properties-s3-bucket.html   Rule Metadata:          Category: Encryption         QueryId: 38c64e76-c71e-4d92-a337-60174d1de1c9  S3 Bucket Without Server-side-encryption(1 occurrences)Severity: HIGH    Occurrences:      - Construct Path: CdkLeftShiftStack/MyFirstBucket/Resource     - Template Path: cdk.out/CdkLeftShiftStack.template.json     - Creation Stack:         \u2514\u2500\u2500  CdkLeftShiftStack(CdkLeftShiftStack)\u2502 Construct: aws-cdk-lib.Stack              \u2502 Library Version: 2.132.1              \u2502 Location: Run with'--debug'to include location info              \u2514\u2500\u2500  MyFirstBucket(CdkLeftShiftStack/MyFirstBucket)\u2502 Construct: aws-cdk-lib.aws_s3.Bucket                   \u2502 Library Version: 2.132.1                   \u2502 Location: Run with'--debug'to include location info                   \u2514\u2500\u2500  Resource(CdkLeftShiftStack/MyFirstBucket/Resource)\u2502 Construct: aws-cdk-lib.aws_s3.CfnBucket                        \u2502 Library Version: 2.132.1                        \u2502 Location: Run with'--debug'to include location info     - Resource ID: MyFirstBucketB8884501     - Template Locations:>Resources.MyFirstBucketB8884501.Properties    Description: S3 Buckets should have server-side encryption at rest enabled to protect sensitive data   How to fix: https://docs.aws.amazon.com/AmazonS3/latest/user-guide/default-bucket-encryption.html   Rule Metadata:          Category: Encryption         QueryId: b2e8752c-3497-4255-98d2-e4ae5b46bbf5  S3 Bucket Should Have Bucket Policy(1 occurrences)Severity: MEDIUM    Occurrences:      - Construct Path: CdkLeftShiftStack/MyFirstBucket/Resource     - Template Path: cdk.out/CdkLeftShiftStack.template.json     - Creation Stack:         \u2514\u2500\u2500  CdkLeftShiftStack(CdkLeftShiftStack)\u2502 Construct: aws-cdk-lib.Stack              \u2502 Library Version: 2.132.1              \u2502 Location: Run with'--debug'to include location info              \u2514\u2500\u2500  MyFirstBucket(CdkLeftShiftStack/MyFirstBucket)\u2502 Construct: aws-cdk-lib.aws_s3.Bucket                   \u2502 Library Version: 2.132.1                   \u2502 Location: Run with'--debug'to include location info                   \u2514\u2500\u2500  Resource(CdkLeftShiftStack/MyFirstBucket/Resource)\u2502 Construct: aws-cdk-lib.aws_s3.CfnBucket                        \u2502 Library Version: 2.132.1                        \u2502 Location: Run with'--debug'to include location info     - Resource ID: MyFirstBucketB8884501     - Template Locations:>Resources.MyFirstBucketB8884501    Description: ChecksifS3 Bucket has the same name as a Bucket Policy,ifit has, S3 Bucket has a Bucket Policy associated   How to fix: https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-properties-s3-bucket.html   Rule Metadata:          Category: Insecure Defaults         QueryId: 37fa8188-738b-42c8-bf82-6334ea567738  SQS With SSE Disabled(1 occurrences)Severity: MEDIUM    Occurrences:      - Construct Path: CdkLeftShiftStack/CdkLeftShiftQueue/Resource     - Template Path: cdk.out/CdkLeftShiftStack.template.json     - Creation Stack:         \u2514\u2500\u2500  CdkLeftShiftStack(CdkLeftShiftStack)\u2502 Construct: aws-cdk-lib.Stack              \u2502 Library Version: 2.132.1              \u2502 Location: Run with'--debug'to include location info              \u2514\u2500\u2500  CdkLeftShiftQueue(CdkLeftShiftStack/CdkLeftShiftQueue)\u2502 Construct: aws-cdk-lib.aws_sqs.Queue                   \u2502 Library Version: 2.132.1                   \u2502 Location: Run with'--debug'to include location info                   \u2514\u2500\u2500  Resource(CdkLeftShiftStack/CdkLeftShiftQueue/Resource)\u2502 Construct: aws-cdk-lib.aws_sqs.CfnQueue                        \u2502 Library Version: 2.132.1                        \u2502 Location: Run with'--debug'to include location info     - Resource ID: CdkLeftShiftQueue1CF96D0B     - Template Locations:>Resources.CdkLeftShiftQueue1CF96D0B.Properties    Description: Amazon Simple Queue Service(SQS)queue should protect the contents of their messages using Server-Side Encryption(SSE)How to fix: https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-properties-sqs-queues.html#aws-sqs-queue-kmsmasterkeyid   Rule Metadata:          Category: Encryption         QueryId: 12726829-93ed-4d51-9cbe-13423f4299e1  IAM Access Analyzer Not Enabled(1 occurrences)Severity: LOW    Occurrences:      - Construct Path: N/A     - Template Path: cdk.out/CdkLeftShiftStack.template.json     - Creation Stack:                 Construct trace not available. Rerun with`--debug`to see trace information     - Resource ID: n/a     - Template Locations:>Resources    Description: IAM Access Analyzer should be enabled and configured to continuously monitor resource permissions   How to fix: https://docs.amazonaws.cn/en_us/AWSCloudFormation/latest/UserGuide/aws-resource-accessanalyzer-analyzer.html   Rule Metadata:          Category: Best Practices         QueryId: 8d29754a-2a18-460d-a1ba-9509f8d359da  Policy Validation Report Summary  \u2554\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2564\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2557 \u2551 Plugin                    \u2502 Status  \u2551 \u255f\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2562 \u2551 kics-cdk-validator-plugin \u2502 failure \u2551 \u255a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2567\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u255d  Validation failed. See the validation report abovefordetails  Subprocess exited with error 1Enter fullscreen modeExit fullscreen modeWhat else can we do with KICS?DisableCategories, Individual QueriesIn the previous report if there was some finding which we want to disable it is possible to do so by 2 options eitherby disabling it as a category or as a query.IAM Access Analyzer Not Enabled(1 occurrences)Severity: LOW    Occurrences:      - Construct Path: N/A     - Template Path: cdk.out/CdkLeftShiftStack.template.json     - Creation Stack:                 Construct trace not available. Rerun with`--debug`to see trace information     - Resource ID: n/a     - Template Locations:>Resources    Description: IAM Access Analyzer should be enabled and configured to continuously monitor resource permissions   How to fix: https://docs.amazonaws.cn/en_us/AWSCloudFormation/latest/UserGuide/aws-resource-accessanalyzer-analyzer.html   Rule Metadata:          Category: Best Practices         QueryId: 8d29754a-2a18-460d-a1ba-9509f8d359daEnter fullscreen modeExit fullscreen modeTo disable the above we can do this bycategory(Best Practices)by adding the following code:constapp=newcdk.App({policyValidationBeta1:[newKicsValidator({excludeCategories:[QueryCategory.BEST_PRACTICES]})],});Enter fullscreen modeExit fullscreen modeTo disable the same we can do this byQueryId(8d29754a-2a18-460d-a1ba-9509f8d359da)by adding the following code:constapp=newcdk.App({policyValidationBeta1:[newKicsValidator({excludeQueries:['8d29754a-2a18-460d-a1ba-9509f8d359da']})],Enter fullscreen modeExit fullscreen modeAfter running adding the above code we can observeIAM Access Analyzer Not Enabled (1 occurrences)is excluded from the final report.Validation Report-----------------\u2554\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2557 \u2551             Plugin Report             \u2551 \u2551   Plugin: kics-cdk-validator-plugin   \u2551 \u2551   Version: N/A                        \u2551 \u2551   Status: failure                     \u2551 \u255a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u255d(Violations)S3 Bucket Without SSL In Write Actions(1 occurrences)Severity: HIGH    Occurrences:      - Construct Path: CdkLeftShiftStack/MyFirstBucket/Resource     - Template Path: cdk.out/CdkLeftShiftStack.template.json     - Creation Stack:         \u2514\u2500\u2500  CdkLeftShiftStack(CdkLeftShiftStack)\u2502 Construct: aws-cdk-lib.Stack              \u2502 Library Version: 2.132.1              \u2502 Location: Run with'--debug'to include location info              \u2514\u2500\u2500  MyFirstBucket(CdkLeftShiftStack/MyFirstBucket)\u2502 Construct: aws-cdk-lib.aws_s3.Bucket                   \u2502 Library Version: 2.132.1                   \u2502 Location: Run with'--debug'to include location info                   \u2514\u2500\u2500  Resource(CdkLeftShiftStack/MyFirstBucket/Resource)\u2502 Construct: aws-cdk-lib.aws_s3.CfnBucket                        \u2502 Library Version: 2.132.1                        \u2502 Location: Run with'--debug'to include location info     - Resource ID: MyFirstBucketB8884501     - Template Locations:>Resources.MyFirstBucketB8884501    Description: S3 Buckets should enforce encryption of data transfers using Secure Sockets Layer(SSL)How to fix: https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-properties-s3-bucket.html   Rule Metadata:          Category: Encryption         QueryId: 38c64e76-c71e-4d92-a337-60174d1de1c9  S3 Bucket Without Server-side-encryption(1 occurrences)Severity: HIGH    Occurrences:      - Construct Path: CdkLeftShiftStack/MyFirstBucket/Resource     - Template Path: cdk.out/CdkLeftShiftStack.template.json     - Creation Stack:         \u2514\u2500\u2500  CdkLeftShiftStack(CdkLeftShiftStack)\u2502 Construct: aws-cdk-lib.Stack              \u2502 Library Version: 2.132.1              \u2502 Location: Run with'--debug'to include location info              \u2514\u2500\u2500  MyFirstBucket(CdkLeftShiftStack/MyFirstBucket)\u2502 Construct: aws-cdk-lib.aws_s3.Bucket                   \u2502 Library Version: 2.132.1                   \u2502 Location: Run with'--debug'to include location info                   \u2514\u2500\u2500  Resource(CdkLeftShiftStack/MyFirstBucket/Resource)\u2502 Construct: aws-cdk-lib.aws_s3.CfnBucket                        \u2502 Library Version: 2.132.1                        \u2502 Location: Run with'--debug'to include location info     - Resource ID: MyFirstBucketB8884501     - Template Locations:>Resources.MyFirstBucketB8884501.Properties    Description: S3 Buckets should have server-side encryption at rest enabled to protect sensitive data   How to fix: https://docs.aws.amazon.com/AmazonS3/latest/user-guide/default-bucket-encryption.html   Rule Metadata:          Category: Encryption         QueryId: b2e8752c-3497-4255-98d2-e4ae5b46bbf5  S3 Bucket Should Have Bucket Policy(1 occurrences)Severity: MEDIUM    Occurrences:      - Construct Path: CdkLeftShiftStack/MyFirstBucket/Resource     - Template Path: cdk.out/CdkLeftShiftStack.template.json     - Creation Stack:         \u2514\u2500\u2500  CdkLeftShiftStack(CdkLeftShiftStack)\u2502 Construct: aws-cdk-lib.Stack              \u2502 Library Version: 2.132.1              \u2502 Location: Run with'--debug'to include location info              \u2514\u2500\u2500  MyFirstBucket(CdkLeftShiftStack/MyFirstBucket)\u2502 Construct: aws-cdk-lib.aws_s3.Bucket                   \u2502 Library Version: 2.132.1                   \u2502 Location: Run with'--debug'to include location info                   \u2514\u2500\u2500  Resource(CdkLeftShiftStack/MyFirstBucket/Resource)\u2502 Construct: aws-cdk-lib.aws_s3.CfnBucket                        \u2502 Library Version: 2.132.1                        \u2502 Location: Run with'--debug'to include location info     - Resource ID: MyFirstBucketB8884501     - Template Locations:>Resources.MyFirstBucketB8884501    Description: ChecksifS3 Bucket has the same name as a Bucket Policy,ifit has, S3 Bucket has a Bucket Policy associated   How to fix: https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-properties-s3-bucket.html   Rule Metadata:          Category: Insecure Defaults         QueryId: 37fa8188-738b-42c8-bf82-6334ea567738  SQS With SSE Disabled(1 occurrences)Severity: MEDIUM    Occurrences:      - Construct Path: CdkLeftShiftStack/CdkLeftShiftQueue/Resource     - Template Path: cdk.out/CdkLeftShiftStack.template.json     - Creation Stack:         \u2514\u2500\u2500  CdkLeftShiftStack(CdkLeftShiftStack)\u2502 Construct: aws-cdk-lib.Stack              \u2502 Library Version: 2.132.1              \u2502 Location: Run with'--debug'to include location info              \u2514\u2500\u2500  CdkLeftShiftQueue(CdkLeftShiftStack/CdkLeftShiftQueue)\u2502 Construct: aws-cdk-lib.aws_sqs.Queue                   \u2502 Library Version: 2.132.1                   \u2502 Location: Run with'--debug'to include location info                   \u2514\u2500\u2500  Resource(CdkLeftShiftStack/CdkLeftShiftQueue/Resource)\u2502 Construct: aws-cdk-lib.aws_sqs.CfnQueue                        \u2502 Library Version: 2.132.1                        \u2502 Location: Run with'--debug'to include location info     - Resource ID: CdkLeftShiftQueue1CF96D0B     - Template Locations:>Resources.CdkLeftShiftQueue1CF96D0B.Properties    Description: Amazon Simple Queue Service(SQS)queue should protect the contents of their messages using Server-Side Encryption(SSE)How to fix: https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-properties-sqs-queues.html#aws-sqs-queue-kmsmasterkeyid   Rule Metadata:          Category: Encryption         QueryId: 12726829-93ed-4d51-9cbe-13423f4299e1  Policy Validation Report Summary  \u2554\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2564\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2557 \u2551 Plugin                    \u2502 Status  \u2551 \u255f\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2562 \u2551 kics-cdk-validator-plugin \u2502 failure \u2551 \u255a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2567\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u255d  Validation failed. See the validation report abovefordetailsEnter fullscreen modeExit fullscreen modeExclude based on SeverityKICS queries can fall under 5 different severities:high, medium, low, info, and trace.Let's say we want to removeMEDIUMcategory serverity form out the report. (only for example per se, not advised for production system)Add the following to the KICS configurationconstapp=newcdk.App({policyValidationBeta1:[newKicsValidator({excludeSeverities:[Severity.MEDIUM],})],});Enter fullscreen modeExit fullscreen modeReport after the above configuration only containsHIGHandLOWcategory issues.Validation Report-----------------\u2554\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2557 \u2551             Plugin Report             \u2551 \u2551   Plugin: kics-cdk-validator-plugin   \u2551 \u2551   Version: N/A                        \u2551 \u2551   Status: failure                     \u2551 \u255a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u255d(Violations)S3 Bucket Without SSL In Write Actions(1 occurrences)Severity: HIGH    Occurrences:      - Construct Path: CdkLeftShiftStack/MyFirstBucket/Resource     - Template Path: cdk.out/CdkLeftShiftStack.template.json     - Creation Stack:         \u2514\u2500\u2500  CdkLeftShiftStack(CdkLeftShiftStack)\u2502 Construct: aws-cdk-lib.Stack              \u2502 Library Version: 2.132.1              \u2502 Location: Run with'--debug'to include location info              \u2514\u2500\u2500  MyFirstBucket(CdkLeftShiftStack/MyFirstBucket)\u2502 Construct: aws-cdk-lib.aws_s3.Bucket                   \u2502 Library Version: 2.132.1                   \u2502 Location: Run with'--debug'to include location info                   \u2514\u2500\u2500  Resource(CdkLeftShiftStack/MyFirstBucket/Resource)\u2502 Construct: aws-cdk-lib.aws_s3.CfnBucket                        \u2502 Library Version: 2.132.1                        \u2502 Location: Run with'--debug'to include location info     - Resource ID: MyFirstBucketB8884501     - Template Locations:>Resources.MyFirstBucketB8884501    Description: S3 Buckets should enforce encryption of data transfers using Secure Sockets Layer(SSL)How to fix: https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-properties-s3-bucket.html   Rule Metadata:          Category: Encryption         QueryId: 38c64e76-c71e-4d92-a337-60174d1de1c9  S3 Bucket Without Server-side-encryption(1 occurrences)Severity: HIGH    Occurrences:      - Construct Path: CdkLeftShiftStack/MyFirstBucket/Resource     - Template Path: cdk.out/CdkLeftShiftStack.template.json     - Creation Stack:         \u2514\u2500\u2500  CdkLeftShiftStack(CdkLeftShiftStack)\u2502 Construct: aws-cdk-lib.Stack              \u2502 Library Version: 2.132.1              \u2502 Location: Run with'--debug'to include location info              \u2514\u2500\u2500  MyFirstBucket(CdkLeftShiftStack/MyFirstBucket)\u2502 Construct: aws-cdk-lib.aws_s3.Bucket                   \u2502 Library Version: 2.132.1                   \u2502 Location: Run with'--debug'to include location info                   \u2514\u2500\u2500  Resource(CdkLeftShiftStack/MyFirstBucket/Resource)\u2502 Construct: aws-cdk-lib.aws_s3.CfnBucket                        \u2502 Library Version: 2.132.1                        \u2502 Location: Run with'--debug'to include location info     - Resource ID: MyFirstBucketB8884501     - Template Locations:>Resources.MyFirstBucketB8884501.Properties    Description: S3 Buckets should have server-side encryption at rest enabled to protect sensitive data   How to fix: https://docs.aws.amazon.com/AmazonS3/latest/user-guide/default-bucket-encryption.html   Rule Metadata:          Category: Encryption         QueryId: b2e8752c-3497-4255-98d2-e4ae5b46bbf5  IAM Access Analyzer Not Enabled(1 occurrences)Severity: LOW    Occurrences:      - Construct Path: N/A     - Template Path: cdk.out/CdkLeftShiftStack.template.json     - Creation Stack:                 Construct trace not available. Rerun with`--debug`to see trace information     - Resource ID: n/a     - Template Locations:>Resources    Description: IAM Access Analyzer should be enabled and configured to continuously monitor resource permissions   How to fix: https://docs.amazonaws.cn/en_us/AWSCloudFormation/latest/UserGuide/aws-resource-accessanalyzer-analyzer.html   Rule Metadata:          Category: Best Practices         QueryId: 8d29754a-2a18-460d-a1ba-9509f8d359da  Policy Validation Report Summary  \u2554\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2564\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2557 \u2551 Plugin                    \u2502 Status  \u2551 \u255f\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2562 \u2551 kics-cdk-validator-plugin \u2502 failure \u2551 \u255a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2567\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u255dEnter fullscreen modeExit fullscreen modeDon't fail the executionThis option is most likely to be used when you have decided the policy on what to exclude and what to include after reviewing the merits and demerits of the policyThe last option be used to list the severities which should cause the execution to fail. By default, this is set to[Severity.HIGH, Severity.MEDIUM].In our case now I just want it to deploy the resources even irrespective of the severities.constapp=newcdk.App({policyValidationBeta1:[newKicsValidator({failureSeverities:[],})],});Enter fullscreen modeExit fullscreen modeWith the following configurationcdk deploywill set the report tosuccessand continue to deploy the resources.From DevSecOps PerspectiveThe support to validate IaC templates against policy-as-code tools has further increased the trust in IaC by enabling Security first and compliance practice during CDK application development cycles.By verifying compliance with organizational policies at the early stages of development, the teams can enhance the success rate of the deployment phase for their CDK applications.There will be many tools and plugins leveraging this feature to further enhance the DevSecops Shift Left principles for IaC which will not be just limited to KICS, Open Policy Agent(OPA),CfnGuardValidator, Checkov etc.No doubt,these tools are still young and many features are still in the experimental phasebut at least we have a starting point to implement Shift Left practices into IaC.I would be happy to know what kind of tools the community is using for their Iac with AWS CDK to take security and compliance first approach."}
{"title": "Find It Fast: Streamline Phone Number Searches with OpenSearch", "published_at": 1710321521, "tags": ["opensearch", "search", "aws", "webdev"], "user": "Alexey Vidanov", "url": "https://dev.to/aws-builders/find-it-fast-streamline-phone-number-searches-with-opensearch-1aol", "details": "This guide empowers you to optimize OpenSearch for lightning-fast and accurate phone number searches. Frustration-free experiences are key for your customers, and by leveraging edge ngrams and custom analyzers, you can empower OpenSearch to efficiently handle even large datasets.Challenge:Traditional relational databases often struggle with the demands of modern applications, particularly those with dynamic search features like \"search as you type.\" This can lead to performance bottlenecks, increased costs, and limitations in functionality. This was the case for our client, a start up company, who develops a cloud-native restaurant management system built on AWS.Solution:In collaboration with our client, we developed and implemented a one-field comprehensive search solution on Amazon OpenSearch Service, including phone number search. This collaborative effort helped bypass relational database limitations, resulting in faster, more accurate queries and significantly enhanced dynamic search functionality.We'll delve into data preparation, phone number indexing optimization, and index setup, all geared towards enhancing search speed and precision. Remember, customization is crucial to aligning search functionality with your specific data needs.PrerequisitesTo proceed with the instructions in this blog and successfully implement real-time phone number searches using OpenSearch, ensure you have the following prerequisites ready:Amazon Web Services (AWS) Account:Access to an AWS account is necessary to utilize Amazon OpenSearch Service. If you don't have one, you can sign up for an AWS accounthere.Basic Understanding of OpenSearch:Familiarity with the fundamentals of OpenSearch, including its architecture and core concepts, will greatly aid in following the instructions provided.Amazon OpenSearch Service Documentation:Review theAmazon OpenSearch Service documentationfor detailed guidance on installation, configuration, and management of your OpenSearch cluster.SDK Installation:Depending on your preferred programming language, ensure the appropriate SDK is installed on your development machine. This will facilitate communication with the OpenSearch cluster through your application.OpenSearch Dashboards:While optional, having OpenSearch Dashboards set up can be beneficial for visualizing your data and testing your search queries in a more user-friendly environment.Sample Data Preparation:To effectively index phone number data, it's essential to have a set of sample data ready. For your convenience, this blog includesdummy data, which we generated usingPython's Faker library. While this dataset serves as a good starting point, we recommend utilizing your own data for more accurate and realistic testing and implementation purposes.Feel free to ask for more details on any topic mentioned. We're here to offer additional clarification and support.1. Prepare your dataWhen storing phone numbers in the OpenSearch index, it's advisable to utilize two fields: one for the country code and one for the number itself. The method for achieving this depends on your setup. However, it's generally straightforward to normalize the phone numbers and separate the country code from the number using libraries like phonenumbers for Python and libphonenumber-js for Node.js and PHP.>>>importphonenumbers>>>x=phonenumbers.parse(\"+442083661177\",None)>>>print(x)CountryCode:44NationalNumber:2083661177LeadingZero:FalseEnter fullscreen modeExit fullscreen modeReferences:Python:phonenumbersNode.js:libphonenumber-jsPHP:libphonenumber-for-php2. Optimizing Phone Number Indexing with Edge NgramsBefore indexing data, it's crucial to establish an appropriate mapping for phone number fields.The Edge Ngram ApproachFor enabling a dynamic \"search as you type\" functionality, the implementation of edge ngrams is preferred. This technique differs significantly from traditional ngrams by generating tokens exclusively from the beginning of the phone number. This method ensures a more targeted and efficient search process, avoiding the pitfalls associated with full-length ngram sliding.The Limitations of Regular NgramsRegular ngrams, despite their comprehensive coverage, introduce several drawbacks:Increased Noise and Index Size: They produce an extensive array of tokens, capturing every possible character combination within the phone number. This not only leads to irrelevant search results but also escalates the index size considerably.Performance and Storage Concerns: An experiment with a simple index demonstrated that an ngrams-based index was three times larger than one without any grams. In stark contrast, edge ngrams required only 20% additional storage, highlighting the inefficiency of regular ngrams.Advantages of Edge NgramsEdge ngrams present a strategic alternative with multiple benefits:Improved Search Relevance: By concentrating on the initial segments of the phone number, they ensure that search results are closely aligned with the user's query, minimizing irrelevant suggestions.Reduced Noise: The generation of fewer tokens directly translates to cleaner search results, enhancing the suggestion quality.Lower Storage Footprint: When compared to regular ngrams, edge ngrams substantially decrease the index size, leading to improved efficiency.A Practical IllustrationConsider the phone number \"+1234567890\". While regular ngrams would produce tokens like \"1\", \"23\", \"234\", etc., edge ngrams generate \"1\", \"12\", \"123\", etc. In a scenario where a user types \"123\", both methods might return the correct phone number. However, edge ngrams eliminate the creation of irrelevant tokens such as \"456\" or \"7890\", thereby streamlining the search experience.Edge ngrams are highly effective for \"search as you type\" features, indexing the start of each term to provide swift and accurate search results. This approach significantly enhances user experience by ensuring relevance, reducing noise, and minimizing storage requirements.3. Improve Accuracy by Handling Incomplete NumbersTo address issues such as missing digits or incorrect country codes, it's beneficial to adopt a strategy that includes an additional subfield and a custom search analyzer. Concentrating on matching the last four digits significantly boosts the chances of accurately identifying the caller. This approach enables us to overlook initial errors, such as incorrect or absent country codes, thus improving the robustness and dependability of the identification process.Please note: The number of digits focused on for matching can be adjusted up or down based on the database size and specific needs, allowing for optimized results.4. Prepare your indexBelow is an example of how to structure your index mapping to incorporate the previously discussed techniques for enhanced accuracy. To streamline this demonstration, we will omit the country code. It's assumed that there is a standardized input normalization process for phone numbers implemented within the code. This normalization process is consistently applied to both the stored data and incoming queries to ensure uniformity and improve match accuracy.PUT/my_phone_numbers_index{\"settings\":{\"analysis\":{\"tokenizer\":{\"edge_ngram_digits_tokenizer\":{\"type\":\"edge_ngram\",\"min_gram\":\"3\",\"max_gram\":\"20\",\"token_chars\":[\"digit\"]}},\"filter\":{\"last_four_digits\":{\"type\":\"pattern_capture\",\"preserve_original\":false,\"patterns\":[\"\"\"(\\d{4})$\"\"\"]}},\"analyzer\":{\"phone_analyzer\":{\"type\":\"custom\",\"tokenizer\":\"edge_ngram_digits_tokenizer\"},\"phone_search_analyzer\":{\"type\":\"keyword\"},\"last_four_digits_analyzer\":{\"type\":\"custom\",\"tokenizer\":\"keyword\",\"filter\":[\"last_four_digits\"]}}}},\"mappings\":{\"properties\":{\"phone_number\":{\"type\":\"text\",\"analyzer\":\"phone_analyzer\",\"search_analyzer\":\"phone_search_analyzer\",\"fields\":{\"last_four_digits\":{\"type\":\"text\",\"analyzer\":\"last_four_digits_analyzer\"}}}}}}Enter fullscreen modeExit fullscreen modeSettingsAnalysis Configuration:This section configures the analysis process, which is how text is processed and indexed.Tokenizeredge_ngram_digits_tokenizer: A custom tokenizer of typeedge_ngramconfigured to create tokens from the input text by breaking it down into edge n-grams of digits only. This tokenizer will generate tokens of lengths ranging from 3 to 20 characters, focusing exclusively on digit characters. This is useful for partial matching of phone numbers.Filterlast_four_digits: A custom filter of typepattern_capturethat captures the last four digits of the indexed phone numbers. It does not preserve the original token.Analyzersphone_analyzer: A custom analyzer using theedge_ngram_digits_tokenizerfor indexing phone numbers. This analyzer is suitable for indexing phone numbers in a way that supports searching for partial numbers.phone_search_analyzer: A keyword analyzer used during the search phase, ensuring that the search input is treated as a single token. This is useful for exact matches.last_four_digits_analyzer: A custom analyzer tailored for indexing the last four digits of phone numbers. It uses thekeywordtokenizer along with thelast_four_digitsfilter.MappingsPropertiesphone_number: Defines how thephone_numberfield is indexed and searched.Type is set totext, making it suitable to applyedge_ngramtokenizer.Usesphone_analyzerfor indexing, enabling partial matches on phone numbers.Usesphone_search_analyzerfor searching, optimizing for exact match searches.Introduces a sub-fieldlast_four_digitsanalyzed bylast_four_digits_analyzer, specifically designed for searches focused on the last four digits of phone numbers.5. Testing of the analyzersTo evaluate the functionality of your analyzer, execute the following commands. These will generate edge n-gram tokens based on the analyzer configuration.For the initial test, use the following request to analyze the phone number \"+19876543210\" with thephone_analyzer:GET/my_phone_numbers_index/_analyze{\"analyzer\":\"phone_analyzer\",\"text\":[\"+19876543210\"]}Enter fullscreen modeExit fullscreen modeThis query will process the input text through the specified analyzer and return the generated tokens.For testing thelast_four_digits_analyzer, which is designed to extract the last four digits of a phone number, input the same phone number as follows:GET/my_phone_numbers_index/_analyze{\"analyzer\":\"last_four_digits_analyzer\",\"text\":[\"+19876543210\"]}Enter fullscreen modeExit fullscreen modeThis request will result in the analyzer isolating and returning \"3210\" as the token, demonstrating the focused functionality of extracting the last four digits from the provided phone number.6. Adding some phone numbersYou can start with thedummy dataor just put some numbers using an API requrst like this:POST/my_phone_numbers_index/_bulk{\"index\":{\"_id\":\"2\"}}{\"phone_number\":\"+19876543210\"}{\"index\":{\"_id\":\"3\"}}{\"phone_number\":\"+11234567890\"}{\"index\":{\"_id\":\"4\"}}{\"phone_number\":\"+1231231234\"}Enter fullscreen modeExit fullscreen mode7. Conduct a SearchTo illustrate how to search within your index, consider the following example. This search operation aims to find documents in themy_phone_numbers_indexthat match the specified criteria for phone numbers.GET/my_phone_numbers_index/_search{\"query\":{\"bool\":{\"should\":[{\"match\":{\"phone_number\":{\"query\":\"29876543210\",\"boost\":2.0}}},{\"match\":{\"phone_number.last_four_digits\":\"29876543210\"}}]}}}Enter fullscreen modeExit fullscreen modeThis query demonstrates the use of aboolquery withshouldclauses, allowing for flexibility in matching documents. The first clause attempts to match the entire phone number with a boost factor of 2.0, giving it higher relevance if matched. The second clause seeks to match the last four digits of the phone number within a specific field designed to store these digits. This approach facilitates a search strategy, accommodating various ways a phone number might be queried or stored.What else you can do? The Indirect Benefits of Highlighting Query MatchesOpenSearch's highlighting feature goes beyond just presenting results. It visually emphasizes how your search terms match the data, making it easier for users to understand why a particular document is relevant. This improves comprehension and speeds up the decision-making process.Here's how highlighting benefits your users:Clarity:Highlighted terms instantly reveal which parts of the document match the search query. This eliminates guesswork and saves users time spent sifting through irrelevant information.Confidence:By seeing the matched terms, users gain confidence in the search results' accuracy. This empowers them to make informed decisions based on the highlighted information.Efficiency:Highlighting streamlines the search process by directing users' attention to the most relevant parts of the document. This reduces time spent scanning through large amounts of text.For a deeper understanding of how highlighting can be implemented within OpenSearch, visitOpenSearch Highlighting Documentation.In ConclusionBy leveraging edge ngrams and custom analyzers, OpenSearch empowers you to achieve blazing-fast and accurate phone number searches, even with massive datasets. This guide equipped you with the knowledge to prepare your data, optimize phone number indexing, and configure your index for superior search performance. Remember, customization is key! Tailor the search functionality to your specific data requirements to maximize effectiveness.OpenSearch can give your business a significant edge by providing a frictionless search experience for your users. Don't be afraid to experiment and refine your approach to achieve the best possible results. If you have any questions or need further guidance on implementing a robust OpenSearch cluster and API, feel free to reach out. We're here to help you elevate your search capabilities beyond expectations. Happy searching!\u2014 Alexey"}
{"title": "Navigating AWS EKS with Terraform: Understanding EKS Cluster Configuration", "published_at": 1710280995, "tags": ["aws", "kubernetes", "terraform", "cloud"], "user": "Oluwafemi Lawal", "url": "https://dev.to/aws-builders/navigating-aws-eks-with-terraform-understanding-eks-cluster-configuration-2f6o", "details": "In our journey through the AWS EKS ecosystem, we have laid a solid foundation with VPC networking and security groups. Now we will discuss the Terraform resources required to implement a working cluster.The entire project will look like this by the end:.\u251c\u2500\u2500 eks.tf \u251c\u2500\u2500 modules \u2502   \u2514\u2500\u2500 aws \u2502       \u251c\u2500\u2500 eks \u2502       \u2502    \u251c\u2500\u2500 main.tf \u2502       \u2502    \u251c\u2500\u2500 outputs.tf \u2502       \u2502    \u251c\u2500\u2500 versions.tf \u2502       \u2502    \u2514\u2500\u2500 variables.tf \u2502       \u2514\u2500\u2500 vpc \u2502            \u251c\u2500\u2500 main.tf \u2502            \u251c\u2500\u2500 outputs.tf \u2502            \u251c\u2500\u2500 versions.tf \u2502            \u2514\u2500\u2500 variables.tf \u251c\u2500\u2500 versions.tf \u251c\u2500\u2500 variables.tf \u2514\u2500\u2500 vpc.tfEnter fullscreen modeExit fullscreen modeCluster ResourceThe Terraform configuration creates an Amazon EKS cluster with enhanced security and logging features:Cluster Configuration:name: Sets the cluster's name to a variable, allowing for customizable deployments.role_arn: Specifies the IAM role that EKS will assume to create AWS resources for the cluster.enabled_cluster_log_types: Enables logging for audit, API, and authenticator logs, enhancing security and compliance.VPC Configuration:Integrates the cluster with specified private and public subnets, allowing workloads to be placed accordingly for both security and accessibility.Associates the cluster with a specific security group to control inbound and outbound traffic.Encryption Configuration:Utilizes a KMS key for encrypting Kubernetes secrets, safeguarding sensitive information.Specifies that the encryption applies to Kubernetessecrets, ensuring that secret data stored in the cluster is encrypted at rest.Dependencies:Ensures the cluster is created after the necessary IAM role and policy attachments are in place, maintaining the deployment order and security posture.# Fetch current AWS account detailsdata\"aws_caller_identity\"\"current\"{}############################################################################################################### EKS CLUSTER############################################################################################################resource\"aws_eks_cluster\"\"main\"{name=var.cluster_namerole_arn=aws_iam_role.eks_cluster_role.arnenabled_cluster_log_types=var.enabled_cluster_log_typesvpc_config{subnet_ids=concat(var.private_subnets,var.public_subnets)security_group_ids=[aws_security_group.eks_cluster_sg.id]endpoint_public_access=true}encryption_config{provider{key_arn=aws_kms_key.eks_encryption.arn}resources=[\"secrets\"]}depends_on=[aws_iam_role_policy_attachment.eks_cluster_policy,]}Enter fullscreen modeExit fullscreen modeThe Kubernetes provider configuration is necessary for Terraform to interact with your Amazon EKS cluster's Kubernetes APIdata\"aws_eks_cluster_auth\"\"main\"{name=var.cluster_name}provider\"kubernetes\"{host=aws_eks_cluster.main.endpointcluster_ca_certificate=base64decode(aws_eks_cluster.main.certificate_authority[0].data)token=data.aws_eks_cluster_auth.main.token}Enter fullscreen modeExit fullscreen modeOIDC for EKS: Secure Access ManagementIntegrating OIDC with your EKS cluster enhances security by facilitating identity federation between AWS and Kubernetes. This configuration allows for secure role-based access control, which is crucial for managing access to your cluster.############################################################################################################### OIDC CONFIGURATION############################################################################################################data\"tls_certificate\"\"eks\"{url=aws_eks_cluster.main.identity[0].oidc[0].issuer}resource\"aws_iam_openid_connect_provider\"\"eks\"{client_id_list=[\"sts.amazonaws.com\"]thumbprint_list=[data.tls_certificate.eks.certificates[0].sha1_fingerprint]url=aws_eks_cluster.main.identity[0].oidc[0].issuer}Enter fullscreen modeExit fullscreen modeSecrets EncryptionIn the EKS cluster resource above, this KMS key is referenced. It is needed to encrypt Kubernetes secrets in the cluster.############################################################################################################### KMS KEY############################################################################################################resource\"aws_kms_key\"\"eks_encryption\"{description=\"KMS key for EKS cluster encryption\"policy=data.aws_iam_policy_document.kms_key_policy.jsonenable_key_rotation=true}# aliasresource\"aws_kms_alias\"\"eks_encryption\"{name=\"alias/eks/${var.cluster_name}\"target_key_id=aws_kms_key.eks_encryption.id}data\"aws_iam_policy_document\"\"kms_key_policy\"{statement{sid=\"Key Administrators\"actions=[\"kms:Create*\",\"kms:Describe*\",\"kms:Enable*\",\"kms:List*\",\"kms:Put*\",\"kms:Update*\",\"kms:Revoke*\",\"kms:Disable*\",\"kms:Get*\",\"kms:Delete*\",\"kms:ScheduleKeyDeletion\",\"kms:CancelKeyDeletion\",\"kms:Decrypt\",\"kms:DescribeKey\",\"kms:Encrypt\",\"kms:ReEncrypt*\",\"kms:GenerateDataKey*\",\"kms:TagResource\"]principals{type=\"AWS\"identifiers=[\"arn:aws:iam::${data.aws_caller_identity.current.account_id}:root\",data.aws_caller_identity.current.arn]}resources=[\"*\"]}statement{actions=[\"kms:Encrypt\",\"kms:Decrypt\",\"kms:ReEncrypt*\",\"kms:GenerateDataKey*\",\"kms:DescribeKey\"]principals{type=\"Service\"identifiers=[\"eks.amazonaws.com\"]}resources=[\"*\"]}}resource\"aws_iam_policy\"\"cluster_encryption\"{name=\"${var.cluster_name}-encryption-policy\"description=\"IAM policy for EKS cluster encryption\"policy=data.aws_iam_policy_document.cluster_encryption.json}data\"aws_iam_policy_document\"\"cluster_encryption\"{statement{actions=[\"kms:Encrypt\",\"kms:Decrypt\",\"kms:ListGrants\",\"kms:DescribeKey\"]resources=[aws_kms_key.eks_encryption.arn]}}# Granting the EKS Cluster role the ability to use the KMS keyresource\"aws_iam_role_policy_attachment\"\"cluster_encryption\"{policy_arn=aws_iam_policy.cluster_encryption.arnrole=aws_iam_role.eks_cluster_role.name}Enter fullscreen modeExit fullscreen modeNode Groups: Scaling and Optimization with Amazon EKSNode Groups in Amazon EKS allow the management of EC2 instances as Kubernetes nodes. These are critical for running your applications and can be customized to suit specific workload needs through AWS Managed Node Groups, Self-Managed Node Groups, and AWS Fargate.AWS Managed Node GroupsManaged Node Groups simplify node management by automating provisioning, lifecycle management, and scaling. They offer:Automated Updates: Automatic application of security patches and OS updates.Customization: Via Launch Templates for AMIs, instance types, etc.Scalability: Directly adjustable scaling configurations, Kubernetes labels, and AWS tags.Self-Managed Node GroupsOffering full control over configuration, Self-Managed Node Groups require manual scaling and updates but allow for:Custom AMIs: For specific compliance or policy requirements.Manual Management: Users handle software and OS updates, leveraging AWS CloudFormation for operations.Serverless Option: AWS Fargate with Amazon EKSAWS Fargate abstracts server management, allowing specifications for CPU and RAM without considering instance types. This offers a simplified, serverless option for running containers in EKS.Comparison TableCriteriaManaged Node GroupsSelf-Managed NodesAWS FargateDeployment to AWS OutpostsNoYesNoWindows Container SupportYesYes (but with at least one compulsory Linux node)NoLinux Container SupportYesYesYesGPU WorkloadsYes (Amazon Linux)Yes (Amazon Linux)NoCustom AMI DeploymentYes (via Launch Template)YesNoOperating System MaintenanceAutomated by AWSManualN/ASSH AccessYesYesNo (No node OS)Kubernetes DaemonSetsYesYesNoPricingEC2 instance costEC2 instance costFargate pricing per PodUpdate Node AMIAutomated notification & one-click update in EKS console for EKS AMI; manual for custom AMICan not be done from EKS console. Manual process using external toolsN/AUpdate Node Kubernetes VersionAutomated notification & one-click update in EKS console for EKS AMI; manual for custom AMICan not be done from EKS console. Manual process using external toolsN/AUse Amazon EBS with PodsYesYesNoUse Amazon EFS with PodsYesYesYesUse Amazon FSx for Lustre with PodsYesYesNoAWS Fargate for EKS provides a higher level of abstraction, managing more server aspects for you. It eliminates the need to select instance types, focusing solely on the required CPU and RAM for your workloads.Our Managed Node Groups ConfigurationFor our EKS module, we will use the managed node group Terraform resource.Our terraform resources below creates a series of node groups for an EKS cluster, where each group's configuration is based on user-defined variables. These node groups are essential for running your Kubernetes workloads on AWS, providing the compute capacity as EC2 instances.Specifies the EKS cluster to which these node groups belong.Defines the size (min, max, desired) of each node group, allowing for scalability.Associates a launch template to dictate the configuration of EC2 instances within the node group.Allows selection of instance types, AMI types, and capacity types (e.g., On-Demand or Spot Instances) for flexibility and cost optimization.Includes a mechanism to force updates to node groups when the launch template changes.############################################################################################################### MANAGED NODE GROUPS############################################################################################################resource\"aws_eks_node_group\"\"main\"{for_each=var.managed_node_groupscluster_name=aws_eks_cluster.main.namenode_group_name=each.value.namenode_role_arn=aws_iam_role.node_role.arnsubnet_ids=var.private_subnetsscaling_config{desired_size=each.value.desired_sizemax_size=each.value.max_sizemin_size=each.value.min_size}launch_template{id=aws_launch_template.eks_node_group.idversion=\"$Default\"}instance_types=each.value.instance_typesami_type=var.default_ami_typecapacity_type=var.default_capacity_typeforce_update_version=true}Enter fullscreen modeExit fullscreen modeLaunch TemplateThis defines a blueprint for the EC2 instances that will be launched as part of the node groups. This template encapsulates various settings and configurations for the instances.Configures instance networking, including security groups for controlling access to/from the instances.Sets instance metadata options to enhance security, like enforcing IMDSv2 for metadata access.Details the block device mappings for instance storage, including the root EBS volume size, type, and deletion policy.Automatically tags instances for easier management, cost tracking, and integration with Kubernetes.Ensures that the launch template is created with a lifecycle policy that avoids conflicts and dangling resources.This combined setup allows for automated management of the underlying infrastructure for Kubernetes workloads, leveraging EKS-managed node groups and EC2 launch templates for fine-grained control over instance properties and scaling behaviours.############################################################################################################### LAUNCH TEMPLATE############################################################################################################resource\"aws_launch_template\"\"eks_node_group\"{name_prefix=\"${var.cluster_name}-eks-node-group-lt\"description=\"Launch template for${var.cluster_name}EKS node group\"vpc_security_group_ids=[aws_security_group.eks_nodes_sg.id]metadata_options{http_endpoint=\"enabled\"http_tokens=\"required\"http_put_response_hop_limit=2instance_metadata_tags=\"enabled\"}block_device_mappings{device_name=\"/dev/xvda\"# Adjusted to the common root device name for Linux AMIsebs{volume_size=20# Disk size specified herevolume_type=\"gp3\"# Example volume type, adjust as necessarydelete_on_termination=true}}tags={\"Name\"=\"${var.cluster_name}-eks-node-group\"\"kubernetes.io/cluster/${var.cluster_name}\"=\"owned\"}lifecycle{create_before_destroy=true}}Enter fullscreen modeExit fullscreen modeEKS Add-ons: Enhancing Cluster CapabilitiesWith theaws_eks_addonresource we can pass a list of addons to install. The defaults for this project will be vpc-cni, kube-proxy, coredns and aws-ebs-csi-driverEKS Add-ons Explanationvpc-cni: The Amazon VPC CNI (Container Network Interface) plugin allows Kubernetes pods to have the same IP address inside the pod as they do on the VPC network. This plugin is responsible for providing high-performance networking for Amazon EKS clusters, enabling native VPC networking for Kubernetes pods.kube-proxy: Manages network rules on each node. This allows network communication to your pods from network sessions inside or outside of your cluster. It makes sure that each node has the latest network rules for communicating with other nodes in the cluster.coredns: A flexible, extensible DNS server that can serve as the Kubernetes cluster DNS. It translates human-readable hostnames likewww.example.cominto IP addresses. CoreDNS is used for service discovery within the cluster, allowing pods to find each other and services to scale up and down.aws-ebs-csi-driver: The Amazon EBS CSI (Container Storage Interface) Driver provides a way to configure and manage Amazon EBS volumes. It allows you to use Amazon EBS volume as persistent storage for stateful applications in EKS. This driver supports dynamic provisioning, snapshots, and resizing of volumes.############################################################################################################# PLUGINS############################################################################################################data\"aws_eks_addon_version\"\"main\"{for_each=toset(var.cluster_addons)addon_name=each.keykubernetes_version=aws_eks_cluster.main.version}resource\"aws_eks_addon\"\"main\"{for_each=toset(var.cluster_addons)cluster_name=aws_eks_cluster.main.nameaddon_name=each.keyaddon_version=data.aws_eks_addon_version.main[each.key].versionresolve_conflicts_on_create=\"OVERWRITE\"resolve_conflicts_on_update=\"OVERWRITE\"depends_on=[aws_eks_node_group.main]}Enter fullscreen modeExit fullscreen modeIAM Roles and Policies: Securing EKSIAM Roles for EKSSecuring your EKS clusters involves creating and associating specific IAM roles and policies, ensuring least privilege access to AWS resources. This step is fundamental in protecting your cluster's interactions with other AWS services.EKS Cluster Role and Policies:A role (eks_cluster_role) is created for the EKS cluster to interact with other AWS services.The role trusts theeks.amazonaws.comservice to assume the role.Attached policies:CloudWatchFullAccess: Grants full access to CloudWatch, allowing the cluster to log and monitor.AmazonEKSClusterPolicy: Provides permissions that Amazon EKS requires to manage clusters.AmazonEKSVPCResourceController: Allows the cluster to manage VPC resources for the cluster.Node Group Role and Policies:An instance profile (eks_node) is created for EKS node groups, associating them with a role (node_role) that nodes assume for AWS service interaction.The role trusts theec2.amazonaws.comservice to assume the role.Attached policies:AmazonEKSWorkerNodePolicy: Grants nodes in the node group the permissions required to operate within an EKS cluster.AmazonEKS_CNI_Policy: Allows nodes to manage network resources, which is necessary for the Amazon VPC CNI plugin to operate.AmazonEBSCSIDriverPolicy: Allows nodes to manage EBS volumes, enabling dynamic volume provisioning.AmazonEC2ContainerRegistryReadOnly: Provides read-only access to AWS Container Registry, allowing nodes to pull container images.VPC CNI Plugin Role:A specific role (vpc_cni_role) is created for the VPC CNI plugin to operate within the EKS cluster.The role trusts federated access via the cluster's OIDC provider, specifically for theaws-nodeKubernetes service account within thekube-systemnamespace.Attached policy:AmazonEKS_CNI_Policy: Grants the necessary permissions for the VPC CNI plugin to manage AWS network resources.############################################################################################################### IAM ROLES############################################################################################################# EKS Cluster roleresource\"aws_iam_role\"\"eks_cluster_role\"{name=\"${var.cluster_name}-eks-cluster-role\"assume_role_policy=data.aws_iam_policy_document.eks_assume_role_policy.json}data\"aws_iam_policy_document\"\"eks_assume_role_policy\"{statement{actions=[\"sts:AssumeRole\"]principals{type=\"Service\"identifiers=[\"eks.amazonaws.com\"]}}}# EKS Cluster Policiesresource\"aws_iam_role_policy_attachment\"\"eks_cloudwatch_policy\"{policy_arn=\"arn:aws:iam::aws:policy/CloudWatchFullAccess\"role=aws_iam_role.eks_cluster_role.name}resource\"aws_iam_role_policy_attachment\"\"eks_cluster_policy\"{policy_arn=\"arn:aws:iam::aws:policy/AmazonEKSClusterPolicy\"role=aws_iam_role.eks_cluster_role.name}resource\"aws_iam_role_policy_attachment\"\"eks_vpc_resource_controller_policy\"{policy_arn=\"arn:aws:iam::aws:policy/AmazonEKSVPCResourceController\"role=aws_iam_role.eks_cluster_role.name}# Managed Node Group roleresource\"aws_iam_instance_profile\"\"eks_node\"{name=\"${var.cluster_name}-node-role\"role=aws_iam_role.node_role.name}resource\"aws_iam_role\"\"node_role\"{name=\"${var.cluster_name}-node-role\"assume_role_policy=data.aws_iam_policy_document.assume_role_policy.json}data\"aws_iam_policy_document\"\"assume_role_policy\"{statement{actions=[\"sts:AssumeRole\"]principals{type=\"Service\"identifiers=[\"ec2.amazonaws.com\"]}}}# Node Group Policiesresource\"aws_iam_role_policy_attachment\"\"eks_worker_node_policy\"{role=aws_iam_role.node_role.namepolicy_arn=\"arn:aws:iam::aws:policy/AmazonEKSWorkerNodePolicy\"}resource\"aws_iam_role_policy_attachment\"\"eks_cni_policy\"{role=aws_iam_role.node_role.namepolicy_arn=\"arn:aws:iam::aws:policy/AmazonEKS_CNI_Policy\"}resource\"aws_iam_role_policy_attachment\"\"eks_ebs_csi_policy\"{role=aws_iam_role.node_role.namepolicy_arn=\"arn:aws:iam::aws:policy/service-role/AmazonEBSCSIDriverPolicy\"}resource\"aws_iam_role_policy_attachment\"\"eks_registry_policy\"{role=aws_iam_role.node_role.namepolicy_arn=\"arn:aws:iam::aws:policy/AmazonEC2ContainerRegistryReadOnly\"}# VPC CNI Plugin Roledata\"aws_iam_policy_document\"\"vpc_cni_assume_role_policy\"{statement{actions=[\"sts:AssumeRoleWithWebIdentity\"]effect=\"Allow\"condition{test=\"StringEquals\"variable=\"${replace(aws_iam_openid_connect_provider.eks.url,\"https://\",\"\")}:sub\"values=[\"system:serviceaccount:kube-system:aws-node\"]}principals{identifiers=[aws_iam_openid_connect_provider.eks.arn]type=\"Federated\"}}}resource\"aws_iam_role\"\"vpc_cni_role\"{assume_role_policy=data.aws_iam_policy_document.vpc_cni_assume_role_policy.jsonname=\"${var.cluster_name}-vpc-cni-role\"}resource\"aws_iam_role_policy_attachment\"\"vpc_cni_policy\"{policy_arn=\"arn:aws:iam::aws:policy/AmazonEKS_CNI_Policy\"role=aws_iam_role.vpc_cni_role.name}Enter fullscreen modeExit fullscreen modeCluster Access and Role Based Access ControlWhen an Amazon EKS cluster is created, the AWS identity (user or role) that creates the cluster is automatically grantedsystem:masterspermissions, providing full administrative access to the cluster. This ensures immediate operational control without additional configuration steps.However, in collaborative environments or larger organizations, more than one individual or service may need administrative access to manage the cluster effectively. To extend administrative privileges securely, additional IAM roles can be created and configured with permissions similar to the cluster creator.This Terraform configuration defines an IAM role (aws_iam_role.eks_admins_role) designed for EKS Administrators, granting them similar administrative capabilities over the EKS cluster. It uses an IAM policy document (data.aws_iam_policy_document.eks_admins_assume_role_policy_doc) to specify that the role can be assumed by specified AWS principals. This approach adheres to the principle of least privilege, allowing for controlled access to the EKS cluster based on organizational requirements.It's best to also add the node role with the usernamesystem:node:{{EC2PrivateDNSName}}in groups\"system:bootstrappers\", \"system:nodes\", though it is actually added to the ConfigMap automatically, it will get removed by your nextterraform applycommand if not defined explicitly.Lastly, thekubernetes_config_map.aws_authresource updates theaws-authConfigMap in the EKS cluster. This action registers the role with the cluster, mapping it tosystem:masters, thus granting it administrative access similar to the cluster creator. This setup ensures that designated administrators can manage the cluster without using the credentials of the original creator, enhancing security and operational flexibility.############################################################################################################### CLUSTER ROLE BASE ACCESS CONTROL############################################################################################################# Define IAM Role for EKS Administratorsresource\"aws_iam_role\"\"eks_admins_role\"{name=\"${var.cluster_name}-eks-admins-role\"assume_role_policy=data.aws_iam_policy_document.eks_admins_assume_role_policy_doc.json}# IAM Policy Document for assuming the eks-admins roledata\"aws_iam_policy_document\"\"eks_admins_assume_role_policy_doc\"{statement{actions=[\"sts:AssumeRole\"]principals{type=\"AWS\"identifiers=[\"arn:aws:iam::${data.aws_caller_identity.current.account_id}:root\"]}effect=\"Allow\"}}# Define IAM Policy for administrative actions on EKSdata\"aws_iam_policy_document\"\"eks_admin_policy_doc\"{statement{actions=[\"eks:*\",\"ec2:Describe*\",\"iam:ListRoles\",\"iam:ListRolePolicies\",\"iam:GetRole\"]resources=[\"*\"]}}# Create IAM Policy based on the above documentresource\"aws_iam_policy\"\"eks_admin_policy\"{name=\"${var.cluster_name}-eks-admin-policy\"policy=data.aws_iam_policy_document.eks_admin_policy_doc.json}# Attach IAM Policy to the EKS Administrators Roleresource\"aws_iam_role_policy_attachment\"\"eks_admin_role_policy_attach\"{role=aws_iam_role.eks_admins_role.namepolicy_arn=aws_iam_policy.eks_admin_policy.arn}# Update the aws-auth ConfigMap to include the IAM groupresource\"kubernetes_config_map\"\"aws_auth\"{metadata{name=\"aws-auth\"namespace=\"kube-system\"}data={mapRoles=yamlencode([{rolearn=aws_iam_role.eks_admins_role.arnusername=aws_iam_role.eks_admins_role.namegroups=[\"system:masters\"]},{rolearn=aws_iam_role.node_role.arnusername=\"system:node:{{EC2PrivateDNSName}}\"groups=[\"system:bootstrappers\",\"system:nodes\"]}])mapUsers=yamlencode([{userarn=data.aws_caller_identity.current.arnusername=split(\"/\",data.aws_caller_identity.current.arn)[1]groups=[\"system:masters\"]}])}}Enter fullscreen modeExit fullscreen modeConclusion: Mastering EKS ManagementBy delving into advanced configuration options, securing your clusters with IAM, and employing effective logging, monitoring, and scaling strategies, you can create Kubernetes clusters that are not only robust and scalable but also optimized for cost, performance, and security. The journey to mastering AWS EKS with Terraform is ongoing, and each step forward enhances your ability to manage complex cloud-native ecosystems effectively.The complete project can be found here:Sample Implementation GitHub Repo"}
{"title": "Frugal SQL data access with Athena and Blue / Green support", "published_at": 1710278463, "tags": ["aws", "database", "sql", "architecture"], "user": "Matt Houghton", "url": "https://dev.to/aws-builders/frugal-sql-data-access-with-athena-and-blue-green-support-1ool", "details": "IntroductionIn this post I look at a frugal architecture for SQL based data access.The prompt for writing this blog post came from a recent discussion on an application a team were looking to migrate to the cloud.The requirements for the migration were the ability to run SQL against the data which was very small in volume (<200Mb).During the discussion I turned to Athena which is one of my favourite AWS Services. Athena offers JDBC drivers so I suggested we could swap from the MySQL database which was going to be provided by RDS.I was also asked how I would handle a blue / green style deployment with Athena. The specific requirement was that each time the application was deployed the database would be replaced with a new version including all data.SQL SetupWith Athena there is no visible database resource to create like there is with RDS. The steps to allow SQL access to data are as follows.A bucket to store the data in.A Glue database / table created that defines the structure of the data held in S3A quick way to test this out is to use a tool likeMockarooto generate some test data and then have a Glue Crawler analyse the data in S3 and create the required data catalog entries.Here is the sample schema definition in Mockaroo.From here I created two S3 buckets. One would hold data for my 'Blue' deployment and one for the 'Green' deployment. I called the buckets myapp.sql.blue and myapp.sql.green.In Glue I created a database called MyApp just to provide a logical separation between this and any other databases I may have in the same account.I downloaded two sets of data from Mockaroo and uploaded a file to each of the S3 buckets.I then created a Glue Crawler for each bucket. Here is an example.Running both crawlers populates the Glue data catalog with two tables.At this point I'm able to run SQL in Athena against each of the tables.Athena Blue / GreenFor the Blue / Green component I utilise a View created in Athena. Just like in RDS views can be created with one more more tables in Athena using an SQL query. We don't need anything too complex here.create view myapp_sql as select * from myapp_sql_blue;Enter fullscreen modeExit fullscreen modeIn the data catalog the view appears alongside the tables.This gives me a consistent name for my application to point to. When I want to switch over the data being used I can simply recreate the view pointing to either the blue or green buckets data.create or replace view myapp_sql as select * from myapp_sql_green;Enter fullscreen modeExit fullscreen modeTestingLets create a lambda function to test this out.import json import pyathena   def lambda_handler(event, context):      connection = pyathena.connect(       s3_staging_dir=\"s3://athena.myapp.work/\",       region_name=\"us-east-1\"     )      cursor = connection.cursor()      query = \"SELECT * FROM myapp.myapp_sql\"      cursor.execute(query)      results = cursor.fetchall()      print(results)      return {         'statusCode': 200     }Enter fullscreen modeExit fullscreen modeRunning this gives the output below.A note on IAM. The Lambda function will require permissions for Athena and S3. For testing purposes I attached the AmazonAthenaFullAccess and AmazonS3FullAccess managed roles. In production you should scope the IAM down to least privileges requiredDeployment SwitchoverLet's now imagine that I have a pipeline that has loaded up my new data to my second bucket. In the pipeline I can run the following step to switch the view to the latest data.export TABLE_SUFFIX=green aws athena start-query-execution --query-string \"create or replace view myapp_sql as select * from myapp_sql_$TABLE_SUFFIX\" --result-configuration \"OutputLocation=s3://athena.myapp.work\" --query-execution-context \"Database=myapp\"Enter fullscreen modeExit fullscreen modeConsiderationsThis setup should work well for simple SQL based access to data where volumes are not too high. You can optimise queries further within Athena by using data formats such as Parquet.Costs for the storage assuming S3 standard tier will be ~$0.023 per GB/Month. Querying via Athena costs $5.00 per TB of data scanned. We only pay when we run a query unlike RDS which we have to pay for even when we are not running SQL.As long as the access characteristics of your application are a match for the performance of the AWS Services used then S3 based SQL access via Athena is a tough one to beat for those looking to beThe Frugal Architect"}
{"title": "Provisioned Concurrency - Reduce Cold Starts in AWS Lambda Functions Part 1", "published_at": 1710276292, "tags": ["cloud", "aws", "python", "devops"], "user": "Jorge Tovar", "url": "https://dev.to/aws-builders/provisioned-concurrency-reduce-cold-starts-in-aws-lambda-functions-part-1-4mob", "details": "This series of AWS Lambda articles focuses on performance and best practices. This is part 1, where we explore Lambda cold starts, warming a lambda, and provisioned concurrency. In parts 2 and 3, we will discuss Deployment preferences, Powertools, Observability and Auto-scaling Provisioned concurrency using metrics and patterns, all of this with Infrastructure as Code \ud83e\uddd1\ud83c\udffb\u200d\ud83d\udcbb.AWS Lambda functions have become one of the most important cloud services provided by Amazon. They are the cornerstone of many applications, serving as the core business logic for Serverless applications. They are also the default choice for operating and handling asynchronous or simple tasks that don't require a constantly running component. Over time, Lambda has evolved to be a fully event-driven service with numerous triggers and seamless integration with almost any AWS service.If you want to go straight to the code: \ud83d\ude80GitHub Code exampleHowever, one challenge remains in our quest for creating reliable and performant applications: Cold Starts. Despite AWS investing heavily to reduce this time, and for many use cases, it's something we can tolerate, it isn't always the case. Sometimes we need a predictable response latency for our product.Cold Starts and LatencyA cold start in AWS Lambda refers to the initial process of spinning up a new execution environment for a function when it's invoked for the first time or after a period of inactivity.To understand this, we need to know the lifecycle of an AWS Lambda function. Here are the steps that need to be performed in order to get a response from our function:Download the code (Cold start)Create a new execution environment (Cold start)Execute initialization code (Cold start)Execute code handlerShutdown# Initialization codeimportboto3s3_client=boto3.client('s3')# Code handlerdeflambda_handler(event,context):response=s3_client.list_buckets()bucket_names=[bucket['Name']forbucketinresponse['Buckets']]return{'statusCode':200,'body':bucket_names}Enter fullscreen modeExit fullscreen modeAs illustrated in the code above, we can conceptualize this process as occurring in three phases:1.Initialization PhaseDuring this phase, our code is downloaded, the execution environment with extensions and runtime is created, and the only aspect we control is the function initialization code. This code, outside of the handler, executes once during every cold start.2. Invoke HandlerThis phase executes every time we call our lambda function.3. ShutdownEnvironments are shut down after a non-deterministic period of inactivity. In such cases, a cold start occurs upon the next invocation.As you can deduce, in terms of cold starts, we only have control over the function initialization. Thus, we should aim to take advantage of any additional invocations within the same execution environment, known as a \"warm start.\"Warm StartWith multiple concurrent calls, AWS can utilize already provisioned environments, eliminating the need to download the code and run the initialization code again. Paradoxically, increased traffic can lead to improved performance.DeploymentsEvery time we update our code, it results in a cold start. This behavior ensures that we run the function with the latest code.Function WarmersIn the serverless community, there are libraries and in-house mechanisms used to \"warm\" Lambda via pings. However, this doesn't eliminate all cold starts, and sometimes it doesn't help in production environments when we experience spikes in traffic and need all of our concurrent environments available to serve user requests.We can use EventBridge rules to keep our Lambda warm, or use tools to simulate traffic. However, I'm not a big fan of this approach. In such cases, it may be worth considering the possibility of having a running container.Reducing Cold Starts with Provisioned ConcurrencyIf our product requires predictable start times and latency, provisioned concurrency is the way to go. Unlike Lambda's on-demand default behavior, we can avoid the initial phase and recurrent tasks of downloading and executing the initialization code.These environments are provided ahead of time, but remember:Everything in software architecture is a trade-off.We may end up with a higher cost of running our application since we have a running component available instead of getting something on demand.Provisioned concurrency works in conjunction with Lambda versions instead of using the $LATEST code as we usually do. We also don't need to optimize initialization code since this is executed before creating the execution environment.We may still have cold starts due to the defined capacity in our provisioned concurrency. Therefore, we can configure the amount of ready-to-use Lambdas in our system.We can create auto-scaling policies to make this more robust and reduce the cost of operating our application. If we have predictable patterns or metrics, we can reduce the amount of provisioned concurrent executions.We can also define some rollout and deployment preferences as part of this strategy, for example, using canary or linear deployments.Hands-on ExampleProvisioned concurrency repositoryEvery time that we make a HTTPs call for the very first time we would see the impact of the Cold start, with a response time that seems to be pretty high -> 2 seconds.{\"message\":\"Hello community builders - from AWS Lambda!\"}Enter fullscreen modeExit fullscreen modeWe usually get a new Log stream if we update the Lambda.Finally in the Logs we would see something like this, where we can validate the Init duration in our case it is 824.46 msREPORT RequestId: e8500c25-5c5f-4ffe-867e-60741c3b1df5 Duration: 18.57 ms Billed Duration: 19 ms Memory Size: 128 MB Max Memory Used: 64 MB **Init Duration: 824.46 ms* XRAY TraceId: 1-65f096ba-200e28165f811e694170deef SegmentId: 13f970807474e003 Sampled: true*We can go to the Configuration, Concurrency tab and set up everything.ConfigurationCreate a Lambda versionConfigure the number of available execution environmentsResultsAfter the deployment has been successful, if we make an HTTP call, we can avoid most of the cold starts and see consistent response times of 400 ms.We can implement all of this in our CI/CD and Infrastructure as Code processes, but I wanted to provide an overview with the console so you can feel comfortable playing with the settings and doing a proof of concept before diving deep and deploying this.Clean UpRemove all the resources using SAM frameworkConclusionProvisioned concurrency is the preferred solution for production workloads that rely on predictable response times and want to avoid cold starts.It is important to emphasize that the biggest problems in the software development and delivery process are organizational and people-related. In the end, building applications is a social activity, but the more we solve technical problems and improve communication, the easier it should be to reach production. Happy coding! \ud83c\udf89LinkedInTwitterGitHubIf you enjoyed the articles, visit my blogjorgetovar.dev"}
{"title": "PartyRock: Unleashing Creativity with SCAMPER", "published_at": 1710266722, "tags": ["partyrock", "beginners", "ai", "aws"], "user": "Gianluigi Mucciolo", "url": "https://dev.to/aws-builders/partyrock-unleashing-creativity-with-scamper-1c09", "details": "I recently shared my fascination with PartyRock at an AWS User Group Rome meeting, discussing how I've ventured from RPG simulators to innovative chat applications. Highlighting the potential of generative AI with AWS, I showcased a new PartyRock app, emphasizing prompt engineering for interactive conversations and creative outputs. Catch the session onYouTube.This journey inspired my latest project for the PartyRock Hackathon, utilizing the SCAMPER method to spur creativity and innovation among users, challenging conventional thinking.Innovation from the Ground UpMy adventure began with a concept deeply rooted in the SCAMPER technique. This tool was imagined as a digital mentor, guiding users through the seven essential steps of creative problem-solving, pushing them to view challenges from perspectives they hadn't considered before.Crafting with Cutting-Edge TechnologyAmazon PartyRock became the foundation of my creation, bringing my ideas to life. Through immediate engineering and the use of the Claude model, I integrated the SCAMPER methodology into the core of my application. This combination was a deliberate choice, with the goal of positioning the tool at the intersection of technology and creativity.A Step-by-Step Guide to Unleashing CreativityDefine the challenge: Start with clarity, introducing a section for users to articulate their challenge, laying the groundwork for targeted solutions.Language Selection: Focusing on inclusivity, the tool includes a language selector, allowing users to explore solutions in the language they are most comfortable with.Kickstart with Substitution: By activating the \"Substitute Enabler,\" users can begin the substitution phase, examining alternatives and replacements for elements of their challenge, thereby broadening their horizons.Explore Substitution in Depth: The tools includes a widget to help identify substitutable elements, offering guidance and summarizing proposed changes for a comprehensive exploration of potential solutions.Navigate through the SCAMPER strategies: The tool includes a widget for each SCAMPER strategy: Replace, Combine, Adapt, Modify, Put to Another Use Items, Eliminate, Reverse, to Encourage a thorough investigation of all the possibilities for innovation.Reveal the Final Solution: The \"SCAMPER Result\" widget presents an innovative solution or improvement at the end of the journey, demonstrating the effectiveness of structured creativity.Address Contradictions: The \"SCAMPER Contradictions\" widget ensures the solution's coherence by addressing any contradictions that might emerge during the brainstorming process.Expand with AI Insights: The \"Critical AI SCAMPER\" widget pushes the boundaries further, offering AI-driven suggestions to refine and enhance the idea.Navigating Challenges and Learning Along the WayA significant challenge was ensuring smooth performance with multiple widgets running at the same time. Introducing an \"ON/OFF\" feature dramatically improved the user experience. This journey wasn't just about creation; it deepened my understanding of prompt engineering and highlighted the importance of performance optimization.The Road AheadLooking forward, I'm focused on enhancing the tool's performance and functionality. The community's engagement excites me, and I'm keen to integrate their feedback into the continuous development of our tool.SCAMPER: A Powerful Tool for Creative Problem-Solving"}
{"title": "Checklist for designing cloud-native applications \u2013 Part 2: Security aspects", "published_at": 1710257384, "tags": ["aws", "cloud", "security", "design"], "user": "Eyal Estrin", "url": "https://dev.to/aws-builders/checklist-for-designing-cloud-native-applications-part-2-security-aspects-4efc", "details": "This post was originally published by theCloud Security Alliance.InChapter 1of this series about considerations when building cloud-native applications, we introduced various topics such as business requirements, infrastructure considerations, automation, resiliency, and more.In this chapter, we will review security considerations when building cloud-native applications.IAM Considerations - AuthenticationIdentity and Access Management plays a crucial role when designing new applications.We need to ask ourselves \u2013 Who are our customers?If we are building an application that will serve internal customers, we need to make sure our application will be able to sync identities from our identity provider (IdP).On the other hand, if we are planning an application that will serve external customers, in most cases we would not want to manage the identities themselves, but rather allow authentication based on SAML, OAuth, or OpenID connect, and manage the authorization in our application.Example of managed cloud-native identity service:AWS IAM Identity Center.IAM Considerations - AuthorizationAuthorization is also an important factor when designing applications.When our application consumes services (such as compute, storage, database, etc.) from a CSP ecosystem, each CSP has its mechanisms to manage permissions to access services and take actions, and each CSP has its way of implementing Role-based access control (RBAC).Regardless of the built-in mechanisms to consume cloud infrastructure, we must always follow the principle of least privilege (i.e., minimal permissions to achieve a task).On the application layer, we need to design an authorization mechanism to check each identity that was authenticated to our application, against an authorization engine (interactive authentication, non-interactive authentication, or even API-based access).Although it is possible to manage authorization using our own developed RBAC mechanism, it is time to consider more cloud-agnostic authorization policy engines such as Open Policy Agent (OPA).One of the major benefits of using OPA is the fact that its policy engine is not limited to authorization to an application \u2013 you can also use it for Kubernetes authorization, for Linux (using PAM), and more.Policy-as-Code ConsiderationsPolicy-as-Code allows you to configure guardrails on various aspects of your workload.Guardrails are offered by all major cloud providers, outside the boundary of a cloud account, and impact the maximum allowed resource consumption or configuration.Examples of guardrails:Limitation on the allowed region for deploying resources (compute, storage, database, network, etc.)Enforce encryption at restForbid the ability to create publicly accessible resources (such as a VM with public IP)Enforce the use of specific VM instance size (number of CPUs and memory allowed)Guardrails can also be enforced as part of a CI/CD pipeline when deploying resources using Infrastructure as Code for automation purposes \u2013 The IaC code is been evaluated before the actual deployment phase, and assuming the IaC code does not violate the Policy as Code, resources are been updated.Examples of Policy-as-Code:AWS Service control policies (SCPs),HashiCorp Sentinel, andOpen Policy Agent (OPA).Data Protection ConsiderationsAlmost any application contains valuable data, whether the data has business or personal value, and as such we must protect the data from unauthorized parties.A common way to protect data is to store it in encrypted form:Encryption in transit \u2013 done using protocols such as TLS (where the latest supported version is 1.3)Encryption at rest \u2013 done on a volume, disk, storage, or database level, using algorithms such as AESEncryption in use \u2013 done using hardware supporting a trusted execution environment (TEE), also referred to as confidential computingWhen encrypting data we need to deal with key generation, secured vault for key storage, key retrieval, and key destruction.All major CSPs have their key management service to handle the entire key lifecycle.If your application is deployed on top of a single CSP infrastructure, prefer to use managed services offered by the CSP.For encryption in use, select services (such as VM instances or Kubernetes worker nodes) that support confidential computing.Secrets Management ConsiderationsSecrets are equivalent to static credentials, allowing access to services and resources.Examples of secrets are API keys, passwords, database credentials, etc.Secrets, similarly to encryption keys, are sensitive and need to be protected from unauthorized parties.From the initial application design process, we need to decide on a secured location to store secrets.All major CSPs have their own secrets management service to handle the entire secret\u2019s lifecycle.As part of a CI/CD pipeline, we should embed an automated scanning process to detect secrets embedded as part of code, scripts, and configuration files, to avoid storing any secrets as part of our application (i.e., outside the secured secrets management vault).Examples of secrets management services:AWS Secrets Manager, andHashiCorp Vault.Network Security ConsiderationsApplications must be protected at the network layer, whether we expose our application to internal customers or customers over the public internet.The fundamental way to protect infrastructure at the network layer is using access controls, which are equivalent to layer 3/layer 4 firewalls.All CSPs have access control mechanisms to restrict access to services (from access to VMs, databases, etc.)Example of Layer 3 / Layer 4 managed service:AWS Security groups.Some cloud providers support private access to their services, by adding a network load-balancer in front of various services, with an internal IP from the customer\u2019s private subnet, enforcing all traffic to pass inside the CSP\u2019s backbone, and not over the public internet.Example of private connectivity solution:AWS PrivateLink.Some of the CSPs offer managed layer 7 firewalls, allowing customers to enforce traffic based on protocols (and not ports), inspecting TLS traffic for malicious content, and more, in case your application or business requires those capabilities.Example of Layer 7 managed firewall:AWS Network Firewall.Application Layer Protection ConsiderationsAny application accessible to customers (internal or over the public Internet), is exposed to application layer attacks.Attacks can range from malicious code injection, data exfiltration (or data leakage), data tampering, unauthorized access, and more.Whether you are exposing an API, a web application, or a mobile application, it is important to implement application layer protection, such as a WAF service.All major CSPs offer managed WAF services, and there are many SaaS solutions by commercial vendors that offer managed WAF services.Example of managed WAF service:AWS WAF.DDoS Protection ConsiderationsDenial-of-Service (DoS) or Distributed Denial-of-Service (DDoS) is a risk for any service accessible over the public Internet.Such attacks try to consume all the available resources (from network bandwidth to CPU/memory), directly impacting the service availability to be accessible by customers.All major CSPs offer managed DDoS protection services, and there are many DDoS protection solutions by commercial vendors that offer managed DDoS protection services.Examples of managed DDoS protection services:AWS Shield, andCloudflare DDoS protection.Patch Management ConsiderationsSoftware tends to be vulnerable, and as such it must be regularly patched.For applications deployed on top of virtual machines:Create a \"golden image\" of a virtual machine, and regularly update the image with the latest security patches and software updates.For applications deployed on top of VMs, create a regular patch update process.For applications wrapped inside containers, create a \"golden image\" of each of the application components, and regularly update the image with the latest security patches and software updates.Embed software composition analysis (SCA) tools to scan and detect vulnerable third-party components \u2013 in case vulnerable components (or their dependencies) are detected, begin a process of replacing the vulnerable components.Example of patch management solution:AWS Systems Manager Patch Manager.Compliance ConsiderationsCompliance is an important security factor when designing an application.Some applications contain personally identifiable information (PII) about employees or customers, which requires compliance against privacy and data residency laws and regulations (such as the GDPR in Europe, the CPRA in California, the LGPD in Brazil, etc.)Some organizations decide to be compliant with industry or security best practices, such as the Center for Internet Security (CIS) Benchmark for hardening infrastructure components, and can be later evaluated using compliance services or Cloud security posture management (CSPM) solutions.Reference for compliance:AWS Compliance Center.Incident ResponseWhen designing an application in the cloud, it is important to be prepared to respond to security incidents:Enable logging from both infrastructure and application components, and stream all logs to a central log aggregator. Make sure logs are stored in a central, immutable location, with access privileges limited for the SOC team.Select a tool to be able to review logs, detect anomalies, and be able to create actionable insights for the SOC team.Create playbooks for the SOC team, to know how to respond in case of a security incident (how to investigate, where to look for data, who to notify, etc.)To be prepared for a catastrophic event (such as a network breach, or ransomware), create automated solutions, to allow you to quarantine the impacted services, and deploy a new environment from scratch.Reference for incident response documentation:AWS Security Incident Response Guide.SummaryIn the second blog post in this series, we talked about many security-related aspects, that organizations should consider when designing new applications in the cloud.In this part of the series, we have reviewed various aspects, from identity and access management to data protection, network security, patch management, compliance, and more.It is highly recommended to use the topics discussed in this series of blog posts, as a baseline when designing new applications in the cloud, and continuously improve this checklist of considerations when documenting your projects.About the AuthorEyal Estrin is a cloud and information security architect, and the author of the bookCloud Security Handbook, with more than 20 years in the IT industry. You can connect with him onTwitter.Opinions are his own and not the views of his employer."}
{"title": "DevOps with Guruu | Chapter 3: Install and Use Docker with EC2 | Build Your First CI/CD Pipeline", "published_at": 1710256049, "tags": ["webdev", "devops", "aws"], "user": "Hoang Guruu", "url": "https://dev.to/aws-builders/devops-with-guruu-chapter-3-install-and-use-docker-with-ec2-build-your-first-cicd-pipeline-4bnb", "details": "Get more detail with YoutubeDevOps with Guruu | Chapter 3: Install and Use Docker with EC2 | Build Your First CI/CD PipelineWelcome back to the third chapter of our DevOps with Guruu series! In this segment, we'll focus on two crucial components: Docker and EC2, along with building your very first CI/CD pipeline. Let's dive in:Setup EC2 Instance:Launch an EC2 instance on AWS with your desired specifications.Connect to your instance using SSH for further configurations.Install Docker on EC2 Instance:Update the package index.Install necessary packages to allow APT to use a repository over HTTPS.Add the Docker GPG key.Add the Docker repository to APT sources.Update the package database.Install Docker.Verify Docker installation.Docker Run Image Nginx:Pull the Nginx image from Docker Hub.Run the Nginx container, mapping port 80 of the container to port 80 of the host.Docker Run Image Jenkins:Pull the Jenkins image from Docker Hub.Run the Jenkins container, mapping ports 8080 and 50000 to the host.Docker Build Your Image:Navigate to your project directory containing your Dockerfile.Build your Docker image.Docker Run Your Image and Push Image:Run your Docker image.Tag your image with the repository URL.Push your image to the repository.By following these steps, you'll have Docker installed and running on your EC2 instance, along with deploying Nginx and Jenkins containers. Additionally, you'll have built your custom Docker image and pushed it to a repository.Join us on this journey to mastering DevOps! These tutorials lay a solid foundation for your DevOps skills. Don't forget to like, share, and subscribe for more insightful content. Happy learning, aspiring DevOps engineers! \ud83c\udf1fStay tuned for the upcoming chapters, where we'll delve into more advanced DevOps concepts. Until then, keep exploring and experimenting! Cheers! \ud83d\ude80"}
{"title": "Unleash Your Creativity with AWS Bedrock: Join Our Virtual Summit (Day 1)", "published_at": 1710253903, "tags": ["bedrock"], "user": "Danny Chan", "url": "https://dev.to/aws-builders/unleash-your-creativity-with-aws-bedrock-join-our-virtual-meetup-day-1-1o8n", "details": "\ud83c\udf89 Last month, we had an amazing time at the AWS post-party on Hong Kong Island!\ud83e\udd1d We're proud partners of Hong Kong's cloud service providers.\ud83d\udc68\u200d\ud83d\udcbc Meet Aaron, the Chief Leader of Cloud Engineers in the Hong Kong Financial Services sector.\ud83c\udfe2 Check out the AWS Market Technology booth \u2013 it's incredible!\ud83c\udfa4 Charlotte, the SDK Guru, gave the opening speech and presented the agenda at the night roadshow.\u2699\ufe0f They're serverless builders, dedicated to stock market trading systems.\ud83d\udc68\u200d\ud83d\udcbb Scott, the Hong Kong Insurance Blockchain Cloud Engineer and swagDAO partner, made a splash.\ud83c\udf1f The SWAG booth stole the show at the three-day Machine Learning Conference in Hong Kong Exhibition Center.\ud83d\udd2e Isabella, the CTO and Co-founder of swagDAO and cloudDAO, showcased a sneak peek of the next-gen blockchain layer-three transaction cloud infrastructure.\ud83d\ude80 Owen, the Chief User Experience Officer of the \"Spaceship from Greater China\" gamefi project, hosted a workshop on \"Building Communities in Dark Space.\"\ud83d\udcbc The winners of the hackathon at the conference were busy chatting with venture capitalists.\ud83d\udca1 Camila, the cloud infrastructure intern, secured third place in the hackathon with her project on blockchain rollup transactions from layer one to layer three.\ud83e\udd1d Jayden, Co-founder of golden Swag community, is dedicated to global online stock market makers and secondary market collaborations.\ud83c\udf99\ufe0f James, our charismatic event host, is also a front-end open-source framework developer.\ud83c\udfa7 The DJ band \"Alias Master\" rocked the party with their incredible performance.\ud83d\udd27 The F1 engineering unit fine-tuned their engines using AWS machine learning services.\ud83d\udc83 The dance crew \"Hong Kong Underground Engineering Team\" wowed the crowd with their breakdance performance.\ud83d\udd7a The dance group \"New City Cleaners\" showcased their ghost step street dance moves at the post-party.\ud83c\udfd9\ufe0f Hudson, the Data Knowledge Center software engineer, won five hackathons and applied smart city IoT applications to hotels.\ud83c\udfb5 The music group \"Sweet Boys\" performed London's new era music at the post-party.\ud83d\udc68\u200d\ud83d\udcbb Wyatt, the DevOps Supervisor at ToKong, is committed to training machine learning models for enterprise power systems.\ud83c\udfad The Hong Kong emerging dance group \"Oh Dear Dream Boy\" performed installation art and Cantonese song-inspired dance on the stage.\ud83d\ude9a An excellent team of solution architects from Hong Kong, dedicated to disaster recovery in logistics systems.\ud83d\udd12 Jack, the security expert, helped the Hong Kong Financial Services Academy adopt multi-cloud security posture.\ud83c\udfb5 Amelia, a former European dancer turned machine learning musician, is developing a new project for Nano DJ.\ud83c\udf03 People relaxed at the outdoor club after the AWS Hong Kong Machine Learning Summit party.\ud83d\udcbc Mila, the Cloud Service Account Manager, is an cloud partner helping businesses adopt cloud infrastructure.\ud83c\udfaa The AWS Hong Kong Machine Learning Summit's main stage offered free food and beverages.\ud83d\udc83 The Hong Kong Marketing Team, \"Queen Eva and Knight Mia,\" welcomed new AWS account sign-ups with dancer hugs.\ud83d\udcbc Emily, Riley, and Victoria, a team of three young female entrepreneurs who won the hackathon's business founder track this year, are working on the Happy Hour Club project.\ud83c\udfa7 The Hong Kong Bookworm DJ team performed silent music, showcasing their finger-dancing skills."}
{"title": "Add alternate contacts to AWS Organization member accounts programmatically", "published_at": 1710227040, "tags": ["aws", "awsorganization"], "user": "Olawale Adepoju", "url": "https://dev.to/aws-builders/add-alternate-contacts-to-aws-organization-member-accounts-programmatically-4ipd", "details": "To manage the alternate contacts (billing, operations, and security) on your member accounts in AWS Organizations can be daunting sometimes especially when there are quite a large number of member account in the AWS Organization. To input it one after the other can be tasking, so i will be showing how to set the same alternate contacts across all of your accounts programmatically across Organization.#### Why Alternate Account?Mostly we want to right people to receive AWS notification regarding billing, operations and security on all of your accounts so that your Cloud Center of Excellence (CCoE) team can receive important notifications about your AWS accounts and take due actions.Managing alternate contacts become even more important as your organization scales to hundreds or thousands of accounts, saving you time and reducing operational burden.We\u2019re going to useAWS CloudShell, a browser-based shell that is automatically authenticated with your AWS console credentials and accessible via the upper navigation bar of the AWS console.Note:First need to make sure that the AWS Identity and Access Management (IAM) user or role you want to manage alternate contacts with has the following permissions:account: GetAlternateContact \u2013 allows the user to view the current alternate contactaccount: PutAlternateContact \u2013 allows the user to set a new alternate contactaccount: DeleteAlternateContact \u2013 allows the user to delete an alternate contactBetter so grant the requisite permissions to manage alternate contacts by attaching theAWSAccountManagementFullAccessmanaged policy to your IAM user or role.Next, you\u2019ll need to enable the AWS Account Management service for your organization so you can centrally manage alternate contacts. You can do this by using this CLI command from the management account:aws organizations enable-aws-service-access --service-principal account.amazonaws.comEnter fullscreen modeExit fullscreen modeFinally, you can register a delegated administrator so users don\u2019t need access to the management account to manage alternate contacts.aws organizations register-delegated-administrator --account-id <YOUR-CHOSEN-ACCOUNT-ID> --service-principal account.amazonaws.comEnter fullscreen modeExit fullscreen mode#### Automating the Alternate contactsloop-accounts.sh \u2013 This script gathers a list of all accounts in your organization and then executes the security-contact.sh script. Paste the script in your CloudShellcat << EOF > loop-accounts.sh #! /bin/bash     managementaccount=\\`aws organizations describe-organization --query Organization.MasterAccountId --output text\\`      for account in \\$(aws organizations list-accounts --query 'Accounts[].Id' --output text); do              if [ \"\\$managementaccount\" -eq \"\\$account\" ]                      then                          echo 'Skipping management account.'                          continue             fi             ./security-contact.sh -a \\$account             sleep 0.2     done EOF chmod 755 loop-accounts.shEnter fullscreen modeExit fullscreen modeNote:The management account is explicitly excluded from the account list. This is because alternate contacts for the management account can only be modified using the standalone context, not the organization context.security-contact.sh \u2013 This script sets the security alternate contact to the member account in the AWS Organization. Paste the script in your CloudShellcat << EOF > security-contact.sh #! /bin/bash while getopts a: flag do     case \"\\${flag}\" in         a) account_id=\\${OPTARG};;     esac done  echo 'Put security contact for account '\\$account_id'...' aws account put-alternate-contact \\   --account-id \\$account_id \\   --alternate-contact-type=SECURITY \\   --email-address=mysecurity-contact@example.com \\   --phone-number=\"+1(111)222-3333\" \\   --title=\"Security Contact\" \\   --name=\"My Name\" echo 'Done putting security contact for account '\\$account_id'.'  EOF chmod 755 security-contact.shEnter fullscreen modeExit fullscreen modeFYI:make sure to replace the contact details with your actual contact information."}
{"title": "High-Speed Packet Processing in Go: From net.Dial to AF_XDP", "published_at": 1710206036, "tags": ["go", "programming", "devops", "performance"], "user": "Andree Toonk", "url": "https://dev.to/aws-builders/high-speed-packet-processing-in-go-from-netdial-to-afxdp-5784", "details": "This post was originally published on my Personal blog athttps://toonk.io/sending-network-packets-in-go/Pushing limits in Go: from net.Dial to syscalls, AF_PACKET, and lightning-fast AF_XDP. Benchmarking packet sending performance..Recently, I wrote a Go program thatsends ICMP ping messagesto millions of IP addresses. Obviously I wanted this to be done as fast and efficiently as possible. So this prompted me to look into the various methods of interfacing with the network stack and sending packets, fast! It was a fun journey, so in this article, I\u2019ll share some of my learnings and document them for my future self :) You\u2019ll see how we get to 18.8Mpps with just 8 cores. There\u2019s alsothis Github repo that has the example code, making it easy to follow along.The use caseLet\u2019s start with a quick background of the problem statement. I want to be able to send as many packets per second from a Linux machine. There are a few use cases, for example, the Ping example I mentioned earlier, but also maybe something more generic like dpdk-pktgen or even something Iperf. I guess you could summarize it as a packet generator.I\u2019m using the Go programming language to explore the various options. In general, the explored methods could be used in any programming language since these are mostly Go-specific interfaces around what the Linux Kernel provides. However, you may be limited by the libraries or support that exist in your favorite programming language.Let\u2019s start our adventure and explore the various ways to generate network packets in Go. I\u2019ll go over the options, and we\u2019ll end with a benchmark, showing us which method is the best for our use case. I\u2019ve included examples of the various methods in a Go package; you can find the code here. We\u2019ll use the same code to run a benchmark and see how the various methods compare.The net.Dial methodThe net.Dial method is the most likely candidate for working with network connections in Go. It\u2019s a high-level abstraction provided by the standard library\u2019s net package, designed to establish network connections in an easy-to-use and straightforward manner. You would use this for bi-directional communication where you can simply read and write to a Net.Conn (socket) without having to worry about the details.In our case, we\u2019re primarily interested in sending traffic, using the net.Dial method that looks like this:conn, err := net.Dial(\"udp\", fmt.Sprintf(\"%s:%d\", s.dstIP, s.dstPort)) if err != nil {     return fmt.Errorf(\"failed to dial UDP: %w\", err) } defer conn.Close()Enter fullscreen modeExit fullscreen modeAfter that, you can simply write bytes to your conn like thisconn.Write(payload)Enter fullscreen modeExit fullscreen modeYou can find our code for this in the fileaf_inet.goThat\u2019s it! Pretty simple, right? As we\u2019ll see, however, when we get to the benchmark, this is the slowest method and not the best for sending packets quickly. Using this method, we can get to about 697,277 ppsRaw SocketMoving deeper into the network stack, I decided to use raw sockets to send packets in Go, unlike the more abstract net.Dial method, raw sockets provide a lower-level interface with the network stack, offering granular control over packet headers and content. This method allows us to craft entire packets, including the IP header, manually.To create a raw socket, we\u2019ll have to make our own syscall, give it the correct parameters, and provide the type of traffic we\u2019re going to send. We\u2019ll then get back a file descriptor. We can then read and write to this file descriptor. This is what it looks like at the high level; seerawsocket.gofor the complete code.fd, err := syscall.Socket(syscall.AF_INET, syscall.SOCK_RAW, syscall.IPPROTO_RAW) if err != nil {     log.Fatalf(\"Failed to create raw socket: %v\", err) } defer syscall.Close(fd)  // Set options: here, we enable IP_HDRINCL to manually include the IP header if err := syscall.SetsockoptInt(fd, syscall.IPPROTO_IP, syscall.IP_HDRINCL, 1); err != nil {     log.Fatalf(\"Failed to set IP_HDRINCL: %v\", err) }Enter fullscreen modeExit fullscreen modeThat\u2019s it, and now we can read and write our raw packet to file descriptor like thiserr := syscall.Sendto(fd, packet, 0, dstAddr)Enter fullscreen modeExit fullscreen modeSince I\u2019m usingIPPROTO_RAW, we\u2019re bypassing the transport layer of the kernel\u2019s network stack, and the kernel expects us to provide a complete IP packet. We do that using the BuildPacket function. It\u2019s slightly more work, but the neat thing about raw sockets is that you can construct whatever packet you want.We\u2019re telling the kernel just to take our packet, it has to do less work, and thus, this process is faster. All we\u2019re really asking from the network stack is to take this IP packet, add the ethernet headers, and hand it to the network card for sending. It comes as no surprise, then, that this option is indeed faster than the Net.Dial option. Using this method, we can reach about 793,781 pps, about 100k PPS more than the net.Dial method.The AF_INET Syscall MethodNow that we\u2019re used to using syscalls directly, we have another option. In this example, we create a UDP socket directly like belowfd, err := syscall.Socket(syscall.AF_INET, syscall.SOCK_DGRAM, syscall.IPPROTO_UDP)Enter fullscreen modeExit fullscreen modeAfter that we can simply write our payload to it using the Sendto method like before.err = syscall.Sendto(fd, payload, 0, dstAddr)Enter fullscreen modeExit fullscreen modeIt looks similar to the raw socket example, but a few differences exist. The key difference is that in this case we\u2019ve created a socket of type UDP, which means we don\u2019t need to construct the complete packet (IP and UDP header) like before. When using this method, the kernel manages the construction of the UDP header based on the destination IP and port we specify and handles the encapsulation process into an IP packet.In this case, the payload is just the UDP payload. In fact, this method is similar to the Net.Dial method before, but with fewer abstractions.Compared to the raw socket method before, I\u2019m now seeing 861,372 pps \u2014 that\u2019s a 70k jump. We\u2019re getting faster each step of the way. I\u2019m guessing we get the benefit of some UDP optimizations in the kernel.The Pcap MethodIt may be surprising to see Pcap here for sending packets. Most folks know pcap from things like tcpdump or Wireshark to capture packets. But it\u2019s also a fairly common way to send packets. In fact, if you look at many of the Go-packet or Python Scappy examples, this is typically the method listed to send custom packets. So, I figured I should include it and see its performance. I was skeptical, but was pleasantly surprised when I saw the pps numbers!First, let\u2019s take a look at what this looks like in Go; again, for the complete example, see my implementation in pcap.go hereWe start by creating a Pcap handle like this:handle, err := pcap.OpenLive(s.iface, 1500, false, pcap.BlockForever) if err != nil {     return fmt.Errorf(\"could not open device: %w\", err) } defer handle.Close()Enter fullscreen modeExit fullscreen modeThen we create the packet manually, similar to the Raw socket method earlier, but in this case, we include the Ethernet headers.After that, we can write the packet to the pcap handle, and we\u2019re done!err := handle.WritePacketData(packet)Enter fullscreen modeExit fullscreen modeTo my surprise, this method resulted in quite a performance win. We surpassed the one million packets per second mark by quite a margin: 1,354,087 pps \u2014 almost a 500k pps jump!Note that, towards the end of this article, we\u2019ll look at a caveat, but good to know that this method stops working well when sending multiple streams (go routines).The af_packet methodAs we explore the layers of network packet crafting and transmission in Go, we next find the AF_PACKET method. This method is popular for IDS systems on Linux, and for good reasons!It gives us direct access to the network device layer, allowing for the transmission of packets at the link layer. This means we can craft packets, including the Ethernet header, and send them directly to the network interface, bypassing the higher networking layers. We can create a socket of type AF_PACKET using a syscall. In Go this will look like this:fd, err := syscall.Socket(syscall.AF_PACKET, syscall.SOCK_RAW, int(htons(syscall.ETH_P_IP)))Enter fullscreen modeExit fullscreen modeThis line of code creates a raw socket that can send packets at the Ethernet layer. With AF_PACKET, we specify SOCK_RAW to indicate that we are interested in raw network protocol access. By setting the protocol toETH_P_IP, we tell the kernel that we\u2019ll be dealing with IP packets.After obtaining a socket descriptor, we must bind it to a network interface. This step ensures that our crafted packets are sent out through the correct network device:addr := &syscall.SockaddrLinklayer{     Protocol: htons(syscall.ETH_P_IP),     Ifindex:  ifi.Index, }Enter fullscreen modeExit fullscreen modeCrafting packets with AF_PACKET involves manually creating the Ethernet frame. This includes setting both source and destination MAC addresses and the EtherType to indicate what type of payload the frame is carrying (in our case, IP). We\u2019re using the same BuildPacket function as we used for the Pcap method earlier.The packet is then ready to be sent directly onto the wire:syscall.Sendto(fd, packet, 0, addr)Enter fullscreen modeExit fullscreen modeThe performance of the AF_PACKET method turns out to be almost identical to that achieved with the pcap method earlier. A quick Google, shows that libpcap, the library underlying tools like tcpdump and the Go pcap bindings, uses AF_PACKET for packet capture and injection on Linux platforms. So, that explains the performance similarities.Using the AF_XDP SocketWe have one more option to try. AF_XDP is a relatively recent development and promises impressive numbers! It is designed to dramatically increase the speed at which applications can send and receive packets directly from and to the network interface card (NIC) by utilizing a fast path through the traditional Linux network stack. Also see my earlier blog on XDP here.AF_XDP leverages the XDP (eXpress Data Path) framework. This capability not only provides minimal latency by avoiding kernel overhead but also maximizes throughput by enabling packet processing at the earliest possible point in the software stack.The Go standard library doesn\u2019t natively support AF_XDP sockets, and I was only able to find one library to help with this. So it\u2019s all relatively new still.I\u2019m using this library github.com/asavie/xdp and this is how you can initiate an AF_XDP socket.xsk, err := xdp.NewSocket(link.Attrs().Index, s.queueID, nil)Enter fullscreen modeExit fullscreen modeNote that we need to provide a NIC queue; this is a clear indicator that we\u2019re working at a lower level than before. The complete code is a bit more complicated than the other options, partially because we need to work with a user-space memory buffer (UMEM) for packet data. This method reduces the kernel\u2019s involvement in packet processing, cutting down the time packets spend traversing system layers. By crafting and injecting packets directly at the driver level. So, instead of pasting the code,please look at my code here.The results look great; using this method, I can now generate 2,647,936 pps. That\u2019s double the performance we saw with AF_PACKET! Whoohoo!Wrap-up and some takeawaysFirst off, this was fun to do and learn! We looked at the various options to generate packets from the traditional net.Dial method, to raw sockets, pcap, AF_PACKET and finally AF_XDP. The graph below shows the numbers per method (all using one CPU and one NIC queue). AF_XDP is the big winner!If interested, you can run the benchmarks yourself on a Linux system like below:./go-pktgen --dstip 192.168.64.2 --method benchmark \\  --duration 5 --payloadsize 64 --iface veth0  +-------------+-----------+------+ |   Method    | Packets/s | Mb/s | +-------------+-----------+------+ | af_xdp      |   2647936 | 1355 | | af_packet   |   1368070 |  700 | | af_pcap     |   1354087 |  693 | | udp_syscall |    861372 |  441 | | raw_socket  |    793781 |  406 | | net_conn    |    697277 |  357 | +-------------+-----------+------+Enter fullscreen modeExit fullscreen modeThe important number to look at is packets per second as that is the limitation on software network stacks. The Mb/s number is simply the packet size x the PPS number you can generate. It\u2019s interesting to see the easy 2x jump from the traditional net.Dial approach to using AF_PACKET. And then another 2x jump when using AF_XDP. Certainly good to know if you\u2019re interested in sending packets fast!The benchmark tool above uses one CPU and, thus, one NIC queue by default. The user can, however, elect to use more CPUs, which will start multiple go routines to do the same tests in parallel. The screenshot below shows the tool running with eight streams (and 8 CPUs) using AF_XDP, generating 186Gb/s with 1200 byte packets (18.8Mpps)! That\u2019s really quite impressive for a Linux box (and not using DPDK). Faster than what you can do with Iperf3 for example.Some caveats and things I\u2019d like to look at in the futureRunning multiple streams (go routines) using the PCAP method doesn\u2019t work well. The performance degrades significantly. The comparable AF_PACKET method, on the other hand, works well with multiple streams and go routines.The AF_XDP library I\u2019m using doesn\u2019t seem to work well on most hardware NICs. I opened a GitHub issue for this and hope it will be resolved. It would be great to see this be more reliable as it kind of limits more real-world AF_XDP Go applications. I did most of my testing using veth interfaces; i\u2019d love to see how it works on a physical NIC and a driver with XDP support.It turns out that for AF_PACKET, there\u2019s a zero-copy mode facilitated by the use of memory-mapped (mmap) ring buffers. This feature allows user-space applications to directly access packet data in kernel space without the need for copying data between the kernel and user space, effectively reducing CPU usage and increasing packet processing speed. This means that, in theory, the performance of AF_PACKET and AF_XDP could be very similar. However, it appears the Go implementations of AF_PACKET do not support zero-copy mode or only for RX and not TX. So I wasn\u2019t able to use that. I found this patch but unfortunately couldn\u2019t get it to work within an hour or so, so I moved on. If this works, this will likely be the preferred approach as you don\u2019t have to rely on AF_XDP support.Finally, I\u2019d love to include DPDK support in this pktgen library. It\u2019s the last one missing. But that\u2019s a whole beast on its own, and I need to rely on good Go DPDK libraries. Perhaps in the future!That\u2019s it; you made it to the end! Thanks for reading!Cheers-Andree"}
{"title": "How to retrieve DynamoDB items using secrets stored in AWS Secrets Manager with AWS Lambda - 1", "published_at": 1710196124, "tags": ["dynamodb", "secretsmangaer", "lambda", "accesskeys"], "user": "Revathi Joshi", "url": "https://dev.to/aws-builders/how-to-retrieve-dynamodb-items-using-secrets-stored-in-aws-secrets-manager-with-aws-lambda-1-4n6b", "details": "AWS Secrets Managerhelps you manage, retrieve, and rotate database credentials, application credentials, OAuth tokens, API keys, Encryption keys, SSH keys and other secrets throughout their lifecycles.You replace hard-coded credentials with a runtime call to the Secrets Manager service to retrieve credentials dynamically when you need them. And AWS Secrets Manager eliminates the need to hardcode sensitive information in plain text.It provides default encryption to your secrets stored in AWS Secrets Manager.Secrets Manager offers pay as you go pricing.AWS DynamoDBis a fast and fully managed NoSQL database designed for applications that need consistent, single-digit millisecond latency at any scale.It is a fully managed database and it supports both document and key value data models.It has a very flexible data model. This means that you don't need to define your database schema upfront. Yet it provides fast, reliable and predictable performance.DynamoDB tablesconsist of:Items (Similar to a row of data in a table).Attributes (Similar to a column of data in a table).Supports key-value and document data structures.Key = the name of the data.  Value = the data itself.Document can be written in JSON, HTML or XML.AWS Lambdais a compute service that lets you run code without provisioning or managing servers.With Lambda, you can run code for virtually any type of application or backend service.Let\u2019s get started!Please visit myGitHub Repository for DynamoDB articleson various topics being updated on constant basis.Objectives:I have divided this article into2 partsfor understanding this process better.Part 11.Create an IAM Role2.Create a lambda Function3.Write a lambda hard-code access keys to create DynamoDB tables and Items.4.View DynamoDB Table created in console.5.Write a lambda code to return the table data.Part 26.Create a Secret Manager to Store Access key and Secret Access keys7.Write a Lambda code to create DynamoDB Items by retrieving the access keys from Secrets Manager.8.View DynamoDB Table created in console.9.Write a lambda code to view the table items using a secret manager.Pre-requisites:AWS user account with admin access, not a root account.IAM roleResources Used:What is Amazon DynamoDB?What is AWS Secrets Manager?What is AWS Lambda?Steps for implementation to this project:1. Create an IAM Role123Next4Next56Create role2. Create a lambda Function123Create function4Selectconfiguration tabin lower side and then click onEdittab53. Write a lambda hard-code access keys to create DynamoDB tables and Items.12Copy the code fromfile1and replace with existing code.Note : change the AWS_Access_Key and AWS_Secret_Access_Key in file1.import json import boto3  def lambda_handler(event, context):     # Input values     Table_name = 'myTable1'     AWS_Access_Key = 'xxxxxxxxxxxxxxxxxxxx'                     AWS_Secret_Access_Key = 'xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx'      # Create a DynamoDB table     print('DynamoDB Table creation started.')      dynamodb = boto3.resource(         'dynamodb',         aws_access_key_id = AWS_Access_Key,         aws_secret_access_key = AWS_Secret_Access_Key,         region_name = 'us-east-1'     )      student_table = dynamodb.create_table(         TableName = Table_name,         KeySchema = [             {                 'KeyType': 'HASH',                 'AttributeName': 'StudId'             }         ],         AttributeDefinitions=[             {                 'AttributeName': 'StudId',                 'AttributeType': 'N'             }         ],         ProvisionedThroughput={             'ReadCapacityUnits': 2,             'WriteCapacityUnits': 2         }     )        # Wait until the Table gets created     student_table.meta.client.get_waiter('table_exists').wait(TableName = Table_name)     print('DynamoDB Table Creation Completed.')      print('Insert Student data to table started.')     # Insert 1st item into DynamoDB table     table = dynamodb.Table(Table_name)     table.put_item(     Item = {             'StudId': 100,             'FirstName': 'Rev1',             'LastName': 'Joshi1',             'Dept': 'Science',             'Age': 11         }     )        # Insert 2nd item into DynamoDB table     table.put_item(     Item = {             'StudId': 200,             'FirstName': 'Rev2',             'LastName': 'Joshi2',             'Dept': 'Science',             'Age': 22         }     )        # Insert 3rd item into DynamoDB table     table.put_item(     Item = {             'StudId': 300,             'FirstName': 'Rev3',             'LastName': 'Joshi3',             'Dept': 'Science',             'Age': 33         }     )     print('Insert Student data to table Completed.')Enter fullscreen modeExit fullscreen mode3456Click on theTestbutton to run the code.Output4. View DynamoDB Table created in console.12Select the tableand click onExplore table itemsButton in the right side5. Write a lambda code to return the table data.1Click on Functions at the left side and select the Function you created.Select theCodetab under the lambdamyFunctionCopy thefile2and replace it with the existing code.import json import boto3  def lambda_handler(event, context):     # Input values     Table_name = 'myTable1'     AWS_Access_Key = 'xxxxxxxxxxxxxxxxxxxx'     AWS_Secret_Access_Key = 'xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx'      # Create a DynamoDB table     print('DynamoDB Table creation started.')      dynamodb = boto3.resource(         'dynamodb',         aws_access_key_id = AWS_Access_Key,                   aws_secret_access_key = AWS_Secret_Access_Key,         region_name = 'us-east-1'     )      # Connect to table & Scan the entire table     table = dynamodb.Table(Table_name)     response = table.scan()      print('---------------------------------------')     print('------------STUDENT DETAILS------------')     print('---------------------------------------')     for item in response['Items']:         print('Student Id : ', item['StudId'])         print('Student Name : ', item['FirstName'], ' ', item['LastName'])         print('Student Department : ', item['Dept'])         print('Student Age : ', item['Age'])         print('_______________________________')     print('---------------------------------------')Enter fullscreen modeExit fullscreen modeDeployTestOutput"}
{"title": "Lambda@Edge: Select test origin or stick to the old", "published_at": 1710195734, "tags": ["aws", "lambda", "dynamodb", "cloudfront"], "user": "Piotr Pabis", "url": "https://dev.to/aws-builders/lambdaedge-select-test-origin-or-stick-to-the-old-563h", "details": "Originally posted on my blogLast Friday I joined the live stream of AWS Power Hour: Architecting Professional Season 2. As in every episode, we went through some example questions. I decided to take one of them and implement a working solution. The question I picked states: there's a company that uses CloudFront to host a website on two origin servers. They deployed a new app version on one of the origins and they want to route the traffic to the new app for a small percentage of users. However, users who currently use the old version, should be able to finish their activities there. The answer is: implement a new Lambda@Edge function that examines requests from users, check if the session in the cookie is new and only then route some users to the new version. Users with a session in progress should remain on their version. For reference, check outthis Power Hour episodeandhere the full scheduleif you are interested.It's possible to just keep application version in the cookie. Other possible ways could be JWT tokens with issue date (iat) and some other attributes or keeping session in ElastiCache for Redis or DynamoDB. I decided to go with the last approach, where session details - such as version - will be stored in a DynamoDB table and user will just send me a cookie with item's primary key. To decrease latency, I will use DynamoDB Global Table and pick the nearest replica in the Lambda function. The diagram below presents an overview of this solution.Also check outrepository for this articleAn example applicationFirst we need some application that will be served by CloudFront but that will also generate some cookies.  In this example we will use a simple FastAPI application combined with AWS SDK that will store sessions in DynamoDB. When the user enters the application they will be issued a session cookie for 10 minutes and with each refresh, the session will be updated in the table.First let's create a simple FastAPI application that will just return an HTML page with some CSS.config.pycontains some values for the CSS and the application version code that is critical to this task as it must be the same as used by Lambda@Edge later.# config.pyBACKGROUND_COLOR=\"#F0A5C1\"FONT_FAMILY=\"Times New Roman, Serif\"COLOR=\"#255\"VERSION_NAME=\"Version 1.0\"VERSION_CODE=\"1\"Enter fullscreen modeExit fullscreen mode# index.pyfromfastapiimportFastAPI,Cookie,Responsefromfastapi.responsesimportHTMLResponsefromtypingimportAnnotatedimportconfigapp=FastAPI()@app.get(\"/\",response_class=HTMLResponse)asyncdefroot(response:Response,sessiontoken:Annotated[str|None,Cookie()]=None):returnf\"\"\"<html>         <head>             <title>Application{config.VERSION_NAME}</title>             <style>                 body {{                     background-color:{config.BACKGROUND_COLOR};                     color:{config.COLOR};                     font-family:{config.FONT_FAMILY};                 }}             </style>             <meta http-equiv=\"Content-Type\"content=\"text/html; charset=utf-8\">         </head>         <body>             <h1>This is application{config.VERSION_NAME}</h1>             <p>You now got a cookie :)</p>         </body>     </html>\"\"\"Enter fullscreen modeExit fullscreen modeFor simplicity of development, I will create aDockerfilethat will embed our application. We will use a build argument to reuse the same one for the next version.FROMpython:3.11-alpineRUNpip3install\"uvicorn[standard]\"\"fastapi\"\"boto3\"WORKDIR/appARGVERSION=version1COPYapp-$VERSION/* /app/ENTRYPOINT[ \"uvicorn\", \"index:app\", \"--host\", \"0.0.0.0\" ]Enter fullscreen modeExit fullscreen modeYou can then build the image locally and run the application for testing. It is just serving a dummy HTML template. In the browser, navigate to localhost and the port you choose when running the container with Docker.$ docker build -t ppabis/cf-lambda-origins:version1 . $ docker run --rm -it -p 38888:8000 ppabis/cf-lambda-origins:version1Enter fullscreen modeExit fullscreen modeCreating sessions tableNow we will create a DynamoDB table that will hold sessions of our users. The table will have only three fields: token, version and expiration. The token will be the primary key, version will determine which version we want to stick to the user and expiration will be the time when the session will expire. We don't have to validate expiration on the application side for now, it is just for the sake keeping the table clean with TTL.Let's fire up the editor and configure our Terraform module. We will use AWS provider version5.40.0and Terraform1.7.4and these version will be used throughout this whole project.provider\"aws\"{region=\"us-east-1\"}terraform{required_providers{aws={source=\"hashicorp/aws\"version=\">= 5.40.0\"}}}Enter fullscreen modeExit fullscreen modeNext we will define ourSessionsTabletable. As previously mentioned, we will enable TTL cleanup (that will remove expired sessions periodically) and set the primary key totoken. We will also make it a global table by addingreplicas- one foreuregion and one forapregion. The original table will reside inus-east-1region. We cannot createversionandexpirationfields yet, as the name suggests, DynamoDB is dynamic. We can in theory use one of those fields as a sort key or define indexes. However, let's just assume that in caseversionis not present, we will default to the first version. We will also usePAY_PER_REQUESTbilling mode so that we don't have to deal with autoscaling, due to using a global table.resource\"aws_dynamodb_table\"\"sessions\"{name=\"SessionsTable\"billing_mode=\"PAY_PER_REQUEST\"hash_key=\"token\"attribute{name=\"token\"type=\"S\"}ttl{attribute_name=\"expiration\"enabled=true}replica{region_name=\"ap-southeast-1\"}replica{region_name=\"eu-west-1\"}}Enter fullscreen modeExit fullscreen modeLet's init Terraform and apply the infrastructure. We continue still with the first version of our application.Adding sessions to the applicationIn theroot()function there are already some hints how the session cookie will look like. We just need to implement the logic that will generate and store the session. We will use AWS SDK for Python,boto3, to store the session in the newly created DynamoDB table. We will generate random token with that will be the primary key. Ifupdate_itemwon't fail, we will set the cookie in the response to user so that their browser can store it. If the cookie was given previously to us with the request, we will just update the expiration time and the version to the one assigned by the current origin processing the request.fromboto3importclientfromrandomimportrandbytesfromhashlibimportsha1fromdatetimeimportdatetimeimportosifos.environ.get(\"AWS_REGION\")isNone:dynamo=client(\"dynamodb\",region_name=\"us-east-1\")else:dynamo=client(\"dynamodb\")defupdate_session(response:Response,sessiontoken:Annotated[str|None,Cookie()]=None):# If there is no session token we pick a random oneifsessiontokenisNone:sessiontoken=sha1(randbytes(128)).hexdigest()# And we update the session token in the database. Update in DynamoDB works also for new items.dynamo.update_item(TableName=\"SessionsTable\",Key={\"sessiontoken\":{\"S\":sessiontoken}},UpdateExpression=\"SET expires = :expires, version = :version\",ExpressionAttributeValues={\":expires\":{\"N\":str(int(datetime.now().timestamp()+600))},\":version\":{\"S\":config.VERSION_CODE}},)# We give the user either a new cookie or the same one as beforeresponse.set_cookie(key=\"sessiontoken\",value=sessiontoken)Enter fullscreen modeExit fullscreen modeNow we only have to edit therootfunction to call theupdate_sessionfunction withtryandexceptblocks to handle the case when DynamoDB throws an error such as no permissions.@app.get(\"/\",response_class=HTMLResponse)asyncdefroot(response:Response,sessiontoken:Annotated[str|None,Cookie()]=None):try:update_session(response,sessiontoken)exceptExceptionase:print(e)response.status_code=500returnf\"\"\"<html><body><h1>ERROR 500</h1><p>We expected unexpected!</p></body></html>\"\"\"returnf\"\"\"<html>...Enter fullscreen modeExit fullscreen modeIf we run the application on our local machine with Docker, we should get this 500 error unless we specified some credentials for AWS. Let's try.docker build-tppabis/cf-lambda-origins:latest.docker run--rm-it-p38888:8000 ppabis/cf-lambda-origins:latestEnter fullscreen modeExit fullscreen modeWe can also verify that we are not receiving any cookies in case of an error. This will ensure that our application doesn't cause any unwanted behavior and errors are handled correctly (assuming 5xx is a correct way to handle it \ud83d\ude01).$curl-vhttp://localhost:38888>GET / HTTP/1.1>< HTTP/1.1 500 Internal Server Error <date: Sat, 09 Mar 2024 10:38:12 GMT < server: uvicorn < content-length: 74 < content-type: text/html;charset=utf-8 <  <html><body><h1>ERROR 500</h1><p>We expected unexpected!</p></body></html>Enter fullscreen modeExit fullscreen modeHosting the applicationLet's spin up some EC2 instance with Docker and host the application there. I will uset4g.nanowith Amazon Linux 2023. During the boot, user data will install Docker for us. We will also define an IAM role that will allow the instance to be accessed via SSM and read/write to DynamoDB table of our choice. We will also open some HTTP port (such as8080) in the security group so that we can test our application.For simplicity, I will not type the whole code for this instance here. Refer to theGitHub repositoryfor complete solution.data\"aws_ssm_parameter\"\"AL2023\"{name=\"/aws/service/ami-amazon-linux-latest/al2023-ami-kernel-6.1-arm64\"}resource\"aws_instance\"\"AppInstance\"{ami=data.aws_ssm_parameter.AL2023.valueinstance_type=\"t4g.nano\"iam_instance_profile=aws_iam_instance_profile.AppEC2Role.namevpc_security_group_ids=[aws_security_group.AppEC2.id]user_data=<<-EOF#!/bin/bash   yum install -y docker   systemctl enable --now docker   usermod -aG docker ec2-userEOFtags={Name=\"AppVersion1\"}}Enter fullscreen modeExit fullscreen modeAfterinitandapply, you should find it in AWS console on the list of instances. Right-click on it and selectConnect. I will be connecting with Session Manager but feel free to use your own method. After you log in rundocker versionto verify that Docker was installed correctly. Now we have to transfer our application to the instance. You can usescpif you SSH'd into it but there are many ways you can deal with it: S3,vi, SSM parameters, etc. I am old-school nerd and will usecatwith<<EOF\ud83e\udd13. With this method you have to remember to escape$with\\$.$sudosu ec2-user$cd$cat<<EOF> index.py from fastapi import FastAPI, Cookie, Response # the file continues...EOF$cat<<EOF> Dockerfile FROM python:3.11-alpine # the file continues... COPY app-\\$VERSION/* /app/EOF$cat<<EOF> config.py ...EOF$mkdir-papp-version1&&mvindex.py app-version1/&&mvconfig.py app-version1/Enter fullscreen modeExit fullscreen modeNow it's time to build and run the application. If you use Session Manager, switch toec2-userwithsu. We will run the application exposed on port which we set up in our security group to be accessible, in my case it is8080.$docker build-tppabis/cf-lambda-origins:latest.$docker run--rm-it-p8080:8000 ppabis/cf-lambda-origins:latestEnter fullscreen modeExit fullscreen modeNow, don't disconnect from your SSH/SSM session. Open AWS console with EC2 instances in a new tab, and find the public IP. Go to the websitehttp://127.255.255.0:8080(replace with IP of your instance) and you should see the page. Open developer console (in Safari it isOption+Command+I) and go toNetworktab. It should show you that you got a cookie in the response. Refresh and it you should send the cookie with the request and get the same in response.Remember first characters of the cookie. Go to DynamoDB in AWS console (N. Virginia/us-east-1) and find theSessionsTabletable. ClickExplore table itemsand look if you have the same cookie stored. If you see it, that means we are successful in this part of the task. Switch to some other region (such as Ireland/eu-west-1) and look if the table is also there. Explore the items. It should be replicated.Now hitCtrl+Cin the terminal where you run the application. Run the Docker container in the background with-dflag. We will not remove the container and name it appropriately so that we can read its logs in case of issues. Also we set the policy torestart alwaysso that the container will start after the failure.docker run-d--name\"app_v1\"-p8080:8000--restartalways ppabis/cf-lambda-origins:latestEnter fullscreen modeExit fullscreen modeNow you can disconnect from this instance. We will continue the next part with updated application version hosted on another instance.New application versionGo back to the Terraform file defining the instance. Addcount = 2so that two instances are created in total. Also update the tags to reflect this change. This instance will be clone of the first one but we will deploy a different app version there.resource\"aws_instance\"\"AppInstance\"{ami=data.aws_ssm_parameter.AL2023.value...tags={Name=\"AppVersion${count.index+1}\"}}Enter fullscreen modeExit fullscreen modeApply this change. Terraformshouldonly create one instance and move the current one into index0but if it doesn't happen, you need to repeat previous steps or import the instance withterraform importbefore hittingapply.Let's change the app for the second version. Copy over both files fromapp-version1toapp-version2directory. We will only modify many parameters inconfig.pyto distinguish between the two applications.BACKGROUND_COLOR=\"#08122C\"FONT_FAMILY=\"Calibri, Helvetica, Sans-Serif\"COLOR=\"#E8C2C8\"VERSION_NAME=\"Version 2.0\"VERSION_CODE=\"2\"Enter fullscreen modeExit fullscreen modeCopy over the files to the instance using your preferred method. I will usecatagain. Next build the Docker image with extra argument that will specify the new version. Execute the container in the background the same was as before.$docker build-tppabis/cf-lambda-origins:2.0--build-argVERSION=version2.$docker run-d--name\"app_v2\"-p8080:8000--restartalways ppabis/cf-lambda-origins:2.0Enter fullscreen modeExit fullscreen modeGo to AWS console. Get the public IP of the new instance. Go to the address in your browser. You should get a newer version of the app. Let's also explore theSessionsTablein DynamoDB to check if the new sessions are created with correct version number.Putting application behind CloudFrontWe have currently two versions of the application. In the question we were told that in the current state, only the first version of the application is served by CloudFront. So lets create a CloudFront distribution that will have just one origin for now, we will figure out the rest later.We will need the DNS name of the EC2 instance to specify as CloudFront origin. There are many ways to get it, not necessarily copy and paste from the console \ud83d\ude09. If we composed previous Terraform directories as modules into one project we could use outputs and variables. But I will use thedatasource in this example.data\"aws_instance\"\"AppInstanceV1\"{instance_tags={Name=\"AppVersion1\"}}resource\"aws_cloudfront_distribution\"\"AppDistribution\"{default_cache_behavior{allowed_methods=[\"GET\",\"HEAD\",\"OPTIONS\"]cached_methods=[\"GET\",\"HEAD\"]target_origin_id=\"AppVersion1\"viewer_protocol_policy=\"redirect-to-https\"forwarded_values{query_string=truecookies{forward=\"all\"}}}viewer_certificate{cloudfront_default_certificate=trueminimum_protocol_version=\"TLSv1.2_2019\"}restrictions{geo_restriction{restriction_type=\"none\"}}enabled=trueorigin{custom_origin_config{http_port=8080https_port=443origin_protocol_policy=\"http-only\"origin_ssl_protocols=[\"TLSv1.2\"]}domain_name=data.aws_instance.AppInstanceV1.public_dnsorigin_id=\"AppVersion1\"}}output\"CloudFrontDNS\"{value=aws_cloudfront_distribution.AppDistribution.domain_name}Enter fullscreen modeExit fullscreen modeApply might take time. The distribution above will have the following behavior:It will acceptGET,HEADandOPTIONSrequests,Not cache anything, forward everything to the origin,Redirect to HTTPS,Use default SSL certificate provided bycloudfront.net,Forward all traffic to our custom origin at port8080.After CloudFront distribution is created, you can get the domain name for it from the AWS console or with Terraform outputs. Go to the address in your browser. You should be connected to the instance via HTTPS (or to CloudFront at least).Creating Lambda@Edge functionNow it's time to finally add some routing logic to our solution. We will create a Lambda function inus-east-1region in Python. The function will be triggered by CloudFront on the origin request event. It will contact the nearest DynamoDB Table based on current execution region (Lambda@Edge is distributed into multiple locations) and check if the user has a valid cookie that points to a valid application version. The function will then change origin if needed. It would be nice to store the second origin in environment variables but it's not possible forLambda@Edge. For simplicity let's just hardcode the values.For convenience of development we will includehashicorp/archiveprovider to create ZIP files. Add this to yourmain.tffile or any other file you use for storing the providers. Runterraform initafterwards.terraform{required_providers{...archive={source=\"hashicorp/archive\"version=\">= 2.0.0\"}}}Enter fullscreen modeExit fullscreen modeLet's start with the Lambda function code in Python. I will use the draft fromAWS docs. The function we will define will first select which DynamoDB to use based on its execution region. We defined replicas only foreu-west-1andap-southeast-1so for all other continents we will just default tous-east-1.importboto3,os# To reduce latency, we will pick one of the tables based on the regionaws_region=os.environ.get(\"AWS_REGION\")aws_region=\"us-east-1\"ifaws_regionisNoneelseaws_regionifaws_region.startswith(\"eu-\"):aws_region=\"eu-west-1\"elifaws_region.startswith(\"ap-\"):aws_region=\"ap-southeast-1\"dynamo=boto3.client(\"dynamodb\",region_name=aws_region)Enter fullscreen modeExit fullscreen modeNext we will create two functions: one for determining the version based on the session token. It will contact DynamoDB to get the details of the session. If session doesn't contain version or there's no session cookie, we will just select random origin with 50/50 chance. Provide the second origin's DNS address in your the function.importrandomsecond_origin=\"ec2-127-128-20-164.compute-1.amazonaws.com\"# Get DNS address of your own secondary instancedefpick_random(request):# If we rolled heads, change the originifrandom.choice([0,1])==0:request['origin']['custom']['domainName']=second_origin# Else just return the request unchangedreturnrequestdefselect_version(token,request):item=dynamo.get_item(TableName=\"SessionsTable\",Key={\"token\":{\"S\":token}})# If this token is found in the table and this session has a version, we stick to itif'Item'initemand'version'initem['Item']:ifitem['Item']['version']['S']==\"2\":request['origin']['custom']['domainName']=second_origin# Session might be invalid so roll any origin 50/50else:request=pick_random(request)returnrequestEnter fullscreen modeExit fullscreen modeThe last part is to define the handler that will process CloudFront request. Here we need to extract cookies from the request's header, look for the session token cookie (if it exists) and call the appropriate function.defhandler(event,context):token=Nonerequest=event['Records'][0]['cf']['request']headers=request['headers']# Check if there is any cookie headercookie=headers.get('cookie',[])ifcookie:forcincookie:# Check if there is a sessiontokenif'sessiontoken'inc['value']:token=c['value'].split('=')[1]breakiftoken:request=select_version(token,request)else:# As with invalid session case, we roll any origin 50/50request=pick_random(request)returnrequestEnter fullscreen modeExit fullscreen modeNow we will define IAM policies for the function. We will need the Lambda to be executable by CloudFront and to be able to read from DynamoDB. The role must be assumable by bothlambda.amazonaws.comandedgelambda.amazonaws.com. We will just give it permissions to read from the table.resource\"aws_iam_role\"\"OriginSelectionLambdaRole\"{name=\"OriginSelectionLambdaRole\"assume_role_policy=jsonencode({Version=\"2012-10-17\",Statement=[{Action=\"sts:AssumeRole\",Effect=\"Allow\",Principal={Service=[\"lambda.amazonaws.com\",\"edgelambda.amazonaws.com\"]}}]})}data\"aws_iam_policy_document\"\"DynamoDBSessionsTable\"{statement{actions=[\"dynamodb:GetItem\"]resources=[\"arn:aws:dynamodb:*:*:table/SessionsTable\"]}}resource\"aws_iam_role_policy\"\"DynamoDBRead\"{name=\"DynamoDBRead\"role=aws_iam_role.OriginSelectionLambdaRole.idpolicy=data.aws_iam_policy_document.DynamoDBSessionsTable.json}Enter fullscreen modeExit fullscreen modeAs stated before, we can utilize Hashicorp'sarchiveprovider to create a ZIP file that is needed for deploying the Lambda function. It also handles generation of the hash for the file in required format. We will also usepublish = trueso that applying the infrastructure will generate actual version of the function as we cannot use$LATESTfor CloudFront.data\"archive_file\"\"lambda\"{type=\"zip\"source_file=\"${path.module}/origin_selector.py\"output_path=\"${path.module}/origin_selector.zip\"}resource\"aws_lambda_function\"\"OriginSelector\"{filename=data.archive_file.lambda.output_pathfunction_name=\"OriginSelector\"role=aws_iam_role.OriginSelectionLambdaRole.arnhandler=\"origin_selector.handler\"runtime=\"python3.11\"source_code_hash=data.archive_file.lambda.output_base64sha256publish=true}Enter fullscreen modeExit fullscreen modeBefore we attach the function to CloudFront, let's test it in AWS console. Go to Lambda service, enter the function and selectTesttab. Create a new test event. Use JSON fromthis page. You can modify request headers. I added thesessiontokencookie as the second cookie to test the function completely. You can also remove it to see if it keeps the origin the same as in the test event.//...],\"cookie\":[{\"key\":\"Cookie\",\"value\":\"tracking=123456\"},{\"key\":\"Cookie\",\"value\":\"sessiontoken=472ed977dc2fd2700dbb03a58634f789fd58d911\"}],\"user-agent\":[//...Enter fullscreen modeExit fullscreen modeI inserted actual working token from the DynamoDB table with version 2 to see if the origin changes fromexample.orgto the one specified in the function.Everything seems to be working. Now we can attach the function to CloudFront. Go back to its resource, to the default cache behavior block. Add the Lambda function for origin request. Also pick the appropriate version in the ARN. If you deployed the working function successfully the first time, it will be1. Otherwise browseVersionstab in the Lambda function in AWS console and choose the latest one.resource\"aws_cloudfront_distribution\"\"AppDistribution\"{default_cache_behavior{lambda_function_association{event_type=\"origin-request\"lambda_arn=\"${data.aws_lambda_function.origin_request.arn}:1\"include_body=false}...}Enter fullscreen modeExit fullscreen modeAfter applying the change, you can try opening and closing a private window in the browser and navigating to the page multiple times or clearing the cookies. You can also use thiscurlalmost-one-liner. It should show you how the versions of the app switch with each request without a cookie - so choosing a random origin.foriin{1..20};docurl-shttps://$(terraform output-rawCloudFrontDNS)|grep\"Version\"sleep3doneEnter fullscreen modeExit fullscreen modeOnce you visit the website with a browser, you should stay on the same page no matter how many times you refresh it, even with cache disabled.Is the table really global?Observe activity in DynamoDB. The apps use onlyus-east-1table. However, I am based in Europe so I expect Lambda@Edge to useeu-west-1table. You can try reaching via VPN or make SSH tunnel via AWS to change region too. The charts below show  usage for bothus-east-1andeu-west-1tables."}
{"title": "Amazon Textracts Overview", "published_at": 1710189900, "tags": ["aws", "cloud", "ai", "machinelearning"], "user": "\u0639\u0628\u062f\u0627\u0644\u0644\u0647 \u0639\u064a\u0627\u062f | Abdullah Ayad", "url": "https://dev.to/aws-builders/amazon-textracts-overview-2713", "details": "Amazon Textract is used toextract texts, so, hence the name.So you extract text, handwriting, or data from any scanned document and behind the scenes, of course, usesAIormachine learning.So we have, for example, a driver license, and then we upload it into Amazon Textract, and then, automatically, will be analyzed, and the results will be given to you as a data file, and so you'll be able to, for example, extract thedate of birth,document ID, and so on.So you can extract any data, even from forms and tables, and you can read PDFs, images, and so on.The use cases for extracting texts are multiple, but you could be for financial services to processinvoicesorfinancial reports, could be forhealthcare, formedical records, andinsurance claims, or for the public sector, for example, fortax forms,ID documents, andpassports.GitHubLinkedInFacebookMedium"}
{"title": "AWS SnapStart - Part 17 Impact of the snapshot tiered cache on the cold starts with Java 21", "published_at": 1710175407, "tags": ["java", "aws", "serverless", "coldstart"], "user": "Vadym Kazulkin", "url": "https://dev.to/aws-builders/aws-snapstart-part-17-impact-of-the-snapshot-tiered-cache-on-the-cold-starts-with-java-21-52ef", "details": "IntroductionIn the course of this blog series, we measured cold starts with very different scenarios mainly with SnapStart enabled. Now let's explore one more SnapStart detail which is called \"tiered caching\" of the microVM snapshot. This procedure was briefly mentioned in the article whereSnapStart was announcedHere is the sentence : With SnapStart, when a customer publishes a function version, the Lambda service initializes the function\u2019s code. It takes an encrypted snapshot of the initialized execution environment,and persists the snapshot in atiered cachefor low latency access.  So what this tiered cache might be?Tiered cache of the snapshotIn our experiment we'll re-use the application introduced inpart 9. Let's take GetProductByIdWithPureJava21Lambda function with SnapStart-enabled (but without priming) and 1024 MB memory and measure the cold start for exactly 1 invocation. Let's assume it's a very first execution after the newer version of the Lambda function has been published. My result was 2270.28 ms. The cold start is still big enough. Without SnapStart enabled the result might be in the range of 3100 and 3600 ms.But what will happen with the subsequent cold start times with SnapStart enabled? Let's see how percentiles change for increasing number of the cold starts.data and timenumber of cold startsp50p75p90p99p99.9max8.3. 18:1512270.282270.282270.282270.282270.282270.288.3. 18:2642078.542196.682270.282270.282270.282270.288.3. 18:3892131.582210.692340.342340.342340.342340.348.3. 18:51141880.212131.582270.282340.342340.342340.348.3. 19:05201792.052015.112196.682340.342340.342340.348.3. 19:20341706.081856.042131.582340.342340.342340.348.3. 19:32491662.71792.052168.882340.342340.342340.348.3. 19:44661642.871709.272078.542340.342340.342340.348.3. 19:44761640.131703.172064.592340.342340.342340.348.3. 20:10851640.131700.92015.112340.342340.342340.348.3. 20:20981642.741703.171880.212340.342340.342340.348.3. 20:301091639.751691.351865.412269.102338.172340.348.3. 20:411201633.211679.561854.252269.102338.172340.348.3. 20:521291629.951676.211854.252269.102338.172340.34So, what we observe is that the cold start times reduce the more cold start we experience. After 50 cold starts the effect becomes less and less visible for p50 and after 100 cold starts for p90. This is the effect of the tiered cache for the mivcorVM snapshot in action. The effect of the tiered cache dependings on percentile and is significant (up to 600ms).If you are interested in the deep details about how Lambda SnapStart (which is currently only available for Java runtime) is implemented and particularly microVM (the whole execution environment) snapshot and its tiered caching works under the hood, I recommend you the talkAWS Lambda Under the Hoodby Mike Danilov. There is also a detailed summary of his talkhereand additional resourceshere.Of course the question was what will happen if we don't invoke my Lambda function for a while and than execute it later? Will the cold start increase. Let's check.Let's then stop Lambda function execution and then invoke it 30 min later at 21.22 - cold start was 1674.06, then stop again and invoke it at 21:52 - cold start was 1702.17 then the same at 23:00 cold start was 1735.06. So, it got slightly bigger, but we don't observe the worst values from the first executions. Then I stopped Lambda execution for 8 hours and executed it next morning then 15.000 times running into 16 cold start with p50 being 1669.07 and from p90 on 2019.88. So tiered caching effect was still there after so many hours and the p90 and higher numbers didn't look that big as during the first invocations.To complete the test of snapshot tiered caching I also did the same experiments on GetProductByIdWithPureJava21LambdaAndPriming which uses SnapStart and DynamoDB request invocation priming on top. Let's summarize them in the table below.data and timenumber of cold startsp50p75p90p99p99.9max8.3. 18:1511189.551189.551189.551189.551189.551189.558.3. 18:2641046.091166.551189.551189.551189.551189.558.3. 18:389801.741046.091189.551189.551189.551189.558.3. 18:5114763.37808.961166.551189.551189.551189.558.3. 19:0523730.28801.741046.091189.551189.551189.558.3. 19:2032720.01796.2941.291189.551189.551189.558.3. 19:3247700758.39903.361189.551189.551189.558.3. 19:4458692.52749.01831.721189.551189.551189.558.3. 19:4468684748.61831.721189.551189.551189.558.3. 20:1080679.44731.52801.741189.551189.551189.558.3. 20:2091688.25748.61799.251189.551189.551189.558.3. 20:30100689.34748.22799.241166.161188.521189.558.3. 20:41110679.76744.49799.241166.161188.521189.558.3. 20:52122679.08744.49799.241166.161188.521188.52So, what we observe is the same as without priming. The cold start time reduces with more cold starts we experience. After 50 cold starts the effect becomes less and less visible and after 80 cold starts the effect becomes negligible for p50 and after 90 cold starts for p90. The effect of the tiered cache dependings on percentile and is significant (up to 500ms).Of course I had the same question was what will happen if I don't invoke my Lambda function for a while and than execute it later?  Let's check.Let's stop the Lambda function execution and then invoke it 30 min later at 21.22 - cold start was 746.63, then stop again and invoke it at 21:52 - cold start was 617.7 then the same at 23:00 cold start was 673.5. Then I stopped Lambda execution for 8 hours and executed it next morning then 15.000 times running into 17 cold start times with p50 being 723.99 and p90 being 894.05. So tiered caching effect was still there as well after so many hours and the p90 and higher numbers didn't look that big as in the first invocations.ConclusionIn this article we saw microVM snapshot tiered cache in action for SnapStart-enabled Lambda function (with and without priming) with Java 21 runtime. The conclusion is quite obvious: don't stop by enabling SnapStart and measuring only one cold start time or a couple of them. Yes, the first cold starts take longer, but it gets better with the number of invocations for the same Lambda function version and seems to stay on a good level independent whether you invoked your Lambda for a while or not. I assume exactly the same or very similar effect to be with Java 17 as it's not about the Java version itself but about technical implementation of the microVM snapshot tiered cache done by AWS."}
{"title": "The What, Who, Why, Where and How of Public Speaking", "published_at": 1710164019, "tags": ["publicspeaking", "aws"], "user": "Julia Furst Morgado", "url": "https://dev.to/aws-builders/the-what-who-why-where-and-how-of-public-speaking-3lhm", "details": "Alright alright, are you thinking about stepping into the world of public speaking? You've come to the right place. Let me tell you a little bit about what public speaking entails and what to talk about, who it's for, why you should start it right away, where to give your first talk, and how you can get started. Meanwhile, I'll show you an awesome AI app created on PartyRock, an Amazon Bedrock Playground that can help you streamline this whole process -Talk Maker!WHATSo what is public speaking exactly? At its core, it's communicating a message or sharing knowledge verbally to an audience. This can cover a wide range of topics from discussions about your personal experiences to teaching others about a complex subject area. The possibilities are endless!Here are a few examples of what you can talk about:What you already knowWhat you learned from other sources (blog, interviews etc)A project you worked onIndustry trendsImportant eventsChallenges you've facedWHOAs for who public speaking is for - everyone! Seriously, you don't need to have a certain job, degree, or background to try your hand at it. Public speaking is a skill that takes practice but can be learned by anyone willing to challenge themselves a bit outside their comfort zone. Stepping up to share your ideas and perspectives is within reach for all. And I recommend it to everyone, the benefits are numerous!WHYNow for the big why - why take on public speaking? A few great reasons to consider it. For one, it gives you visibility and credibility as an expert in your field. Anytime you're up speaking before an audience, you cement yourself as a leader. It's also a fantastic way to network and meet others. Plus, public speaking does aid your learning and growth because by teaching others we teach ourselves. Preparing talks forces you to organize your knowledge, and getting hands-on experience speaking only sharpens your communication skills. It's a personal challenge well worth undertaking.WHEREOkay, where can you speak? Lots of options - both online and off. Consider doing a talk for your company's internal meetings, at local meetups likeAWS User Groups,Cloud Native Community Groupsor submit a proposal to a conference. There are so many conferences out there. I recommend you take a look at the AWS Community Day or KCD near you and submit to speak there. Otherwise, you can check thisdevelopers conferences agendarepo with tons of other conferences you can apply to. And in the digital age, there are options like creating videos for YouTube or podcasts too where you can practice your public speaking skills. The venues are endless.HOWHere you have two avenues to start with:Avenue #1: Start with the what in mind. First, decide what you want to discuss (can be a high-level topic) - whether that's based on your personal experiences, your work, trends in your field, and so on. Make sure it's a topic you're passionate and knowledgeable about.Avenue #2: Start with the where in mind. Depending on the event, for instance, a security event you will need to give a talk related to security. If the event doesn't have a specific topic or has several talk tracks you can go with the first avenue.Once you've decided you want to give a talk and have a high-level topic, you need to define your audience and the outcomes of your talk. Understanding who you want to talk to is crucial. What are their interests, pain points, and knowledge levels? Tailor your content and language accordingly. For example, a talk for senior leadership would require more of an executive summary than granular details. The outcome refers to what you want your audience to learn, think, feel, or do after they've heard your message. What action do you want them to take? Be specific.From there you need to hone in on a specific subject so you can start brainstorming on a catchy title that draws people in, and an abstract about your talk and break the talk into 2-3 main actionable points or takeaways.Here's a tip: if you're planning to deliver a technical talk, try to present a demo even if it's just for a couple of minutes or a few screenshots. Demos can aid audience understanding and cement your message because they reinforce concepts in a visual, interactive way versus passive viewing. Sticks better in minds! If it's something complex, a short demo plus sharing your slides/notes afterward is very helpful.Now it's time to prepare your visual presentation. Once you have all your main elements in hand (title, abstract, demo and 2-3 core points) start building out your slides. Keep it simple with a few lines of text and clear - people shouldn't have to strain to read tiny text. Include relevant anecdotes, stories, data, or images to reinforce your messages. Remember to never use images, content or code in your presentation without proper permission!Now you're almost ready! Polish your presentation and rehearse it aloud for others. Practice in front of friends, family, or co-workers. Ask for honest feedback on how you can improve your delivery, if the content flows well, and thoughts on visuals. Incorporate any suggestions before showtime. You can also record practice runs to review later. Repetition is key to feeling comfortable when presenting.On the day, relax and remember that people are there to learn from you! Make frequent eye contact with the audience, change up the tone and pace of your speech, sprinkle in some humor, and use visible energy and body language. Speak loudly and clearly into the mic.Once you're done with the talk, you'll want to leave some time for Q&A (questions and answers). Anticipate what types of questions may come up based on your topic. Feel free to say you don't know an answer, but offer to follow up. Thank your attendees for their great questions!After the talk, take some notes on what went well, where you could improve, and any additional feedback received. Save this for the next time you present - you'll get better and better! Just keep practicing and continuing to challenge yourself.Finally and most importantly, have fun with the whole process! Public speaking gets easier the more you do it. After a few talks, you're going to love doing them!If you want to try the PartyRock app, you can do so here:https://partyrock.aws/u/juliafmorgado1/oGddt5yV8/Talk-MakerOr watch a short demo on how to use it here:https://www.youtube.com/watch?v=APjp8gs8Ag8If you liked this article, follow me onTwitter(where I share my tech journey daily), connect with me onLinkedIn, check out myIG, and make sure to subscribe to myYoutubechannel for more amazing content!!"}
{"title": "A Priority Based Scheduler for Amazon SageMaker Training Jobs", "published_at": 1710147974, "tags": ["sagemaker", "aws", "deeplearning", "mlop"], "user": "Chaim Rand", "url": "https://dev.to/aws-builders/a-priority-based-scheduler-for-amazon-sagemaker-training-jobs-30f0", "details": "Optimizing the use of limited AI training accelerators \u2014 Part 2Photo byAdrien AlettionUnsplashThis post was created in collaboration withMax Rabin.This is the second part of a series of posts on the topic of maximizing the utility of scarce AI resources. In thefirst postwe noted the increasing limitations on the ability to scale up AI resources at will and, as a consequence, the growing trend of AI development teams to guarantee AI compute capacity by means such as building up an in-house AI server farm and/or reserving dedicated instances in the cloud. The scarcity of AI compute resources motivates the design of specialized scheduling solutions to minimize idle time and prioritize critical workloads. Please see ourprevious postin which we proposed a detailed list of requirements for such solutions. The approach we took there was to leverage the existing priority-basedschedulerthat comes withKubernetesand align our training development workflow to its use. In this post we explore the option of maintaining our existing framework for training AI models and enhancing it with our own custom implementation of a priority-based scheduler. Importantly, the need for this type of solution is often motivated not just by the scarcity of AI resources, but also by the desire to increase control over the orchestration and prioritization of training workloads so as to reduce development costs. For example, even in a scenario of abundant capacity, you may choose to limit your use to a fixed number of training instances so as to cap your training expenditure.For the purposes of this post, we will assume that our training framework of choice is AWS\u2019s managed service for AI model training,Amazon SageMaker. The solution we will propose will use additional AWS services such asAmazon DynamoDBandAWS Lambda. The choice to demonstrate our solution using AWS services should not be viewed as endorsement. There are many cloud-based service offerings available and the best one for you will depend on the particular details of your project. Similar solutions to the one that we will describe can be designed on other cloud-based environments and/or using alternative cloud-based services.The Traditional Method for Starting Up SageMaker Training JobsTraditionally, we would start up a SageMaker training job using theAmazon SageMaker Python SDK. In the code block below we use the SageMaker SDK (version 2.208) to run a PyTorch training workload on a single instance of typep5.48xlarge.from sagemaker.pytorch import PyTorch    # define job   estimator = PyTorch(       role='<sagemaker role>',       entry_point='train.py',       instance_type='ml.p5.48xlarge',       instance_count=1,       framework_version='2.0.1',       py_version='py310',       tags=[{'Key': 'priority', 'Value': '100'}   )    # start job   estimator.fit()Enter fullscreen modeExit fullscreen modeWhen theestimator.fit()function is called, the SageMaker library uploads our code to Amazon S3 and then transforms the request to a boto3 SageMaker clientcreate_training_jobrequest (seehere).This method for starting up training jobs is dependent on the availability of the requested resources for its success. In our scenario of scarce AI resources, it is likely to fail more often than not. Although this can be partially mitigated byretaining provisioned compute instances for successive workloads, the API doesnotprovide the appropriate tooling for maximizing their utility. Let\u2019s suppose that we wish to utilize precisely twop5.48xlargeinstances. To simplify our discussion, let\u2019s assume that each training workload runs on a single instance. Typically, during an AI model development cycle there will be periods when there are more than two training workloads that are waiting to be processed. The existing API would try to start up a thirdp5.48xlargeinstance and would most likely fail due to its limited availability. Even when there is instance availability, we may wish to limit our training to just our two designated instances to increase our control over the costs of training.We require a new API for submitting jobs for training, one that does not immediately start up a newp5.48xlargeinstance, but rather enters the jobs to a priority queue. And we need an associated job scheduler that manages the use of our two resources while prioritizing critical workloads.Importantly, please note that as of the time of this writing, Amazon SageMaker doesnotsupport the option of training onreserved Amazon EC2 instances. And althoughAmazon SageMaker Savings Planshas similar properties to instance reservations, it doesnotguarantee instance capacity. In aprevious postwe addressed this limitation and proposed usingSageMaker managed warm poolsas an alternative method for retaining access to provisioned instances. For the remainder of the post, we will assume that we are able to attain two instances of our choice whether it be through this or some other method.Priority-Based Scheduling for Amazon SageMakerIn this section we will describe the components of our proposed solution. We will use theAWS Serverless Application Model (SAM) specification. More specifically, we will create anAWS SAM template YAML fileand gradually add the AWS resources that we need. Please see thedocumentationfor details on how to define and deploy serverless solutions using AWS SAM.AWS Architecture Diagram (by Author)A Private API for Submitting Training JobsWe start by usingAmazon API Gatewayto define aprivate REST APIfor submitting training job requests. We name the APItraining-job-queue. Later, we will add a POST method calledadd-joband modify our training-job creation code to use this method instead of the SageMaker clientcreate_training_jobAPI. The code block below contains the definition of the privateAPI resourcein SAM. In practice you will likely want to specify access limitations to the API and/or a method of authorization.AWSTemplateFormatVersion: '2010-09-09'   Transform: AWS::Serverless-2016-10-31    Resources:     InternalAPI:       Type: AWS::Serverless::Api         # Auth: # Add access control to API         EndpointConfiguration:           Type: PRIVATE           # VPCEndpointIds: # Specify VPC Endpoint(s)         Name: training-job-queue         StageName: prodEnter fullscreen modeExit fullscreen modeDefine an AWS DynamoDB Table for Storing Training Job RequestsWe will use anAmazon DynamoDBtable namedsagemaker-queueto store the submitted training workloads. Each entry will have the following fields:jobName: Stores the unique name of the training job.entryTime: Stores the date and time that the job was added.jobState: Stores the current state of the training job. The valid values are \u2018pending\u2019, \u2018running\u2019, and \u2018preempted\u2019.priority: Stores an integer value representing the relative priority of the job.jobDetails: Stores the details of the job request.We define our DynamoDB table in our SAM template YAML file using theAWS::Serverless::SimpleTableresource.DynamoSMQueue:       Type: AWS::Serverless::SimpleTable       Properties:         PrimaryKey:           Name: jobName           Type: String         TableName: sagemaker-queueEnter fullscreen modeExit fullscreen modeWe define a function that creates a table entry from a given training job request. We assume that request contains the same contents as the input to thecreate_training_jobAPI in JSON format. We further assume that thepriorityof the workload is entered as a key-valuetagin the training job definition.import json, boto3, datetime    dynamodb = boto3.resource('dynamodb')   table = dynamodb.Table('sagemaker-queue')    def add_job_entry(job_json):       job_details = json.loads(job_json)        # extract job_name       job_name = job_details['TrainingJobName']       print(f'add entry {job_name}')        # get current time       entry_time = datetime.now().strftime(\"%Y-%m-%dT%H:%M:%S\")        # default priority is 0       priority = 0        # update priority based on tags       tags = job_details['Tags']       for tag in tags:           if tag['Key'] == 'priority':               priority = int(tag['Value'])               break        # create entry       entry = {          'jobName': job_name,          'entryTime': entry_time,          'jobState': 'pending',          'priority': priority,          'jobDetails': job_json       }       table.put_item(Item=entry) #TODO handle errors       print(f'Added job {job_name} to queue')Enter fullscreen modeExit fullscreen modeThe REST APIadd-jobmethod that we will soon define will be programmed to call theadd_job_entryfunction.We define a second function that extracts the pending jobs from the database and returns them in order of priority. In the case that multiple jobs have the same priority, they are ordered according to the amount of time they have been waiting in the queue.from boto3.dynamodb.conditions import Attr    # Get a list of all pending jobs sorted by priority   def get_pending_jobs():       response = table.scan(           ProjectionExpression='jobName, priority, entryTime',           FilterExpression=Attr('jobState').ne('running')       )       jobs = response.get('Items', [])        # sort jobs, first by priority (descending) and then by entryTime       sorted_jobs = sorted(jobs,                            key=lambda x: (-x['priority'], x['entryTime']))        return sorted_jobsEnter fullscreen modeExit fullscreen modeThe following utility functions will come in handy in the next sections.# Get a jobName -> priority mapping of all running jobs   def get_running_jobs_dict():       # Get all running jobs       response = table.scan(           ProjectionExpression=\"jobName, priority\",           FilterExpression=Attr('jobState').eq('running')       )       jobs = response.get('Items', [])        running_jobs = {job['jobName']: job['priority'] for job in jobs}        return running_jobs    # Print the queue state   def print_queue_state():       response = table.scan(           ProjectionExpression='jobName, jobState, priority'       )       jobs = response.get('Items', [])        print_table = []       for job in jobs:           print_table.append([job['jobName'], job['jobState'], job['priority']])        # sort by priority       sorted_table = sorted(print_table,                            key=lambda x: -x[2])       # Print the table       from tabulate import tabulate       print(tabulate(sorted_table, headers=['Job Name', 'State', 'Priority']))    # get job details   def get_job_details(job_name):       response = table.get_item(           Key={'jobName': job_name},           ProjectionExpression='jobDetails'       )       return json.loads(response.get('Item').get('jobDetails'))    # get job state or None if the job does not exist   def get_job_state(job_name):       response = table.get_item(           Key={'jobName': job_name},           ProjectionExpression='jobState'       )       job = response.get('Item')       return job.get('jobState') if job else None    # update the job state   def update_job_state(job_name, new_state):       table.update_item(           Key={'jobName': job_name},           UpdateExpression=\"SET jobState = :new_state\",           ExpressionAttributeValues={\":new_state\": new_state}       )       print(f'Update job {job_name} to {new_state}')    # remove a job entry   def remove_job(job_name):       table.delete_item(           Key={'jobName': job_name}       )       print(f'Removed job {job_name} from queue')Enter fullscreen modeExit fullscreen modeBoth our choice of DynamoDB and its usage (e.g., our use of theScanAPI rather than theQueryAPI) assume that the overall number of jobs in our queue will be in the dozens, at most. For a larger scale solution, you may be better off with a heavier duty database (e.g., one that performs the sorting operation for you) or a more sophisticated use of DynamoDB (e.g., seehere).Define the Training Job Queue ManagerThe main component of our solution is the training job scheduler. Here we implement a rather simple manager that performs the following steps:Extract the list of queued jobs, ordered by priority. If none exist, return.Discover unused instance capacity. For each free instance, start one pending job on SageMaker. If no jobs remain after that, return.Calculate the number of SageMaker jobs in theStoppingstate. If greater than the number of pending jobs, return.Assess the need for preemption of running SageMaker jobs by comparing theirprioritiesto those of our pending jobs.# set the limit on total number of instances/jobs   MAX_CAPACITY = 2    sagemaker = boto3.client('sagemaker')    # apply a queue stamp to identify that the job came from the queue   def apply_qstamp(job_name):       return f'{job_name}-qstamp-{datetime.now().strftime(\"%d%H%M\")}'    # strip the queue stamp   def strip_qstamp(job_name):       return job_name.split('-qstamp-')[0]    # start a SageMaker job and update job entry in queue   def start_job(job_name):       print(f'start job {job_name}')       job_details = get_job_details(job_name)       job_details['TrainingJobName'] = apply_qstamp(job_name)       if(job_details):           # start job with detail from queue           # (you may optinally overwrite fields such as the iam role)           response = sagemaker.create_training_job(**job_details)           if response['ResponseMetadata']['HTTPStatusCode'] == 200:               print(f'started job {job_name}')               update_job_state(job_name, 'running')    # preempt a SageMaker job and update job entry in queue   def preempt_job(job_name):       print(f'preempt job {job_name}')       response = sagemaker.stop_training_job(TrainingJobName=job_name)       if response['ResponseMetadata']['HTTPStatusCode'] == 200:           print(f'preempted job {job_name}')           update_job_state(strip_qstamp(job_name), 'preempted')    # get SageMaker jobs   def get_sagemaker_jobs(status):       running = sagemaker.list_training_jobs(StatusEquals=status)       return running.get('TrainingJobSummaries', [])    # queue manager   def manage_queue():       # extract pending jobs to run       pending = get_pending_jobs()        if not pending:           return        if len(pending) > MAX_CAPACITY:           pending = pending[:MAX_CAPACITY]        # get running sagemaker jobs       running = get_sagemaker_jobs('InProgress')       total_running = len(running)        # get stopping sagemaker jobs       stopping = get_sagemaker_jobs('Stopping')       total_stopping = len(stopping)        # calculate the number of free instances        free_slots = MAX_CAPACITY - total_running - total_stopping        jobs_to_start = min(len(pending), free_slots)        # for each free instance, start a job       for i in range(jobs_to_start):           start_job(pending[i].get('jobName'))        still_pending = pending[jobs_to_start:]        if not still_pending:           return        # assume that 'total_stopping' number of jobs will start soon       test_for_preemption = len(still_pending) - total_stopping       if test_for_preemption <= 0:           return        # check if preemption is required       test_priority = still_pending[total_stopping:]        running_jobs = get_running_jobs_dict()       priority_dict = {}       for job in running:           job_name = job['TrainingJobName']           priority_dict[job_name] = running_jobs[strip_qstamp(job_name)]        # sort running jobs from lowest to highest priority       sorted_running = sorted(priority_dict.items(), key=lambda item: item[1])        index = 0       while index < test_for_preemption and \\             test_priority[index].get('priority') > sorted_running[index][1]:           preempt_job(sorted_running[index][0])           index = index + 1Enter fullscreen modeExit fullscreen modeImportant notes:Our implementation is highly optimistic in the sense that we assume that all the jobs that are inserted are valid and that we will be able to start them up on SageMaker without issue. In practice, appropriate error handling should be added (e.g., removing faulty jobs from the queue with appropriate logging).In a production environment, we would need to take into consideration the likely occurrence of arace conditionwhen ourqueue_manageris triggered by multiple concurrent events. There are several ways of addressing this problem (e.g., seehere) including enforcing atomicity (e.g., by setting ourLambda function concurrencyto one), using some form of locking mechanism (e.g., as donehere), or making our functionidempotent. Here we have taken the approach of what we call \u201coptimistic idempotence\u201d, where we rely on appropriate use of the API and on the idempotency of our underlying calls to the SageMaker APIs.We emphasize that our implementation is na\u00efve. In practice, we recommend a more sophisticated algorithm that 1) accounts for the use of different types of instances and jobs that require more than one instance, 2) takes all edge cases into consideration, and 3) is tailored towards the specific needs of your project.Define the AWS Lambda FunctionThe next component of the solution is the Lambda function. The following code block includes theSAMdefinition of our serverless function. We program the function to run on two different types of events: any call toadd-jobon our private API gateway and achange to the state of a SageMaker training job.ManagedTrainingJobQueue:       Type: AWS::Serverless::Function       Properties:         CodeUri: job-queue/ # the directory containing our index.py file         Handler: index.lambda_handler         Runtime: python3.12         Architectures:           - arm64 # use graviton         Policies: # allow access to SageMaker and DynamoDB           - !Sub \"arn:${AWS::Partition}:iam::aws:policy/AmazonSageMakerFullAccess\"           - DynamoDBCrudPolicy:               TableName: !Ref DynamoSMQueue         Events:           CreateTraining:             Type: Api             Properties:               Path: /add-job               Method: post               RestApiId: !Ref InternalAPI           SageMakerEvent:             Type: EventBridgeRule             Properties:               Pattern:                 source:                   - aws.sagemaker                 detail-type:                   - SageMaker Training Job State Change                 detail:                   TrainingJobStatus:                     - \"Completed\"                     - \"Failed\"                     - \"Stopped\"Enter fullscreen modeExit fullscreen modeThelambda_handlerfunction is implemented as follows:def lambda_handler(event, context):       # identify source of event and take appropriate action       if 'requestContext' in event and 'apiId' in event['requestContext']:           print('Lambda triggerred by API Gateway')           job_details = json.loads(event.get('body'))           add_job_entry(job_details)       elif 'source' in event and event['source'] == 'aws.sagemaker':           print('Lambda triggerred by SageMaker job state change')           job_name = event['detail']['TrainingJobName']           job_status = event['detail']['TrainingJobStatus']           print(f'{job_name} status changed to {job_status}')            # strip qstamp from job_name           job_name = strip_qstamp(job_name)            if job_status in ['Completed' , 'Failed']:               remove_job(job_name)           elif job_status == 'Stopped':               # check if it was manually stopped or preempted by queue manager               if get_job_state(job_name) == 'preempted':                   print(f'job {job_name} preemption completed')               else:                   print(f'job {job_name} {job_status}, remove from queue')                   remove_job(job_name)        # in all cases invoke queue manager       manage_queue()Enter fullscreen modeExit fullscreen modeIntercept the Create Training Job RequestThe final modification required to make our solution complete is to intercept the call to the SageMakercreate_training_jobAPI and reroute it to ouradd-jobmethod. We do this by overriding the_intercept_create_requestfunction of theSageMaker Session class:from sagemaker.pytorch import PyTorch   from sagemaker.session import Session   import requests, logging   logger = logging.getLogger('sagemaker')    def submit_to_training_queue(job):       logger.info(f'Adding training-job {job['TrainingJobName']} to queue')       logger.debug('train request: {json.dumps(job, indent=4)}')        vpce='<vpc endpoint>' # insert id of vpc endpoint       region='us-east-1' # specify region       url=f'https://{vpce}.execute-api.{region}.vpce.amazonaws.com/prod/add-job'       headers = {'x-apigw-api-id': '<api-id>'} # insert api gateway id        # submit job       response = requests.post(url, headers=headers, json=job)    class QueueTrainingJobSession(Session):       def _intercept_create_request(self, request, create, func_name = None):           \"\"\"This function intercepts the create job request            Args:             request (dict): the create job request             create (functor): a functor calls the sagemaker client create method             func_name (str): the name of the function needed intercepting           \"\"\"           if func_name == 'train':               submit_to_training_queue(request)           else:               super()._intercept_create_request(request,create,func_name)    # define job   estimator = PyTorch(       role='<sagemaker role>',       entry_point='train.py',       instance_type='ml.p5.48xlarge',       instance_count=1,       framework_version='2.0.1',       py_version='py310',       tags=[{'Key': 'priority', 'Value': '100'},       keep_alive_period_in_seconds=60, # keep warm for 1 minute       # use our custom Session class       sagemaker_session=QueueTrainingJobSession()   )    estimator.fit(wait=False)Enter fullscreen modeExit fullscreen modeUse Case ExampleTo test our solution we submit the following sequence of jobs. After each call we print the status of the queue (using theprint_queue_statefunction) and sleep for twenty seconds.Start job1 with priority 1.Start job2 with priority 2.Start job3 with priority 1.Start job4 with priority 3.The first two jobs are immediately submitted to SageMaker and updated to therunningstate. Since the third job has low priority and we have precisely two training instances, it remains in thependingstate and waits its turn. After submitting the first three jobs, the queue state appears as:Job Name    State      Priority   ----------  -------  ----------   job2        running           2   job1        running           1   job3        pending           1Enter fullscreen modeExit fullscreen modeThe fourth job we submit has a higher priority than all of the jobs in the queue. Consequently, the running job with the lowest priority,job1, is preempted. The corresponding SageMaker job is stopped and once the instance is released, the queue state becomes:Job Name    State        Priority   ----------  ---------  ----------   job4        running             3   job2        running             2   job1        preempted           1   job3        pending             1Enter fullscreen modeExit fullscreen modeThe SageMaker job runningjob2is the first to finish,job2is removed from the queue, and our preempted job is resumed:Job Name    State      Priority   ----------  -------  ----------   job4        running           3   job1        running           1   job3        pending           1Enter fullscreen modeExit fullscreen modeOncejob4is completed, it too is removed from the queue, making room forjob3. The remaining jobs are also run to completion, ultimately leaving our queue empty.SummaryThe increasing difficulty of acquiring AI compute capacity has forced AI development teams to reevaluate the processes they use for training AI models. The approach we have demonstrated in this post is to augment the traditional APIs for training models with a custom-made priority queue and an associated job scheduler. Importantly, the proposal we have put forth should be viewed as a general scheme, not as a production-worthy solution. Appropriate modifications and enhancements would be required to address the specifics needs of your project."}
{"title": "Powering AWS Fargate with IaC - AWS CloudFormation", "published_at": 1710147716, "tags": ["aws", "devops", "infrastructureascode", "containers"], "user": "Engin ALTAY", "url": "https://dev.to/aws-builders/powering-aws-fargate-with-iac-aws-cloudformation-3n99", "details": "Today's tech world, there's a clear truth that your organization needs to be agile as well as your workloads need to run smooth. Especially in containers area, there are plenty of methods to deploy your containerized workloads to your environment.In this post, I'd like to mention powering AWS Fargate -a serverless compute that run containers without needing to manage your infrastructure, with Infrastructure as Code AWS CloudFormation - to provision your fargate workloads, including load balancing and rolling-update deployment features.I assume that you already have that in use or familiar with tech stack that I mentioned below.Amazon ECS Cluster - AWS FargateAWS CloudFormationGitLab CI/CDImagine the following case:You have created your ECS cluster with Fargate option,You already have provisioned internet-facing ELB,You already have VPC, subnets and security group for your application.But in the continuation, you need to:Provision your AWS Fargate workloads,Attach security group,Expose as a ECS service,Associate with ELB, create your ELB listener routing rule and more.All these steps become unmanageable and tedious after number of your application, environment (dev,test,staging,prod) and workload grows.To make this agile and automated, we'll leverage AWS CloudFormation, combining with GitLab CI/CD.Step 1 - Building our CloudFormation templateUsing AWS CloudFormation to provision and update our resources in AWS environment helps us in a way to centralize and track our each change.We need to define each resource definitions to our CloudFormation template. This is the core component we'll work on it.deploy-fargate.yamlAWSTemplateFormatVersion: 2010-09-09 Description: An example CloudFormation template for Fargate. Parameters:   VPC:     Type: String     Default: <VPC_ID_HERE>   SubnetPublicA:     Type: String     Default: <PUBLIC_SUBNET_A>   SubnetPublicB:     Type: String     Default: <PUBLIC_SUBNET_B>   SubnetPublicC:     Type: String     Default: <PUBLIC_SUBNET_C>   Image:     Type: String     Default: <ACCOUNT_ID>.dkr.ecr.eu-central-1.amazonaws.com/nginx:latest   ClusterName:     Type: String     Description: ECS_CLUSTER_NAME here     Default: <ECS_CLUSTER_NAME>      ServiceName:     Type: String     Description: ECS_SERVICE_NAME here     Default: \"API_NAME-prod-svc\"   TaskDefinitionName:      Type: String     Description: Task Definition Name     Default: \"API_NAME-prod-fargate\"   ContainerPort:     Type: Number     Default: 3000   ContainerSecurityGroup:     Type: String     Description: api-container-sec-rules         Default: <SECURITY_GROUP_ID>   ELBListenerArn:     Type: String     Default: <ELB_LISTENER_ARN>   Resources:   TaskDefinition:     Type: AWS::ECS::TaskDefinition     Properties:       # Name of the task definition.       Family: !Ref TaskDefinitionName       NetworkMode: awsvpc       RequiresCompatibilities:         - FARGATE       # 1024 (1 vCPU) - Available memory values: 2GB, 3GB, 4GB, 5GB, 6GB, 7GB, 8GB       Cpu: 1024       # 2GB, 3GB, 4GB, 5GB, 6GB, 7GB, 8GB - Available cpu values: 1024 (1 vCPU)       Memory: 3GB       # \"The ARN of the task execution role that containers in this task can assume. All containers in this task are granted the permissions that are specified in this role.\"       ExecutionRoleArn: !GetAtt ExecutionRole.Arn       # \"The (ARN) of an IAM role that grants containers in the task permission to call AWS APIs on your behalf.\"       TaskRoleArn: !Ref TaskRole       ContainerDefinitions:         - Name: API_NAME           Image: !Ref Image           Cpu: 0           Essential: true           PortMappings:             - ContainerPort: !Ref ContainerPort               Protocol: tcp           LogConfiguration:             LogDriver: awslogs             Options:               awslogs-region: !Ref AWS::Region               awslogs-group: !Ref LogGroup               awslogs-stream-prefix: ecs                  LogGroup:     Type: AWS::Logs::LogGroup        Properties:       LogGroupName: !Join  ['', [/ecs/, !Ref TaskDefinitionName]]       RetentionInDays: 14    # A role needed by ECS   ExecutionRole:     Type: AWS::IAM::Role     Properties:       RoleName: !Join ['', [!Ref ServiceName, \"ECSExecutionRole\"]]       AssumeRolePolicyDocument:         Statement:           - Effect: Allow             Principal:               Service: ecs-tasks.amazonaws.com             Action: 'sts:AssumeRole'       ManagedPolicyArns:         - 'arn:aws:iam::aws:policy/service-role/AmazonECSTaskExecutionRolePolicy'         -'arn:aws:iam::aws:policy/AmazonEC2ContainerRegistryReadOnly'    # A role for the containers   TaskRole:     Type: AWS::IAM::Role     Properties:       RoleName: !Join ['', [!Ref ServiceName, \"ECSTaskRole\"]]       AssumeRolePolicyDocument:         Statement:           - Effect: Allow             Principal:               Service: ecs-tasks.amazonaws.com             Action: 'sts:AssumeRole'       ManagedPolicyArns:         - 'arn:aws:iam::aws:policy/service-role/AmazonECSTaskExecutionRolePolicy'         - 'arn:aws:iam::aws:policy/AmazonEC2ContainerRegistryReadOnly'     Service:     Type: AWS::ECS::Service     DependsOn:       - LoadBalancerListenerRule         Properties:        ServiceName: !Ref ServiceName       Cluster: !Ref ClusterName       TaskDefinition: !Ref TaskDefinition       DeploymentConfiguration:         MinimumHealthyPercent: 100         MaximumPercent: 200       DesiredCount: 1       HealthCheckGracePeriodSeconds: 120       CapacityProviderStrategy:         - CapacityProvider: FARGATE_SPOT           Base: 0           Weight: 1             NetworkConfiguration:          AwsvpcConfiguration:           AssignPublicIp: ENABLED           Subnets:             - !Ref SubnetPublicA             - !Ref SubnetPublicB             - !Ref SubnetPublicC           SecurityGroups:             - !Ref ContainerSecurityGroup       LoadBalancers:         - ContainerName: API_NAME           ContainerPort: !Ref ContainerPort           TargetGroupArn: !Ref TargetGroup    TargetGroup:     Type: AWS::ElasticLoadBalancingV2::TargetGroup     Properties:       HealthCheckIntervalSeconds: 30       HealthCheckPath: /API_NAME/health       HealthCheckTimeoutSeconds: 5       UnhealthyThresholdCount: 2       HealthyThresholdCount: 3       TargetType: ip       Name: !Ref ServiceName       Port: !Ref ContainerPort       Protocol: HTTP       TargetGroupAttributes:         - Key: deregistration_delay.timeout_seconds           Value: 30  #default 300 seconds       VpcId: !Ref VPC    LambdaDescribeELBListenerPriority:     Type: 'Custom::LambdaDescribeELBListenerPriority'     Properties:       ServiceToken: 'arn:aws:lambda:eu-central-1:<ACCOUNT_ID>:function:DescribeELBListener'    LoadBalancerListenerRule:     Type: AWS::ElasticLoadBalancingV2::ListenerRule     #DependsOn: GetListenerRulesLambdaFunction     Properties:       Actions:         - Type: forward           TargetGroupArn: !Ref TargetGroup       Conditions:         - Field: host-header           HostHeaderConfig:             Values:               - \"api.example.com\"         - Field: path-pattern           PathPatternConfig:             Values:               - \"/API_NAME*\"       ListenerArn: !Ref ELBListenerArn       Priority: !GetAtt LambdaDescribeELBListenerPriority.NextPriorityValue  Outputs:   NextPriorityValue:     Value: !GetAtt LambdaDescribeELBListenerPriority.NextPriorityValueEnter fullscreen modeExit fullscreen modeIn this template, a few sections need to be mentioned to clarify the case we are dealing with.In the Parameters section, provide some constants that we already created before such as VPC Id, Subnets, ECS Cluster Name, Security Group, ELB Listener etc.You can also create these from scratch but in my case they all have created before.As a Capacity Provider, FARGATE_SPOT is used, but you can change it to FARGATE as your needs or you can benefit combining both.As a placeholder, API_NAME is used. Replace API_NAME with application name that needed to be run on AWS Fargate.We did not declare auto scale actions for Fargate service. Desired count set to 1. Planning to mention auto scale policy for Fargate in the next post.Declared Custom Resource LambdaDescribeELBListenerPriority which describes the ELB Listener and finds next available priority number to create listener routing rule.That custom resource is a bit headache, I expect CloudFormation to handle automatically finding next available ELB Listener priority number and put my rule to there. But it does not. It expects you to provide priority number. In a development lifecycle and CI/CD perspective, it's impossible to know which priority number is free and set it before running CloudFormation template. Therefore, we write a simple lambda function that describes ELB Listener, takes max priority number and adds +1 to create available priority number.I provide the related lambda function below.DescribeELBListenerimport boto3 import json import urllib3  http = urllib3.PoolManager() SUCCESS = \"SUCCESS\" FAILED = \"FAILED\"  def lambda_handler(event, context):     elbv2 = boto3.client('elbv2')      # Get all listener rules for the provided ARN     response = elbv2.describe_rules(ListenerArn='<ELB_LISTENER_ARN>')      # Filter out rules with non-numeric priorities     filtered_rules = [rule for rule in response['Rules'] if str(rule['Priority']).isdigit()]      # Find the maximum priority among the remaining rules     max_priority = max(filtered_rules, key=lambda x: x['Priority'])['Priority']      # Prepare the CloudFormation (CF) stack event response payload         responseValue = int(max_priority) + 1     responseData = {'NextPriorityValue': responseValue}      send(event, context, SUCCESS, responseData)  def send(event, context, responseStatus, responseData, physicalResourceId=None, noEcho=False, reason=None):     responseUrl = event['ResponseURL']      responseBody = {         'Status': responseStatus,         'Reason': reason or f'See the details in CloudWatch Log Stream: {context.log_stream_name}',         'PhysicalResourceId': physicalResourceId or context.log_stream_name,         'StackId': event['StackId'],         'RequestId': event['RequestId'],         'LogicalResourceId': event['LogicalResourceId'],         'NoEcho': noEcho,         'Data': responseData     }      json_responseBody = json.dumps(responseBody, default=str)      headers = {         'content-type': '',         'content-length': str(len(json_responseBody))     }     try:         response = http.request('PUT', responseUrl, headers=headers, body=json_responseBody)         print(\"Status code:\", response.status)      except Exception as e:         print(\"send(..) failed executing http.request(..):\", e)Enter fullscreen modeExit fullscreen modeStep 2 - Prepare CI/CD Pipeline - GitLabNow we need to prepare CI/CD side to run our CloudFormation template.Never use your own credentials to access AWS and run CloudFormation template from your local environment.Here, we use centralized private GitLab to run CloudFormation stack and provided AWS access by following least-privileged permission policy to access resources.Pay attention to \"image\": \".dkr.ecr.eu-central-1.amazonaws.com/nginx:latest\" section. Latest tag will be replaced with our respectful container image tag during CI/CD job.Building GitLab CI/CD PipelineTo automate and run our IaC template, we leverage version control system, so each our iteration is trackable and easy to apply changes.Below fully ready .gitlab-ci.yml file that includes build & IaC deployment stages.variables:   API: nginx   REGISTRY: \"<your_aws_account_number_here>.dkr.ecr.eu-central-1.amazonaws.com\"   ECS_CLUSTER_NAME: \"YOUR_ECS_CLUSTER_NAME\"   ECS_SERVICE_NAME: \"${API}-prod-svc\"   ECS_TASK_FAMILY: \"${API}-prod-fargate\"   CF_STACK_NAME: '${API}-cf-template-${CI_PIPELINE_IID}'    stages:   - build   - deploy   - update  before_script:   - echo \"Build Name:\" \"$CI_JOB_NAME\"   - echo \"Branch:\" \"$CI_COMMIT_REF_NAME\"   - echo \"Build Stage:\" \"$CI_JOB_STAGE\"   build:   stage: build   script:     - $(aws ecr get-login --no-include-email --region eu-central-1)     - VER=$(cat ${PWD}/package.json | jq --raw-output '.version')     - echo $VER         - docker build -t ${API} .     - docker tag ${API} ${REGISTRY}/${API}:${VER}-${CI_ENVIRONMENT_NAME}-${CI_PIPELINE_IID}     - docker push ${REGISTRY}/${API}:${VER}-${CI_ENVIRONMENT_NAME}-${CI_PIPELINE_IID}   environment:     name: prod   deploy_cloudformation:   stage: deploy   when: manual   image:     name: amazon/aws-cli:latest     entrypoint: ['']   rules:     - if: $API != \"null\" && $CI_COMMIT_BRANCH == \"master\"   script:     - echo \"Deploying your IaC CloudFormation...\"     - yum install jq -y     - jq -Version     - VER=$(cat ${PWD}/package.json | jq --raw-output '.version')     - echo $VER     - echo \"API Name ----> ${API} <----\"     - echo \"ECS FARGATE Cluster is = ${ECS_CLUSTER_NAME}\"     - sed -i 's/API_NAME/'\"${API}\"'/g' deploy-fargate.yaml #replace API_NAME placeholder with the container that we want to run on AWS Fargate.     - cat deploy-fargate.yaml     - |       aws cloudformation create-stack \\         --stack-name $CF_STACK_NAME \\         --template-body file://deploy-fargate.yaml \\         --capabilities CAPABILITY_NAMED_IAM \\         --parameters \\       ParameterKey=Image,ParameterValue=${REGISTRY}/${API}:${VER}-${CI_ENVIRONMENT_NAME}-${CI_PIPELINE_IID}     - echo \"Visit https://api.example.com to see changes\"   needs:     - job: build       optional: true   tags:     - gitlab-dind-runner   environment:     name: prod     url: https://api.example.comEnter fullscreen modeExit fullscreen modeWith this, in every CI/CD pipeline we run, build our container image, tag with respectful CI/CD pipeline id, then inject it to CloudFormation template to run it on ECS Fargate.Updating ECS service - rolling updateIn the final step, we need to update ECS service with our updated task revision. To do this, we need to run cloudformation update-stack instead of create-stack. Below pipeline will trigger a rolling update for ECS service.update_cloudformation:   stage: update   when: manual   image:     name: amazon/aws-cli:latest     entrypoint: ['']   rules:     - if: $API != \"null\" && $CI_COMMIT_BRANCH == \"master\"   script:     - echo \"Deploying your IaC CloudFormation...\"     - yum install jq -y     - jq -Version     - VER=$(cat ${PWD}/package.json | jq --raw-output '.version')     - echo $VER     - echo \"API Name ----> ${API} <----\"     - echo \"ECS FARGATE Cluster is = ${ECS_CLUSTER_NAME}\"     - sed -i 's/API_NAME/'\"${API}\"'/g' deploy-fargate.yaml #replace API_NAME placeholder with the container that we want to run on AWS Fargate.     - cat deploy-fargate.yaml     - |       aws cloudformation update-stack \\         --stack-name $CF_STACK_NAME \\         --template-body file://deploy-fargate.yaml \\         --capabilities CAPABILITY_NAMED_IAM \\         --parameters \\       ParameterKey=Image,ParameterValue=${REGISTRY}/${API}:${VER}-${CI_ENVIRONMENT_NAME}-${CI_PIPELINE_IID}     - echo \"Visit https://api.example.com to see changes\"   needs:     - job: build       optional: true   tags:     - gitlab-dind-runner   environment:     name: prod     url: https://api.example.comEnter fullscreen modeExit fullscreen modeThat's it! Now you are able to deploy your containerized application with zero downtime, and in an automated way.In this post, I wanted to mention how to automate deployment process of your container to Amazon ECS Fargate using AWS CloudFormation & GitLab CI/CD.References:https://docs.gitlab.com/runner/https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/AWS_ECS.html"}
{"title": "DevOps with Guruu | Chap 1: Start with AWS Cloud \u2013 Basic EC2 | Build your First CI/CD Pipeline", "published_at": 1710130142, "tags": ["webdev", "devops", "aws", "beginners"], "user": "Hoang Guruu", "url": "https://dev.to/aws-builders/devops-with-guruu-chap-1-start-with-aws-cloud-basic-ec2-build-your-first-cicd-pipeline-29fp", "details": "DevOps with Guruu | Chap 1: Start with AWS Cloud \u2013 Basic EC2 | Build your First CI/CD PipelineWelcome to my Series DevOps with GuruuOverview of Our First CI/CD PipelineStep-by-Step Guide with AWS ConsoleAccessing EC2 - Various MethodsCommand Line BasicsJoin me on this journey to mastering DevOps. Let's get hands-on and build a robust foundation together! Don't forget to like, share, and subscribe for more exciting content. Happy coding, future DevOps!\""}
{"title": "Passing the AWS Security Speciality in 2024 (SCS-C02)", "published_at": 1710123675, "tags": ["aws", "security", "certification"], "user": "rowan", "url": "https://dev.to/aws-builders/passing-the-aws-security-speciality-in-2024-scs-c02-1aik", "details": "Last week I passed the latest version of the AWS Security Speciality (SCS-C02). TheSecurity Speciality certificationassesses your knowledge of the various AWS security services, and the security capabilities of more general services offered by AWS.I really like this exam, and have been taking it since it was announced in beta back in early 2018:It helps that I like AWS IAMmore than most, and IAM is a big part of the exam. That being said, I think the new version of the exam (released in 2023) is a really good update that gives it more breadth, while still focusing on the most relevant security-related services in AWS - it helps that there are so many of them now!If you want to pass the Security Speciality, you need knowthe official exam guide (PDF)intimately. All the exam questions are written with the guide in mind, so you can't be too familiar with it.Here are the high level observations from my experience with the exam, keeping in mind that the questions are pulled from a large pool and are continually updated (I know becauseI wrote a few of them), so your milage may vary!Exam ImpressionsFor all questions, even for other AWS certification exams, keep in mind the following:Focus on the specific services mentioned in the question, as this will help you eliminate some of the distractor responses. Look specifically for services not mentioned by function rather than name, for example \"global cache\" is CloudFront, \"object storage\" is S3, \"threat detection\" is GuardDuty, etc.Be clear what the priority is: the common ones areoperational overheadorcost efficiency, which will enable to you pick between two similar responses. In most cases, a question that asks \"least operation overhead\" generally wants you to pick the response(s) with more AWS services.Specific ExamplesSystems Manager Parameter Store (specifically Secure String parameters) is often positioned as a cheaper (aka. \"cost-effective\") option toAWS Secrets Manager. Just remember thatonlySecrets Manager can automatically rotate secrets, so some solutions will require it, regardless of cost.Know the differences betweenAWS KMSandAWS CloudHSM. This is particularly relevant when deleting keys: KMS has a mandatory waiting period, but CloudHSM can delete keys immediately. KMS also supportsdifferent types of key material, and there are limits to when you can/can't use them which you should know. Spend some time understandingKMS key grants- they are like resource policies for KMS keys, and will definitely feature in your exam.IAM policy syntax and evaluation is a must! You will have multiple questions in this exam that will require you to read, understand, and choose between different policies. Of course if you need help with that, I got you \ud83d\ude09This also includesSCPs, which are technically part ofAWS Organizationsbut follow the same policy syntax as IAM. You should also know some of the commonAWS IAM Conditionslike those for checking MFA is present, etc. I had a couple of questions which requiredNotAction, which can be counterintuitive, but required to achieve some scenarios.Know the difference betweengateway endpointsandinterface endpointsfor VPCs. Gateway endpoints are an older (pre-PrivateLinkapproach) only for S3 and DDB, and require routing updates; Interface endpoints support more services.Know the difference between services likeAmazon GuardDuty(which detect anomalous behaviour and threats) andAmazon Inspector(which is for vulnerability management).Any mention ofPIIusually meansAmazon Macie.Encrypting data is a key strategy to prevent the accidental leakage of data. By controlling access to the decryption keys, you give yourself another layer of control to prevent unintended exposure of sensitive data, regardless of the services being used.Security Hubaggregates the result from other services (such as Config, Inspector, WAF, Macie, etc) to generate its reports and alerts, so make sure they're available.AWS Service Catalogue featured in multiple questions where it was used to limit deployments to approved resources and configurations.When quarantining a compromised instance, pay attention to the uptime requirements. You can quarantine by changing its security groups, or by moving the instance in to an isolated subnet; but changing an instance's subnet will require you to take it offline, but changing its SG won't.Knowing how to set upa SAML identity providerin IAM helped on a few different questions about federation and SAML metadata changes.Don't forget thatAWS Certificate Managercertifications forCloudFrontneed to be provisioned in theus-east-1(N. Virginia) region.There were quite a few questions including CloudWatch Log. Keep in mind that CW Logs's IAM actions are in thelogs:namespace, notcloudwatch:, and log deliverability is usually handled by theSSM agent.Amazon Identity Centerhas a built-in user directory that can be used as anIdPfor AWS-only solutions.Know the difference between VPCNACLs(stateless, coarse) andSecurity Groups(stateful, fine-grained), as well as their defaults, features, and limitations. By default, Security Groups will allow all outbound traffic, and no inbound traffic.BothS3 lifecycleconfiguration andDynamoDB item TTLprovide a way to expire data for security reasons, limiting the potential for unintended exposure.AWS Audit Managerfeatured in multiple questions; keep in mind it can work with on-premises resources for hybrid reports.Know the various ways for delivering notifications:Amazon SNScan be used for low-cost email notifications  (aka. \"cost-effective\"),Amazon SES(preferred for scale and deliverability), andAmazon SQS(used for messaging, but not email!)SurprisesFortunately, there were only a few surprises for me on this exam:I had a question aboutDNSSEC for Route53which caught me unawares.X-Frame-Options headers can be added to origin response policiesin CloudFront distributions.Somehow I'd managed to skipAWS Signerin my review, but it only featured lightly and unlike a lot of other AWS services, the name actually tells you what it does (code signing for containers and functions) \ud83d\ude43I haven't had a chance to use Glacier Vaults, so I wasn't sure if or when you could change them; it turns outyou have a 24-hour window to abort the lock and update the policybefore it become immutable.There were also a few omissions that surprised me, but this could have just beenluck of the draw:I didn't get any questions about session policies.I only sawAWS CodeArtifactin distractor responses.Having seen the exam evolve and change over time, I think this new version of the certification is an improvement over previous iterations, and would recommended it to people working with AWS seriously or on a daily basis - it's hard to overstate the importance of security in the cloud!Being able to build secure applications on AWS, or assess an existing solution for potential security issues or improvements is a valuable skill, and one that I think will only become more important in the future. I think this exam is a good measure of someone's understanding of the security offerings and features on AWS."}
{"title": "Launching a simple EC2 Instance with Apache Web Server \u2014 Part 1", "published_at": 1710122775, "tags": ["aws", "ec2"], "user": "Scott Burgholzer", "url": "https://dev.to/aws-builders/launching-a-simple-ec2-instance-with-apache-web-server-part-1-33nl", "details": "Little backstory:I\u2019ve had this in my drafts for a while, wanting to do it all as one article. I\u2019ve decided that is not the best way of presenting this information. As a newer content creator, I\u2019ve been playing with how I want my pages to look like, and have decided to go with how it looks right now.IntroductionIn this article, I will be doing what some call \u201cclick-ops\u201d in AWS console as that is the easiest way to get started. If you want more challenges, I will also be posting on how to use AWS CLI (an article post on this will come at a later date, when completed, I will be updating this with a link to it!) to do the same thing. I will also show how to use a Cloud Development Kit (CDK). I will be using Python in my CDK examples. I will also use AWS CloudFormation. Finally, for an even more advanced challenge, I will be creating this process again using Terraform. If you are new, the console way will be the easiest, but I also want to introduce you to the CLI, CDK, CloudFormation and Terraform as these are advanced tools used!Scope of this tutorialIn this multi-part tutorial, we are wanting to set up an EC2 instance, configure security group(s), create a key-pair SSH Key to remote into the instance, and install Apache Web Server.Method 1:Using AWS Console (without using userdata)There are actually two ways we can use the AWS console to create the EC2 instance, and install Apache and run it and create the test HTML file. This method will require us to use SSH to remote into our EC2 instance to preform some steps.Step 1:When you are logged into the AWS Console, search for EC2 in the search bar.Step 2:Click on Launch InstanceStep 3: Configure your instance and install Apache and test web pageGive the instance a name, I called mine \u201cWeb Server Test\u201d    * Keep the application and OS Images as isKeep the instance type as t2.microClick on new key par and give it a name, then click on Create key pair. It\u2019ll download a file, move it to a safe location so you don\u2019t loose it!Under Network settings we will be keeping the selection Create Security Group. Allow SSH traffic from should be checked, and you\u2019ll also want to check the Allow HTTP traffic from the internet. Next to SSH there is a dropdown menu that you can say any IP can connect to the instance, a custom IP or range, or your IP only. For this example, I\u2019m keeping it as anywhereWe will leave everything else as defaultClick on Launch InstanceClick on the instance ID shown in the green success boxClick the checkbox next to the instance, then go to actions and click on ConnectYou\u2019ll see there are four options of connecting. The only two that will work with our instance, due to the way we set it up, is EC2 Instance Connect and SSH client. If on Mac or Linux you can use command line to SSH into the instance\u2019s command line, on Windows it is easiest to use a tool such as Putty.We will be using EC2 Instance connect in our case, so click on that tab, then click on connectAfter waiting a few moments, you\u2019ll see a command line terminal for our instance.We now want to install Apache and start the service. Run the following commands in the command prompt.sudoyum update-y# We are asking the packacge manager to install any updatessudoyuminstallhttpd-y# We are asking the package manager to install Apachesudosystemctlenablehttpd# We are telling the server to automatically start Apache upon start upsudosystemctl start httpd# we are telling the server to start the Apache Serversudosystemctl status httpd# we are telling the server to tell us the status of the Apache ServerEnter fullscreen modeExit fullscreen modeWe are going to create a simple HTML file, run the below command to open a new file using a command line text editorsudo nano /var/www/html/index.htmlPut the following code into the text editor<!DOCTYPE html><html><head><title>Test Web Page</title></head><body><h1>Welcome to my website!</h1><p>I love AWS</p></body></html>Enter fullscreen modeExit fullscreen modeWhen you are done with the file, press ctl and x, it\u2019ll prompt you if you want to save, type Y, press enter, then press enter again.Go back to the EC2 Instances page, click on your instance, then click on open address under the Public IPv4 address, change the HTTPS to HTTPIf you see your test page, you were successful!Step 4: Clean UpClose all windows except for the Instances window.Select it, then click on Instance state then click on Terminate instance so you don\u2019t use up your free trial credits or get charged!Congrats! You have just launched a simple EC2 Web server!Launching a simple EC2 Instance with Apache Web Server \u2014 Part 1 (AWS Console)Launching a simple EC2 Instance with Apache Web Server \u2014 Part 2 (AWS Console with Userdata) (Coming Soon)Launching a simple EC2 Instance with Apache Web Server \u2014 Part 3 (AWS CLI with user data) (Coming Soon)Launching a simple EC2 Instance with Apache Web Server \u2014 Part 4 (Terraform with user data) (Coming Soon)"}
{"title": "AWS cross account access (switch role)", "published_at": 1710119230, "tags": ["aws", "iam", "iamrole"], "user": "Olawale Adepoju", "url": "https://dev.to/aws-builders/aws-cross-account-access-switch-role-3bn", "details": "In this tutorial we will switch role delegated to access a resources in different AWS accounts. You share resources in one account with users in a different account. By setting up cross-account access in this way, you don't have to create individual IAM users in each account.Access AWS consoleOpen Identity and Access Management (IAM)Click \"Roles\" on left side menuSelect the AWS accountSince it is a cross account access, give the the Account ID to which you want to grant access to your resourcesNext is to give the permission policies, type the policy you want to attach in the search bar.Add the Role name, and an option description. Then create the roleFinally role can be used in cross account by clicking on switch role in the consoleRole can be switched by inputting the Account ID, Role name."}
{"title": "Issue 35 of AWS Cloud Security Weekly", "published_at": 1710116246, "tags": ["cloudsecurity", "cybersecurity", "newsletter", "aws"], "user": "AJ", "url": "https://dev.to/aws-builders/issue-35-of-aws-cloud-security-weekly-1788", "details": "(Summary of Issue 35 of AWS Cloud Security weekly @https://aws-cloudsec.com/p/issue-35)What happened in AWS CloudSecurity & CyberSecurity last week March 04-10, 2024?Amazon Neptune has obtained authorization for Department of Defense Cloud Computing Security Requirements Guide Impact Levels 4 and 5 (DoD SRG IL4 and IL5) within the AWS GovCloud (US-East and US-West) Regions.AWS WAF now allows the examination of up to 64KB of the content in incoming HTTP/S requests for Amazon API Gateway, Cognito user pools, App Runner, and AWS Verified Access regional resources. The default inspection size has been adjusted from 8KB to 16KB.(Please note: Support for increased body limits for Application Load Balancers and App Sync is currently not available and you will be charged extra for each additional 16KB analyzed beyond the default body inspection limit). Here\u2019s my WAF ACL rule sample for AWS Cognito (default body size limit) setting:Amazon Simple Email Service (SES) has introduced a new capability that allows you to define custom headers while utilizing SES v2 sending APIs for email transmission. The flexibility to set headers can be used for example for incorporating one-click unsubscribe functionality by appending list-unsubscribe headers to their emails. Check THIS blog out for details. (Note: The change was in response to Gmail and Yahoo Mail announcing a new set of requirements for senders effective from February 2024.) For example, I set the Unsubscribe option header for emails I send using python script below and could observe the \u201cUnsubscribe\u201d one-click option:Wickr on AWS has secured FedRAMP High authorization within the AWS GovCloud (US-West) Region. Now, you have the capability to employ Wickr to safeguard communications that fall under the FedRAMP High requirements.AWS X-Ray has introduced support for logging eight new data and one additional management event APIs within AWS CloudTrail. You can now capture all AWS X-Ray API activities related to both data and management events through AWS CloudTrail, like PutTraceSegments and GetTraceSummaries, as well as management events such as GetSamplingStatisticSummaries.Trending on the news & advisories:Microsoft released an Update on the attack by Nation State Actor Midnight Blizzard via the 8-K/A (an amandement to previous 8K).CISA and NSA Release Cybersecurity Information Sheets on Cloud Security Best Practices.NSA released Advancing Zero Trust Maturity Throughout the Network and Environment Pillar.Ex-Google engineer charged with stealing AI trade secrets while working with Chinese companies.CrowdStrike to Acquire Flow Security to Expand its Data Security Posture Management (DSPM)"}
{"title": "OpenSearch with AWS Lambda Rust (part 2) - business logic", "published_at": 1710090366, "tags": ["aws", "rust", "elasticsearch", "serverless"], "user": "szymon-szym", "url": "https://dev.to/aws-builders/opensearch-with-aws-lambda-rust-part-2-business-logic-2fnf", "details": "In theprevious partwe prepared a local environment for testing lambda function integration with OpenSearch. The code for the first part ishereThe code for this partis on branch 2-implement-logicPrerequisitesI assume, that the OpenSearch cluster runs locally. Please check the Part 1 if you want to follow the same steps as I did.GoalsWith the lambda function and OpenSearch running locally, I can now iterate over the code and add business logic.I would like my lambda to receive an event with a list of fields to use in the OpenSearch query. Later, I could connect the function to the REST API Gateway or use it as a data source for AppSync.My next steps are the following:prepare OpenSearch queryupdate lambda input typetest lambda locallyOpenSearch queryIn my case, a query to OpenSearch looks more or less like this:{\"size\":2,\"query\":{\"bool\":{\"must\":[{\"range\":{\"AvgTicketPrice\":{\"gte\":500,\"lte\":900}}},{\"match\":{\"DestWeather\":\"Sunny\"}}]}}}Enter fullscreen modeExit fullscreen modeI have some matches and range. For the sake of my example, it is enough.My idea is to create astructthat reflects the structure of the query and to implement a builder pattern, so I can construct the query with ease.Let's go to thefunctions/opensearch_service/src/lib.rsand start creating types. Going from the top I createuery->bool->muststructure#[derive(Debug,Serialize,Deserialize)]pubstructOpenSearchQuery{query:BoolQuery,}#[derive(Debug,Serialize,Deserialize)]pubstructBoolQuery{bool:MustQuery,}#[derive(Debug,Serialize,Deserialize)]pubstructMustQuery{must:Vec<QueryStatement>,}Enter fullscreen modeExit fullscreen modeThanks toserdeandserde_jsonit's enough to annotate structs withSerializeandDeserializeto have both operations covered.In my caseQueryStatementmight have two variants:matchorrange. Rust provides a native way to define discriminated unions (\"OR types\"):#[derive(Debug,Serialize,Deserialize)]#[serde(untagged)]enumQueryStatement{MatchStatement(MatchStatement),RangeStatement(RangeStatement),}Enter fullscreen modeExit fullscreen modeEnums can be represented in JSON in different ways. I letserdeknow that I want to keep themuntagged. This way they \"disappear\" in the JSON leaving only properties of enumerated types.#[derive(Debug,Serialize,Deserialize)]pubstructMatchStatement{#[serde(rename=\"match\")]match_statement:Value,}#[derive(Debug,Serialize,Deserialize)]pubstructRangeStatement{range:Value,}Enter fullscreen modeExit fullscreen modeThe word \"match\" is a reserved keyword. I use match_statement as a struct property and rename it only during serialization.Valueis a type provided byserde_json.It might represent various types available in JSON. I probably could spend more time on making my domain a bit more precise, however, I decided to leave it as this. The main reason is that I will use a builder to construct the query, so I don't expect any ambiguity in the final object.pubstructOpenSearchQueryBuilder{query:OpenSearchQuery,}implOpenSearchQueryBuilder{pubfnnew()->Self{Self{query:OpenSearchQuery{query:BoolQuery{bool:MustQuery{must:vec![]},},},}}pubfnwith_must_match(mutself,field:&str,value:String)->Self{ifvalue.is_empty(){returnself;}self.query.query.bool.must.push(QueryStatement::MatchStatement(MatchStatement{match_statement:json!({field:value}),}));self}pubfnwith_must_range(mutself,field:&str,from:Option<f64>,to:Option<f64>)->Self{letrange=json!({field:{\"gte\":from,\"lte\":to}});self.query.query.bool.must.push(QueryStatement::RangeStatement(RangeStatement{range}));self}pubfnbuild(self)->OpenSearchQuery{self.query}}Enter fullscreen modeExit fullscreen modeWe are almost done here. I updated my first iteration ofquery_all_docsfuntion so it can be used with createdOpenSearchQuerytypepubasyncfnquery<T>(&self,index:&str,limit:i64,offset:i64,query:OpenSearchQuery,)->anyhow::Result<Vec<T>>whereT:DeserializeOwned,{letquery_json=json!(query);println!(\"query: {}\",query_json);letresponse=self.client.search(SearchParts::Index(&[index])).size(limit).from(offset).body(query_json).send().await?;letresponse_body=response.json::<Value>().await?;letresult=response_body[\"hits\"][\"hits\"].as_array().unwrap().iter().map(|raw_value|serde_json::from_value::<T>(raw_value[\"_source\"].clone()).unwrap()).collect::<Vec<_>>();Ok(result)}Enter fullscreen modeExit fullscreen modeLooks good. Now let's update lambda function code.Lambda functionIn thefunctions/query/src/main.rsFirst of all, I update theRequesttype.#[derive(Deserialize,Clone,Copy)]structPagination{limit:Option<i64>,offset:Option<i64>,}#[derive(Deserialize)]#[serde(rename_all=\"camelCase\")]structRequest{destination_city_name:Option<String>,origin_city_name:Option<String>,destination_weather:Option<String>,origin_weather:Option<String>,max_avg_ticket_price:Option<f64>,min_avg_ticket_price:Option<f64>,pagination:Option<Pagination>,}Enter fullscreen modeExit fullscreen modeI can query by destination city and weather, the same for the origin city. There is a way to define average ticket price limits. Finally, we have some basic pagination.The main point is that all properties are optional, so the caller has a lot of flexibility in querying data.The function handler stays straightforward - the only new action is building the queryasyncfnfunction_handler(os_client:&OpenSearchService,event:LambdaEvent<Request>)->Result<Response,Error>{letrequest_body=event.payload;letindex=\"opensearch_dashboards_sample_data_flights\";letlimit=request_body.pagination.and_then(|p|p.limit).unwrap_or(10);letoffset=request_body.pagination.and_then(|p|p.offset).unwrap_or(0);letquery=OpenSearchQueryBuilder::new().with_must_match(\"OriginWeather\",request_body.origin_weather.unwrap_or(\"\".to_string())).with_must_match(\"DestWeather\",request_body.destination_weather.unwrap_or(\"\".to_string())).with_must_match(\"DestCityName\",request_body.destination_city_name.unwrap_or(\"\".to_string())).with_must_match(\"OriginCityName\",request_body.origin_city_name.unwrap_or(\"\".to_string())).with_must_range(\"AvgTicketPrice\",request_body.min_avg_ticket_price,request_body.max_avg_ticket_price).build();letquery_result=os_client.query::<FlightData>(index,limit,offset,query).await?;// Prepare the responseletresp=Response{flights:query_result,};Ok(resp)}Enter fullscreen modeExit fullscreen modeTesting locallyUsingcargo lambdaI run the functioncdfunctions/query cargo lambda watchEnter fullscreen modeExit fullscreen modeNow I can invoke it with the provided input. I update myevents/flights.json{\"destinationCityName\":\"London\",\"maxAvgTicketPrice\":600,\"minAvgTicketPrice\":400,\"pagination\":{\"limit\":1,\"offset\":10}}Enter fullscreen modeExit fullscreen modeAnd, in the second terminal window, I runcargo lambda invoke-Fevents/flights.jsonEnter fullscreen modeExit fullscreen mode\ud83c\udf89 \ud83c\udf89 \ud83c\udf89It works.Now I can update test events, or create more events, to test my query.SummaryIn this part, we implemented anOpenSearchQuerywith a builder to be used by the lambda function. We also updated the lambda function itself, so now it prepares queries based on incomingRequest.For now, we didn't need to touch the AWS cloud. The whole integration was created locally thanks to OpenSearchdocker composeandcargo lambda.I believe that the ability to work locally with the short feedback loop helps boost the developer's performance and feels really nice.Next stepsIn the next part, I plan to define IaC and deploy the solution to the AWS.Stay tuned!"}
{"title": "AWS GitHub & S3 Backup", "published_at": 1710090351, "tags": ["aws"], "user": "pablosalas81", "url": "https://dev.to/aws-builders/aws-github-s3-backup-2opg", "details": "Introduction:Data backup and disaster recovery services are critical aspects of protecting a business\u2019s most asset \u2014 its data.Losing this data can result in severe consequences, including financial loss, reputational damage, and operational disruptions. Therefore, it\u2019s essential to understand the importance of data backup and disaster recovery planning and implement effective strategies to safeguard your business\u2019s assets.AWS S3 BACKUPS STORAGE:Always consider both folders:Backup procedures:Github:Docker file:FROM debian:12-slimENV DEBIAN_FRONTEND=noninteractiveRUN apt-get update && \\apt-get install -y \\awscliCOPY script.sh /usr/local/bin/script.shRUN chmod +x /usr/local/bin/script.shCMD [\"/usr/local/bin/script.sh\"]Script:!/bin/bashDirectory pathsSOURCE_DIR=\"/app/storage\"DEST_DIR=\"/app/backup\"TIMESTAMP=$(date \"+%Y-%m-%d\")Ensure destination directory existsmkdir -p \"$DEST_DIR\"Count total directories, excluding lost+foundTOTAL_DIRS=$(find \"$SOURCE_DIR\" -mindepth 1 -maxdepth 1 -type d ! -name 'lost+found' | wc -l)Counter for processed directoriesPROCESSED_DIRS=0RETENTION_PERIOD=13List directories in the source directoryfor dir in \"$SOURCE_DIR\"/*; doif [ -d \"$dir\" ]; then# Get directory namedir_name=$(basename \"$dir\")# Skip the lost+found directory     if [ \"$dir_name\" = \"lost+found\" ]; then         continue     fi      echo \"Processing directory: $dir\"      # Copy directory to destination     cp -r \"$dir\" \"$DEST_DIR/$dir_name\"      # Compress the copied directory     tar -czf \"$DEST_DIR/$dir_name.tar.gz\" -C \"$DEST_DIR\" \"$dir_name\"      # Upload to AWS S3     aws s3 cp /app/backup/${dir_name}.tar.gz s3://${BUCKET}/files/${TIMESTAMP}/${NAMESPACE}/${dir_name}/${dir_name}.tar.gz      # Clean up     rm -rf \"$DEST_DIR/$dir_name\" \"$DEST_DIR/$dir_name.tar.gz\"      # Increment processed directories counter     PROCESSED_DIRS=$((PROCESSED_DIRS+1))      # Calculate and display progress     PROGRESS=$(( (PROCESSED_DIRS * 100) / TOTAL_DIRS))     echo \"Progress: $PROGRESS% ($PROCESSED_DIRS/$TOTAL_DIRS directories processed)\"      # Deleting folders older than retention period     RETENTION_DATE=$(date -d \"${TIMESTAMP} -${RETENTION_PERIOD} days\" \"+%Y-%m-%d\")      # List all date folders     FOLDER_LIST=$(aws s3 ls s3://${BUCKET}/files/ | awk '$0 ~ /PRE/ {print $2}' | grep -E '^[0-9]{4}-[0-9]{2}-[0-9]{2}/' | sed 's/\\/$//')      # Loop through each folder and delete if older than retention period     for folder in $FOLDER_LIST; do         FOLDER_TIMESTAMP=$(date -d \"${folder}\" \"+%s\")         RETENTION_TIMESTAMP=$(date -d \"${RETENTION_DATE}\" \"+%s\")         if [ $FOLDER_TIMESTAMP -lt $RETENTION_TIMESTAMP ]; then             aws s3 rm s3://${BUCKET}/files/${folder}/ --recursive --quiet         fi     done  fiEnter fullscreen modeExit fullscreen modedoneThe backup retention period is 14 days, you can restore back any day you want.YAML file:apiVersion: batch/v1kind: CronJobmetadata:name: backup-filesnamespaces: backupspec:schedule: \"0 3 * * *\"jobTemplate:spec:template:spec:containers:- name: backup-containerimage: ghcr.io/backup/backup-files:latestenv:- name: NAMESPACEvalue: \"backup\"- name: BUCKETvalue: \"nombre del bucket\"- name: AWS_ACCESS_KEY_IDvalueFrom:secretKeyRef:name: aws-secretkey: AWS_ACCESS_KEY_ID- name: AWS_SECRET_ACCESS_KEYvalueFrom:secretKeyRef:name: aws-secretkey: AWS_SECRET_ACCESS_KEYvolumeMounts:- name: my-pvcmounthPath: /app/storagerestartPolicy: OnFailureimagePullSecrets:- name: registry-credentials-backvolumes:- name: my-pvcpersistentVolumeClaim:claimName: backend-upload-storage-pvcbackoffLimit: 4Postgres YAML file:apiVersion: batch/v1kind: CronJobmetadata:name: backup-filesnamespaces: backupspec:schedule: \"0 3 * * *\"jobTemplate:spec:template:spec:containers:- name: backup-containerimage: ghcr.io/backup/backup-files:latestenv:- name: NAMESPACEvalue: \"backup\"- name: BUCKETvalueFrom:secretKeyRef:name: postgres-pguser-backupkey: host- name: PG_PORTvalueFrom:secretKeyRef:name: postgres-pguser-backupkey: host- name: PG_USERvalueFrom:secretKeyRef:name: postgres-pguser-backupkey: user- name: PG_PASSvalueFrom:secretKeyRef:name: postgres-pguser-backupkey: password- name: BUCKETvalue: \"nombre del bucket\"- name: AWS_ACCESS_KEY_IDvalueFrom:secretKeyRef:name: aws-secretkey: AWS_ACCESS_KEY_ID- name: AWS_SECRET_ACCESS_KEYvalueFrom:secretKeyRef:name: aws-secretkey: AWS_SECRET_ACCESS_KEYrestartPolicy: OnFailureimagePullSecrets:- name: registry-credentials-backbackoffLimit: 4Thank you for your time."}
{"title": "Hosting an Angular application in a Docker container on Amazon EC2 deployed by Amazon ECS", "published_at": 1710065867, "tags": ["angular", "docker", "aws", "container"], "user": "Rodrigo Kamada", "url": "https://dev.to/aws-builders/hosting-an-angular-application-in-a-docker-container-on-amazon-ec2-deployed-by-amazon-ecs-2195", "details": "IntroductionIn this article, a WEB application using the latest version ofAngularin a builtDockerimage will be hosted onAmazon EC2(Elastic Compute Cloud) and deployed byAmazon ECS(Elastic Container Service) using anAmazon ECR(Elastic Container Registry) containers repository.PrerequisitesBefore you start, you need to install and configure the tools below to create the Angular application, the Docker image and push the image to the repository on Amazon ECR.git: Git is a distributed version control system and it will be used to sync the repository.Node.js and npm: Node.js is a JavaScript code runtime software based on Google's V8 engine. npm is a package manager for Node.js (Node.js Package Manager). They will be used to build and run the Angular application and install the libraries.Docker Engine: Docker Engine is a command line utility tool for Docker and it will be used to create and run containers.IDE (e.g.Visual Studio CodeorWebStorm): IDE (Integrated Development Environment) is a tool with a graphical interface to help in the development of applications and it will be used to develop the Angular application.AWS CLI: AWS Command Line Interface is a command line utility tool for interacting with all Amazon Web Services services.Getting startedCreate the repository on Amazon ECRAmazon ECRis a Docker containers registry service that enables you store and manage conteiner images.1.Let's create and configure the account. Access the sitehttps://aws.amazon.com/ecr/and click on the buttonGet started with Amazon ECS.2.Now we will click on the optionRoot user, fill in the fieldRoot user email addressand click on the buttonNext.Note:If you don't have an Amazon account, do steps 1 to 9 of the postAuthentication using the Amazon Cognito to an Angular applicationin the sessionCreate and configure the account on the Amazon Cognito.3.Next, we will fill in the fieldPasswordand click on the buttonSign in.4.Then, we will typeecrin the search field and click on the optionElastic Container Registry.5.After click on the optionElastic Container Registry, we will click on the buttonGet started.6.Now we will click on the optionPublic, fill in the fieldRepository nameand click on the buttonCreate repository.7.Next, we will click on the created repository and click on the buttonView push commands.8.Then, we will copy the commands because they will be used to push the Docker image of the Angular application and click on the buttonClose.9.Ready! The repository to store the Docker image of the Angular application was created.Push the Docker image of the Angular application1.Let's user the Angular application created in the postCreating and running an Angular application in a Docker container. We will clone the repositoryhttps://github.com/rodrigokamada/angular-docker.gitclonehttps://github.com/rodrigokamada/angular-dockerangular-docker-amazon-ecsEnter fullscreen modeExit fullscreen mode2.Now we will change the fileDockerfilewith the content below.FROMnode:alpineASappWORKDIR/usr/src/appCOPY./usr/src/appRUNnpminstallRUNnpmrunbuildFROMnginx:alpineCOPY--from=app/usr/src/app/dist/angular-docker/browser/usr/share/nginx/htmlRUNls/usr/share/nginx/htmlCOPY./nginx.conf/etc/nginx/conf.d/default.confEXPOSE80Enter fullscreen modeExit fullscreen modeNotes:TheFROM node:alpine AS appsetting defines the base Docker image of Node.js to build the Angular application.TheWORKDIR /usr/src/appsetting defines the default application directory. The defined directory is created if it does not exist.TheCOPY . /usr/src/appsetting copies the local application files and directories to the defined directory.TheRUN npm installsetting installs the Angular application dependencies.TheRUN npm run buildsetting compiles the Angular application.TheFROM nginx:alpinesetting defines the base Docker image of NGINX to serve the Angular application files.TheCOPY --from=app /usr/src/app/dist/angular-docker/browser /usr/share/nginx/htmlsetting copies the compiled Angular application files to the NGINX directory.TheCOPY ./nginx.conf /etc/nginx/conf.d/default.confsetting copies the NGINX configuration file to serve the Angular application.TheEXPOSE 80setting defines the Docker port. NGINX uses the default port 80.3.Next, we will create the filenginx.confwith the NGINX configuration in the root directory of the Angular application.touchnginx.confEnter fullscreen modeExit fullscreen mode4.Then, we will configure the filenginx.confwith the content below.server{listen80;location/{root/usr/share/nginx/html;indexindex.htmlindex.htm;try_files$uri$uri//index.html=404;}}Enter fullscreen modeExit fullscreen mode5.After configure the filenginx.conf, we will authenticate the Docker client with the repository registry.awsecr-publicget-login-password--regionus-east-1|dockerlogin--usernameAWS--password-stdinpublic.ecr.aws/d3o6w2v8LoginSucceededEnter fullscreen modeExit fullscreen mode6.Now we will create the Docker image.dockerbuild-tangular-docker-amazon-ecs.[+]Building45.5s(10/10)FINISHEDdocker:default=>[internal]loadbuilddefinitionfromDockerfile0.1s=>=>transferringdockerfile:191B0.0s=>[internal]loadmetadatafordocker.io/library/node:alpine2.4s=>[internal]load.dockerignore0.1s=>=>transferringcontext:2B0.0s=>[1/5]FROMdocker.io/library/node:alpine@sha256:d3271e4bd89eec4d97087060fd4db0c238d9d22fcfad090a73fa9b51286998884.5s=>=>resolvedocker.io/library/node:alpine@sha256:d3271e4bd89eec4d97087060fd4db0c238d9d22fcfad090a73fa9b51286998880.0s=>=>sha256:6af33fd59f0638fb0acc2992b2b54c7baf68fe593083f0f52815cf29d12888037.13kB/7.13kB0.0s=>=>sha256:4abcf20661432fb2d719aaf90656f55c287f8ca915dc1c92ec14ff61e67fbaf83.41MB/3.41MB1.0s=>=>sha256:2997c41553473c7c926a796f330b4a7b03e9d2a7a9ee059a66bf68e02040bf4043.53MB/43.53MB2.0s=>=>sha256:803074618b54a85228c5e10d79b5320e5eba82a2f89abddf233e852420430ba22.37MB/2.37MB1.0s=>=>sha256:d3271e4bd89eec4d97087060fd4db0c238d9d22fcfad090a73fa9b51286998881.43kB/1.43kB0.0s=>=>sha256:f5d3a6aea1b1d35066e6c034f5c264cd5b051fc7c7cb0160bb88899e7b1f0c831.16kB/1.16kB0.0s=>=>extractingsha256:4abcf20661432fb2d719aaf90656f55c287f8ca915dc1c92ec14ff61e67fbaf80.1s=>=>sha256:249f9271d1d17cdc2d12d106e2dcfafdc306da21f65123a06aa2290baa5c4fac451B/451B1.4s=>=>extractingsha256:2997c41553473c7c926a796f330b4a7b03e9d2a7a9ee059a66bf68e02040bf401.7s=>=>extractingsha256:803074618b54a85228c5e10d79b5320e5eba82a2f89abddf233e852420430ba20.3s=>=>extractingsha256:249f9271d1d17cdc2d12d106e2dcfafdc306da21f65123a06aa2290baa5c4fac0.0s=>[internal]loadbuildcontext0.1s=>=>transferringcontext:683.88kB0.0s=>[2/5]WORKDIR/usr/src/app0.3s=>[3/5]COPY./usr/src/app0.1s=>[4/5]RUNnpminstall-g@angular/cli20.1s=>[5/5]RUNnpminstall13.8s=>exportingtoimage4.0s=>=>exportinglayers4.0s=>=>writingimagesha256:e6b87cf9530701fe49f487780093e6bd0e87c2052147750debf8fdf3e65448510.0s=>=>namingtodocker.io/library/angular-docker-amazon-ecsEnter fullscreen modeExit fullscreen mode7.Next, we will create the image tag.dockertagangular-docker-amazon-ecs:latestpublic.ecr.aws/d3o6w2v8/angular-docker-amazon-ecs:latestEnter fullscreen modeExit fullscreen mode8.Then, we will push the Docker image to the repository.dockerpushpublic.ecr.aws/d3o6w2v8/angular-docker-amazon-ecs:latestThepushreferstorepository[public.ecr.aws/d3o6w2v8/angular-docker-amazon-ecs]0d182ddaeba2:Pushed692f273c8005:Pushedf19fda91d840:Pushed118d5d6c8a47:Pushed6ac923e38b05:Pushed18ffdcddf862:Pushedaab2cca0cf91:Pushedd4fc045c9e3a:Pushedlatest:digest:sha256:2f09379b02c2669c915fc63116536d1bb5040c3d608a68b8f3306f10af4d125dsize:2000Enter fullscreen modeExit fullscreen modeNote:The AWS CLI user must have the permissionAmazonElasticContainerRegistryPublicFullAccess.9.After push the Docker image to the repository, we will check if the Docker image was pushed to the repository. We will go back to the site and click on the repository name.10.Now we will see the Docker image pushed.11.Ready! The Docker image of the Angular application was pushed to the repository on Amazon ECR.Create the conteiner on Amazon ECSAmazon ECSis a Docker containers orchestration service that enables you deploy, manage and scale containerized applications.1.Let's typeecsin the search field and click on the optionElastic Container Service.2.Now we will click on the buttonCreate cluster.3.Next, we will fill in the fieldCluster name, click on the optionAmazon EC2 instances, select the optionAmazon Linux 2in the fieldOperating system/Architecture, select the optionc1.mediumin the fieldEC2 instance type, fill in the fieldMaximum, select the optionTurn onin the fieldAuto-assign public IPand click on the buttonCreate.4.Then, we will click on the menuTask definitions.5.After click on the menuTask definitions, we will click on the buttonCreate new task definition.6.Now we will fill in the fieldTask definition family, click on the optionAmazon EC2 instances, select the optiondefaultin the fieldNetwork mode, fill in the fieldCPU, fill in the fieldMemory, fill in the fieldNamewith the tag name, fill in the fieldImage URIwith the Docker image name, fill in the fieldHost portwith the external port, fill in the fieldContainer portwith the NGINX port and click on the buttonCreate.7.Next, we will click on the buttonDeployand click on the optionRun task.8.Then, we will click on the buttonCreatebecause the fieldsExisting cluster,FamilyandRevisionwas filled in.9.After click on the buttonCreate, we will wait for the EC2 instance to be created in the sessionTasks.10.Now we will typeec2in the search field and click on the optionEC2.11.Next, we will click on the menuInstances.12.Then, we will click on the created instance.13.After click on the created instance, we will click on the icon to copy the domain inPublic IPv4 DNS.14.Now we will access the URLhttp://ec2-54-146-190-174.compute-1.amazonaws.comin a browser.15.Ready! The Docker image of the Angular application was deployed and the application is working.ConclusionSummarizing what was covered in this article:We created a public repository on Amazon ECR.We changed the Docker configuration of the Angular application.We created the NGINX configuration file in the Angular application.We created a Docker image.We pushed the Docker image to the repository.We created a container on Amazon ECS for the deployment.We tested the Angular application inside the Docker container served on EC2 instance.You can use this article to create a Docker container with an Angular application image, push the image to a repository on Amazon ECR and deploy the container using Amazon ECS to an EC2 instance where the Angular application will be served.Thank you for reading and I hope you enjoyed the article!This tutorial was posted on myblogin portuguese.To stay updated whenever I post new articles, follow me onTwitterandLinkedIn."}
{"title": "Format and Parse Amazon S3 URL", "published_at": 1710051424, "tags": ["webdev", "javascript", "typescript", "programming"], "user": "Chris Cook", "url": "https://dev.to/aws-builders/format-and-parse-amazon-s3-url-5e10", "details": "Amazon S3 URLs come in different flavors. There are those starting withs3:,http:, orhttps:. Then, there are the ones withs3.amazonaws.com,s3.us-east-1.amazonaws.com, or evens3-us-west-2.amazonaws.com(note the dash instead of the dot between s3 and the region code). And where do you put the bucket: is it<bucket>.s3.us-east-1.amazonaws.com/<key>ors3.us-east-1.amazonaws.com/<bucket>/<key>? And when it comes to static website hosting, of course, there is also<bucket>.s3-website-us-east-1.amazonaws.comand<bucket>.s3-website-us-east-1.amazonaws.com(again, note the dash and the dot).There are even more when you include the dual-stack, FIPS, access point, and S3 control endpoints. Here's the full list ofAmazon S3 endpoints. But for this post, I will focus on the more common URLs that I mentioned before.GlobalThe global URL has the simplest format with the following structure:s3://<bucket>/<key>. This URL is also displayed by the AWS management console.Path-style vs. Virtual-hosted-styleThe difference betweenpath-styleandvirtual-hosted-styleURLs is how the bucket name is included in the URL. Path-style URLs have the bucket name in the pathname of the URL:https://s3.<region>.amazonaws.com/<bucket>/<key>Enter fullscreen modeExit fullscreen modeOn the other hand, virtual-hosted-style URLs have the bucket name in the hostname of the URL:https://<bucket>.<region>.s3.amazonaws.com/<key>Enter fullscreen modeExit fullscreen modeHaving the bucket name in the host has the advantage of using DNS to route different buckets to different IP addresses. If the bucket name is in the path, all requests have to go to one IP address even for different buckets. That is the reason path-style URLs are deprecated, and support for this style was supposed to end in 2020, but AWS changed their plan and continues to support this style for buckets created on or before September 30, 2020. There's an interesting blog post about the background:Amazon S3 Path Deprecation Plan \u2013 The Rest of the StoryLegacy vs. RegionalSome regions like US East (N. Virginia)us-east-1have a legacy global endpoint that doesn't need a region code in the hostname:# Legacy hostname with path-style https://s3.amazonaws.com/<bucket>/<key> # Legacy hostname with virtual-hosted-style https://<bucket>.s3.amazonaws.com/<key>Enter fullscreen modeExit fullscreen modeIf you use this type of URL for other regions that don't support it, you might either get anHTTP 307 Temporary Redirector, in the worst case, anHTTP 400 Bad Requesterror, depending on when the bucket was created.AWS recommends always using the regional endpoints with the region code in the hostname:# Regional hostname with path-style https://s3.<region>.amazonaws.com/<bucket>/<key> # Regional hostname with virtual-hosted-style https://<bucket>.<region>.s3.amazonaws.com/<key>Enter fullscreen modeExit fullscreen modeDot-style vs. Dash-styleBut also here is a caveat: some regions used to have a dash-instead of a dot.betweens3and<region>:# Dot-style https://s3.<region>.amazonaws.com/<bucket>/<key> # Dash-style https://s3-<region>.s3.amazonaws.com/<bucket>/<key>Enter fullscreen modeExit fullscreen modeFor example, the US West (Oregon)us-west-2region would support the legacy dash-style URL likehttps://s3-us-west-2.amazonaws.com/<bucket>/<key>. Nevertheless, the standard formathttps://s3.us-west-2.amazonaws.com/<bucket>/<key>is also available for these outliers.REST vs. WebsiteAll the URL formats we have seen so far, except the global S3 URL, are called REST endpoints. They are hosted on either thes3.amazonaws.comors3.<region>.amazonaws.comhostname, but more importantly, they support secure HTTPS connections. That means all these URLs work withhttps://as the protocol.Amazon S3 also has a website endpoint for static website hosting. The website endpoint does not support HTTPS, only HTTP. These URLs have the following formats:# Website hostname with dot-style http://<bucket>.s3-website.<region>.amazonaws.com/<key> # Website hostname with dash-style http://<bucket>.s3-website-<region>.amazonaws.com/<key>Enter fullscreen modeExit fullscreen modeAgain, depending on the region, there is a dash-or a dot.separatings3-websiteand<region>. To see which one is right for your region, you have to check the list ofAmazon S3 website endpoints.Format and Parse S3 URLsDepending on how you interact with Amazon S3, you might use one of the previous URLs. For example, the AWS CLI for S3 expects the S3 URL in the global formats3://<bucket>/<key>. Other clients and SDKs probably use the regional REST endpoint with the bucket name either in the hostname or pathname.If you're using the wrong format or endpoint, you might get an error like this:com.amazonaws.services.s3.model.AmazonS3Exception: The bucket is in this region: eu-west-1. Please use this region to retry the request (Service: Amazon S3; Status Code: 301; Error Code: PermanentRedirect;)Enter fullscreen modeExit fullscreen modeThe right URL really depends on the individual client and how it is requesting from S3. To lift some of this burden, I created a tiny JavaScript library to check, format, and parse S3 URLs in the various formats I described earlier.zirkelc/amazon-s3-urlFormat and parse Amazon S3 URLAmazon S3 URL Formatter and ParserThis small and dependency-free library is designed to help you check, format and parse Amazon S3 URLs Please note that this library does only rudimentary URL validation on the structure of the URL. It currently does not validatebucket namesandobject keysagainst the rules defined in the AWS documentation.Amazon S3 URL FormatsAmazon S3 supports a combination of different styles:Virtual-hosted-style and Path-styleThe difference between these two styles is how the bucket name is included in the URL, either as part of the hostname or as part of the pathname.Virtual-hosted-style URLs have the bucket name as part of the host:<bucket>.s3.amazonaws.com/<key>Path-style URLs have the bucket name as part of the path, e.g.s3.amazonaws.com/<bucket>/<key>WarningPath-style URLs will be discontinued in the future SeeAmazon S3 backward compatibilityfor more information.Regional and LegacyThe difference between these two\u2026View on GitHubAt the moment, the library exports only three functions:formatS3Url,parseS3Url, andisS3Url.import{formatS3Url,parseS3Url,isS3Url,S3Object}from'amazon-s3-url';/* Types */typeS3UrlFormat=|\"s3-global-path\"|\"s3-legacy-path\"|\"s3-legacy-virtual-host\"|\"https-legacy-path\"|\"https-legacy-virtual-host\"|\"s3-region-path\"|\"s3-region-virtual-host\"|\"https-region-path\"|\"https-region-virtual-host\";typeS3Object={bucket:string;key:string;region?:string;};/* Signatures */functionformatS3Url(s3Object:S3Object,format?:S3UrlFormat):string;functionparseS3Url(s3Url:string,format?:S3UrlFormat):S3Object;functionisS3Url(s3Url:string,format?:S3UrlFormat):boolean;/* Examples */// Global path// Without format param (defaults to s3-global-path)formatS3Url({bucket:'bucket',key:'key'});parseS3Url('s3://bucket/key');isS3Url('s3://bucket/key');// Legacy path-style// With format param for explicit formatting and parsingformatS3Url({bucket:'bucket',key:'key'},'https-legacy-path');parseS3Url('https://s3.amazonaws.com/bucket/key','https-legacy-path');isS3Url('https://s3.amazonaws.com/bucket/key','https-legacy-path');// Regional virtual-hosted-style// With region property for regional endpointsformatS3Url({region:'us-west-1',bucket:'bucket',key:'key'},'https-region-virtual-host');parseS3Url('https://bucket.s3.us-west-1.amazonaws.com/key','https-region-virtual-host');isS3Url('https://bucket.s3.us-west-1.amazonaws.com/key','https-region-virtual-host');Enter fullscreen modeExit fullscreen modeLimitationsThe library does only rudimentary URL validation on the structure of the URL, but it doesn't validate the bucket name, object keys, and regions. Also, it doesn't support the dual-stack, FIPS, access point, control, and website endpoints yet. But I'm happy to welcome any external contribution."}
{"title": "GenLearn - Your Personalized Learning Assistant!", "published_at": 1710048359, "tags": ["genai", "bedrock", "partyrock", "llm"], "user": "Aravind V", "url": "https://dev.to/aws-builders/genlearn-your-personalized-learning-assistant-56l", "details": "\ud83d\ude80 Introducing: GenLearn \u2013 Your Personalized Learning Assistant! \ud83d\udcdaBuilt on PartyRock Playground during #partyrock-hackathonGenLearn - Your Personalised Learning Assistant!Get Started with GenLearn today and unlock your full learning potential! \ud83c\udf10App linkhttps://partyrock.aws/u/AravindVCyber/2N3IdneP6/GenLearn-Your-Personalised-Learning-Assistant!Snapshothttps://partyrock.aws/u/AravindVCyber/2N3IdneisP6/GenLearn-Your-Personalised-Learning-Assistant!/snapshot/qDBzjmXhWTry some experiment with this and share your snapshots or create your own app with PartyrockPost in Community.AWSCommunity | GenLearn - Your Personalised Learning Assistant!GenLearn helps users learn, summarise, and present any topic with presentation slide notes in language of choice with links to Search, YouTube, Wikipedia, Images to unlock learning possibilitiescommunity.awsDo like and share my hackathon submission in DEVPOSTGenLearn - Personalised Learning Assistant | DevpostGenLearn helps users learn, summarise, and present any topic with presentation slide notes in language of choice with links to Search, YouTube, Wikipedia, and Images to unlock learning possibilitiesdevpost.comGenLearnGenLearn helps users learn, summarise, and present any topic with presentation slide notes in language of choice with links to Search, YouTube, Wikipedia, and Images to unlock learning possibilitiesAre you tired of spending countless hours scouring through textbooks and online resources to grasp a concept or prepare a presentation? Look no further! GenLearn is here to revolutionize your learning experience.Say goodbye to tedious research and hello to efficient, personalised learning with GenLearn. Join us in shaping the future of education and empowerment. Let's learn smarter, together! \ud83c\udf1fProblem Addressed:In today's fast-paced world, individuals often struggle to find the time and resources needed to effectively learn and present information. Traditional methods of learning can be time-consuming and inefficient, leaving learners feeling overwhelmed and uninspired.Solution:GenLearn offers a comprehensive solution to this problem by leveraging generative technologies to create personalised learning materials and presentation slide notes. By harnessing the power of AI, users can quickly access curated content tailored to their learning preferences, saving time and effort.Key Features:Personalised Learning: Tailored learning materials and presentation slide notes on any topic.Multi-Language Support: Content available in your preferred language for global accessibility.Actionable Links: Direct access to Google Search, YouTube, Wikipedia, and Images for further exploration.Efficiency: Streamlined learning process, allowing users to focus on understanding and application rather than searching for information.Assistant: Bot is ready to clarify further on the topic requestedInteresting Hacks:I also used some simple hacks to let the LLM know how to format the search links under different languages, and the results are amazing, with a few snippets of code like the below the LLM is able to understand how to generate linksimportwikipediadefgenerate_google_search_link(query):returnf\"https://www.google.com/search?q={'+'.join(query.split())}\"defgenerate_google_image_search_url(query):base_url=\"https://www.google.com/search\"params={\"tbm\":\"isch\",\"q\":query}url=base_url+\"?\"+\"&\".join([f\"{key}={value}\"forkey,valueinparams.items()])returnurldefgenerate_youtube_links(query,num_links=1):youtube_links=[]foriinrange(1,num_links+1):youtube_links.append(f\"[YouTube Video{i}](https://www.youtube.com/results?search_query={'+'.join(query.split())}&page={i})\")returnyoutube_linksdefgenerate_wikipedia_links(query,num_links=1):wikipedia_links=[]search_results=wikipedia.search(query,results=num_links)forresultinsearch_results:page_title=wikipedia.page(result).titlewikipedia_links.append(f\"[{page_title}](https://en.wikipedia.org/wiki/{'_'.join(page_title.split())})\")returnwikipedia_linksdefgenerate_combo_links(query,google_links=1,youtube_links=1,wikipedia_links=1):google_links=generate_google_search_link(query)google_images_link=generate_google_image_search_url(query)youtube_links=generate_youtube_links(query,youtube_links)wikipedia_links=generate_wikipedia_links(query,wikipedia_links)combo_links={\"Google Search\":google_links,\"Google Images\":google_images_link,\"YouTube Videos\":youtube_links,\"Wikipedia Pages\":wikipedia_links}returncombo_linksEnter fullscreen modeExit fullscreen modesuggested readings as hyperlinksNotes on the developmental journey for the hackathon.InspirationIn today's fast-paced world, individuals often struggle to find the time and resources needed to effectively learn and present information. Traditional methods of learning can be time-consuming and inefficient, leaving learners feeling overwhelmed and uninspired.What it doesWhether you're a student, a professional, or a lifelong learner, GenLearn adapts to your needs, providing concise summaries and engaging content in your preferred language.But that's not all! With actionable links to Google Search, YouTube, Wikipedia, and Images, you'll have a wealth of additional resources at your fingertips, enriching your learning journey like never before.Say goodbye to tedious research and hello to efficient, personalised learning with GenLearn. Join us in shaping the future of education and empowerment. Let's learn smarter, together! \ud83c\udf1fHow we built itOur app utilises cutting-edge generative solutions to create tailored learning materials and presentation slide notes on any topic you desire.We have used Amazon partyrock playground to build this app, which helped us with easy access to generative llm models like Claude and Stability AIChallenges we ran intoWhile connecting widget, we occasionally hand trouble in getting the required out in relevant format and under the current scope, eventually we iterated this using some prompt engineering to get around with the solution.Had some challenges in getting the hyperlinks and eventually we mastered getting the links with the right modelAccomplishments that we're proud ofI think this is only the tip of the iceberg, if we could do this and help students with an interactive experience for students to learn and personalise the content in their language and summaries it with partyrock, it opens a lot of possibilities.What we learnedWhen we started it is was giving output more similar to a usual ChatGpt query, but when we linked the various widgets, we managed to get a interactive experience. More over we started with text outputs, and later converted it to markdown and ensured to use emojis, to make the students feel cool. Besides which we went ahead and try to generate web search link to help student explore beyond and help opened the generated content to reference. Language translation would also be a great experience for studentsWhat's next for Personalised Learning AssistantI believe we could generate actionable links could help students reference and explore wide content from the web.At the same time they could also be used to track user events to further interact with the playgroundAlso currently the playground does not support additional input component types, and input files/video. This could open more possibilities to make the interactive experience coolIt could as well in future support audio or video playback, and it could really make the learning experience engagingAlso the app is still using open pre-learned material, hopefully in future we could use RAG feeds to help industrial use cases make similar playground to benefits their users with relevant and appropriate data, more personalisation.Other Features and Images:Fun Facts for Users on TopicEasy summary in the form of slides for presentation or notessummarised notes in the form of slides 1summarised notes in the form of slides 2Brief explanation on the topic in the requested languageCasual image generation based on topicGet Started with GenLearn today and unlock your full learning potential! \ud83c\udf10App linkhttps://partyrock.aws/u/AravindVCyber/2N3IdneP6/GenLearn-Your-Personalised-Learning-Assistant!Snapshothttps://partyrock.aws/u/AravindVCyber/2N3IdneisP6/GenLearn-Your-Personalised-Learning-Assistant!/snapshot/qDBzjmXhWFeel free to share any constructive feedback and reach me at twitter @Aravind_V7Do like and share my hackathon submission in DEVPOSThttps://devpost.com/software/personalised-learning-assistant-5vr28l"}
{"title": "How to move an Amazon RDS DB instance from an Amazon Virtual Private Cloud (Amazon VPC) to a new VPC", "published_at": 1710025944, "tags": ["rds", "vpc", "securitygroups", "subnets"], "user": "Revathi Joshi", "url": "https://dev.to/aws-builders/how-to-move-an-amazon-rds-db-instance-from-an-amazon-virtual-private-cloud-amazon-vpc-to-a-new-vpc-2gp2", "details": "In this article, I am going to show you how to migrate an Amazon Relational Database Service (Amazon RDS) DB instance from one virtual private cloud (VPC) to another in the same AWS account. This is useful especially when you want to place your application stack and database in different VPCs for security reasons.But this sort of approach has few limitations.When you move the RDS DB instance to a new network and configure the new VPC, the DB instance reboots. So, change the VPC during a planned change window that is outside the RDS weekly maintenance window.The DB instance you\u2019re migrating must be a single instance with no standby. It must not be a member of a cluster.Amazon RDS must not be in multiple Availability Zones. Convert the DB instance to a single AZ, and then convert it back to a Multi-AZ DB instance after moving to the new VPC.Amazon RDS must not have any read replicas. Remove the read replicas, and then add read replicas after the DB instance is moved to the new VPC.The subnet group created in the target VPC must have subnets from the Availability Zone where the source database is running. If the AZs are different, then the operation fails.Let\u2019s get started!Please visit myGitHub Repository for RDS articlesandGitHub Repository for VPC articleson various topics being updated on constant basis.Objectives:1.Create RDS Database Instance2.Create a new VPC, Subnets, Route Tables, VPC Security Group3.Create a new DB subnet group4.Modify the Amazon RDS DB instance to use the new DB subnet groupPre-requisites:AWS user account with admin access, not a root account.an IAM role with permissions required for the VPC, subnets, and Amazon RDS consoleResources Used:Amazon RDS documentationAWS IAM DocumentationAmazon VPC documentationSteps for implementation to this project:1. Create RDS Database Instance12345678910111213141516172. Create a new VPC, Subnets, Route Tables, VPC Security GroupCreate a new VPC123Create new Subnets123Create subnetCreate a new Route Table1234Create a new VPC Security Group123Create security group3. Create a new DB subnet group.12Create4. Modify the Amazon RDS DB instance to use the new DB subnet group1234567891011Continue121314When the migration to the target VPC is complete, the target VPC's default security group is assigned to the Amazon RDS DB instance.1516choose Events in the left navigation pane.Confirm that the process moved the DB instance to the target VPC17Cleanupdelete RDS instancedelete VPC and its componentsWhat we have done so farI have successfully moved an Amazon RDS DB instance to a new VPC, by first changing its subnet group."}
{"title": "Analyze AWS Application Load Balancer logs using Amazon Athena", "published_at": 1710019884, "tags": [], "user": "Yasitha Bogamuwa", "url": "https://dev.to/aws-builders/analyze-aws-application-load-balancer-logs-using-amazon-athena-nhp", "details": "1. IntroductionThe AWS Application Load Balancer logs show details about the requests sent to your ALB, like where they came from, what was requested, and if it was successful. These logs help with fixing problems and understanding how well your system is working.I recently wanted to analyze ALB access logs to identify suspicious activity. However, the logs are stored in gzip format in an S3 bucket with hundreds of thousands of files, making manual analysis impractical. To analyze these logs, you can use Amazon Athena, a tool that lets you query data stored in Amazon S3 using regular SQL commands.2. What is Amazon AthenaAmazon Athena is an interactive query service provided by AWS that allows you to analyze and query data stored in Amazon S3 using standard SQL. It enables you to run ad-hoc queries on data in S3 without the need for complex ETL processes or data movement.Since Athena is serverless, you don't need to handle any infrastructure, and you are charged solely based on the queries you execute.3. Analyzing ALB Access Logs with Athena3.1. Make sure to enable Application Load Balancer access logs as describedhereso that the access logs can be saved to your Amazon S3 bucket.3.2. Open the Athena console and clickLaunch query editor.3.3. Create an Athena database and table for Application Load Balancer logs. To create an Athena database, please run the following command in Query Editor. It's recommended to create the database in the same AWS Region as the Amazon S3 bucket.CREATEDATABASE<DATABASE_NAME>Enter fullscreen modeExit fullscreen mode3.4. Then, select the database from the dropdown and create analb_logstable for the ALB logs. Make sure to replace the<YOUR-ALB-LOGS-DIRECTORY>,<ACCOUNT-ID>, and<REGION>with the correct values.CREATEEXTERNALTABLEIFNOTEXISTSalb_logs(typestring,timestring,elbstring,client_ipstring,client_portint,target_ipstring,target_portint,request_processing_timedouble,target_processing_timedouble,response_processing_timedouble,elb_status_codeint,target_status_codestring,received_bytesbigint,sent_bytesbigint,request_verbstring,request_urlstring,request_protostring,user_agentstring,ssl_cipherstring,ssl_protocolstring,target_group_arnstring,trace_idstring,domain_namestring,chosen_cert_arnstring,matched_rule_prioritystring,request_creation_timestring,actions_executedstring,redirect_urlstring,lambda_error_reasonstring,target_port_liststring,target_status_code_liststring,classificationstring,classification_reasonstring)ROWFORMATSERDE'org.apache.hadoop.hive.serde2.RegexSerDe'WITHSERDEPROPERTIES('serialization.format'='1','input.regex'='([^ ]*) ([^ ]*) ([^ ]*) ([^ ]*):([0-9]*) ([^ ]*)[:-]([0-9]*) ([-.0-9]*) ([-.0-9]*) ([-.0-9]*) (|[-0-9]*) (-|[-0-9]*) ([-0-9]*) ([-0-9]*)\\\"([^ ]*) (.*) (- |[^ ]*)\\\"\\\"([^\\\"]*)\\\"([A-Z0-9-_]+) ([A-Za-z0-9.-]*) ([^ ]*)\\\"([^\\\"]*)\\\"\\\"([^\\\"]*)\\\"\\\"([^\\\"]*)\\\"([-.0-9]*) ([^ ]*)\\\"([^\\\"]*)\\\"\\\"([^\\\"]*)\\\"\\\"([^ ]*)\\\"\\\"([^\\s]+?)\\\"\\\"([^\\s]+)\\\"\\\"([^ ]*)\\\"\\\"([^ ]*)\\\"')LOCATION's3://<YOUR-ALB-LOGS-DIRECTORY>/AWSLogs/<ACCOUNT-ID>/elasticloadbalancing/<REGION>/';Enter fullscreen modeExit fullscreen mode3.5. In the Query Editorsettings, choose an S3 bucket to store the results of your Athena queries.3.6. Now you can use SQL syntax to query the access logs.The following SQL query in counts the occurrences of different request verbs for requests containing 'hs' in the URL from the 'alb_logs' table. It groups the results by request verb, client IP, and request URL, and limits the output to the first 100 results.SELECTCOUNT(request_verb)AScount,request_verb,client_ip,request_urlFROMalb_logsWHERErequest_urlLIKE'%hs%'GROUPBYrequest_verb,client_ip,request_urlLIMIT100;Enter fullscreen modeExit fullscreen mode4. References4.1.Querying Application Load Balancer logs4.2.Analyzing ALB Access Logs with Amazon Athena4.3.How do I use Amazon Athena to analyze ALB access logs"}
{"title": "Smart Chaos: LLMs, No More Human Modeling", "published_at": 1710009471, "tags": ["chaosengineering", "awspartyrock", "sre", "genai"], "user": "Indika_Wimalasuriya", "url": "https://dev.to/aws-builders/smart-chaos-llms-no-more-human-modeling-28fc", "details": "Modern enterprise distributed systems are very complex in nature, and, of course, it's hard to manage them too. This complexity arises from various issues, including those in cloud hardware, cloud networking services, databases, serverless or complex compute layers, and caching layers. Operating such a setup is highly challenging, and failures can occur at any or all logical layers. Sometimes, failures can happen in combination with other errors. Bugs may manifest long after changes are propagated to production. What's worse is that bugs may propagate to other layers too. Problems tend to exacerbate at higher levels of the system due to recursion.If you look at the history, even the largest companies like Facebook have had their share of issues. In 2021, Fastly had an outage that impacted Amazon, eBay, Reddit, Spotify, Twitch, The Guardian, and The New York Times. The reason for this is that traditional testing is falling short.The testing verifies the known, but in the dance with failures, the steps are often unrehearsed and spontaneous. The inability to test these unknowns is the greatest risk systems are carrying.The answer for this is Chaos Engineering. Chaos engineering is able to identify weak points in the systems, which helps us to:Develop cost-effective failover and restoration solutionsObserve how systems respond to real-world eventsBuild confidence against failureImprove recovery time (MTTR)Identify weaknesses and fix them proactivelyPrepare and educate SREsThe emergence of Generative AI has enabled us to find an innovative solution for this problem. Generative AI refers to machine learning models that can create new, original content like text, images, audio, and more.Two of the most prominent Generative AI use cases are Text generation and Image generation. Let's park this thought for a moment.Chaos Engineering is the discipline of experimenting on a distributed system to build confidence in its ability to withstand turbulent conditions in the target environment. Let's see the key components of the chaos engineering workflow.Below are the key processes of the Chaos Engineering workflow.One thing to highlight is, parallel to your chaos experiments, you need to measure everything by enabling observability.Before we go further, feel free to check the app i developed -Smart Chaos powered by AWSNow let's see how we can leverage GenAI in Chaos Engineering, going through each and every process in the workflowDiscovery Phase:Leverage GenAI for anomaly detection in historical data, uncovering potential weaknesses or areas of interest.Dependency Analysis:Utilize GenAI to analyze system dependencies, identifying points of failure or vulnerabilities in the architecture.Steady State Definition:Train GenAI models to predict performance metrics, aiding in defining the system's steady state.Hypothesis Creation:Implement GenAI for automated hypothesis generation.Experiment Design:Use GenAI for scenario simulation, assisting in designing controlled experiments and optimizing testing efficiency.Blast Radius Definition:Utilize GenAI to assess the potential blast radius of chaos experiments, considering dependencies and system topology.Rumsfeld Matrix - \"Known Knowns, Known Unknowns, and Unknown Unknowns\":Leverage GenAI for data analysis to categorize knowns and unknowns, aiding in identifying potential unknown unknowns.Monitoring and Analysis:Integrate GenAI models into real-time monitoring for anomaly detection during chaos experiments and root cause analysis.Documentation and Reporting:Employ GenAI for automated reporting, summarizing chaos experiment outcomes and consolidating insights.Iterative Improvement:Establish a continuous feedback loop with GenAI, allowing for adaptive experimentation and improved chaos engineering approaches.Now, that's what I think a comprehensive solution would look like. Let me walk you through what I have done with the AWS PartyRock.Smart Chaos Apps use LLMs to automate the Chaos Engineering workflow:Discovery Phase:Refer to the architecture diagram or service map to discover services using GenAI.Dependency Analysis:Utilize GenAI to analyze system dependencies, identifying points of failure or vulnerabilities in the architecture.Steady State Definition:Utilize GenAI for defining the system's steady state.Hypothesis Creation:Utilize GenAI for hypothesis generation.Experiment Design:Use GenAI for scenario/experiment design simulation.Blast Radius Definition:Utilize GenAI to assess the potential blast radius of chaos experiments, considering dependencies and system topology.Rumsfeld Matrix - \"Known Knowns, Known Unknowns, and Unknown Unknowns\":Leverage GenAI for data analysis to categorize knowns and unknowns, aiding in identifying potential unknown unknowns.Smart Chaos Chatbot:Leverage GenAI-based chatbot to improve the entire process (continuous improvement).Let me walk through what it is doing:Users can input a link to the System Architecture Diagram or Service Map details.Dependency Analysis:LLM analyzes the provided architecture diagram, identifying key system dependencies for potential chaos testing.Steady State Definition:LLM examines the architecture diagram, performs Dependency Analysis, and generates the top 10 Steady State Definitions for chaos testing.Hypothesis Generation:LLM utilizes the architecture diagram, Dependency Analysis, and Steady State Definitions to automatically generate hypotheses supporting chaos testing.Experiment Design:LLM considers the architecture diagram, Steady State Definitions, and Hypothesis Generation to create Chaos testing test cases and an experiment list.Rumsfeld Matrix - Known Unknown:LLM reviews the architecture diagram and lists the Rumsfeld Matrix \u2013 Known Unknown for chaos testing.Test Case:Users can input one of the generated Test Cases.Blast Radius:LLM refers to the Test Case and the architecture diagram, generating the Blast Radius for the specified Test Case during chaos testing.Chat Bot:Having the ability to provide additional on-demand assistanceThe main challenges I faced were:The primary challenge was determining the amount of data to feed to LLM. Although LLM performed well, I decided to simplify the process by allowing users to provide either an architecture diagram or a runtime service map. While additional data such as observability data could provide better details beyond service discovery, I opted for simplicity.What are the best LLMs for this work?There are a lot of LLM options available, and let's try to find out which LLMs are suitable for each use caseDiscovery Phase- Claude: Claude excels at analyzing and understanding architecture diagrams and service maps. Its visual comprehension skills make it ideal for service discovery.Dependency Analysis- Jurassic-2: Jurassic-2's capabilities in highly technical and logical analysis of systems architecture make it well-suited for analyzing dependencies.Steady State Definition- Claude's: Advanced natural language generation capabilities and technical knowledge base make it well-suited for automatically generating potential steady state definitions during chaos engineering experiments.Hypothesis Creation- Command: Command's conversational nature lends itself to rapid hypothesis ideation and iteration.Experiment Design- Claude LLM: Claude's safety-focused capabilities help design chaos experiments that minimize the blast radius.Blast Radius Definition- Jurassic-2: Jurassic-2's technical precision helps accurately assess the potential blast radius across dependencies.Rumsfeld Matrix- Liama 2: Liama 2's nuanced language understanding aids in categorizing knowns/unknowns and identifying unknown unknowns.Test cases- Claude: Claude's natural language capabilities and technical knowledge allow it to effectively comprehend and analyze user input test cases, making it well-suited for the Test Case input step in Smart Chaos experiments.Chatbot- Claude: Claude's natural language capabilities make it the ideal choice for powering a Smart Chaos chatbot.What is needed to get Smart Chaos develop outside PartyRock?Finally, let's explore how we can migrate it from PartyRock to host on AWS. Here are some key considerations for moving the Smart Chaos application to a dedicated deployment on AWS:Frontend: Create an Angular application for the frontend UI. Host the static assets in an S3 bucket and serve them through CloudFront for performance.Backend: Implement the core backend functions like discovery, dependency analysis etc. as Lambda functions.Leverage Step Functions to orchestrate the workflow between the Lambda functions.Connect the Lambdas to AWS Bedrock to access the LLMs for processing.Storage: Upload architecture diagrams and test cases to S3 buckets.Use Amazon Elasticsearch Service for the knowledge graph storage and lookups.In summary, to transition the Smart Chaos application from PartyRock to AWS, you'll need to create an Angular frontend, implement core backend functions as Lambda functions, orchestrate workflows with Step Functions, connect Lambdas to AWS Bedrock for LLM processing, utilize S3 for storage, and employ Amazon Elasticsearch Service for knowledge graph storage.Btw, if your looking for quick solution to take out your PartyRock app, there is nice app develop by Stephen Sennett -GenStack - Bring PartyRock apps to your placeThis post is an extension of the presentation I did as part of Conf42 Chaos Engineering 2024 - 'Smart Chaos: Leveraging Generative AI.' You can watch the video here. Here again I used AWS PartyRock for the POC."}
{"title": "Contours of Log Metric Filter on CloudTrail Log Group via Filter Patterned", "published_at": 1710008440, "tags": ["aws", "awscloudwatch", "awscloudtrail", "awssns"], "user": "GargeeBhatnagar", "url": "https://dev.to/aws-builders/contours-of-log-metric-filter-on-cloudtrail-log-group-via-filter-patterned-e2m", "details": "\u201c I have checked the documents of AWS to get the solution for setup of log metric filter on cloudtrail log group so that whenever some config changes in any service we will be notified of the activity for it. In terms of cost, there is no charge for aws cloudwatch log metric filter feature but only need to pay for services integrated with it.\u201dAmazon Cloudwatch monitors your Amazon Web Services resources and the applications you run on AWS in real time. Amazon Cloudwatch logs to monitor, store and access your log files from Amazon Elastic Compute Cloud instances, AWS Cloudtrail, Route53 and other sources.Cloudwatch logs enable you to centralize the logs from all of your systems, applications and AWS services that you use, in a single, highly scalable service. You can then easily view them, search them for specific error codes or patterns, filter them based on specific fields or archive them securely for future analysis.A log stream is a sequence of log events that share the same source. Each separate source of logs in cloudwatch logs makes up a separate log stream. A log group is a group of log streams that share the same retention, monitoring and access control settings. You can define log groups and specify which streams to put into each group. There is no limit on the number of log streams that can belong to one log group.In this post, you will experience how contours of log metric filter on cloudtrail log group via filter patterned . Here I have created a sns topic with subscription, cloudwatch log group, cloudwatch alarm, cloudwatch log metric filter and cloudtrail.Architecture OverviewThe architecture diagram shows the overall deployment architecture with data flow, aws cloudtrail, amazon cloudwatch, amazon sns and email.Solution overviewThe blog post consists of the following phases:Create of Log Metric Filter and Alarm on Cloudwatch Log GroupOutput as Changes in Cloudtrail Configuration via Alarm Notification on EmailPhase 1: Create of Log Metric Filter and Alarm on Cloudwatch Log GroupOpen the console of Cloudwatch, create a log metric filter on cloudtrail log group with required filter pattern. Also create an alarm on the metric filter.Phase 2: Output as Changes in Cloudtrail Configuration via Alarm Notification on EmailClean-upDelete of Cloudtrail, Cloudwatch and SNS.PricingI review the pricing and estimated cost of this example.Cost of Simple Notification Service = $0.0Cost of CloudWatch = $0.02Cost of Cloudtrail = $0.0Total Cost = $0.02SummaryIn this post, I showed \u201chow to contours of log metric filter on cloudtrail log group via filter patterned\u201d.For more details on Amazon Cloudwatch, Checkout Get started Amazon Cloudwatch, open theAmazon Cloudwatch console. To learn more, read theAmazon Cloudwatch documentation.Thanks for reading!Connect with me:Linkedin"}
{"title": "4 Inspiring Tips to Think Like a Consultant", "published_at": 1709998860, "tags": ["leadership", "howto", "productivity", "discuss"], "user": "Benjamen Pyle", "url": "https://dev.to/aws-builders/4-inspiring-tips-to-think-like-a-consultant-51pm", "details": "I'm early on in this new venture as a full-time consultant but something that isn't new for me is being someone people go to for help.  Every time I speak to someone about this topic I tend to drive back to the same core concepts that I'll talk about below.  Being a consultant isn't about rates, engagements, or hustling for business.  Sure, it can be a career.  But being a consultant is a mindset that can elevate your position and influence first in your team and then more broadly into your organization.  Following along I give you 4 tips to think like a consultant.Why this mattersEvery team or circle of builders has a handful of people who are go-to people.  Have you ever wondered why that is?  I tend to look at those individuals in three buckets.Bucket one is the long-timer.  They've been at the company or in their role for years.  You can't shortcut experience and this person has experience in spades.Bucket two is the helpful, selfless, and thoughtful person who you can approach with any problem and they will either help find the answer or understand what they don't know and who to connect you with.Bucket three is when you have someone in bucket two who is also in bucket one.  When you blend someone who is super helpful and genuine with someone who has experience and expertise, well as they say, \"the world is your oyster\".Bucket one has to be earned.  There's nothing you can do about it beyond just putting in the work.  And I don't mean day after day, it's more year after year.  And then one day you'll look up and realize you are now in bucket one.  Pat yourself on the back, because you've earned it.However, for the rest of this article, I want to give you 4 tips that will help you start to think like a consultant.  Because bucket number two is about being a consultant.  And again, forget the term consultant as a career but as a mindset.  A trusted advisor whose purpose is to solve problems and deliver value.  My definition of a consultant as it relates to bucket number two is this.A consultant is someone who solves problems for an organization that drives value by either directly having the ability to obtain the knowledge needed or through establishing relationships and bringing others together to accomplish the goal.How to think like a consultantConvinced so far?  Anyone can think like a consultant and therefore be that person who finds themself in bucket two.  You don't have to be far along in your career honestly to do this.  I've seen junior developers be leaders on their teams simply by learning to think like a consultant and being that helpful team member.  Here are the 4 tips:ListenFind a way to add valueWork backwardsIt dependsSounds simple right?  But in practice, things get much trickier because all of these go against human nature to want to be right.  But when you think like a consultant, you aren't trying to be right.  You are trying to solve a problem to drive business value.  The outcome is what matters but more importantly, how you achieve that outcome will determine your ability to level up your influence.ListenListening is a skill that we don't practice enough.  If you want to think like a consultant, learning to listen is a critical skill.  If a consultant solves problems and connects with others to accomplish things for the collective, it only makes sense that gathering the requirements of problems is a highly valuable skill.  And while those requirements might be written, most of the variability and detail will come from conversation.  This could be an auditory conversation or it could be over a social channel likeDiscordorSlack.  The point is, to think like a consultant, you have to be able to listen to what people are saying so that you can then begin to help solution.My mother used to tell me, that God gave you two listening devices and one speaking device (ears and mouth), He intended for you to listen twice as much as you speak.  The art of listening and especially listening for understanding is something that will be greater in your career than if you can implement a binary tree in C on a whiteboard during an interview. (aBinary Heap).So practice listening more.  And while someone is talking, stop trying to plan your response but listen.  Here's what that is going to do for you.  When a person knows that when they speak to you they are going to be heard, you've started to build trust.  Buildingtrustis a critical component of buildinginfluence.  And as you build this trust and influence, your listening skills will spread beyond the one you helped.Trust me on this, great listeners are always in demand.  And being in demand elevates your value as a team member.Find a way to add valueNo one likes a team member who doesn't add value to the group.  It doesn't matter how nice of a person they are.  If they aren't adding value, it's hard to work with them.  High-performing teams want people that are trustworthy and can be counted on.  So if you aren't adding value, you aren't serving a purpose on the team.Adding value though comes in several different forms.  Some people add value by producing amazing code.  Some add value by being the conduit to others.  Some add value by being amazing organizers.  There are so many different ways to add value, but the important thing to remember is that if you want to be that consultant, then you need to be adding value.  In the long term, learning to think like a consultant means learning to find places you can add value.  It's the concept of filling in the gaps or cracks in a team.When I was early on in my career, I had a consultant give me some advice that has shaped 20 years of my career.  He told me, \"Find a way to add value on day 1 of any engagement\".  So that's what I've done.  Even if the value was documented in a meeting, find that value and add it.The tip here is that when learning to think like a consultant, you need to look for those places where you can add value.  And the longer you are on the team,  the more important and deeper that value will become.  But if you pair the listening skills from tip 1, with the ability to find and deliver even the simplest of value, you are on your way to being in bucket number two.Work backwardsRemember, to think like a consultant you must be looking to solve problems and add value.  One of the keys to solving technical problems is to understand what the end goal needs to look like.  Listening helps here as outlined in tip 1.  However many people struggle with building plans and designs to achieve an outcome.  These types of problem solvers fall into the \"we'll know when we get there\".  That might be good enough in some cases, but by working backward from outcome to problem, you can give your team a better chance at not missing a critical detail.  Keeping the customer's outcome in mind will become your north star as you implement.Working backward is something that you can learn.  In my career, I've found that there are some more naturally gifted in this ability, but that everyone can become skilled at it.  But why does it matter in the context of becoming that person that people go to?Problem-solving skills in my opinion are more important than language-level capabilities.  Why is that?  Because language proficiency can be looked up on demand.  And while the act of writing code more efficiently is important, the act of solving the problem correctly trumps that efficiency.  What's the point of writing more code if it's in the wrong direction?Now I'm not trying to say that language level skills and tooling proficiency aren't important.  But what I am saying is that to think like a consultant and elevate yourself as a go-to person, learning to solve problems and work backward will take you further.  You'll spend more time designing solutions, refactoring code, or troubleshooting production issues than you will write greenfield code in your life.  Learning to identify problems, build solutions, and lay out plans backwards to forwards will serve you well in your career.Be a problem solver who listens and adds value and guess what?  You are starting to think like a consultant and put yourself into bucket two.It dependsDoes this answer ever frustrate you? How many times have you asked your internal consultant person or architect and they've responded with this answer?  Don't let yourself fall into the trap of thinking that they are dodging the question.  They aren't because they understand that the answer depends upon the outcome and a host of other factors.When you work backward to solve problems, you will be presented with several choices along the way.  Contrast that with solving problems forward, and will you make decisions one at a time hoping you are stacking the solutions correctly?  But by working backward, you get the opportunity to look at the entire solution as composable blocks.  And each of those blocks comes with pros and cons.  Trade-offs.  Don't fall into the trap that there are perfect solutions.  There aren't.  Even software you buy off the shelf won't fit your needs perfectly.  This is important, you need to understand that there are only solutions that you understand how to operate due to their limitations.When you start to think like a consultant you will start to answer a lot of questions with \"it depends\".Should I use Lambda or Fargate here?It dependsDynamoDB or RDS?It dependsShould my team build this Lambda inRustorGo?  Again, It depends.Thinking like a consultant who listens, adds value, and works backward will give you a macro-level perspective on things.  And people like consultants who take a macro-level view of problems.  And when you pair this macro-level thinking with experience in assembling details like the type of person in bucket number one, you start to find yourself as a bucket three person.The tip to elevating yourself here by learning that it depends is OK, lies in your ability to process trade-offs and communicate those to stakeholders.  This is next to impossible though if you don't start off by listening to what's required.  These tips all stack together.Wrapping upBeing a consultant is more than just a job.  It's a mindset.  My opinion is the mindset comes before the job.  If I didn't enjoy growing the skills I shared above, I doubt I would have startedPyle Cloud Technologies(Website content coming soon)But these skills are useful in any capacity because when you add them up they align with being a good team member.  And remember, software is a team sport.The job market is always going to go in waves.  There will be periods of growth and periods of recession.  Today the hot languages are Rust, Go, and TypeScript but 20 years ago it was Java and C#.  Things come and go but what never goes out of style is someone who can listen, add value, work backward, solve problems, and articulate trade-offs to stakeholders (it depends).If you focus on growing skills that never go out of style, you can begin to differentiate yourself from others.  I guarantee you that when you do this, it'll enhance your growing technical skills and one day you'll be a bucket three person.  And a bucket three person is always in demand.Thanks for reaching and happy building!"}
{"title": "AWS PartyRock Event Planner Assistant", "published_at": 1709998186, "tags": [], "user": "Abdullah Paracha", "url": "https://dev.to/aws-builders/aws-partyrock-event-planner-assistant-4il", "details": "Concept: Event Planner AssistantThe \"Event Planner Assistant\" is a generative AI application designed to help users plan their virtual parties. It uses AI to suggest party themes, music playlists, and interactive activities tailored to the host's preferences and the nature of the event.Key FeaturesTheme Suggestion:Generates creative party themes based on the type of event and user preferences.Playlist Creation:Recommends music playlists that match the party's theme and mood.Activity Ideas:Proposes interactive games and activities suitable for the event.Building the Application on AWS PartyRockLet's imagine the steps and simple code snippets that might be involved in creating such an application on AWS PartyRock, using a mix of pseudocode and descriptions to illustrate the process.Step 1: Set Up User Input InterfaceFirst, we create a simple user interface where the host can input details about the party, such as the type of event, preferred music genres, and any specific themes or activities they're interested in.InputForm: - EventType: [Birthday, Graduation, Casual Get-together, ...] - MusicPreferences: [Pop, Rock, Electronic, Jazz, ...] - ThemePreferences: [Input Text] - ActivityInterest: [Games, Quizzes, Dance, ...]Enter fullscreen modeExit fullscreen modeStep 2: Theme Suggestion LogicUsing a predefined AI model, the application generates a list of party themes based on the event type and theme preferences.def generate_theme(event_type, theme_preferences):     # Imagine calling an AI model here     suggested_themes = AIModel.generate_themes(event_type, theme_preferences)     return suggested_themesEnter fullscreen modeExit fullscreen modeStep 3: Playlist Creation LogicThe application uses another AI model to curate a playlist based on the party's theme and the host's music preferences.def create_playlist(theme, music_preferences):     # Imagine calling an AI model here     playlist_links = AIModel.create_playlist(theme, music_preferences)     return playlist_linksEnter fullscreen modeExit fullscreen modeStep 4: Activity Ideas GenerationFinally, the application suggests interactive activities and games that match the chosen theme and the host's interest in activities.def suggest_activities(theme, activity_interest):     # Imagine calling an AI model here     activities = AIModel.suggest_activities(theme, activity_interest)     return activitiesEnter fullscreen modeExit fullscreen modeStep 5: Compile and Present SuggestionsThe application compiles the suggestions from each step and presents them to the user in an interactive format, possibly with options to customize further or explore alternatives.user_input = gather_user_input() theme_suggestions = generate_theme(user_input.eventType, user_input.themePreferences) playlist = create_playlist(theme_suggestions[0], user_input.musicPreferences)  # Assume first theme activities = suggest_activities(theme_suggestions[0], user_input.activityInterest)  present_to_user(theme_suggestions, playlist, activities)Enter fullscreen modeExit fullscreen modeConclusionWhile this example uses pseudocode and assumes the existence of AI models and a platform like AWS PartyRock, it illustrates how one could conceptualize and design an AI-driven application for event planning. The real power of generative AI applications lies in their ability to personalize and enhance experiences, offering unique and tailored suggestions that cater to individual preferences and needs."}
{"title": "What's New With AWS Security? | February Edition", "published_at": 1709955000, "tags": ["aws", "security", "cloud"], "user": "Lahiru Hewawasam", "url": "https://dev.to/aws-builders/whats-new-with-aws-security-february-edition-2l5c", "details": "It's that time of the month where we go through the latest and greatest updates to AWS security services.Feel free to check out the previous articles within this series to get yourself updated on what AWS has been up to in terms of security.Series:What's New With AWS Security?What's New With AWS Security Now?It's not everyday that you get to hear something being updated on AWS security services and it was a case where AWS only announced a handful of updates to its services.Let's take a look at the latest additions to the AWS security services.Announcement Date: 01/02/2024Amazon Cognito adds signing, encryption, and Identity Provider-initiated SSO for SAML federationAnnouncement Date: 06/02/2024AWS WAF announces Captcha improvementsAnnouncement Date: 09/02/2024Amazon GuardDuty Malware Protection now supports scanning EBS managed key encrypted volumesAWS IoT Core supports Online Certificate Status Protocol Stapling for server certificatesAnnouncement Date: 13/02/2024Amazon GuardDuty Runtime Monitoring protects clusters running in shared VPCAnnouncement Date: 23/02/2024Remediating non-compliant resources with AWS Config rules is now available in Canada West (Calgary)Announcement Date: 29/02/2024Amazon Security Lake now supports audit logs from Amazon EKSAmazon Security Lake enhances analytics performance with OCSF 1.1.0 and Apache IcebergAWS Backup now supports restore testing for Amazon Aurora continuous backupsNoteworthy Updates To ServicesLike always some of these announcements stood out of the rest of the list, so let's take a look at them!1. Amazon CognitoAmazon Cognito now comes with 3 brand new features that SAML federation:1. IdP-Initiated Login 2. Encrypted SAML assertion/response 3. Signed SAML requestsEnter fullscreen modeExit fullscreen modeThisvideotalks in-depth about these new features and how you can also start using it today!2. Amazon GuardDutyNow you can runon-demandmalware scans on EBS volumes attached to EC2 instances and container workloads that are encrypted by EBS managed keys!Findings may include information such as Threat, File Name, File Path, EC2 instance ID, Container ID and Container Image usedAmazon GuardDuty Runtime Monitoringcan now detect threats running in all supported compute services running in ashared VPC.3. AWS WAFAWS WAF introduced support for 8 additional languages within the audio captcha; Newly added languages include Spanish, German, French, Portuguese, Italian, Turkish, Dutch, and Arabic.The service also introduced a new form of captcha puzzle called Grid Captcha. This improves user pass through rates thus improving the overall user experience.Admins can now deactivate or rotate any captcha API keys if they suspect any suspicious activity before they can be misused.Wrapping UpI must say that the month of February brought some good improvements into the AWS security services arsenal that's definitely going to help organizations stay on top of their security game!Within this article I've highlighted some of the major service announcements and feature introductions that were noteworthy. There may have been some announcements that I didn't cover in this month's announcement, therefore feel free to mention what you think was important in the comment section.Stay Tuned for the next edition of \"What's New With AWS Security\"!Thank you for reading. I hope you found this useful."}
{"title": "How to minimize your cloud spending on AWS resources using Systems Manager Automation documents", "published_at": 1709941810, "tags": ["systemsmanager", "iam", "ec2"], "user": "Revathi Joshi", "url": "https://dev.to/aws-builders/how-to-minimize-your-cloud-spending-on-aws-resources-using-systems-manager-automation-documents-fjc", "details": "An easy way to accomplish unneccessary spending and to minimize infrastructure when it\u2019s not under heavy use, is - by turning off Amazon Elastic Compute Cloud (Amazon EC2) and Amazon Relational Database Service (Amazon RDS) instances for workloads outside of business hours. For workloads that cannot be turned off (due to dependencies on other systems), downsizing instance types is a good alternative.Applying these measures you can save companies up to 70% in infrastructure costs.In this post, I will show you how to useAWS Systems Manager Automation documents to turn offyour Amazon EC2 and Amazon RDS instances. These Automation documents can then be scheduled for known low usage periods such as nights, holidays, and weekends.You can use pre-defined Automation documents (prefixed with AWS) or define your own. These can be invoked on a schedule or via a trigger. For more information, checkSystems Manager automation.Let\u2019s get started!Please visit myGitHub Repository for EC2 articleson various topics being updated on constant basis.Please visit myGitHub Repository for RDS articleson various topics being updated on constant basis.For my article, I am going to use pre-defined Automation documents:AWS-StartEC2InstanceAWS-StopEC2InstanceAWS-StartRdsInstanceObjectives:1.Create an RDS database2.Create EC2 Instance3.Create an IAM Role4.Add an Inline Policy to the IAM Role to allow Systems Manager to manage RDS Instance5.Create an Association - Scheduling through State Manager to stop EC2 Instance6.Create an Association - Scheduling through State Manager to stop RDS databasePre-requisites:AWS user account with admin access, not a root account.Create an IAM roleResources Used:AWS Systems Manager AutomationAWS Systems ManagerState Manager AssociationsIAMEC2RDSSteps for implementation to this project:1. Create an RDS database1234567891011122. Create EC2 Instance123456783. Create an IAM Role123Next4Next56Create role4. Add an Inline Policy to the IAM Role to allow Systems Manager to manage RDS InstanceFind out the ARN of your RDS databaseChoose the role that you just created.In the Permissions tab, choose Add inline policy, select the JSON tab, and replace the JSON content with the following code.Make sure you replace both resource parameters with one or more ARNs of your databases{     \"Version\": \"2012-10-17\",     \"Statement\": [         {             \"Sid\": \"VisualEditor0\",             \"Effect\": \"Allow\",             \"Action\": [                 \"rds:StopDBInstance\",                 \"rds:StartDBInstance\"             ],             \"Resource\": \"arn:aws:rds:us-east-1:xxxxxxxxxxxx:cluster:database-1\"         },         {             \"Sid\": \"VisualEditor1\",             \"Effect\": \"Allow\",             \"Action\": \"rds:DescribeDBInstances\",             \"Resource\": \"arn:aws:rds:us-east-1:xxxxxxxxxxxx:cluster:database-1\"         }     ] }Enter fullscreen modeExit fullscreen mode12Next3Create policy5. Create an Association - Scheduling through State Manager to stop EC2 Instance12345select the SSM Automation document namedAWS-StopEC2Instance.press Enter67For a single EC2, choose Simple execution.in the Input parameters section enter the EC2 instance idIn the AutomationAssumeRole box, pick the roleEC2toSystemsManager.8In the Specify schedule section, choose On Schedule and CRON schedule builder.Under Association runs, choose the last option, then choose Day, enter 18 and 00. You should now have the following: \u201cEvery Day at 18:00\u201d. (that means Stop the EC2 instance at 6:00 pm or 18:00).To ensure that the association doesn\u2019t run upon creation, choose the Apply association only at the next specified cron interval check box.Create associationyou should now see your association in the list.After waiting for 10-15 min, stops the EC2 instance6. Create an Association - Scheduling through State Manager to stop RDS databaseFollow all the steps as you did forAWS-StopEC2Instance123UseAWS-StopRDSInstance45In the Specify schedule section, choose On Schedule and CRON schedule builder.Under Association runs, choose the last option, then choose Day, enter 18 and 20. You should now have the following: \u201cEvery Day at 18:20\u201d. (that means Stop the EC2 instance at 6:20 pm or 18:20).To ensure that the association doesn\u2019t run upon creation, choose the Apply association only at the next specified cron interval check box.6After waiting for 10-15 min, stops the RDS databaseCleanupdelete RDS Databasedelete EC2 instancedelete AssociationWhat we have done so farI showed you how to cut off your cloud spending for stopping EC2, and Amazon RDS instances based on a schedule."}
{"title": "Terraform cloud", "published_at": 1709930273, "tags": ["terraform", "cloud", "iac", "terraformcloud"], "user": "Srinivasulu Paranduru", "url": "https://dev.to/aws-builders/terraform-cloud-g65", "details": "Terraform CloudGo to the linkTerraform Cloudand register for a new accountAfter the successful registration, we need to confirm the emailCreate an organisationPricing in Terraform Cloudhttps://www.hashicorp.com/products/terraform/pricingCreate a workspace in terraform cloudworkspace creation process will take some time and we need to waitClick on Version control and configure with githubSelect the github organisation -Create a github repo in the selected organisation with name as terraform_cloudUpdate Environmental Variables:State file :Destruction of resources:Conclusion : Discussed about terraform cloud, creation and deatruction of resources using github repo.\ud83d\udcac If you enjoyed reading this blog post and found it informative, please take a moment to share your thoughts by leaving a review and liking it \ud83d\ude00 and follow me in linkedin"}
{"title": "Enhancing Data Security with Spark: A Guide to Column-Level Encryption - Part 1", "published_at": 1709915189, "tags": ["aws", "spark", "encryption", "dataprotection"], "user": "Mostefa Brougui", "url": "https://dev.to/aws-builders/enhancing-data-security-with-spark-a-guide-to-column-level-encryption-part-1-1oaa", "details": "This post describes how you can use PySparkaes_encrypt()function to encrypt sensitive columns when ingesting data. It is part of a series that shows how column-level encryption can be deployed at scale using AWS Glue, AWS KMS and Amazon Athena or Amazon Redshift.IntroductionIn an era where data breaches are increasingly common, securing sensitive data is not just a best practice but a necessity. As Werner Vogels, Amazon's CTO, wisely put it:\"Dance Like Nobody\u2019s Watching. Encrypt Like Everyone Is.\"In this series, we explore how to deploy column-level encryption at scale using an integration of PySpark, AWS Glue, AWS KMS, and Amazon Redshift or Amazon Athena.Why Column-Level Encryption?Column-level encryption allows for fine-grained control over data access. It's particularly useful in scenarios where different groups require access to specific data subsets. This post serves as an introductory guide, focusing on the implementation of PySpark's aes_encrypt() function for encrypting sensitive data during ingestion.Setting the StageBefore diving into the encryption process, it's important to understand the relevant technologies:PySpark: A Python API for Apache Spark, used for large-scale data processing.AWS Glue: A serverless data integration service that makes it easy to discover, prepare, and combine data for analytics, machine learning, and application development.AWS KMS: AWS Key Management Service, a managed service that makes it easy to create and manage cryptographic keys.Amazon Athena: An interactive query service that makes it easy to analyze data in Amazon S3 using standard SQL.Amazon Redshift: A fast, fully managed, petabyte-scale data warehouse service that makes it simple and cost-effective to efficiently analyze all your data.Getting StartedPrepare Your EnvironmentRun a Jupyter Notebook on Docker.Here's how.Clone this repository for sample data and scripts:GitHub Link.Read Sample DataGenerate / Upload a CSV file containing a data sample to work with to the notebook. I provide a sample file in the provided GitHub repository:CSV file.Read the sample data using the code below.importbase64asb64frompyspark.sqlimportSparkSessionfrompyspark.sql.typesimport*spark=SparkSession.builder.appName(\"MyApp\").getOrCreate()df=spark.read.option(\"header\",True).csv(\"sample-pci-data.csv\")df.show()Enter fullscreen modeExit fullscreen modeYou should see something similar to the below.+-------------------+-----------+-------------------+ |First and Last Name|        SSN| Credit Card Number| +-------------------+-----------+-------------------+ |      Robert Aragon|489-36-8350|4929-3813-3266-4295| |      Ashley Borden|514-14-8905|5370-4638-8881-3020| ... only showing top 2 rowsEnter fullscreen modeExit fullscreen modeUnderstanding aes_encrypt()The aes_encrypt() function in PySpark 3.3 offers AES encryption with various configurations.Read more about it here.Below is the syntax of the function. It uses AES encryption with 16, 24, or 32-bit keys in ECB, GCM, or CBC modes with appropriate padding. IVs are optional for CBC and GCM modes and are auto-generated if absent. For GCM, optional AAD must match during encryption and decryption. GCM is the default mode.def aes_encrypt(     input: \"ColumnOrName\",     key: \"ColumnOrName\",     mode: Optional[\"ColumnOrName\"] = None,     padding: Optional[\"ColumnOrName\"] = None,     iv: Optional[\"ColumnOrName\"] = None,     aad: Optional[\"ColumnOrName\"] = None, ) -> Column:Enter fullscreen modeExit fullscreen modeThe following exanple shows the encryption and decryption of the stringSpark SQLusing the keyabcdefghijklmnopwhich is a 16 characters long = 16 bytes = 128 bits key.df=spark.createDataFrame([(\"Spark SQL\",\"abcdefghijklmnop\",)],[\"input\",\"key\"])df.select(aes_decrypt(unbase64(base64(aes_encrypt(df.input,df.key))),df.key).cast(\"STRING\").alias('r')).collect()[Row(r='Spark SQL')]Enter fullscreen modeExit fullscreen modeGenerating an Encryption KeyUse AWS KMS'sGenerateDataKeyAPI for a robust key generation. This requires an existing KMS key.To generate the key using the AWS CLIgenerate-data-keycommand, use the example shown below. Make sure to replaceAWS_PROFILEandKMS_KEY_IDwith the correct values.aws kms generate-data-key--profileAWS_PROFILE--key-idKMS_KEY_ID--key-specAES_256The response shown below contains the key inPlaintextbase64-encoded format and in encrypted (CiphertextBlob) format.{\"CiphertextBlob\":\"AQIDAHjhvhi8z21Psjp5qjRibLXgHGkMMP/BmNWdIKTuZnLdNwHHYQjpmQo0oyk5rk07mjZTAAAAfjB8BgkqhkiG9w0BBwagbzBtAgEAMGgGCSqGSIb3DQEHATAeBglghkgBZQMEAS4wEQQM6uOfZ7iyJ2UztfidAgEQgDuFQ5fajsvnhfaCmN/q8kk1JinY7gHqT/Bz9W3RxqkEv5ggPZKELcQtqNbRYdEzSwKPDTs+Grp0RtRqWw==\",\"Plaintext\":\"tEZPizBEj5EG5IDY1SvAECa5yZa5fVP1SrJGsGimx9I=\",\"KeyId\":\"KMS_KEY_ID\"}If you generate the data key using boto3kmsclient, you will get a byte string, similar tob\"\\xc27~\\x15\\xc7\\x9a\\x8a|\\xb48\\\\\\xd7\\x894g-v\\xac\\xb5\\n%\\x17\\x96g\\xab\\x88\\x8a;|bU/\". This value can be used as-is with theaes_encrypt()function.Alternatively, derive a key from a string usingPBKDF2. (Link to code sample)Encrypting Sensitive ColumnsWe'll encrypt the \"SSN\" and \"Credit Card Number\" columns as shown below.encryption_key=\"tEZPizBEj5EG5IDY1SvAECa5yZa5fVP1SrJGsGimx9I=\"# Decode the key from Base64decoded_key=b64.b64decode(encryption_key)# Encrypt two columns at the \"same time\" using the same keydf_encrypted=df.withColumn('SSN_Encrypted',base64(expr(f\"aes_encrypt(SSN, unhex('{decoded_key.hex()}'),'GCM')\")))\\.withColumn('CreditCardNumber_Encrypted',base64(expr(f\"aes_encrypt(Credit_Card_Number, unhex('{decoded_key.hex()}'),'GCM')\")))df_encrypted.show(truncate=True)# +----------------+-----------+-------------------+--------------------+--------------------------+ # | First_Last_Name|        SSN| Credit_Card_Number|       SSN_Encrypted|CreditCardNumber_Encrypted| # +----------------+-----------+-------------------+--------------------+--------------------------+ # |   Robert Aragon|489-36-8350|4929-3813-3266-4295|HI39DkVYN9WLTWAH3...|      XqvsWgRty1CBkJ7c9...| # |   Ashley Borden|514-14-8905|5370-4638-8881-3020|5ME9bErsff7Zhzw5z...|      mhizKvf5053KqStxp...| #  ... # only showing top 2 rows# Output path to store the new CSVoutput_path=\"./encrypted/sample-pci-data-encrypted.csv\"df_encrypted.write.csv(path=output_path,mode=\"overwrite\",header=True,sep=\",\")Enter fullscreen modeExit fullscreen modeDecryption ProcessHere's how you can decrypt the data using the same key.df=spark.read.option(\"header\",True).csv(\"encrypted/sample-pci-data-encrypted.csv\")df.show()df_decrypted=df.withColumn('SSN_Decrypted',expr(f\"aes_decrypt(unbase64(SSN_Encrypted), unhex('{decoded_key.hex()}'),'GCM')\").cast(\"STRING\")).withColumn('CC_Decrypted',expr(f\"aes_decrypt(unbase64(CreditCardNumber_Encrypted), unhex('{decoded_key.hex()}'),'GCM')\").cast(\"STRING\"))# +----------------+-----------+-------------------+--------------------+--------------------------+ # | First_Last_Name|        SSN| Credit_Card_Number|       SSN_Encrypted|CreditCardNumber_Encrypted| # +----------------+-----------+-------------------+--------------------+--------------------------+ # |   Robert Aragon|489-36-8350|4929-3813-3266-4295|Ft4y7DmP8xN8QtdHn...|      4f7Usr2dvVWbG26Bd...| # |   Ashley Borden|514-14-8905|5370-4638-8881-3020|nsdeq//on5T/L64Ow...|      8LN7pIP1pa7Nmoma1...| # only showing top 2 rows# +----------------+-----------+-------------------+--------------------+--------------------------+-------------+-------------------+ # | First_Last_Name|        SSN| Credit_Card_Number|       SSN_Encrypted|CreditCardNumber_Encrypted|SSN_Decrypted|       CC_Decrypted| # +----------------+-----------+-------------------+--------------------+--------------------------+-------------+-------------------+ # |   Robert Aragon|489-36-8350|4929-3813-3266-4295|Ft4y7DmP8xN8QtdHn...|      4f7Usr2dvVWbG26Bd...|  489-36-8350|4929-3813-3266-4295| # |   Ashley Borden|514-14-8905|5370-4638-8881-3020|nsdeq//on5T/L64Ow...|      8LN7pIP1pa7Nmoma1...|  514-14-8905|5370-4638-8881-3020| # only showing top 2 rowsEnter fullscreen modeExit fullscreen modeConclusionIn this post, I walked through the steps of encrypting and decrypting sensitive data columns using PySpark in a Jupyter Notebook. This setup serves as a foundational step towards a more complex and scalable data ingestion and consumption model, which I'll explore in upcoming posts, including the integration of AWS Glue and AWS KMS.Stay tuned for further insights on scaling this approach and managing encryption keys with AWS KMS!"}
{"title": "Billing for SaaS with EMF and CloudWatch Metric Streams", "published_at": 1709915078, "tags": ["aws", "sass", "data", "softwareengineering"], "user": "Matt Houghton", "url": "https://dev.to/aws-builders/billing-for-saas-with-emf-and-cloudwatch-metric-streams-4i6p", "details": "In this post I'm looking at how Software as a Service (SaaS) providers running on AWS can use a few AWS Services to build out a mechanism for collecting billing/metering metrics from their software and process them in order to bill a customer based on usage.The main services I will cover are use of AWS CloudWatch embedded metric format (EMF) together with AWS CloudWatch Metric Streams.What is EMF?The CloudWatch embedded metric format allows you to generate custom metrics asynchronously in the form of logs written to CloudWatch Logs. You can embed custom metrics alongside detailed log event data, and CloudWatch automatically extracts the custom metrics so that you can visualize and alarm on them.What is CloudWatch Metric Streams?You can use metric streams to continually stream CloudWatch metrics to a destination of your choice, with near-real-time delivery and low latency. Supported destinations include AWS destinations such as Amazon Simple Storage Service and several third-party service provider destinations.Using EMF in your ApplicationImagine a sample Python application returning \"hello world\" to simulate a successful call. Each call to the application is captured for billing purposes using EMF. LambdaPowertoolsis used to reduce the amount of code we need to write.metrics.add_metric(name=\"SuccessfulGet\", unit=MetricUnit.Count, value=1) metrics.add_dimension(name=\"Customer\", value=\"MattHoughton\")Enter fullscreen modeExit fullscreen modeThese two lines output the required billing metrics.The SuccessfulGet can be customised for your application. This value should indicate a sensible identifier for the chargeable action. For example in the world of insurance you may have actions such as CreatePolicy, CreateQuote, UpdateCar etc.On the Lambda function configuration the following environment variables also need to be set.POWERTOOLS_SERVICE_NAME: SuggestTheNameOfYourSoftware POWERTOOLS_METRICS_NAMESPACE: SuggestSomethingLikeBillingEnter fullscreen modeExit fullscreen modeHere is the sample Lambda function.import json from aws_lambda_powertools import Metrics from aws_lambda_powertools.metrics import MetricUnit  metrics = Metrics()  @metrics.log_metrics def lambda_handler(event, context):      #do something of value      metrics.add_metric(name=\"CreatePolicy\", unit=MetricUnit.Count, value=1)     metrics.add_dimension(name=\"Customer\", value=\"MattHoughton\") #just an example dont hard code this for real source it from the payload or something      return {         \"statusCode\": 200,         \"body\": json.dumps({             \"message\": \"hello world\"         }),     }Enter fullscreen modeExit fullscreen modeTesting the function in the console you should get this response.{   \"statusCode\": 200,   \"body\": \"{\\\"message\\\": \\\"hello world\\\"}\" }  START RequestId: xxxx Version: $LATEST {   \"_aws\": {     \"Timestamp\": 1709911806737,     \"CloudWatchMetrics\": [       {         \"Namespace\": \"DemoBilling\",         \"Dimensions\": [           [             \"Customer\",             \"service\"           ]         ],         \"Metrics\": [           {             \"Name\": \"CreatePolicy\",             \"Unit\": \"Count\"           }         ]       }     ]   },   \"Customer\": \"MattHoughton\",   \"service\": \"DemoProductName\",   \"CreatePolicy\": [     1   ] } END RequestId: xxxx REPORT RequestId: xxxx  Duration: 1.42 ms   Billed Duration: 2 ms   Memory Size: 128 MB Max Memory Used: 37 MB  Init Duration: 177.72 msEnter fullscreen modeExit fullscreen modeWhen the Lambda is executed the billing metrics get stored in CloudWatch Logs and are visible in CloudWatch Metrics.Costs for EMF are based on CloudWatch log ingestion which in EU-WEST-1 is $0.57 per GB. When I was testing with an example 624 byte payload that is generated by Powertools the costs came out as:Each metric above stored costs $0.0000003One million metrics stored costs: $0.33Ten million metrics stored costs: $3.31Collecting and Processing the Billing MetricsTo pull out all of the EMF metrics relating to billing we will setup a Metric Stream to send them to an S3 bucket.Under Cloudwatch in the console select Metric Steams and Create a metric stream. We will walk through the Quick setup for S3.Under metrics to be streamed limit this to only the metrics related to billing.Looking at the metric stream that is created you will see details for the other components created for you.Amazon Data FirehoseIAM RolesS3 BucketIf you run the Lambda function a few more times then view the Data Firehose you will see the metrics being delivered.Now if you look in the S3 bucket you will find object created. By default they are partitioned by Year/Month/Day/Hour.Here is sample content published to S3.{\"metric_stream_name\":\"DemoBillingMetricStream\",\"account_id\":\"xxxx\",\"region\":\"us-east-1\",\"namespace\":\"DemoBilling\",\"metric_name\":\"CreatePolicy\",\"dimensions\":{\"Customer\":\"MattHoughton\",\"service\":\"DemoProductName\"},\"timestamp\":1709913900000,\"value\":{\"max\":1.0,\"min\":1.0,\"sum\":9.0,\"count\":9.0},\"unit\":\"Count\"} {\"metric_stream_name\":\"DemoBillingMetricStream\",\"account_id\":\"xxxx\",\"region\":\"us-east-1\",\"namespace\":\"DemoBilling\",\"metric_name\":\"CreatePolicy\",\"dimensions\":{\"Customer\":\"MattHoughton\",\"service\":\"DemoProductName\"},\"timestamp\":1709913960000,\"value\":{\"max\":1.0,\"min\":1.0,\"sum\":30.0,\"count\":30.0},\"unit\":\"Count\"}Enter fullscreen modeExit fullscreen modeFurther ProcessingFrom this point we have a lot of flexibility in how we can choose to process this data.We can trigger a Lambda function that sends these metric payloads to an accounting / invoicing system.We can also continue to use AWS Services. As the data is in S3 we can easily add this to a Glue data catalog and query it using Athena. We could even start to build dashboards and reports using QuickSight."}
{"title": "A Continuous Delivery Press Release", "published_at": 1709910080, "tags": ["aws", "cicd", "agile"], "user": "Seth Orell", "url": "https://dev.to/aws-builders/a-continuous-delivery-press-release-1gj8", "details": "Sometimes even your best ideas will fail to persuade. You can have evidence, examples, and enthusiasm, but still, you fall short of convincing others to give it a try. Today, I want to talk about another tool you can use; imagination1. Project yourself into the future where you are looking back at the problem after you solved it. You recount the steps you took along the way, and the problems you fixed, and then celebrate all that you've achieved.In this article, I'll talk about the \"PR/FAQ\" and how you can use one to help your cause of \"Going Serverless,\" \"Strangling the Monolith\" or, in this case, \"Achieving Continuous Deployment.\" I think you'll find that the process of writing a PR/FAQ will clarify your thoughts and sharpen your arguments. Hopefully, it will help you put your idea into action.Introducing the PR/FAQOne of the more interesting things I learned during my time at Amazon is only tangentially related to engineering, but it has stuck with me: the PR/FAQ. Amazon didn't invent the Press Release / Frequently Asked Questions format, but they did popularize it, and they get credit for introducing it to me.The PR/FAQ is essentially a press release from the future that describes the release of the new feature/product that you wish to begin in the present. The PR/FAQ is the starting point for discussions with leadership, engineering, marketing, and any relevant stakeholders. It presents your idea in a concise, easy-to-follow format that describes both the problems it solves and the value it brings.While primarily utilized by Product Management, I've used the PR/FAQ to illustrate broad engineering change proposals. I like how it fosters conversation and allows those discussions to become part of the document via FAQs. I want to show one example of using a PR/FAQ that is similar to one I've created in the past.I must note that, while this is inspired by real events and real companies, it is entirely fictitious (it'sthe future, after all).The SituationA large, publicly-traded company (\"Acme\") hires you to help them move from an expensive, error-prone, high-touch deployment process into something more resembling CI/CD. Your team (\"Zodiac\") builds multiple business-critical applications that take multiple days and multiple different team sign-offs to deploy.Acme has a single AWS production account that all teams deploy into. The monthly spend on this account approaches $5MM/month and most of that comes from teams that are not yours. The Acme Operations team that controls the account is constantly worried about changes breaking the system. Some in leadership are interested in CI/CD but don't know how to get there. Many in engineering are interested in CI/CD but don't know how to get there. There is a QA team that is currently in charge of quality and a Product team that wants to know the minute a feature goes live.Your goal is to take the Engineering pain away from your deploys while reducing risk for Operations. You also need to help Engineering take control of the whole software development lifecycle while supporting Product. Your PR/FAQ will include all these issues and wrap it in a happy ending that gives everyone involved a well-needed shot of confidence. Here's an example.The Press Release (PR)Zodiac Hits Continuous Delivery Milestone with Daily DeploysAcme's Zodiac group adopts new practices to deliver features and fixes 10x fasterAugust 01, 2024. Zodiac is excited to announce improvements to its delivery pipeline that provide faster cycle times and safer deploys. This new practice \"shifts left\" the testing, infrastructure, and code management to allow deploys to happen in minutes instead of days. The engineering team can easily release bug fixes or add new features as a self-sufficient unit. Because of the many vertical silos of the previous process, releases were slow, error-prone, and expensive. With the new improvements, we have reduced the number of people involved in a release by 300%.Releasing applications in 2023 was a repetitive task, with Architects manually creating/merging code branches, QA and Product manually testing both changes and existing functionality and then Operations taking action from hand-written instructions. By the time the release happened, the people who truly understood the changes (Engineering) were 7 days into their next cycle. In the event an issue was raised, they struggled to regain the context they had when they coded the change in the first place.To give Engineering the self-sufficiency they need, the business needed risk mitigation. Working with Operations, the new per-team, per-environment AWS accounts provided both. Separate accounts provided a narrow blast radius and a clearly delineated area of responsibility for the team. The business also needed accountability, which is why the new process records every delivery in a central change log document, outlining who, what, why, and when. Finally, the adoption of a feature-flag framework allowed the team to decouple delivery from release. This gives the Product team full control of revealing the feature to customers while Engineering can focus on fast delivery.Zodiac engineers benefit from this approach by following a few simple guidelines: write good tests, use feature flags, and put your JIRA ticket number in your commit message. That's it! The trunk-based release approach will, if all tests pass, deploy your changes to all environments. Product can then flip flags on/off as they see fit.\"This allows Engineering to own their changes without impacting what Operations is being asked to manage.\" said senior DevOps engineer, Josh Gordon. \"The [AWS] account separation makes this possible.\"\"It was an eye-opener for us,\" said engineer Pat Wilson. \"Knowing that nobody else was going to verify our code changes gave us tremendous incentive to write good tests.\"If you'd like to introduce this to your team, speak to Stephen Mallory, Austen Heller, or Seth Orell to learn more about continuous delivery and how it can help your team succeed.F.A.Q.sWhat is a \"self-sufficient\" engineering team?A team is self-sufficient when it is empowered to take all the actions necessary for it to create, release, and maintain software. This idea was succinctly captured by Amazon CTO Werner Vogels when he said \"You build it, you run it\".What does \"shift left\" mean?Shift Left is a practice intended to find and prevent defects early in the software process. These defects can be bugs, deployment issues, runtime performance, etc. In contrast, take a look at a traditional vertical separation of concerns: Engineering -> QA -> UAT -> Operations. Notice how the arrows go left to right as the new silo takes control. \"Shift Left\" aims to push responsibility leftward, to Engineering, for all things.What is a \"Feature Flag\"?Feature flags (or toggles) allow a team to modify system behavior without changing code. They separate the code deployment from the feature \"reveal\". This allows for new features to be delivered silently (or \"darkly\") and then turned on at an appropriate time without a separate release.How many AWS Accounts are necessary?The minimum \"best practice\" suggested by Amazon is one per team per environment.2For example, the Zodiac team would have one AWS account dedicated to them for each environment supported. In addition, Many organizations also choose to assign a \"personal\" account to each engineer to encourage experimentation and exploration.Who manages the multiple AWS Accounts?Typically, this is an Operations area of control. Using tools like AWS Organizations and AWS Control Tower, Operations can create hierarchical account structures that match business units, create guardrail controls, and organize cost tracking. This can require a high degree of specialization that is often not found in Engineering.Why is releasing more frequently preferred over less?With our software-as-a-service platform (vs. desktop executable or firmware), we have the opportunity to release at any time. One of the great strengths of Agile is the ability to respond quickly. A quick release cycle means a bug fix takes less time to reach our customers. It also lowers the risk of any particular deployment. This has the additional positive side-effect of a hot context; engineers have their changes fresh in mind during deployment and can more quickly address problems should they arise. By automating the release tasks to make them fast, we also eliminate the errors that come along with a manual process.Why was the old process error-prone?The two primary sources of release errors were release branch management and the separation of those who make changes from those who deploy changes. In the first case, an architect would - for each product released - execute a series of manual steps to merge code both up and down from specific release branches. On the occasional merge conflict, this often required hand editing the files involved. Does the release contain the same code as the engineer intended? Probably (maybe). In the second case, the engineer's changes were interpreted by different silos of teams as it moved toward release: Development -> QA -> UAT/Product -> Operations. At each transition, more context was lost and more noise was introduced (have you ever played the \"telephone\" game?) Moving deployment responsibility up to Engineering - coupled with a trunk-based release pipeline - removes the common sources of release distortion.How auditable is this system? Can't anyone just push anything?This whole process relies heavily on both Trust and Accountability. We trust our engineers to do the right things. At the same time, we hold them accountable for the behavior of the system to our customers. Every code change is captured through our source-control process, including what was changed, who made the change, and when it happened. In addition, the engineer can enter extra information into the commit message to explain why she was making the change. One good practice is to include the JIRA ticket number here. Each release takes the commit history since the last release and records it in a release log. By automating the process, we now have a full trace of the What, When, Who, Why, and How of every deployment.What role does QA/DevOps fit into this process?DevOps is an ideology, not a team or a title. Every engineering team member should be seeking to implement DevOps principles. We want a team of generalists where everyone does everything. To this end, quality becomes an engineering concern. Traditionally, QA picks up the story when the engineer is \"done\" with it and writes tests to verify. With the new process, the engineer is responsible for the quality of the change he is making and writes his own tests. Someone from QA or DevOps is welcome to become an engineer on the team (I have examples of both to share) but we are looking to build the team with generalist engineers.Further ReadingRobert (Munro) Monarch:PR FAQs for Product DocumentsMartin Fowler:Continuous DeliveryIan McAllister:Working Backwards Press Release Template and ExampleAWS Whitepaper:Organizing workload-oriented OUsThere's nothing mystical about this approach; you are telling a story. A story is a powerful vehicle to concretize, to make real to someone, to express an idea.\u21a9Theoptimal approachis one account PER SERVICE (or \"Workload\") per environment, where a single team might deploy many services.\u21a9"}
{"title": "Organize Your CDK lib Folder by Function Not Service", "published_at": 1709908853, "tags": ["aws", "cdk", "devops"], "user": "Dakota Lewallen", "url": "https://dev.to/aws-builders/organize-your-cdk-lib-folder-by-function-not-service-28np", "details": "Don\u2019t: Organize Your CDK lib Folder by ServiceWhen working with IaC tools, it's standard to organize your work based on the service. You specify a service and then describe how you would like it configured. Carrying that pattern into CDK might look something like so.At first, this will feel great! Things are going great until you start to work on functionality across many services. At this point, changes will span many directories resulting in many CloudFormation stacks needing to be deployed.Do: Organize Your CDK lib Folder by FunctionIf we follow the principle of working backward, we\u2019ll end up with a structure that more aligns with how we\u2019re going to work in the future.Starting with our business use case. Let\u2019s say you are going to receive flat files through an SFTP server from clients. You will need to perform some basic ETL processes to house it within your system. Instead of having a lambda folder, a step function folder, and an event bridge folder, you build a singleFlatFileHandlerfolder. With a single stack, that deploys the resources necessary to perform this function.Now whenever work is needed on the flat file handling system, you can find everything you need in a singular place.ConclusionOne of the greatest strengths of CDK, is the ability to perform more complex organizational patterns for your resources. This is one example of a pattern that can be useful. Like anything that's in code, you're now free to organize as you see fit! If you find a more useful pattern, let the world know (me in particular \ud83d\ude09)!Find me onLinkedIn|Github"}
{"title": "Trace & Observe Modern Apps using AWS X-Ray", "published_at": 1709894652, "tags": ["monitoring", "javascript", "node", "aws"], "user": "Mohammad Quanit", "url": "https://dev.to/aws-builders/trace-observe-modern-apps-using-aws-x-ray-2bbl", "details": "Hi fellas, In this blog I am going to share my experience using one of the coolest AWS services namedAWS X-Ray.AWS X-Rayis a fully managed monitoring and observability service that helps you collect data about requests that the application serves. It provides tools that enable you to filter, view, and gain insights into the collected data, helping you identify optimization opportunities and issues.AWS X-Ray is a service that can help you figure out what's going on across all your systems. It lets you see how requests are routed through different service touchpoints and gives you a good idea of how your applications are performing. You can use it to monitor performance, identify bottlenecks, and troubleshoot errors. With AWS X-Ray, you can keep an eye on your systems and make sure everything's running smoothly. It allows you to visualize complex and detailed service relationships within highly distributed applications. You can trace message pathways and call stacks at any scale.Below is the AWS X-Ray workflow image from AWS's official blog.AWS X-Ray has been designed to work seamlessly with distributed systems. Over the last decade or two, as complex distributed systems have emerged, debugging has changed and has taken on a new meaning. Engineers can now analyze and debug applications, audit their applications securely, and compile data from AWS resources to determine bottlenecks in cloud architecture and improve application performance.Now let's see the implementation of AWS X-Ray with a nodejs application. AWS X-Ray provides an SDK that can be imported and utilized within your application.constexpress=require('express');constapp=express();constserviceName=\"HELLO-MICROSERVICE\"constport=8000;// Require AWS X-Ray SDKconstAWSXRay=require('aws-xray-sdk');AWSXRay.captureHTTPsGlobal(require('http'));// Use AWS X-Ray middlewareapp.use(AWSXRay.express.openSegment('MyApp'));app.get('/hello',(req,res)=>{constseg=xray.getSegment();seg.addAnnotation('hello-microservice',serviceName);seg.addMetadata(\"Request Meta\",req);res.send('Hello AWS X-Ray!');});// Close the X-Ray segment for the current requestapp.use(AWSXRay.express.closeSegment());app.listen(port,()=>{console.log(`App listening at http://localhost:${port}`);});Enter fullscreen modeExit fullscreen modeMake sure to installaws-xray-sdkbefore, when setting up instrumentation in nodejs app.X-Ray DaemonTheAWS X-Ray daemonis a core software application that listens for traffic on UDP port 2000. It gathers raw segment data and relays it to the AWS X-Ray API. The daemon needs to work in conjunction with the AWS X-Ray SDKs and must be running so that the data sent by the SDKs can reach the X-Ray service. The X-Ray daemon is an open-source project that you can follow on GitHub. See more details of the AWS X-Ray daemonhere.SegmentsAWS X-Ray receives service data insegments. XRay then groups segments that have a common request into traces. The resources running your application logic send data about their work as segments. Technically, it's an object that contains some metadata about the request including,The host - a hostname, alias, or IP addressThe request \u2013 method, client address, path, user agentThe response \u2013 status, contentThe work done \u2013 start and end times, subsegmentsIssues that occur \u2013 errors, faults, and exceptions, including automatic capture of exception stacks.SubsegmentsTo better monitor the work done by your application, you can use subsegments to break down the data into smaller pieces. Subsegments provide detailed timing information about downstream calls made by your application to complete the original request. They also contain additional details about calls to external services, such as AWS, HTTP APIs, or SQL databases. Furthermore, you can define custom subsegments to instrument-specific functions or lines of code within your application.TracesAWS X-Ray can help you trace requests as they move across various services. It captures important information about the path, duration, and performance of each request. A unique ID, called aTrace ID, is used to track the path of a request through your application. A trace is essentially a collection of all the segments generated by a single request. This request is usually an HTTP GET or POST request that passes through a load balancer, hits your application code, and generates downstream calls to other AWS services or external web APIs.A trace ID and a sampling decision are added to HTTP requests in tracing headers named X-Amzn-Trace-Id. The first X-Ray-integrated service that receives the request adds the tracing header, which is then read by the X-Ray SDK and included in the response.X-Amzn-Trace-Id: Root=1-5759e988-bd862e3fe1be46a994272793;Sampled=1Enter fullscreen modeExit fullscreen modeService GraphAWS X-Ray sends data about your app's services to create a graph that shows all the resources and services your app consists of. This graph is a JSON document that contains important information about your app's components. By using this graph, X-Ray can create a visual map of your app's pieces. This map helps you see how everything works together.X-Ray CostAWS X-Ray doesn't charge upfront fees or commitment. you only pay for what you use based on the number of traces, and segments recorded and retrieved. For the Free tier,The first 100,000 traces recorded each month are free.The first 1,000,000 traces retrieved or scanned each month are free.After the free tier or specific usage in the free tier, this is how this service charges you, which I think is not that expensive.Traces recorded cost $5.00 per 1 million traces recorded ($0.000005 per trace)Traces retrieved cost $0.50 per 1 million traces retrieved ($0.0000005 per trace).Traces scanned cost $0.50 per 1 million traces scanned ($0.0000005 per trace).X-Ray Insights traces stored costs $1.00 per million traces recorded ($0.000001 per trace).AWS X-Ray enables customers to choose their sampling rate. Customers considering AWS X-Ray may want to estimate their costs for recorded traces by multiplying their request or API call rate by the chosen sampling rate. Read morehereabout the specific regions and their costs.Earlier I mentioned that this service works smoothly with distributed systems or microservices in general. This is a microservices-based project on nodejs that is available onGitHubthat is provided byCloudAcademy. You can clone and do hands-on with X-Ray in a real microservices-based application.SummaryLet's recap what we have learned in this article. We learned what AWS X-Ray is and how we can use it with our applications irrespective of its architecture. AWS X-Ray is a service that helps you monitor and optimize your applications. It tracks requests, identifies issues, and provides insights into complex service relationships. You can trace message pathways and call stacks at scale, ensuring everything is running smoothly.ResourcesHere are the resources to get you started with AWS X-Ray.AWS X-Ray hands-on workshopAWS X-Ray with NodejsAWS X-Ray with GoAWS X-Ray troubleshootingAWS X-Ray Costingif you like this article, please like and share it with your cloud friends and follow me onLinkedin,Github,Twitter.Peace \u270c\ud83c\udffb"}
{"title": "How to run commands remotely on an EC2 Instance with AWS Systems Manager", "published_at": 1709853253, "tags": ["ec2", "systemsmanager", "iam", "role"], "user": "Revathi Joshi", "url": "https://dev.to/aws-builders/how-to-run-commands-remotely-on-an-ec2-instance-with-aws-systems-manager-1m2j", "details": "Systems Manageris a good choice when you need to view operation data for groups of resources, automate operational actions, understand and control the current state of your resources, manage hybrid environments, and maintain security and compliance.Using the run command, one of the automation features of Systems Manager, you can simplify management tasks by eliminating the need to use bastion hosts, SSH, or remote PowerShell.In this article, I am going to show you how to update the packages on an EC2 instance. At first, you will create an Identity and Access Management (IAM) role, enable an agent on your instance that communicates with Systems Manager, then follow best practices by running the AWS-UpdateSSMAgent document to upgrade your Systems Manager Agent, and finally use Systems Manager to run a command on your instance.Please visit myGitHub Repository for EC2 articleson various topics being updated on constant basis.Let\u2019s get started!Objectives:1.Create an Identity and Access Management (IAM) role2.Create an EC2 instance3.Update the Systems Manager Agent4.Run a remote shell scriptPre-requisites:AWS user account with admin access, not a root account.Create an IAM roleResources Used:IAMEC2AWS Systems ManagerSteps for implementation to this project:1. Create an Identity and Access Management (IAM) role123Next456Create role2. Create an EC2 instanceAttach a Systems Manager role to Amazon Elastic Compute Cloud (Amazon EC2) instances to make them managed nodes.12345673. Update the Systems Manager AgentNow that you have an EC2 instance running the Systems Manager agent, you can automate administration tasks and manage the instance. In this step, you run a pre-packaged command, called a document, that will upgrade the agent. It is best practice to update the Systems Manager Agent when you create a new instance.123456type in AWS-UpdateSSMAgentthen press Enterselect the radio button on the left of AWS-UpdateSSMAgent.This document will upgrade the Systems Management agent on the instance7Run8you will see a page documenting your running command, and then overall success in green.94. Run a remote shell scriptNow that your EC2 instance has the latest Systems Manager Agent, you can upgrade the packages on the EC2 instance.In this step, you will run a shell script through Run Command.123On the Run a command page, click in the search bar andselect, Document name prefix,then click on Equals,then type in AWS-RunShellScriptthen press Enter4On the Command Parameters panel and insert the following command in the Commands text box:sudo yum update5Run6While your script is running remotely on the managed EC2 instance, the Overall status will beIn Progress.7Then the Overall status will turn toSuccess.When it does, scroll down to theTargets and outputs paneland select the Instance ID of your instance.YourInstance IDwill be different than the one pictured.8select the header of the Output panel to view the output of the update command from the instance.CleanupDelete EC2 InstanceWhat we have done so farSuccessfully created an EC2 instance and remotely run a command using AWS Systems Manager. You first set up a correct role/permissions through IAM. Next you launched an Amazon Linux instance that was preinstalled with the Systems Manager agent. Finally, you used Run Command to update the agent and remotely perform a yum update."}
{"title": "Comprehending JIRA Tickets with Amazon Bedrock", "published_at": 1709850425, "tags": ["aws", "machinelearning", "tutorial", "communitybuilders"], "user": "AMELIA HOUGH-ROSS", "url": "https://dev.to/aws-builders/comprehending-jira-tickets-with-amazon-bedrock-520f", "details": "Back in August, I wanted to gain insights from JIRA ticket data to better understand what our customers needed from our team, and if there were any trends I could identify to provide better self-service and FAQ documentation.  I started that journey using AWS Comprehend which you can read about here:https://community.aws/posts/comprehend-jira-ticketsFor those keeping track,Amazon Bedrockbecame generally available in September of 2023.  My team had access to a preview, so when the AWS Comprehend entity analysis did not lend itself well to my use case; and I didn't feel like training a model, I started to get familiar with Bedrock.  The following post is a follow-on to the Community article above and fleshes out a few details that will help those newer to Amazon Bedrock navigate the product.First, getting into Amazon Bedrock from the AWS Console is pretty simple.  You select \"Bedrock\" from the console and select \"get started\" and it takes you to a great Overview page where you can explore several foundation models.  These foundation models are pre-trained by industry so you don't have to pay to train a model of your own.  These models are ready to be applied to your use case.To get access to a model and use it, that can be a bit counter-intuitive.  You need to click on the \"model access\" link to see which models your account has access to, if any,As you can see in the image below, some models are \"available to request\" and some others list \"access granted\".  If this is your first time using Bedrock, most likely, you will need to request access to the models you are interested in for your project.To finalize the request for the Large Language Model (LLM) of your choice, you have to click on \"Manage model access\".Once you've clicked the checkbox next to the model you're interested in, you need to \"save changes\" and wait a few minutes for the model to become available in your account.  AWS points out that you don't incur fees for using these LLM's until you've requested them and started using them.Now that you have access to a specific LLM, you can start working with it.For my Jira ticket use case, I leveraged the Bedrock Text Playground and selected Claude V2 as the LLM that I wanted to test using a sample of Jira ticket data.  Leveraging the playground, I could drop a large amount of text, and then use prompt engineering to see what the Claude V2 model could pull out.  I was happy to see that Bedrock's Anthropic option worked out of the box and appeared to support my use case in a way AWS Comprehend could not.What continues to surprise me is prompt engineering.  How you ask a question can drastically change the results returned by the LLM.For example, if I ask Claude V2 the following question:\"Provide a list of tickets that contain text AWS\"Claude V2 replies with:\"Here is the list of tickets that contain the text \"AWS\":\"BUT,If I ask Claude V2 to\"Provide a list of tickets that reference \"AWS\"\"It responds with this:\"Here are some of the tickets in the provided summary that reference AWS:\"The two ticket lists will include different tickets.  Using Claude V2 for this type of analysis seems to require defining a vocabulary for questions that will elicit the response my human brain is expecting.This outcome has been educational and highlights the opportunity to dive deeper into prompt engineering to ensure the Claude v2 output is what my human brain expects.All this being said, it is eye-opening how crucial someone's understanding of a native language is to be able to determine if the results you receive from an LLM are truly accurate.  You need to fully understand the data you're using and diagnose many different outcomes before you achieve the result you're looking for.Stay tuned for my next writeup that will discuss how to work with Amazon Bedrock from VisualStudio Code."}
{"title": "The Essential Guide to Building and Deploying Pay-per-Use ML APIs on AWS", "published_at": 1709812270, "tags": ["aws", "model", "machinelearning", "devops"], "user": "Mursal Furqan Kumbhar", "url": "https://dev.to/aws-builders/the-essential-guide-to-building-and-deploying-pay-per-use-ml-apis-on-aws-8kf", "details": "Whenever you create a machine-learning model, you always do your best! But how do you share its potential with the world and potentially earn from its use? This guide will show you how to create a pay-per-use API on the AWS cloud platform, allowing others to leverage your model's capabilities for a fee seamlessly.We'll explore three popular deployment approaches, each with its own advantages and considerations:1. SageMaker: Your Guided Machine Learning JourneyThink of SageMaker as a comprehensive machine learning workbench in the cloud. It assists you throughout the entire process, from building and training your model to deploying it in various ways:Real-time Endpoints:Imagine a service where users receive predictions instantly, just like responding to a query in a search engine. This is ideal for low latency scenarios, like real-time fraud detection or spam filtering in emails.Batch Transform Jobs:Have a massive dataset that needs offline processing for tasks like customer churn prediction or image analysis? SageMaker efficiently handles these bulk prediction jobs, saving you time and effort.Model Registry:Keep track of different versions of your model, monitor their performance, and manage them effectively. This ensures you're always serving the best possible version to your users, constantly improving your model's accuracy and reliability.SageMaker offers apay-as-you-gopricing model, making it a cost-effective choice for many projects. You only pay for the resources your model uses, whether for training, storage, or inference (making predictions).How to Deploy with SageMaker:a. Prerequisites:An AWS account with proper permissionsYour trained machine learning model saved in a compatible format (e.g., TensorFlow, PyTorch)An S3 bucket for storing your model artefactsb. Steps:Create a SageMaker Model:Upload your model artefacts to your S3 bucket and create a SageMaker model by specifying the model's source, container image (if applicable), and execution role.Create an Endpoint Configuration:Define the resources required for your model to run, such as instance type and memory allocation.Deploy a SageMaker Endpoint:Combine your model and endpoint configuration to create a real-time endpoint for inference.Integrate with API Gateway:Use API Gateway to create a public API endpoint that interacts with your SageMaker endpoint. Users can then send requests through this API to receive predictions from your model.Pros:User-friendly interface with pre-built functionalitiesPay-per-use pricing for resources usedIntegrated model management and monitoringCons:Less control over the underlying infrastructure compared to containersResources:SageMaker documentation:https://docs.aws.amazon.com/sagemaker/Deploying models as APIs:https://docs.aws.amazon.com/sagemaker/latest/dg/how-it-works-deployment.html2. Serverless Simplicity with AWS Lambda: Cost-Effective and ScalableFor cost-conscious projects with low expected traffic, consider AWS Lambda. This serverless technology allows you to run your model code only when someone interacts with your API, significantly reducing costs. It's like renting a tiny server that springs to life only when needed, saving you from managing and paying for idle resources.How to Deploy with Lambda:a. Prerequisites:An AWS account with proper permissionsYour trained machine learning model packaged as a Python function (including dependencies)b. Steps:Create a Lambda Function:Define a Python function that encapsulates your model's inference logic. Upload this function along with its dependencies to AWS Lambda.Configure the Lambda Function:Set the memory and timeout limits for your function based on your model's requirements.Choose an appropriate execution role that grants your function access to necessary resources (e.g., S3 bucket for model artefacts).Create an API Gateway:Create a public API endpoint that triggers your Lambda function when a request is received. Define the API method (e.g., GET, POST) and the data format for requests and responses.Test and Deploy:Test your API endpoint by sending requests and ensuring it functions as expected. Once satisfied, deploy your API to make it publicly accessible.Pros:Highly cost-effective for low-traffic APIsServerless architecture eliminates infrastructure managementEasy to scale automatically based on demandCons:Limited to Python runtime environmentMay not be suitable for complex models or real-time requirements with stringent latency needsResources:Lambda documentation:https://docs.aws.amazon.com/lambda/Serverless ML APIs on AWS:https://aws.amazon.com/tutorials/machine-learning-tutorial-deploy-model-to-real-time-inference-endpoint/3. Containerized Power with ECS or EKS: For Advanced UsersContainerization offers greater control and flexibility for deploying complex models or those requiring specific runtime environments. However, it also involves managing the underlying infrastructure. Consider using services like Amazon Elastic Container Service (ECS) or Amazon Elastic Kubernetes Service (EKS) for container orchestration.How to Deploy with Containers (general overview):Containerize your Model:Package your model and its dependencies into a Docker container image. This ensures consistent execution across different environments.Deploy the Container Image:Push your container image to a registry like Amazon Elastic Container Registry (ECR).Create a Container Service:Choose between ECS or EKS depending on your needs and expertise. Both offer ways to define and manage containerized applications.Configure the Service:Specify the container image, desired resources (CPU, memory), and scaling policies for your service.Expose the Service:Configure your service to be accessible through a public endpoint (e.g., using a load balancer).Integrate with API Gateway:Similar to Lambda, create an API Gateway endpoint that interacts with your containerized service. This allows users to send requests and receive predictions through the API.Note:This is a high-level overview, and the specific steps involved will vary depending on the chosen container orchestration service and your specific use case. Refer to the respective documentation for detailed instructions.Pros:Greater control over the underlying infrastructureSupports various programming languages and frameworksSuitable for complex models and real-time applicationsCons:Requires more setup and management compared to serverless optionsSteeper learning curve for container orchestrationResources:ECS documentation:https://docs.aws.amazon.com/ecs/EKS documentation:https://docs.aws.amazon.com/eks/Choosing the Right Champion:The ideal option for you depends on several factors, including:Model complexity:More complex models might benefit from the flexibility of containers.Cost considerations:If budget is a major concern, Lambda's pay-per-use model can be highly cost-effective.Performance requirements:Real-time applications might necessitate the lower latency offered by containers.I hope this guide provides a helpful overview of deploying your machine learning model as a pay-per-use API on AWS. By understanding the available options and their unique characteristics, you can make an informed decision and unlock the full potential of your creation!"}
{"title": "Say goodbay to the past and modernize your Mainframe with AWS", "published_at": 1709807894, "tags": ["aws", "cloud", "mainframe", "modernization"], "user": "John Bulla", "url": "https://dev.to/aws-builders/say-goodbay-to-the-past-and-modernize-your-mainframe-with-aws-587b", "details": "Note:\u270b This post was originally published on my blogwiki-cloud.coIntroductionIn today's business world, the technology is evolving at an unprecedented speed, and companies are constantly looking for ways to adapt, update their technology and improve their operations. In this context, mainframe modernization has become a pressing need for many organizations that depend on these robust infrastructures to run their critical and business applications, with the aim of improving and strengthening their infrastructures in an increasingly competitive business environment offering better solutions to their clients.The companies that obtain to modernize their mainframes in the cloud, not only improve their ability to respond to market changes, but also benefit from cost and resource optimization, improved business agility, and the ability to offer more innovative services and products adapted to consumer demands.Let us remember that IBM (International Business Machines Corporation) was one of the main pioneers in the development and evolution of mainframes, where IBM defines the mainframe as follows:\"Mainframes are high-performance computers with large amounts of memory and processors, which process billions of simple calculations and transactions in real time. The mainframe is critical for business databases, transaction servers, and applications that require resiliency, security, and agility. Mainframes are data servers designed to process up to 1 billion transactions daily with the highest levels of security and reliability.\"Mainframe historyThe history of the mainframe dates back to the mid-20th century and has been a fundamental part of the development of enterprise computing. Below I summarize the main milestones in the history of the mainframe:1940s:In 1944 IBM, together with Harvard University, developed theASCC, \u201cAutomatic Sequence Controlled Calculator\u201d, also know as\u201cThe Harvard Mark I\u201d, considered the first electromechanical computer, marking the beginning of the mainframe era.1950s:IBM emerges as a leader in the mainframe industry with its 700/7000 series. UNIVAC I, created by Remington Rand, became the first commercial computer sold on a large scale.1960s:In 1962 IBM announced that COBOL would be its primary development language. In 1964, IBM launched the System/360 series, an innovative family of mainframes that offered compatibility across multiple models and laid the foundation for future developments in mainframe architecture. Transistors were adopted in this generation of mainframes, which significantly increased processing speed and reduced power consumption.1970s:In this decade, IBM introduced the System/370 series, a family of mainframes that marked a before and after in the industry, where introduced the integrated circuits and semiconductor memory, further improving performance and reducing physical size.1980s and 1990s:IBM embraced open systems and introduced the System/390 series, which could run multiple operating systems, including Linux, along with traditional mainframe operating systems such as z/OS. This move allowed mainframes to remain relevant in a rapidly changing computing landscape.2000s:In the new millennium, modern mainframes (zSeries) released by IBM continued to advance in processing power, memory, and I/O capabilities. Mainframe vendors introduced virtualization technologies that allowed multiple virtual machines to run simultaneously on a single mainframe.2010s to today:The cloud and artificial intelligence appear, opening new opportunities for mainframe modernization.Over the decades, the mainframe market has seen the emergence of various manufacturers, but it is indisputable that IBM has stood out as the undeniable pioneer in this technology. Since its first steps, IBM has led the evolution of mainframes, establishing standards, continuously innovating and consolidating its position as a benchmark in the industry. Although new competitors have emerged over time, IBM's lasting influence and continued dedication have left an indelible mark on the history of mainframes, cementing its reputation as the undisputed leader in the field.Mainframe Modernization on AWSAWS currently offers a cloud mainframe application modernization and migration service calledAWS Mainframe Modernization, which is much more than a simple mainframe migration to the cloud. It is a comprehensive approach that addresses the complexity of legacy systems, allowing companies to overcome technological limitations and embrace the agility, scalability and efficiency inherent in the AWS cloud.This service offers a set of tools and best practices designed to facilitate the modernization of mainframe applications to managed execution environments in the AWS cloud. It allows you to select migration and modernization patterns such as replatforming or automatic refactoring by completely rewriting and modernizing mainframe applications. AWS Mainframe Modernization allows organizations to choose the path that best suits their specific business needs.Additionally, it allows you to analyze existing mainframe applications, develop and update these using languages such as COBOL or PL/I, and implement automated continuous integration and continuous deployment (CI/CD) pipelines using the AWS toolset.During the November 2021 re:Invent event, AWS Mainframe Modernization wasreleased to the public (GA)for the benefit of customers and partners, making this innovative solution available to them in the AWS cloud.AWS has been recognized as a leader in the report\u201c2023 ISG Provider Lens Mainframes \u2013 Services and Solutions for Mainframe Application Modernization Software\u201d. This report ranks software vendors and toolsets that enable legacy application assessments and application conversion (replatform, rehost, refactor, rewrite, or redesign).How does the AWS Mainframe Modernization Service work?AWS Mainframe Modernization is a set of services and tools designed to facilitate the migration and modernization of applications and workloads running on mainframes to the AWS.Next, I will explain in a general way how this service works and each of its stages:AnalysisAt this stage, are established clear objectives for modernization, such as improving agility, reducing costs, or taking advantage of new cloud capabilities.A comprehensive assessment of existing mainframe applications and workloads is performed to understand their architecture and dependencies.Applications are prioritized for modernization based on criteria such as business criticality, technical complexity, and the potential for quick wins.Finally, a migration plan is established that may include the progressive modernization of applications in several phases.TransformationThe transformation stage is essential to lay the foundation for a successful migration and ensure that cloud-adapted applications are optimal in terms of performance, costs and business agility.In this stage, the mainframe modernization strategy is evaluated and selected where you can opt for a pattern focused on refactoring the application or a pattern focused on replatforming the existing mainframe infrastructure in the AWS cloud.Choosing the appropriate strategy will depend on the specific objectives of the company and the characteristics of the mainframe applications to be modernized.DevelopmentAt this stage are offer tools that allow to the applications to be adapted by making modifications to them to take advantage of the advantages of the cloud and new technologies.It can also be used to integrate modernized applications with other existing systems in the cloud or on-premises environments.TestAt this stage, tools are offered to create and run automated application tests and verify the correct operation of the applications.Deployment and operationThe deployment stage is essential because we will now be able to see the modernized mainframe application implemented within the native AWS execution environments.Additionally, continuous monitoring mechanisms are established to monitor the performance, security and availability of applications in the AWS cloud.AWS Mainframe Modernization is a managed service, so AWS is responsible for the management and maintenance of the infrastructure within the cloud environment.PatternsTheAWS Mainframe Modernization patternsare a set of predefined and validated solutions that help migrate and modernize mainframe applications quickly and efficiently. These patterns are based on industry best practices and are designed to help overcome common mainframe modernization challenges.Automatic refactoring with Blu AgeA couple of years ago AWS acquired the company Blu Age that provides software and tools to modernize mainframe applications. Currently, Blu Age, through its AWS Blu Insights solution, has been integrated into the AWS Mainframe Modernization service solution stack.AWS Blu InsightsIt is a comprehensive service ofAWS Mainframe Modernizationthat facilitates planning, collaboration, and modernization of legacy mainframe applications. This service is presented as a Software as a Service (SaaS) tool that automatically performs refactoring in AWS, converting existing COBOL code to Java while preserving the original logic and functionality. This process not only changes the language, but also modernizes the code structure to take advantage of the Java platform.The benefits of the Blu Age solution are as follows:Time and effort savings:Significantly reduces the time and effort required to modernize mainframe applications.Preservation of logic and functionality:The resulting Java code preserves the logic and functionality of the original COBOL code.Leverage the benefits of Java:Take advantage of the benefits of the Java platform, such as portability, scalability, and the broad developer community.Code modernization:Java code is modernized using techniques such as refactoring and reengineering to improve its readability, maintainability, and scalability.Performance improvement:Optimizations are made to improve the performance of Java code in the cloud.The AWS solution for refactoring with Blu Age works as follows:Blu Age analyzes the original COBOL code to understand its logic and structure.An equivalent Java code is generated that reflects the logic and functionality of the original COBOL code.Java code is refactored to improve its readability, maintainability, and scalability.Optimizations are made to improve the performance of Java code in the cloud.Blu Age supports a wide range of COBOL applications, including:Business applicationsFinancial applicationsData management applicationsTelecommunications applicationsReplatform with Micro FocusAWS partners with Micro Focus to offer mainframe migration as a service. Micro Focus is an AWS replatform solution that allows you to run COBOL and PL/I mainframe applications without modifications in a virtualized mainframe environment in the cloud. This option offers a familiar, low-risk path to mainframe modernization.The benefits of the Micro Focus solution are as follows:Risk reduction:Minimize the risk of errors during migration by avoiding code modifications.Gradual transition:Allows a gradual transition to the cloud, migrating applications at an optimal pace.Familiar environment:Provides a familiar environment for mainframe developers who do not need to learn new technologies.Leverage the cloud:Take advantage of the benefits of the cloud, such as scalability, security, and cost efficiency.The AWS replatform solution with Micro Focus works as follows:A virtualized mainframe environment is created on AWS using Micro Focus technology.Mainframe applications are migrated to the virtualized environment without the need to modify the code.Mainframe applications run in the virtualized environment as if they were on a physical mainframe.Benefits of modernizing with AWSAgility and scalability:The cloud allows applications to scale quickly and efficiently to meet changing business demands.Cost reduction:Reduces operating and maintenance expenses associated with traditional mainframe infrastructure.Accelerated innovation:Modernization opens the door to new technologies and frameworks, driving innovation and the development of new functionalities.Security and Reliability:AWS provides a highly secure and reliable environment for critical mainframe applications.AWS Migration Acceleration Program (MAP) for MainframeThe AWS Migration Acceleration Program (MAP) for Mainframe is an extension of theAWS MAP programexisting, this program is designed to help organizations achieve their migration goals even faster using AWS services, best practices, tools, and incentives. AWS provides a three-step approach to help reduce the uncertainty, complexity, and cost of migrating to the cloud.Through MAP for Mainframe, customers gain access to a complete ecosystem of mainframe technology and partners with professional services expertise who can assist with migration and modernization processes. Customers can enhance their core applications and data portfolios by leveraging the agility, ease of access, and community of the cloud.ConclusionsUsing the AWS Mainframe Modernization service offers organizations a clear path to digital transformation, enabling the efficient migration and modernization of mainframe applications to the AWS cloud. This process highlights AWS's ability to deliver flexible modernization strategies, from rapid, low-risk approaches to the adoption of cloud-native architectures. The benefits include greater business agility, cost reduction, scalability and leveraging the latest technologies. Additionally, the integration of services such as AWS Lambda, Amazon RDS, and CI/CD practices drives innovation and accelerates the development cycle, positioning organizations to successfully meet the technological challenges of the future.ResourcesReview the following resources to learn more about mainframe modernization on AWS:AWS Mainframe ModernizationYouTube \u2013 Introduction AWS Mainframe ModernizationBlog \u2013 AWS Mainframe ModernizationAWS Mainframe Modernization User GuideAWS Mainframe Modernization Refactoring with AWS Blu AgeAWS Mainframe Modernization Replatform with Micro FocusAWS MAP for MainframeI hope this information is useful.Best Regards,Follow me:\ud83d\udc49Wiki Cloud| \ud83d\udc49Twitter| \ud83d\udc49LinkedIn| \ud83d\udc49Github| \ud83d\udc49Youtube"}
{"title": "Tapping into a more productive mindset for AWS certification exam preparation", "published_at": 1709805400, "tags": ["aws", "certification", "study"], "user": "Arpad Toth", "url": "https://dev.to/aws-builders/tapping-into-a-more-productive-mindset-for-aws-certification-exam-preparation-5god", "details": "Having AWS certifications comes with many benefits. But it does matter how and how long we prepare for the exams. The way we see this question can be different for everyone.1. Why do I think I'm authentic enough to talk about it?It's known that AWS certifications validate our cloud knowledge and show that we have some experience (depending on the level) in the AWS ecosystem.I've been working with AWS services for six years. I got my first certification, theSolutions Architect (SA) Associate, in 2019 and took my last exam (Solutions Architect (SA) Professionalrecertification) 10 days ago. But, my exam preparation process and my mindset about the certifications is different now compared to years ago.I currently holdseven certifications, including both professional ones. Some people have reached out to me asking for advice on how to prepare for exams. What courses do I recommend? How long does it take to prepare for an exam?So, I decided to share my thoughts and perceptions on the certifications and the exam preparation. This post is not a collection of specific tips and tricks. Instead, it's about a general approach to the exams that made my preparation process more fun and gave me more free time.The mandatory disclaimer before moving on. The approach below has worked for me. I can't guarantee that it works for you. Do what suits you best when you prepare for any exam.2. You don't need to get the perfect scoreI always knew it, but it took me some time to become mindful of it. I didn't want to accept it earlier because I have always been a perfectionist and wanted to score as much as possible.I liked being proud of the work I gave out of my hands regardless of what I did. But the harsh truth is that I didn't get much benefit from it most of the time. I wasn't assigned any large project or a promotion because I produced high-quality code. In fact, my manager told me that my velocity could be improved.So, I started forcing myself to accept the less-than-perfect, which proved to be an enlightening experience.Again,you don't need a 100% scoreto succeed in the exam.The pass scores forassociateandprofessional/specialtylevels are 720 and 750 out of 1000, respectively. It's what you need, nothing more.Would you study five months to get 950+ points or two weeks for 820? (The numbers and durations are arbitrary.)I now choose the latter.But it wasn't always the case. For many years, I studied hard and long because I wanted the perfect score. But it never came, no matter how much effort I put into my studies. There was always at least one question that I couldn't answer. A configuration I didn't remember or forgot to check. I felt it was outside my control.But then I discovered something.No one had ever asked what my scores were. People around me at work didn't care. Chances are it won't matter to you either.So now I do my best to find the right amount of study to pass the exams. With this approach, I found that preparation takes less time, and I achieved the same certification with more fun and less stress. I don't spend late nights and weekends studying. I did enough overtime and weekend work in my career. I can now enjoy more time with my family and hobbies, and I'm still getting the same results (the certifications) as earlier.You might ask if I study for a lower score and am happy about it, won't my knowledge have gaps?The intuitive answer seems yes, it will, but I don't think this way. As of this writing, the SA Pro exam guide lists 128 services candidates should be familiar with. There are superheroes out there who know all of them and score 1000.But I'm an average person, not a superhero. I admit I'm not familiar with all services, settings and configurations. Of course, I look at them to see what they do. But I will dig deep into them when I start working on a project that requires a more detailed understanding.I could have never stopped studying if I wanted to be familiar with everything. I will forget a large chunk of it after the exam anyway because I don't use it. Instead, I focus on the certification-specific core services and want a good knowledge of them. It has proved to be enough for me.3. Be readyI only book an exam when Iamready. This seems obvious again, but there is a difference between being and feeling prepared.Actually, I never really felt ready for an exam. But I was prepared since I passed all, so my knowledge was enough even if I thought differently. The mindset shift began when I realized that being and feeling ready are different.It relates to the previous point, i.e., you don't need the perfect score. Once I realized it, I had the guts to book the date even if I didn't study months for a professional or specialty exam.4. Get practical experienceI found that hands-on experience increases the probability of passing the exam. It also reduces the time I spend studying. This point might again sound straightforward, and you might not understand why I'm writing this.When I got my first certification (the Cloud Practitioner didn't exist back then), I didn't have much AWS experience. I just started to get my feet wet. I decided to take the SA Associate because I wanted to learn how AWS and the cloud in general work. I studied for six months, almost every night after work and on weekends.Finally, I got the certification with a high score, but I wouldn't do it this way again.I now go to my account and come up with someexercisesor follow workshops. I still read the documentation and listen to videos. But now I do more practice than studying in the areas I'm less familiar with. I try various configurations and, of course, make mistakes. I occasionally write blog posts on these exercises. :)As a result, my scores are not in the high 900s anymore, but I enjoy the process more. And, as stated above, you don't need 100%. The pass score is sufficient.5. Walk the ladderAWS doesn't require that candidates have the associate certification before they take the professional exam. But I wouldn't skip them, and here's why.5.1. Have solid foundationsAssociate-level concepts provide the foundations for the specific path (architecting or DevOps). I started to study for the DevOps Engineer Professional exam years ago without having the Developer Associate or the SysOps Administrator Associate. I gave up after a few weeks because it didn't make sense.Now, I aim to have solid foundations first. For example, I plan to take the Machine Learning Specialty exam later this year or early next year. But want to be better at data engineering first. I want to know the basics.How am I planning to get the knowledge? Through practice.Years ago, I would have enrolled on a course, studied it through to get the paper, and then forgotten almost everything because I didn't use it. I'll now build projects, which will be fun! Then, the actual study process will be shorter and less stressful. If you are in a role where you use the services daily, great! The practice field is provided. If not, like in my case, I'll build something for myself and write some articles about the process.AWS Skill Builderhas greatlearning plans, and not only for architecting. You can get deeper in storage, SQL databases, serverless and many more. I love them, and many plans come with labs (need a subscription) and will give you a badge, too. I found them useful when I prepared for the professional exams.5.2. Get the associate certifications with the professional onesAnother reason to have the associate (and Cloud Practitioner) certifications is that you'll automatically get them extended if you pass the relevant professional exam.If you pass the SA Pro, you will also get the SA Associate, provided you have it already. In this case, both will be valid until the same day.Similarly, if you succeed on the DevOps Engineer Professional exam, you'll get the Developer and SysOps Administrator Associate certifications. But you have to have them already.I only need to retake three exams and will get all seven certifications I  have now!5.3. Two or more birds with one-and-a-half stoneThe professional and some specialty exam requirements have a significant overlap.For example, the concepts you need to know for the DevOps Engineer Pro can be reused on the Solutions Architect Pro exam. If you study for the DevOps Engineer Pro, the SA Pro will be a few more concepts and services away. It's the same with the Security Specialty. It's a low-hanging fruit, and it's not worth waiting months or years to take the other exams.I now allocate some weeks I use for intensive study. I could take both professional exams in two weeks. In fact, there are people out there who sit them on consecutive days. I chose to go slower because I wanted free weekends and nights. The point is that I used the momentum and, with not too many extra hours invested, I could get another valuable certification.6. SummaryIn summary, a change in my mindset towards exam preparation led me to get the same outcome with fewer study hours. I enjoy the journey better compared to earlier years because I do many practical exercises. I also do my best to build a solid foundation in the main topics before I attempt to take an exam.I hope you can use something from this post. Please let me know if you are interested in reading (or watching!) similar content. All the best to your AWS certification journey!7. Further referencesExplore all AWS Certification exams- Everything about the certifications at one placeAWS Workshops- Collection of hands-on exercisesFree exam retake- If you book your exam until 15 April 2024"}
{"title": "CDK Stack Notification Options", "published_at": 1709803125, "tags": ["cdk", "aws", "infrastructureascode"], "user": "NaDia", "url": "https://dev.to/aws-builders/cdk-stack-notification-options-35f2", "details": "Today, I discovered yet again that there are countless ways to tackle a single task as a developer.I was tasked with automating a workflow that involved an AWS Lambda Function triggered by an SNS event source. The goal was to publish a message to an SNS topic in a different AWS account when the status of a CloudFormation stack updated.We useAWS CDKfor infrastructure as code (IAC). While exploring the documentation and blog posts, I found that there is no direct equivalent of the Notification Policy in CloudFormation to publish notifications to an SNS topic on a CloudFormation stack status change. Instead, there are several common patterns to achieve this. Let's start with solution diagram. Here's a simplified version of the architecture diagram of what I implemented:If this is what you are looking for, you can simply achieve it in 3 different ways:Using AWS Event BridgeCreate SNS Topic as a Stack B CDK resource:import { Topic } from \"aws-cdk-lib/aws-sns\";     const SNSTopic = new Topic(this, \"SNS_TOPIC_ID\", {       displayName: \"YOUR DISPLAY NAME\",     });Enter fullscreen modeExit fullscreen mode_Note: you need to add an event source to your function or any other resources that is going to subscribe to this topic. in my case I needed to configure a lambda event source as _bellow:MyFunction.addEventSource(new SnsEventSource(SNSTopic));Enter fullscreen modeExit fullscreen modeAdd Event Rulenew Rule(this, \"Trigger\", {       eventPattern: {         source: [\"aws.cloudformation\"],         detailType: [\"CloudFormation Stack Status Change\"],         detail: {           eventName: [\"CREATE_COMPLETE\", \"UPDATE_COMPLETE\", \"DELETE_COMPLETE\"],           requestParameters: {             stackName: [this.stackName],           },         },       },       targets: [new SnsTopic(SNSTopic)],     });Enter fullscreen modeExit fullscreen modeUsing AWS Custom ResourcesSecond approach to achieve this is by using AWS Custom Resources.Create an AWS Custom Resource within the Stack B CDK:import { AwsCustomResource, AwsCustomResourcePolicy, PhysicalResourceId } from \"aws-cdk-lib/custom-resources\";      const Trigger = new AwsCustomResource(this, \"TriggerOnSuccess\", {       onUpdate: {         service: \"SNS\",         action: \"publish\",         parameters: {           TopicArn: \"YOUR_TOPIC_ARN\",           Message: \"Stack updated successfully\",         },         physicalResourceId: PhysicalResourceId.of(\"TriggerOnSuccess\"),       },       onDelete: {         service: \"SNS\",         action: \"publish\",         parameters: {           TopicArn: \"YOUR_TOPIC_ARN\",           Message: \"Stack deleted successfully\",         },         physicalResourceId: PhysicalResourceId.of(\"TriggerOnSuccess\"),       },        policy: AwsCustomResourcePolicy.fromStatements( [new PolicyStatement({         actions: [\"sns:Publish\"],         effect: Effect.ALLOW,         resources: [SNSTopic.topicArn],       })]),     });Enter fullscreen modeExit fullscreen modeAdd Dependency order so that CDK doesn't return Dependency Cycle errorTrigger.node.addDependency(YOUR_FUNCTION);  Trigger.node.addDependency(SNSS_TOPIC);Enter fullscreen modeExit fullscreen modeUsing AWS Custom Resource with Lambda Invoke ActionThere is also a 3rd solution for this as well which I am not a big fan of it and that is. I personally prefer to use fan out approach, to populate events to Lambda via an \"Event Service\" such as SNS or EventBridge. If you look for a simplified Custom Resource, here is what you should update your AWS Custom Resource to:import { AwsCustomResource, AwsCustomResourcePolicy, PhysicalResourceId } from \"aws-cdk-lib/custom-resources\";      const Trigger = new AwsCustomResource(this, \"TriggerOnSuccess\", {       onUpdate: {         service: \"Lambda\",         action: \"invoke\",         parameters: {           FunctionName: \"YOUR_FUNCTION_ARN\",           InvokationType: \"Event\"         },         physicalResourceId: PhysicalResourceId.of(\"TriggerOnSuccess\"),       },       onDelete: {         service: \"Lambda\",         action: \"invoke\",         parameters: {           FunctionName: \"YOUR_FUNCTION_ARN\",           InvokationType: \"Event\"         },         physicalResourceId: PhysicalResourceId.of(\"TriggerOnSuccess\"),       },        policy: AwsCustomResourcePolicy.fromStatements( [new PolicyStatement({         actions: [\"lambda:InvokeFunction\"],         effect: Effect.ALLOW,         resources: [YOUR_FUNCTION_ARN],       })]),     });Enter fullscreen modeExit fullscreen modeThis concludes our brief discussion. While I'm still hopeful about discovering if CDK offers an API for configuring Stack Notification Options, I wanted to share these workarounds in the meantime."}
{"title": "Amazon Q", "published_at": 1709796907, "tags": [], "user": "Kenneth Aladi", "url": "https://dev.to/aws-builders/amazon-q-38gc", "details": "Introducing Amazon Q: A Generative AI Assistant Tailored for Your BusinessAmazon is making waves in the world of artificial intelligence with the introduction ofAmazon Q, a newgenerative AI (GenAI)assistant designed specifically for the workplace. This innovative tool promises to revolutionize the way businesses interact with information, solve problems, and boost productivity.What is GenAI?GenAI refers to a type of AI that cangenerate new text, code, or other creative content. Unlike traditional AI which focuses on analyzing and interpreting existing data, GenAI takes things a step further by creating entirely new outputs. Amazon Q leverages this powerful technology to provide a dynamic and interactive experience for users.How Does Amazon Q Work?Imagine having a personal AI assistant at your fingertips, one that understands your business inside and out. That's the essence of Amazon Q. This versatile tool can be tailored to your specific needs by connecting to your company's:Information repositories:This includes documents, knowledge bases, and other internal resources.Code:Q can access and understand your codebase, assisting with development tasks.Data:Analyze data and gain insights with Q's help.Enterprise systems:Q can interact with various existing systems, streamlining workflows.Benefits of Amazon Q:Engages in conversations:Q doesn't just provide answers; it fosters interactive dialogue, helping you solve problems and explore solutions.Generates content:Need help with reports, emails, or other written materials? Q can generate drafts based on your needs.Takes action:Q can connect to your systems and perform actions based on your requests, saving you time and effort.Security and Customization:Security and privacy are paramount for businesses. Amazon Q is built with these concerns in mind, featuring:Fine-grained access controls:Q ensures responses align with your permission levels and only use trusted information sources.Citations and references:Q provides references for its responses, enabling fact-checking and transparency.Customization:Q can be tailored to your business domain and terminology, ensuring its responses are relevant and contextual.Current Applications:Amazon Q is still in preview, but early integrations showcase its potential:AWS Services:Q acts as an expert on AWS services, guiding users, exploring new features, and suggesting best practices.Amazon QuickSight:Gain data insights faster with Q's assistance in formulating queries and interpreting results.Amazon Connect:Contact center agents can leverage Q for real-time assistance, providing faster and more accurate customer service.The Future of GenAI in the Workplace:Amazon Q represents a significant step forward in GenAI technology. Its ability to learn, adapt, and interact with business-specific information positions it as a transformative tool for organizations of all sizes. As GenAI continues to evolve, we can expect even more innovative applications that enhance productivity, streamline workflows, and unlock new possibilities in the workplace."}
{"title": "Building a Serverless CRM integration with Twilio and AWS", "published_at": 1709786140, "tags": ["aws", "serverless", "twilio", "crm"], "user": "Olawale Adepoju", "url": "https://dev.to/aws-builders/building-a-serverless-crm-integration-with-twilio-and-aws-59mm", "details": "Overview of the SystemWe all want a system in which we want near-real-time access to our web-based application as well as the application server for the engineers, or almost zero downtime of our application. Also well we want our engineers to be informed almost immediately to have an idea of \u200b\u200bwhat's going on at the backend of the application server even when not present.The system shows the integration of   Twilio, Auth0 Authentication, and different AWS Serverless services represent the user can access the application over the internet, Auth0 authenticates the user credentials based on the organization users, and when the application server is down, it can trigger a call for the engineer to inform the status of the server.What is TwilioTwilio is a cloud platform, that allows software developers to programmatically make and receive phone calls, send and receive text messages, and also carry out other communication functions with the use of its APIs.Twilio bridges a gap between web-based applications and telephonesTwilio StudioTwilio Studio is a powerful visual editor used for building, editing, and managing communication workflows.The studio can be used to design systems from notifications, IVRs, and chatbots.Studio flows are the entry points for all calls and messages within flex.Twilio functionTwilio Functions is a Serverless development environment that allows you to build an event-driven and scalable Twilio application.Functions are server-side code written in Node.js and run directly on the Twilio platform. Also, functions replace the need to find hosting or run a server to serve any other HTTP-based responses. The worry of maintaining your web infrastructure to support your application is not needed.AuthenticationAuthentication is the process of identifying a user\u2019s identity. It is the process of linking an inward request with a set of identifying credentials. The credentials given are used in comparison to those in the database of the authorized user\u2019s information within an authenticated server.Different systems may require a different type of credentials to confirm a user\u2019s identity. The types of authentication include:Password-based authenticationMulti-factor authenticationCertificate-based AuthenticationBiometrics AuthenticationToken-based AuthenticationAuth0Auth0 is a flexible solution to add authentication and authorization services to your applications. In this system, Auth0 authenticates the user\u2019s identity using password-based authentication and compares it to the credentials of the user in the organization.Auth0 OrganizationAuth0 Organizations feature represent a wide range of Auth0 platform that helps business-to-business (B2B) customers manage their partners and customers efficiently and also customize the accessibility of the end-users to their application.In this system, each user belongs to a unique Organization and can be identified and authenticated using the credentials of the user as registered in the organization.Each Organization has a unique Organization ID, each user is added to the organization with a user name and password which makes the user a unique member of the organization.Only a member of an organization can access the organization.Referencestwilio.comhttps://www.twilio.com/docs/flex/admin-guide/core-concepts/studio-flows-functions-assets-twimlhttps://auth0.com/docs/https://auth0.com/docs/organizations"}
{"title": "AWS To Provide Free Data Transfer Out To Internet", "published_at": 1709777497, "tags": [], "user": "Jason Paul", "url": "https://dev.to/aws-builders/aws-to-provide-free-data-transfer-out-to-internet-4beo", "details": "IntroductionA few days ago, AWS announced in ablog postthat they would be providing free DTO (Data Transfer Out) of AWS, when moving data to another cloud provider or to an on-premises data center. This is a pretty big deal, because one of the common concerns from companies looking to adopt the cloud, is the expense of moving their data out, causing vendor lock-in.In this article, we will explore details on transferring data in and out of AWS, some of the more recent regulatory changes, and the potential business impact.UpdatesUpdate: 03/17/2024- On March 13, 2024, Microsoft announced that Azure would be following suit and allowing free egress for customers leaving Azure when taking their data out of the Azure infrastructure via the internet to switch to another cloud provider or an on-premises data center.  This would be on top of the first 100GB/month free egress data already provided.Azure Announcement PostTechCrunch News ArticleUpdate: 03/26/2024- It was pointed out that AWS Snowmobile is no longer available.  Recently, theweb page for the servicehas been removed, and redirects to the AWS Snowball page. I received confirmation from AWS that the service was discontinued, and was provided with this statement:AWS is always innovating on behalf of customers, which means we must sometimes make the decision to pivot when we believe our resources should be invested elsewhere to better serve customers. For that reason, we no longer offer AWS Snowmobile.Data Transfer InTypically, there is no charge for inbound data transfer to AWS.  The challenge becomes how to get the data into AWS quickly and efficiently.If a small amount of data needs to be transferred, it can be uploaded directly via the internet, for example to an S3 bucket.For larger datasets, or to set up a larger job to connect between an on-premises data center and AWS, the two most common options areSite-to-Site VPNandDirect Connect, both which come with a cost.AWS DataSynccan also be set up to copy between on-premises and AWS, to automate moving data to a number of AWS services.For extremely large datasets that cannot be transferred over the internet, or in an environment where there is no consistent network connectivity, theAWS Snow Familyis available.  This allows for some edge computing to collect and process data, and move it to the AWS cloud by physically shipping the device to AWS.AWS Snowconehas an HDD and SSD device type, which can transfer 8-14TB of data securely on a device small enough to put in a backpack.AWS Snowballhas anumber of devices optimizedfor edge computing and data transfer.  The service allows you to order a ruggedized device that can hold multiple terrabytes to petabytes of data, to transfer to AWS.You would set up aSnowball Edge Jobto define how to import the data into S3. Once the data is copied to the Snowball, it can be shipped back to the proper AWS datacenter to be uploaded and complete the job.If you have extremely large amounts of data to transfer, such as hundreds of petabytes or into exabytes,AWS Snowmobilecan move up to 100PB at once via a ruggedized shipping container. The ruggedized shipping container is tamper-resistant, water-resistant, temperature controlled, and GPS-tracked.  The service wasannounced in 2016, and one of the trucks shown during a presentation that year at AWS re-Invent:https://www.youtube.com/watch?v=8vQmTZTq7nwFor all of the above methods, the charges would be for the method of data transfer, but the actual data transfer into AWS would cost $0.00/GB.This CNBC piece details all of the AWS Snow family options in much more detail:https://www.youtube.com/watch?v=H3_ZqnqLyVoData Transfer OutMany of the methods for transferring data into AWS can also be used to transfer data out.  The catch is that until recently, you would also be charged per GB of data transferred out.  For example, see thisAWS blog postfrom 2010, when outbound data transfer prices were reduced.  Companies would pay $0.08 - $0.15 per GB transferred out per month.  This would mean that for companies that have extremely large data sets in AWS, getting that data out would be prohibitively expensive, which could lead to some companies being locked into the platform.The example above is for data transfer from EC2 out to the internet, and is available on thepricing page.In 2021, AWSincreased the free tierto make the first 100GB of data per month transferred out to the internet free.Changing The GameIn January 2024, Google Cloudannounced in a blog postthat they would be eliminating data transfer fees when moving off Google Cloud. This would allow customers to use Google Cloud without having to worry about a gigantic cloud exit data transfer bill.  Customers do have toapply for free data transferwhen exiting Google Cloud, but knowing the policy definitely could be reassuring.Since 2022, the European Union has been revisingThe European Data Act, and thefinal textwas published in December 2023.  The European Data Act will become fully effective in September 2025.  The act specifies:The Data Act also improves the conditions under which businesses and consumers can use cloud and edge services in the EU. It becomes easier to move data and applications (from private photo archives to entire business administrations) from one provider to another without incurring any costs, because of new contractual obligations that the proposal presents for cloud providers, and a new standardisation framework for data and cloud interoperability.Part of the text includes a provision for a self-regulatory code of conduct for facilitating the switching of data processing services and the porting of data.  Many Cloud providers have signed on to theSWIPO(Switching Cloud Providers and Porting Data) group, including members of the Cloud Infrastructure Services Providers In Europe (CISPE).In themost recent blog articlefrom AWS, they refer to the waiver on data transfer following the direction set by the European Data Act, and being available to all AWS customers around the world, and from any AWS region.Interestingly,Microsoft has declared adherenceto a SWIPO Code of Conduct, but thepricing detailsfor Microsoft Azure still indicate the first 100GB of Internet Egress is free, but charged per GB after that.  I was not able to find an exception policy when exiting their cloud to another provider or moving back to on-premises.  It will be interesting to see if they follow suit with their competitors.ConclusionIt appears that the European Data Act is working to level the playing field among cloud providers, and making it less costly for companies to port their data out, removing one of the barriers that would cause \"lock-in\" with specific providers.  It will be interesting to see how other cloud providers and services react to the new pricing and upcoming regulations, and if they will follow suit.ResourcesAWS Blog Post - Free data transfer out to internet when moving out of AWSAWS Blog Post - Introducing AWS SnowballAWS FAQ - Migrate Petabyte Scale DataAWS Snowmobile FAQEuropean Data ActSWIPO (Switching Cloud Providers and Porting Data) Stakeholder GroupCloud Infrastructure Services Providers in Europe"}
{"title": "How to scan thru Amazon RDS for SQLServer error logs using metric filters", "published_at": 1709776102, "tags": ["ec2", "rds", "vpc", "cloudwatch"], "user": "Revathi Joshi", "url": "https://dev.to/aws-builders/how-to-scan-thru-amazon-rds-for-sqlserver-error-logs-using-metric-filters-18ld", "details": "SQL Serveris a relational database management system developed by Microsoft. Amazon RDS for SQL Server makes it easy to set up, operate, and scale SQL Server deployments in the cloud. With Amazon RDS, you can deploy multiple editions of SQL Server including Express, Web, Standard and Enterprise, in minutes with cost-efficient and re-sizable compute capacity.We can useAmazon CloudWatch metricsand Amazon Relational Database Service (Amazon RDS) event notifications to monitor different metrics and events.In this post, I have created to receiveAmazon Simple Notification Service (Amazon SNS)notifications for your RDS SQL Server instances - online or offline.Please visit myGitHub Repository for RDS articleson various topics being updated on constant basis.Let\u2019s get started!Objectives:1.Create RDS for SQL Server2.Publish Amazon RDS for SQL Server error logs to CloudWatch.3.Create filter patterns for offline or online databases4.Create alarms for the filtered metrics5.Test the solutionPre-requisites:AWS user account with admin access, not a root account.IAM RoleResources Used:Amazon RDS for Microsoft SQL ServerCloudWatchAmazon Simple Notification ServiceSteps for implementation to this project:1. Create RDS for SQL Server12345678910111213Wait for 4-5 min to finish creating the database.2. Publish Amazon RDS for SQL Server error logs to CloudWatch.1Click on database-12Click on Modify3In the Log exports section, choose the logs that you want to start publishing to CloudWatch.Select Error log453. Create filter patterns for offline or online databases1Open the CloudWatch console and from the Logs section, choose Log groups.Select the Amazon RDS for SQL Server error log of your DB instance (/aws/rds/instance/database-1/error).Under Actions, Choose Metric filters and choose Create metric filter.For Filter pattern, type OFFLINE.You can Test pattern using following steps:Select log data to test from the dropdown in Test pattern section (in our case it is database-1.node1)Click Test patternTest pattern will only work if one of the databases is in OFFLINE state and corresponding entry is available in the error log.In our case, the database-1 is online, so nothing in Results sectionNextFor Filter name, Metric namespace, and Metric name, enter OFFLINE Database(s).For Metric value, enter 1.Choose Next.Choose Create metric filter.Follow the same steps tocreate a metric filter for ONLINE databases. Adjust the filter patternONLINE, filter nameDatabase(s) are ONLINE, metric namespaceDatabase(s) are ONLINE, and metric nameDatabase(s) are ONLINEaccordingly.Click NextChoose Create metric filter.4. Create alarms for the filtered metricsTo create alarms for our filtered metrics, complete the following steps:Select the filter OFFLINE Database(s) and choose Create alarmEnter a metric nameOFFLINE Database(s)Choose Minimum for Statistic and set Period to 30 SecondsFor Threshold type, select Static.Select Greater and enter 0.NextSelect In alarm for Alarm state trigger.For Send a notification to the following SNS topic, select Create new topic and enter a topic nameOFFLINE-SQLDB-SNS-TOPIC.Enter a valid email for receiving the notifications.Choose Create topic, thenchoose Next.For Alarm name enter a nameOFFLINE-ALARM.Review the configuration, then choose Create alarm.Now that the SNS topic is created, make sure you confirm the subscription by choosing the Confirm subscription link in the email.Repeat the same steps tocreate alarms for ONLINE databasesand adjust the conditions accordingly.ONLINE-ALARM created5. Test the solutionCleanupdelete CloudWatch log groupdelete RDS database instancedelete SNC topic and subscriptionWhat we have done so farI have demonstrated how to use metric filters to scan through Amazon RDS for SQL Server error logs and set up alerts as per your requirements."}
{"title": "Local automation", "published_at": 1709760494, "tags": ["aws", "devops", "softwareengineering"], "user": "Jakub Wo\u0142ynko", "url": "https://dev.to/aws-builders/local-automation-1hb4", "details": "WelcomeOnce again I have a goal, write every month at least one article. Sounds easy, but unfortunately for me, it is not. The idea behind this blog is to write content that will be helpful for me(and hopefully for my readers as well). For example, when I need basic Nginx configuration I used to openthis post from 2021. Now I\u2019m older a bit, so I decided to switch once again to Doom Emacs(?!!) and start writing my own code/config snippets. As it also can be useful, probably I will write about it someday.IntroductionOk, so what do I mean bylocal automation? I would say all actions, that allowme to do things faster and make fewer errors while (mostly) typing. Also, itcould be useful during presentations, or live coding - as less typing is asmaller chance of typo. That is why I will break this article into threeparts; HTTP requests and wrappers.HTTP requestsFor a very long time, the go-to tool wascurl. Great, always available command line tool. Unfortunately, there is one small issue. It\u2019s hard to keep requests and collect them in collections, it\u2019s great for one-time shots or debugging, but for constant working with API could be painful.To solve it, I started working with tools likePostman/Insomnia. Then eh... strange licensing model, or changes which occurred from Kong sideclick, definitely push me again for some lookup.After checking different very popular tools and those not such well known I decided to use\u2026 Ansible. Sounds strange right?Let me explain this decision. For example, look at this code.----name:Testing dummyjson.comhosts:localhostgather_facts:falsetasks:-name:Get random contentregister:outputansible.builtin.uri:url:https://dummyjson.com/products/return_content:true-name:Filter contentansible.builtin.set_fact:stock:\"{{output['json']['products']|selectattr('title','==','iPhone9')|map(attribute='stock')|first}}\"-name:Assert results fromansible.builtin.assert:that:-\"{{stock}}iseq(94)\"Enter fullscreen modeExit fullscreen modeAs you can see within 19 lines of easy-to-read code, I\u2019m able to query the data endpoint, select needed variables, and assert the result. But\u2026 let\u2019s focus on Ansible filters.\"{{ output['json']['products'] | selectattr('title','==','iPhone 9') | map(attribute='stock') | first }}\"Enter fullscreen modeExit fullscreen modeIn general, it\u2019sjqreplacement. First, we define a dictionary with our needed content.selectattris an Ansible filter that allows us to select the object(list or dict) based on our query. Then we havemap, which chooses the needed element from the dict, at the end we\u2019re usingfirstfilter, which just gets the first element of the created dict.Now you can ask me about authorization. Easy.----name:Query endpoint with authhosts:localhostgather_facts:falsetasks:-name:Query endpointvars:username_password:\"admin:password\"ansible.builtin.uri:url:https://example.com/method:GETheaders:Authorization:Basic {{ username_password | string | b64encode }}validate_certs:falseregister:query_resultsEnter fullscreen modeExit fullscreen modeMaybe it\u2019s a bit more complex, than adding it to Postman, but:It\u2019s easy to control as codegit friendlyfully open-sourcesometimes fasthigh level of flexibility with data manipulation, so we can chain our requests!WrappersThat is probably my favorite and most used category. I\u2019m using two types of them. Linux aliases and Makefiles. Both have different use cases, as well as usability levels.AliasesIn most cases, when we become Unix users, after a while our.bashrc/.zshrcbecome long, and complex. For example, my zsh configuration is 112 lines long. And it\u2019s great, starting from:# AWS CDKexportJSII_SILENCE_WARNING_UNTESTED_NODE_VERSION=trueEnter fullscreen modeExit fullscreen modeThrough AWS aliases:# get IDs of all named instancesaliasec=\"aws ec2 describe-instances --output yaml --query 'Reservations[*].Instances[*].[InstanceId,Tags[?Key==\\`Name\\`].Value]' --no-cli-pager\"# connect with instance over ssmaliasssm=\"aws ssm start-session --target\"Enter fullscreen modeExit fullscreen modeThen at the end I have aliases for working with remote docker, over sockets:dxdo(){exportDOCKER_CONTEXT=digital-oceanexportDOCKER_BUILDKIT=\"1\"exportCOMPOSE_DOCKER_CLI_BUILD=\"1\"unsetDOCKER_CERT_PATHunsetDOCKER_TLS_VERIFY}dcc(){docker context create${1}--docker\"host=tcp://${1}.host.link:2376,ca=/Users/kuba/.docker/c/ca.pem,cert=/Users/kuba/.docker/c/cert.pem,key=/Users/kuba/.docker/c/key.pem\"}dodcc(){docker context update digital-ocean--docker\"host=ssh://kuba@$(tf output-rawpublic_ip):4444\"}Enter fullscreen modeExit fullscreen modeRight, so what\u2019s wrong with aliases? Mostly all of them are global, and rather simple, without error handling. Also not very good for cooperating with friends, or showing up during public talks.Now, let\u2019s move to Makefiles. I\u2019m using them for most of my project/presentation-related activities. For example, that is Makefile from my The Hack Summit talk:define deploy$(eval$@_REGION=\"eu-central-1\")$(eval$@_STACK_NAME=$(1))cfn-lint$($@_STACK_NAME).yaml     aws cloudformation deploy--stack-name$($@_STACK_NAME)\\--template-file$($@_STACK_NAME).yaml\\--region$($@_REGION)\\--capabilitiesCAPABILITY_NAMED_IAM     aws cloudformationwaitstack-create-complete--stack-name$($@_STACK_NAME)aws cloudformation describe-stacks--stack-name$($@_STACK_NAME)--query\"Stacks[].Outputs\"--no-cli-pagerendef  define destroy$(eval$@_REGION=\"eu-central-1\")$(eval$@_STACK_NAME=$(1))aws cloudformation delete-stack--stack-name$($@_STACK_NAME)--region$($@_REGION)endef  clean-1:     @$(call destroy,\"example-1\")clean-2:     @$(call destroy,\"example-2\")clean-3:     @$(call destroy,\"example-3\")clean-4:     @$(call destroy,\"example-4\")example-1:     @$(call deploy,\"example-1\")example-2:     @$(call deploy,\"example-2\")example-3:     @$(call deploy,\"example-3\")example-4:     @$(call deploy,\"example-4\")generate-ssh-config:     aws cloudformation describe-stacks--stack-nameexample-2--query\"Stacks[].Outputs\"--no-cli-pager| jq-r'.[][] | if .OutputKey == \"THSInstance\" then \"Host \\(.OutputKey)\\n  ProxyJump THSBastion\\n  PreferredAuthentications publickey\\n  IdentitiesOnly=yes\\n  IdentityFile /Users/kuba/.ssh/id_ed2219_ths\\n  User ec2-user\\n  Hostname \\(.OutputValue)\\n  Port 22\\n\\n\" else \"Host \\(.OutputKey)\\n  PreferredAuthentications publickey\\n  IdentitiesOnly=yes\\n  IdentityFile /Users/kuba/.ssh/id_ed2219_ths\\n  User ec2-user\\n  Hostname \\(.OutputValue)\\n  Port 22\\n\\n\" end'>~/.ssh/config.d/thsEnter fullscreen modeExit fullscreen modeIt\u2019s one big wrapper around CloudFormation, with implementedstack watch, as well asssh/config.dconfig generation.Cool staff, and very easy to use, just after implementation. However, we can make it even smaller, for example this small file:JSON_FILE :=output.json STACK_NAME :=Gowaw56Stack# Parse JSON dataVM_IP :=$(shell jq-r'.Gowaw56Stack.vmippublicsubnet'$(JSON_FILE))VM_ID_SSM :=$(shell jq-r'.Gowaw56Stack.vmidwithssm'$(JSON_FILE))VM_ID_ICE :=$(shell jq-r'.Gowaw56Stack.vmidwithice'$(JSON_FILE))# Define commandsSSH_CMD :=ssh ec2-user@$(VM_IP)-i/Users/kuba/.ssh/id_ed25519_gowaw56 SSM_CMD :=aws ssm start-session--target$(VM_ID_SSM)EC2_CMD :=aws ec2-instance-connect ssh--instance-id$(VM_ID_ICE)# Define targets.PHONY: ssh ssm ec2 deploy clean# Target to SSH into the VMssh:     @echo\"Connecting to VM IP:$(VM_IP)\"$(SSH_CMD)# Target to start SSM sessionssm:     @echo\"Starting SSM session for instance ID:$(VM_ID_SSM)\"$(SSM_CMD)# Target to SSH into EC2 instance using EC2 Instance Connectec2:     @echo\"Connecting to EC2 instance ID:$(VM_ID_ICE)\"$(EC2_CMD)deploy:     @echo\"Deploy stack:$(STACK_NAME)\"cdk deploy$(STACK_NAME)--require-approvalnever-O$(JSON_FILE)clean:     @echo\"Cleaning stack:$(STACK_NAME)\"cdk destroy$(STACK_NAME)--forceEnter fullscreen modeExit fullscreen modeThis time I decided that it was much more efficient to use Makefileduring the public talk than typing everything manually, and exposing myself to unexpected history presentations.SummaryAs you can see, It\u2019s quite easy to automate our DevOps workflow.Almost everything we can just share with others, and make them work more time efficiently, also what is very important.Everyone will be on the same page, with the same results(let\u2019s say ;) ).As Makefiles and aliases are easy to understand, I believe that using Ansible as an HTTP request tool isn\u2019t the first idea.I had the same, it was a matter of filter understanding. After I realize how to use them correctly, versioning HTTP requests and result manipulation become very smooth."}
{"title": "AWS re:Invent \u2014 Why should you attend? | My personal experience", "published_at": 1709747466, "tags": ["cloud", "aws", "reinvent", "cloudcomputing"], "user": "Fernando Pena", "url": "https://dev.to/aws-builders/aws-reinvent-why-should-you-attend-my-personal-experience-39k5", "details": "Re:Inventis the biggest AWS event and one of the most significant tech events in the cloud world.This blog is about my personal experience attending the event, so please don\u2019t expect a technical blog. Of course, I will highlight some announcements, but the most important thing to me is to share with you why you should participate in this event in the future.I made aYouTube videoa couple of months ago to help you understand this event's importance and magnitude.In less than 5 minutes of this video, I\u2019ll speak about (or you can read it here):Event itselfHighlight the announcementsAWS CommunityReasons to participateBig NumbersAWS re:Invent usually happens inLas Vegas. And generally, in the last week of November.In 2023, the event occurred betweenNov 27 and Dec 1and attracted more than50,000 peoplefrom all over the world. That\u2019s amazing.Over 2900 technical sessionsin 6 different venues, what makes you walk a lot. Sessions are at various levels, from beginners to advanced sessions, chalk talks, breakout sessions, keynotes, workshops, and so much more.Every year, this event brings a lot of news in the world of cloud, and let\u2019s look at some of the most exciting announcements.Highlight AnnouncementsAWS launchedAmazon Q, an assistant using generative AI specifically for work. You can use it in your account to ask best practices and other questions, for example. And obviously, I\u2019ll make a video about it, showing how it works! If you want to know when I post a new video like that, subscribe to my YouTube Channel (https://www.youtube.com/PenaRocks).Next-generation AWS-designed chips, Graviton4 and Trainium2, will make the workloads and genAI applications run faster, more efficient, and less expensive.Announcements for Bedrock: if you are a Bedrock enthusiast, now Bedrock has new powerful capabilities that empower customers to customize models. With the expanded choice of models, customers now have more models to build and scalegenerative AIapplications.As expected, the event broughta lot of things for AI. Consequently, the innovations forAmazon SageMaker, with five new capabilities, will make it easier to build, train, and deploy new AI models.Amazon S3 Express One Zone. A new storage class for S3, designed for running applications that require extremely fast data access.(Of course, I took a photo with the new S3 storage class)Besides that,it has many more announcements, for example, for Serverless, Compute, Data, and other areas.Here is the link with top announcements where you can navigate by area and better understand each of these new announcements:https://aws.amazon.com/blogs/aws/top-announcements-of-aws-reinvent-2023/Besides that, you can watch the keynotes and a lot of sessions of the re:Invent on YouTube:https://www.youtube.com/user/AmazonWebServicesAWS CommunityOne of the best reasons to attend this event is the possibility of connecting with many people in theworld of cloud, professionals, AWS leaders, and people from different programs in the AWS, like AWS Heros, Ambassadors, and community builders.AWS has afantastic community, and it offers the opportunity to make new friends, share ideas, and learn much more from these experienced guys; it is also a chance tostay connected, meetnew customers, gainnew insights, and so much more.Of course, that\u2019s not just business or technical stuff. You have so much fun with the community!I will speak better and explain theAWS Community Builders Programin my next blog and video. Stay tuned!My experience with the community has been unique! I met new people, I made new friends, and I connected with many people from different parts of the world; that was a fantastic experience.I hardly recommend staying connected with your community, with the User Groups, Community Builders, Heros, and Ambassadors. It worths! Believe me.Reason to participate in the eventLet\u2019s summarize the reasons to attend this event, and if I could choose five top reasons, that would be:Stay updated with new launches. You can do it online, but you will have a better experience in person.Hands-on workshops: the possibility to learn more and try new services.Exclusive sessions: talks and discussions that are not available online.Connect with AWS Community: AWS Heros, Ambassadors, User Group Leaders, and community builders.Expand your network: Meet new customers, meet the experts, create new connections and new friends, and learn and share your experience. That is one of the most valuable reasons to me personally.Well, I hope you liked this post. And if you arrived here, please subscribe to this blog.That\u2019s all, let\u2019s rock the future, see you soon!Subscribe to my Youtube channel:Youtube: Pena RocksFollow me on social networks:Instagram:https://www.instagram.com/pena.rocks/Twitter:https://twitter.com/nandopenaLinkedIn:https://www.linkedin.com/in/nandopena/"}
{"title": "# How to Deploy a Word Counter Application to AWS S3 Bucket", "published_at": 1709724756, "tags": [], "user": "Nwokocha wisdom maduabuchi", "url": "https://dev.to/aws-builders/-how-to-deploy-a-word-counter-application-to-aws-s3-bucket-5bdn", "details": "AWS S3 is a cloud storage service provided by Amazon Web Services. It is a simple storage service that offers scalability, security, and performance for storing and retrieving data.S3 allows developers to host static websites, store and retrieve data anytime, and manage data using a web interface.You\u2019ll deploy a word counter application using an AWS S3 bucket. You can access this application in a web browser at the end of this article.What is an S3 Bucket?Amazon Simple Storage Service S3is a scalable object storage service offered by AWS. It allows you to store and retrieve data from anywhere on the web.An S3 bucket is essentially a container for objects stored in S3. It serves as the top-level folder in which you store your files.S3 is an object storage service that stores data as objects in containers called buckets. An object can be any kind of file: a text file, an image, a video, etc.Project OverviewThis project will use the AWS console to create an S3 bucket and deploy a word counter application to the bucket.The word counter application contains HTML, CSS, and JavaScript files. The application will count the number of words in a given text.Once the bucket is created, you'll upload the Word Counter application files to it and make it public to access it using a web browser.PrerequisitesThis tutorial assumes that you have anAWS accountand are familiar with the AWS Management Console. Every other step will be explained in detail.Application setupThis word counter app can be found on GitHub. Clone the repository to your local machine using the following command:git clone https://github.com/Aahil13/Word-counter.gitEnter fullscreen modeExit fullscreen modeThe word counter application contains the following files:index.html: The main HTML file that contains the structure of the application.style.css: The CSS file that contains the styles for the application.index.js: The JavaScript file that contains the logic for the word counter application.README.md: The README file that contains the instructions for running the application.Dockerfile: The Dockerfile that contains the instructions for building the Docker image.Favicon folder: The folder that contains the favicon for the application.Step 1: Create an S3 BucketTo create an S3 bucket, follow these steps:Open the AWS Management Console and navigate to the S3 service.Click theCreate bucketbutton to create a new bucket.Enter a unique name for your bucket and select the region where you want to create it. Then click on theCreate bucketbutton to create the bucket.Figure 1: Enter a unique name for your bucket.Once the bucket is created, click on the bucket name to open the bucket.Figure 2: Bucket created.Step 2: Configure Bucket for Static Website HostingTo configure the bucket for static website hosting, follow these steps:Click on thePropertiestab and then click on theStatic website hostingcard.Click onEditand select theEnableoption. Enter the name of the index document (e.g,index.html) and click on theSave changesbutton to save the changes.Figure 3: Configure the bucket for static website hosting.Step 3: Make the Bucket PublicTo make the bucket public, follow these steps:On thePermissionstab, click on theBlock public accessbutton and then click on theEditbutton.Figure 4: Block public access.Uncheck theBlock all public accessoption and click on theSave changesbutton.Figure 5: Uncheck the block-all public access option.Still on thePermissionstab, click on theBucket Policybutton.ClickEditand enter the following bucket policy to make the bucket public. Replaceyour-bucket-namewith the name of your bucket.{\"Version\":\"2012-10-17\",\"Statement\":[{\"Sid\":\"PublicReadGetObject\",\"Effect\":\"Allow\",\"Principal\":\"*\",\"Action\":\"s3:GetObject\",\"Resource\":\"arn:aws:s3:::your-bucket-name/*\"}]}Enter fullscreen modeExit fullscreen modeClick on theSave changesbutton to save the bucket policy.Figure 6: Bucket policy.Step 4: Upload Application Files to the BucketTo upload the application files to the bucket, follow these steps:On theObjectstab, click on theUploadbutton to upload the application files to the bucket.Figure 7: Upload the application files to the bucket.Click theAdd filesbutton and select the application files from your local machine. Then, click onAdd folderto upload the favicon folder.Figure 8: Add files to the bucket.Click theUploadbutton to upload the files to the bucket.Figure 9: Application files uploaded to the bucket.Step 5: Access the ApplicationTo access the application, follow these steps:Open the bucket and click on thePropertiestab.Click on theStatic website hostingcard and copy the endpoint URL.Figure 10: Copy the endpoint URL.Open a web browser and paste the endpoint URL in the address bar.Figure 11: Word counter application.Clean UpAfter deploying and playing around with your application, you can delete the S3 bucket to avoid incurring unnecessary charges.To delete the S3 bucket, follow these steps:On theword-counterbucket, select all the files and click on theDeletebutton to delete the files.Afterward, go back to your bucket and click theDeletebutton to delete the bucket.ConclusionIn this article, you learned about AWS S3 and how to deploy a word counter application to an S3 bucket.You created an S3 bucket, configured it for static website hosting, uploaded the application files to the bucket, and accessed the application using a web browser.This is a simple and fast way of hosting static websites on AWS!"}
{"title": "Chaos Engineering in AWS with FIS", "published_at": 1709719173, "tags": [], "user": "Manu Muraleedharan", "url": "https://dev.to/aws-builders/chaos-engineering-in-aws-with-fis-3h04", "details": "Chaos EngineeringChaos Engineering is the discipline of experimenting on a system in order to build confidence in the system\u2019s capability to withstand turbulent conditions in production. It is not about creating chaos, it is about making the inherent chaos in real-world applications visible to you.One of the famous Chaos Engineering tools was Netflix Chaos Monkey, which would shut down random machines in the environment and check the effect on the availability.In the Well-Architected Framework, Chaos Engineering is a part of the Reliability pillar.Design Principles of Reliability say:Automatically recover from failureTest recovery proceduresScale horizontally to increase aggregate workload availabilityStop guessing capacityManage change through automationAWS FIS (Fault Injection Simulator) corresponds directly to the 2nd principle and indirectly to the 1st principle.FIS allows you to create Chaos Experiments and test your workload to see if your workload is designed reliably. It shows whether your recovery procedures work, and whether you can automatically recover from failures. It also gives you an idea of downtime that can be expected in the particular DR strategy you have chosen.We recently had this question from a customer. They have only 2 AZ in their architecture, and they have a multi-AZ RDS database with a cross-region read replica. If one of the AZs goes down, will the CRR replica continue to function? Will it keep in sync with the DB after the multi-AZ failover happens? How much is the time, that we won't be able to access the RDS db, and the read replica? We want to optimistically answer with Yes for these questions and add \"a very short time\" to the last one. But how can we be sure?You create an FIS experiment and test it out.Key Features of AWS FISManaged Chaos Experiments: AWS FIS provides templates for creating and managing chaos experiments, allowing teams to focus on results rather than the intricacies of setup and execution.  Broad Fault Injection Capabilities: Simulate a wide range of failures, including server outages, network latency, unavailability of EC2 resources, and throttled database access, to understand their impact on your application.  Integration with AWS Services: Seamlessly integrate with other AWS services such as Amazon EC2, Amazon RDS, Amazon ECS, AWS Lambda, VPC, etc, enabling a comprehensive testing environment.  Safety and Security: AWS FIS is built with safety in mind, offering mechanisms to limit the blast radius of experiments and ensure that your production environments remain secure.Enter fullscreen modeExit fullscreen modeExperiment:An experiment in AWS Fault Injection Simulator (FIS) is a controlled procedure designed to assess the resilience of your AWS infrastructure by intentionally introducing faults or disruptions.Template:This template defines the actions (faults) to be executed, the targets (AWS resources) those actions will affect, and any conditions or constraints. Templates ensure experiments are reproducible and standardized.An experiment is a template in action.Actions:Actions are the specific faults or disruptions you want to introduce. AWS FIS supports a variety of actions, such as stopping an EC2 instance, injecting latency into a network, and throttling database I/O operations, among others.Targets:Targets are the AWS resources upon which actions will be performed. Targets can be specified explicitly or selected dynamically based on tags or other identifiers, allowing for flexibility in defining the scope of the experiment. Eg: Which RDS to stop, Which subnet to have network disruption.Stop Conditions:To ensure safety and prevent unintended consequences, experiments can include stop conditions. These are criteria that, when met, will automatically terminate the experiment. This can be defined as a CloudWatch Alarm.Creating an Experiment TemplateNavigate to the AWS FIS console -> Experiment Template.Select the account this experiment will run onAfter giving a name and description, select the actions you want to include. I want to include a network disruption action that would disrupt all network connectivity in one AZ us-east-1a.Action Type: NETWORK, aws-network-disrupt-connectivityTarget: (a target node is created automatically for the action which you will edit later)Duration: I will disrupt for 2 minutesScope: (All all types of network connectivity will be disrupted including regional services like S3)Now edit the Targets node.You can select targets using resource tags, filters or directly using resource IDs. I am selecting the subnet ID:Now, I create the action for DB instance going down:Action type: RDS aws:rds:reboot-db-instancesStart-After: network-disruption (I want db to go down after the first action)Target: (a target node is created automatically for the action which you will edit later)Force failover: Yes (this will cause failover to the standby instance)Now edit the Targets node: (Select the DB instance I have)Actions are done, now specify the additional options:Empty target resolution mode: What if you gave some tags to find the targets by, and at runtime, no targets were found? I am specifying that I want the experiment to fail in that case.Service Access: What IAM Role would be used by the experiment? This needs to have access to CloudWatch if you want to log experiment logs to a CloudWatch log group. The default IAM role for FIS does not have CloudWatch access, edit the policy to add that.Stop Condition: You wanted to bring down one AZ but ended up bringing down everything. Now you are panicking and want to stop the experiment. Specifying a CloudWatch Alarm will allow you to stop an experiment by putting the Alarm in Alarm State.For eg: This could be set so it goes into Alarm if there are more than X messages in a queue.You can also stop the experiment from the console by clicking \"stop experiment\" if you have access.Logs: You can send the logs of the experiment to an S3 bucket or a CloudWatch Log Group.Running the ExperimentNow let's look at our steady state. I have a multi-AZ RDS MySQL DB instance. Currently primary is us-east-1aThis DB sits over 2 AZs, evident from the Subnet Group for the DB, which includes us-east-1a and us-east-1b.Please note: Both subnets in the group must share the same network accessibility. This could be done by having them associate with the same route table. Otherwise, after failover, it may be stuck in a subnet with no access.At this point, both DB and the replica are in sync:Once you save the experiment, start the experiment by clicking Start ExperimentYou can see the status changing in actions.In logs, you can see the targets that have been resolved.It has found the following subnets:It has found the DB:When the network disruption is under process, your MySQL connectivity will seem stuck.When the DB action starts, you will see the DB instance rebooting.While rebooting you won't be able to insert into the DB.When the console comes back to show DB as \"Available\" create a new connection to the DB.Note that the console may not yet show the AZ change. It takes some time for it to reflect. But DB has failed over to the other AZ when it says available. It is a DNS change behind the scenes.We can also query from the replica and it is still in sync!We could notice a downtime of about 2 minutes for the RDS db to come back up, for our tiny db.t3.micro instance. More powerful instances take less time.After some minutes console shows the new AZ for primary: us-east-1bActions should show as completed now.The experiment is completed.ConclusionBy using FIS, we can test the resiliency of our applications in AWS. In this demo, we saw how we tested the resilience of a multi-az db and read replica in the face of losing one of the 2 AZs.More info can be found here:https://aws.amazon.com/fis/Great demo at ReInvent:https://www.youtube.com/watch?v=N0aZZVVZiUwHope this was helpful!"}
{"title": "Optimizing EKS Fargate: Exposing K8s Service as a LoadBalancer", "published_at": 1709717061, "tags": ["aws", "k8s", "eks"], "user": "Ahmed Zidan", "url": "https://dev.to/aws-builders/optimizing-eks-fargate-exposing-k8s-service-as-a-loadbalancer-3pce", "details": "Fargate, a groundbreaking technology, streamlines container orchestration by providing on-demand, perfectly-sized compute capacity. You escape the complexities of manual provisioning, configuring, and scaling of virtual machines. It's the go-to choice when the nature of workloads is uncertain, and rapid deployment is paramount, saving valuable time in capacity planning.However, leveraging EKS Fargate poses challenges, especially when exposing K8s services as LoadBalancers. Is it worth it? Today, we unravel this mystery and provide a smooth path for its implementation.Challenges with LoadBalancer Type in EKS Fargate:In standard EKS, exposing services as LoadBalancers is straightforward. You define your service manifest with type: LoadBalancer, and the magic happens. But in EKS Fargate, you might notice your LoadBalancer stuck in a \"pending\" status.Example:apiVersion:v1kind:Servicemetadata:name:nlb-sample-servicenamespace:test-1annotations:service.beta.kubernetes.io/aws-load-balancer-type:externalservice.beta.kubernetes.io/aws-load-balancer-nlb-target-type:ipservice.beta.kubernetes.io/aws-load-balancer-scheme:internet-facingspec:ports:-port:80targetPort:80protocol:TCPtype:LoadBalancerselector:app:nginxEnter fullscreen modeExit fullscreen modeUpon applying this in Fargate, you'll witness a perpetually pending external LoadBalancer.~ kubectl get svc  NAME                 TYPE           CLUSTER-IP      EXTERNAL-IP   PORT(S)AGE nlb-sample-service   LoadBalancer   172.20.39.142   <pending>     80:30843/TCP   3m14sEnter fullscreen modeExit fullscreen modeChecking the service description reveals an \"Ensuring LoadBalancer\" event.Events:   Type    Reason                Age    From                Message-------------------------Normal  EnsuringLoadBalancer  3m55s  service-controller  Ensuring load balancerEnter fullscreen modeExit fullscreen modeHow to Make It Run Smoothly?Steps to Deploy K8s Service as LoadBalancer Type in EKS Fargate:deploy the AWS Load Balancer Controller to an Amazon EKS clusterbefore you start using any service of typeLoadBalanceryou will need to deploy AWS LoadBalancer Controller to your Fargate cluster.Download an IAM policy that allows the AWS Load Balancer Controller to make calls to AWS APIs on your behalf, using the following command.A. For AWS GovCloud (US-East) or AWS GovCloud (US-West) AWS Regionscurl-Ohttps://raw.githubusercontent.com/kubernetes-sigs/aws-load-balancer-controller/v2.5.4/docs/install/iam_policy_us-gov.jsonEnter fullscreen modeExit fullscreen modeB. All other AWS Regionscurl-Ohttps://raw.githubusercontent.com/kubernetes-sigs/aws-load-balancer-controller/v2.5.4/docs/install/iam_policy.jsonEnter fullscreen modeExit fullscreen modeCreate an IAM policy using the policy downloaded in the previous step. If you downloaded iam_policy_us-gov.json, change iam_policy.json to iam_policy_us-gov.json before running the command.aws iam create-policy\\--policy-nameAWSLoadBalancerControllerIAMPolicy\\--policy-documentfile://iam_policy.jsonEnter fullscreen modeExit fullscreen modeCreate a service account named aws-load-balancer-controller in the kube-system namespace for the AWS Load Balancer Controller. Use the following command:eksctl create iamserviceaccount\\--cluster=YOUR_CLUSTER_NAME\\--namespace=kube-system\\--name=aws-load-balancer-controller\\--attach-policy-arn=arn:aws:iam::<AWS_ACCOUNT_ID>:policy/AWSLoadBalancerControllerIAMPolicy\\--override-existing-serviceaccounts\\--approveEnter fullscreen modeExit fullscreen modeThe output should be something like the following.2024-03-06 16:27:17[\u2139]  1 iamserviceaccount(kube-system/aws-load-balancer-controller)was included(based on the include/exclude rules)2024-03-06 16:27:17[!]metadata of serviceaccounts that existinKubernetes will be updated, as--override-existing-serviceaccountswasset2024-03-06 16:27:17[\u2139]  1 task:{2 sequential sub-tasks:{....... 2024-03-06 16:27:50[\u2139]  created serviceaccount\"kube-system/aws-load-balancer-controller\"Enter fullscreen modeExit fullscreen modeInstall the AWS Load Balancer Controller with Helm using the following command.helm repo add eks https://aws.github.io/eks-charts  helm upgrade--installaws-load-balancer-controller eks/aws-load-balancer-controller--setclusterName=Your-luster-Name-nkube-system--setserviceAccount.create=false\\--setserviceAccount.name=aws-load-balancer-controller\\--setregion=Your-region\\--setvpcId=Your-VPCEnter fullscreen modeExit fullscreen modeNote here we have to setregionandvpcIdwhy?The Amazon EC2 instance metadata service (IMDS) isn't available to Pods that are deployed to Fargate nodes.So if you didn't specify the region and your VPCId the pod will not able to get them from the metadata.Verify that the controller is installed.$kubectl get deployment-nkube-system aws-load-balancer-controller  NAME                           READY   UP-TO-DATE   AVAILABLE   AGE aws-load-balancer-controller   2/2     2            2           84sEnter fullscreen modeExit fullscreen mode2. Ready to Deploy Your Service:Now that the controller is in place, reapply your LoadBalancer service.~ kubectl get svc    NAME                 TYPE           CLUSTER-IP      EXTERNAL-IP                                                                  PORT(S)AGE nlb-sample-service   LoadBalancer   172.20.176.78   k8s-test1-nlbsampl-xxxx.onaws.com   80:31406/TCP   95mEnter fullscreen modeExit fullscreen modeNow you can seamlessly use the service of type LoadBalancer in EKS Fargate.Things to Consider When Exposing Your Service as a LoadBalancer:Network Load Balancers and Application Load Balancers (ALBs) can be used with Fargate with IP targets only.Once you deploy the AWS Load Balancer controller in your cluster, it becomes the default class for all your services with type LoadBalancer.Conclusion:EKS Fargate offers incredible simplicity and flexibility. With the AWS LoadBalancer Controller, hurdles in exposing K8s services as LoadBalancers are conquered. Seamless integration of this essential feature enriches your container orchestration experience.For further insights or any questions, connect with me on:LinkedinTwitter"}
{"title": "Create a load-balanced web server with auto-scaling", "published_at": 1709683200, "tags": [], "user": "Mawuli Denteh", "url": "https://dev.to/aws-builders/create-a-load-balanced-web-server-with-auto-scaling-6d", "details": "OverviewThis tutorial walks you through the process of creating a web server which is managed by an auto scaling group. The auto scaling group uses a launch template to create the servers. The auto scaling group launches the servers into a target group. An internet-facing load balancer directs traffic to the target group.Architecture DiagramCreate a VPCA VPC is an isolated, private network you can create to run your workloads. You have complete control over your VPC when you create one.Click onCreate VPCfrom the VPC Dashboard to create a new VPCSelectVPC and moreEnter a name of your choice underAuto-generateChoose a10.0.0.0/16IPV4 Cidr blockNumber of Availability Zones (AZs) =\"2\"Number of public subnets =\"2\"NAT gateways ($) =\"In 1 AZ\"Optional: VPC endpoints= \"S3 Gateway\"Leave all other settings as defaultClickCreate VPCYour configuration should match what is shown below:Create a Launch TemplateThe launch template will serve as the blueprint for creating the exact type of server we need to meet our web server demands. A launch template can be modified to create new versions when you need to change a config.Click onCreate launch templatefrom the EC2 console to create a new launch templateLaunch template name - required =\"autoscale-webserver\"Check theProvide guidance to help me set up a template that I can use with EC2 Auto ScalingboxUnderApplication and OS Images (Amazon Machine Image) - required, chooseAmazon Linux 2 AMI (HVM) - Kernel 5.10, SSD Volume TypeInstance type =\"t2.micro\"Key pair - Create a new one or use existing key pairSubnet -Don't include in launch templateCreate security group =\"autoscale-webserver-sg\"Allow SSH and HTTP traffic from 0.0.0.0/0 (Ignore the warning about security group. We will edit it later)VPC - Select the VPC you createdUnderAdvanced network configuration, choose\"Enable\"underAuto-assign public IPUnderStorage, leave all other configuration as default and choose\"gp3\"forVolume typeResource tags:Key: Name,Value: autoscale-webserverUnderAdvanced details, scroll down to theUser datasection and enter the following lines of code exactly as shown#!/bin/bash -ex sudo su yum -y update yum install httpd -y systemctl start httpd systemctl enable httpd systemctl status httpd echo \"<html>Hello World, welcome to my server</html>\" > /var/www/html/index.html systemctl restart httpd amazon-linux-extras install epel -y yum install stress -yEnter fullscreen modeExit fullscreen modeYour configuration should look like this:Create Target GroupA target group will route requests to the web servers we create. Our load balancer will need this target group to know what set of servers to distribute traffic to. Our auto scaling group will also be associated with this target group so it launches our servers into the target group.Click onCreate target groupfrom the EC2 console to create a target groupChoose a target type:\"Instances\"Target group name:\"autoscale-webserver\"Protocol:\"HTTP\"Port:\"80\"VPC: Select the VPC you createdLeave every other value on this page as default.NextRegister Targets: Leave as is.ClickCreate target groupCreate Load BalancerAn application load balancer acts as the entry point for traffic to our webservers. Instead of allowing users to access our application directly, we will use the load balancer to distribute traffic equally among our autoscaling group of web servers. This is better for load management, security and reliability of our application.Click onCreate load balancerfrom the EC2 console to create a load balancerType:\"Application Load Balancer\"Scheme:\"Internet-facing\"IP address type:\"IPV4\"VPC: Select the VPC you createdMappings: Check the box beside the two AZs listedSubnet: For each AZ selected, choose the public subnet in the dropdown menuAt this point, go to the Security groups console and create a new security group for the load balancer. The inbound rule should allow HTTP traffic from anywhere.Select this security group as the load balancer security groupListeners and routing: Leave protocol and port asHTTP:80. Select the target group you created as target groupLeave every other config as default and clickCreate load balancerCreate Auto Scaling GroupThe auto scaling group configures and controls how your application scales automatically in response to varying traffic situations.Click onCreate Auto Scaling groupfrom the EC2 console to create an auto scaling groupEnter a nameChoose the launch template you created. ClickNextSelect your webserver VPC created from the VPC stepUnderAvailability Zones and subnets, select the two public subnets in your VPC, in different AZs. ClickNext**NB:* Note that you can use the auto scaling group to override your instance type config from the launch template*UnderLoad balancing, choose the option\"Attach to an existing load balancer\"SelectChoose from your load balancer target groupsSelect the target group you createdSelect VPC Lattice service to attach:\"No VPC Lattice service\"Additional health check types - optional:\"Turn on Elastic Load Balancing health checks\"Leave every other config as default.NextGroup size: Desired:\"2\", Minimum:\"1\", Maximum:\"4\"Scaling policies:\"Target Tracking Policy\"Metric type:\"Average CPU Utilization\"Target Value:\"50%\"Add notifications - optional (Skipped)Add tags - optional (Skipped)Create Auto Scaling GroupCheck your configuration below:Immediately you create your autoscaling group, you should see two new instances getting created in the EC2 console. This is because we specified a desired count of 2. Also note that they are automatically placed one in each AZ to support high availability.Test your web serverClick on one of the web servers, copy the public IP or DNS name and paste it in your browser. You should see the following content:This means your apache web server is running and you are able to reach it from the internet.Restrict web traffic to serversWith the current design, users are directly accessing our web server. We don't want that. That is why we created a load balancer. To restrict incoming HTTP traffic destined for our servers to only the load balancer, we need to update the web servers' security group to accept HTTP traffic from only our application load balancer. This means no request gets to our servers without first making it through the load balancer.Edit web server security groupGo to theautoscale-webserver-sgsecurity group and click on\"Edit inbound rules\".Delete the existing HTTP rule.Add a new HTTP rule. In theSourcebox, scroll down to select the security group of the load balancer.Save rules.You have successfully restricted traffic going to the servers to the load balancer.You should no longer be able to access your web server using the server IPs or DNS names. You should now be able to use the load balancer DNS name to access the servers. Test this out.Live Autoscaling TestWe will now simulate a scenario of high CPU usage on our web server to allow the auto scaling group to respond.We will SSH to our server and run a command to stress the server and this will raise CPU usage across our auto scaling group above 50%. This will make it respond by adding new servers until our maximum number of servers specified is reached.Observe your EC2 console after running this command and you will see new instances being added by the auto scaling group to handle the simulated surge in traffic.If you stopped or terminated the instance on which CPU usage has been simulated, instances will get terminated from your auto scaling group accordingly (scale-in).ConclusionYou have built a load-balanced and highly available web application that auto scales out based on a target of CPU utilization."}
{"title": "Infrastructure As Code on AWS made simple", "published_at": 1709675808, "tags": ["kaiac", "iac", "aws", "infrastructureascode"], "user": "jekobokidou", "url": "https://dev.to/aws-builders/infrastructure-as-code-on-aws-made-simpler-3gm", "details": "This post will introduce you into KaiaC, a tool made to simplify Infrastructure As Code. KaiaC is built on top of Terraform and allow you to create cloud resources just with few key values.Key features of KaiaCInfrastructure as Key Value: Infrastructure is described using a simple configuration syntax. This allows a blueprint of your datacenter to be versioned and treated as you would any other code. Additionally, infrastructure can be shared and re-used.Terraform features: KaiaC mostly used Terraform in backgroup so you can inherit all the powerfull Terraform features like execution plans. For more information on Terraform, refer to the Terraform website.Let's create an EC2 t2.micro with KaiaCStep 1 : Create a KvBookA KvBook is just the file that contains your key/value(s) configuration informations. You have to create a kvbook to tell KaiaC which type of resources you need.Hit the following command!$ kaiac create vmonlyEnter fullscreen modeExit fullscreen modeThe result should look like :KvBook /root/kvbooks/vmonly-3Bpkcs9rPV1a.cfg created!Enter fullscreen modeExit fullscreen modeStep 2 : Edit your KvBookHit the following command!$ nano /root/kvbooks/vmonly-3Bpkcs9rPV1a.cfgEnter fullscreen modeExit fullscreen modeThe file will look like this, if it is ok for you, leave it as is :GL_KAIAC_MODULE=\"vmonly\" GL_NAMESPACE=\"vmonly001\" GL_NAME=\"demo1\" GL_STAGE=\"staging\" BE_AMI_NAME=\"ubuntu/images/hvm-ssd/ubuntu-jammy-22.04-amd64-server-20230919\" BE_EC2_TYPE=\"t2.micro\" BE_EC2_VOLUME_SIZE=\"8\" BE_PATH_EC2_PUBKEY=\"~/.ssh/id_rsa.pub\"Enter fullscreen modeExit fullscreen modeStep 3 : Register your KvBookHit the following command!$ kaiac register /root/kvbooks/vmonly-3Bpkcs9rPV1a.cfgEnter fullscreen modeExit fullscreen modeStep 4 : Check your KvBookHit the following command!$ kaiac planEnter fullscreen modeExit fullscreen modeStep 5 : Apply your KvBookHit the following command!$ kaiac applyEnter fullscreen modeExit fullscreen modeStep 6 : Check your resourcesConnect to yourAWS EC2 Console.You should see your created EC2 instanceStep 7 : Destroy your resourcesHit the following command!$ kaiac destroyEnter fullscreen modeExit fullscreen modeStep 8 : Check your resources againConnect to yourAWS EC2 Console.You should see your terminated EC2 instanceWhere can you find KaiaC ?KaiaC is distributed as a Debian package and installation instructions can be found herehttps://www.kaiac.io/install.If you find this post interesting please leave comments."}
{"title": "Amazon Personalize Overview", "published_at": 1709671020, "tags": ["aws", "machinelearning", "cloud", "ai"], "user": "\u0639\u0628\u062f\u0627\u0644\u0644\u0647 \u0639\u064a\u0627\u062f | Abdullah Ayad", "url": "https://dev.to/aws-builders/amazon-personalize-overview-487o", "details": "It is afully machine learning serviceto build apps with real-time personalizedrecommendations.So what could be a recommendation?For example, a personalized product recommendation, or re-ranking, or customized direct marketing.For example, a user has bought a lot of gardening tools, and you want to provide recommendations on the next one to buy based on a personalized service.So this is the same technology used byAmazon.com.When you go and shop onAmazon.comand after buying a few products, what you will see is that Amazon.com start recommending products in the same category or in completely different categories based on how you've been searching and how you've been buying and user interest and that kind of things.So, you read your input data from Amazon S3, for example, it could be user interactions, those kind of things.Also, you can use theAmazon Personalize APIto have real-time data integration into the Amazon Personalize service, and then this will expose a customized personalized API for yourwebsites and applications, yourmobile applications.Also, you can sendSMSoremailsfor personalization as well.You have all these integrations.It takes days to build this model, not months, and you don't need to build, train, and deploy ML solutions, you can just use this bundled as is.Some use cases is going to be retail stores, and media, and entertainment.GitHubLinkedInFacebookMedium"}
{"title": "Becoming an AWS and Google Authorised Trainer", "published_at": 1709670403, "tags": ["aws", "googlecloud", "training", "certification"], "user": "Ryan Pothecary", "url": "https://dev.to/aws-builders/becoming-an-aws-and-google-authorised-trainer-407j", "details": "Becoming an AWS and Google GCP Authorised TrainerThe last 12 months have been busy for me.In summer 2023 I became an AWS Authorised Instructor (AAI) and helped my company,www.DigitalFutures.combecome an AWS Authorised Training Partner (ATP).Then in February 2024 became a Google Authorised Instructor and in March 2024 we\u2019ve become a Google GCP Authorised Training Partner also.Before we begin, you may be wondering where Microsoft Azure is in all this. Well, that\u2019s a job for 2024.https://aws.amazon.com/training/aai/We\u2019ll start with becoming an Authorised AWS Instructor.  I\u2019ve been fortunate enough to collect a few AWS certifications over the last few years. I currently hold Cloud Practitioner along with 3 Associate level, both Professional level certs and a few Speciality certs. I\u2019ve also got a 10 year history of working with AWS in both a customer setting as well as being an AWS Professional Services Senior Consultant as well as a Specialist Solution Architect and Partner Trainer when I worked for AWS for nearly 5 years (2017-2021).Plus, I\u2019m part of the truly wonderful AWS Community Builders programme (take a look athttps://aws.amazon.com/developer/community/community-builders/for more info and to apply)You do not need this level of certification or history with the platform to become an Authorised Trainer.  In fact you need just two things: -1)  A current AWS Solution Architect certification2)  An Authorised Training Partner that will sponsor you.I hold the first and my company was going through the process of becoming an ATP which covered the second and allowed me onto the programmeOnce registered onto the programme there are two main steps to progress through.1)  Complete the on-demand training2)  Successfully pass the Trainer EvaluationI want to talk about the first of those. The training is fully on-demand.  The Instructor training is GOOD. It takes you through some solid training on AWS Solution Architect certification, so if you don\u2019t already hold this certification then by the time the 200+ hours of training are over you\u2019ll have gained it.  There are also some really interesting sessions focused on how people learn in a classroom setting, how best to train over a virtual setting etc.  I have to admit that I sprinted through most of this training and it\u2019s been a goal of mine to retrace my steps here and go through the whole training again at a slower pace.I don\u2019t enjoy on-demand training.  Unfortunately, both AWS and Google (and I fear Microsoft) seem to be favouring this form of training over in-person. But I did enjoy the AWS AAI training, there were lots of valuable nuggets here for any potential trainer to enjoy.Once you\u2019ve gone through the 200+ hours of training (yes it really is that long), and you\u2019ve obtained the AWS Solution Architect Associate certification, then you can schedule your 3 day Trainer Evaluation bootcamp.  This is the final hurdle before becoming an AAI (AWS Authorised Instructor).I loved this bootcamp.  A small group of potential AAI\u2019s meet virtually with an experienced AWS Trainer and you teach a number of modules while learning about the lab system and troubleshooting lab exercises.The AWS Solution Architect Associate course offers a wide range of technologies to cover and the way my bootcamp was structured was that each AAI chose a module subject that they would like to teach on each of the three days and then the AWS Trainer moderator chooses 3 subjects for you.You present these modules to the class, comprising of the moderator and your potential AAI classmates.  You then hear feedback on the module from all those present. This can be invaluable if you\u2019ve not taught before.  After teaching all 6 modules there is then a troubleshooting lab exercise where you use a real lab used by the course where the moderator has created a number of errors and you\u2019ve got to find them.  This mimics the real-life scenario that all Instructors face of things going wrong within a lab environment and you have been asked to fix it.Once complete you\u2019ll have the results delivered in a few days time, and if successful, you\u2019ll begin the process of onboarding as an AAI.A few more things to note here before we move on.  There are a number of different internal systems that you\u2019ll get access to which allow you to purchase courses, deliver coursework content, manage labs and send out CSAT surveys. Take your time to understand these. Your AWS ATP Partner Manager will be able to help navigate all these different systems, but make sure you keep notes to help yourself.Secondly, you are only able to teach courses that a). You hold a current certification on & b). You take and successful pass the onboarding on-demand training for that course.Lastly, your obligations as an Authorised AWS Instructor are to deliver amazing training (of course!), ensure you hold a current certification and after every class receive a CSAT score of above 4.5 out of 5.As you can tell.  This isn\u2019t going to be something you pass once and then forget about.  You\u2019ll be constantly training and delivering new material alongwith gaining new certifications in order to train new classes.The process is similar when you are becoming a Google Authorised Instructor.  It\u2019s a similar amount of time and effort although there are some differences.What made this far more difficult for me personally is that I donothave 10 years of experience in using Google GCP in a customer environment.  Firstly I had to learn the platform.I think you go through a number of phases when learning Google GCP as an AWS Cloud engineer.At first you are looking for the similarities. Yes there\u2019s an IAM service and a VPC service and you\u2019ll know what they are about from your AWS experience.  There are virtual machines, containers, databases and all the normal Infrastructure building blocks that you are used to.As you dive in a little more, you\u2019ll start to notice the differences. IAM has Basic as well as Predefined and Custom roles (policies) as well as Service Accounts which are more like the Instance Profiles we expect to see in AWS (although the scope is larger). Another nice surprise was that VPC\u2019s are now global in scope which is an oft-wanted feature in AWS. Subnets can also span Availability Zones which simplifies networking. However, I think the most noticeable difference is the use of Projects to isolate resources and the use of Folders to provide structure and security. Oh, and don\u2019t get me started on Cloud Spanner. That service blows my mind.To become a Google Authorised Instructor you\u2019ll need to choose a Tech-Track of subjects you wish to teach and from that you\u2019ll know which Professional-level certification you\u2019ll need to pass.I chose the Cloud Infrastructure track and therefore had to pass the Google Cloud Infrastructure Professional certification.  Since we had already approached Google about becoming an Authorised Training Partner our account team really helped us by pointing us in the right direction to navigate through the  Partner Portal and provided training credits to get us through the Professional level certifications.This is where I spent a huge amount of time.  You\u2019ll see via the Partner Training portal there\u2019s already a pathway called Google GCP Professional Cloud Architect for AWS Professionals.  On paper this looks like a fairly light and doable pathway covering 4 modules of a mixture of lessons and labs.I don\u2019t know where I went wrong here, I completed modules and clicked next for the next module or lab but the whole process of going through the training was a full time job for near 6 weeks.  You can earn points for every lab, module and knowledge check you go through.  I had so many points at one stage I was expecting a 5-star holiday or new car to arrive\u2026.I had a huge amount of trepidation before scheduling The Google Cloud Solution Architect Professional exam, but I was pleasantly surprised that it was not as difficult as I expected. Certainly in comparison to the AWS Pro-level or Speciality level exams I\u2019ve completed. Although, to put this in perspective, my colleague completed the Google AI/ML Professional certification around the same time and found it far more difficult than he expected. Exams huh ?!Before you chose which certification, please bear in mind that in the large list of available Professional level Google GCP certifications, only a few of them are applicable for the Authorised Instructor path.  If you noticed in the above graphic there are four Instructor Tech-tracks and basically either the Google GCP Solution Architect Professional or the Google GCP Data Engineer Professional certifications which are applicable.On to the Instructor Enablement Session.  You\u2019ll need to schedule this via your Google Training Partner account team.  Mine took quite a long amount of time to schedule so prepare to do some nagging and chasing.You\u2019ll be sent three modules to revise via a Google docs folder.  Each of these modules is on a different Infrastructure topic. In the hour-long 1-1 session (Thats correct, its a single hour long session not 3 days..) you are asked to present one of these modules as if doing it in front of a class, along with a nice amount of questions from the session moderator.Being completely, I'll admit that I failed my first attempt at the Instructor Enablement session.During the session I presented on the topic of GCP Networking, a topic that was new to me, but I have enough knowledge of general cloud networking that at the time I was confident.Until the questions started\u2026\u2026  I did as every good Instructor should do and didnotanswer questions incorrectly.  That is a cardinal sin for any instructor.  If you do not know the answer, you take it away and research during a break.I was asked a lot of questions during the session and took too many away to answer. As the session progressed, I knew I\u2019d failed.But it got worse once I got the feedback.It was nice that they highlighted the way I delivered a session but questioned my knowledge of GCP and wondered why I didn\u2019t use the slide notes.Slide notes?  WHAT SLIDE NOTES?Remember that Google drive that contained the modules?  Inside this folder is a sub-folder with the same content, but this time containing slide notes covering whatshouldbe said during the delivery of the module.I was very upset with myself for not finding this earlier.  It was a hard lesson, but something I won\u2019t forget in a hurry.You must wait at least 14 days until scheduling another Instructor Enablement Session, once again sessions were difficult to schedule but I found a date that worked in mid February.   I spent all available time revising the slides (and slide notes), thinking of what questions could be asked and researching answers.  It was nice getting back into learning about Google GCP again, picking up new things once more.Although now the pressure was very much on,  since a failure for a second time means that you cannot re-take the Instructor Enablement session for at least 3 months.  This would have put our goal of becoming a Google ATP on hold and we had plans to teach our students about Google GCP asap.The second Instructor Enablement session was a LOT better.  It was done over Zoom and once again was for an hour.  This time the amazing session moderator asked which module I wanted to teach and in all honesty I couldn\u2019t decide out of the three available choices of Storage & Database, Compute or IAM. They all had sections I enjoyed talking about. So the moderator suggested I cover 10 minutes of each module, something I really enjoyed.  The time went very quickly and the questions that were asked I actually had the answers to.  There was one question I couldn\u2019t answer but didn\u2019t try and guess it, which the moderator commented was the correct thing to do.Overall, it was a really enjoyable experience. And a week later I had confirmation that I\u2019d become a Google GCP Authorised Instructor.There\u2019s always more to learn.  My immediate plans are to cover the AWS Data Engineering Associate exam and follow that up with the Google GCP Data Engineering Professional Certification.  Then on to Microsoft Azure\u2026.  I\u2019ll update this post when I get there.Hope you\u2019ve enjoyed reading, feel free to reachout if you have any questions."}
{"title": "Building Elastic and Fully Managed Cloud-Native VectorDB Milvus Infrastructure on AWS", "published_at": 1709653917, "tags": ["database", "architecture", "networking", "aws"], "user": "Gianluigi Mucciolo", "url": "https://dev.to/aws-builders/building-elastic-and-fully-managed-cloud-native-milvus-infrastructure-on-aws-4ng7", "details": "IntroductionMilvus is an advanced open-source vector database designed to revolutionize AI and analytics applications. With its ability to handle high-performance similarity search on massive-scale vector datasets, Milvus has become an essential tool for businesses in today's data-driven world. From recommendation systems to image recognition and natural language processing, Milvus enables organizations to unlock the full potential of generative AI algorithms and extract valuable insights from complex data.To fully leverage the power of Milvus, deploying it within a robust, scalable, and managed infrastructure is crucial. In this blog post, I will explore how you can build an elastic and fully managed cloud-native Milvus infrastructure on AWS, taking advantage of its scalability, reliability, and ease of management. By harnessing the capabilities of Milvus in combination with AWS services, businesses can supercharge their generative AI initiatives and achieve remarkable results in fields such as content generation, recommendation engines, and personalized user experiences.Overview of Milvus ArchitectureAt the core of Milvus lies its shared-storage architecture, consisting of four essential layers: the access layer, coordinator service, worker node, and storage. This architectural design allows for scalability, as well as the disaggregation of storage and computing resources, resulting in a cost-efficient and flexible infrastructure. The independent scalability of each layer further enhances the system's agility and resilience, ensuring seamless disaster recovery.Milvus architecture distinctively separates compute resources from storage, incorporating dedicated storage nodes. A significant advantage of this infrastructure is the ability to scale the compute cluster down to zero while maintaining the data nodes active. This setup ensures that data remains accessible in Amazon S3, providing a flexible and efficient way to manage resources without compromising data availability.The aboveMilvus architectureis from the official documentation.Existing Infrastructure on KubernetesThe current deployment of Milvus operates on Kubernetes, utilizing components such as etcd for distributed key-value storage, MinIO for object storage, and Pulsar for distributed messaging. While this setup is functional, Milvus architecture is designed to be portable, allowing it to run on various infrastructures. To leverage the benefits of an AWS-native solution and further enhance the deployment, you can introduce updates to leverage Serverless and fully-managed solutions available on AWS.Serverless on AWS provides technologies for running code, managing data, and integrating applications without the need to manage servers. It offers automatic scaling, built-in high availability, and a pay-for-use billing model, increasing agility and optimizing costs. By leveraging serverless technologies, you can enhance the scalability and efficiency of the Milvus deployment on AWS.AWS fully-managed services, on the other hand, are Amazon's cloud computing provisions where AWS handles the entire infrastructure and manages the required resources to deliver reliable services. This includes managing servers, storage, operating systems, databases, and other critical resources fundamental to the service infrastructure. By utilizing fully-managed services, you can ensure a robust and reliable Milvus deployment on AWS, reducing operational overhead and increasing the focus on utilizing Milvus's capabilities effectively.By transitioning the existing Kubernetes deployment of Milvus to leverage serverless and fully-managed solutions on AWS, you can unlock the full potential of Milvus in terms of scalability, reliability, and ease of management. In the next sections, I will explore the proposed infrastructure using AWS services and its benefits in building an elastic and fully managed cloud-native Milvus infrastructure.Proposed Infrastructure with AWS ServicesTo enhance the Milvus deployment on AWS, I propose replacing certain components with AWS services that offer scalability, reliability, and ease of management. These replacements include:MSK (Managed Service for Apache Kafka): MSK replaces Pulsar for messaging and log management. It provides a fully managed Kafka service that ensures robust messaging and log processing, allowing for seamless integration into your Milvus deployment. For future exploration, it is worthwhile to consider utilizing AWS Kinesis, a fully managed streaming service that offers seamless integration with the AWS ecosystem.Aurora Serverless: Aurora Serverless replaces etcd as the metadata storage and coordination system. It offers a serverless database service that automatically scales to match workload demands. With Aurora Serverless, you can ensure efficient and scalable management of metadata in your Milvus infrastructure. Currently Milvus only supports MySQL, but as an alternative metastore, it is also worth exploring the use of AWS DynamoDB, a highly scalable NoSQL database optimized for key-value workloads.Application Load Balancer (ALB): ALB handles load balancing and routing of Milvus requests, ensuring high availability and efficient distribution of traffic to the various components. ALB's dynamic routing capabilities enable seamless traffic management within the Milvus infrastructure.Amazon S3: Amazon S3 replaces MinIO for data persistence. It offers highly scalable, reliable, and cost-effective object storage. By leveraging Amazon S3, you can achieve seamless data persistence for your Milvus deployment, while benefiting from the scalability and durability of AWS's object storage service.Amazon ECS: Milvus containers can be effortlessly deployed on AWS Fargate, a serverless compute engine specifically designed for containers. By utilizing ECS Fargate, you liberate yourself from the complexities of managing underlying infrastructure, enabling you to devote your attention to fine-tuning resource utilization and elevating the performance of your Milvus deployment. For future explorations, you can draw inspiration from the design considerations of Aurora Serverless for high throughput cloud-native vector databases. This involves separating storage and computation, ensuring that you only pay for computational power when it is actually needed, resulting in optimized cost efficiency and enhanced scalability.AWS Cloud Map: Milvus distributed infrastructure requires effective service discovery mechanisms to enable efficient management and scaling of applications. With AWS Cloud Map, you can easily locate and communicate with the services you need, without the hassle of managing your own service registry.By incorporating these AWS services and considering future possibilities, you can build an elastic and fully managed cloud-native Milvus infrastructure that maximizes scalability, reliability, and operational efficiency. In the next sections, I will delve into the architecture of this new infrastructure and explore its benefits in detail.Architecture of the New InfrastructureIn the proposed infrastructure, AWS services seamlessly integrate into the Milvus deployment, enhancing scalability, manageability, and overall performance. MSK, Aurora Serverless, ALB, Amazon S3, and ECS Fargate play pivotal roles in ensuring a robust and elastic infrastructure.Benefits of AWS Services for MilvusThe adoption of AWS services brings several key advantages to Milvus deployments:Scalability: AWS services such as MSK, Aurora Serverless, and ECS Fargate enable effortless scaling of resources based on workload demands. This ensures efficient management of high-volume data, allowing your Milvus deployment to handle growing datasets with ease.Managed Services: By leveraging managed services, you can significantly reduce operational overhead. AWS takes care of the underlying infrastructure, ensuring high availability and durability. This allows you to focus on leveraging Milvus's capabilities without the burden of managing the infrastructure yourself.Reliability: AWS services provide a robust and reliable infrastructure, offering stability and performance for your Milvus deployment. With built-in redundancy and fault-tolerant designs, you can trust that your Milvus infrastructure will operate smoothly and reliably.Cost Efficiency: AWS services offer cost-effective solutions for Milvus deployments. Services like Aurora Serverless and ECS Fargate enable you to pay only for the computational resources you actually use, optimizing cost efficiency. Additionally, Amazon S3 provides highly scalable and cost-effective object storage, eliminating the need for managing and provisioning your own storage infrastructure. By leveraging AWS services, you can achieve significant cost savings while maintaining the scalability and reliability required for your Milvus deployment.By incorporating these benefits into your Milvus deployment, you can unleash the full potential of Milvus for high-performance similarity search on massive-scale vector datasets, while ensuring scalability, reliability, and cost efficiency.Deployment Process on AWS and ChallengesI began by following the official instructions currently available in the documentation, which presents two options for deploying Milvus in Kubernetes: using Terraform and Ansible, or employing Docker Compose (not recommended for production environments). Initially, I opted for Docker Compose and attempted to deploy it on Amazon ECS using the ecs-cli. However, I encountered several incompatibilities and, after many hours of effort, decided to abandon Docker Compose. Despite this setback, the experience proved to be invaluable, as it greatly enhanced my understanding of both ecs-cli and Milvus' internal architecture.Consequently, I decided to build the entire infrastructure from scratch. Given my previous experience, this approach seemed far simpler to manage. I began by deploying the Virtual Private Cloud (VPC), the ECS cluster, and then proceeded to install each of the Milvus components individually. During this process, Milvus introduced support for multiple coordinators in both active and standby modes, further complicating deployment, but in a more exciting way.One of the most significant challenge I faced\u2014and continue to face\u2014is related to ETCD. As you may know, ETCD utilizes the Raft protocol, enabling a cluster of nodes to maintain a replicated state machine. I managed to deploy a single ETCD node in ECS, but to get Raft working, I had to implement several workarounds, such as assigning task names using tags. While not ideal, it was the only viable solution, particularly since ECS does not yet support StatefulSets.Currently, I have a functioning cluster with ETCD that lacks high availability. If you have any suggestions on how to enhance the architecture, or if you're interested in collaborating on this project, your participation would be greatly appreciated.Additionally, if you're willing, please consider helping to make the StatefulSets feature available on ECS by supporting this request:https://github.com/aws/containers-roadmap/issues/127. \ud83d\ude4fConclusionDeploying Milvus on AWS using managed services like MSK, Aurora Serverless, ALB, Amazon S3, and ECS Fargate offers significant benefits in terms of scalability, reliability, and ease of management. By adopting this infrastructure, businesses can unlock the full potential of Milvus for high-performance similarity search on massive-scale vector datasets. With AWS services, you can build an elastic and fully managed cloud-native Milvus infrastructure that can handle the most demanding AI and analytics workloads."}
{"title": "Understanding Database Indexes And Their Data Structures: Hashes, SS-Tables, LSM Trees And B-Trees", "published_at": 1709629260, "tags": ["database", "programming", "tutorial", "architecture"], "user": "Ayokunle Adeniyi", "url": "https://dev.to/aws-builders/understanding-database-indexes-and-their-data-structures-hashes-ss-tables-lsm-trees-and-b-trees-2dk5", "details": "There's often a huge fuss about making data-driven decisions, leveraging data analytics, using data science and data-centred thinking. From a technological point of view, data is usually stored and accessed using databases. Databases, often referred to as RDBMS, are sophisticated systems that abstract away the fairly complex logic and engine behind data storage on disk. Several databases exist in today's tech landscape, but this article will be focused on something common to data storage and retrieval.Drum roll \ud83e\udd41\ud83e\udd41\ud83e\udd41\ud83e\udd41\ud83e\udd41\ud83e\udd41\ud83e\udd41. We will be discussing indexes. The topic gives it away anyway.Log-based Data StructuresMy approach draws a slightly historical perspective to database indexes. To begin, we look at the simplest form of a database - A file. Think of this file as alogfile. Every time we need to store stuff, we append the data to the log file. To retrieve the data, we traverse each entry till we get the information we want.The above illustration sounds straightforward and is very efficient for storing data (write operations) because all it needs to do isan appendoperation. However, it introduces a huge challenge to retrieve data (read operations) when the data grows in volume. This is because the program has to go through all entries all the time. In computer science, the big-O notation for this sort of operation is O(n), n being the number of records.For clarity, we can think of each entry as having a key and value. The indexes in this article will refer to the key in each entry.This is where indexes come in. Imagine if we had a separate data structure that tells us where the information is. something like the famousindicesat the back of the Oxford Dictionary. An index in this context is a data structure that is derived from the primary data that helps retrieve information quickly. There are multiple variations of indexes and we will get into them in the next sectionsHash indexesA hash index is represented using a similar data structure to the dictionary data type in Python or a hashMap in Java.Let's refer back to our simple example database where we have multiple entries appended to a log-based file. A hash index would be akin to having an in-memory key-value pair of every key in the data appended and its byte offset in the data file. In so doing, when we look up some data, we search the index for the key and find a pointer to the location of the actual values on the data file. Also, every time a value is added to the database, the hash index is updated to include that new entry using its key.Before going into the pros and cons of hash indexes, I will build on the illustration of this database. since the existing database is a log-based database (append-only), when an already existing value is to be updated, the database does not seek for the data and modify the values in place. Rather, it appends the entire entry and read operations are built to make sure to get the latest value for any given key. This bolsters the need for an index because the hash map will also be updated to point to the most recent location for that entry.Assuming we have a service that is transactional in nature. This implies that new entries will be added and existing entries will be changed frequently. For entirely new entries, this is no problem. However, every change to an existing entry will append a new entry and will make the older values redundant. If we use hash indexes to solve the issues of long lookup times, reclaiming disk space used by those redundant values in the database is still a significant challenge.But how do we prevent the disk from running out of space? A simple solution to this is to split the log files. Each split can be referred to as asegment. After splitting the files into segments, acompactionprocess can be run in the background. Thiscompactiontakes a segment or multiple segments and merges them. In this process, only the latest values are kept and others are discarded and written into a new segment. After which, new operations are redirected from the older segments to the newly compact segments. Note that the database is still split into files but compaction reduces the amount of redundant keys in the database.In relation to hash indexes, each segment will have its own in-memory hash maps, and these are also updated after merging and compaction also. When a lookup operation is done, it first checks the hash map of the most recent segment, then traverses backwards to the next most recent and the next and so on.Limitations of hash indexesIn practice, log-based databases and hash indexes are very efficient but still have limitations. Some of the core limitations of the example above are poor concurrency control, crash recovery, partial writes, no support for delete operations and range queries are inefficient. Because the index is in-memory (RAM), if the server is restarted, all hash maps are lost. Additionally, the entire hash index must fit in memory. These limitations do not fit the requirements for how we interact with databases and the deluge of data we work with today.To address these limitations, enhancements and changes are made to the existing data structure housing the hash map indexes per segment. Referring back to the current state of the database, we recall that we now have our data split into different segments and segments undergo compaction.SSTablesWe make a fairly simple change to how the data is stored in these segment files. The change we introduce is to sort thedata (key-value) by the key. By doing this, the data is stored in a sorted format on disk using the keys. This is referred to as aSorted String Table (SSTable). The obvious limitation it solves now when compared to plain hash maps is that we can now fully support range queries.The term was coined inGoogle's Bigtable paper, along with the termmemtable.In comparison to log-based storage with hash indexes, SSTables introduce an additional layer tomergingandcompaction. What it does is use an algorithm very similar to the popularmergesortalgorithm to maintain the order of all entries in the SSTableIt is worth noting that sometimes, SSTables are not referred to as indexes but as a data structure itself, which seems to be a better description of what an SSTable is. In this case, the accompanying index can be referred to as an SSIndex or SSTable index or Memtable. However, for the purpose of this article, SSTable will refer to the combination of both the datafile (sorted key-value pairs on disk) and its corresponding in-memory index containing the keys and their bytes offset.SSTables vs Hash IndexesAll the advantages of hash indexes are preserved in SSTables. That is,it is still efficient for write operations since it is log-based (append-only)the in-memory index will act as a pointer to the actual location of the data on the diskThecompactionprocess in the background makes it efficient from a storage perspectiveKey AdvantagesThe additional advantage when compared to Hash indexes are in 2 folds. First, we can nowquery rangesbecause our data is sorted by the key. Second, the index can besparse. To explain the second point, let's use the example that we have keys from A0 to A1000. The index will not need to have 1000 keys but can have half of that which is 500 keys. So the keys could be A0, A2, A4, A6 and so on with their corresponding pointers to their location on the disk. When trying to retrieve the values associated with key A5, even though it is not in the index, we know that it is in between A4 and A6 and we can begin our search from there. Thus, making the index sparse without trading off read performance.Drawbacks of SSTablesHowever, it is not without its limitations. The entire index still needs to fit within the memory of the server. If the server crashes, the index is lost. In a very busy transactional database, that is a lot of work required to keep the SSTable up to date. In the next section, we continue to build on our knowledge of log-based databases and indexes with LSM Trees.Log-Structured Merge (LSM) TreesWe have established that log-based approaches to data storage can be very efficient. Just like the SSTables, LSM Trees are also log-based in their way of storing data on disk and have an in-memory data structure akin toMemtablein SSTrees. In fact, LSM Trees make use of SSTrees.LSM Trees are layered collections of Memtables and SSTables. The first layer is thememtablestored in memory. The following layers are cascaded SSTables and these layers are stored on disk. Its major characteristic can be observed in how it handles write operations. During the write operations, the entries are initially added to the memtable and are then flushed to SSTables after an interval or when it reaches a certain size. This mechanism makes writes very fast but can slow up reads.Implication on read and write operationsReads have to look up the key in thememtablefirst for the most recent entries, then traverse through the layers of SSTables. Therefore, it is a painful operation to look up a key that does not exist in the LSM tree because it ends up searching through all the layers of data available.Bloom filtershelp mitigate and optimize these experiences by being able to quickly determine if a key might exist in the SSTable.Overall, LSM Trees seem to provide superior performance when it comes to workloads that involve a lot of write operations while SSTables are preferable when quicker reads are essential.Databases like CassandraDB, LevelDB use LSM Trees.To address the limitation of preserving the indexes when the server crashes, it sounds intuitive to store the index on disk as it is smaller in size. Since it is smaller, it should be easier to read and update. However, that is not the case because of how storage disks are designed (SSDs and HDDs). In practice, what is done is that the index has a file that is read into memory when the database server is back up and running. Furthermore, for every write operation the key is added into thememtable- remember thememtableis the in-memory data structure - and aWrite Ahead Log (WAL)which is persistent on disk. Recall that write operations are very efficient. Therefore, the essence of having a persistent WAL is so that in the event of a server crash, the WAL has all that is required to rebuild the in-memory index. This WAL can be applied to both the SSTable, LSM Trees and other indexing strategies.B-TreesAll the indexes and data structures discussed above have something in common, which is they are all log-based. Here, I discuss a very popular and mature data structure that is widely used in the most popular databases today. B-Trees are data structures that keep and maintain sorted data such that they allow searches and sequential access to the data in logarithmic time. They are self-balancing and very similar in nature to a binary search tree, if you are familiar with programming.They are tree-like and essentially break down the database into fixed-sized blocks referred to aspages. These pages are commonly 4KB in size by default, although, many RDBMS systems offer the option to change the page size. In comparison to the log-based which usessegmentswhere data is append-only, B-Trees allows us to access and manipulate the data in place on the disk using references to thosepages.Because it is tree-like, it has one root, inner and leaf nodes. One node is designated to the root of the B-Tree and has pointers to children nodes. Every time a lookup is done using the key, you have to start there and traverse through different hierarchies to the key being looked for, guaranteeing access in logarithmic time.Remember, we made the assumption that our data entries are key-value pairs. Therefore, it is worth noting that out of all the 3 types of nodes - root, inner and leaf - only the leaf nodes contain the actual information (values), the others only contain references to other nodes and so on till they point to the corresponding leaf node. Additionally, within the tree hierarchy, the outermost child references indicate the boundaries in that range.When a write operation (insert or update in SQL terminology) is done, the idea is to locate the page where the values should be on disk and write the value to that page. Therefore, we must consider what happens when a page becomes full. In this case, a split operation occurs, The page is split into 2 and all cascading nodes above must be adequately updated to reflect the change that has occurred.B-Trees have depth and width that are inversely proportional to one another. That is, the deeper the B-Tree the slimmer it is. The technical term for the width is referred to as thebranching factor, which is defined as the number of references to child nodes within a single node. Linking back to a write operation that may cause a page to split, this will, in turn, require several updates if the B-Tree has a lot of depth. Additionally, careful measures must be put in place to protect the tree's data structure during split and concurrent operations, and to achieve this,internal locks (latches)are placed for the time they are being updated.B-Trees are not log-based but they also use Write-Ahead logs to recover from crashes and for redundancy.B-Trees in comparison to LSM TreesWrites are slower in B-Trees in comparison to LSM Trees. On the other hand, reads are faster when using B-Trees. It is, however, important to experiment and test extensively for any use case. Benchmarking is essential when choosing the database and indexing strategy that would best support your workload.TL;DREach data structure supporting different indexes has its strong points as well as areas of weakness. In this article, we have discussed 4 main data structures that power indexes. These are hash indexes, SSTables, LSM Trees and B-Trees. The first 3 have a log-based data structure, such that, they are append-only. Their respective limitations were mentioned, and how some address certain limitations, for instance, SSTables support range queries because the data is sorted. Some general optimizations such as the use of a Write Ahead Log for crash recovery, compaction and merging for saving disk space, and latches for concurrency control. Lastly, we briefly compared the performance of different pairs of data structures at the tail end of each section.This article is strongly inspired by my current read:Designing Data-Intensive Applications by Martin Kleppmann. I completely recommend it if you want to broaden your understanding of data systems. Please share in the comment sections interesting books, articles, and posts that have inspired and helped you understand a concept better."}
{"title": "Navigating AWS EKS with Terraform: Understanding VPC Essentials for EKS Cluster Management", "published_at": 1709590269, "tags": ["aws", "networking", "cloud", "kubernetes"], "user": "Oluwafemi Lawal", "url": "https://dev.to/aws-builders/navigating-aws-eks-with-terraform-understanding-vpc-essentials-for-eks-cluster-management-51e3", "details": "AWS EKS (Elastic Kubernetes Service) offers a lighthouse for those navigating the complex waters of cloud computing, providing a managed Kubernetes service that simplifies running containerized applications. While EKS alleviates much of the heavy lifting associated with setting up a Kubernetes cluster, a deeper understanding of its components and requirements can significantly enhance your cloud infrastructure's efficiency and security.PrerequisitesEmbarking on this journey requires:A grasp of AWS services and Kubernetes fundamentalsThe AWS CLI, configured with your credentialsTerraform, to define infrastructure as codeProject directory structure:.\u251c\u2500\u2500 modules \u2502   \u2514\u2500\u2500 aws \u2502       \u2514\u2500\u2500 vpc \u2502            \u251c\u2500\u2500 main.tf \u2502            \u251c\u2500\u2500 outputs.tf \u2502            \u251c\u2500\u2500 variables.tf \u2502            \u2514\u2500\u2500 versions.tf \u251c\u2500\u2500 variables.tf \u251c\u2500\u2500 versions.tf \u2514\u2500\u2500 vpc.tfEnter fullscreen modeExit fullscreen modeversions.tf file content:terraform{required_version=\"1.5.1\"required_providers{aws={version=\"5.20.0\"source=\"hashicorp/aws\"}}backend\"s3\"{bucket=\"\"key=\"\"region=\"\"dynamodb_table=\"\"encrypt=true}}provider\"aws\"{region=var.regiondefault_tags{tags=var.global_tags}}Enter fullscreen modeExit fullscreen modeYou can remove the S3 backend configuration if you want to store state locally, otherwise you should ensure you have created an S3 bucket and DynamoDB table to use as values for the backend.variables.tf file:variable\"region\"{type=stringdefault=\"eu-west-1\"description=\"Target AWS region\"}variable\"cluster_name\"{type=stringdefault=\"demo-cluster\"description=\"Name of the EKS cluster\"}variable\"aws_account_number\"{type=numberdescription=\"AWS account number used for deployment.\"}variable\"global_tags\"{type=map(string)default={\"ManagedBy\"=\"Terraform\"\"Environment\"=\"dev\"}}Enter fullscreen modeExit fullscreen modeMastering VPC Networking for AWS EKS ClustersWhen deploying a Kubernetes cluster in AWS using the Elastic Kubernetes Service (EKS), understanding the underlying Virtual Private Cloud (VPC) networking is crucial. It not only ensures secure communication within your cluster but also affects your cluster's interaction with the outside world, including how services are exposed and how resources are accessed.The Importance of VPC Networking in EKSVPC networking forms the backbone of your EKS cluster, dictating everything from pod communication to external access via load balancers. Here\u2019s why getting your VPC setup right is critical:Security:Properly configured VPCs contain and protect your cluster, allowing only authorized access.Connectivity:Your VPC setup determines how your cluster communicates with other AWS services, the internet, and on-premises networks.Service Discovery & Load Balancing:Integrating with AWS services like Elastic Load Balancers (ELB) requires specific VPC configurations for seamless operation.Architecting Your VPC: Public and Private SubnetsA well-architected VPC for EKS typically involves both public and private subnets:Private Subnetsare used for your worker nodes. This ensures that your workloads run securely, isolated from direct access to and from the internet. Private subnets connect to the internet through a NAT Gateway, allowing outbound traffic (e.g., for pulling container images) without exposing the nodes to inbound traffic.Public Subnetsare utilized for resources that need to be accessible from the internet, like ELBs that route external traffic to your services.Understanding NAT Gateways and Route Tables in Your VPCNAT GatewayA NAT Gateway in your VPC enables instances in a private subnet to initiate outbound traffic to the internet (for updates, downloading software, etc.) without allowing inbound traffic from the internet. This is crucial for the security and integrity of your Kubernetes nodes, ensuring they have access to necessary resources while maintaining a strong security posture.Route TablesRoute tables in AWS VPC define rules, known as routes, which determine where network traffic from your subnets or gateways is directed. In the context of EKS:Public Route Table: Directs traffic from the public subnet to the internet gateway, allowing resources in the public subnet (like ELBs) to be accessible from the internet.Private Route Table: Uses the NAT Gateway for routing outbound traffic from private subnets, ensuring that worker nodes can access the internet for essential tasks while remaining unreachable directly from the internet.Importance of TagsTags in AWS serve as identifiers for your resources, enabling you to organize, track, and manage your infrastructure components efficiently. For EKS, tagging subnets correctly is crucial:Kubernetes.io/cluster/<cluster-name>: This tag, with the value of eithersharedorowned, is essential for the Kubernetes cluster to identify which VPC resources it can manage and utilize for deploying services like Load Balancers.kubernetes.io/role/elb: This tag, set to1, identifies subnets that should be considered for hosting internet-facing ELBs. By tagging a subnet withkubernetes.io/role/elb, you're explicitly allowing Kubernetes to provision external load balancers in these subnets, facilitating access from the internet to services running in your cluster.kubernetes.io/role/internal-elb: Similarly, this tag, set to1, designates subnets for hosting internal ELBs. Internal load balancers are used to route traffic within your VPC, offering a method to expose services within your cluster to other components or services in your VPC without exposing them to the internet.Why Tagging MattersTagging subnets with these roles guides the EKS control plane and the AWS Cloud Provider implementation in Kubernetes when automatically creating ELBs for services of type LoadBalancer. Without these tags:Kubernetes may not be able to correctly provision an ELB for your service, leading to deployment issues or accessibility problems.You could manually manage load balancer provisioning and association, which increases operational overhead and complexity.Our completed VPC module will look something like this:versions.tf:terraform{required_version=\">= 1.5.1\"required_providers{aws={source=\"hashicorp/aws\"version=\">= 5.20.0\"}}}Enter fullscreen modeExit fullscreen modevariables.tf:variable\"nat_gateway\"{type=booldefault=falsedescription=\"A boolean flag to deploy NAT Gateway.\"}variable\"vpc_name\"{type=stringnullable=falsedescription=\"Name of the VPC.\"}variable\"cidr_block\"{type=stringdefault=\"10.0.0.0/16\"description=\"The IPv4 CIDR block for the VPC.\"validation{condition=can(cidrnetmask(var.cidr_block))error_message=\"Must be a valid IPv4 CIDR block address.\"}}variable\"enable_dns_support\"{type=booldefault=truedescription=\"A boolean flag to enable/disable DNS support in the VPC.\"}variable\"enable_dns_hostnames\"{type=booldefault=falsedescription=\"A boolean flag to enable/disable DNS hostnames in the VPC.\"}variable\"default_tags\"{type=map(string)default={}description=\"A map of tags to add to all resources.\"}variable\"public_subnet_count\"{type=numberdefault=3description=\"Number of Public subnets.\"}variable\"public_subnet_additional_bits\"{type=numberdefault=4description=\"Number of additional bits with which to extend the prefix.\"}variable\"public_subnet_tags\"{type=map(string)default={}description=\"A map of tags to add to all public subnets.\"}variable\"private_subnet_count\"{type=numberdefault=3description=\"Number of Private subnets.\"}variable\"private_subnet_additional_bits\"{type=numberdefault=4description=\"Number of additional bits with which to extend the prefix.\"}variable\"private_subnet_tags\"{type=map(string)default={}description=\"A map of tags to add to all private subnets.\"}Enter fullscreen modeExit fullscreen modemain.tf:data\"aws_availability_zones\"\"available\"{}############################################################################################################### VPC############################################################################################################resource\"aws_vpc\"\"main\"{cidr_block=var.cidr_blockenable_dns_support=var.enable_dns_supportenable_dns_hostnames=var.enable_dns_hostnamestags=merge(var.default_tags,{Name=var.vpc_name})}resource\"aws_default_security_group\"\"main\"{vpc_id=aws_vpc.main.id}############################################################################################################### SUBNETS############################################################################################################## Public subnetsresource\"aws_subnet\"\"public\"{count=var.public_subnet_countvpc_id=aws_vpc.main.idcidr_block=cidrsubnet(var.cidr_block,var.public_subnet_additional_bits,count.index)availability_zone=data.aws_availability_zones.available.names[count.index]tags=merge(var.default_tags,var.public_subnet_tags,{Name=\"${var.vpc_name}-public-subnet-${count.index+1}\"})}## Private Subnetsresource\"aws_subnet\"\"private\"{count=var.private_subnet_countvpc_id=aws_vpc.main.idcidr_block=cidrsubnet(var.cidr_block,var.private_subnet_additional_bits,count.index+var.public_subnet_count)availability_zone=data.aws_availability_zones.available.names[count.index]tags=merge(var.default_tags,var.private_subnet_tags,{Name=\"${var.vpc_name}-private-subnet-${count.index+1}\"})}############################################################################################################### INTERNET GATEWAY############################################################################################################resource\"aws_internet_gateway\"\"main\"{vpc_id=aws_vpc.main.idtags=merge(var.default_tags,{Name=\"${var.vpc_name}-internetgateway\"})}############################################################################################################### NAT GATEWAY############################################################################################################resource\"aws_eip\"\"nat_gateway\"{count=var.nat_gateway?1:0domain=\"vpc\"}resource\"aws_nat_gateway\"\"main\"{count=var.nat_gateway?1:0allocation_id=aws_eip.nat_gateway[0].idsubnet_id=aws_subnet.public[0].idtags=merge(var.default_tags,{Name=\"${var.vpc_name}-natgateway-default\"})depends_on=[aws_internet_gateway.main]}############################################################################################################### ROUTE TABLES############################################################################################################# Public Route tableresource\"aws_route_table\"\"public\"{vpc_id=aws_vpc.main.idtags=merge(var.default_tags,{Name=\"${var.vpc_name}-routetable-public\"})}## Public Route Table rulesresource\"aws_route\"\"public\"{route_table_id=aws_route_table.public.idgateway_id=aws_internet_gateway.main.iddestination_cidr_block=\"0.0.0.0/0\"}## Public Route table associationsresource\"aws_route_table_association\"\"public\"{count=length(aws_subnet.public)subnet_id=aws_subnet.public[count.index].idroute_table_id=aws_route_table.public.id}# Private Route tableresource\"aws_route_table\"\"private\"{vpc_id=aws_vpc.main.idtags=merge(var.default_tags,{Name=\"${var.vpc_name}-routetable-private\"})}## Private Route Table rulesresource\"aws_route\"\"private\"{route_table_id=aws_route_table.private.idnat_gateway_id=var.nat_gateway?aws_nat_gateway.main[0].id:nulldestination_cidr_block=\"0.0.0.0/0\"}## Private Route table associationsresource\"aws_route_table_association\"\"private\"{count=length(aws_subnet.private)subnet_id=aws_subnet.private[count.index].idroute_table_id=aws_route_table.private.id}Enter fullscreen modeExit fullscreen modeoutputs.tfoutput\"vpc_id\"{value=aws_vpc.main.id}output\"public_subnets\"{value=[forsubnetinaws_subnet.public:subnet.id]}output\"private_subnets\"{value=[forsubnetinaws_subnet.private:subnet.id]}output\"aws_internet_gateway\"{value=aws_internet_gateway.main}output\"aws_route_table_public\"{description=\"The ID of the public route table\"value=aws_route_table.public.id}output\"aws_route_table_private\"{description=\"The ID of the private route table\"value=aws_route_table.private.id}output\"nat_gateway_ipv4_address\"{value=var.nat_gateway?aws_eip.nat_gateway[0].public_ip:null}Enter fullscreen modeExit fullscreen modeThe usage of the module will look like this:vpc.tf:module\"vpc\"{source=\"./modules/aws/vpc/v1\"vpc_name=\"${var.cluster_name}-vpc\"cidr_block=\"10.0.0.0/16\"nat_gateway=trueenable_dns_support=trueenable_dns_hostnames=truepublic_subnet_count=3private_subnet_count=3public_subnet_tags={\"kubernetes.io/cluster/${var.cluster_name}\"=\"shared\"\"kubernetes.io/role/elb\"=\"1\"}private_subnet_tags={\"kubernetes.io/cluster/${var.cluster_name}\"=\"shared\"\"kubernetes.io/role/internal-elb\"=\"1\"}}Enter fullscreen modeExit fullscreen modeSecuring AWS EKS Networking with Security GroupsIn the realm of AWS EKS, securing the network traffic to and from your Kubernetes cluster is paramount. Security groups serve as the first line of defence, defining the rules that allow or deny network traffic to your EKS cluster and worker nodes. This article explores the role of security groups in EKS and outlines the necessary rules for secure communication within your cluster.Security Groups: The GatekeepersEKS Cluster Security GroupThe EKS Cluster Security Group acts as a shield for the control plane, governing the traffic to the Kubernetes API server, which is hosted by AWS. It's critical for enabling secure communication between the worker nodes and the control plane.resource\"aws_security_group\"\"eks_cluster_sg\"{name=\"${var.cluster_name}-eks-cluster-sg\"description=\"Security group for EKS cluster control plane communication with worker nodes\"vpc_id=module.vpc.vpc_idtags={Name=\"${var.cluster_name}-eks-cluster-sg\"}}Enter fullscreen modeExit fullscreen modeKey Rules:Ingress from Worker Nodes:Allows inbound traffic on port 443 from worker nodes to the control plane, facilitating Kubernetes API calls.resource\"aws_security_group_rule\"\"eks_cluster_ingress_nodes\"{type=\"ingress\"from_port=443to_port=443protocol=\"tcp\"security_group_id=aws_security_group.eks_cluster_sg.idsource_security_group_id=aws_security_group.eks_nodes_sg.iddescription=\"Allow inbound traffic from the worker nodes on the Kubernetes API endpoint port\"}Enter fullscreen modeExit fullscreen modeEgress to Kublet:Permits the control plane to initiate traffic to the kubelet running on each worker node, crucial for log collection and commands execution.resource\"aws_security_group_rule\"\"eks_cluster_egress_kublet\"{type=\"egress\"from_port=10250to_port=10250protocol=\"tcp\"security_group_id=aws_security_group.eks_cluster_sg.idsource_security_group_id=aws_security_group.eks_nodes_sg.iddescription=\"Allow control plane to node egress for kubelet\"}Enter fullscreen modeExit fullscreen modeWorker Nodes Security GroupWorker Nodes Security Group safeguards your worker nodes, which run your applications. It controls both inbound and outbound traffic to ensure only legitimate and secure communication occurs.resource\"aws_security_group\"\"eks_nodes_sg\"{name=\"${var.cluster_name}-eks-nodes-sg\"description=\"Security group for all nodes in the cluster\"vpc_id=module.vpc.vpc_idtags={Name=\"${var.cluster_name}-eks-nodes-sg\"\"kubernetes.io/cluster/${var.cluster_name}\"=\"owned\"}}Enter fullscreen modeExit fullscreen modeKey Rules:Control to Worker Node: Even though we gave the Cluster security group an egress rule to the Worker Nodes on the kubelet port10250, the Worker Nodes security group still has actually to allow that traffic.resource\"aws_security_group_rule\"\"worker_node_ingress_kublet\"{type=\"ingress\"from_port=10250to_port=10250protocol=\"tcp\"security_group_id=aws_security_group.eks_nodes_sg.idsource_security_group_id=aws_security_group.eks_cluster_sg.iddescription=\"Allow control plane to node ingress for kubelet\"}Enter fullscreen modeExit fullscreen modeNode-to-Node Communication:Allows nodes to communicate among themselves, which is essential for distributed systems like Kubernetes.resource\"aws_security_group_rule\"\"worker_node_to_worker_node_ingress_ephemeral\"{type=\"ingress\"from_port=1025to_port=65535protocol=\"tcp\"self=truesecurity_group_id=aws_security_group.eks_nodes_sg.iddescription=\"Allow workers nodes to communicate with each other on ephemeral ports\"}Enter fullscreen modeExit fullscreen modeEgress to the Internet:Enables nodes to initiate outbound connections to the internet via the NAT Gateway, vital for pulling container images or reaching external services. This also covers any other extra egress rules that would be needed, such as being able to communicate to the control plane on port 443.resource\"aws_security_group_rule\"\"worker_node_egress_internet\"{type=\"egress\"from_port=0to_port=0protocol=\"-1\"cidr_blocks=[\"0.0.0.0/0\"]security_group_id=aws_security_group.eks_nodes_sg.iddescription=\"Allow outbound internet access\"}Enter fullscreen modeExit fullscreen modeCoreDNS Rules:When deploying a Kubernetes cluster on AWS EKS, managing DNS resolution and traffic flow between pods is crucial for the stability and performance of your applications. CoreDNS plays a vital role in this ecosystem, serving as the cluster DNS service that enables DNS-based service discovery in Kubernetes. Its integration into an EKS cluster facilitates seamless communication between service endpoints within and outside your cluster. It also has dynamic DNS updates with pod state changes.resource\"aws_security_group_rule\"\"worker_node_to_worker_node_ingress_coredns_tcp\"{type=\"ingress\"from_port=53to_port=53protocol=\"tcp\"security_group_id=aws_security_group.eks_nodes_sg.idself=truedescription=\"Allow workers nodes to communicate with each other for coredns TCP\"}resource\"aws_security_group_rule\"\"worker_node_to_worker_node_ingress_coredns_udp\"{type=\"ingress\"from_port=53to_port=53protocol=\"udp\"security_group_id=aws_security_group.eks_nodes_sg.idself=truedescription=\"Allow workers nodes to communicate with each other for coredns UDP\"}Enter fullscreen modeExit fullscreen modeYou can then deploy your project:$terraform init$terraform plan$terraform applyEnter fullscreen modeExit fullscreen modeThe output will look something like this:Our deployed VPC:ConclusionAs we conclude our exploration of VPC essentials for EKS cluster management with Terraform, it's clear that a well-architected network foundation is pivotal for the successful deployment and operation of Kubernetes clusters on AWS. Through the thoughtful configuration of NAT Gateways, Route Tables, and appropriate tagging, we ensure our EKS clusters are both secure and efficient, positioned to leverage the best of AWS infrastructure.Security groups are essential for crafting a secure environment for your AWS EKS cluster. By meticulously defining ingress and egress rules, you ensure that your cluster communicates securely within its components and the outside world. Implementing these security measures lays the foundation for a robust and secure Kubernetes ecosystem on AWS."}
{"title": "Optimising your OpenSearch Ingestion pipeline using AWS\u00a0CDK", "published_at": 1709585654, "tags": ["javascript", "devops", "aws", "data"], "user": "Peter McAree", "url": "https://dev.to/aws-builders/optimising-your-opensearch-ingestion-pipeline-using-aws-cdk-20dm", "details": "In myprevious post, I delved into how you can use one of the latest features that AWSannouncedat re:Invent 2023, combining DynamoDB and OpenSearch providing an incredibly powerful zero-ETL ingestion mechanism for driving search and analytic applications.This integration uses OpenSearch Ingestion Service (OSIS) pipelines under the hood to consume from the DynamoDB streams. The pricing model for the ingestion pipelines is separate from the OpenSearch domains, so you will be charged additionally on top of your instance running.Depending on your use case, and workloads, you can tweak your ingestion pipelines to optimise for cost\u200a-\u200aespecially for the likes of non-production environments.Let's take a look at how we might achieve this.\ud83d\udcb0 Cost OptimisationOSIS pipelines aredefined in the OpenSearch pricing pageunder the ingestion section. The charge for majority of regions is $0.24 per OCR per hour\u200a-\u200awhilst this is a pretty competitive price, depending on your use case, data throughput, how many pipelines you require, and how many environments you are running, the cost can scale up quite quickly.For example, if you have many pipelines that push into indexes in OpenSearch, some of these may not need their data refreshed as often as others.An optimisation that can be made, especially for non-production environments, is to only have your pipeline(s) run as often as you need them to (up to a max of 24 hours).This is made possible by the fact that theDynamoDB streamwill retain any data for up to 24 hours and whenever the ingestion pipeline begins, it will consume all of the items on the stream.Combine this with EventBridge Scheduler, and we have a very powerful optimisation. EventBridge Scheduler allows us to emitStartPipelineandStopPipelineevents to target individual pipelines within OSIS. This means that I can have one particular pipeline only run once per day (for an hour) and others running constantly.\u270d\ufe0f Define some\u00a0CDKWith the introduction of EventBridge Scheduler, it allows direct integration with thousands of APIs, including the OSIS pipeline APIs.For my non-production environments, I had no need to run the pipeline for more than once per day, simply because I didn't need the data to be updated and ingested that often.To run my target pipelines once per day, I just created two separate EventBridge scheduler rules\u200a-\u200aone for starting and one for stopping.IAM RoleFirstly, we'll need to create an IAM role that gets assumed by the schedules and has permissions to interface with OSIS:consteventBridgeOsisIamRole=newcdk.aws_iam.Role(this,\"eventBridgeOsisIamRole\",{assumedBy:newcdk.aws_iam.ServicePrincipal(\"scheduler.amazonaws.com\"),managedPolicies:[cdk.aws_iam.ManagedPolicy.fromAwsManagedPolicyName(\"AmazonOpenSearchIngestionFullAccess\",),],roleName:\"eventBridgeOsisIamRole\",},);Enter fullscreen modeExit fullscreen modeEventBridge SchedulesNext we create our two schedules which target the specific OSIS pipeline by the unique name:constcreateStartSchedule=(pipelineName:string,scheduleIamRole:cdk.aws_iam.Role,)=>{constCRON_START_PIPELINE_SCHEDULE=\"cron(0 5 ? * MON-FRI *)\";// 5am WeekdaysconststartScheduleName=`${pipelineName}-start`;returnnewcdk.aws_scheduler.CfnSchedule(this,startScheduleName,{flexibleTimeWindow:{mode:\"OFF\",},name:startScheduleName,scheduleExpression:CRON_START_PIPELINE_SCHEDULE,target:{arn:\"arn:aws:scheduler:::aws-sdk:osis:startPipeline\",roleArn:scheduleIamRole.roleArn,input:JSON.stringify({PipelineName:pipelineName,}),},});};constcreateStopSchedule=(pipelineName:string,scheduleIamRole:cdk.aws_iam.Role,)=>{constCRON_STOP_PIPELINE_SCHEDULE=\"cron(0 6 ? * MON-FRI *)\";// 6am WeekdaysconststopScheduleName=`${pipelineName}-stop`;returnnewcdk.aws_scheduler.CfnSchedule(this,stopScheduleName,{flexibleTimeWindow:{mode:\"OFF\",},name:stopScheduleName,scheduleExpression:CRON_STOP_PIPELINE_SCHEDULE,target:{arn:\"arn:aws:scheduler:::aws-sdk:osis:stopPipeline\",roleArn:scheduleIamRole.roleArn,input:JSON.stringify({PipelineName:pipelineName,}),},});};Enter fullscreen modeExit fullscreen modeI've abstracted these out to separate methods so that they can be reused in the event that you have multiple pipelines\u200a-\u200adefine the pipeline names and then iterate over them:[\"first-osis-pipeline\",\"second-osis-pipeline\"].map((pipelineName:string)=>{createStartSchedule(pipelineName,eventBridgeOsisIamRole);createStopSchedule(pipelineName,eventBridgeOsisIamRole);});Enter fullscreen modeExit fullscreen modeThese constructs will create the two schedules (at 5am & 6am respectively as I've defined in the cron expression), targeting the pipelines and starts/stops using the OSIS API.And that's it! EventBridge scheduler makes this super easy since it has a first-class integration with the ingestion pipeline API.ConclusionRemember to cdk destroy once you  are complete to destroy the CloudFormation stack in order to avoid any  unnecessary costs if you're just testing this demo out!Leveraging the fact that DynamoDB streams retain their data change for 24 hours, means that we can optimise our OpenSearch ingestion rate.This approach works really well for non-production environments and use cases where you don't need to have real-time (or even slightly less frequent) ingestion into OpenSearch.We can utilise the power of EventBridge Scheduler and the fact that it has a first-class integration with OSIS pipelines to start & stop our pipelines on a cron schedule.Whenever the pipeline runs and there are items on the DynamoDB stream that haven't been read, it begins consuming and ingesting into OpenSearch.This allows us to optimise our cost and ensure that we are only consuming OCUs whenever we need to.Happy optimising!"}
{"title": "Lambda-less AppSync for SaaS", "published_at": 1709585111, "tags": ["saas", "aws", "appsync", "stepfunctions"], "user": "Jason Wadsworth", "url": "https://dev.to/aws-builders/lambda-less-appsync-for-saas-4mbj", "details": "As a builder of SaaS software, I often find myself looking at services like AppSync with a bit of jealousy. See, AppSync has a way for you to interact directly with services like DynamoDB, removing the need for a Lambda function, and the cold starts that come with it. As a SaaS builder, these direct integrations have always been out of reach because of the inability to secure the data at the tenant level. Due to some features introduced by the Step Functions team last year, there now is a way. In this post, I'll walk you through how you can access DynamoDB data from an AppSync API without the need for a Lambda function, all while maintaining tenant data isolation.Tenant IsolationBefore getting into the details of how this solution works let's be sure we understand the problem we are trying to solve. If you are building a multi-tenant SaaS application your application must be built in such a way that one tenant isn't able to access another tenant's data. I talk about this in some of my talks and have written some blogs about it as well. It's not enough to write code that isn't supposed to access the wrong tenant's data, you need to build the protection into the system; so that the code doesn't work at a permission level if it attempts to access the wrong tenant. An attempt to access the wrong tenant's data shouldn't just be a bug, it should be a failed operation. It just shouldn't be possible. This is where the problem with AppSync direct integrations comes in. When you have AppSync querying DynamoDB, for instance, you grant AppSync specific permission to do so. Those permissions aren't unique to the caller, they are only unique to the specific integration. So if tenant 1 calls the API it looks the same, at the permission level, as if tenant 2 makes the call. Not great for isolation.The typical solution to this problem is to have your AppSync talk to a Lambda function. Somewhere along the way, you do an STS AssumeRole operation to get credentials specifically for the tenant on which you want to operate and use those credentials to talk to DynamoDB. These credentials are scoped to the tenant, so you can only get data for that tenant. There are some different ways of accomplishing this, but in the end, it comes down to each call to the DynamoDB table being made with credentials specific to the tenant making the request. If you were to request data for another tenant the permissions wouldn't allow it.Unfortunately, that option isn't available to us with direct integrations. I'd love to see that change, but for now, it's just not possible.Step Functions Cross Account AccessSometime back in 2023, theStep Functions team announced a feature that would allow you to run a state machine task from one account and have it access resources in another. It turns out that this feature has a use within the same account, too.While Step Functions Cross Account Access was designed for...well, cross-account access, it's really just telling Step Functions what role to assume to perform the task. You can use that same mechanism within an account to have the state machine assume a specific role for a task. For example, let's say you want your state machine to assume a role for a specific tenant, with permissions scoped down to just that tenant's data. See where I'm going here? Because the role in the state machine can be dynamic, you can have a Step Function that assumes the role of the specified tenant, and reuse the Step Function across all your tenants, much like you would a Lambda function.Step Functions SDK IntegrationsOne of the great things about Step Functions is that it has literally hundreds of integrations available. You can make calls to most AWS services via theSDK integrations, or use theoptimized integrationsfor a smaller set of integrations. And with the ability to specify the role you want to be assumed you can call them with the permissions scoped to just the current tenant.AppSync and Step FunctionsSo far we have talked about how to have Step Functions access data for a particular tenant. What we want is for an integration with AppSync that doesn't require a Lambda function. This is where theExpress State Machine integration with AppSynccomes into play. With Express functions you can make synchronous Step Function calls that run the state machine directly.There isn't anything new about this feature, so I won't go into the details. The main point is that you can call a Step Function from AppSync and return data from there.Putting it All TogetherNow that we understand that we can use Step Functions to make direct API calls with a tenant-specific IAM role, and we can call Step Functions from AppSync, how do we put this together?To get this all to work safely we need to step back a bit and talk about the authentication. If you've read any of my previous posts on SaaS you've seen that I'll typically have acustom authorizerthat not only validates the user (typically via JWT validation) but also obtains credentials for that user's tenant. In this case, we'll take a slightly different approach. Because Step Functions will be assuming the role for us we don't need to provide credentials, but we do want to provide the role that the Step Function should use. We'll add the tenant-specific role ARN to theresolverContextof the custom authorizer. This value will be available as part of the input to your Step Function. Specifically, you can access anything that you put in theresolverContextat$.identity.resolverContextin your state machine.Tip:You can access the input arguments of your Step Functions state machine from anywhere in your state machine by going through the context object, which is accessible by using the$$notation. More information about thecontext object can be found here.You may be tempted to make the name of the role something likeTenantRole<tenant id>so that you can easily put together the role name anywhere that needs it. Doing so is not advisable because it can lead to the very problem we are trying to avoid. If the Step Function is determining the role to assume then it can make a mistake and use the wrong role. This, combined with a mistake about which tenant to access, allows the wrong tenant's data to be returned. Instead, you should have the tenant's role names be somewhat random. I like to use a ULID and store the name of the role with information about the tenant.There is one more thing to keep in mind here. You probably want to limit what roles your Step Function can assume, but you don't know the names of the roles. I like to take advantage of thepathof the role to make this easier. All my tenant-specific roles have a path that is something like/tenant-role/, which allows me to create an IAM policy that only allows assuming roles that are at that path. You can also limit what services can assume the role via the assume role policy document. Just be sure to keep in mind all the places you might want to assume this role (it's probably not just Step Functions).The TradeoffsThis may all sound a bit too good to be true. That's probably because there are some tradeoffs to be aware of.The first, and probably the most important, is that each tenant must have their own role. Quite often we use asingle role, with a dynamic policy, for tenant isolation. This has several advantages, not the least of which is that you don't have to manage all the roles. Unfortunately, the Step Functions integration doesn't allow for a dynamic policy (wouldn't that be nice?). The importance of this tradeoff can't be overstated. There is a hard limit of 5000 IAM roles per account. If you expect to have more than maybe 1000-2000 tenants you need to consider how you will manage the role limit. You might look into tenant sharding to help (Bill Tarr talks about this a bit in his talkSaaS architecture pitfalls: Lessons from the fieldfrom re:Invent 2023). In addition to the IAM limits, you need to be able to update these roles if and when your application's needs change. There are several options here, just know that this is something you have to deal with that isn't an issue when using dynamic policies.Another tradeoff here is that there aren't utility functions in either AppSync or Step Functions for unmarshalling DynamoDB formatted data. Interestingly there is a way to marshall the data in AppSync, but the AppSync direct integration automatically unmarshalls the data on the way out, so there isn't a way to do that. What does this mean? A lot of very specific mapping code that has to convert the ugliness of DynamoDB JSON into something a bit more useful.ConclusionAppSync direct integrations are a great way to allow your API to get data without needing a Lambda function. Until recently, these integrations didn't work for multi-tenant SaaS apps. With the introduction of cross-account Step Function tasks, we can now leverage the direct integrations in AppSync and Step Functions to allow us to build a multi-tenant API using AppSync without using a Lambda function, all while still isolating each tenant's data."}
{"title": "AWS VPC Endpoint is JUST a FinOps topic!?", "published_at": 1709581171, "tags": ["aws", "finops", "security", "networking"], "user": "Walid BATTOU", "url": "https://dev.to/aws-builders/aws-vpc-endpoint-is-just-a-finops-topic-5fd0", "details": "This article follows up on the previous one, where we attempted to address the following question:AWS VPC Endpoint is NOT a security topic!?Clarification about my last articleEven if we demonstrate that the VPC Endpoint is not focused on security or performance because the traffic stays on the AWS backbone. In a real-world use case, we can have compliance requirements.If you have compliance requirements such asHIPPA(health) orPCI-DSS(Bank) it is not feasible to tell the auditor \"Hey, do not check the AWS documentation, VPC Endpoint is not about security, I checked it.\"However, if you have no compliance requirements, be frugal! You will have the options described below.Why is it a topic related to cost?How VPC endpoints are billed?They are billed based on the elastic network interfaces(ENI) per availability zone behind the endpoint. We can deploy it across 1 to 3 availability zones based on your infrastructure requirements. Of course, it is recommended to have at least two for high availability.Split VPC endpoints into two categories!You will find thelist of AWS servicesthat support VPC endpoints.I split this list into two categories:Endpoints related to anapplication(eg. API gateway, Athena).Endpoints related to theinfrastructure(eg. CloudWatch, SSM).Based on that, we can start with the costs incurred when using or not using an \"infrastructure\" VPC endpoint like CloudWatch Log for example (price for eu-west-1).Let's focus on the CloudWatchLog VPC EndpointConsider a scenario where we have 10 VPCs, each hosting multiple workloads that utilize CloudWatch Logs for application logging. On average, the monthly log volume is 1TB for all of them.We have 3 options:Without VPC endpoint (with NAT Gateway).With VPC Endpoint (one per VPC).With Centralized VPC Endpoint.Cost without VPC Endpoint (with NAT Gateway)What we pay in this case:1TB(DataProcessing per month)*0.048=$48/monthTotal: $48/monthCost with VPC EndpointWhat we pay in this case:VPCEndpoint/month for 3 AZs for each VPC:$0.011*730(hours in month)*3=$~24/monthVPCEndpoint data processed:$0.01*1000=$10/monthTotal for 10VPCs: (24*10)+10=$250/monthCost with Centralized EndpointOn AWS, we usually have multiple accounts and VPCs. The general practice is to have a network account with a transit gateway in it. For more information, please refer to thisdocumentation.What we pay in this case:VPCEndpoint/month for 3 AZs:$0.011*730(hours in month)*3=$~24/monthVPCEndpoint data processed:$0.01*1000=10$/monthTransit Gateway data processed:$0.02*1000=20$/monthTotal: $54/monthConclusionWith 1TB of data transfer for 10 VPCs. The best scenario is to not use the VPC endpoint and keep using the NAT Gateway.The decision to utilize a VPC endpoint or not varies based on individual use cases. It's essential to evaluate the specific requirements and factors involved.Make your own calculations!When keeping a NAT Gateway is more cost-effective!?When does it become more economical to use the centralized solution?Based on the previous calculation, the following table shows that above approximately 1.3TB of DataTransfer, the centralized endpoint is more cost-effective. Below this amount, you can continue using the NAT Gateway.In the next article, we will see how to deploy a centralized VPC Endpoint in an air-gapped environment on AWS."}
{"title": "Checklist for designing cloud-native applications \u2013 Part 1: Introduction", "published_at": 1709580543, "tags": ["aws", "cloud", "architecture", "design"], "user": "Eyal Estrin", "url": "https://dev.to/aws-builders/checklist-for-designing-cloud-native-applications-part-1-introduction-3277", "details": "This post was originally published by theCloud Security Alliance.When organizations used to build legacy applications in the past, they used to align infrastructure and application layers to business requirements, reviewing hardware requirements and limitations, team knowledge, security, legal considerations, and more.In this series of blog posts, we will review considerations when building today's cloud-native applications.Readers of this series of blog posts can use the information shared, as a checklist to be embedded as part of a design document.IntroductionBuilding a new application requires a thorough design process.It is ok to try, fail, and fix mistakes during the process, but you still need to design.Since technology keeps evolving, new services are released every day, and many organizations now begin using multiple cloud providers, it is crucial to avoid biased decisions.During the design phase, avoid locking yourself to a specific cloud provider, instead, fully understand the requirements and constraints, and only then begin selecting the technology and services you will be using to architect your application\u2019s workload.Business RequirementsThe first thing we need to understand is what is the business goal. What is the business trying to achieve?Business requirements will impact architectural decisions.Below are some of the common business requirements:Service availability\u2013 If an application needs to be available for customers around the globe, design a multi-region architecture.Data sovereignty\u2013 If there is a regulatory requirement to store customers data in a specific country, make sure it is possible to deploy all infrastructure components in a cloud region located in a specific country. Example of data sovereignty service:AWS Digital Sovereignty.Response time\u2013 If the business requirement is to allow fast response to customer requests, you may consider the use of API or caching mechanisms.Scalability\u2013 If the business requirement is to provide customers with highly scalable applications, to be able to handle unpredictable loads, you may consider the use of event-driven architecture (such as the use of message queues, streaming services, and more).Compute ConsiderationsCompute may be the most important part of any modern application, and today there are many alternatives for running the front-end and business logic of our applications:Virtual Machines\u2013 Offering the same alternatives as we used to run legacy applications on-premise, but can also be suitable for running applications in the cloud. For most cases, use VMs if you are migrating an application from on-premise to the cloud. Example of service:Amazon EC2.Containers and Kubernetes\u2013 Most modern applications are wrapped inside containers, and very often are scheduled using Kubernetes orchestrator. Considered as a medium challenge migrating container-based workloads between cloud providers (you still need to take under consideration the integration with other managed services in the CSPs eco-system). Example of Kubernetes service:Amazon EKS.Serverless / Functions-as-a-Service\u2013 Modern way to run various parts of applications. The underlying infrastructure is fully managed by the cloud provider (no need to deal with scaling or maintenance of the infrastructure). Considered as a vendor lock-in since there is no way to migrate between CSPs, due to the unique characteristics of each CSP's offering. Example of FaaS:AWS Lambda.Data Store ConsiderationsMost applications require a persistent data store, for storing and retrieval of data.Cloud-native applications (and specifically microservice-based architecture), allow selecting the most suitable back-end data store for your applications.In a microservice-based architecture, you can select different data stores for each microservice.Alternatives for persistent data can be:Object storage\u2013 The most common managed storage service that most cloud applications are using to store data (from logs, archives, data lake, and more). Example of object storage service:Amazon S3.File storage\u2013 Most CSPs support managed NFS services (for Unix workloads) or SMB/CIFS (for Windows workloads). Example of file storage service:Amazon EFS.When designing an architecture, consider your application requirements such as:Fast data retrieval requirements \u2013 Requirements for fast read/write (measures in IOPS)File sharing requirements \u2013 Ability to connect to the storage from multiple sourcesData access pattern \u2013 Some workloads require constant access to the storage, while other connects to the storage occasionally, (such as file archive)Data replication \u2013 Ability to replicate data over multiple AZs or even multiple regionsDatabase ConsiderationsIt is very common for most applications to have at least one backend database for storing and retrieval of data.When designing an application, understand the application requirements to select the most suitable database:Relational database\u2013 Database for storing structured data stored in tables, rows, and columns. Suitable for complex queries. When selecting a relational database, consider using a managed database that supports open-source engines such as MySQL or PostgreSQL over commercially licensed database engine (to decrease the chance of vendor lock-in). Example of relational database service:Amazon RDS.Key-value database\u2013 Database for storing structured or unstructured data, with requirements for storing large amounts of data, with fast access time. Example of key-value database:Amazon DynamoDB.In-memory database\u2013 Database optimized for sub-millisecond data access, such as caching layer. Example of in-memory database:Amazon ElastiCache.Document database\u2013 Database suitable for storing JSON documents. Example of document database:Amazon DocumentDB.Graph database\u2013 Database optimized for storing and navigating relationships between entities (such as a recommendation engine). Example of Graph database:Amazon Neptune.Time-series database\u2013 Database optimized for storing and querying data that changes over time (such as application metrics, data from IoT devices, etc.). Example of time-series database:Amazon Timestream.One of the considerations when designing highly scalable applications is data replication \u2013 the ability to replicate data across multiple AZs, but the more challenging is the ability to replicate data over multiple regions.Few managed database services support global tables, or the ability to replicate over multiple regions, while most databases will require a mechanism for replicating database updates between regions.Automation and DevelopmentAutomation allows us to perform repetitive tasks in a fast and predictable way.Automation in cloud-native applications, allows us to create a CI/CD pipeline for taking developed code, integrating the various application components, and underlying infrastructure, performing various tests (from QA to securing tests) and eventually deploying new versions of our production application.Whether you are using a single cloud provider, managing environments on a large scale, or even across multiple cloud providers, you should align the tools that you are using across the different development environments:Code repositories\u2013 Select a central place to store all your development team\u2019s code, hopefully, it will allow you to use the same code repository for both on-prem and multiple cloud environments. Example of code repository:AWS CodeCommit.Container image repositories\u2013 Select a central image repository, and sync it between regions, and if needed, also between cloud providers, to keep the same source of truth. Example of container image repository:Amazon ECR.CI/CD and build process\u2013 Select a tool to allow you to manage the CI/CD pipeline for all deployments, whether you are using a single cloud provider, or when using a multi-cloud environment. Example of CI/CD build service:AWS CodePipeline.Infrastructure as Code\u2013 Mature organizations choose an IaC tool to provision infrastructure for both single or multi-cloud scenarios, lowering the burden on the DevOps, IT, and developers\u2019 teams. Examples of IaC:AWS CloudFormation, andHashiCorp Terraform.Resiliency ConsiderationsAlthough many managed services in the cloud, are offered resilient by design by the cloud providers, consider resiliency when designing production applications.Design all layers of the infrastructure to be resilient.Regardless of the computing service you choose, always deploy VMs or containers in a cluster, behind a load-balancer.Prefer to use a managed storage service, deployed over multiple availability zones.For a persistent database, prefer a managed service, and deploy it in a cluster, over multiple AZs, or even better, look for a serverless database offer, so you won\u2019t need to maintain the database availability.Do not leave things to the hands of faith, embed chaos engineering experimentations as part of your workload resiliency tests, to have a better understanding of how your workload will survive a failure. Example of managed chaos engineering service:AWS Fault Injection Service.Business Continuity ConsiderationsOne of the most important requirements from production applications is the ability to survive failure and continue functioning as expected.It is crucial to design both business continuity in advance.For any service that supports backups or snapshots (from VMs, databases, and storage services), enable scheduled backup mechanisms, and randomly test backups to make sure they are functioning.For objects stored inside an object storage service that requires resiliency, configure cross-region replication.For container registry that requires resiliency, configure image replication across regions.For applications deployed in a multi-region architecture, use DNS records to allow traffic redirection between regions.Observability ConsiderationsMonitoring and logging allow you insights into your application and infrastructure behavior.Telemetry allows you to collect real-time information about your running application, such as customer experience.While designing an application, consider all the options available for enabling logging, both from infrastructure services and from the application layer.It is crucial to stream all logs to a central system, aggregated and timed synched.Logging by itself is not enough \u2013 you need to be able to gain actionable insights, to be able to anticipate issues before they impact your customers.It is crucial to define KPIs for monitoring an application's performance, such as CPU/Memory usage, latency and uptime, average response time, etc.Many modern tools are using machine learning capabilities to review large numbers of logs, be able to correlate among multiple sources and provide recommendations for improvements.Cost ConsiderationsCost is an important factor when designing architectures in the cloud.As such, it must be embedded in every aspect of the design, implementation, and ongoing maintenance of the application and its underlying infrastructure.Cost aspects should be the responsibility of any team member (IT, developers, DevOps, architect, security staff, etc.), from both initial service cost and operational aspects.FinOps mindset will allow making sure we choose the right service for the right purpose \u2013 from choosing the right compute service, the right data store, or the right database.It is not enough to select a service \u2013make sure any service selected is tagged, monitored for its cost regularly, and perhaps even replaced with better and cost-effective alternatives, during the lifecycle of the workload.Sustainability ConsiderationsThe architectural decision we make has an environmental impact.When developing modern applications, consider the environmental impact.Choosing the right computing service will allow running a workload, with a minimal carbon footprint \u2013 the use of containers or serverless/FaaS wastes less energy in the data centers of the cloud provider.The same thing when selecting a datastore, according to an application\u2019s data access patterns (from hot or real-time tier, up to archive tier).Designing event-driven applications, adding caching layers, shutting down idle resources, and continuously monitoring workload resources, will allow to design of an efficient and sustainable workload.Sustainability related reference:AWS Sustainability.Employee Knowledge ConsiderationsThe easiest thing is to decide to build a new application in the cloud.The challenging part is to make sure all teams are aligned in terms of the path to achieving business goals and the knowledge to build modern applications in the cloud.Organizations should invest the necessary resources in employee training, making sure all team members have the required knowledge to build and maintain modern applications in the cloud.It is crucial to understand that all team members have the necessary knowledge to maintain applications and infrastructure in the cloud, before beginning the actual project, to avoid unpredictable costs, long learning curves, while running in production, or building a non-efficient workload due to knowledge gap.Training related reference:AWS Skill Builder.SummaryIn the first blog post in this series, we talked about many aspects, organizations should consider when designing new applications in the cloud.In this part of the series, we have reviewed various aspects, from understanding business requirements to selecting the right infrastructure, automation, resiliency, cost, and more.When creating the documentation for a new development project, organizations can use the information in this series, to form a checklist, making sure all-important aspects and decisions are documented.In the next chapter of this series, we will discuss security aspects when designing and building a new application in the cloud.About the AuthorEyal Estrin is a cloud and information security architect, and the author of the bookCloud Security Handbook, with more than 20 years in the IT industry. You can connect with him onTwitter.Opinions are his own and not the views of his employer."}
{"title": "Data API for Amazon Aurora Serverless v2 with AWS SDK for Java - Part 3 Executing update and insert SQL statements in batch", "published_at": 1709570510, "tags": ["aws", "serverless", "java", "database"], "user": "Vadym Kazulkin", "url": "https://dev.to/aws-builders/data-api-for-amazon-aurora-serverless-v2-with-aws-sdk-for-java-part-3-executing-update-and-insert-sql-statements-in-batch-3ajc", "details": "IntroductionIn thefirst partof the series we set up the sample application which has API Gateway in front of Lambda functions which communicate with Aurora Serverless v2 PostgreSQL database via Data API to create the products and retrieve them (by id). In thesecond partwe dove deeper into the new Data API for Aurora Serverless v2 itself and its capabilities like executing SQL Statements and used AWS SDK for Java for it. In this part of the series, we'll take a look at Data API capabilities to batch SQL statement over an array of data for bulk update and insert operations. A batch SQL statement can provide a significant performance improvement over individual insert and update statements.Data API for Aurora Serverless v2 to batch SQL statement over an array of data for bulk update and insert operationsTo show the database transaction capability I used the samesample applicationbut added the possibility to create the multiple products within HTTP PUT /products request (see the CreateProductsDataApiFunction Lambda definition in theSAM template). The correspondingLambda functionexpects the JSON as HTTP body like :[ \u2002\u2002\u2002\u2002\u2002\u2002{  \u2002\u2002\u2002\u2002\u2002\u2002\"name\": \"Calendar A3\", \u2002\u2002\u2002\u2002\u2002\u2002\"price\": \"23.56\" \u2002\u2002\u2002\u2002\u2002\u2002}, \u2002\u2002\u2002\u2002\u2002\u2002{  \u2002\u2002\u2002\u2002\u2002\u2002\"name\": \"Photobook A4\", \u2002\u2002\u2002\u2002\u2002\u2002\"price\": \"45.21\" \u2002\u2002\u2002\u2002\u2002\u2002}, \u2002\u2002\u2002\u2002\u2002\u2002{  \u2002\u2002\u2002\u2002\u2002\u2002\"name\": \"Mug red\", \u2002\u2002\u2002\u2002\u2002\u2002\"price\": \"10.31\" \u2002\u2002\u2002\u2002\u2002\u2002}\u2002\u2002\u2002\u2002\u2002 ]Enter fullscreen modeExit fullscreen modeIn this concrete example we'll create 3 products. We can then convert this JSON to the list of products usingGson.List<Product> products = new Gson(). fromJson(body, new TypeToken<List<Product>>(){}.getType());Enter fullscreen modeExit fullscreen modeThe only major difference between the ExecuteStatementRequest and BatchExecuteStatementRequest is that with the latter we need to pass the parameterSet consisting of the collection of SQL paremeters.  Below is the code sample for it.final String CREATE_PRODUCT_SQL =  \"INSERT INTO tbl_product (id, name, price)  VALUES (:id, :name, :price);\";\u2002 Set<Set<SqlParameter>> parameterSets= new HashSet<>(); for(Product product : products) {\u2002\u2002\u2002    long productId= getNextSequenceValue(\"product_id\");    product.setId(productId);     final SqlParameter productIdParam=     SqlParameter.builder().name(\"id\").value(Field.builder().    longValue(Long.valueOf(productId)).build()).build();     final SqlParameter productNameParam=          SqlParameter.builder().name(\"name\").value(Field.builder().    stringValue(product.getName()).build()).build();     final SqlParameter productPriceParam=       SqlParameter.builder().name(\"price\").value(Field.builder().    doubleValue(product.getPrice().doubleValue()).build()).build(); \u2002\u2002\u2002\u2002\u2002\u2002\u2002\u2002\u2002\u2002\u2002\u2002\u2002\u2002\u2002\u2002\u2002\u2002    Set<SqlParameter> sqlParams=       Set.of(productIdParam,productNameParam,productPriceParam);    parameterSets.add(sqlParams); }Enter fullscreen modeExit fullscreen modeWhat we do in the code above is for each product we generate its id by asking the \"product id\" sequence for its next value and construct 3 SQLParameters : id, name and price which are required to create one product, doing the same for all products in the request and finally constructing the set of SQL parameters.Then we create BatchExecuteStatementRequest by passing all the required parameters including the SQL statement and set of SQL parameters created above to iterate over it and execute the same SQL statement with the different SQL parameters.final BatchExecuteStatementRequest request=  BatchExecuteStatementRequest.builder().database(\"\"). resourceArn(dbClusterArn).secretArn(dbSecretStoreArn). sql(CREATE_PRODUCT_SQL).parameterSets(parameterSets). //formatRecordsAs(RecordsFormatType.JSON). build();Enter fullscreen modeExit fullscreen modeThen RdsDataClient needs to invoke the previously created BatchExecuteStatement.final BatchExecuteStatementResponse response=   rdsDataClient.batchExecuteStatement(request);Enter fullscreen modeExit fullscreen modeAfter it we use BatchExecuteStatementResponse to iterate over the result of each execution of the batch statement (which is insert or update SQL statement) like this.for(final UpdateResult updateResult: response.updateResults()) { \u2002\u2002\u2002\u2002\u2002\u2002....\u2002\u2002\u2002\u2002\u2002\u2002\u2002\u2002 }Enter fullscreen modeExit fullscreen modeConclusionIn this part of the series we took a look at Data API capabilities  to batch SQL statement over an array of data for bulk update and insert operations and learned about batchExecuteStatement capabilities of the RdsDataClient which uses BatchExecuteStatement Request/Response model.In the next part of series, we'll explore how to work with transactions with the Data API for the Aurora Serverless v2."}
{"title": "How to Upload Files to Amazon S3 with React and AWS SDK", "published_at": 1709567654, "tags": ["aws", "react", "s3", "javascript"], "user": "Femi Akinyemi", "url": "https://dev.to/aws-builders/how-to-upload-files-to-amazon-s3-with-react-and-aws-sdk-b0n", "details": "INTRODUCTIONHave you ever needed to upload files from your React application to the cloud?Amazon S3 (Simple Storage Service)provides a robust and scalable solution for storing and managing various file types.Using theAWS SDK, this article shows how to build aReactapplication that allows users to upload files to S3.In the article, you'll learn how to handle file selection, validate file types, and handle the upload process with error handling.By the end, you'll have a basic understanding of integrating file uploads to S3 with React and AWS.Prerequisite:Basic understanding of React and JavaScript.An AWS account and access credentials to configure the S3 connection.Getting StartedCreating S3 BucketThe first thing to do is start by creating an S3 bucket. Log in to the AWS S3 console and follow the instructions to create a new bucket, as shown in the image below.Making the files publicly accessibleNext is to edit the bucket policy to make the file publicly accessible.Here's how:Open the bucket: Go to the AWS S3 console and navigate to your specific bucket.Access permissions: Click on the 'Permissions' tab.Edit policy: Locate the 'Bucket policy' section and click the 'Edit Policy' button.Paste JSON code: Copy and paste the following JSON code into the policy editor.{\"Version\":\"2012-10-17\",\"Statement\":[{\"Sid\":\"PublicListGet\",\"Effect\":\"Allow\",\"Principal\":\"*\",\"Action\":[\"s3:List*\",\"s3:Get*\"],\"Resource\":[\"arn:aws:s3:::YOUR_BUCKET_NAME\",\"arn:aws:s3:::YOUR_BUCKET_NAME/*\"]}]}Enter fullscreen modeExit fullscreen modeSave changes: Review the policy and click 'Save'.The policy above makes all objects in the S3 bucket namedYOUR_BUCKET_NAMEpublicly accessible. Anyone online can list and download all files in the bucket and its subfolders.The policy should be used cautiously as it can be a security risk.Editing CORS PolicyNow, you must enableCross-origin resource sharing (CORS).The CORS configuration, written in JSON, allows clients to access resources in a different domain from applications within the same domain.Scroll down a bit more, and you will see a section where you can edit CORS settings.Paste the below JSON into it to enable the upload of files from the front end, or you will receive a CORS error while uploading files.[{\"AllowedHeaders\":[\"*\"],\"AllowedMethods\":[\"PUT\",\"POST\",\"DELETE\",\"GET\"],\"AllowedOrigins\":[\"*\"],\"ExposeHeaders\":[]}]Enter fullscreen modeExit fullscreen modeUploading files to your AWS S3 bucket is now possible.Next, we'll talk about the front end.Run the command below to Scaffold your react application.npxcreate-react-appfilestoragecdfilestoragenpmstartEnter fullscreen modeExit fullscreen modeInstalling AWS SDK for JavaScriptYou must install the AWS SDK for JavaScript, allowing your React JS application to communicate with S3.Run the command below to install AWS SDKnpminstallaws-sdkEnter fullscreen modeExit fullscreen modeNext is to replace the default code in the App.js file with the code belowimportReact,{useState}from'react';// Uncomment one of the following import options:importAWSfrom'aws-sdk';// Import entire SDK (optional)// import AWS from 'aws-sdk/global'; // Import global AWS namespace (recommended)importS3from'aws-sdk/clients/s3';// Import only the S3 clientimport'./App.css';functionApp(){const[file,setFile]=useState(null);const[uploading,setUploading]=useState(false)constallowedTypes=['image/jpeg','image/png','application/pdf','video/mp4','video/quicktime','audio/mpeg','audio/wav',// Add more supported types as needed];consthandleFileChange=(event)=>{constselectedFile=event.target.files[0];if(allowedTypes.includes(selectedFile.type)){setFile(selectedFile);}else{alert('Invalid file type. Only images and PDFs are allowed.');}};constuploadFile=async()=>{setUploading(true)constS3_BUCKET=\"your_bucket_name\";// Replace with your bucket nameconstREGION=\"your_region\";// Replace with your regionAWS.config.update({accessKeyId:\"your_accesskeyID\",secretAccessKey:\"your_secretAccessKey\",});consts3=newS3({params:{Bucket:S3_BUCKET},region:REGION,});constparams={Bucket:S3_BUCKET,Key:file.name,Body:file,};try{constupload=awaits3.putObject(params).promise();console.log(upload);setUploading(false)alert(\"File uploaded successfully.\");}catch(error){console.error(error);setUploading(false)alert(\"Error uploading file:\"+error.message);// Inform user about the error}};return(<><divclassName=\"\"><inputtype=\"file\"requiredonChange={handleFileChange}/><buttononClick={uploadFile}>{uploading?'Uploading...':'Upload File'}</button></div></>);}exportdefaultApp;Enter fullscreen modeExit fullscreen modeThe React code above creates a simple file uploader app. Users can select a file that is uploaded to an Amazon S3 bucket.Here's a brief breakdown:State Management:It uses React's useState hook to manage the selected file and uploading state.Allowed File Types:Defines an array of allowed file types.File Change Handler:When a file is selected, it checks if the file type is allowed and updates the state with the selected file.Upload Function:Constructs S3 parameters and uploads the file to the specified S3 bucket. It handles success and error cases, providing feedback to the user.User Interface:Provides a file input field and a button for uploading\u2014the button text changes based on the uploading state.AWS SDK:Imports necessary modules from the AWS SDK to interact with S3 and configures it with credentials and region.The code offers a basic file uploading functionality to S3 from a React app.ConclusionYou've successfully uploaded a file to your S3 bucket with these steps! You can consult the documentation for tailored options that suit your specific requirements."}
{"title": "Deploying a Serverless Dash App with AWS SAM and Lambda", "published_at": 1709560169, "tags": ["aws", "lambda", "serverless", "webdev"], "user": "Maurice Borgmeier", "url": "https://dev.to/aws-builders/deploying-a-serverless-dash-app-with-aws-sam-and-lambda-16i7", "details": "Today I'm going to show you how to deploy a Dash app in a Lambda Function behind an API Gateway. This setup is truly serverless and allows you to only pay for infrastructure when there is traffic, which is an ideal deployment model for small (internal) applications.Dashis a Python framework that enables you to build interactive frontend applications without writing a single line of Javascript. Internally and in projects we like to use it in order to build a quick proof of concept for data driven applications because of the nice integration withPlotlyandpandas. For this post, I'm going to assume that you're already familiar with Dash and won't explain that part in detail. Instead, we'll focus on what's necessary to make it run serverless.There are many options to deploy Serverless Applications in AWS and one of them isSAM, the Serverless Application Model. I chose to use it here, because it doesn't add too many layers of abstraction between what's being deployed and the code we write and our infrastructure is quite simple.You can find the full applicationon Githubif you want to deploy it yourself or just to follow along.Let's talk about what we're going to deploy before we dive deeper. The architecture diagram is pretty simple, we're only using two AWS services: API Gateway as our entrypoint of the Frontend and Lambda to run the code in a Python 3.12 runtime.Below, you can find a slightly abbreviated form of the SAM template that describes this infrastructure. As you can see it describes a pretty simple Lambda function that has two event sources. These refer to the root path and all other paths of the API gateway and basically route all requests to the API gateway to our Lambda function. Additionally we let the API Gateway know that we're handling some binary data trough theBinaryMediaTypesconfiguration. It's important that we increase both the Lambda function's memory as well as the timeout so that we get snappy responses and to account for potentially complex requests.# template.ymlAWSTemplateFormatVersion:'2010-09-09'Transform:AWS::Serverless-2016-10-31Globals:Api:Function:Timeout:30MemorySize:1024BinaryMediaTypes:# The following is important for correct handling (b64)# of binary data, e.g. png, jpg-\"*/*\"Resources:FrontendFunction:Type:AWS::Serverless::FunctionProperties:CodeUri:frontend/Handler:app.lambda_handlerRuntime:python3.12Architectures:-x86_64Events:# We want to capture both the root as well as# everything after the root pathRootIntegration:Type:ApiProperties:Path:\"/\"Method:ANYProxyIntegration:Type:ApiProperties:Path:\"/{proxy+}\"Method:ANY# ...Enter fullscreen modeExit fullscreen modeSince the AWS part of the infrastructure is rather boring, let's talk about what's necessary to make Dash play nicely with the API Gateway. Both are originally not designed to work with each other - Dash is intended to be executed as a long running process in a webserver and the API gateway is notreallyintended to serve this kind of static and dynamic website content.When the API Gateway calls the Lambda function, it embeds the information from the request in a JSON object that doesn't really match what Dash or its preferred webserver, Flask, expect. Dash would like to see a request matching theweb server gateway interface (WSGI)specification. Fortunately there's a nice community project calledapig-wsgithat aims to bridge that gap and convert one into the other.This already does 95% of what we need, we just have to adjust it a little bit to handle some corner cases. Before we do that, let's take a brief look at the Dash app that I defined:# frontend/dash_app.pyfromdashimportDash,html,dcc,Input,Output,callbackdefbuild_app(dash_kwargs:dict=None)->Dash:dash_kwargs=dash_kwargsor{}app=Dash(name=__name__,**dash_kwargs,)app.layout=html.Div(children=[html.H1(children=\"Hello World!\"),html.P(\"This is a dash app running on a serverless backend.\"),html.Img(src=\"assets/architecture.jpeg\",width=\"500px\"),# ...],# ....)returnappEnter fullscreen modeExit fullscreen modeThebuild_appfunction creates a Dash app that displays some text, an image, and some dynamic functionality. I've omitted some of the code for brevity, you can find the full codeon Github. I wrapped the instantiation in a function that allows me to pass additional arguments to the apps' constructor.Before we talk about the actual Lambda handler that does the conversion and invocation of that app, I want to introduce you to a few quirks of the API Gateway event. You may already be familiar with the concept of a stage in the API Gateway. When you deploy an API, you always deploy it to a stage which is added as a suffix to the URL:https://<id>.execute-api.<region>.amazonaws.com/<stage>/Enter fullscreen modeExit fullscreen modeWhen the Lambda function is called, API Gateway omits the/<stage>/part of the path in the top levelpathattribute of the event, which is a problem if you tell Dash to use that as part of its internal URL management logic, because any link it generates needs the/<stage>/prefix otherwise they won't work. Fortunately theactualpath is also available as part of therequestContext.This gets a bit more confusing if you're using a custom domain for the API Gateway and there suddenly is no prefix anymore. If your request is coming from that kind of source, we shouldn't add the/<stage>/prefix to URLs that we return. Long story short, we want to replace thepathattribute at the top level with theoriginalpath and have a separate handling for URLs that we send to the outside world. Here's some pseudocode of what we need to do:iforiginal_path.startswith(\"/<stage>/\"):returnbuild_dash_app_that_adds_stage_prefix()else:returnbuild_dash_app_without_stage_prefix()Enter fullscreen modeExit fullscreen modeThe actual implementation is a bit more verbose than that, but it gets the job done.# frontend/app.pyimportjsonfromfunctoolsimportlru_cachefromapig_wsgiimportmake_lambda_handlerfromdash_appimportbuild_app@lru_cache(maxsize=5)defbuild_handler(url_prefix:str)->\"Dash\":# If there's no prefix, it's a custom domainifurl_prefixisNoneorurl_prefix==\"\":returnmake_lambda_handler(wsgi_app=build_app().server,binary_support=True)# If there's a prefix we're dealing with an API gateway stage# and need to return the appropriate urls.returnmake_lambda_handler(wsgi_app=build_app({\"url_base_pathname\":url_prefix}).server,binary_support=True,)defget_raw_path(apigw_event:dict)->str:# ...returnapigw_event.get(\"requestContext\",{}).get(\"path\",apigw_event[\"path\"])defget_url_prefix(apigw_event:dict)->str:# ...apigw_stage_name=apigw_event[\"requestContext\"][\"stage\"]prefix=f\"/{apigw_stage_name}/\"raw_path=get_raw_path(apigw_event)ifraw_path.startswith(prefix):returnprefixreturn\"\"deflambda_handler(event:dict[str,\"Any\"],context:dict[str,\"Any\"])->dict[str,\"Any\"]:# We need the path with the stage prefix, which the API gateway hides a bit.event[\"path\"]=get_raw_path(event)handle_event=build_handler(get_url_prefix(event))response=handle_event(event,context)returnresponseEnter fullscreen modeExit fullscreen modeThis basically ensures that our frontend works with plain API gateway URLs as well as custom domains. If you're only using custom domains without a prefix, you can replace all of that with the following. Not everyone wants to deal with or has custom domains though.lambda_handler=make_lambda_handler(wsgi_app=build_app().server,binary_support=True)Enter fullscreen modeExit fullscreen modeNow that we've walked through most of the infrastructure code, let's deploy our app:$sam build$sam deployEnter fullscreen modeExit fullscreen modeIn the output you'll find the URL of the API Gateway, just click on it and you should be greeted with this website:As you can see, this setup allows us to service some static content in the form of text and images, but also some interactive functionality with the date picker that updates the text below it through this simple Python function:@callback(Output(\"birthdate-output\",\"children\"),Input(\"birthdate-picker\",\"date\"),prevent_initial_call=True,)defcalculate_days_since_birth(birthday_iso_string):now=datetime.now().date()birthday=date.fromisoformat(birthday_iso_string)age_in_days=(now-birthday).daysoutput=[\"You're\",html.Strong(age_in_days),\"days young!\"]returnoutputEnter fullscreen modeExit fullscreen modeWhile this setup is cool, it has some limitations:The API Gateway only listens to HTTPS traffic, that means there's no easy way to add an HTTP to HTTPS redirect (but you could add CloudFront)There is a response size limit of 6 MB, that means it's not suitable to handle large objectsAPI Gateway enforces a timeout of 29 seconds, so longer-running computations aren't possibleThe Lambda package size limits how many libraries you can use and in this small example I didn't bundle pandas for that reason. Using layers or Image-based Lambdas allows you to circumvent this limit.There are some things that can be done to make this more scalable, e.g. serving static assets from CloudFront or S3 or adding Cognito Authentication and I may get into that in a future blog post.For now that's it. I hope you learned something new today and don't forget to check out the codeon Github.\u2014 Maurice"}
{"title": "Retaining Amazon SageMaker Instance Capacity with SageMaker Managed Warm Pools", "published_at": 1709545161, "tags": ["sagemaker", "cloudml", "aws"], "user": "Chaim Rand", "url": "https://dev.to/aws-builders/retaining-amazon-sagemaker-instance-capacity-with-sagemaker-managed-warm-pools-4na4", "details": "An Alternative Solution to Cloud Instance ReservationPhoto byIvan SladeonUnsplashIn previous posts (e.g.,hereandhere), we covered some of the pros and cons of training ML workloads usingAmazon SageMaker. In this post we address one of its more inconvenient limitations \u2014 its lack of support (as of the time of this writing) for training onreserved Amazon EC2 instances. This limitation has become more and more restrictive of late due to the increasing difficulty to acquire the instance types required in a reliable and timely fashion. Recent advances in the field of generative AI have led tounprecedented demandfor AI compute while challenges in theglobal supply chaincontinue to linger. In this post we propose a partial mitigation to this limitation usingSageMaker managed warm pools. By usingSageMaker managed warm poolsyou can, under certain circumstances, retain access to provisioned instance capacity for successive training workloads. Not only will this hold on to acquired capacity for as long as you need (up to four weeks), but it can also reduce the latency between experiments.Training With Managed Warm Pools - ExampleIn the example below, we start up a PyTorch training job on ap5.48xlargeinstance of type using theAmazon SageMaker Python SDK(version 2.208). We use thekeep_alive_period_in_secondscontrol to configure the instance to remainwarmfor ten minutes.from sagemaker.pytorch import PyTorch    # define job   estimator = PyTorch(       role='<sagemaker role>',       entry_point='train.py',       instance_type='ml.p5.48xlarge',       instance_count=1,       framework_version='2.0.1',       py_version='py310',       keep_alive_period_in_seconds=60 # keep warm for 1 minute   )    # start job   estimator.fit()Enter fullscreen modeExit fullscreen modeAs long as we start up another job withmatching settingswithin the sixty seconds allotted, the same instance will be retained for the next job. Thus, by configuring the use of SageMaker warm pools we have guaranteed instance capacity for our next workload. As an added bonus, thestart-up timeof the second workload will be noticeably reduced since the instance has already been provisioned.Limitations of Managed Warm PoolsAlthough this technique offers an instance capacity guarantee similar to the one provided byAmazon EC2 reservations(and without the long-term commitment!!), it is important to note its significant limitations.The method relies on our ability to secure instance capacity for the first training job. Generally speaking, this is a safe assumption \u2014 sooner or later, we will succeed in securing an instance, but it is hard to know how much time and patience will be required.The method assumes that our workloads have matching settings, particularly with regards to the number and types of instances. Although AI development teams will frequently have multiple workloads with similar instance requirements, the inability to share resources between jobs with different settings (e.g., with differentSageMaker Roles) is limiting.The method works only if the subsequent workload is started within the specified warm pool duration, with the maximum duration being one hour. Unless we want to constantly monitor our training jobs to detect when they stop, we will need to implement an automated system for submitting new jobs when our provisioned instances become available.In cases where a matching training job is not found during the warm pool durations, we still need to pay for the provisioned instance. Thus, there is a certain risk of waste associated with this method and the way it is used (e.g., the most appropriate warm pool duration setting) should be planned accordingly.The maximum period of time during which an instance can be retained in this manner istwenty-eight days.Please see theofficial documentationfor more details on how warm pooling works as well as additionalconsiderationsassociated with its use.Reducing Cost with SageMaker Savings PlansThe method we have described for retaining control of instances is relevant for AI teams that have a consistent requirement for AI compute. This is manifested as a continuous backlog of training experiments waiting to be processed. In situations where this requirement is expected to last for an extended period of time,Amazon SageMaker Savings Plansmay provide a great opportunity for training-cost savings. SageMaker Savings Plans offer significant discounts in exchange for a commitment to pay for consistent usage. The instance types offered under this plan can vary \u2014 please refer to the documentation for the most up-do-date details. Importantly, despite some similarities toAmazon EC2 reservations, SageMaker Savings Plans doesnotguarantee instance capacity. However, the method described in this post for retaining control of provisioned instance capacity can help you take the most advantage of the instances you have committed to using.SageMaker Savings Plans is not for everyone. Make sure to fully understand all the terms of the offering before deciding whether it is the right solution for your team.SummaryA common approach for dealing with the difficulty of acquiring AI/ML compute resources in the cloud is to guarantee capacity by purchasing instance reservations. Unfortunately, as of the time of this writing, Amazon SageMaker does not support instance reservation. In this post, we have demonstrated how SageMaker warm pools can be used to maintain control over instance capacity for successive training workloads. We noted that for this type of solution to be effective, we require some form of mechanism for automating the detection of available warm pool instances and triggering a job with matching settings. In a future post we will propose a solution that addresses this challenge."}
{"title": "A Mental Model for DynamoDB Write Limits", "published_at": 1709521041, "tags": ["dynamodb", "aws", "serverless", "cdk"], "user": "Roger Chi", "url": "https://dev.to/aws-builders/a-mental-model-for-dynamodb-write-limits-2e4e", "details": "Amazon'sDynamoDBis promoted as a zero-maintenance, virtually unlimited throughput and scale*NoSQL database.  Because of its' very low administrative overhead and serverless billing model, it has long been my data store of choice when developing cloud applications and services.When working with any tool, it is important to understand the limits involved to be able to create a mental model that can be used to most effectively plan and implement the usage of that tool.  It might seem easy to think that because DynamoDB bills itself as \"unlimited throughput and scale\" you do not need to think about limitations on write and read throughput.    However, as we will explore in this post, there are actually very well defined limits that we need to take into account when implementing solutions with DynamoDB.Partition limitsAmazon documents these partition level limits:3,000 RCU/s and 1,000 WCU/s.  Let's verify these limits and put them to the test.  We want to understand what these limits are for anUpdateItemcommand and if the limits are any different for directPutItemcommands.Test setupCDK code can be found here:https://github.com/rogerchi/ddb-writes-modelWe create Items of a predefined size by filling them with random binary data.  We then set up lambda functions which will spam our DynamoDB Item with either 10,000 atomic updates, or 10,000 full puts.  We set up a stream handler to capture the times of the first and last updates to get an idea of how much time it takes to complete these 10,000 operations.UpdateItemcodeWe first initialize an Item with a count of 0 and then run this lambda function against it.asyncfunctionupdateItem(id:string){constparams={TableName:TABLE_NAME,Key:{pk:{S:id},sk:{S:'ITEM'},},UpdateExpression:'set #count = #count + :one',ExpressionAttributeNames:{'#count':'count',},ExpressionAttributeValues:{':one':{N:'1'},},};letattempt=0;while(true){try{constcommand=newUpdateItemCommand(params);awaitclient.send(command);return;}catch(error){attempt++;console.error(`Update failed for item${id}. Attempt${attempt}.`);}}}interfaceEvent{id:string;}exports.handler=async(event:Event)=>{constid=event.id;if(!id){thrownewError('Event does not contain an ID.');}consttotalUpdates=10000;constcommands=[];for(leti=0;i<totalUpdates;i++){commands.push(updateItem(id));}awaitPromise.all(commands);console.log('All updates complete.');};Enter fullscreen modeExit fullscreen modeUpdateItemresults (10,000 updates)Item Size (bytes)Total time (seconds)Op/sWCU/s1000101000100020002050010004000402501000As we can see, the results match the expected write throughput described in the documentation.  Does the throughput change if we are using thePutIteminstead of theUpdateItemcommand, since the underlying service doesn't need to retrieve an existing item to update it?PutItemcodeasyncfunctionputItem(id:string,itemBody:any,count:number){constupdatedItem={...itemBody,count:{N:`${count}`}};constparams={TableName:TABLE_NAME,Item:updatedItem,};letattempt=0;while(true){try{constcommand=newPutItemCommand(params);awaitclient.send(command);return;}catch(error){attempt++;console.error(`Put failed for item${id}. Attempt${attempt}.`,error);}}}interfaceEvent{id:string;size:number;}exports.handler=async(event:Event)=>{const{id,size}=event;constdata=generateData(id,size);constitem=marshall({pk:id,sk:'PUTS',size,data:Uint8Array.from(data),});consttotalUpdates=10000;constcommands=[];for(leti=0;i<totalUpdates;i++){commands.push(putItem(id,item,i));}awaitPromise.all(commands);console.log('All updates complete.');};Enter fullscreen modeExit fullscreen modePutItemresultsItem Size (bytes)Total time (seconds)Op/sWCU/s1000101000100020002050010004000402501000OurPutItemresults are identical to ourUpdateItemresults, and conform with the idea that the maximum write throughput for an Item is 1,000 WCU/s.TransactWriteItemTransactWriteItemcosts two WCU per 1KB to write the Item to the table.  One WCU to prepare the transaction, and one to commit the transaction.  You might assume that for a 1KB Item, given what we learned above, we will be able to perform 500 transactions per second.  Does that prove out?CodeasyncfunctiontransactUpdateItem(id:string){constparams={TableName:TABLE_NAME,Key:{pk:{S:id},sk:{S:'TRXS'},},UpdateExpression:'set #count = #count + :one',ExpressionAttributeNames:{'#count':'count',},ExpressionAttributeValues:{':one':{N:'1'},},};letattempt=0;while(true){try{constcommand=newTransactWriteItemsCommand({TransactItems:[{Update:{...params}}],});awaitclient.send(command);return;}catch(error){attempt++;console.error(`Update failed for item${id}. Attempt${attempt}.`);}}}interfaceEvent{id:string;}exports.handler=async(event:Event)=>{constid=event.id;if(!id){thrownewError('Event does not contain an ID.');}consttotalUpdates=10000;constcommands=[];for(leti=0;i<totalUpdates;i++){commands.push(transactUpdateItem(id));}awaitPromise.all(commands);console.log('All updates complete.');};Enter fullscreen modeExit fullscreen modeTransactWriteItemresultsItem Size (bytes)Total time (seconds)Op/sWCU/s1000601673332000901114444000130776158000188538511600037027864We see from theTransactWriteItemresults that we cannot achieve the full 1000 WCU/s through transactions, and that it seems like there is some fundamental overhead so that smaller items fall far from the 1000 WCU/s throughput and it's not until the item sizes get beyond 8KB that we get closer (but not quite reaching) the 1000 WCU/s partition throughput.Our mental model for DynamoDB WritesSo what's our foundational mental model for DynamoDB write limits?  You need to understand that writes target a specific Item at all times.  This Item can have up to 1000 WCUs ofPutorUpdateactions performed on it.  This means an item of up to 1KB of size can be written or updated 1,000 times a second.  An item of 2KB of size can be written or updated 500 times a second, and an item of 200KB of size can only be written or updated 5 times a second.  To be able to exceed these limits, you must shard your writes.  But because ofinstantadaptive capacity, you can think of each Item as being its own separate partition, literally a whole set of infrastructure dedicated to that single Item, which is pretty darn amazing.  It's why there's no other service that shards quite like DynamoDB, and why it is the ultimate multi-tenant service, a real example of the power of scale in the cloud.The caveat is that write throughput is significantly decreased with introducing transactions, from around 167 operations per second for a 1KB item to 27 operations per second for a 16KB item.About meI am a Staff Engineer @Vehowith a passion for serverless."}
{"title": "AWS CloudTrail Centralized logging", "published_at": 1709513725, "tags": ["aws", "cloudtrail"], "user": "Olawale Adepoju", "url": "https://dev.to/aws-builders/aws-cloudtrail-centralized-logging-4j55", "details": "AWS CloudTrail is an AWS service that allows you to implement operational and risk auditing, governance, and compliance for your AWS accounts. CloudTrail records actions made by users, roles, and AWS services as events. Events include activities performed in the AWS Management Console, AWS Command Line Interface, and AWS SDKs and APIs.CloudTrail is activated in your AWS account when you create it and does not require any manual configuration. When action occurs in your AWS account, it is captured as a CloudTrail event.The creation of an audit account and collecting of CloudTrail logs into that account are recommended as best practices if you have several accounts.Turn on CloudTrail in the account where the destination S3 bucket will belong (111111111111 in this example). Do not turn on CloudTrail in any other accounts yet. Choose use existing bucket if you have an S3 bucket already created.Note:If you configuring for AWS Organization, check the \"Enable for all accounts in organization\" and this will automatically turn on cloudtrail in all member account of the organization, So skip step 3.Choose the Event type you want to log, i will choosing the management events. Also choose the activity performed that you want to log.Review and create the Trails.Go to S3 bucket, the new bucket should have been created.Turn on CloudTrail in the other accounts you want (2222222222, 3333333333, and 4444444444 in this example). Configure CloudTrail in these accounts to use the same bucket belonging to the account that you specified in step 1 (111111111111 in this example).Under the permission tab, Update the bucket policy on your destination bucket to grant cross-account permissions to CloudTrail.{   \"Version\": \"2012-10-17\",   \"Statement\": [     {       \"Sid\": \"AWSCloudTrailAclCheck20131101\",       \"Effect\": \"Allow\",       \"Principal\": {         \"Service\": \"cloudtrail.amazonaws.com\"       },       \"Action\": \"s3:GetBucketAcl\",       \"Resource\": \"arn:aws:s3:::myBucketName\",       \"Condition\": {            \"StringEquals\": {              \"aws:SourceArn\": [        \"arn:aws:cloudtrail:region:111111111111:trail/TrailName\"             ]           }        }     },     {       \"Sid\": \"AWSCloudTrailWrite20131101\",       \"Effect\": \"Allow\",       \"Principal\": {         \"Service\": \"cloudtrail.amazonaws.com\"       },       \"Action\": \"s3:PutObject\",       \"Resource\": [         \"arn:aws:s3:::myBucketName/AWSLogs/111111111111/*\",         \"arn:aws:s3:::myBucketName/AWSLogs/222222222222/*\",         \"arn:aws:s3:::myBucketName/AWSLogs/333333333333/*\",         \"arn:aws:s3:::myBucketName/AWSLogs/444444444444/*\"       ],       \"Condition\": {          \"StringEquals\": {            \"aws:SourceArn\": [       \"arn:aws:cloudtrail:region:111111111111:trail/TrailName\"        ],           \"s3:x-amz-acl\": \"bucket-owner-full-control\"         }       }     }   ] }Enter fullscreen modeExit fullscreen mode"}
{"title": "Failed to convert \u2018Body\u2019 to string S3.InvalidContent arn:aws:states:::aws-sdk:s3:getObject in step function", "published_at": 1709513689, "tags": ["stepfunctions", "aws", "lambda", "s3"], "user": "Olawale Adepoju", "url": "https://dev.to/aws-builders/failed-to-convert-body-to-string-s3invalidcontent-arnawsstatesaws-sdks3getobject-in-step-function-lnh", "details": "ProblemI wrote a step function state machine that uses the AWS SDK to retrieve a file from S3. I input the bucket name and file name directly as the parameter.{ \"Comment\": \"A description of my state machine\", \"StartAt\": \"GetAudioFromS3\", \"States\": { \"GetAudioFromS3\": { \"Type\": \"Task\", \"Resource\": \"arn:aws:states:::aws-sdk:s3:getObject\" \"Parameters\": { \"Bucket\": \"BucketName\", \"Key\": \"FileName.wav\" }, \"End\": true } } }Enter fullscreen modeExit fullscreen modeWhen I run the task to get the file from S3, it fails withS3.InvalidContent in step: Get Audio from S3CAUSEFailed to convert 'Body' to string After expanding and trying to check the root cause I got this TaskFailed event.{ \"resourceType\": \"aws-sdk:s3\", \"resource\": \"getObject\", \"error\": \"S3.InvalidContent\", \"cause\": \"Failed to convert 'Body' to string\" }Enter fullscreen modeExit fullscreen modeHow I fixed itI change the format of my step-function code and called the bucket using variable.{ \"Comment\": \"A description of my state machine\", \"StartAt\": \"GetAudioFromS3\", \"States\": { \"GetAudioFromS3\": { \"Type\": \"Task\", \"Resource\": \"arn:aws:states:::aws-sdk:s3:getObject\" \"Parameters\": { \"Bucket.$\": \"$.S3BucketName\", \"Key.$\": \"$.InputKey\" }, \"End\": true } } }Enter fullscreen modeExit fullscreen modeThis is the JSON definition of variable{ \"S3BucketName\": \"<name of the bucket where the original file is>\", \"InputKey\": \"<name of the original file>\" }Enter fullscreen modeExit fullscreen modeWhen you are about to start the execution of the step function, it gives an option of inputting the JSON statement.NOTE:This requires inputting every time you want to run an execution, so I used lambda for automation. Once the S3 bucket details are used in lambda.{ \"Comment\": \"A State Machine that process a video file\", \"StartAt\": \"GetInput\", \"States\": { \"GetInput\": { \"Type\": \"Task\", \"Resource\": \"arn:aws:lambda:ap-northeast-1:********:function:FunctionName\", \"Next\": \"GetAudioFromS3\" }, \"GetAudioFromS3\": { \"Type\": \"Task\", \"Resource\": \"arn:aws:states:::aws-sdk:s3:getObject\" \"Parameters\": { \"Bucket.$\": \"$.S3BucketName\", \"Key.$\": \"$.InputKey\" }, \"End\": true } } }Enter fullscreen modeExit fullscreen mode"}
{"title": "Leveraging Custom Resources in AWS CloudFormation", "published_at": 1709469840, "tags": ["aws", "awcommunity", "cloudformation", "cdk"], "user": "Benjamin Ajewole", "url": "https://dev.to/aws-builders/leveraging-custom-resources-in-aws-cloudformation-4fcj", "details": "AWS CloudFormation is a powerful AWS infrastructure-as-code service, it enables you to define and provision AWS infrastructure resources in a declarative way using JSON or YAML templates. These templates describe the desired state of the AWS environment, including resources such as VPCs, EC2 instances, S3 buckets, Step Functions, Lambda functions, and more. CloudFormation automates the provisioning and management of these resources, making it easier to deploy and maintain complex AWS architectures.Limitations of CloudFormationWhile CloudFormation is an efficient tool for managing AWS resources, it does have some limitations. One major limitation is its inability to directly interact with external systems or perform actions beyond the scope of AWS services. This can be restrictive when needing to integrate with external APIs and databases or perform custom actions during stack creation or updates.Introducing Custom ResourceTo overcome the limitations of CloudFormation, AWS provides Custom Resources. Custom Resources allow you to extend CloudFormation's capabilities by incorporating custom logic or integrating with external systems during stack creation, update, or deletion. Essentially, Custom Resources enables you to define and manage AWS resources that are not natively supported by CloudFormation.Other Use Cases of Custom ResourceCustom Resources can be utilized for various use cases, including:Integration with External Systems: Execute custom logic or interact with external APIs, databases, or services during stack operations.Dependency Management: Manage dependencies between AWS resources that are not directly supported by CloudFormation.Configuration Management: Dynamically configure resources based on parameters or conditions not directly supported by CloudFormation.Data Transformation: Perform data transformations or enrichments during resource creation or updates.CloudFormation for Creating Custom ResourceResources:MyCustomResource:Type:Custom::MyCustomResourceProperties:ServiceToken:arn:aws:lambda:REGION:ACCOUNT_ID:function:MyCustomResourceFunctionResourceName:MyResourceEnter fullscreen modeExit fullscreen modeCDK Code for Creating a Custom Resourceimport*ascdkfrom'@aws-cdk/core';import*aslambdafrom'@aws-cdk/aws-lambda';import*ascrfrom'@aws-cdk/custom-resources';exportclassMyCustomResourceStackextendscdk.Stack{constructor(scope:cdk.Construct,id:string,props?:cdk.StackProps){super(scope,id,props);// Define the Lambda function for the Custom ResourceconstmyLambdaFunction=newlambda.Function(this,'MyCustomResourceHandler',{runtime:lambda.Runtime.NODEJS_20_X,handler:'index.handler',code:lambda.Code.fromAsset('lambda'),});// Define the Custom Resource ProviderconstmyProvider=newcr.Provider(this,'MyCustomResourceProvider',{onEventHandler:myLambdaFunction,});// Create the Custom Resourcenewcdk.CustomResource(this,'MyCustomResource',{serviceToken:myProvider.serviceToken,properties:{// Custom properties for the resource if needed},});}}Enter fullscreen modeExit fullscreen modeWhen to Use AwsCustomResource and ProviderAWS CDK offers two primary mechanisms for implementing Custom Resources:AwsCustomResourceandProvider.AwsCustomResource: It provides a means to extend CloudFormation's capabilities by integrating custom AWS-specific logic seamlessly into the stack operations. You can use this when you have simple Custom Resource requirements and prefer a more streamlined, high-level abstraction. It's suitable for quick implementations and scenarios where simplicity outweighs advanced customization.Provider: When the necessity arises to communicate with external systems or services beyond the AWS ecosystem, the Provider mechanism is utilized. It allows for the integration with external APIs or services through the use of Lambda functions or SDKs. With Providers, custom logic can be efficiently managed to orchestrate interactions with external systems as part of the CloudFormation stack operations.ConclusionCustom Resources offers a powerful way to extend the capabilities of AWS CloudFormation beyond its native functionalities. By leveraging Custom Resources, developers can integrate with external systems, perform custom actions, and manage dependencies more effectively within CloudFormation templates. Whether it's executing AWS API calls or interacting with external services, Custom Resources provides the flexibility needed to orchestrate complex AWS environments seamlessly.Read more on Custom Resource:https://docs.aws.amazon.com/cdk/api/v2/docs/aws-cdk-lib.custom_resources.AwsCustomResource.htmlhttps://docs.aws.amazon.com/cdk/api/v2/docs/aws-cdk-lib.custom_resources.Provider.htmlhttps://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-resource-cloudformation-customresource.html#cfn-cloudformation-customresource-servicetoken"}
{"title": "Top 10 Security Best Practices we learned the hard way", "published_at": 1709463586, "tags": ["aws", "security", "webdev", "coding"], "user": "Raphael Jambalos", "url": "https://dev.to/aws-builders/top-10-security-best-practices-we-learned-the-hard-way-3f55", "details": "Our users entrusted us with their data and it's our duty to keep this data secure as they use our application. Unfortunately, security best practices aren't the first things that developers learn when they start out. It's usually learned through experience, when the incident already happened.In this article, I'll share our team's experiences and research on how to make your applications secure. It is by no means an exhaustive list, but these are the most common things to miss.Let's dive in with an example eCommerce StoreFor this article, let's imagine Fred is an application developer for GoodProducts Manila (GP Manila). It's an eCommerce company that has a websitegpmanila.com. One of his ex-workers Nate is trying to do some damage. Here are a few ways he might do it:[1] Users can access the data of others by URL hijackingThis is by far the simplest security exploit to do, (and thankfully), the simplest to remediate.Let's dive into an example:Users of GP Manila can register for an account and log in. They can view products and place orders for these products. There is also an orders page where she can track her current and past orders, the URL looks like this:https://gpmanila.com/orders?user_id=12345orhttps://gpmanila.com/users/12345/ordersAs you may infer, this URL allows users to place the user_id of another user and possibly be able to access their order data. Nothing is stopping Nate from accessing a URL likehttps://gpmanila.com/orders?user_id=66666to try and see the data of other users. Even if the user_id is random, I can create an automated script to keep trying different combinations until I get something.Another variation of this exploit is when the frontend URL is safe, something likehttps://gpmanila.com/ordersbut the backend URL being called is something like:https://api.gpmanila.com/orders?user_id=12345This made it a little difficult for non-tech users to see this vulnerability. But our more techy friends will just have to see open the web browser's console and see this waiting for them.[2] Users can access the data of others by Form hijackingAnother similar vulnerability is when our form submissions are too lax. For example, our \"Checkout Order\" API endpoint for GP Manila contains the following request body:POST:https://gpmanila.com/orders/:checkout_order{\"items\":[{\"name\":\"silver bag\",\"price\":100,\"quantity\":1}],\"total\":100,\"user_id\":12345,\"address\":\"8 Somewhere Street, Manila City, Philippines\"}Enter fullscreen modeExit fullscreen modeWhile this method is more secure than #1, it is merely security by obscurity. By hiding the user_id inside the API request data, it is harder to find. But it is still there.Nate the hacker can just change the user_id of the API request so that his order will be charged to another user but will be delivered to his address.Resolve [1] and [2] by using JWT tokensIn this case, we are using an OAuth service called Amazon Cognito which handles the authentication of our app separately from our backend.When a user logs in, he should receive an ID token and a Refresh token from Cognito. The frontend then adds the ID token to theAuthorizationheader of every API request being sent to the backend.The ID token expires every hour. Once it expires, the backend throws an \"IdTokenExpired\" error to the FE. The FE takes this as a signal to use the refresh token to get another ID token from Cognito. This ID token is valid for another hour. And is renewed for every hour since.Both the ID token and the Refresh token are in JWT token format. Before Cognito sends these tokens to the frontend on login, it signs the JWT token with a private key. The process of signing makes sure that if the JWT body gets tampered with, the verification process will fail.Once the frontend sends the ID token back on every API request, the backend uses the public key to verify that the JWT token body has not been tampered with.So even when Nate tries to change the user_id in the ID token, the backend's verification of the signature will fail.Learn more about JWT tokenshere[3] Form submissions allow XSS attacksAt GoodProducts Manila, we have 1000 products. And we let our users review these products. Since its contents are not moderated, the reviews are posted right away.Nate the hacker can exploit this by adding JavaScript runnable content on one of our most famous products.<script>alert(\"this is a popup\")</script>Enter fullscreen modeExit fullscreen modeThis automatically gets saved in the database and becomes immediately visible to our other users. Since the review is JavaScript code, your frontend may recognize this as executable JavaScript. Hence, when other users view this review, the JS code executes.Now, my sample JS code looks safe. It simply displays a popup for every user who sees this review. But imagine the more sinister things that Nate can do when he can execute JS code on other users' browsers. He can access the user's session cookies and send them to a remote server he controls. That way, he can log in as that user and create transactions on their behalf.This exploit is called Cross-Site Scripting (XSS).Resolve this by adding AWS WAF or any web firewallAdding web firewalls like AWS WAF to your frontend and backend deployment can help remediate this. Before the review even gets created to your BE, the firewall takes a quick look at the request body and compares it with the filter rules you have set up. Most web firewalls have anti-XSS capability covered by default, helping you filter our requests with JavaScript code inside.[4] Uploaded files have a virusWe're also looking for good developers for Good Products Manila. So we created a \"Contact Us\" form where you can upload your resume along with other contact information.Nate the hacker can upload his resume in PDF format and add a virus along with it. When your HR opens the file on their end, the virus is also executed and goes on to infect his laptop.Resolve this by adding anti-virusOne way to remediate this is to add anti-virus to our application. In our case, we upload the resume to the S3 bucket and just save an S3 object URL to the database.We can build our own lambda function that scans every object that gets uploaded to the S3 bucket... Or we can buy one from the AWS Marketplace. Here are the top two options we have tried.For both options, once you have subscribed through the marketplace, you will be redirected to AWS CloudFormation to provision the service to your AWS account.Cloud Storage SecurityCloud Storage Security is the top option in terms of anti-virus. It provides access to an account in their web application. You can also add other users/groups to this account, and manage what type of access they have.It also can protect EBS and EFS volumes. And it integrates nicely with AWS Security Hub, AWS CloudTrail Lake, and AWS Transfer Family.However, the downside of this product is the cost. It charges 99USD per month for the first 100GB scanned, and 0.80USD per GB after that. All of this is on top of the cost of the AWS resources provisioned, which is usually at 40 USD (for a basic EC2 instance).bucketAV powered by ClamAV - Antivirus for Amazon S3BucketAV is cheaper but it does the basic protection job just as well. But it skimps on more advanced features that Cloud Storage offers.It charges 0.05 USD per hour on instance sizes up to m5.large, but you have to shell out for the costs of running the container infrastructure on ECS.Its UI is built on top of a CloudWatch dashboard.[5] Error messages are too specificIn the spirit of having UI that's helpful to the customer, we tell them where they went wrong. Example error notices are:User with this email does not existThe password entered for this email is wrongThe password used has already been expiredThe user is inactiveBut these errors give Nate a lot more information when doing a brute-force attack. He can brute force 1000 emails and be able to know which emails have user accounts on our side. For each user, he can also brute force the password and know the password is wrong. He can keep trying until he gets it right. That's why the best error notification we can give is:User credentials are invalidIt provides little context for Nate, just that he didn't get the password right.[6] Password policies are too simpleWe often have to protect users against themselves. Simple passwords can be cracked instantly. By forcing them to a minimum character count, and adding symbols/letters/numbers, we are increasing the complexity of the password, making it harder to crack:[7] No API or FE rate-limitingThe easiest way to brute force a password is by accessing the APIs. One way to prevent it is by adding basic rate-limiting to your APIs and frontend assets. Essentially, we limit each IP address to having a maximum of 500 calls per 5 minutes (for example). When the quota is exceeded, the FE or BE returns an error.Rate-limiting is also the easiest way to protect against DDoS attacks. Inthis blog post, we tell our experience of how we survived a DDoS attack and why rate-limiting was central to our strategy.By experience, we have to fine-tune the number. If the limit is too small, we will end up rate-limiting normal users, and end up penalizing heavy users of our site. If the limit is too big, it offers no real protection against brute force attacks. We also figured out that frontends needs a higher rate limit. Because we also serve fonts, images, and other static assets. This eats up on our limit, and we can reach it quicker compared to the backend.[8] API logs contain the entire request bodyThere is no doubt that having some sort of API request/response logging is very helpful for developers to debug issues in production. But if devs aren't careful, they may be creating more headaches for themselves later on.If we apply this logging blindly, we'll end up saving passwords, credentials, and other PII data on API logs. These logs tend to be less secure than the actual database. These logs usually sit as files inside our server, waiting for a potential hacker to penetrate the server.Resolve this issue by truncating valuable data from the logs as you write them#insteadof{\"email\":\"raphael.jamby@gmail.com\",\"password: \"imhandsome12\"}  # do instead: { \"email\": \"XXXX\", \"password:\"XXXX\"}Enter fullscreen modeExit fullscreen mode[9] Credentials are pushed on the frontendOne final issue is adding hardcoded credentials on the frontend. Sometimes, the frontend needs to access AWS resources directly: upload files to S3, access Cognito APIs for login, etc.The fastest way to do this is to use the AWS JavaScript SDK and add the AKID and secret key onto the frontend. This is a big mistake, especially if the frontend is a single-page application. There are a lot of web crawlers out there that scour the internet for these keys. And since SPAs are uploaded and delivered to the client in their entirety, the keys will be visible. In a matter of hours after deployment, you'll feel the effects of being hacked. And if the keys you uploaded are admin access, The Armageddon is coming your way.The best way to resolve this is not to use AWS AKID and Secret, but instead use AWS Amplify with Cognito. Amplify is a library that helps you call Cognito when your users log in and register. When your user logs in, Cognito and Amplify grant temporary access to specified AWS resources.[10] Not adding API request sanitationWhen you are developing your application, you know exactly what each of your API endpoints expects for it to work: attribute name, the data type, the range of valid values, etc. You should use tools likePydanticorCerberus. These tools check request data against the rules you these expectations to make sure your app gets only what it expects, otherwise, it throws an error.When you don't do request sanitation, you allow users to enter values in the API request that may potentially break the system.[BONUS] Your security is only effective if all of your team knows it tooThis final tip is not a technical one. We can have all of these best practices but if at the end of the day, your team doesn't understand why they are doing it, they probably won't end up implementing it.Want to share other security best practices? Comment them below!Photo byFlyDonUnsplash"}
