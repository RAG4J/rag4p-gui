{"title": "A Look at NAT Gateways and VPC Endpoints in AWS", "published_at": 1719579640, "tags": ["aws", "cloud", "networking"], "user": "Brandon Damue", "url": "https://dev.to/aws-builders/a-look-at-nat-gateways-and-vpc-endpoints-in-aws-28pn", "details": "Every time I get the chance, I like to write articles that are geared towards enabling you make your cloud infrastructure on AWS and other cloud platforms more secure. In today\u2019s edition of writing about AWS services, we will be learning about NAT Gateways, what they are, how they work and how they enhance our cloud infrastructure. From NAT gateways we will finish it off by talking about VPC endpoints. Allons-y (FYI: that\u2019s \u201clet\u2019s go\u201d in French \ud83d\ude09)NAT GatewaysFirst and foremost, NAT stands for Network Address Translation. Let\u2019s look at what NAT really is before moving on to NAT gateways proper. Network Address Translation is a process in which private IP addresses used on a network (usually a local area network) are translated into public IP addresses that can be used to access the internet.To understand how NAT gateways work, we are going to use the example of a two-tier architecture with a web tier deployed on EC2 instance in a public subnet (a public subnet is a subnet that has a route to an Internet gateway on the route table associated with it) and an application tier deployed on EC2 instances in a private subnet ( a private subnet has no route to an internet gateway on its route table). With this architecture, the EC2 instances that make up the application tier are unable to access the internet because they the subnet in which they reside has no route to an IGW on its route table. How will the instances go about performing tasks like downloading update patches from the internet? The answer lies in using NAT gateways. For the application tier to have access to the internet, we need to provision a NAT gateway in the public subnet housing our web tier.When an instance in the application tier wants to connect to the internet, it sends a request which carries information such as the IP address of the instance and the destination of the request to the NAT gateway in the public subnet. The NAT gateway then translates the private IP address of the instance to a public elastic IP address in its address pool and uses it to forward the request to the internet via the internet gateway. One important thing to note about NAT gateways is that, they won\u2019t accept or allow any inbound communication initiated from the internet as it only allows outbound traffic originating from your VPC. This can significantly improve the security posture of your infrastructure.NAT gateways are managed by AWS. To create a NAT gateway, all you have to do is specify the subnet it will reside in and then associate an Elastic IP address (EIP). AWS handles every other configuration for you.VPC EndpointsVPC endpoints allow private access to an array of AWS services using the internal AWS network instead of having to go through the internet using public DNS endpoints. These endpoints enable you to connect to supported services without having to configure an IGW (Internet Gateway), NAT Gateway, a VPN, or a Direct Connect (DX) connection.There are two types of VPC endpoints available on AWS. They are the Interface Endpoints and Gateway EndpointsInterface Endpoints\u2014 They are fundamentally Elastic Network Interfaces (ENI) placed in a subnet where they act as a target for any traffic that is being sent to a supported service. To be able to connect to an interface endpoint to access a supported service, you use PrivateLink. PrivateLink provides a secure and private connection between VPCs, AWS services and on-premises applications through the internal AWS network.To see the suite of services that can be accessed via interface endpoints, check out thisAWS documentation.Gateway Endpoints\u2014 They are targets within your route table that enable you to access supported services thereby keeping traffic within the AWS network. At the time of writing, the only services supported by gateway endpoints are: S3 and DynamoDB. Be sure to check the appropriate AWS documentation for any addition to the list of services. One last thing to keep in mind about gateway endpoints is that they only work with IPv4ConclusionSome say the mark of a good dancer is to know when to bow out of the stage. With that, we have officially reached the end of this article about VPC endpoints and NAT gateways. I will like to implore you to keep learning and getting better at using tools such as these for you don\u2019t know when they will come in handy. That could be sooner rather than later. Thank you for riding with me to the very end. Best of luck in all your endeavors."}
{"title": "Going on an Industry Quest: Manufacturing and Auto", "published_at": 1719562351, "tags": ["beginners", "skillbuilder", "iot", "aws"], "user": "Maurice Borgmeier", "url": "https://dev.to/aws-builders/going-on-an-industry-quest-manufacturing-and-auto-125", "details": "I currently have some time on my hands and decided to dive deeper into the AWS Skill Builder. Originally, I was interested in learning about Timestream for a training that I delivered, and then I stumbled upon the AWS Industry Quests. I remember not being very interested when theylaunched a while ago, but since they're included in my subscription, I decided to give it a chance.Because my initial search for Timestream led me to it, I started the Industry Quest for Manufacturing and Automotive. Initially, I was asked to create a company name and a character, and since I didn't care too much, I just went with a random character, although there aremanycustomization options. Once you've completed the initial setup, you're dropped into your industrial area and a tutorial that guides you through the initial steps.To become a Titan of Industry, you have to solve problems in your manufacturing plant with the help of AWS services. Doing that improves your company's and your own KPIs and level. Additionally, you collect Gems, which allow you to upgrade parts of your factory.At the time of writing, the Industry Quest: Manufacturing and Auto has 18 problems to solve, i.e., 18 labs to complete. Each Lab introduces a business context and provides an overview of the AWS services you'll be interacting with.When you start the task, you get an overview of the solution architecture, including links and additional videos explaining the services and concepts involved. This allows you to familiarize yourself with the building blocks that you'll interact with.Next, the environment introduces you to the two interactive parts of the experience. A guided practice phase in which you get step-by-step instructions to build up parts of the infrastructure (you don't start from scratch) and a later DIY phase in which you have to do some things on your own.In the practice phase, you first launch the lab environment, which may take a few minutes, depending on the Lab. You get a fresh AWS account for each Lab that's available for a couple of hours. Once that's provisioned, you log in and go through the instructions. They're usually quite accurate, although sometimes the UI screenshots are slightly out of date. I experienced some problems here, but more on that later.It's a bit frustrating that you can only switch between the steps using the two arrow buttons. That means if you're at step 50 and notice that something is not working as expected and you want to double-check instruction 15, you get to click the back button 35 times (you can also enable a scrolling feature, but that was very buggy in the browsers I tried it in. The second annoyance I experienced here was that when you switch between browser windows or tabs, it always takestwoclicks for the first button press to register, but you'll get used to that after a while.Once you have completed the practice section there's a DIY part in which you're asked to complete one or more tasks on your own. These are usually not too difficult to complete and I found them useful because they forced me to think about the problem myself. If you're struggeling, there are also some hints that can help you solve the tasks.It also has a built-in validation system to check if you've actually solved the tasks. Each DIY section explains its \"scoring\" method, and you get decent feedback for partially correct solutions.For each completed Lab, you get some experience points and gems that help you expand and improve your factory. To me, it seems like another attempt at gamification because games are supposed to have in-game currencies and levels, but I don't really see the point. At least it doesn't hurt, although, about midway through the game, I stopped being able to spend my gems on anything; I'm not sure if that was a bug or the new customizations were just not available.While these game mechanisms are part of the fun, they're not the focus of the experience. The goal is to learn about building solutions for manufacturing and automotive in AWS, primarily achieved through the labs or tasks you solve. This is where the industry quest shines.I'm probably not the intended target audience here, because I've been working with AWS for 5+ years but mostly in the serverless and data spaces, which means I have a decent amount of pre-existing knowledge that may bias how I feel about the labs.Most of the labs focus on IoT use cases and the vast majority of the tasks start out with configuring some things inIoT coreor Greengrass. You learn a lot about provisioning and updating (custom) components and software on IoT devices, which I imagine is very useful to people working in that industry.Once your things are connected to IoT core, the labs teach you what you can do with all that data. You explore machine learning services for anomaly detection and predictive maintenance. You build solutions for real-time monitoring and alerting based on Timestream and Grafana. You build fancy dashboards in Grafana that visualize machine operations using AWS TwinMaker and so much more.Aside from these metrics and monitoring you also build reporting solutions using Quicksight, which teaches you how to make management happy. Moreover you learn how to handle location data and track people through hazardous areas while visualizing and alerting when they enter dangerous parts of the factory using the Amazon Location service.One of my favorite labs is called \"Using a Digital Shadow of a Connected Vehicle\", which teaches you how you can use the IoT Device Shadows to build an app that allows you to control the state of headlights, doors, and other car components from a nice webapp. While none of the graphics here are on the level of AAA games, it's always fun to see virtual doors open when you click a checkbox, especially when the underlying synchronization mechanism could be used to have the same effect on your physical car.I can't cover all of the many labs here, but for me, this process was enjoyable for the most part. The labs cover areas of AWS that I haven't interacted (much) with so far, and they have made me interested in diving deeper. Most of the labs can be completed within about an hour, some more, some less. It depends on which services are being used. Some operations, like training a machine learning model, just take a while, but the time limits on the labs are generous, so I never felt pressured to rush anything.Here are some of my favorite labs in no particular order:Using a Digital Shadow of a Connected VehicleUses Devices Shadows in IoT to sync interactions between an app and a carConnected Vehicles TelemetryBuild a fully managed streaming data pipeline from IoT to a Grafana DashboardIndustrial Personnel and Hazard TrackingDeploy an Amplify Webapp to visualize people moving through factories and send an alert when they enter hazardous areasContextualising Industrial Equipment DataMy first interaction with SiteWiseOrganizing devices into hierarchies (e.g., factories/locations) and visualizing near-real-time metricsWhere there is light, there's also shadow. In order to complete the game and get the badge, you have to finish all the labs and not all of them work out of the box. Sometimes the instructions are out of date because something in AWS has changed.Two or three labs require you to sign up for Quicksight, and the UI is entirely different now, which means that some of the steps in the instructions are in the wrong order. This makes it a bit annoying if you've never touched Quicksight before.There's another lab that asks you to create a CodePipeline v1 through the AWS Console. Unfortunately, only v2 pipelines can be created through the console, and the older ones are no longer supported. Fortunately, all the instructions work pretty much the same way for v2 pipelines, and it's possible to complete the Lab.The worst offender is also one of the coolest labs, \"Edge to Cloud Architecture for Digital Twins\". It asks you to create a scene in TwinMaker to visualize the operations of a drill press. Quite a few things are wrong here that prevent the inexperienced user from completing the Lab and gaining the badge. Since the details are probably not super interesting to most people, I've added the problems and workarounds in theappendix, which may be helpful to you if you plan to finish the game before all the issues are fixed.Based on these problems, I think AWS should grant you the badge that you receive for completing the game if you finish at least 17 of the 18 challenges; the service teams appear to move faster than the team that's trying to keep the labs up to date. Once you finish the game, you get this beautiful badge to show off on your social media accounts.ConclusionNow that I've spent more than 18h on this game, which is the longest I've spent on any video game in the last couple of years, it's time for a conclusion. While there are some hickups with wonky controls and some out of date screenshots I mostly enjoyed myself in these labs. I'm going to specifically exclude the buggy lab from theappendixbecause that's something I expect AWS to fix soon.I liked that the labs were embedded into business problems and encompassed many parts of the AWS ecosystem that I'mnotusing daily. This means I learned about lots of new services and integration patterns, which will help me make better decisions and recommendations for our customers in the future. If you're looking for a game to speed-run, this is not it, but if you want to learn something interesting, it's an excellent place to start.AWS Skill builder offers many more features and courses than this one. If you're interested in getting it for your team, we're able to offer theteam subscription for five or more people at a discount. Justget in touch.\u2014 MauriceAppendixThe UI in step 49 asks you to create a new scene unfortunately something has changed in the mean time and by default a dynamic scene will be created. The problem is that there is some permission issue that prevents you from adding the 3D-model to this scene, which I only figured out after debugging this for a while with my colleague Franck (thanks again). If you create astaticscene, it works as expected and you can use the pretty UI.In another step of that Lab, you're asked to configure the integration of TwinMaker and Grafana, which means configuring some roles. When that's done it wants you to copy the Role ARN from the interface. Unfortunately, that's no longer displayed, so you have to navigate to IAM and copy it from there. Easy,if you already know that, probably a problem if you're not experienced.The most significant bug in that Lab is that the authentication for Grafana doesn't work out of the box. The initial environment setup creates an EC2 instance with Keycloak on it and connects that to Amazon Grafana - or is supposed to connect it to that. Unfortunately, it doesn't, so there's no way to log into Grafana. I debugged that by analyzing the EC2 instance's user data and then figured out that the setup script was pulling the latest image of Keycloak (quay.io/keycloak/keycloak:latest), which seems incompatible with the rest of the configuration. Pinning that to 22.0.1, which the script was written for, and re-running it fixed it for me.The next problem in that Lab was in Step 77, where the instructions tell you to selectrpm, whereas you need to selectrpmandstatefor the visualization to work. At that point, I was frustrated enough to accept this as a minor inaccuracy.Iwas able to fix these things with some help, because I've been working with the platform for years, but anyone coming to this game to learn about AWS would most likely be out of their depth and frustrated. I'm trying to get in touch with the right people at AWS to report this issues, once they're fixed I'll update this post."}
{"title": "AWS SnapStart - Part 23 Measuring cold and warm starts with Java 17 using asynchronous HTTP clients", "published_at": 1719500746, "tags": ["aws", "java", "serverless", "coldstart"], "user": "Vadym Kazulkin", "url": "https://dev.to/aws-builders/aws-snapstart-part-23-measuring-cold-and-warm-starts-with-java-17-using-asynchronous-http-clients-5hk4", "details": "IntroductionIn the previous parts we've done many measurements with AWS Lambda using Java 17 runtime with and without using AWS SnapStart and additionally using SnapStart and priming DynamoDB invocation :cold starts usingdifferent deployment artifact sizescold starts and deployment time usingdifferent Lambda memory settingswarm startsusing different Lambda memory settingscold and  warm startsusing different compilation optionscold and warm starts withusing different synchronous HTTP clientsIn this article we'll now add another dimension to our Java 17 measurements : the choice of the asynchronous HTTP Client implementation. AWS own offering, the asynchronous CRT HTTP client has been generally available since February 2023.I will also compare it with the same measurements for Java 21 already performed in the articleMeasuring cold and warm starts with Java 21 using different asynchronous HTTP clients.Measuring cold and warm starts with Java 17 using asynchronous HTTP clientsIn our experiment we'll re-use the application introduced inpart 8for this and rewrite it to use asynchronous HTTP client. You can the find application codehere. There are basically 2 Lambda functions which both respond to the API Gateway requests and retrieve product by id received from the API Gateway from DynamoDB. One Lambda function GetProductByIdWithPureJava17AsyncLambda can be used with and without SnapStart and the second one GetProductByIdWithPureJava17AsyncLambdaAndPriming uses SnapStart and DynamoDB request invocation priming. We give both Lambda functions 1024 MB memory.There are2 asynchronousHTTP Clients implementations available in the AWS SDK for Java.NettyNioAsync (Default)AWS CRT  (asynchronous)This is the order for the look up and set of asynchronous HTTP Client in the classpath.Let's figure out how to configure such asynchronous HTTP Client.  There are 2 places to do it :pom.xmlandDynamoProductDaoLet's consider 2 scenarios:Scenario 1)NettyNioAsync HTTP Client. It's configuration looks like thisIn pom.xml the only enabled HTTP Client dependency has to be:<dependency>         <groupId>software.amazon.awssdk</groupId>         <artifactId>netty-nio-client</artifactId>      </dependency>Enter fullscreen modeExit fullscreen modeIn DynamoProductDao the DynamoDBAsyncClient should be created like this:DynamoDbAsyncClient.builder()     .region(Region.EU_CENTRAL_1)      .httpClient(NettyNioAsyncHttpClient.create())     .overrideConfiguration(ClientOverrideConfiguration.builder()       .build())     .build();Enter fullscreen modeExit fullscreen modeScenario 2)AWS CRT synchronous HTTP Client. It's configuration looks like thisIn pom.xml the only enabled HTTP Client dependency has to be:<dependency>         <groupId>software.amazon.awssdk</groupId>         <artifactId>aws-crt-client</artifactId>      </dependency>Enter fullscreen modeExit fullscreen modeIn DynamoProductDao the DynamoDBAsyncClient should be created like this:DynamoDbAsyncClient.builder()     .region(Region.EU_CENTRAL_1)      .httpClient(AwsCrtAsyncHttpClient.create())     .overrideConfiguration(ClientOverrideConfiguration.builder()       .build())     .build();Enter fullscreen modeExit fullscreen modeFor the sake of simplicity, we create all asynchronous HTTP Clients with their default settings. Of course, there is a potential to optimize there figuring out the right settings.Using the asynchronous DynamoDBClient means that we'll be using the asynchronous programming model, so the invocation ofgetItemwill returnCompletableFutureand this is the code to retrieve the item itself (for the complete codesee)CompletableFuture<GetItemResponse> getItemReponseAsync =  dynamoDbClient.getItem(GetItemRequest.builder(). key(Map.of(\"PK\",AttributeValue.builder(). s(id).build())).tableName(PRODUCT_TABLE_NAME).build()); GetItemResponse getItemResponse = getItemReponseAsync.join(); if (getItemResponse.hasItem()) { \u2002\u2002\u2002return Optional.of(ProductMapper.productFromDynamoDB(getItemResponse.item())); \u2002}  else { \u2002\u2002\u2002return Optional.empty(); }Enter fullscreen modeExit fullscreen modeThe results of the experiment below were based on reproducing more than 100 cold and approximately 100.000 warm starts with experiment which ran for approximately 1 hour. For it (and experiments from my previous article) I used the load test toolhey, but you can use whatever tool you want, likeServerless-artilleryorPostman. I ran all these experiments for all 2 scenarios using 2 different compilation options in template.yaml each:no options (tiered compilation will take place)JAVA_TOOL_OPTIONS: \"-XX:+TieredCompilation -XX:TieredStopAtLevel=1\" (client compilation without profiling)We found out in the articleMeasuring cold and warm starts with Java 17 using different compilation optionsthat with them both we've got the lowest cold and warm start times.  We\u2019ve also got good results with \"-XX:+TieredCompilation -XX:TieredStopAtLevel=2\u201d compilation option but I haven\u2019t done any measurement with this option yet.Let's look into the results of our measurements.Cold and warm start time with compilation option \"tiered compilation\" without SnapStart enabled in ms:Scenario Numberc p50c p75c p90c p99c p99.9c maxw p50w p75w p90w p99w p99.9w maxNettyNioAsync3760.753800.163898.234101.464254.094410.896.517.519.3824.3059.112475.66AWS CRT2313.422346.892399.72502.562670.432812.785.686.457.6920.3369.90975.35Cold and warm start time with compilation option \"-XX:+TieredCompilation -XX:TieredStopAtLevel=1\" (client compilation without profiling) without SnapStart enabled in ms:Scenario Numberc p50c p75c p90c p99c p99.9c maxw p50w p75w p90w p99w p99.9w maxNettyNioAsync3708.133773.563812.513854.034019.234198.236.217.168.8022.8157.272377.48AWS CRT2331.252377.142451.722598.252756.012934.435.736.518.0021.0772.661033.18Cold and warm start time with compilation option \"tiered compilation\" with SnapStart enabled without Priming in ms:Scenario Numberc p50c p75c p90c p99c p99.9c maxw p50w p75w p90w p99w p99.9w maxNettyNioAsync2324.192380.612625.602864.132892.902895.296.727.879.9926.311683.661991.13AWS CRT1206.471348.031613.741716.901778.031779.765.736.518.0022.45692.16997.82Cold and warm start time with compilation option \"-XX:+TieredCompilation -XX:TieredStopAtLevel=1\" (client compilation without profiling) with SnapStart enabled without Priming in ms:Scenario Numberc p50c p75c p90c p99c p99.9c maxw p50w p75w p90w p99w p99.9w maxNettyNioAsync2260.042338.172586.532847.012972.032972.726.517.639.5325.091657.152132.46AWS CRT1225.921306.901618.581846.861856.111857.265.646.407.8722.09703.241069.55Cold and warm start time with compilation option \"tiered compilation\" with SnapStart enabled and with DynamoDB invocation Priming in ms:Scenario Numberc p50c p75c p90c p99c p99.9c maxw p50w p75w p90w p99w p99.9w maxNettyNioAsync744.49821.10996.801130.581255.681256.496.217.168.9423.17158.16351.03AWS CRT677.05731.94983.931279.751282.321283.55.826.728.2623.92171.221169.44Cold and warm start time with compilation option \"-XX:+TieredCompilation -XX:TieredStopAtLevel=1\" (client compilation without profiling) with SnapStart enabled and with DynamoDB invocation Priming in ms:Scenario Numberc p50c p75c p90c p99c p99.9c maxw p50w p75w p90w p99w p99.9w maxNettyNioAsync697.66747.47967.351137.381338.631339.046.417.519.3823.54155.67224.87AWS CRT694.18779.511017.941234.521243.191243.385.646.417.8721.40171.22891.36ConclusionOur measurements revealed that \"tiered compilation\" and \"-XX:+TieredCompilation -XX:TieredStopAtLevel=1\" (client compilation without profiling) values are close enough. The same we observed also with Java 21.In terms of the HTTP Client choice, AWS CRT Async HTTP Client outperformed the NettyNio Async HTTP client by far for the cold start and warm start times. The only one exception was SnapStart enabled with priming where results have been quite close. The same we observed also with Java 21.In terms of the individual comparison between Java 17 and 21 when we see lower cold starts for Java 21 for the cases where SnapStart is not enabled and it is enabled but priming is not applied. If priming is applied the cold start for Java 17 and Java 21 are very close to each other.Warm start times between Java 17 and Java 21 are very close to each other for all use cases with some deviations in both directions for the higher percentiles which might depend on the experiment.To see the full measurements for Java 21 please read my articleMeasuring cold and warm starts with Java 21 using different asynchronous HTTP clients.Can we reduce the cold start a bit further? In the previous articleMeasuring cold and warm starts with Java 17 using synchronous HTTP clientsin the \"Conclusion\" section we described how to reduce the deployment artifact size and therefore the cold start time for the AWS CRT synchronous HTTP Client. The same can also be applied for the asynchronous use case. Especially this looks promising: for the AWS CRT client we can define a classifier (i.e. linux-x86_64) in our POM file to only pick the relevant binary for our platform and reduce the size of the package. Seeherefor the detailed explanation . In this article I measured the cold and warms starts only by using the uber-jar containing binaries for all platforms, so please set the classifier and re-measure it for our platform. Be aware that currently not all platforms/architectures like aarch_64 support SnapStart.The choice of HTTP Client is not only about minimizing cold and warm starts. The decision is much more complex and also depends on the functionality of the HTTP Client implementation and its settings, like whether it supports HTTP/2. AWS published the decision tree whichHTTP client to choosedepending on the criteria."}
{"title": "Events vs streaming data in AWS", "published_at": 1719496186, "tags": ["aws", "eventdriven", "datastreaming"], "user": "Jimmy Dahlqvist", "url": "https://dev.to/aws-builders/events-vs-streaming-data-in-aws-ii9", "details": "Recently I got a question regarding streaming data vs events. Can streaming data be events? Is all streaming data events? Are all events streaming data?In this post I will give my perspective on the matter, and how I see the correlation between the two. I will also introduce some AWS based architectures for different scenarios.Defining streaming data and eventsLet's start with a small definition of streaming data and events. Streaming Data: is a continuous flows of data generated by different sources that can be processed and analyzed in real-time. This type of data can be produced at a high velocity and volume. Some examples of streaming data include sensor data from IoT devices, log files from web servers, and click stream data from websites.Events: are a record of a change in state that is important within a system. Events are often discrete, encapsulated pieces of information that indicate something has happened at a particular point in time. Examples of events are a temperature sensor reaching a threshold, a user being created in a system, or a door sensor changing state.Differences between streaming data and eventsTo understand the differences between streaming data and events there a few things we can look at. Data flowStreaming data is normally compromised by a continuous flow of data points. The granularity and velocity of the data can vary. As an example, a temperature sensor that sends current reading every second even if there is no change in temperature. The flow of points are just continuous.Events are a data flow where there has been a change in the system state that is important to the system. As example, a temperature sensor that only send temperature reading when there is a change in temperature or when a set threshold is crossed. The flow of data points depends on the change in temperature.Volume and velocityStreaming data can have extreme high volume and velocity, as this is continuous data generation and transmission. Require a robust infrastructure to handle and process the influx of data. Since data is continuous loosing a reading is often not a problem. Events can also be high in volume however as the focus on changes of state, the velocity is often lower. Still require a robust infrastructure since not loosing an event can be crucial for the system, a storage first architecture pattern is a good approach to secure this. Data structuresStreaming data can have varying structures, often including raw data that needs processing and filtering to extract meaningful information. The data might be unstructured, semi-structured, or structured.Events are usually well-structured and contain predefined attributes, making them easier to interpret and act upon. Each event has a clear schema that describes its properties. Purpose and usageStreaming data is often used for real-time or near real-time analytics and monitoring. It enables immediate insights and actions, such as anomaly detection, real-time dashboards, and live analytics. Events are focused on capturing changes in state that can invoke actions within a system. Events are often used in event-driven architectures to drive workflows, notifications, and automated responses. Similarities between streaming Data and eventsTo understand the similarities between streaming data and events there a few things we can look at. Data volumeBoth streaming data and events can generate a high volume of data that require a robust and scalable infrastructure. Often the volume over the day can fluxuate adding more requirements on the infrastructure. Real-timeBoth streaming data and events can have real-time processing requirements, with different use-cases and purposes, which can be critical for applications requiring low latency and quick decision-making. Data sourcesStreaming data and events can originate from similar sources, such as IoT devices, user interactions, and system logs. The distinction lies in how the data is captured and utilized. Practical applications and use-casesThis is some practical applications and use cases that leverage both concepts, and example architectures for implementation in AWS.Real-time analyticsCombining streaming data with event processing enables businesses to gain real-time insights. For example, companies can equip factories and production lines with IoT sensors, constantly measuring things like air quality, engine temperatures, and much more. This way it's  possible to early detect problems that might impact the quality of the product. In this solution we can rely on IoT core and have IoT sensors send data directly to the cloud over MQTT. We can create business logic and analytics to alert in case of problems. We could also have sensors send data to a central hub in the factory that then send data to a kinesis data stream for analytics. Monitoring and alertingIn cloud based applications, continuous monitoring of the system can identify issues and triggering alerts. We can utilize services like CloudWatch logs, CloudTrail, and AWS Config to gain insight and take action. This approach is enables us to understand system health and security. IoT sensorsEvent-driven architectures allow for automated responses to specific events. For example, in smart homes, events like a door opening or a motion detected can trigger actions such as turning on lights or sending notifications. This automation can enhance convenience and security. To implement this scenario we rely on IoT core for sensor events, door open or motion detected. With the powerful rules engine in IoT core we send this as an event to EventBridge, that will act as our broker. IoT core don't have a direct integration to EventBridge so we rely on SQS with EventBridge pipes. We can utilize StepFunctions to implement the business logic and then send the action back through IoT core.ConclusionIn conclusion, while streaming data and events are distinct concepts, they share similarities and can often intersect. Streaming data represents continuous flows of information, whereas events are changes in state. Understanding the nuances between the two is crucial for designing systems that leverage real-time insights and enable timely actions. Almost all the time events can be seen as streaming data, while streaming data most often is not events. Final WordsThis was a post looking the the differences and similarities between streaming data and events. Streaming data is not always events, while events often can be treated as streaming data. Check outMy serverless Handbookfor some of the concepts mentioned in this post. Don't forget to follow me onLinkedInandXfor more content, and read rest of myBlogsAs Werner says! Now Go Build!"}
