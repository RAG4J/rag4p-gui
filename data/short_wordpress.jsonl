{"post_id": 39613, "title": "Het Nederlands Gebarencentrum lanceert gebarenwoordenboek app", "url": "https://www.luminis.eu/blog-en/nederlands-gebarencentrum-de-app/", "updated_at": "2015-10-28T13:12:11", "body": "Gebarenwoordenboek iSignNGT voor Apple (iOS) en Android\nIn samenwerking met Luminis Arnhem lanceert het Nederlands Gebarencentrum de applicatie iSignNGT. iSignNGT is een Basis Gebarenwoordenboek met standaard gebaren uit de Nederlandse Gebarentaal (NGT), de taal van dove mensen in Nederland. De gebruiker krijgt de basis set van ruim 600 gebaren gratis. Voor alle gebaren is een filmpje beschikbaar, daarnaast kunnen er ook tekeningen van het gebaar, een pictogram en een afbeelding te zien zijn. De lijst met begrippen is alfabetisch en thematisch georganiseerd.\nDe gebruiker kan makkelijk switchen tussen het filmpje en de overige gegevens. De app is heel geschikt voor iedereen die in gebaren wil communiceren met dove kinderen en volwassen. Aan het basisgebarenwoordenboek kunnen thema\u2019s toegevoegd worden via een in-app purchase. De thema\u2019s die aangeschaft kunnen worden zijn te vinden in de themalijst. Dit zijn thema\u2019s als Museumlexicon, Sprookjes en verhalen en lexicon thema\u2019s behorend bij cursusmodules zoals NmGAB1 en NGTAB1. Het aantal gebaren en de prijs per thema verschilt.\n\n\n\n\n\n\n\n\n\nOver het Nederlandse Gebarencentrum\nHet Nederlands Gebarencentrum is opgericht in 1996 en is sinds 2004 door de overheid erkend als het Lexicografisch Instituut op het gebied van de Nederlandse Gebarentaal in al haar verschijningsvormen. Ze verzamelen gebaren van dove gebruikers van de Nederlandse Gebarentaal en maken de gegevens toegankelijk onder andere via een online gebarenwoordenboek waarin zowel standaardgebaren als regionale varianten te vinden zijn. Daarnaast ontwikkelen en produceren ze cursusmaterialen, (digitale) woordenboeken en educatief materiaal, wordt er onderzoek gedaan naar de grammatica van NGT en kennis gedeeld in de vorm van workshops NGT en NmG, cursussen NmG. Tevens geeft het Nederlandse Gebarencentrum voorlichting en advies over de toepassing van gebaren(taal) in de communicatie met diverse doelgroepen.\nOver Luminis Arnhem\n Luminis Arnhem is een software-technologiebedrijf dat business modellen van bedrijven aansluit op het internet. De kansen die het internet hiervoor biedt, diept Luminis samen met haar klanten uit. Beveiliging, schaalbaarheid en een continue veranderende markt vragen om visie en strategie in plaats van een appje of \u2018een stukje software\u2019. Bij Luminis werken gepassioneerde vakmensen aan nieuwe toepassingen. Software is overal, dat betekent dat gebruikers steeds vaker en in totaal verschillende contexten hiermee geconfronteerd worden. Luminis Arnhem heeft dan ook naast software-experts, User eXperience designers in dienst. De gebruiker staat van begin af aan centraal; de combinatie van UX en technologie in e\u0301e\u0301n team levert haalbare innovaties op!\niSignNGT Gebarenwoordenboek downloaden\n", "tags": [], "categories": ["Blog"]}
{"post_id": 2533, "title": "Infrastructure metrics with Elasticsearch stack", "url": "https://www.luminis.eu/blog-en/search-en/infrastructure-metrics-with-elasticsearch-stack/", "updated_at": "2020-11-12T17:09:56", "body": "For the operations team of any IT organisation it\u2019s of utmost importance to have an overview of it\u2019s infrastructure at any given point of time, whether it\u2019s the response time of various customer facing systems or memory consumption stats of the processes running across servers.\nTraditionally, these stats were\u00a0explored after some problem has occurred which can be anything from slow response time to certain processes taking over the CPU cycles. Thus it\u2019s better to have a real-time insight into the infrastructure stats which enables DevOps team to quickly find and fix the cause rather than waiting for the problem to occur. In this blog post we would be exploring the Elasticsearch product stack to propose a solution for effective monitoring of the infrastructure using Elasticsearch, Kibana and Beats.  Beats shippers are part of the Elasticsearch product stack, mainly classified into PacketBeat and TopBeat. These shippers integrate seamlessly with Elasticsearch and Kibana and thus are very useful for generating infrastructure metrics. PacketBeat is used for monitoring real-time network traffic for application level protocols like HTTP, MySQL etc as well as support for DNS which is based on UDP protocol. Whereas TopBeat is the new addition to the Elastic family and it\u2019s basically the visual counter-part of the Top command used in the terminal. Using TopBeat we can get system wide stats of memory usage (free/available) and also process specific stats, i.e. Top 10 memory consuming processes, disk space used by the process etc. PacketBeat It analyzes network packets in real-time and correlates requests with responses in order to log them as specific transactions. For each transaction it records the protocol specific fields in the ElasticSearch indices. Some examples \u2013 Mysql fields Mysql. affected_rows\u00a0 \u2013 Number of rows affected by the operation Mysql. num_fields \u2013 Incase of successful \u201cSelect\u201d how many fields are returned. Mysql.num_rows \u2013 Incase of successful \u201cSelect\u201d how many rows are returned. Query \u2013 complete query which was executed. Along with other fields like server and client IP, timestamp and if the query failed then related error information. Http Fields\u00a0 Http.code \u2013 Http code Http.content_length \u2013 size of the entity-body Along with other fields like path, params, query, response time, status etc Apart from the protocol specific fields there are some fields which are generic for all transactions for a complete list of fields please see\u00a0here Protocols supported by PacketBeat are \u2013\n\nHTTP\nPostgreSQL\nMySQL\nThrift-RPC\nRedis\nMongodb\nMemcache\nDNS\n\nPacketbeat can either be installed on the existing application servers or dedicated servers. Installing them on dedicated servers reduces the load on the application servers but it\u2019s also more costly specially in an cloud hosted environment. By using port mirroring or tap devices the network packets can be analyzed in case the packbeat is installed on the dedicated server and if it\u2019s on the same server as the application then by analyzing packets on network interfaces specified in the configuration file. The network stats would either be sent directly by PacketBeat to ElasticSearch for creating index or if an logstash indexing instance is being used for aggregating logs from various sources and then sending them finally to ElasticSearch thus in that case we would write the json output of Packetbeat first to Redis as currently the direct integration between logstash and Packetbeat doesn\u2019t exist.\n\nFigure 1- Taken from official documentation of Packetbeat Packetbeat installation is straightforward for linux and mac, for windows you need to have the powershell installed on your machine. Since I have a mac thus the commands and configuration would be mac specific and it\u2019s not much different from linux configuration. Before installing Packetbeat please make sure you have ElasticSearch and Kibana installed on your machine. PacketBeat ships the json data to ElasticSearch for indexing and that index data is displayed in graphs in Kibana. Once you have downloaded and unzipped the Packetbeat, following are the important sections prefilled in packetbeat.yml configuration file.\n\u00a0\ninterfaces:\r\n  device: any\nIn the device section, it\u2019s the network interfaces that need to be monitored are mentioned, using the keyword \u201cany\u201d implies that all network interfaces need to be sniffed for packets, current \u201cany\u201d is supported on Linux and\u00a0not on mac. On Mac for every individual network interface there needs to be separate Packetbeat instance running , for example If I want to monitor the internet traffic as well as the MySQL running on localhost then I would need to start 2\u00a0 instances of PacketBeat one having the device as \u201cen0\u201d (HTTP/DNS monitoring) and other as \u201clo0\u201d (MySQL monitoring).\u00a0 For each protocol to be monitored you need to mention the network port, Example \u2013\n\u00a0\nmemcache:\r\n  ports: [11211]\r\nmysql:\r\n  ports: [3306]\nThe configuration file already contains the default ports of the protocols supported, additional ports can be added by using comma separated values. Like in HTTP \u2013\nhttp:\r\n  ports: [80, 8080, 8000, 8002]\r\n  send_headers: [\"Host\"]\nAlso note, you can add the http header to the JSON o/p of the packetBeat by specifying the \u201csend_headers\u201d options which can contains HTTP headers.\u00a0 In my case I wanted the domain name of the website visited which is contained in the \u201cHost\u201d header of the request. You can further add more headers like \u201cUser-Agent\u201d, \u201cCookie\u201d etc. In the http configuration, you can specify the scenario in which you would like to hide certain parameters like \u201cpassword\u201d from being written to JSON o/p, thus the http configuration becomes \u2013\n\u00a0\nhttp:\r\n   ports: [80, 8080, 8000, 5000, 8002]\r\n   send_headers: [\"Host\"]\r\n   hide_keywords: [\"pass\", \"password\", \"passwd\"]\nApart from the application protocols, you can also customize the shipper that Packetbeat uses, following are the major options available \u2013\nshipper:\r\n  name: \u201cMyShipper\u201d\r\n  tags: [\u201cwebserver\u201d]\r\n  ignore_outgoing: true\nName\u00a0attribute specifies the name of the shipper, the\u00a0tags\u00a0attribute helps in grouping together of various shippers, for example if you have a cluster of webservers then all the different shippers should have a common binding tag data which make it easy to create grouped visualisation in Kibana based on common tag values. The \u201cignore_outgoing\u201d attribute is\u00a0basically is avoid the scenario of a single transaction across multiple servers to be logged twice, i.e. only incoming transaction are logged and outgoing transactions are ignored thus removing the chance of logging duplicate transactions. PacketBeat output PacketBeat can write to either Elasticsearch, Redis or File. Example configuration \u2013\n\u00a0\noutput:\r\n# Elasticsearch as output\r\nelasticsearch:\r\n   enabled: true\r\n   hosts: [\"localhost:9200\"]\r\n   save_topology: true\nThe value of enabled attribute identifies the output type out of the 3 supported. \u201csave_topology\u201d being set to true means that the Packetbeat shippers publish their IPs to an ElasticSearch index and all the shippers can share this information via the output plugin. You can also provide Elasticsearch authentication credentials in the yml configuration if Elasticsearch is using Shield plugin, thus Packetbeat shippers would need to be authenticated before being able to write to ES cluster. Now we are done with the configuration of Packetbeat and before you could start Packetbeat you need to make sure that the Elasticsearch cluster that you would be writing to has the template stored i.e. in Elasticsearch you can store templates such that when new indexes are created then the mapping of the fields of these new indexes conform to the template rules. The template is provided inside the file packetbeat.template.json in the installation directory to store the template on ES cluster we just need to execute the curl command (make sure ES is up and running!) \u2013\ncurl -XPUT \u2018http://localhost:9200/_template/packetbeat\u2019\u00a0-d@packetbeat.template.json\nNow we have the template store on ES cluster and new indexes would have the naming format as \u2013 packetbeat-YYYY.MM.DD Starting Packetbeat sudo ./packetbeat -e -c packetbeat.yml -d \u201cpublish\u201d (on mac) sudo /etc/init.d/packetbeat start (on linux) Now, I browsed internet while Packetbeat was running and since it was listening to http requestthus it logged those requests in the ES cluster with index name packetbeat-2015.10.01 I go to my marvel (ES plugin) console and execute \u2013\nGET packetbeat-2015.10.01/_search?size=1000\nAnd get back a total of 585 hits after few minutes of browsing which ofcourse includes all the js and css file\u2019s GET requests as well \ud83d\ude42 Here\u2019s the complete output of a single such document \u2013\n\u00a0\n{\r\n\"_index\":\"packetbeat-2015.10.01\",\r\n\"_type\":\"http\",\r\n\"_id\":\"AVAkFdJyMolTkkADd_vF\",\r\n\"_score\":1,\r\n\"_source\":{\r\n\"bytes_in\":1733,\r\n\"bytes_out\":439,\r\n\"client_ip\":\"10.108.xx.xx\",\r\n\"client_port\":53293,\r\n\"client_proc\":\"\",\r\n\"client_server\":\"Taruns-MacBook-Pro.local\",\r\n\"count\":1,\r\n\"http\":{\r\n\"code\":301,\r\n\"content_length\":178,\r\n\"phrase\":\"Permanently\",\r\n\"request_headers\":{\r\n\"host\":\"cnn.com\"\r\n},\r\n\"response_headers\":{\r\n}\r\n},\r\n\"ip\":\"157.166.226.25\",\r\n\"method\":\"GET\",\r\n\"params\":\"\",\r\n\"path\":\"/\",\r\n\"port\":80,\r\n\"proc\":\"\",\r\n\"query\":\"GET /\",\r\n\"responsetime\":104,\r\n\"server\":\"\",\r\n\"shipper\":\"Taruns-MacBook-Pro.local\",\r\n\"status\":\"OK\",\r\n\"timestamp\":\"2015-10-01T15:47:00.519Z\",\r\n\"type\":\"http\"\r\n}\r\n}\nNow, if we want to analyze network packets for the mysql connection then as I mentioned earlier that on mac we can monitor only one device interface using one packetbeat instance hence we would need to start another packetbeat instance for monitoring mysql traffic (device :lo0 is the change in the configuration file), on unix systems using \u201cany\u201d keyword in the device option of interface makes it possible to analyze data packets on all network interfaces.\n\u00a0\ninterfaces:\r\n device: lo0\nNow, packetbeat\u2019s instance is started again after making changes in the configuration file and now it will start analyzing mysql traffic and sending json data for indexing in the ES cluster.Let\u2019s look at one of document indexed Elasticsearch.\n{\"_index\": \"packetbeat-2015.10.01\",\r\n \"_type\": \"mysql\",\r\n \"_id\": \"AVAlD_IdtouTb4teCHYB\",\r\n \"_score\": 1,\r\n \"_source\": {\r\n \"bytes_in\": 51,\r\n \"bytes_out\": 52,\r\n \"client_ip\": \"127.0.0.1\",\r\n \"client_port\": 60083,\r\n \"client_proc\": \"\",\r\n \"client_server\": \"taruns-mbp.home\",\r\n \"count\": 1,\r\n \"ip\": \"127.0.0.1\",\r\n \"method\": \"UPDATE\",\r\n \"mysql\": {\r\n \"affected_rows\": 1,\r\n \"error_code\": 0,\r\n \"error_message\": \"\",\r\n \"insert_id\": 0,\r\n \"iserror\": false,\r\n \"num_fields\": 0,\r\n \"num_rows\": 0\r\n },\r\n \"path\": \"\",\r\n \"port\": 3306,\r\n \"proc\": \"\",\r\n \"query\": \"update authors set name = \\\"taruns\\\" where id =1\",\r\n \"responsetime\": 2,\r\n \"server\": \"taruns-mbp.home\",\r\n \"shipper\": \"taruns-mbp.home\",\r\n \"status\": \"OK\",\r\n \"timestamp\": \"2015-10-01T20:20:12.832Z\",\r\n \"type\": \"mysql\"\r\n }\r\n}\nNow in the above document we notice we have the mysql method \u201cUpdate\u201d the \u201caffected rows\u201d, the complete query and \u201cresponetime\u201d using responsetime and Kibana graphs we can clearly see the queries that took maximum amount of time during a certain duration. TopBeat TopBeat is the new addition to the Elastic\u2019s product stack, it\u2019s basically the visual alternative of the terminal\u2019s top command. \u00a0After downloading the zip file of the project and creating the Topbeat template, we just need to unzip it and start the executable (just make sure the that elasticsearch is running as well). Following is the sample default configuration provided in the topbeat.yml file\n\u00a0\ninput:\r\n period: 20\r\n procs: [\".*\"]\r\nshipper:\r\n name:\r\noutput:\r\nelasticsearch:\r\n  enabled: true\r\n  hosts: [\"localhost:9200\"]\nThe default name of the Topbeat shipper is the hostname, the period 20 secs basically means the time period interval of collecting stats and currently we are monitoring all the processes but we can give specific processes in a comma separated manner. This project ships json data to Elasticsearch which can later be visualised using Kibana. Since it\u2019s analogous to the top command thus it monitors the memory consumption of a process, it\u2019s state, process\u2019s cpu usage user space, system space and start-time. Following is a document from Elasticsearch sent by Topbeat shipper.\n\u00a0\n{\r\n \"_index\":\"topbeat-2015.09.23\",\r\n \"_type\":\"proc\",\r\n \"_id\":\"AU_5ns2rNiMHwRXM6TOU\",\r\n \"_score\":1,\r\n \"_source\":{\r\n \"count\":1,\r\n \"proc.cpu\":{\r\n \"user\":9620,\r\n \"user_p\":0.04,\r\n \"system\":4504,\r\n \"total\":14124,\r\n \"start_time\":\"10:48\"\r\n },\r\n \"proc.mem\":{\r\n \"size\":1635956,\r\n \"rss\":163396,\r\n \"rss_p\":0.97,\r\n \"share\":0\r\n },\r\n \"proc.name\":\"Microsoft Word\",\r\n \"proc.pid\":14436,\r\n \"proc.ppid\":1,\r\n \"proc.state\":\"running\",\r\n \"shipper\":\"Taruns-MacBook-Pro.local\",\r\n \"timestamp\":\"2015-09-23T09:52:57.283Z\",\r\n \"type\":\"proc\"\r\n }\r\n}\nThe name of the index(if not provide) is topbeat-YYYY.MM.DD. So in the above document we can see that the proc.name as \u201cMicrosoft Word\u201d , the process id, the parent process id. The start time, the process\u2019 cpu stats i.e. user space and system space and the memory stats i.e. virtual memory,\u00a0 resident memory(rss) (and percentage) and the total memory. In case of Topbeat as well we are writing the shipper output to Elasticsearch, we can also use Redis and File system just as in Packetbeat.\u00a0Since I am using my local machine for the blog, but in production environments, you would be having indexes generated on a daily basis and since Elasticsearch provides excellent cluster support thus scaling shouldn\u2019t be a problem. Let\u2019s now create some visualizations using Kibana, I am assuming you have basic Kibana understanding, in case you don\u2019t know much about it, you can always read the official\u00a0docs. We first need to add both the indexes in Kibana (settings section) \u2013\n\nMysql method breakdown \u2013\n\nHttp response code overview \u2013\n\nWe can further use sub-aggregations to find reponsetime for each code, domains visited etc. i.e. we have so many fields that the possibilities are pretty much endless when it comes to creating various charts based on fields available to us for each indexes. Similarly, for the Topbeat indexes we have lot of possibilities, on of it being top 10 process based on resident memory consumption.\n\nAll the visualizations can then be added onto a real-time dashboard based on \u201clast 24 hour\u201d condition thus we could easily have an overview of last 24 hours of activity. Conclusion \u2013\u00a0 We have looked at Packetbeat and Topbeat, two products which when used with ElasticSearch and Kibana can help generate some fantastic real-time Dashboard and give us insights into our Infrastructure.\n", "tags": [], "categories": ["Blog", "Search"]}
{"post_id": 2544, "title": "ElasticSearch 2.0 and Pipeline Aggregations", "url": "https://www.luminis.eu/blog-en/search-en/elasticsearch-2-0-and-pipeline-aggregations/", "updated_at": "2020-11-11T08:25:51", "body": "ElasticSearch 2.0.0 beta is out and apart from many performance related updates, one major addition has been the pipeline aggregations. This has been one of the most anticipated feature requests of the new version, as the name suggests it allows us to set up a pipleline aggregation which is able to perform computation on the buckets produced as a result of the earlier aggregation.\n\nElasticSearch Pipeline aggregations are broadly classified in two types \u2013\n\u2022 Parent \u2013 A pipeline aggregation computes it\u2019s output (bucket/aggregation) and this output gets added to the bucket/aggregation of the parent aggregation.\n\u2022 Sibling \u2013 An existing aggregation becomes an input of a pipeline of aggregations and you get new aggregations at the same level as the sibling aggregation instead of it becoming part existing buckets on the input aggregations.\n\nWe will look at some examples for each of these two to get a better understanding of the concept.\nPipeline Aggregations don\u2019t support sub-aggregations but they do support chaining, thus in a chain of pipeline aggregations the final output contains the output of each aggregation in the chain. In order to reference the aggregation which would be computed upon in a pipeline, the keyword used is \u201cbuckets_path\u201d . The syntax is as follows \u2013\n\u201c\u201dbuckets_path\u201d: \u201cAggs>Metric\u201d\u201d\nAs we see in the above syntax, bucket_path refers to an aggregation and the metric in that aggregation. Let\u2019s see some examples.\nLet us first create an index, based on the data provided in the\u00a0ElasticSearch \u2013 definitive guide. For all the commands, I have used the \u201csense\u201d extension of chrome,\u00a0as currently the 2.0.0 Beta version doesn\u2019t support the marvel plugin installation from command prompt.\n\nPOST /cars/transactions/_bulk\r\n{ \"index\": {}}\r\n{ \"price\" : 10000, \"color\" : \"red\", \"make\" : \"honda\", \"sold\" : \"2014-10-28\" }\r\n{ \"index\": {}}\r\n{ \"price\" : 20000, \"color\" : \"red\", \"make\" : \"honda\", \"sold\" : \"2014-11-05\" }\r\n{ \"index\": {}}\r\n{ \"price\" : 30000, \"color\" : \"green\", \"make\" : \"ford\", \"sold\" : \"2014-05-18\" }\r\n{ \"index\": {}}\r\n{ \"price\" : 15000, \"color\" : \"blue\", \"make\" : \"toyota\", \"sold\" : \"2014-07-\nFor brevity purposes I have only shared 4 documents, but in all I have inserted 16 records, you can improvise the above data to add 12 more records in a similar schema.\nLink for sample data to get started\u00a0\u2013\u00a0https://gist.github.com/tarunsapra/d2e5338bfb2cc032afe6\nAverage Bucket Aggregation\nThis is sibling aggregation as it calculates the avg. of a metric of a specified metric in a sibling aggregation. The sibling aggregation must be multi-bucket i.e. it should have multiple grouped values for a certain field (grouping of cars based on sold monthly). Now each group can have it\u2019s total sales per month and with the help of avg. bucket pipleline aggregation we can calculate the average total monthly sales.\n\nGET /cars/transactions/_search?search_type=count\r\n{\r\n   \"aggs\":{\r\n      \"sales_per_month\":{\r\n         \"date_histogram\":{\r\n            \"field\":\"sold\",\r\n            \"interval\":\"month\",\r\n            \"format\":\"yyyy-MM-dd\"\r\n         },\r\n         \"aggs\":{\r\n            \"monthly_sum\":{\r\n               \"sum\":{\r\n                  \"field\":\"price\"\r\n               }\r\n            }\r\n         }\r\n      },\r\n      \"avg_monthly_sales\":{\r\n         \"avg_bucket\":{\r\n            \"buckets_path\":\"sales_per_month>monthly_sum\"\r\n         }\r\n      }\r\n   }\r\n}\nNow we are calculating the average of monthly total in sales and the key syntax is the expression\n\u201cbuckets_path\u201d: \u201csales_per_month>monthly_sum\u201d\nHere the aggregation , \u201csales_per_month\u201d and it\u2019s metric \u201cmonthly_sum \u201cis specified using the buckets_path syntax and this aggregation of \u201csales_per_month\u201d gives us the sum of prices of cars sold on monthly basis and the sibling aggregation \u201cavg_monthly_sale\u201d generate the aggregation value of average total monthly sales.\n\n\"aggregations\": {\r\n      \"sales_per_month\": {\r\n         \"buckets\": [\r\n            {\r\n               \"key_as_string\": \"2014-01-01\",\r\n               \"key\": 1388534400000,\r\n               \"doc_count\": 3,\r\n               \"monthly_sum\": {\r\n                  \"value\": 185000\r\n               }\r\n            },\r\n           {\r\n               \"key_as_string\": \"2014-02-01\",\r\n               \"key\": 1391212800000,\r\n               \"doc_count\": 1,\r\n               \"monthly_sum\": {\r\n                  \"value\": 25000\r\n               }\r\n            },            \u2026\u2026\u2026\u2026\u2026\u2026  14 more records\r\n         ]\r\n      },\r\n      \"avg_monthly_sales\": {\r\n         \"value\": 41900\r\n      }\r\n   }\nThus we get the \u201cavg_monthly_sales\u201d which is in parallel to the aggregation \u201csales_per_month\u201d aggregation thus this aggregation is sibling aggregation. In the aggregation query we can also change the interval from \u201cmonth\u201d to \u201cquarter\u201d and we get the average of quarterly total.\n\n\"avg_quaterly_sales\": {\r\n\"value\": 104750\r\n}\nMaximum and Minimum bucket aggregations\nJust like average bucket aggregation, both Max and Min. bucket aggregations are sibling aggregation which are producing the output aggregation in parallel to the input aggregation in our case being \u201csales_per\u201dmonth\u201d. Max. and min. pipeline aggregation were eagerly awaited \u00a0by ES users as now it becomes straightforward to find the bucket with a max. or min. value based on the metric. In our previous example if we replace \u201cavg_monthly_sale\u201d \u00a0by-\n\n\"max_monthly_sales\": {\r\n          \"max_bucket\": {\r\n              \"buckets_path\": \"sales_per_month>monthly_sum\"\r\n          }\r\n      }\nand then by\n\"min_monthly_sales\": {\r\n           \"min_bucket\": {\r\n               \"buckets_path\": \"sales_per_month>monthly_sum\"\r\n           }\r\n       }\nWe get the following in the output\n\"min_monthly_sales\": {\r\n        \"value\": 10000,\r\n        \"keys\": [\r\n           \"2014-10-01\"\r\n        ]\r\n     }\nand for maximum \u2013\n\"max_monthly_sales\": {\r\n         \"value\": 185000,\r\n         \"keys\": [\r\n            \"2014-01-01\"\r\n         ]\r\n      }\nThus we get the max. and min. bucket key along with the value ( this is really cool! ).\nSum Bucket Aggregation\nThis aggregation is again a sibling aggregation and helps in calculating the sum of all the bucket\u2019s metrics.\u00a0 For example if in our original aggregation statement, we add the following query before the \u201caggs\u201d starts i.e. \u2013\n\n\"query\" : {\r\n        \"match\" : {\r\n            \"make\" : \"bmw\"\r\n        }\r\n   },\r\n\u201caggs\u201d \u2026..\r\n\u2026\u2026\nAnd now we do the aggregation \u201csum_bmw_sales\u201d for the maker BMW and then just like and max and min. bucket pipeline aggregation we can add \u2013\n\n\"sum_bmw_sales\": {\r\n      \"sum_bucket\": {\r\n            \"buckets_path\": \"sales_per_month>monthly_sum\"\r\n            }\r\n        }\nThus now we have the per monthly total sale of the BMWs and the total yearly sum of the BMW label as well, in similar manner instead of the car label we can also specify date range or color based search and sum.\nDerivative Aggregation\nThis aggregation is a\u00a0parent aggregation\u00a0as the computed derivative of the specified metric becomes part of the bucket of the input aggregation.\n\nGET /cars/transactions/_search?search_type=count\r\n{\r\n   \"aggs\": {\r\n      \"sales_per_month\": {\r\n         \"date_histogram\": {\r\n            \"field\": \"sold\",\r\n            \"interval\": \"month\",\r\n            \"format\": \"yyyy-MM-dd\"\r\n         },\r\n         \"aggs\": {\r\n            \"monthly_sum\": {\r\n               \"sum\": {\r\n                  \"field\": \"price\"\r\n               }\r\n            },\r\n            \"sales_deriv\": {\r\n               \"derivative\": {\r\n                  \"buckets_path\": \"monthly_sum\"\r\n               }\r\n            }\r\n         }\r\n      }\r\n   }\r\n}\nIn the above query we are calculating the derivative  of the \u201cmonthly_sum\u201d and output is ..\n\"aggregations\": {\r\n      \"sales_per_month\": {\r\n         \"buckets\": [\r\n            {\r\n               \"key_as_string\": \"2014-01-01\",\r\n               \"key\": 1388534400000,\r\n               \"doc_count\": 3,\r\n               \"monthly_sum\": {\r\n                  \"value\": 185000\r\n               }\r\n            },\r\n            {\r\n               \"key_as_string\": \"2014-02-01\",\r\n               \"key\": 1391212800000,\r\n               \"doc_count\": 1,\r\n               \"monthly_sum\": {\r\n                  \"value\": 25000\r\n               },\r\n               \"sales_deriv\": {\r\n                  \"value\": -160000\r\n               }\r\n            },\r\n            {\r\n               \"key_as_string\": \"2014-03-01\",\r\n               \"key\": 1393632000000,\r\n               \"doc_count\": 1,\r\n               \"monthly_sum\": {\r\n                  \"value\": 30000\r\n               },\r\n               \"sales_deriv\": {\r\n                  \"value\": 5000\r\n               }\r\n            }, \u2026\u2026..13 more records\nFor the first bucket there is no derivate as derivate needs atleast 2 points.\nCumulative Sum Derivative\nThis is another Parent pipeline aggregation and calculates the cumulative sum of the specified metric of the input aggregation. In our\u00a0case it would help in giving us the cumulative sum of the total sales over a monthly basis.\nWe can replace the \u201csales_deriv\u201d part in our pervious query with this \u2013\n\n\"cumulative_sales\": {\r\n               \"cumulative_sum\": {\r\n                  \"buckets_path\": \"monthly_sum\"\r\n               }\r\n            }\nand get the following output\n\"aggregations\": {\r\n      \"sales_per_month\": {\r\n         \"buckets\": [\r\n            {\r\n               \"key_as_string\": \"2014-01-01\",\r\n               \"key\": 1388534400000,\r\n               \"doc_count\": 3,\r\n               \"monthly_sum\": {\r\n                  \"value\": 185000\r\n               },\r\n               \"cumulative_sales\": {\r\n                  \"value\": 185000\r\n               }\r\n            },\r\n            {\r\n               \"key_as_string\": \"2014-02-01\",\r\n               \"key\": 1391212800000,\r\n               \"doc_count\": 1,\r\n               \"monthly_sum\": {\r\n                  \"value\": 25000\r\n               },\r\n               \"cumulative_sales\": {\r\n                  \"value\": 210000\r\n               }\r\n            },\r\n            {\r\n               \"key_as_string\": \"2014-03-01\",\r\n               \"key\": 1393632000000,\r\n               \"doc_count\": 1,\r\n               \"monthly_sum\": {\r\n                  \"value\": 30000\r\n               },\r\n               \"cumulative_sales\": {\r\n                  \"value\": 240000\r\n               }\r\n            }, ..13 more records..\nWith this aggregation we can easily visualize the cumulative sum over certain peak period for various products to get more insights.\nBucket Script Aggregation\nThis is s parent pipeline aggregation and uses scripts to perform arithmetic computation on specified metrics of each bucket of a multi-bucket aggregation. A use-case can be to add/subtract or calculate percentage of a sub-aggregation in context of a bucket. For example if you want to calculate monthly percentage of total sales of the BMW car then first we would need to put a sub-aggregation in place in each bucket for BMW maker and then calculate the percentage of BMWs sold monthly in context of total sales.\nThis is Pipeline Aggregation uses scripting, please read\u00a0here\u00a0for more details. Currently I would be using inline scripting which as advised by elastic is not secure for production environment. Thus to enable inline scripting please add the following line to your elasticsearch.yml file in config folder.\nscript.inline: on\n\n\"aggs\": {\r\n      \"sales_per_month\": {\r\n         \"date_histogram\": {\r\n            \"field\": \"sold\",\r\n            \"interval\": \"month\",\r\n            \"format\": \"yyyy-MM-dd\"\r\n         },\r\n         \"aggs\": {\r\n            \"monthly_sum\": {\r\n               \"sum\": {\r\n                  \"field\": \"price\"\r\n               }\r\n            },\r\n            \"bmw_car\": {\r\n               \"filter\": {\r\n                  \"term\": {\r\n                     \"make\": \"bmw\"\r\n                  }\r\n               },\r\n                \"aggs\": {\r\n                    \"sales\": {\r\n                      \"sum\": {\r\n                        \"field\": \"price\"\r\n                      }\r\n                    }\r\n                  }\r\n            },\r\n            \"bmw_percentage\": {\r\n                    \"bucket_script\": {\r\n                        \"buckets_path\": {\r\n                          \"bmwSales\": \"bmw_car>sales\",\r\n                          \"totalSales\": \"monthly_sum\"\r\n                        },\r\n                        \"script\": \"bmwSales / totalSales * 100\"\r\n                    }\r\n                }\r\n         }\r\n      }\nResponse is \u2013\n{\r\n   \"key_as_string\": \"2014-01-01\",\r\n      \"key\": 1388534400000,\r\n      \"doc_count\": 3,\r\n       \"monthly_sum\": {\r\n          \"value\": 185000\r\n         },\r\n        \"bmw_car\": {\r\n          \"doc_count\": 2,\r\n           \"sales\": {\r\n              \"value\": 160000\r\n                 }\r\n             },\r\n           \"bmw_percentage\": {\r\n                  \"value\": 86.48\r\n             }\r\n        },\nThus for the month of Jan we can see 3 cars were sold and 2 were bmw and we get the % as 86.48.\nBucket Selector Aggregation\nThis parent pipeline aggregation is very useful in scenarios wherein you don\u2019t want certain buckets in the output based on a conditions supplied by you. Total_sum greater than some X, or Percentage greater than some value, Count > X etc.\nThis is again a script based aggregation thus we would need to have scripting enabled.\u00a0We just need to add the following snippet in the exact place where we added \u201csales_deriv\u201d aggregation as this aggregation is also parent aggregation.\n\n\"sales_bucket_filter\": {\r\n    \"bucket_selector\": {\r\n        \"buckets_path\": {\r\n            \"totalSales\": \"monthly_sum\"\r\n                  },\r\n            \"script\": \"totalSales >= 30000\"\r\n           }\r\n        }\r\n     }\nand now in the output we would only see the months where monthly sale is over 30000.\nHere\u2019s the Gist for Posting the 16 initial records \u2013\u00a0https://gist.github.com/tarunsapra/d2e5338bfb2cc032afe6\nConclusion\nThere are lot of real world use-cases for the pipeline aggregations and these would surely help in getting more insights from the data store in ES. I haven\u2019t covered Moving-average aggregation as that would be covered in a separate post as that\u2019s too vast for this blog post. Feel free to share your feedback/comments.\n", "tags": [], "categories": ["Blog", "Search"]}
{"post_id": 1706, "title": "First steps with elastic watcher", "url": "https://www.luminis.eu/blog-en/search-en/first-steps-with-elastic-watcher/", "updated_at": "2020-11-11T16:10:02", "body": "Some time a go elastic introduced a new product called watcher. Watcher is product that lets you take action based on the outcome of queries executed against elasticsearch but also against other http end points. The main purpose for the product is creating notification on some condition.\nIn this blog post I will explain the basic concepts and you can follow a long using some example code that I present in the blog post. At the end of the post you should have a general idea about what you can do with the watcher product.\nInstallation\nWatcher is a commercial plugin from elasticsearch. At the moment it is still in beta and you need to register with elastic to get beta access. More information can be found here:\nhttps://www.elastic.co/downloads/watcher\nLike with other commercial offerings from elastic, watcher comes as a plugin for elasticsearch. You have to install the license plugin as well as the watcher plugin. You can install the watcher plugin in your main cluster, but you can also install it in a seperate cluster. I have choosen to follow the second path. So I run two clusters on my local machine. The commercial cluster containing the commercial plugins runs on port 9300/9200 and the other cluster on 9301/9201.\nAfter installation you can check that it works using curl.\nCheck that it works:\ncurl -XGET 'http://localhost:9200/_watcher/stats?pretty'\r\n\nThe response in my case is.\n{\r\n  \"watcher_state\" : \"started\",\r\n  \"watch_count\" : 1,\r\n  \"execution_queue\" : {\r\n    \"size\" : 0,\r\n    \"max_size\" : 20\r\n  }\r\n}\nWhat we want to accomplish\nWe are going to log the amount of snapshots that we have in the repository. We also want to have a look at the watcher history\nStructure of a watcher\nTime to take one step further with the watcher. In this section we take a first look at the different elements of a watcher.\n\nMetadata \u2013 Static data added to the watch (payload), can be used in scripts or templates\nTrigger \u2013 Determines when a watch is checked. At the moment there are only scheduled triggers, but the future will most likely also bring us document event based triggers. All sorts of scheduled triggers are available. Think about hourly, dayly, weekly, etc. Each trigger can be configured to run on certain minutes every hour, or certain hours every day, etc.\nInput \u2013 Three types of input, Simple for testing, search for querying the local cluster and http for calling a remote service which can be a remote elasticsearch instance.\nConditions \u2013 Four type of conditions: always, never, compare ad script. Compare enables you to do some basic comparisons on the watch payload values. If you need a real script you also need to enable dynamic scripting. Since elasticsearch 1.6 there is fine grained control for the scripting support. More on this late on.\nTransforms \u2013 Three type of transforms: search, script and chain. Search replaces the current payload, script transforms the current payload and with chain you can add multiple search and script transformations in a chain. A good example is to determine the amount of errors and check if there is an action required based on the number of errors. Than in the transform we obtain all the errors and replace the payload with these obtained exceptions.\nActions \u2013 Watcher supports four types of actions email, webhook, index, and logging. Important to notice that you can have multiple actions. Log to the log service as well as send an email.\n\nLet us create a watcher\nBefore we can add the watcher let us have a look at the command to obtain snapshots. The following command shows all available snapshots for the repository named \u201ctemp-bck\u201d.\ncurl \"http://localhost:9201/_snapshot/temp-bck/_all\"\r\n\nThe actual response contains more items in the array, but each item looks like this snapshot.\n\n{\r\n  \"snapshots\" : [ {\r\n    \"snapshot\" : \"conferences-20150619094018\",\r\n    \"indices\" : [ \"conferences-20150619092724\" ],\r\n    \"state\" : \"SUCCESS\",\r\n    \"start_time\" : \"2015-06-19T07:40:18.466Z\",\r\n    \"start_time_in_millis\" : 1434699618466,\r\n    \"end_time\" : \"2015-06-19T07:40:18.540Z\",\r\n    \"end_time_in_millis\" : 1434699618540,\r\n    \"duration_in_millis\" : 74,\r\n    \"failures\" : [ ],\r\n    \"shards\" : {\r\n      \"total\" : 2,\r\n      \"failed\" : 0,\r\n      \"successful\" : 2\r\n    }\r\n  }]\r\n}\r\n\nNow let us use this request and response to create the watcher\nInput\nWe are using another cluster and we use a different elastic api than the search api. Therefore we have to use the http input.\n\n{\r\n  \"input\" : {\r\n    \"http\" : {\r\n      \"request\" : {\r\n        \"host\" : \"localhost\",\r\n        \"port\" : 9201,\r\n        \"path\" : \"/_snapshot/temp-bck/_all\"\r\n      }\r\n    }\r\n  }\r\n}\nThe trigger\nEvery watch needs a trigger. Since we only have schedule based triggers at the moment, we create a schedule based trigger. The following code shows a scheduled trigger to run every 5 seconds. A bit to much for this use case, but for testing purposes better. We don\u2019t want to wait to long for results. In reality you could be better of using one of the hourly/daily/weekly/monthly/yearly triggers. For this sample we stick with the interval based schedule.\n\n{\r\n  \"trigger\" : {\r\n    \"schedule\" : {\r\n      \"interval\" : \"5s\"\r\n    }\r\n  }\r\n}\nOne feature for calling the action is called throtteling. Only one instance of a watcher can run at a time and the action is by default only performed if it did not perform the past 5 seconds. This behavior can be customized using the property:\nthrottle_period\n. There is an additional concept for throtteling called acknowledgment. With acknowledgment the action will not be performed again after the first time before the action is acknowledged. More information about this can be found here.\nactions-ack-throttle\nCondition\nFor now we use the always condition, this means that every watch is passed on to the actions. This is the default condition when not providing a condition at all.\nTransform\nBefore taking action we want to transform the payload into some interesting numbers. We want to transform using a script. Before being able to execute scripts you have to configure the cluster to support scripts. Check the following resource for more information:\nmodules-scripting\nIn this case I needed to add the following setting to the elasticsearch.yml file\nscript.engine.groovy.inline.plugin: on\n\n{\r\n  \"transform\": {\r\n    \"script\": \"return [total_snapshots : ctx.payload.snapshots.size()]\"\r\n  }\r\n}\r\n\nAction\nThe most simple action is a log action, for now let us stick with that. We want to log the amount of available snapshots at the time of running the watcher.\n\n{\r\n  \"actions\" : {\r\n    \"log\" : { \r\n      \"logging\" : {\r\n        \"text\": \"Found {{ctx.payload.total_snapshots}} snapshots at {{ctx.execution_time}}\"\r\n      }\r\n    }\r\n  }\r\n}\r\n\nIf you are learning about the response, you can also print out \u201cctx.payload\u201d or even \u201cctx\u201d using this action. That way you can get an idea what information is available in the watcher context.\nPutting it all together\n\ncurl -XPUT \"http://localhost:9200/_watcher/watch/monitor_snapshots\" -d'\r\n{\r\n  \"trigger\": {\r\n    \"schedule\": {\r\n      \"interval\": \"5s\"\r\n    }\r\n  },\r\n  \"input\": {\r\n    \"http\": {\r\n      \"request\": {\r\n        \"host\": \"localhost\",\r\n        \"port\": 9201,\r\n        \"path\": \"/_snapshot/temp-bck/_all\"\r\n      }\r\n    }\r\n  },\r\n  \"transform\": {\r\n    \"script\": \"return [total_snapshots : ctx.payload.snapshots.size()]\"\r\n  },\r\n  \"actions\": {\r\n    \"log\": {\r\n      \"logging\": {\r\n        \"text\": \"Found {{ctx.payload.total_snapshots}} snapshots at {{ctx.execution_time}}\"\r\n      }\r\n    }\r\n  }\r\n}'\nNow you can check the log of elasticsearch to see the log lines coming by.\n[2015-06-21 12:22:42,899][INFO ][watcher.actions.logging  ] [Node-jc] Found 2 snapshots at 2015-06-21T10:22:42.882Z\r\n\nDo not forget to remove the watcher if you are done, or else it keeps logging \ud83d\ude42\ncurl -XDELETE \"http://localhost:9200/_watcher/watch/monitor_snapshots\"\r\n\nLook at the watcher history\nThere is a complete REST api for the watcher. Using this api you can create new watchers, remove watchers but also get information about a specific watcher. Given the watcher we have created in this blog, the following command can be used to obtain information about the watcher.\ncurl -XGET \"http://localhost:9200/_watcher/watch/monitor_snapshots\"\r\n\nWhat I did not expect was the status information of the watcher. This status information makes it easy to read the last time the watcher was executed, whether is was succesfull or not. Below I show you only the status part of the response\n\n{\r\n  \"status\": {\r\n    \"last_checked\": \"2015-06-21T21:14:28.547Z\",\r\n    \"last_met_condition\": \"2015-06-21T21:14:28.547Z\",\r\n    \"actions\": {\r\n      \"log\": {\r\n        \"ack\": {\r\n          \"timestamp\": \"2015-06-21T21:14:18.495Z\",\r\n          \"state\": \"ackable\"\r\n        },\r\n        \"last_execution\": {\r\n          \"timestamp\": \"2015-06-21T21:14:28.547Z\",\r\n          \"successful\": true\r\n        },\r\n        \"last_successful_execution\": {\r\n          \"timestamp\": \"2015-06-21T21:14:28.547Z\",\r\n          \"successful\": true\r\n        }\r\n      }\r\n    }\r\n  }  \r\n}\r\n\nTest execution of a watcher\nThis is such a feature that you learn to appreciate from the elastic guys. They like to make it easier for the developers. There is an execution api to force execution of a watcher. There are a lot of things to configure, there is something like a dry run. But what is even more interesting is providing a watcher inline, so without actually storing it in elasticsearch first. More information can be found here.\nInline watch\nThere are lots of other endpoints to be used using the REST api. You can start/stop/restart the watcher process. You can ask for stats about the watcher, but you can also query the history of the watchers using plain queries against the following index\nwatch_history-yyyy.MM.dd\n. This index is the trace of the watcher. It contains everything you need to find out what your watcher did, when it did it and what the resutls were. Since it is a normal index you can do everything you want with it. The following query just obtains the last 10 log entries. If you want to learn more about the response try it out yourself, you won\u2019t be disappointed, I promise.\ncurl -XGET \"http://localhost:9200/.watch_history-*/_search\" -d'\r\n{\r\n  \"sort\": [\r\n    {\r\n      \"trigger_event.triggered_time\": {\r\n        \"order\": \"desc\"\r\n      }\r\n    }\r\n  ]\r\n}'\r\n\nAdditional reading and watching\nFor a product in beta there is a fair amount of documentation. Also check the video with an introduction into watcher.\nhttps://www.elastic.co/guide/en/watcher/current/introduction.html\nWebinar by Uri Boness and Steve Kearns\n", "tags": [], "categories": ["Blog", "Search"]}
{"post_id": 2605, "title": "Improve my AngularJS project with grunt", "url": "https://www.luminis.eu/blog-en/search-en/improve-my-angularjs-project-with-grunt/", "updated_at": "2020-11-12T11:17:55", "body": "Some time a go I created issue 63 for my elasticsearch gui plugin. This issue was about improving the plugin with tools like grunt and bower. The plugin is an angularjs based library and it makes use of a number of other libraries like: d3, c3js, elasticsearch.js. In this blog post I am describing the steps I took to improve my web application using grunt and bower.\nBefore I started\nThe structure of the plugin was one from a basic one page web site. The starting page is index.html, this is in the root of the project. Within this file we include all the stylesheets, the javascript files and the javascript libraries. Libraries are in the lib folder, the angularjs files are in the js folders. There are some other parts, but these are the folders we are about to change. Before we can start improving the javascript inclusion we introduce grunt.\n\nInitialise grunt\nBefore you can use grunt, you need to install it. Installing grunt is easy if you have nodejs and npm already running. If you need help installing node, please check the references. What is different from a lot of other tools is that there is a thin client for the command line, grunt-cli, this wrapper uses the grunt that you usualy install per project. Installing the command line interface (cli) is done using npm.\n\nnpm install -g grunt-cli\nNow we are ready to start using grunt for our project. First step is to setup our project using the nodejs package.json file. There are some options to generate this file, one using npm init and another using grunt-init project templates. This is discussed in the next section.\nCreate the package.json\nThis is the file used to manage dependencies and give information about the project at hand. If you are a java developer like me, you can compare this with the pom.xml for java projects. Below you\u2019ll find the package.json from my project.\n\n{\r\n  \"name\": \"elasticsearch-gui\",\r\n  \"version\": \"1.2.0\",\r\n  \"description\": \"Elasticsearch plugin to show you what data you have and learn about queries\",\r\n  \"main\": \"index.html\",\r\n  \"repository\": {\r\n    \"type\": \"git\",\r\n    \"url\": \"https://github.com/jettro/elasticsearch-gui.git\"\r\n  },\r\n  \"keywords\": [\r\n    \"elasrticsearch\",\r\n    \"plugin\",\r\n    \"gui\"\r\n  ],\r\n  \"author\": \"Jettro Coenradie\",\r\n  \"license\": \"Apache-2.0\",\r\n  \"bugs\": {\r\n    \"url\": \"https://github.com/jettro/elasticsearch-gui/issues\"\r\n  },\r\n  \"homepage\": \"https://github.com/jettro/elasticsearch-gui\",\r\n  \"devDependencies\": {\r\n    \"grunt\": \"~0.4.5\",\r\n    \"grunt-contrib-concat\": \"~0.4.0\",\r\n    \"grunt-contrib-uglify\": \"~0.5.0\",\r\n    \"grunt-contrib-watch\": \"~0.6.1\"\r\n  }\r\n}\nOf course you write this file by hand, but there is an easier way by using\u00a0npm init. This will present a wizzard to help you create the file. Than you can start adding dependencies. You need dependencies when using grunt plugins for instance but also for the actual grunt version used to build the project. Npm has a shorthand to install these development dependencies into the package.json. The following example shows how to install the grunt library into the project. The other dependencies can be installed the same way.\n\nnpm install grunt --save-dev\nYou can run npm install to install all dependencies locally as well. Next step is to start configuring the grunt project. This is discussed in the next section.\nCreate Gruntfile.js\nThe gruntfile is where we include plugins, create tasks and combine tasks in aliases. The following code block shows the empty template for a grunt file.\n\n'use strict';\r\nmodule.exports = function (grunt) {\r\n};\nBefore I am going to show what is in mine, I want to mention a wizzard like appraoch using project templates. Grunt comes with a grunt-init cli. You have to install it as a node module using the following command.\nnpm install -g grunt-init\nThan you have to add templates. This is done using a git clone of the template repository. Of course you can also create your own templates. An example template is in the following url:\nhttps://github.com/gruntjs/grunt-init-gruntfile\nJust clone it into the folder .grunt-init and you are good to go. Now execute the following command and a wizzard will be presented to generate the Gruntfile.js.\ngrunt-init gruntfile\nNow we are going to look at some of the elements in my Gruntfile. First we are looking at template support. Again templates? Yes, but now we are talking about templates or strings with placeholdes, to include in files.\nShow template support\nGrunt comes with options to use templates for files or headers. In our generated javascript file we create a header. The following code block shows the part in the configuratio file.\n\nmodule.exports = function (grunt) {\r\n    grunt.initConfig({\r\n        pkg:grunt.file.readJSON('package.json'),\r\n        banner: '/*! <%= pkg.title || pkg.name %> - v<%= pkg.version %> - ' +\r\n        '<%= grunt.template.today(\"yyyy-mm-dd\") %>\\n' +\r\n        '<%= pkg.homepage ? \"* \" + pkg.homepage + \"\\\\n\" : \"\" %>' +\r\n        '* Copyright (c) <%= grunt.template.today(\"yyyy\") %> <%= pkg.author.name %>;' +\r\n        ' Licensed <%= _.pluck(pkg.licenses, \"type\").join(\", \") %> */\\n',\r\n    });\r\n};\nNotice that we assign the package.json file to the pkg variable. This way we can read properties like pkg.homepage and reuse them in the template. The second variable we create is the banner. I think you can undertand how spring concatenation with parameters works. Now we have the string in the banner variable and we can later on use it.\nThe next step is to gather all javascript files and concatenate them in one file.\nConcatenate my own javascript files\nBefore moving into the libraries we want to gather all javascript files for controllers, services, filters, etc into one big file.\n\nmodule.exports = function (grunt) {\r\n    grunt.initConfig({\r\n        concat: {\r\n            options: {\r\n                banner: '<%= banner %>',\r\n                stripBanners: true\r\n            },\r\n            dist: {\r\n                src: [\r\n                    'js/app.js','js/controllers/*','js/directives.js','js/filters.js','js/services.js'\r\n                    ],\r\n                dest: 'assets/js/<%= pkg.name %>.js'\r\n            }\r\n        },\r\n    });\r\n    grunt.loadNpmTasks('grunt-contrib-concat');\r\n};\nNotice that we do a loadNpmTasks. This is the module system of Grunt. In the initConfig we configure the concat task with options and the tasks that can be executes. In this case there is just one task\u00a0dist. You configure the input folders and well as the destination file. That is it. Testing the task is easy by typing:\n\ngrunt concat:dist\nSince we are now generating one file, it is also easy to move all the lengthy controllers into separate files. I did not do the services yet, but this will be done in the future. I also need to extract some copy paste functions into a service or helper thing. Enough to do. In the src you can see we include\u00a0js/controllers/*\u00a0meaning all the files in the controllers directory.\nTo make the file smaller to download, we need to uglify it. That is the next plugin that we are going to use.\nUglify the generated javascript file\nThe uglify part is not so important to me, for me the fact that it makes the script a lot smaller is much more important. There is a catch though. You have to be carefull with angularjs and the dependency injection. A lot has been written on this topic, I like this\u00a0blogpost by Aaron Smith. It took me a long time to get it right, changed it and changed it back again. In the end it turned out I forgot factory in the app.js file. Most important is that you need to inject objects with a string containing the name. That way it is not changed by the minification process and the injection is not broken. Below the example of how it works with the one method that I forgot.\n\nmyApp.factory('$exceptionHandler',['$injector', function($injector) {\r\n    return function(exception, cause) {\r\n        var errorHandling = $injector.get('errorHandling');\r\n        errorHandling.add(exception.message);\r\n        throw exception;\r\n    };\r\n}]);\nIf you omit the\u00a0\u2018$injector\u2019\u00a0than it will give you problems. Now we have one big file including the banner we dicussed before. We still have a lot of other javascript files. We have all the libraries that were manually inserted into the project. Of course we want to improve this as well using bower. This is discussed in the next section.\nObtain libraries using bower\nBefore I manually checked the libraries I needed, downloaded them and put them in the lib folder. This is fine in the beginning, but after a few iteration it becomes annoying. Other already asked me to include bower for dependency inclusion.\u00a0Bower\u00a0is again a nodejs project or npm module. The following code block shows the command.\n\nnpm install -g bower\nThe next step is to create a bower.json file. This file contains the dependencies that your project requires. Below is my bower.json file.\n{\r\n  \"name\": \"elasticsearch-gui\",\r\n  \"dependencies\": {\r\n    \"ui-bootstrap\": \"~0.11.0\",\r\n    \"angular\": \"~1.2.16\",\r\n    \"angular-route\": \"~1.2.27\",\r\n    \"c3\": \"~0.4.7\",\r\n    \"elasticsearch\": \"~3.0.1\"\r\n  }\r\n}\nThe dependencies are the minimum required dependencies. Beware though what bower brings in. The ui-bootstrap project for instance needs to be build before you can use it. For now this is a manual step, but I am sure this can be automated as well. If the file is correct, run\u00a0bower install<\u00a0and watch the default folder\u00a0bower_components. Now we have all the required libraries, we have to run\u00a0npm install\u00a0and\u00a0grunt\u00a0in the ui-bootstrap folder and we are ready for the next section.\n\n\n\nAdd the libraries to the concatenated javascript file\nWe prefer as little javascript files as possible, therefore we also want to include the libraries into the minified file. Since we are first gathering all the files and only then minify the complete file. We copy the non minified version of the libraries into the big file and minify it at once. The following code block show the extended concat plugin configuration.\n\nmodule.exports = function (grunt) {\r\n    grunt.initConfig({\r\n        concat: {\r\n            options: {\r\n                banner: '<%= banner %>',\r\n                stripBanners: true\r\n            },\r\n            dist: {\r\n                src: [\r\n                    'bower_components/angular/angular.js',\r\n                    'bower_components/angular-route/angular-route.js',\r\n                    'bower_components/ui-bootstrap/dist/ui-bootstrap-0.11.2.js',\r\n                    'bower_components/elasticsearch/elasticsearch.angular.js',\r\n                    'bower_components/d3/d3.js',\r\n                    'bower_components/c3/c3.js',\r\n                    'js/c3js-directive.js',\r\n                    'js/app.js','js/controllers/*','js/directives.js','js/filters.js','js/services.js'\r\n                    ],\r\n                dest: 'assets/js/<%= pkg.name %>.js'\r\n            }\r\n        },\r\n    });\r\n    grunt.loadNpmTasks('grunt-contrib-concat');\r\n};\nThat is it, we now have improved the loading of javascript files and we make it easier to upgrade the libraries. Of course you can run grunt from the command line, but if you are an intellij user like me you can also use the grunt runner.\nShow intellij support\nIntellij comes with support for grunt. Intellij recognises the Gruntfile.js and abstracts the tasks available in the file. Using intellij you can run the grunt tasks. The following image shows the runner from within intellij.\n\nFinal thoughts\nI know it is not perfect yet, so there will be future improvement. If you spot improvement, feel free to comment and create an issue in the github project. I personally don\u2019t like the separate package.json and bower.json, so maybe there is a better option. Like I mentioned before I want to improve the copy paste code in the controllers, take apart the services. Still I really like the improvements I could made with Grunt and Bower. For me it will be standard to use them the coming projects.\nReferences\n\nAutomate with Grunt\u00a0\u2013 I used this book to get basic knowledge about Grunt, not to many pages, enough to get you started.\nElasticsearch GUI\u00a0\u2013 Source repository on github for the plugin.\nInstalling nodejs\u00a0\u2013 You can install node using this resource or on the mac just do brew install node\nBlogpost about angularjs safe minification\n\n", "tags": [], "categories": ["Blog", "Search"]}
