{"post_id": 39613, "title": "Het Nederlands Gebarencentrum lanceert gebarenwoordenboek app", "url": "https://www.luminis.eu/blog-en/nederlands-gebarencentrum-de-app/", "updated_at": "2015-10-28T13:12:11", "body": "Gebarenwoordenboek iSignNGT voor Apple (iOS) en Android\nIn samenwerking met Luminis Arnhem lanceert het Nederlands Gebarencentrum de applicatie iSignNGT. iSignNGT is een Basis Gebarenwoordenboek met standaard gebaren uit de Nederlandse Gebarentaal (NGT), de taal van dove mensen in Nederland. De gebruiker krijgt de basis set van ruim 600 gebaren gratis. Voor alle gebaren is een filmpje beschikbaar, daarnaast kunnen er ook tekeningen van het gebaar, een pictogram en een afbeelding te zien zijn. De lijst met begrippen is alfabetisch en thematisch georganiseerd.\nDe gebruiker kan makkelijk switchen tussen het filmpje en de overige gegevens. De app is heel geschikt voor iedereen die in gebaren wil communiceren met dove kinderen en volwassen. Aan het basisgebarenwoordenboek kunnen thema\u2019s toegevoegd worden via een in-app purchase. De thema\u2019s die aangeschaft kunnen worden zijn te vinden in de themalijst. Dit zijn thema\u2019s als Museumlexicon, Sprookjes en verhalen en lexicon thema\u2019s behorend bij cursusmodules zoals NmGAB1 en NGTAB1. Het aantal gebaren en de prijs per thema verschilt.\n\n\n\n\n\n\n\n\n\nOver het Nederlandse Gebarencentrum\nHet Nederlands Gebarencentrum is opgericht in 1996 en is sinds 2004 door de overheid erkend als het Lexicografisch Instituut op het gebied van de Nederlandse Gebarentaal in al haar verschijningsvormen. Ze verzamelen gebaren van dove gebruikers van de Nederlandse Gebarentaal en maken de gegevens toegankelijk onder andere via een online gebarenwoordenboek waarin zowel standaardgebaren als regionale varianten te vinden zijn. Daarnaast ontwikkelen en produceren ze cursusmaterialen, (digitale) woordenboeken en educatief materiaal, wordt er onderzoek gedaan naar de grammatica van NGT en kennis gedeeld in de vorm van workshops NGT en NmG, cursussen NmG. Tevens geeft het Nederlandse Gebarencentrum voorlichting en advies over de toepassing van gebaren(taal) in de communicatie met diverse doelgroepen.\nOver Luminis Arnhem\n Luminis Arnhem is een software-technologiebedrijf dat business modellen van bedrijven aansluit op het internet. De kansen die het internet hiervoor biedt, diept Luminis samen met haar klanten uit. Beveiliging, schaalbaarheid en een continue veranderende markt vragen om visie en strategie in plaats van een appje of \u2018een stukje software\u2019. Bij Luminis werken gepassioneerde vakmensen aan nieuwe toepassingen. Software is overal, dat betekent dat gebruikers steeds vaker en in totaal verschillende contexten hiermee geconfronteerd worden. Luminis Arnhem heeft dan ook naast software-experts, User eXperience designers in dienst. De gebruiker staat van begin af aan centraal; de combinatie van UX en technologie in e\u0301e\u0301n team levert haalbare innovaties op!\niSignNGT Gebarenwoordenboek downloaden\n", "tags": [], "categories": ["Blog"]}
{"post_id": 2533, "title": "Infrastructure metrics with Elasticsearch stack", "url": "https://www.luminis.eu/blog-en/search-en/infrastructure-metrics-with-elasticsearch-stack/", "updated_at": "2020-11-12T17:09:56", "body": "For the operations team of any IT organisation it\u2019s of utmost importance to have an overview of it\u2019s infrastructure at any given point of time, whether it\u2019s the response time of various customer facing systems or memory consumption stats of the processes running across servers.\nTraditionally, these stats were\u00a0explored after some problem has occurred which can be anything from slow response time to certain processes taking over the CPU cycles. Thus it\u2019s better to have a real-time insight into the infrastructure stats which enables DevOps team to quickly find and fix the cause rather than waiting for the problem to occur. In this blog post we would be exploring the Elasticsearch product stack to propose a solution for effective monitoring of the infrastructure using Elasticsearch, Kibana and Beats.  Beats shippers are part of the Elasticsearch product stack, mainly classified into PacketBeat and TopBeat. These shippers integrate seamlessly with Elasticsearch and Kibana and thus are very useful for generating infrastructure metrics. PacketBeat is used for monitoring real-time network traffic for application level protocols like HTTP, MySQL etc as well as support for DNS which is based on UDP protocol. Whereas TopBeat is the new addition to the Elastic family and it\u2019s basically the visual counter-part of the Top command used in the terminal. Using TopBeat we can get system wide stats of memory usage (free/available) and also process specific stats, i.e. Top 10 memory consuming processes, disk space used by the process etc. PacketBeat It analyzes network packets in real-time and correlates requests with responses in order to log them as specific transactions. For each transaction it records the protocol specific fields in the ElasticSearch indices. Some examples \u2013 Mysql fields Mysql. affected_rows\u00a0 \u2013 Number of rows affected by the operation Mysql. num_fields \u2013 Incase of successful \u201cSelect\u201d how many fields are returned. Mysql.num_rows \u2013 Incase of successful \u201cSelect\u201d how many rows are returned. Query \u2013 complete query which was executed. Along with other fields like server and client IP, timestamp and if the query failed then related error information. Http Fields\u00a0 Http.code \u2013 Http code Http.content_length \u2013 size of the entity-body Along with other fields like path, params, query, response time, status etc Apart from the protocol specific fields there are some fields which are generic for all transactions for a complete list of fields please see\u00a0here Protocols supported by PacketBeat are \u2013\n\nHTTP\nPostgreSQL\nMySQL\nThrift-RPC\nRedis\nMongodb\nMemcache\nDNS\n\nPacketbeat can either be installed on the existing application servers or dedicated servers. Installing them on dedicated servers reduces the load on the application servers but it\u2019s also more costly specially in an cloud hosted environment. By using port mirroring or tap devices the network packets can be analyzed in case the packbeat is installed on the dedicated server and if it\u2019s on the same server as the application then by analyzing packets on network interfaces specified in the configuration file. The network stats would either be sent directly by PacketBeat to ElasticSearch for creating index or if an logstash indexing instance is being used for aggregating logs from various sources and then sending them finally to ElasticSearch thus in that case we would write the json output of Packetbeat first to Redis as currently the direct integration between logstash and Packetbeat doesn\u2019t exist.\n\nFigure 1- Taken from official documentation of Packetbeat Packetbeat installation is straightforward for linux and mac, for windows you need to have the powershell installed on your machine. Since I have a mac thus the commands and configuration would be mac specific and it\u2019s not much different from linux configuration. Before installing Packetbeat please make sure you have ElasticSearch and Kibana installed on your machine. PacketBeat ships the json data to ElasticSearch for indexing and that index data is displayed in graphs in Kibana. Once you have downloaded and unzipped the Packetbeat, following are the important sections prefilled in packetbeat.yml configuration file.\n\u00a0\ninterfaces:\r\n  device: any\nIn the device section, it\u2019s the network interfaces that need to be monitored are mentioned, using the keyword \u201cany\u201d implies that all network interfaces need to be sniffed for packets, current \u201cany\u201d is supported on Linux and\u00a0not on mac. On Mac for every individual network interface there needs to be separate Packetbeat instance running , for example If I want to monitor the internet traffic as well as the MySQL running on localhost then I would need to start 2\u00a0 instances of PacketBeat one having the device as \u201cen0\u201d (HTTP/DNS monitoring) and other as \u201clo0\u201d (MySQL monitoring).\u00a0 For each protocol to be monitored you need to mention the network port, Example \u2013\n\u00a0\nmemcache:\r\n  ports: [11211]\r\nmysql:\r\n  ports: [3306]\nThe configuration file already contains the default ports of the protocols supported, additional ports can be added by using comma separated values. Like in HTTP \u2013\nhttp:\r\n  ports: [80, 8080, 8000, 8002]\r\n  send_headers: [\"Host\"]\nAlso note, you can add the http header to the JSON o/p of the packetBeat by specifying the \u201csend_headers\u201d options which can contains HTTP headers.\u00a0 In my case I wanted the domain name of the website visited which is contained in the \u201cHost\u201d header of the request. You can further add more headers like \u201cUser-Agent\u201d, \u201cCookie\u201d etc. In the http configuration, you can specify the scenario in which you would like to hide certain parameters like \u201cpassword\u201d from being written to JSON o/p, thus the http configuration becomes \u2013\n\u00a0\nhttp:\r\n   ports: [80, 8080, 8000, 5000, 8002]\r\n   send_headers: [\"Host\"]\r\n   hide_keywords: [\"pass\", \"password\", \"passwd\"]\nApart from the application protocols, you can also customize the shipper that Packetbeat uses, following are the major options available \u2013\nshipper:\r\n  name: \u201cMyShipper\u201d\r\n  tags: [\u201cwebserver\u201d]\r\n  ignore_outgoing: true\nName\u00a0attribute specifies the name of the shipper, the\u00a0tags\u00a0attribute helps in grouping together of various shippers, for example if you have a cluster of webservers then all the different shippers should have a common binding tag data which make it easy to create grouped visualisation in Kibana based on common tag values. The \u201cignore_outgoing\u201d attribute is\u00a0basically is avoid the scenario of a single transaction across multiple servers to be logged twice, i.e. only incoming transaction are logged and outgoing transactions are ignored thus removing the chance of logging duplicate transactions. PacketBeat output PacketBeat can write to either Elasticsearch, Redis or File. Example configuration \u2013\n\u00a0\noutput:\r\n# Elasticsearch as output\r\nelasticsearch:\r\n   enabled: true\r\n   hosts: [\"localhost:9200\"]\r\n   save_topology: true\nThe value of enabled attribute identifies the output type out of the 3 supported. \u201csave_topology\u201d being set to true means that the Packetbeat shippers publish their IPs to an ElasticSearch index and all the shippers can share this information via the output plugin. You can also provide Elasticsearch authentication credentials in the yml configuration if Elasticsearch is using Shield plugin, thus Packetbeat shippers would need to be authenticated before being able to write to ES cluster. Now we are done with the configuration of Packetbeat and before you could start Packetbeat you need to make sure that the Elasticsearch cluster that you would be writing to has the template stored i.e. in Elasticsearch you can store templates such that when new indexes are created then the mapping of the fields of these new indexes conform to the template rules. The template is provided inside the file packetbeat.template.json in the installation directory to store the template on ES cluster we just need to execute the curl command (make sure ES is up and running!) \u2013\ncurl -XPUT \u2018http://localhost:9200/_template/packetbeat\u2019\u00a0-d@packetbeat.template.json\nNow we have the template store on ES cluster and new indexes would have the naming format as \u2013 packetbeat-YYYY.MM.DD Starting Packetbeat sudo ./packetbeat -e -c packetbeat.yml -d \u201cpublish\u201d (on mac) sudo /etc/init.d/packetbeat start (on linux) Now, I browsed internet while Packetbeat was running and since it was listening to http requestthus it logged those requests in the ES cluster with index name packetbeat-2015.10.01 I go to my marvel (ES plugin) console and execute \u2013\nGET packetbeat-2015.10.01/_search?size=1000\nAnd get back a total of 585 hits after few minutes of browsing which ofcourse includes all the js and css file\u2019s GET requests as well \ud83d\ude42 Here\u2019s the complete output of a single such document \u2013\n\u00a0\n{\r\n\"_index\":\"packetbeat-2015.10.01\",\r\n\"_type\":\"http\",\r\n\"_id\":\"AVAkFdJyMolTkkADd_vF\",\r\n\"_score\":1,\r\n\"_source\":{\r\n\"bytes_in\":1733,\r\n\"bytes_out\":439,\r\n\"client_ip\":\"10.108.xx.xx\",\r\n\"client_port\":53293,\r\n\"client_proc\":\"\",\r\n\"client_server\":\"Taruns-MacBook-Pro.local\",\r\n\"count\":1,\r\n\"http\":{\r\n\"code\":301,\r\n\"content_length\":178,\r\n\"phrase\":\"Permanently\",\r\n\"request_headers\":{\r\n\"host\":\"cnn.com\"\r\n},\r\n\"response_headers\":{\r\n}\r\n},\r\n\"ip\":\"157.166.226.25\",\r\n\"method\":\"GET\",\r\n\"params\":\"\",\r\n\"path\":\"/\",\r\n\"port\":80,\r\n\"proc\":\"\",\r\n\"query\":\"GET /\",\r\n\"responsetime\":104,\r\n\"server\":\"\",\r\n\"shipper\":\"Taruns-MacBook-Pro.local\",\r\n\"status\":\"OK\",\r\n\"timestamp\":\"2015-10-01T15:47:00.519Z\",\r\n\"type\":\"http\"\r\n}\r\n}\nNow, if we want to analyze network packets for the mysql connection then as I mentioned earlier that on mac we can monitor only one device interface using one packetbeat instance hence we would need to start another packetbeat instance for monitoring mysql traffic (device :lo0 is the change in the configuration file), on unix systems using \u201cany\u201d keyword in the device option of interface makes it possible to analyze data packets on all network interfaces.\n\u00a0\ninterfaces:\r\n device: lo0\nNow, packetbeat\u2019s instance is started again after making changes in the configuration file and now it will start analyzing mysql traffic and sending json data for indexing in the ES cluster.Let\u2019s look at one of document indexed Elasticsearch.\n{\"_index\": \"packetbeat-2015.10.01\",\r\n \"_type\": \"mysql\",\r\n \"_id\": \"AVAlD_IdtouTb4teCHYB\",\r\n \"_score\": 1,\r\n \"_source\": {\r\n \"bytes_in\": 51,\r\n \"bytes_out\": 52,\r\n \"client_ip\": \"127.0.0.1\",\r\n \"client_port\": 60083,\r\n \"client_proc\": \"\",\r\n \"client_server\": \"taruns-mbp.home\",\r\n \"count\": 1,\r\n \"ip\": \"127.0.0.1\",\r\n \"method\": \"UPDATE\",\r\n \"mysql\": {\r\n \"affected_rows\": 1,\r\n \"error_code\": 0,\r\n \"error_message\": \"\",\r\n \"insert_id\": 0,\r\n \"iserror\": false,\r\n \"num_fields\": 0,\r\n \"num_rows\": 0\r\n },\r\n \"path\": \"\",\r\n \"port\": 3306,\r\n \"proc\": \"\",\r\n \"query\": \"update authors set name = \\\"taruns\\\" where id =1\",\r\n \"responsetime\": 2,\r\n \"server\": \"taruns-mbp.home\",\r\n \"shipper\": \"taruns-mbp.home\",\r\n \"status\": \"OK\",\r\n \"timestamp\": \"2015-10-01T20:20:12.832Z\",\r\n \"type\": \"mysql\"\r\n }\r\n}\nNow in the above document we notice we have the mysql method \u201cUpdate\u201d the \u201caffected rows\u201d, the complete query and \u201cresponetime\u201d using responsetime and Kibana graphs we can clearly see the queries that took maximum amount of time during a certain duration. TopBeat TopBeat is the new addition to the Elastic\u2019s product stack, it\u2019s basically the visual alternative of the terminal\u2019s top command. \u00a0After downloading the zip file of the project and creating the Topbeat template, we just need to unzip it and start the executable (just make sure the that elasticsearch is running as well). Following is the sample default configuration provided in the topbeat.yml file\n\u00a0\ninput:\r\n period: 20\r\n procs: [\".*\"]\r\nshipper:\r\n name:\r\noutput:\r\nelasticsearch:\r\n  enabled: true\r\n  hosts: [\"localhost:9200\"]\nThe default name of the Topbeat shipper is the hostname, the period 20 secs basically means the time period interval of collecting stats and currently we are monitoring all the processes but we can give specific processes in a comma separated manner. This project ships json data to Elasticsearch which can later be visualised using Kibana. Since it\u2019s analogous to the top command thus it monitors the memory consumption of a process, it\u2019s state, process\u2019s cpu usage user space, system space and start-time. Following is a document from Elasticsearch sent by Topbeat shipper.\n\u00a0\n{\r\n \"_index\":\"topbeat-2015.09.23\",\r\n \"_type\":\"proc\",\r\n \"_id\":\"AU_5ns2rNiMHwRXM6TOU\",\r\n \"_score\":1,\r\n \"_source\":{\r\n \"count\":1,\r\n \"proc.cpu\":{\r\n \"user\":9620,\r\n \"user_p\":0.04,\r\n \"system\":4504,\r\n \"total\":14124,\r\n \"start_time\":\"10:48\"\r\n },\r\n \"proc.mem\":{\r\n \"size\":1635956,\r\n \"rss\":163396,\r\n \"rss_p\":0.97,\r\n \"share\":0\r\n },\r\n \"proc.name\":\"Microsoft Word\",\r\n \"proc.pid\":14436,\r\n \"proc.ppid\":1,\r\n \"proc.state\":\"running\",\r\n \"shipper\":\"Taruns-MacBook-Pro.local\",\r\n \"timestamp\":\"2015-09-23T09:52:57.283Z\",\r\n \"type\":\"proc\"\r\n }\r\n}\nThe name of the index(if not provide) is topbeat-YYYY.MM.DD. So in the above document we can see that the proc.name as \u201cMicrosoft Word\u201d , the process id, the parent process id. The start time, the process\u2019 cpu stats i.e. user space and system space and the memory stats i.e. virtual memory,\u00a0 resident memory(rss) (and percentage) and the total memory. In case of Topbeat as well we are writing the shipper output to Elasticsearch, we can also use Redis and File system just as in Packetbeat.\u00a0Since I am using my local machine for the blog, but in production environments, you would be having indexes generated on a daily basis and since Elasticsearch provides excellent cluster support thus scaling shouldn\u2019t be a problem. Let\u2019s now create some visualizations using Kibana, I am assuming you have basic Kibana understanding, in case you don\u2019t know much about it, you can always read the official\u00a0docs. We first need to add both the indexes in Kibana (settings section) \u2013\n\nMysql method breakdown \u2013\n\nHttp response code overview \u2013\n\nWe can further use sub-aggregations to find reponsetime for each code, domains visited etc. i.e. we have so many fields that the possibilities are pretty much endless when it comes to creating various charts based on fields available to us for each indexes. Similarly, for the Topbeat indexes we have lot of possibilities, on of it being top 10 process based on resident memory consumption.\n\nAll the visualizations can then be added onto a real-time dashboard based on \u201clast 24 hour\u201d condition thus we could easily have an overview of last 24 hours of activity. Conclusion \u2013\u00a0 We have looked at Packetbeat and Topbeat, two products which when used with ElasticSearch and Kibana can help generate some fantastic real-time Dashboard and give us insights into our Infrastructure.\n", "tags": [], "categories": ["Blog", "Search"]}
